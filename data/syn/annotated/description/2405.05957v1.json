{
    "title": "OpenBA-V2: Reaching 77.3% High Compression Ratio with Fast Multi-Stage Pruning",
    "abstract": "Large Language Models (LLMs) have played an important role in many fields due to their powerful capabilities.\nHowever, their massive number of parameters leads to high deployment requirements and incurs significant inference costs, which impedes their practical applications.\nTraining smaller models is an effective way to address this problem.\nTherefore, we introduce OpenBA-V2, a 3.4B model derived from multi-stage compression and continual pre-training from the original 15B OpenBA model.\nOpenBA-V2 utilizes more data, more flexible training objectives, and techniques such as layer pruning, neural pruning, and vocabulary pruning to achieve a compression rate of 77.3% with minimal performance loss.\nOpenBA-V2 demonstrates competitive performance compared to other open-source models of similar size, achieving results close to or on par with the 15B OpenBA model in downstream tasks such as common sense reasoning and Named Entity Recognition (NER).\nOpenBA-V2 illustrates that LLMs can be compressed into smaller ones with minimal performance loss by employing advanced training objectives and data strategies, which may help deploy LLMs in resource-limited scenarios.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In recent years, Large Language Models (LLMs) have demonstrated powerful capabilities in natural language understanding and generation, leading to significant achievements in various tasks such as dialogue generation, code generation, text summarization, and machine translation (OpenAI, 2023  ###reference_b50###; Touvron et al., 2023  ###reference_b64###; Jiang et al., 2023  ###reference_b33###; Bai et al., 2023  ###reference_b4###; Li et al., 2023b  ###reference_b37###).\nHowever, their extensive demand for computing resources makes them impractical in resource-limited scenarios, such as PCs or mobile phones (Thawakar et al., 2024  ###reference_b62###).\nFurthermore, the high costs of inference and storage impede their widespread application across various industries (Bai et al., 2024  ###reference_b3###).\nTo address these challenges, many researchers attempt to reduce the computational and storage requirements of LLMs by designing smaller models.\nThese smaller models are usually obtained by training from scratch (Geng & Liu, 2023  ###reference_b24###; Zhang et al., 2024  ###reference_b80###; Li et al., 2023c  ###reference_b40###; mic, 2024  ###reference_b1###) or compressing larger models (Xia et al., 2024  ###reference_b71###; Ma et al., 2023a  ###reference_b48###).\nSome previous works (Li et al., 2023c  ###reference_b40###; Bai et al., 2023  ###reference_b4###) emphasize the importance of prioritizing data quality over quantity when training smaller models from scratch. They demonstrate that small models can potentially outperform their larger counterparts with lower training costs.\nThis insight offers a promising approach to training more powerful models with fewer resources.\nFrom another perspective, model compression, which includes pruning, distillation, and quantization, are presented as a method to strike a balance between efficiency and performance for existing LLMs.\nPruning accelerates LLMs by removing non-essential parameters of the network with specialized hardware (Ma et al., 2023a  ###reference_b48###; Frantar & Alistarh, 2023  ###reference_b20###; Sun et al., 2024  ###reference_b58###).\nDistillation enables the model to acquire knowledge rapidly from a teacher model by mimicking the teacher\u2019s behavior (Wu et al., 2023  ###reference_b69###; Hsieh et al., 2023  ###reference_b30###).\nQuantization can lower the costs of model storage and inference by converting the model to lower precision, more computationally efficient data types (Frantar et al., 2023  ###reference_b21###; Dettmers et al., 2024  ###reference_b16###).\nTo accommodate low-resource and low-cost requirements, we introduce OpenBA-V2, an encoder-decoder Transformer model with 3.4B parameters.\nOpenBA-V2 achieves a 77.3% compression ratio of OpenBA (Li et al., 2023b  ###reference_b37###), significantly lowering the resource requirements for deployment while maintaining high performance.\nOpenBA-V2 adopts a multi-stage compression strategy that employs layer pruning or neural pruning at each stage, followed by a period of fast and efficient recovery training to minimize performance loss due to model compression.\nAfter several compression stages, the model size has been reduced from 15B to 3.8B.\nSubsequently, we use 700B tokens to continually pre-train the compressed model with an optimized objective, further boosting training efficiency and enhancing the model\u2019s capabilities.\nFinally, we prune the model\u2019s vocabulary because of its redundancy, reducing the model size from 3.8B to 3.4B with almost no performance loss.\nIn addition, we have compiled a more extensive and diverse dataset from various sources compared to OpenBA.\nThese strategies have enabled OpenBA-V2 to achieve high performance with much fewer parameters.\nThrough OpenBA-V2, we aim to demonstrate that smaller models can achieve comparable performance to larger models through better training objectives and data strategies, facilitating the deployment across various industries of LLMs. Our code and model weights are available at https://github.com/OpenNLG/OpenBA-v2  ###reference_###."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Lightweight LLMs Pre-trained from Scratch",
            "text": "Large language models (LLMs) have been proven to be very effective and have brought unprecedented success to various fields of artificial intelligence. Despite the trend towards developing larger models (with over a hundred billion parameters), lightweight LLMs have also played a minimal role in today\u2019s landscape for enabling efficient inference in limited hardware resources and edge devices. Most lightweight LLMs were the by-product of the process when researchers explored the larger models and proposed alongside their larger version in a model family, such as OPT (Zhang et al., 2022  ###reference_b81###), BLOOMZ (Workshop et al., 2022  ###reference_b68###), and more recent releases including GPT-Neo (Black et al., 2022  ###reference_b10###), Galactica (Taylor et al., 2022  ###reference_b61###), QWEN (Bai et al., 2023  ###reference_b4###), as well as the LLM analyzing suite Pythia (Biderman et al., 2023  ###reference_b7###) and the transformer variant RWKV (Peng et al., 2023  ###reference_b52###). This type of lightweight LLMs generally follows the scaling law (Hoffmann et al., 2022  ###reference_b29###), which recommends the relationship between model parameters and training data size for training LLMs. However, the recent evidence demonstrates that relatively small models, when trained with more data, can also match or even outperform their larger counterparts (Touvron et al., 2023  ###reference_b64###), indicating that when training smaller models for a longer time, the existing scaling law may not hold. Therefore, researchers try to explore new lightweight LLMs with less than three billion parameters beyond the existing scaling law. These lightweight LLMs follow the core structure of popular LLMs but with their specific designs for more competitive performance such as adopting their own data-collecting strategy for a higher quality training corpus and extending the training phase for more tokens, including phi-1 (Gunasekar et al., 2023  ###reference_b25###), phi-1.5 (Li et al., 2023c  ###reference_b40###), Falcon-RW (Penedo et al., 2023  ###reference_b51###), Stable LM (Bellagente et al., 2024  ###reference_b5###), H2O-Danube (Singer et al., 2024  ###reference_b55###), TinyLlama (Zhang et al., 2024  ###reference_b80###), and even MobiLlama (Thawakar et al., 2024  ###reference_b62###) with sub-billion parameters. These lightweight models can reduce the inference budget and extend the applications of LLMs in real scenarios."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Model Compression for LLMs",
            "text": "Another way to achieve lightweight LLMs is model compression.\nModel compression, including pruning, quantization, and distillation, achieves the trade-off between performance and efficiency based on existing outstanding LLMs.\nPruning reduces model parameters by eliminating modules, neurons, or individual connections that have a minimal impact on performance.\nPruning can be categorized into unstructured and structured pruning. Unstructured pruning targets individual connections between neurons, resulting in sparse weight matrices that require specific hardware for efficiency gains (Frantar & Alistarh, 2023  ###reference_b20###; Sun et al., 2024  ###reference_b58###; van der Ouderaa et al., 2024  ###reference_b65###).\nStructured pruning removes entire rows or columns of weights, creating smaller dense matrices that are more hardware-friendly. However, it faces significant performance degradation at high compression ratios (> 30%) (Ma et al., 2023b  ###reference_b49###; Zhang et al., 2023  ###reference_b78###; An et al., 2023  ###reference_b2###).\nTo improve the performance of the structured pruned model, Xia et al. (2024  ###reference_b71###) introduce the dynamic continual pre-training strategy, which resamples critical data for performance recovery for training.\nQuantization uses lower-bit-width integers (Frantar et al., 2023  ###reference_b21###; Dettmers et al., 2024  ###reference_b16###; Liu et al., 2024  ###reference_b42###; Li et al., 2024b  ###reference_b39###) or floats (Liu et al., 2023  ###reference_b44###; Perez et al., 2023  ###reference_b53###) to represent weights, activations and KV caches.\nIt reduces the memory required for LLMs, which is essential when memory is a bottleneck for model deployment.\nCurrently, 8-bit quantization achieves nearly lossless performance compression and is compatible with most GPUs (Li et al., 2024a  ###reference_b38###). However, researchers are not satisfied with this and continue to push the boundaries of ultra-low quantization precisions below 4-bit (Yuan et al., 2023  ###reference_b75###; Ma et al., 2024  ###reference_b47###).\nIn LLMs, many previous elaborate knowledge distillation strategies do not work (Jha et al., 2023  ###reference_b32###), and much work now uses the data generated by large models directly as distillation signals for training smaller models and improving the small models\u2019 commonsense, reasoning ability, and so on (Wu et al., 2023  ###reference_b69###; Hsieh et al., 2023  ###reference_b30###)."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Dataset Preparation",
            "text": "Compared to OpenBA (Li et al., 2023b  ###reference_b37###), we employ a more meticulous data processing process in OpenBA-V2 to ensure the quality of the training data.\nSpecifically, during the pre-training phase, we incorporate a greater diversity of pre-training data sources and combine more Chinese data, thereby further expanding the dataset domain distribution range. As for the instruction data, we introduce BiFlan-V2, which builds upon the BiFlan dataset (Li et al., 2023b  ###reference_b37###) by implementing additional template designs and incorporating a wider variety of instructions.\nWe will introduce the collection and processing process of pre-training data and the instruction data in Sec. 3.1  ###reference_### and Sec. 3.2  ###reference_###, respectively.\nDue to the rapid development of the open-source community, there are quite a few publicly available pre-training data.\nConsidering the computation budget and data distribution, we collect a total of 4.4 TB pre-training data to enrich the data diversity and ensure comprehensive coverage.\nSpecifically, we collect English pre-training data from two sources: Pile (Gao et al., 2020  ###reference_b22###) and RedPajama (Together, 2023  ###reference_b63###) 111https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T  ###reference_mputer/RedPajama-Data-1T###.\nAll the 22 diverse high-quality subsets of Pile are kept for pre-training, while we use six subsets of RedPajama: ArXiv, Books, C4, GitHub, StackExchange, and Wikipedia.\nFor Chinese pre-training data, we collect data from the following sources: an open-source version corpus released from Yuan (Wu et al., 2021  ###reference_b70###), WanJuan (He et al., 2023  ###reference_b27###), SkyPile (Wei et al., 2023  ###reference_b66###), CBook-150K 222https://github.com/FudanNLPLAB/CBook-150K  ###reference_###, Encyclopedias (i.e., Baidu Baike 333https://baike.baidu.com/  ###reference_baike.baidu.com/###, Chinese Wikipedia 444https://zh.wikipedia.org/wiki/  ###reference_zh.wikipedia.org/wiki/###) and Chinese Q&A community (Zhihu 555https://www.zhihu.com/  ###reference_www.zhihu.com/###).\nWe use five subsets of WanJuan as pre-training data: ChinaNews-cn, Exam-cn, Law-cn, Patent-cn, and WebText-cn.\nAlthough we carefully select high-quality pre-training data sources and remove the same part from different sources, the rest may still have low-quality and repetitive data.\nTherefore, we conduct the following data processing process to further improve the pre-training data quality and prevent potential risks:\nPrivacy Filtering: To prevent potential privacy leakage, we removed all phone numbers, email addresses, and web links from the collected pre-training data.\nDeduplication: Our pre-training data are collected from various open-sourced datasets.\nTo ensure data quality after merging, we employ deduplication strategies at multiple levels: document, character, and paragraph.\nAt the document level, each sample is treated as a document, and redundant documents are eliminated using a hash algorithm, thus retaining only unique documents.\nBesides, at the paragraph level, we utilize a hash algorithm combined with an extra sentence segmenter to identify and remove duplicate sentences or paragraphs (where consecutive 1-99 sentences are considered a paragraph).\nFinally, at the character level, redundant characters are removed, and sequences of repeated characters are condensed to a single character.\nLanguage Filtering We utilize Polyglot 666https://github.com/aboSamoor/polyglot  ###reference_### to ascertain the language of the text, retaining only those texts confidently identified as either Chinese or English.\nThis filtering process proves invaluable in filtering out gibberish, particularly for texts extracted from PDFs using OCR algorithms.\nInternet Data Cleaning The data collected from the Internet frequently contains incompletions, unrecognizable characters, and web page tags.\nConsequently, we implement filtering procedures to remove sentences containing fewer than 10 words and filter out unusual characters and HTML tags.\nAll the pre-training data mentioned above requires 4.4 TB disk space to save, and the final pre-training data consists of 57.0% English data and 43.0% Chinese data. The pre-training data distribution is illustrated in Fig. 1  ###reference_###.\n###figure_1### ###figure_2### Following the distribution and collection source of the BiFlan dataset introduced in OpenBA (Li et al., 2023b  ###reference_b37###), our English instruction data is mainly collected from the Flan Collection (Chung et al., 2022  ###reference_b11###; Longpre et al., 2023  ###reference_b46###).\nThe Flan Collection encompasses more than 1800 tasks, which is currently the most comprehensive instruction collection.\nWe follow the official guidelines to collect and process the English Flan collection with two steps,\ni.e., downloading five sub-mixtures from the Flan Collection and then combining them according to the\nspecified mixture rates 777https://github.com/google-research/FLAN/tree/main/flan/v2  ###reference_ee/main/flan/v2###.\nBesides, we also incorporate the MathInstruct dataset (Yue et al., 2023  ###reference_b76###)  888https://huggingface.co/datasets/TIGER-Lab/MathInstruct  ###reference_MathInstruct### to improve the model reasoning ability.\nFor the Chinese instruction data, apart from the Chinese Flan data introduced in OpenBA sourcing from various competitions, academic papers, and open-source projects, we incorporate more math reasoning, text-matching, question-answering, reading comprehension, and event-extraction data in this version.\nBesides, we also use BELLE School Math dataset (BELLEGroup, 2023  ###reference_b6###; Yunjie Ji, 2023  ###reference_b77###; Wen et al., 2023  ###reference_b67###)  999https://huggingface.co/datasets/BelleGroup/school_math_0.25M  ###reference_/school_math_0.25M### to improve the math reasoning abilities.\nFinally, the Chinese Instruction data is collected from 44 different Chinese tasks with 50 million data entries.\nThe Chinese instructions for each task are still designed manually.\nWe show the instruction data distribution in Fig. 1  ###reference_###. Following OpenBA, we filter out samples with lengths exceeding the encoder\u2019s maximum length, ensuring the critical parts of instructions are not truncated. Finally, the instruction data consists of 54.0% English data and 46.0% Chinese data."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Pre-training Data Collection and Processing",
            "text": "Due to the rapid development of the open-source community, there are quite a few publicly available pre-training data.\nConsidering the computation budget and data distribution, we collect a total of 4.4 TB pre-training data to enrich the data diversity and ensure comprehensive coverage.\nSpecifically, we collect English pre-training data from two sources: Pile (Gao et al., 2020  ###reference_b22###  ###reference_b22###) and RedPajama (Together, 2023  ###reference_b63###  ###reference_b63###) 111https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T  ###reference_mputer/RedPajama-Data-1T###  ###reference_mputer/RedPajama-Data-1T###.\nAll the 22 diverse high-quality subsets of Pile are kept for pre-training, while we use six subsets of RedPajama: ArXiv, Books, C4, GitHub, StackExchange, and Wikipedia.\nFor Chinese pre-training data, we collect data from the following sources: an open-source version corpus released from Yuan (Wu et al., 2021  ###reference_b70###  ###reference_b70###), WanJuan (He et al., 2023  ###reference_b27###  ###reference_b27###), SkyPile (Wei et al., 2023  ###reference_b66###  ###reference_b66###), CBook-150K 222https://github.com/FudanNLPLAB/CBook-150K  ###reference_###  ###reference_###, Encyclopedias (i.e., Baidu Baike 333https://baike.baidu.com/  ###reference_baike.baidu.com/###  ###reference_baike.baidu.com/###, Chinese Wikipedia 444https://zh.wikipedia.org/wiki/  ###reference_zh.wikipedia.org/wiki/###  ###reference_zh.wikipedia.org/wiki/###) and Chinese Q&A community (Zhihu 555https://www.zhihu.com/  ###reference_www.zhihu.com/###  ###reference_www.zhihu.com/###).\nWe use five subsets of WanJuan as pre-training data: ChinaNews-cn, Exam-cn, Law-cn, Patent-cn, and WebText-cn.\nAlthough we carefully select high-quality pre-training data sources and remove the same part from different sources, the rest may still have low-quality and repetitive data.\nTherefore, we conduct the following data processing process to further improve the pre-training data quality and prevent potential risks:\nPrivacy Filtering: To prevent potential privacy leakage, we removed all phone numbers, email addresses, and web links from the collected pre-training data.\nDeduplication: Our pre-training data are collected from various open-sourced datasets.\nTo ensure data quality after merging, we employ deduplication strategies at multiple levels: document, character, and paragraph.\nAt the document level, each sample is treated as a document, and redundant documents are eliminated using a hash algorithm, thus retaining only unique documents.\nBesides, at the paragraph level, we utilize a hash algorithm combined with an extra sentence segmenter to identify and remove duplicate sentences or paragraphs (where consecutive 1-99 sentences are considered a paragraph).\nFinally, at the character level, redundant characters are removed, and sequences of repeated characters are condensed to a single character.\nLanguage Filtering We utilize Polyglot 666https://github.com/aboSamoor/polyglot  ###reference_###  ###reference_### to ascertain the language of the text, retaining only those texts confidently identified as either Chinese or English.\nThis filtering process proves invaluable in filtering out gibberish, particularly for texts extracted from PDFs using OCR algorithms.\nInternet Data Cleaning The data collected from the Internet frequently contains incompletions, unrecognizable characters, and web page tags.\nConsequently, we implement filtering procedures to remove sentences containing fewer than 10 words and filter out unusual characters and HTML tags.\nAll the pre-training data mentioned above requires 4.4 TB disk space to save, and the final pre-training data consists of 57.0% English data and 43.0% Chinese data. The pre-training data distribution is illustrated in Fig. 1  ###reference_###  ###reference_###.\n###figure_3### ###figure_4###"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "BiFlan-V2: Instruction Data Collection",
            "text": "Following the distribution and collection source of the BiFlan dataset introduced in OpenBA (Li et al., 2023b  ###reference_b37###  ###reference_b37###), our English instruction data is mainly collected from the Flan Collection (Chung et al., 2022  ###reference_b11###  ###reference_b11###; Longpre et al., 2023  ###reference_b46###  ###reference_b46###).\nThe Flan Collection encompasses more than 1800 tasks, which is currently the most comprehensive instruction collection.\nWe follow the official guidelines to collect and process the English Flan collection with two steps,\ni.e., downloading five sub-mixtures from the Flan Collection and then combining them according to the\nspecified mixture rates 777https://github.com/google-research/FLAN/tree/main/flan/v2  ###reference_ee/main/flan/v2###  ###reference_ee/main/flan/v2###.\nBesides, we also incorporate the MathInstruct dataset (Yue et al., 2023  ###reference_b76###  ###reference_b76###)  888https://huggingface.co/datasets/TIGER-Lab/MathInstruct  ###reference_MathInstruct###  ###reference_MathInstruct### to improve the model reasoning ability.\nFor the Chinese instruction data, apart from the Chinese Flan data introduced in OpenBA sourcing from various competitions, academic papers, and open-source projects, we incorporate more math reasoning, text-matching, question-answering, reading comprehension, and event-extraction data in this version.\nBesides, we also use BELLE School Math dataset (BELLEGroup, 2023  ###reference_b6###  ###reference_b6###; Yunjie Ji, 2023  ###reference_b77###  ###reference_b77###; Wen et al., 2023  ###reference_b67###  ###reference_b67###)  999https://huggingface.co/datasets/BelleGroup/school_math_0.25M  ###reference_/school_math_0.25M###  ###reference_/school_math_0.25M### to improve the math reasoning abilities.\nFinally, the Chinese Instruction data is collected from 44 different Chinese tasks with 50 million data entries.\nThe Chinese instructions for each task are still designed manually.\nWe show the instruction data distribution in Fig. 1  ###reference_###  ###reference_###. Following OpenBA, we filter out samples with lengths exceeding the encoder\u2019s maximum length, ensuring the critical parts of instructions are not truncated. Finally, the instruction data consists of 54.0% English data and 46.0% Chinese data."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Fast Multi-Stage Pruning",
            "text": "In this section, we introduce the Fast Multi-Stage Pruning, including layer pruning, neural pruning, vocabulary pruning, and the objectives used in each stage."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Pruning Strategies",
            "text": ""
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1 Layer Pruning",
            "text": "Layerdrop (Fan et al., 2019  ###reference_b19###; Zhang & He, 2020  ###reference_b79###) is a straightforward method for pruning Transformer model parameters by randomly dropping entire layers of the model.\nLayer pruning does not severely damage the model\u2019s architecture.\nTherefore, a pruned model can maintain a relatively low perplexity (PPL) even without recovery training. Thus, we choose to perform layer pruning first. We conduct preliminary experiments to determine the optimal layers to prune, leading to the following insights:\nCompared with the top and bottom layers, pruning intermediate layers causes less damage to the model, which is also shown in LLM-Pruner (Ma et al., 2023a  ###reference_b48###).\nPruning layers with more intervals cause less damage to the model.\nAdditionally, we investigate the effects of the quantity of pruned parameters on the model\u2019s performance. We observe that once the quantity of pruned parameters reaches a certain threshold, the model\u2019s performance will plummet.\nConsequently, we adopt a staged approach to pruning the model\u2019s layers. After each stage, we use a small amount of data to recover the model performance."
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "4.1.2 Neural Pruning",
            "text": "Existing works have explored how to prune the matrix parameters of the model while minimizing performance degradation (Ma et al., 2023a  ###reference_b48###; Sun et al., 2023  ###reference_b57###; Han et al., 2015  ###reference_b26###; Xia et al., 2024  ###reference_b71###).\nSome of the works attempt to compute the importance of each element in the parameter matrix and zero out some of these elements, utilizing coefficient matrices for computation.\nThe importance of parameters often correlates with their absolute values, parameter gradients, and activation values. Therefore, these methods typically require the model to undergo forward and backward propagation on a certain amount of data.\nAnother limitation is that these methods do not truly prune the parameter matrix to another shape; instead, they zero out some parameters and utilize sparse matrix operations to perform model inference.\nSuch methods have minimal impact on the model when pruning a small number of parameters. However, when the target pruning amount reaches 30% or more, the model\u2019s performance will drop sharply. Moreover, since they only sparsify the matrix and do not facilitate subsequent retraining, this becomes disadvantageous.\nBoth Sheard-LLaMA (Xia et al., 2024  ###reference_b71###) and LLM-Pruner (Ma et al., 2023a  ###reference_b48###) reveal that when pruning the parameter matrix of the model into another shape of a dense matrix, we should not disrupt the dependent structures within the model. The dependent structures have been clearly defined in Ma et al. (2023a  ###reference_b48###).\nFurthermore, experiments from various studies have shown that the method of pruning, whether following specific importance criteria or being random, has little impact on the model\u2019s performance after it has been pruned and subsequently retrained for a while.\nTherefore, our approach involves directly randomly pruning the rows and columns of the matrix based on dependent structures and the dimensions of the target model. Figure 2  ###reference_### can help better understand our method.\n###figure_5###"
        },
        {
            "section_id": "4.1.3",
            "parent_section_id": "4.1",
            "section_name": "4.1.3 Vocabulary Pruning",
            "text": "The original OpenBA model employs a multilingual vocabulary comprising approximately 260,000 tokens. However, it is primarily trained on Chinese and English corpora and is designed to serve the Chinese-English language pair exclusively. Consequently, many tokens in the vocabulary exhibit very low usage frequencies, resulting in a considerable number of idle or rarely used embedding vectors within the model\u2019s embedding matrix.\nTo address this issue, we conduct a comprehensive analysis of token occurrences in the pre-training corpus and sort all tokens based on their frequency of occurrence. Then, we retain the top K tokens with the highest occurrence frequencies while pruning the remaining tokens. Additionally, we prune the embedding associated with these tokens and reorganized the token IDs and embedding matrices accordingly. This approach enables us to further reduce the number of parameters in the model, thereby decreasing the memory footprint.\nFigure 3  ###reference_### shows how we prune the embedding weights and rearrange the token IDs according to the pruned embedding.\n###figure_6###"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Training Objective",
            "text": ""
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1 UL2",
            "text": "The 15B OpenBA model employs the UL2 training strategy, a mixture of denoisers approach proposed by (Tay et al., 2022  ###reference_b60###), which requires the model to reconstruct sentences in various types of noise.\nR-Denoising Regular denoising is the standard span corruption that sets a range of 2 to 5 tokens as the masked span length and masks ratio about 15% of the input tokens. This denoising task is relatively simple since the span is short and efficient for the model to acquire knowledge embedded in the text.\nS-Denoising Sequence denoising aims to endow the model with generation capability, where the input text is split into two sub-sequences, and the model should predict the latter sequence conditioned on the first sequence. In the S-Denoising setting, the model can acquire the generation ability.\nX-Denoising To bridge the gap between the R-Denoising and S-Denoising, X-Denoising can be viewed as an extreme version of denoising, where approximately 50% of the input sequence is masked by increasing either the masked span length or the corruption rate.\nSuch a denoising strategy simulates the situation where a model needs to generate long targets from memory with relatively limited information.\nThe detailed information for corruption ratio and span length can be found in Table 2  ###reference_###."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2 Dynamic-UL2",
            "text": "Previous studies have shown that model pruning can affect different capabilities to different extents. Consequently, works such as Xia et al. (2024  ###reference_b71###); Xie et al. (2024  ###reference_b72###) suggest dynamically adjusting the sampling ratio for each domain according to the model\u2019s loss.\nThe UL2 objective utilized in OpenBA incorporates various types of noise, with each denoising process specifically training the model to enhance specific capabilities. For example, the <S> task improves the model\u2019s capabilities in text continuation and generation, while the <S> and <X> tasks help the model attain better comprehension and extraction capabilities.\nInspired by previous works, we propose Dynamic-UL2, which dynamically adjusts the proportion of each type of noise based on the loss for each noise on the valid set.\nThe Dynamic-UL2 Algorithm can be found in Algorithm 1  ###reference_###."
        },
        {
            "section_id": "4.2.3",
            "parent_section_id": "4.2",
            "section_name": "4.2.3 Optimized-UL2",
            "text": "###figure_7### While UL2 demonstrates significant performance enhancements by integrating various noise types, mixing multiple noises necessitates extensive padding tokens to accommodate diverse noise types within a batch.\nTherefore, the training efficiency of UL2 is relatively low.\nAs shown in Figure 1, different noise types will influence the input length of the encoder and decoder.\nFor a given sentence, if more tokens need to be masked based on the selected noise type, the length of the encoder will increase while the length of the decoder will decrease, and vice versa.\nHence, we must pad the input of the encoder and decoder to a predefined maximum length throughout the training process.\nOur estimations show that roughly 40% of the tokens in UL2 are padding tokens, impeding actual training efficiency.\nTo address this issue, we keep a global mask rate and fuse various noise types into a single sentence called Mix-Denoising.\nSpecifically, we initially apply S-noise to introduce noise to a single sentence. Subsequently, we employ R-noise and X-noise to add noise to the rest of the sentence, ensuring the total number of masked tokens reaches a predefined target.\nFor S-noise, we set a lower and an upper bound, and randomly sample the number of masked tokens from a normal distribution within these limits.\nFor R-noise and X-noise, we can calculate the number of tokens that need to be masked based on the length of the rest of the sentence.\nFor simplicity, we select [X] as the prefix token.\nWith Mix-Denoising, we can keep the input length of the encoder and decoder as a fixed value through the whole training process with few padding tokens, thus improving the training efficiency of UL2.\nHowever, Mix-Denoising may cause the model to lose some generation ability, as R-noise and X-noise have disrupted the natural language order of its decoder.\nTherefore, we preserve the original S-Denoising task to enhance the model\u2019s generation capability.\nIt is worth noting that we fix the number of masked tokens for S-noise to ensure consistency.\nFurthermore, Mix-Denoising has more sentinel tokens than S-Denoising, resulting in more input tokens. To standardize the input lengths between the two tasks, we truncate the original sentence in Mix-Denoising. During training, Mix-Denoising and S-Denoising comprise about 20% and 80% of our training data, respectively.\nThese adaptations mitigate the need for excessive padding, achieving model performance comparable to UL2 with almost no padding token.\nOur approach has more valid tokens than UL2, thus enabling superior performance at equivalent training costs."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Implementation",
            "text": ""
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Model Architecture",
            "text": "Despite the reduction in model parameters, OpenBA-V2 maintains the same model architecture as OpenBA (Li et al., 2023b  ###reference_b37###), including an Encoder-Decoder model structure, rotary embedding scheme (Su et al., 2024  ###reference_b56###),and the SwiGLU Activation Function (Shazeer, 2020  ###reference_b54###)."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Training",
            "text": "###table_1### We first use a relatively small amount of tokens to compress the 15B model to 3.8B without a significant loss in model capability. Subsequently, we use a large number of tokens to train the model further for better performance. The entire process can be divided into multiple stages, and table 3  ###reference_### illustrates the model sizes and training objectives at different stages.\nWe use a cosine scheduler for stages 1-4 with the max learning rate 1e-4 and the min learning rate 5e-5.\nFor stage 5, we use the max learning rate 5e-5 and the min learning rate 1e-5.\nAfter pruning, We directly prune 140,000 tokens from the vocabulary, reducing the model size from 3.8B to 3.4B."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Experiments & Evaluation",
            "text": ""
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Baseline Models",
            "text": "We mainly select open-source models of approximately 3B parameters for comparing model performance. Additionally, we include some state-of-the-art (SOTA) models with around 7B parameters.\nThe details of the selected models are shown in table 4  ###reference_###."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Settings",
            "text": "We select a diverse range of tasks for evaluation. For world common knowledge, we evaluate the models on MMLU (Hendrycks et al., 2020  ###reference_b28###), CMMLU (Li et al., 2023a  ###reference_b36###), C-EVAL (Huang et al., 2024  ###reference_b31###) and BBH (Suzgun et al., 2022  ###reference_b59###). For commonsense reasoning and reading comprehension, we select SciQ Johannes Welbl (2017  ###reference_b34###), PIQA Bisk et al. (2020  ###reference_b8###), ARC Clark et al. (2018  ###reference_b13###), LogiQA Liu et al. (2020  ###reference_b41###), and BoolQ Clark et al. (2019  ###reference_b12###). We annotate the num-shots used during evaluation in the parentheses on the right of the dataset name. For the performance of the baseline models, if results for the corresponding dataset are available in the original paper, we directly report those results; otherwise, we report the results we reproduced. We use the LM-Evaluation-Harness repository Gao et al. (2023  ###reference_b23###) to reproduce the results of the baseline models.\nSince LM-Evaluation-Harness is not well adapted for encoder-decoder architecture, we have modified the original repository and redeveloped the evaluation code for OpenBA-V2."
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "Main Results",
            "text": "We compare the model performance with other baselines in Table 5  ###reference_### and 6  ###reference_###.\nIn each table, the first group includes models with more than 6B parameters. The second group includes models with 3B or fewer parameters trained on open-sourced data. The third group includes models with 3B or fewer parameters trained on Non-open-sourced data. The final group includes all versions of OpenBA models.\nOverall, OpenBA-V2 outperforms all models with 3B or fewer parameters trained on open-sourced data, demonstrating its strong competitiveness among models of the same size.\nOpenBA-V2 is weaker than larger models, but it still demonstrates its competitiveness.\nFor example, OpenBA-V2 surpasses Chinese-LLaMA2 on the Chinese benchmark and shows no significant gap compared to other larger models in commonsense reasoning and reading comprehension tasks.\nCompared to models with fewer than 3B parameters trained on non-open-source data, OpenBA-V2 outperforms them only in specific tasks, and there is a gap in most scenarios, highlighting the importance of high-quality data for LLMs.\nCompared to OpenBA, OpenBA-V2 achieves strong performance with just 23% parameters, indicating a significant potential for model compression.\nThe model can be more lightweight and cost-effective by implementing effective compression strategies and recovery training."
        },
        {
            "section_id": "6.4",
            "parent_section_id": "6",
            "section_name": "NER Finetuning Performance",
            "text": "We further explore the potential of the OpenBA model series for Named-Entity-Recognition (NER).\nWe utilize the Pile-NER (Zhou et al., 2023  ###reference_b82###) dataset as our training dataset, comprising approximately 240,000 entities across 13,000 distinct categories.\nFollowing this, we evaluate the model on the MIT (Liu et al., 2013  ###reference_b43###) and CrossNER (Liu et al., 2021  ###reference_b45###) datasets, where the entity labels are mostly unseen during the training phase.\nWe adopt the method outlined in GNER (Ding et al., 2024  ###reference_b17###) for incorporating negative instances into the training phase.\nAdditionally, we compare our approach with three baselines: sheard-llama (Xia et al., 2024  ###reference_b71###), open-llama-v2 (Geng & Liu, 2023  ###reference_b24###) and OpenBA-15B (Li et al., 2023b  ###reference_b37###), employing identical training methodologies and strict entity-level  score as evaluation metrics.\nThe results are summarized in table 7  ###reference_###.\nOur model significantly outperforms the others, achieving superior performance that exceeds the original 15B model before its pruning."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Analysis",
            "text": "In this section, we mainly illustrate the motivation behind multi-stage pruning (Sec. 7.1  ###reference_###), the effectiveness of Dynamic-UL2 strategy (Sec. 7.2  ###reference_###), as well as the impact of vocabulary pruning on the final performance (Sec. 7.3  ###reference_###).\nIn these stages, the Dynamic-UL2 strategy keeps <S> noisier dominant throughout, with the proportion of <S> noisier gradually increasing along with the training progress.\nAfter the pruning of 2.7B model parameters in stage 1, we observe a gentle descent in the loss curve (Subfig. 6(a)  ###reference_sf1###).\nHowever, during the subsequent pruning stages, the training loss curve takes on a U-shape, indicating that it becomes increasingly challenging to recover the model performance as more parameters are pruned.\nAdditionally, we observe that in stages 2 and 3, the proportion of <S> noise is higher than in Stage 1. This suggests that the Dynamic-UL2 strategy effectively facilitates performance recovery by adapting to more challenging tasks.\nIn this stage, the model\u2019s parameters are reduced from 9.9B to 3.8B through neural pruning, resulting in a notable increase in loss (from 1.96 at the end of Stage 3 to 2.28 at the beginning of Stage 4). Therefore, Dynamic-UL2 focuses more on <S> noise to facilitate model performance recovery, as shown in Subfig. 6(d)  ###reference_sf4###."
        },
        {
            "section_id": "7.1",
            "parent_section_id": "7",
            "section_name": "Motivation behind Multi-Stage Pruning",
            "text": "###figure_8### ###figure_9### ###figure_10### ###figure_11### ###figure_12### ###figure_13### ###figure_14### ###figure_15### ###figure_16### ###figure_17### Before performing multi-stage pruning, we conduct a preliminary study to determine if we can directly reduce the LLM parameters on the depth dimension (layer pruning) and width dimension (neural pruning).\nWe conduct the study with the LLaMA-2-7B model (Touvron et al., 2023  ###reference_b64###). We iteratively prune the model\u2019s parameters and observe the pruned model\u2019s PPL on the development set 101010We randomly sample the development set from The Pile dataset..\nWe plot the relationship between model performance and pruning parameters in Fig. 5  ###reference_###. We can observe that: (1) For both layer pruning and neural pruning, the model\u2019s PPL increases as the number of pruned parameters grows. (2) For a 7B model, after pruning 3B parameters, which accounts for 42.8% of the original model parameters, there is a significant explosion in PPL. Such a phenomenon suggests that aggressively pruning a large number of model parameters through directed pruning can lead to a collapse in model performance, which may be irrecoverable even with recovery training. (3) Compared with Neural Pruning, Layer Pruning has a smaller impact on the model, as reflected by lower PPL.\nBased on the findings mentioned above, we adopt a multi-stage pruning strategy. Concretely, we first conduct layer pruning and then conduct neural pruning, aiming to retain as much original model knowledge as possible during the pruning process. After each pruning stage, we retrain the model to help it recover its capabilities."
        },
        {
            "section_id": "7.2",
            "parent_section_id": "7",
            "section_name": "Effectiveness of Dynamic-UL2 Strategy",
            "text": "In this section, we analyze the model\u2019s recovery during the pruning process using the Dynamic-UL2 Strategy.\nWe plot all loss curves during the training process in Fig. 6  ###reference_### and the noise ratios in the Dynamic-UL2 training strategy in Fig. 7  ###reference_###.\nIt is worth mentioning that at each stage, the model\u2019s loss on the development set steadily decreases. However, the training loss exhibits different characteristics at different stages, which will be analyzed below.\nIn these stages, the Dynamic-UL2 strategy keeps <S> noisier dominant throughout, with the proportion of <S> noisier gradually increasing along with the training progress.\nAfter the pruning of 2.7B model parameters in stage 1, we observe a gentle descent in the loss curve (Subfig. 6(a)  ###reference_sf1###  ###reference_sf1###).\nHowever, during the subsequent pruning stages, the training loss curve takes on a U-shape, indicating that it becomes increasingly challenging to recover the model performance as more parameters are pruned.\nAdditionally, we observe that in stages 2 and 3, the proportion of <S> noise is higher than in Stage 1. This suggests that the Dynamic-UL2 strategy effectively facilitates performance recovery by adapting to more challenging tasks.\nIn this stage, the model\u2019s parameters are reduced from 9.9B to 3.8B through neural pruning, resulting in a notable increase in loss (from 1.96 at the end of Stage 3 to 2.28 at the beginning of Stage 4). Therefore, Dynamic-UL2 focuses more on <S> noise to facilitate model performance recovery, as shown in Subfig. 6(d)  ###reference_sf4###  ###reference_sf4###."
        },
        {
            "section_id": "7.3",
            "parent_section_id": "7",
            "section_name": "Impact of Vocabulary Pruning",
            "text": "###figure_18### ###figure_19### We present the results of the model performance with vocabulary pruning in Fig 8  ###reference_###. We can observe that, for Chinese tasks, performance does not deteriorate but instead shows improvement, even when the vocabulary is pruned from 120K to 20K. In contrast, the model\u2019s performance declines for English tasks as the vocabulary size is reduced. This disparity may be attributed to the fact that each Chinese character is represented by independent tokens, and there is a relative redundancy of Chinese tokens in the vocabulary. On the contrary, English words often consist of multiple tokens, meaning a reduction in vocabulary size has a more pronounced effect on English."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We release OpenBA-V2, an encoder-decoder Transformer model with 3.4B parameters. OpenBA-V2 is derived from the 15B OpenBA model by compressing and continually pre-training. During the compressing stage, we achieve a compression ratio of 77.3% through multi-stage compression combined with recovery training, with minimal loss in model performance.\nIn the continual pre-training stage, we optimize the UL2 objective and reduce the number of padding tokens in UL2 from about 40% to close to 0, which significantly increases the training efficiency and reduces the waste of resources while bringing almost no loss of model performance.\nOpenBA-V2 leverages a more diverse dataset and employs multiple levels of filtering strategies to enhance text quality. Overall, OpenBA-V2 demonstrates notable competitiveness among open-source models of similar size."
        }
    ],
    "url": "http://arxiv.org/html/2405.05957v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "4",
            "4.1",
            "4.1.1",
            "4.1.2",
            "4.1.3",
            "4.2",
            "4.2.1",
            "4.2.2",
            "4.2.3"
        ],
        "main_experiment_and_results_sections": [
            "6.1",
            "6.2",
            "6.3",
            "6.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.1.1",
            "4.1.2",
            "4.1.3",
            "7.1",
            "7.2",
            "7.3"
        ]
    },
    "research_context": {
        "paper_id": "2405.05957v1",
        "paper_title": "OpenBA-V2: Reaching 77.3% High Compression Ratio with Fast Multi-Stage Pruning",
        "research_background": "### Paper's Motivation\n\nLarge Language Models (LLMs) exhibit exceptional capabilities in natural language tasks, but their extensive computational and storage demands pose significant challenges, particularly in resource-limited environments like PCs and mobile phones. The high costs associated with inference and storage further hinder their broader application across diverse industries. The motivation behind this paper lies in addressing these impediments by exploring methods to reduce the computational and storage requirements of LLMs, thereby facilitating their deployment in resource-constrained scenarios without compromising performance.\n\n### Research Problem\n\nThe core research problem tackled by this paper is how to achieve significant compression of LLMs while maintaining high performance. Specifically, the paper aims to develop a compression strategy that balances efficiency and performance, making LLMs practical for broader deployment. This involves investigating multi-stage pruning techniques combined with efficient recovery training and leveraging optimized training objectives to minimize performance loss due to compression.\n\n### Relevant Prior Work\n\n1. **Large Language Models and Their Impact**:\n   - Recent breakthroughs in LLMs have led to exceptional performance in tasks like dialogue generation, code generation, text summarization, and machine translation (OpenAI, 2023; Touvron et al., 2023; Jiang et al., 2023; Bai et al., 2023; Li et al., 2023b).\n\n2. **Challenges Due to Resource Demands**:\n   - LLMs demand extensive computing resources, making them impractical for use in environments like PCs or mobile phones (Thawakar et al., 2024).\n   - High inference and storage costs limit their widespread adoption (Bai et al., 2024).\n\n3. **Approaches to Reducing Model Size**:\n   - One approach to reducing these demands involves designing smaller models, either by training from scratch or compressing larger models (Geng & Liu, 2023; Zhang et al., 2024; Li et al., 2023c).\n\n4. **Training Small Models from Scratch**:\n   - Emphasis on data quality over quantity can result in smaller models potentially outperforming larger counterparts with lower training costs (Li et al., 2023c; Bai et al., 2023).\n\n5. **Model Compression Techniques**:\n   - Methods like pruning, distillation, and quantization strike a balance between efficiency and performance.\n   - Pruning removes non-essential parameters (Ma et al., 2023a; Frantar & Alistarh, 2023; Sun et al., 2024).\n   - Distillation involves the student model mimicking a teacher model's behavior (Wu et al., 2023; Hsieh et al., 2023).\n   - Quantization lowers storage and inference costs by converting models to lower precision data types (Frantar et al., 2023; Dettmers et al., 2024).\n\nBy leveraging these insights and methods, this paper proposes OpenBA-V2, which utilizes a multi-stage compression strategy to effectively reduce the model size and resource requirements while retaining high performance.",
        "methodology": "The proposed method, OpenBA-V2, leverages a methodology known as Fast Multi-Stage Pruning to achieve a high compression ratio while maintaining efficiency and performance. This methodology can be broken down into several key stages and components:\n\n1. **Layer Pruning**: In this initial stage, the methodology focuses on pruning entire layers of the neural network. This helps in significantly reducing the complexity of the model by removing less critical layers, thereby streamlining the architecture.\n\n2. **Neural Pruning**: Following layer pruning, the next step involves neural pruning, where individual neurons within the layers are pruned. This fine-grained pruning helps in further reducing the model size by eliminating neurons that contribute minimally to the output.\n\n3. **Vocabulary Pruning**: This stage targets the vocabulary used in the model, pruning it down to retain only the most essential elements. By doing so, the model's input and output layers handle less data, thus reducing overall computational requirements.\n\n4. **Objectives**: Each stage of the pruning process is governed by specific objectives that are designed to maximize compression while preserving the model's performance. These objectives guide which layers, neurons, and vocabulary elements are pruned, ensuring that the final compressed model remains highly effective.\n\nBy combining these stages - layer pruning, neural pruning, and vocabulary pruning - along with carefully crafted objectives, OpenBA-V2 achieves an impressive compression ratio of 77.3%, significantly enhancing both model efficiency and performance.",
        "main_experiment_and_results": "### Main Experiment Setup:\nWe mainly select open-source models of approximately 3B parameters to compare model performance. Additionally, we include some state-of-the-art (SOTA) models with around 7B parameters to provide a comprehensive evaluation.\n\n**Datasets:**\n- The specific datasets used for evaluation are not detailed in the provided text, but given the context, it is likely that standard benchmark datasets for language or model performance evaluation were utilized.\n\n**Baselines:**\n- Open-source models with approximately 3B parameters.\n- Some state-of-the-art models with around 7B parameters.\n\n**Evaluation Metrics:**\n- While specific evaluation metrics are not mentioned, typical metrics include:\n  - Performance accuracy or other domain-specific performance measurements.\n  - Compression ratio to evaluate how much the model size has been reduced.\n\n### Main Experimental Results:\nThe paper reports that their approach, OpenBA-V2, achieves a high compression ratio of 77.3% while maintaining substantial performance.\n\nIn summary, OpenBA-V2 demonstrates significant model compression, achieving a 77.3% reduction in size, which is both an impressive and practical result for model optimization."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To determine the impact of layer pruning on the performance of a large language model and identify the optimal layers to prune.",
            "experiment_process": "Layerdrop is used to prune Transformer model parameters by randomly dropping entire layers. Preliminary experiments are conducted to identify which layers (top, bottom, or intermediate) cause less damage when pruned. It is observed that pruning intermediate layers causes less damage and pruning with more intervals is more beneficial. A staged pruning approach is adopted, and after each stage, a small amount of data is used to recover the model's performance.",
            "result_discussion": "Pruning intermediate layers and those separated by larger intervals causes less damage to the model. A staged pruning approach helps in recovering the model performance after each pruning stage.",
            "ablation_id": "2405.05957v1.No1"
        },
        {
            "research_objective": "To examine the effects of neural pruning on model performance and to determine the most effective pruning strategy for minimizing performance degradation.",
            "experiment_process": "Existing methods for pruning matrix parameters based on importance criteria are reviewed. These criteria include absolute values, parameter gradients, and activation values, requiring forward and backward propagation on data. A limitation is noted: these methods sparsify the matrix rather than reshaping it, which becomes disadvantageous when retraining. The study reveals that pruning based on dependent structures without disrupting them has minimal impact on the model. Consequently, the approach chosen involves randomly pruning rows and columns of the matrix based on dependent structures and target model dimensions.",
            "result_discussion": "The results indicate that regardless of the specific importance criteria, random pruning followed by retraining can effectively maintain model performance. Pruning based on dependent structures is crucial for maintaining performance.",
            "ablation_id": "2405.05957v1.No2"
        },
        {
            "research_objective": "To reduce the number of parameters in the model by pruning rarely used tokens from the multilingual vocabulary.",
            "experiment_process": "The original model's vocabulary of approximately 260,000 tokens is analyzed for token occurrence frequency. Tokens are sorted, and the top K tokens with the highest occurrence frequencies are retained; the remaining tokens are pruned. The associated embeddings are also pruned, and token IDs and embedding matrices are reorganized to further reduce the model's parameters.",
            "result_discussion": "Vocabulary pruning significantly reduces the model's memory footprint. Pruning infrequently used tokens does not deteriorate performance but improves performance in Chinese tasks, while there is a decline in English tasks due to the representation of English words with multiple tokens.",
            "ablation_id": "2405.05957v1.No3"
        }
    ]
}