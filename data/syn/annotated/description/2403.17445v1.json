{
    "title": "Incorporating Exponential Smoothing into MLP: A Simple but Effective Sequence Model",
    "abstract": "Modeling long-range dependencies in sequential data is a crucial step in sequence learning. A recently developed model, the Structured State Space (S4), demonstrated significant effectiveness in modeling long-range sequences. However, It is unclear whether the success of S4 can be attributed to its intricate parameterization and HiPPO initialization or simply due to State Space Models (SSMs). To further investigate the potential of the deep SSMs, we start with exponential smoothing (ETS), a simple SSM, and propose a stacked architecture by directly incorporating it into an element-wise MLP. We augment simple ETS with additional parameters and complex field to reduce the inductive bias. Despite increasing less than 1% of parameters of element-wise MLP, our models achieve comparable results to S4 on the LRA benchmark.111Our codes and scripts are available at https://github.com/PKUAI-LINGroup/ETSMLP.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Transformer Vaswani et al. (2017  ###reference_b41###) and its variants have been the most successful architecture in various domains of deep learning. However, the self-attention layer, which plays a crucial role in contextualizing the input, poses a significant computational and memory burden with a complexity of . This limitation hinders the application of the transformers in modeling long sequences, particularly when operating under hardware constraints, which is a common scenario for large language models. To alleviate this issue, several models have been proposed to reduce the computational and memory requirements of the transformers Beltagy et al. (2020  ###reference_b1###); Choromanski et al. (2020  ###reference_b5###); Kitaev et al. (2020  ###reference_b22###); Wang et al. (2020  ###reference_b42###); Guo et al. (2021  ###reference_b15###); Kasai et al. (2021  ###reference_b20###); Peng et al. (2021  ###reference_b30###); Dao et al. (2022  ###reference_b7###); Hua et al. (2022  ###reference_b17###); Tay et al. (2022  ###reference_b38###); Fournier et al. (2023  ###reference_b9###); Zandieh et al. (2023  ###reference_b48###). Despite these efforts, all the models are only partial modifications of the attention mechanism and struggle to perform well on long-range sequence benchmarks such as Long Range Arena (LRA) Tay et al. (2020  ###reference_b37###).\nIn a recent breakthrough result, Gu et al. (2021  ###reference_b14###) introduced a novel framework called the \"structured state space sequence\" (S4) that leveraged the State Space Models (SSMs). S4 builds upon continuous-time SSMs and addresses the computational bottleneck of previous approaches by introducing the Normal Plus Low-Rank (NPLR) decomposition of the state matrices. Additionally, the initialization of state matrices utilizes HiPPO matrices which have been demonstrated to be effective in sequence learning in Gu et al. (2020  ###reference_b12###). Notably, S4 exhibited exceptional performance across various sequential tasks, particularly in the LRA, where it outperformed the existing transformer variants by an impressive\naccuracy.\nDespite the impressive performance of S4, its intricate parameterization and strict initialization schemes impede researchers from fully comprehending, implementing, and analyzing the model. Although there have been attempts to simplify the S4 framework by Smith et al. (2022  ###reference_b33###); Gupta et al. (2022  ###reference_b16###), these models still required the HiPPO initialization process. Other studies have explored the relationship between SSMs and recurrent units or global convolutions and demonstrated strong performance on various tasks Li et al. (2022  ###reference_b23###); Orvieto et al. (2023  ###reference_b28###). These works highlight the potential of SSMs and suggest that simpler yet effective SSM architectures may exist.\nIn our work, we deviate from the methodology proposed by S4, which begins with the continuous SSM and then simplifies the process. We initiate our approach with a discrete SSM, namely Exponential Smoothing (ETS), and introduce additional parameters to reduce the inductive bias. This alternative approach offers two notable advantages. Firstly, it circumvents the simplification of the continuous SSMs that need sophisticated mathematical derivations and thus enhances accessibility and comprehensibility. Secondly, it explores the possibility of random initialization departing from HiPPO initialization for continuous SSMs. For the streamlining of the model, our architecture directly integrates a parameterized ETS into an element-wise Multi-Layer Perceptron (MLP). By incorporating less than 3% of the total parameters after the initial linear layer of the MLP, we successfully transform a channel-only MLP into a sequence learner.\nWe conducted experiments on multiple datasets, including the LRA and several Natural Language Understanding (NLU) datasets. Despite its simplicity, surprisingly, our model performs comparably to S4. In all six tasks in the LRA, our results slightly surpass the performance of S4 and DSS by 2.61 points on average and significantly outperform the transformer variants by about 20 points. In addition, we evaluated our model on seven NLU datasets and consistently achieved comparable performance with the transformer encoders. The findings of our work shed light on the potential of SSMs from a unique standpoint, where simply incorporating an ETS into an MLP can achieve a similar effect as the transformer model. A thorough examination of the proposed model was undertaken through an ablation study on the hyperparameters and an evaluation of the initialization method. Additional experiments were conducted to compare our model with the transformer model for efficiency and memory utilization, especially in handling lengthy texts. The results of these experiments provide evidence of the advantages of our model over the transformer model in terms of time and memory complexity.\nIn summary, our main contributions are as follows:\nWe introduce the Exponential Smoothing Multi-Layer Perceptron (ETSMLP) model. We integrate the enhanced ETS module into an element-wise MLP to create an effective sequence model.\nWe evaluate ETSMLP on the LRA and conduct comparative experiments with transformer encoders on various NLU datasets. The empirical results demonstrate the effective capacity in long-range sequence modeling.\nWe conduct ablation studies on the proposed parameters and initialization methods. Additionally, we emphasize the advantages of SSMs over the attention mechanism in speed and memory efficiency."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Preliminaries",
            "text": "We introduce basic notations and briefly review SSMs and ETS in this section. Focusing on time-invariant sequence models, we aim to transform a sequence of inputs  into a corresponding sequence of outputs  with each output  is exclusively based on historical data .\n###figure_1###"
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "State space models",
            "text": "The continuous-time SSM is characterized by the differential equation (1  ###reference_###), which establishes the relationship between a continuous-time scalar input  to a scalar output  with the state matrix  and vectors , :\nIf we set a sample time interval , and assume that the duration of sampling remains constant , we convert the continuous-time SSM into a discrete-time one using a recursive equation in the following:\nwhere , and . With , we unroll this recursion explicitly as the equation (3  ###reference_###) which can be vectorized into a convolution in the equation (5  ###reference_###) with the SSM convolution kernel defined in the equation (4  ###reference_###) as follows:\nIf we obtain the kernel , the convolution function aforementioned can be efficiently computed with Fast Fourier Transform (FFT) in  Cormen et al. (2022  ###reference_b6###). Nevertheless, the main challenge in the computation of SSMs is how to efficiently compute  from the matrices  and . S4 proposes an effective parameterization through decomposing matrix  to the NPLR matrices Gu et al. (2021  ###reference_b14###), and diagonal state spaces (DSS) only consider the circumstances when  is diagonalizable over  Gupta et al. (2022  ###reference_b16###). Both methods involve intricate mathematics, sophisticated parameterization, and strict initialization, all of which are indispensable for achieving excellent performance. Our method will start from a special SSM, namely ETS, which gives a new insight into\nthis problem, and requires fewer mathematical operations, fewer parameters, and more flexible initialization."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Exponential smoothing",
            "text": "ETS is a time series forecasting method that utilizes a weighted average of past observations to predict future values Winters (1960  ###reference_b45###); Hunter (1986  ###reference_b18###); Hyndman et al. (2008  ###reference_b19###). The fundamental idea behind ETS is to give more weight to recent observations and less to older ones, with the weights decreasing exponentially as the observations get older. The core recursive equation for this method is the equation (6  ###reference_###) with the smoothing factor  in the range :\nETS is a special SSM, with the substitution . However, compared with SSMs,\nETS cannot capture sequential information effectively. Figure 1  ###reference_### illustrates the relationship among\nSSMs, S4, DSS, and ETS. S4 and DSS are derived from the continuous-time SSMs with the difference that S4 decomposes the matrix  into an NPLR matrix, while DSS assumes  to be diagonalizable. As a result, the HiPPO initialization in S4 cannot directly adapt to DSS Gu et al. (2020  ###reference_b12###). The initialization in DSS is skew-Hippo initialization which is the normal part of the HiPPO matrix.\nETS serves as a special case within the realm of discrete-time SSM. In our approach, we incorporate parameters directly from ETS, distinguishing ours from S4 and DSS methods that simplify equations based on continuous-time SSMs."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Exponential Smoothing Multi-layer Perceptrons",
            "text": "In this section, we present our ETSMLP. We first introduce a complex exponential smoothing module which is the pivotal component of our architecture. We then describe the full architecture with two proposed versions, ESMLP and ESMLP-Gate."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Complex exponential smoothing module",
            "text": "Learnable damped factors. Damped factors are a commonly used technique of ETS for attenuating the influence of specific factors Gardner Jr (1985  ###reference_b10###); McKenzie and Gardner Jr (2010  ###reference_b27###). We introduce two learnable damped factors  and  into simple ETS in equation (7  ###reference_###). The factor  controls the learning of  in an exponential scalar. A small  close to zero amplifies the impact of  and results in  approximating 1 while a large  diminishes its impact, driving the combination closer to 0. The factor  serves as a multiplicative factor that controls the influence of the current input . The recursion can be unrolled in equation (8  ###reference_###) with the kernel defined by the equation (9  ###reference_###) as follows:\nComplex parameters. Complex parameters in ETS have been demonstrated to capture both level and trend information in forecasting Svetunkov et al. (2022  ###reference_b35###). By extending the learning capacity and enlarging the parameter space, the transformation from real to complex numbers is beneficial. Therefore, we treat , ,  as complex numbers, and keep the input  and the output  real. Consequently, only the real part of the kernel coefficients is utilized, and the corresponding computation formula is as follows:\n###figure_2### ###figure_3### Exponential parameterization. Directly training the parameters  is infeasible, due to the rapid explosion of the gradient  within a few steps. This challenge becomes evident upon inspecting the equation (6  ###reference_###). Given the gradients of  , the gradient of   could be derived from the formulas (11  ###reference_###). This reveals that the gradients of  are proportional to . Consequently, as  approaches 1, the gradients of  will explode.\nTo address this issue, we propose an exponential transformation of the parameters. We trains the parameter  instead of the . We prove that the stability of learning  constrains the gradients of  within a specified range, as described in Proposition 1.\nLet  be within the interior of the hollow unit disc . We define  which substitutes  in the equation (10  ###reference_###). If the gradients of  satisfy , the gradients of the real and imaginary parts of  are bounded for all .\nThe proof of Proposition 1 is elementary and is provided in the appendix A  ###reference_###. This proposition proves that the exponential parameterization  gradients are stable in the feasible region of .\nConstraint function and shortcut. In addition to the settings as aforementioned,  must lie within the feasible field . To address this, we introduce a constraint function to enforce the validity of these parameters, which can be formulated in equation (12  ###reference_###):\nAlthough this solution is simple, it yields remarkably effective results. We also explored an alternative approach inspired by the separation of real and imaginary parts, as discussed in Gu et al. (2022  ###reference_b13###); Orvieto et al. (2023  ###reference_b28###). Unfortunately, its performance is unsatisfactory, because the gradients of imaginary parts appear unstable and may explode in a few steps.\nMoreover, we introduce a parameter  to establish a shortcut from input to output, a commonly used technique in deep learning. This parameter serves as a gating unit that regulates the incoming input. The final output of our model can be described with the sigmoid function  as follows:\nBidirectional. We describe a bidirectional model incorporating backward recursion as . By employing a bidirectional model, the influence of tokens is determined by both preceding and succeeding tokens, resulting in a wide-ranging receptive field. The kernel function is formed by combining the forward and backward kernels in equation (14  ###reference_###). We employ the circular convolution to compute the output with the input being zero-padded on the right side to twice its length.\nA sketch of the full computation is presented on the left of Figure 2  ###reference_###. Initially, we calculate the kernel corresponding to the sequence length and subsequently apply FFT to compute the convolution of the inputs and the kernel. The Complex Exponential Smoothing (CES) module produces the final results by combining the shortcut and the convolution outputs. Although the current code is designed for a unidirectional kernel, a bidirectional kernel can be easily achieved by connecting two unidirectional kernels using the equations as aforementioned."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "ETSMLP blocks",
            "text": "We incorporate the CES module into the element-wise MLP to learn token-level information. The CES module facilitates the mix of input information at the token level, resulting in a mixed output containing sequence information. We integrated the CES module just before the activation function into the MLP in the full architecture, depicted in Figure 2  ###reference_###. The functions are described as follows:\nCompared to standard MLP, the increased parameters constitute only  of the original MLP, where  is the embedding dimension. For a typical model with , a modest increase 0.58% parameters enables channel-only MLP to learn sequence information, which is previously unattainable. Moreover, the computational and memory complexity is lower than that of the self-attention, as detailed in Section 4.3.3  ###reference_.SSS3###\nGated architecture. To further enhance the expressive capacity of our model, we add a gate mechanism like Cho et al. (2014  ###reference_b4###); Shazeer (2020  ###reference_b32###); Hua et al. (2022  ###reference_b17###). This gate unit controls the output of each block. After obtaining the representation after layernorm, we pass it through a linear layer, derive the score using the sigmoid function, multiply it with the output from the preceding module, and obtain the output of our layer through a residual connection. As in Figure 2  ###reference_###, we express these processes as follows:"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We present an empirical comparison between our ETSMLP and other baseline models. Our experiments encompass a set of sequence modeling tasks, including LRA, MNLI, IMDB, etc. The main experiment results are divided into two subsections: LRA and NLU benchmarks. Furthermore, we conduct an ablation study to examine the influence of hyperparameters. Additional information about the experimental details and datasets can be found in Appendix B  ###reference_###.\n###table_1###"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "LRA",
            "text": "The LRA benchmarks are a collection of long sequence modeling tasks ranging from 1024 to over 16000 Tay et al. (2020  ###reference_b37###). In Table 1  ###reference_###, we compare our models to several variants of SSM\nand Transformer. We observe that our model outperforms all the Transformer variants and achieves the comparable performance of S4 on average which is 83.09 vs 80.48. Although we don\u2019t gain the highest average scores among all concurrent works, it still produces comparable results without relying on the attention in MEGA Ma et al. (2022  ###reference_b25###), or Hippo initialization in S5 Smith et al. (2022  ###reference_b33###). When comparing the individual tasks horizontally, we observe that our model performs significantly better in text tasks such as ListOps and Text, while slightly underperforming on image tasks like Image. This discrepancy may be attributed to the weight decaying exponentially with distance, which is unsuitable for flattened patches.\nWe provide the hyperparameters used in our experiments in Appendix B  ###reference_###."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "NLU",
            "text": "The LRA results demonstrate the benefits of our method in sequential text tasks. Furthermore, we conduct experiments on various NLU tasks and compare our models with a transformer encoder architecture trained from scratch. Our experimental evaluations were divided into three categories: sentence classification, including CoLA Warstadt et al. (2019  ###reference_b43###), SST-2 Socher et al. (2013  ###reference_b34###), and IMDB Maas et al. (2011  ###reference_b26###); sentence similarity, including QQP, 222https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs  ###reference_taset-Release-Question-Pairs### and MRPC Dolan and Brockett (2005  ###reference_b8###); and natural language inference, including MNLI Williams et al. (2018  ###reference_b44###) and QNLI Rockt\u00e4schel et al. (2015  ###reference_b31###). We present the experiment results in Table 2  ###reference_###, which reveal that our architecture can achieve comparable or even superior performance to transformers on all the datasets. Considering the simple computation and slight increase in parameters on MLP, these results suggest that the ETS has tremendous potential in sequence learning tasks."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Analysis",
            "text": ""
        },
        {
            "section_id": "4.3.1",
            "parent_section_id": "4.3",
            "section_name": "4.3.1 Role of damped factors and fields",
            "text": "The experimental results presented above are encouraging and demonstrate the effectiveness of the ETS for sequence modeling tasks. It is proved empirically that even the simplest SSM like ETS can achieve a competitive result compared with other state space model variants. To further consider if we would simplify the ETS in fewer parameters, we conducted ablation studies on the damped factors and the number fields. Table 3 shows the accuracy results of Listops if we remove , , or  or change all parameters from complex to real fields. We could observe that whether to remove the  or  or  or the complex field, the performance of our method drops significantly, especially  and complex field. These experiments illustrate the necessity of our architecture in sequence modeling tasks."
        },
        {
            "section_id": "4.3.2",
            "parent_section_id": "4.3",
            "section_name": "4.3.2 Initialization of parameters",
            "text": "S4 and its variants conducted several experiments on  initialization and concluded that random initialization may hurt performance Gu et al. (2021  ###reference_b14###, 2022  ###reference_b13###). Because of the different computation processes,  initialization doesn\u2019t work in our models. Therefore, we consider the ring initialization method, which involves uniform sampling on a ring defined by the range . By predefining values for  and , we uniformly sample  along the ring, between circles with radii  and . In addition to examining the effects of different initializations, we conducted experiments using fixed-value initialization operations. Our experimental results on listops are displayed in Figure 4  ###reference_###. It can be observed that our model exhibits consistent performance across rings of varying sizes. However, when dealing with fixed points, the effectiveness diminishes significantly.\n###figure_4### ###figure_5###"
        },
        {
            "section_id": "4.3.3",
            "parent_section_id": "4.3",
            "section_name": "4.3.3 Efficiency and memory analysis",
            "text": "To assess the speed and memory efficiency across different lengths, we performed experiments using a synthetic dataset that combines multiple sentences to achieve sufficient length. Our chosen task is language modeling, as it allows us to segment sentences into desired lengths. The maximum length of our synthetic dataset is 8192. We adjusted the sample length within each batch to compare the words per second (WPS) and memory usage (in GB) between the transformer, S4, and our model at comparable sizes. The batch size was uniformly set to 1 to ensure accurate memory usage. All training procedures are carried out on an NVIDIA GeForce GTX 2080 GPU.\nThe comparison results are presented in Figure 3  ###reference_###. Notice that our approach consistently achieves the highest WPS for all the sequence lengths. The slower performance of S4 can be attributed to its complex calculations on the NPLR. Both our model and S4 share a common characteristic: the WPS remains constant as the sequence length increases, while the transformer shows a decrease. Furthermore, the memory requirements of the transformer exhibit an almost quadratic growth, whereas our model and S4 demonstrate linear growth, with our model having a lower slope. For sequence lengths below 3072, there is minimal difference between our model and the transformer. However, as the training length increases, the undesirable quadratic growth in computation and memory complexity becomes apparent in the transformer, whereas our method avoids this issue."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Related Works",
            "text": "Since the Transformer was introduced, the linear time cost of the attention operation has been numerously researched. Optimizing this operation can improve the efficiency when training and inferencing long context for large language models Xiao et al. (2023  ###reference_b46###). Recently, many transformer variants have been introduced to reduce the complexity of attentions Tay et al. (2022  ###reference_b38###), including sparse attention Beltagy et al. (2020  ###reference_b1###); Kitaev et al. (2020  ###reference_b22###); Guo et al. (2021  ###reference_b15###), kernel-based methods Choromanski et al. (2020  ###reference_b5###); Kasai et al. (2021  ###reference_b20###); Peng et al. (2021  ###reference_b30###), chunked attention with gating Hua et al. (2022  ###reference_b17###); Ma et al. (2022  ###reference_b25###) and other efficient methods Wang et al. (2020  ###reference_b42###); Dao et al. (2022  ###reference_b7###). Another line of research tries to replace the attention mechanism with other modules for long sequences and avoid quadratic time costs. A dizzying number of attention-free models have emerged, where SSMs are becoming one of the most promising models among them. SSMs. S4 first investigated the SSM for long sequence modeling Gu et al. (2021  ###reference_b14###). They showed that naive instantiations of the SSM did not perform well but HiPPO-LegS matrix did Gu et al. (2020  ###reference_b12###), and hence introduced the DPLR that efficiently computed the complex diagonal plus low-rank matrix. DSS observed that a fully diagonal matrix could preserve the performance of the original S4 Gupta et al. (2022  ###reference_b16###), and S4D Gu et al. (2022  ###reference_b13###) then showed that the initialization is critical for DSS. Inspired by S4, many SSM variants emerged recently. S5 replaced single-input, single-output (SISO) SSMs in S4 with multi-input, multi-output (MIMO) Smith et al. (2022  ###reference_b33###). SGConv viewed the SSM as a global convolution model and suggested that the convolution kernel\u2019s sub-linear decay in sequence length is indispensable Li et al. (2022  ###reference_b23###). Linear Recurrent Unit (LRU) explored the relationship between the SSM and linear RNN and showed the importance of initialization, exponential parameterization, and normalization for SSMs Orvieto et al. (2023  ###reference_b28###). MEGA was the most similar approach to ours and plugged exponential moving average into the attention mechanism to improve position-wise local dependency Ma et al. (2022  ###reference_b25###). Our CES mechanism only considered a position-aware but representation-agnostic dependency which is completely different from the attention mechanism but matches the performance of the transformer. Other attention free models. MLP-Mixer Tolstikhin et al. (2021  ###reference_b39###), and its variants proposed to replace the attention with MLPs in computer vision task Touvron et al. (2022  ###reference_b40###); Yu et al. (2022  ###reference_b47###); Tatsunami and Taki (2022  ###reference_b36###). Another MLP-based model gMLP showed the potential of MLPs to model sequence dependency and achieved comparable results in pretraining and downstream NLP tasks Liu et al. (2021  ###reference_b24###). The Attention Free Transformer (AFT) replaced the attention mechanism with an element-wise multiplication and avoided the quadratic computation burden of the attention matrix Zhai et al. (2021  ###reference_b49###). Recurrent Memory Transformer (RMT) added a special cache token and utilized the recursive components to increase the context length in the transformer Bulatov et al. (2022  ###reference_b3###, 2023  ###reference_b2###). Receptance Weighted Key Value (RWKV) leveraged token shift for parallel training a simple linear RNN Peng et al. (2023  ###reference_b29###). Our models do not conflict with those models in spirit. Our CES modules can be integrated into their models to improve their capabilities of sequence learning."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We proposed the ETSMLP model for long-range sequence modeling. Our approach began with a special SSM, namely ETS, and incorporated additional hyperparameters. Moreover, we proposed an exponential parameterization and a constraint function essential for stable training. The experimental results demonstrated the effectiveness of the ETSMLP in sequence learning. Our proposed module could become a plug-in module on other models to enhance their sequence learning capabilities. We hope our research could provide valuable insights into the application of SSMs and encourage further exploration in this area."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "Our approach focuses on evaluating datasets containing fewer than 100,000 samples, where the influence of prior knowledge on performance is substantial. In the next phase, we aim to conduct experiments on pretraining. The considerable disparity between pretraining and training from scratch requires meticulous adjustment of exponential smoothing and ingenious design of the architecture, something like Mamba Gu and Dao (2023  ###reference_b11###).\nAnother limitation of our approach lies in the empirical design of the constraint function. This arises from the potential for lambda to surpass the precision of 32-bit floating point numbers if its training range is not restricted, and leads to NaN results during backpropagation. We believe that a low granularity parameterization can effectively mitigate this concern. Our future work will prioritize establishing a smooth training process on the parameter space."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A Proof of Proposition 1",
            "text": "We first restate Proposition 1 for the sake of convenience.\nLet  be within the interior of the hollow unit disc . We define  which substitutes  in the equation (10  ###reference_###). If the gradients of  satisfy , the gradients of the real and imaginary parts of  are bounded for all .\nLet the real and imaginary parts of  be  and , thus .\nWe define intermediate variables  for  in range  and\nCompared with the equation (10  ###reference_###), we find that .\nWe have that if , then . This is proved by the following:\nIt is obvious that the ratio  is bounded for the finite summation  is bounded too.\nTo compute the gradients of  and , we consider the gradients of  for . The function of  is holomorphic function for  thus the gradients is:\nAs  is a holomorphic function for  and , we thus have:\nThus, the gradients of  and  is computed by the chain rule in the following:\nObviously,  and  are continue on all .\nWe consider the boundary of . We first take a look at the zero point. For , we can easily compute the limitation :\nFor the point  on the unit cycle, we can find a constant  which satisfies  for all . Therefore,\nWe know that  is finite on the unit cycle except for  and the  is also finite. As a result, we can find a constant  which satisfies  for all  and . Thus, for all  on the unit cycle we have:\nSimilarly, we have:\nAs the function  and  are continues and the boundaries are finite, by the boundedness theorem, we conclude that the gradients of  and  are bounded for all .\n\u220e"
        },
        {
            "section_id": "Appendix 2",
            "parent_section_id": null,
            "section_name": "Appendix B Experimental setup",
            "text": "Architecture. We present an overview of our architecture in Figure 2  ###reference_###. The ETSMLP architecture contains  blocks and each block contains a normalization, skip connection, and an MLP plus CES. We use the ReLU activation function in MLPs. For ETSMLP-Gate architecture, an extra gate mechanism is added parallel to the original architecture. For the sake of performance, we add extra normalization like LRU Orvieto et al. (2023  ###reference_b28###). We use bidirectional models for all datasets.\nExperimental details. We use the Adam optimizer Kingma and Ba (2017  ###reference_b21###), with the hyperparameter , . We use warmup for the learning rate  that we start from a value of  and increase the learning rate linearly up a specified value for the first 10% of training. Then a linear annealing schedule is conducted for the rest of the training. All experiments except for Path-X were carried out on an NVIDIA GeForce GTX 2080 GPU, while Path-X requires 8 NVIDIA GeForce GTX 2080 GPUs.\nHyperparameters. We follow the general optimization approach used by Mega Ma et al. (2022  ###reference_b25###). Table 5  ###reference_### presents the main hyperparameters for each experiment. For all the experiments, we tune the embedding size , the number of layers , and the hidden features . We also tune the learning rate  and weight decay  for all the datasets. Besides, the  of the constraint function is all set to ."
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T1\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T1.2\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T1.2.3.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt\" id=\"S3.T1.2.3.1.1\">models</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S3.T1.2.3.1.2\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.2.3.1.2.1\">ListOps</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S3.T1.2.3.1.3\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.2.3.1.3.1\">Text</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S3.T1.2.3.1.4\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.2.3.1.4.1\">Retrieval</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S3.T1.2.3.1.5\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.2.3.1.5.1\">Image</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S3.T1.2.3.1.6\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.2.3.1.6.1\">Pathfinder</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S3.T1.2.3.1.7\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.2.3.1.7.1\">Path-X</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S3.T1.2.3.1.8\">Avg.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.4.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"S3.T1.2.4.2.1\">Transformer</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.2.4.2.2\">36.37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.2.4.2.3\">64.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.2.4.2.4\">57.46</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.2.4.2.5\">42.44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.2.4.2.6\">71.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.2.4.2.7\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.2.4.2.8\">53.66</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.5.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S3.T1.2.5.3.1\">Reformer</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.5.3.2\">37.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.5.3.3\">56.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.5.3.4\">53.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.5.3.5\">38.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.5.3.6\">68.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.5.3.7\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.5.3.8\">50.36</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.6.4\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S3.T1.2.6.4.1\">Linear Trans</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.6.4.2\">16.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.6.4.3\">65.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.6.4.4\">53.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.6.4.5\">42.34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.6.4.6\">75.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.6.4.7\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.6.4.8\">50.46</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.7.5\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S3.T1.2.7.5.1\">Performer</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.7.5.2\">18.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.7.5.3\">65.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.7.5.4\">53.82</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.7.5.5\">42.77</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.7.5.6\">77.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.7.5.7\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.7.5.8\">51.18</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.8.6\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"S3.T1.2.8.6.1\">S4</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.2.8.6.2\">58.35</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.2.8.6.3\">76.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.2.8.6.4\">87.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.2.8.6.5\">87.26</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.2.8.6.6\">86.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.2.8.6.7\">88.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.2.8.6.8\">80.48</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S3.T1.1.1.1\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.1.1.2\">60.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.1.1.3\">84.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.1.1.4\">87.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.1.1.5\">85.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.1.1.6\">84.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.1.1.7\">87.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.1.1.8\">81.88</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S3.T1.2.2.1\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.2.2\">59.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.2.3\">84.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.2.4\">87.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.2.5\">84.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.2.6\">84.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.2.7\">85.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.2.8\">81.18</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.9.7\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S3.T1.2.9.7.1\">S5</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.9.7.2\">62.15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.9.7.3\">89.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.9.7.4\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.2.9.7.4.1\">91.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.9.7.5\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.2.9.7.5.1\">88.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.9.7.6\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.2.9.7.6.1\">95.33</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.9.7.7\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.2.9.7.7.1\">98.58</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.9.7.8\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.2.9.7.8.1\">87.46</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.10.8\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S3.T1.2.10.8.1\">Mega-chunk</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.10.8.2\">58.76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.10.8.3\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.2.10.8.3.1\">90.19</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.10.8.4\">90.97</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.10.8.5\">85.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.10.8.6\">94.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.10.8.7\">93.81</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.10.8.8\">85.66</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.11.9\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"S3.T1.2.11.9.1\">ETSMLP</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.2.11.9.2\">61.35</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.2.11.9.3\">87.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.2.11.9.4\">85.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.2.11.9.5\">78.14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.2.11.9.6\">85.86</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.2.11.9.7\">87.21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.2.11.9.8\">80.92</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.12.10\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_l ltx_border_r\" id=\"S3.T1.2.12.10.1\">ETSMLP-Gate</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S3.T1.2.12.10.2\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S3.T1.2.12.10.2.1\">62.55</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S3.T1.2.12.10.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.2.12.10.3.1\">88.49</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S3.T1.2.12.10.4\">86.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S3.T1.2.12.10.5\">75.34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S3.T1.2.12.10.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.2.12.10.6.1\">91.66</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S3.T1.2.12.10.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.2.12.10.7.1\">93.78</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S3.T1.2.12.10.8\">83.09</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Performance on the LRA benchmark tasks. We follow the procedure reported in <cite class=\"ltx_cite ltx_citemacro_cite\">Ma et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.17445v1#bib.bib25\" title=\"\">2022</a>)</cite>, and report means across three seeds for our methods. The Bold scores indicate the best performance between S4, DSS, and our models. We also include and underline the state-of-art results of concurrent methods such as Mega and S5.</figcaption>\n</figure>",
            "capture": "Table 1: Performance on the LRA benchmark tasks. We follow the procedure reported in Ma et\u00a0al. (2022), and report means across three seeds for our methods. The Bold scores indicate the best performance between S4, DSS, and our models. We also include and underline the state-of-art results of concurrent methods such as Mega and S5."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T2\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T2.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S4.T2.1.1.1.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.1.1.1.1.1\">models</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"3\" id=\"S4.T2.1.1.1.2\">Classification</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\" id=\"S4.T2.1.1.1.3\">Similarity</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_tt\" colspan=\"2\" id=\"S4.T2.1.1.1.4\">Inference \u00a0\u00a0.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.2.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.2.2.1\">CoLA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.2.2.2\">SST-2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.2.2.3\">IMDB</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.2.2.4\">QQP</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.2.2.5\">MRPC</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.2.2.6\">MNLI</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.2.2.7\">QNLI</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.3.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.3.3.1\">transformer</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.3.3.2\">69.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.3.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.3.3.3.1\">81.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.3.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.3.3.4.1\">88.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.3.3.5\">80.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.3.3.6\">71.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.3.3.7\">58.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.3.3.8\">61.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.4.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.4.4.1\">ESMLP</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.4.4.2\">69.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.4.4.3\">81.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.4.4.4\">87.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.4.4.5\">81.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.4.4.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.4.4.6.1\">71.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.4.4.7\">60.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.4.4.8\">64.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.5.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T2.1.5.5.1\">ESMLP-Gate</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T2.1.5.5.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.5.5.2.1\">69.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T2.1.5.5.3\">81.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T2.1.5.5.4\">87.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T2.1.5.5.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.5.5.5.1\">82.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T2.1.5.5.6\">70.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T2.1.5.5.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.5.5.7.1\">61.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.1.5.5.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.5.5.8.1\">64.8</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Performance on the several NLU tasks. We report accuracy scores averaged across three seeds for all the datasets. All models are trained from scratch and are of a fairly similar size. The bold scores indicate the highest performance of each dataset.</figcaption>\n</figure>",
            "capture": "Table 2: Performance on the several NLU tasks. We report accuracy scores averaged across three seeds for all the datasets. All models are trained from scratch and are of a fairly similar size. The bold scores indicate the highest performance of each dataset."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T3\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T3.3\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T3.3.4.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_tt\" id=\"S4.T3.3.4.1.1\">model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T3.3.4.1.2\">Arch</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T3.3.4.1.3\">Acc</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T3.3.5.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_l ltx_border_r ltx_border_t\" id=\"S4.T3.3.5.1.1\" rowspan=\"5\"><span class=\"ltx_text\" id=\"S4.T3.3.5.1.1.1\">ETSMLP</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.3.5.1.2\">Real</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.3.5.1.3\">40.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.1.1\">No \n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.1.2\">41.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.2.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.2.2.1\">No \n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.2.2.2\">40.75</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.3.3.1\">No \n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.3.3.2\">56.05</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.6.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T3.3.6.2.1\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T3.3.6.2.2\">61.35</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Ablation analysis of the learnable parameters  on <span class=\"ltx_text ltx_font_italic\" id=\"S4.T3.7.1\">ListOps</span>. \"-\" means the keeping all the parameters of our methods.</figcaption>\n</figure>",
            "capture": "Table 3: Ablation analysis of the learnable parameters  on ListOps. \"-\" means the keeping all the parameters of our methods."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T4\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T4.5\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T4.5.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_tt\" id=\"S4.T4.5.1.1.1\">model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T4.5.1.1.2\">Initialization</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T4.5.1.1.3\">Acc</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T4.5.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_l ltx_border_r ltx_border_t\" id=\"S4.T4.5.2.1.1\" rowspan=\"6\"><span class=\"ltx_text\" id=\"S4.T4.5.2.1.1.1\">ETSMLP</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.5.2.1.2\">Stable(0.3,0)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.5.2.1.3\">52.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.5.3.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.5.3.2.1\">Stable(0.7,0)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.5.3.2.2\">52.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.5.4.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.5.4.3.1\">Stable(0.5,0)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.5.4.3.2\">52.95</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.5.5.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.5.5.4.1\">Ring(0.1,0.6)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.5.5.4.2\">60.55</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.5.6.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.5.6.5.1\">Ring(0.6,0.9)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.5.6.5.2\">58.95</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.5.7.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T4.5.7.6.1\">Ring(0.1,0.9)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T4.5.7.6.2\">61.35</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span> Performance of <span class=\"ltx_text ltx_font_italic\" id=\"S4.T4.7.1\">ListOps</span> with different initialization. \"Ring\" means uniform sampling on a ring . \"Stable\" means initialization  on the same point.</figcaption>\n</figure>",
            "capture": "Table 4:  Performance of ListOps with different initialization. \"Ring\" means uniform sampling on a ring . \"Stable\" means initialization  on the same point."
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"A0.T5\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A0.T5.7\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A0.T5.7.7\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_tt\" id=\"A0.T5.7.7.8\">Task</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"A0.T5.1.1.1\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"A0.T5.2.2.2\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"A0.T5.3.3.3\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"A0.T5.4.4.4\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"A0.T5.5.5.5\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"A0.T5.6.6.6\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"A0.T5.7.7.7\"></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A0.T5.7.8.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"A0.T5.7.8.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"A0.T5.7.8.1.1.1\">ListOps</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A0.T5.7.8.1.2\">160</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A0.T5.7.8.1.3\">12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A0.T5.7.8.1.4\">160</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A0.T5.7.8.1.5\">layer</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A0.T5.7.8.1.6\">0.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A0.T5.7.8.1.7\">0.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A0.T5.7.8.1.8\">0.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A0.T5.7.9.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"A0.T5.7.9.2.1\"><span class=\"ltx_text ltx_font_italic\" id=\"A0.T5.7.9.2.1.1\">Text</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.9.2.2\">160</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.9.2.3\">4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.9.2.4\">160</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.9.2.5\">layer</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.9.2.6\">0.005</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.9.2.7\">0.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.9.2.8\">0.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A0.T5.7.10.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"A0.T5.7.10.3.1\"><span class=\"ltx_text ltx_font_italic\" id=\"A0.T5.7.10.3.1.1\">Retrieval</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.10.3.2\">160</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.10.3.3\">6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.10.3.4\">160</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.10.3.5\">layer</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.10.3.6\">0.005</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.10.3.7\">0.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.10.3.8\">0.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A0.T5.7.11.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"A0.T5.7.11.4.1\"><span class=\"ltx_text ltx_font_italic\" id=\"A0.T5.7.11.4.1.1\">Image</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.11.4.2\">160</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.11.4.3\">12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.11.4.4\">320</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.11.4.5\">batch</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.11.4.6\">0.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.11.4.7\">0.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.11.4.8\">0.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A0.T5.7.12.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"A0.T5.7.12.5.1\"><span class=\"ltx_text ltx_font_italic\" id=\"A0.T5.7.12.5.1.1\">Pathfinder</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.12.5.2\">128</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.12.5.3\">6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.12.5.4\">256</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.12.5.5\">batch</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.12.5.6\">0.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.12.5.7\">0.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.12.5.8\">0.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A0.T5.7.13.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"A0.T5.7.13.6.1\"><span class=\"ltx_text ltx_font_italic\" id=\"A0.T5.7.13.6.1.1\">Path-X</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.13.6.2\">128</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.13.6.3\">6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.13.6.4\">256</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.13.6.5\">batch</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.13.6.6\">0.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.13.6.7\">0.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.13.6.8\">0.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A0.T5.7.14.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"A0.T5.7.14.7.1\"><span class=\"ltx_text ltx_font_italic\" id=\"A0.T5.7.14.7.1.1\">CoLA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.14.7.2\">512</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.14.7.3\">3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.14.7.4\">512</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.14.7.5\">layer</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.14.7.6\">1e-5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.14.7.7\">0.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.14.7.8\">0.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A0.T5.7.15.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"A0.T5.7.15.8.1\"><span class=\"ltx_text ltx_font_italic\" id=\"A0.T5.7.15.8.1.1\">SST-2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.15.8.2\">512</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.15.8.3\">12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.15.8.4\">512</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.15.8.5\">layer</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.15.8.6\">1e-5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.15.8.7\">0.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.15.8.8\">0.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A0.T5.7.16.9\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"A0.T5.7.16.9.1\"><span class=\"ltx_text ltx_font_italic\" id=\"A0.T5.7.16.9.1.1\">iMDB</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.16.9.2\">512</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.16.9.3\">4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.16.9.4\">512</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.16.9.5\">layer</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.16.9.6\">1e-5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.16.9.7\">0.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.16.9.8\">0.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A0.T5.7.17.10\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"A0.T5.7.17.10.1\"><span class=\"ltx_text ltx_font_italic\" id=\"A0.T5.7.17.10.1.1\">QQP</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.17.10.2\">512</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.17.10.3\">6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.17.10.4\">512</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.17.10.5\">layer</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.17.10.6\">1e-5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.17.10.7\">0.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.17.10.8\">0.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A0.T5.7.18.11\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"A0.T5.7.18.11.1\"><span class=\"ltx_text ltx_font_italic\" id=\"A0.T5.7.18.11.1.1\">MRPC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.18.11.2\">512</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.18.11.3\">6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.18.11.4\">512</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.18.11.5\">layer</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.18.11.6\">1e-5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.18.11.7\">0.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.18.11.8\">0.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A0.T5.7.19.12\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"A0.T5.7.19.12.1\"><span class=\"ltx_text ltx_font_italic\" id=\"A0.T5.7.19.12.1.1\">MNLI</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.19.12.2\">512</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.19.12.3\">6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.19.12.4\">512</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.19.12.5\">layer</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.19.12.6\">1e-5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.19.12.7\">0.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A0.T5.7.19.12.8\">0.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A0.T5.7.20.13\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_l ltx_border_r\" id=\"A0.T5.7.20.13.1\"><span class=\"ltx_text ltx_font_italic\" id=\"A0.T5.7.20.13.1.1\">QNLI</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A0.T5.7.20.13.2\">512</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A0.T5.7.20.13.3\">6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A0.T5.7.20.13.4\">512</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A0.T5.7.20.13.5\">layer</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A0.T5.7.20.13.6\">1e-5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A0.T5.7.20.13.7\">0.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A0.T5.7.20.13.8\">0.1</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>The hyperparameters of the ESMLP on LRA and NLU tasks.  is the embedding size,  is the hidden features, and  is the number of layers.  is the learning rate,  is weight decay and  is dropout.  and  in the column  refer to Batch Normalization and Layer Normalization. For NLU tasks, the small and base models have different model scales.</figcaption>\n</figure>",
            "capture": "Table 5: The hyperparameters of the ESMLP on LRA and NLU tasks.  is the embedding size,  is the hidden features, and  is the number of layers.  is the learning rate,  is weight decay and  is dropout.  and  in the column  refer to Batch Normalization and Layer Normalization. For NLU tasks, the small and base models have different model scales."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.17445v1_figure_1.png",
            "caption": "Figure 1: The relations among SSM, S4, DSS, and ETS. The HiPPO initialization is pointed out in red while the Skew-Hippo initialization is pointed out in orange."
        },
        "2": {
            "figure_path": "2403.17445v1_figure_2.png",
            "caption": "Figure 2: Overview of the ETSMLP architecture. The left is the pseudo-code of the complex exponential smoothing (CES) module. The right is the entire architecture with a gate mechanism."
        },
        "3": {
            "figure_path": "2403.17445v1_figure_3.png",
            "caption": "Figure 2: Overview of the ETSMLP architecture. The left is the pseudo-code of the complex exponential smoothing (CES) module. The right is the entire architecture with a gate mechanism."
        },
        "4": {
            "figure_path": "2403.17445v1_figure_4.png",
            "caption": "Figure 3: A training speed and memory comparison between the transformer and ETSMLP. Both models have approximately 30M parameters, and the batch size remains constant at 1 under all circumstances."
        },
        "5": {
            "figure_path": "2403.17445v1_figure_5.png",
            "caption": "Figure 3: A training speed and memory comparison between the transformer and ETSMLP. Both models have approximately 30M parameters, and the batch size remains constant at 1 under all circumstances."
        }
    },
    "references": [
        {
            "1": {
                "title": "Longformer: The long-document transformer.",
                "author": "Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020.",
                "venue": "arXiv preprint arXiv:2004.05150.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Scaling transformer to 1m tokens and beyond with rmt.",
                "author": "Aydar Bulatov, Yuri Kuratov, and Mikhail S Burtsev. 2023.",
                "venue": "arXiv preprint arXiv:2304.11062.",
                "url": null
            }
        },
        {
            "3": {
                "title": "Recurrent memory transformer.",
                "author": "Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev. 2022.",
                "venue": "Advances in Neural Information Processing Systems, 35:11079\u201311091.",
                "url": null
            }
        },
        {
            "4": {
                "title": "On the properties of neural machine translation: Encoder\u2013decoder approaches.",
                "author": "Kyunghyun Cho, Bart van Merri\u00ebnboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014.",
                "venue": "In Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 103\u2013111, Doha, Qatar. Association for Computational Linguistics.",
                "url": "https://doi.org/10.3115/v1/W14-4012"
            }
        },
        {
            "5": {
                "title": "Rethinking attention with performers.",
                "author": "Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. 2020.",
                "venue": "arXiv preprint arXiv:2009.14794.",
                "url": null
            }
        },
        {
            "6": {
                "title": "Introduction to algorithms.",
                "author": "Thomas H Cormen, Charles E Leiserson, Ronald L Rivest, and Clifford Stein. 2022.",
                "venue": "MIT press.",
                "url": null
            }
        },
        {
            "7": {
                "title": "Flashattention: Fast and memory-efficient exact attention with io-awareness.",
                "author": "Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. 2022.",
                "venue": "Advances in Neural Information Processing Systems, 35:16344\u201316359.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Automatically constructing a corpus of sentential paraphrases.",
                "author": "William B. Dolan and Chris Brockett. 2005.",
                "venue": "In Proceedings of the Third International Workshop on Paraphrasing (IWP2005).",
                "url": "https://aclanthology.org/I05-5002"
            }
        },
        {
            "9": {
                "title": "A practical survey on faster and lighter transformers.",
                "author": "Quentin Fournier, Ga\u00e9tan Marceau Caron, and Daniel Aloise. 2023.",
                "venue": "ACM Comput. Surv., 55(14s).",
                "url": "https://doi.org/10.1145/3586074"
            }
        },
        {
            "10": {
                "title": "Exponential smoothing: The state of the art.",
                "author": "Everette S Gardner Jr. 1985.",
                "venue": "Journal of forecasting, 4(1):1\u201328.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Mamba: Linear-time sequence modeling with selective state spaces.",
                "author": "Albert Gu and Tri Dao. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2312.00752"
            }
        },
        {
            "12": {
                "title": "Hippo: Recurrent memory with optimal polynomial projections.",
                "author": "Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. 2020.",
                "venue": "Advances in neural information processing systems, 33:1474\u20131487.",
                "url": null
            }
        },
        {
            "13": {
                "title": "On the parameterization and initialization of diagonal state space models.",
                "author": "Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. 2022.",
                "venue": "Advances in Neural Information Processing Systems, 35:35971\u201335983.",
                "url": null
            }
        },
        {
            "14": {
                "title": "Efficiently modeling long sequences with structured state spaces.",
                "author": "Albert Gu, Karan Goel, and Christopher R\u00e9. 2021.",
                "venue": "arXiv preprint arXiv:2111.00396.",
                "url": null
            }
        },
        {
            "15": {
                "title": "Longt5: Efficient text-to-text transformer for long sequences.",
                "author": "Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang. 2021.",
                "venue": "arXiv preprint arXiv:2112.07916.",
                "url": null
            }
        },
        {
            "16": {
                "title": "Diagonal state spaces are as effective as structured state spaces.",
                "author": "Ankit Gupta, Albert Gu, and Jonathan Berant. 2022.",
                "venue": "Advances in Neural Information Processing Systems, 35:22982\u201322994.",
                "url": null
            }
        },
        {
            "17": {
                "title": "Transformer quality in linear time.",
                "author": "Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. 2022.",
                "venue": "In International Conference on Machine Learning, pages 9099\u20139117. PMLR.",
                "url": null
            }
        },
        {
            "18": {
                "title": "The exponentially weighted moving average.",
                "author": "J Stuart Hunter. 1986.",
                "venue": "Journal of quality technology, 18(4):203\u2013210.",
                "url": null
            }
        },
        {
            "19": {
                "title": "Forecasting with exponential smoothing: the state space approach.",
                "author": "Rob Hyndman, Anne B Koehler, J Keith Ord, and Ralph D Snyder. 2008.",
                "venue": "Springer Science & Business Media.",
                "url": null
            }
        },
        {
            "20": {
                "title": "Finetuning pretrained transformers into rnns.",
                "author": "Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, and Noah A Smith. 2021.",
                "venue": "arXiv preprint arXiv:2103.13076.",
                "url": null
            }
        },
        {
            "21": {
                "title": "Adam: A method for stochastic optimization.",
                "author": "Diederik P. Kingma and Jimmy Ba. 2017.",
                "venue": null,
                "url": "http://arxiv.org/abs/1412.6980"
            }
        },
        {
            "22": {
                "title": "Reformer: The efficient transformer.",
                "author": "Nikita Kitaev, \u0141ukasz Kaiser, and Anselm Levskaya. 2020.",
                "venue": "arXiv preprint arXiv:2001.04451.",
                "url": null
            }
        },
        {
            "23": {
                "title": "What makes convolutional models great on long sequence modeling?",
                "author": "Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. 2022.",
                "venue": "arXiv preprint arXiv:2210.09298.",
                "url": null
            }
        },
        {
            "24": {
                "title": "Pay attention to mlps.",
                "author": "Hanxiao Liu, Zihang Dai, David So, and Quoc V Le. 2021.",
                "venue": "Advances in Neural Information Processing Systems, 34:9204\u20139215.",
                "url": null
            }
        },
        {
            "25": {
                "title": "Mega: moving average equipped gated attention.",
                "author": "Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. 2022.",
                "venue": "arXiv preprint arXiv:2209.10655.",
                "url": null
            }
        },
        {
            "26": {
                "title": "Learning word vectors for sentiment analysis.",
                "author": "Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011.",
                "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142\u2013150, Portland, Oregon, USA. Association for Computational Linguistics.",
                "url": "https://aclanthology.org/P11-1015"
            }
        },
        {
            "27": {
                "title": "Damped trend exponential smoothing: a modelling viewpoint.",
                "author": "Eddie McKenzie and Everette S Gardner Jr. 2010.",
                "venue": "International Journal of Forecasting, 26(4):661\u2013665.",
                "url": null
            }
        },
        {
            "28": {
                "title": "Resurrecting recurrent neural networks for long sequences.",
                "author": "Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. 2023.",
                "venue": "arXiv preprint arXiv:2303.06349.",
                "url": null
            }
        },
        {
            "29": {
                "title": "Rwkv: Reinventing rnns for the transformer era.",
                "author": "Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. 2023.",
                "venue": "arXiv preprint arXiv:2305.13048.",
                "url": null
            }
        },
        {
            "30": {
                "title": "Random feature attention.",
                "author": "Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng Kong. 2021.",
                "venue": "arXiv preprint arXiv:2103.02143.",
                "url": null
            }
        },
        {
            "31": {
                "title": "Reasoning about entailment with neural attention.",
                "author": "Tim Rockt\u00e4schel, Edward Grefenstette, Karl Moritz Hermann, Tom\u00e1\u0161 Ko\u010disk\u1ef3, and Phil Blunsom. 2015.",
                "venue": "arXiv preprint arXiv:1509.06664.",
                "url": null
            }
        },
        {
            "32": {
                "title": "Glu variants improve transformer.",
                "author": "Noam Shazeer. 2020.",
                "venue": "arXiv preprint arXiv:2002.05202.",
                "url": null
            }
        },
        {
            "33": {
                "title": "Simplified state space layers for sequence modeling.",
                "author": "Jimmy TH Smith, Andrew Warrington, and Scott W Linderman. 2022.",
                "venue": "arXiv preprint arXiv:2208.04933.",
                "url": null
            }
        },
        {
            "34": {
                "title": "Recursive deep models for semantic compositionality over a sentiment treebank.",
                "author": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013.",
                "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631\u20131642, Seattle, Washington, USA. Association for Computational Linguistics.",
                "url": "https://aclanthology.org/D13-1170"
            }
        },
        {
            "35": {
                "title": "Complex exponential smoothing.",
                "author": "Ivan Svetunkov, Nikolaos Kourentzes, and John Keith Ord. 2022.",
                "venue": "Naval Research Logistics (NRL), 69(8):1108\u20131123.",
                "url": null
            }
        },
        {
            "36": {
                "title": "Raftmlp: How much can be done without attention and with less spatial locality?",
                "author": "Yuki Tatsunami and Masato Taki. 2022.",
                "venue": "In Proceedings of the Asian Conference on Computer Vision, pages 3172\u20133188.",
                "url": null
            }
        },
        {
            "37": {
                "title": "Long range arena: A benchmark for efficient transformers.",
                "author": "Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2020.",
                "venue": "arXiv preprint arXiv:2011.04006.",
                "url": null
            }
        },
        {
            "38": {
                "title": "Efficient transformers: A survey.",
                "author": "Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2022.",
                "venue": "ACM Comput. Surv., 55(6).",
                "url": "https://doi.org/10.1145/3530811"
            }
        },
        {
            "39": {
                "title": "Mlp-mixer: An all-mlp architecture for vision.",
                "author": "Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. 2021.",
                "venue": "Advances in neural information processing systems, 34:24261\u201324272.",
                "url": null
            }
        },
        {
            "40": {
                "title": "Resmlp: Feedforward networks for image classification with data-efficient training.",
                "author": "Hugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaeldin El-Nouby, Edouard Grave, Gautier Izacard, Armand Joulin, Gabriel Synnaeve, Jakob Verbeek, et al. 2022.",
                "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(4):5314\u20135321.",
                "url": null
            }
        },
        {
            "41": {
                "title": "Attention is all you need.",
                "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017.",
                "venue": "In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS\u201917, page 6000\u20136010, Red Hook, NY, USA. Curran Associates Inc.",
                "url": null
            }
        },
        {
            "42": {
                "title": "Linformer: Self-attention with linear complexity.",
                "author": "Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. 2020.",
                "venue": "arXiv preprint arXiv:2006.04768.",
                "url": null
            }
        },
        {
            "43": {
                "title": "Neural network acceptability judgments.",
                "author": "Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. 2019.",
                "venue": "Transactions of the Association for Computational Linguistics, 7:625\u2013641.",
                "url": "https://doi.org/10.1162/tacl_a_00290"
            }
        },
        {
            "44": {
                "title": "A broad-coverage challenge corpus for sentence understanding through inference.",
                "author": "Adina Williams, Nikita Nangia, and Samuel Bowman. 2018.",
                "venue": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112\u20131122, New Orleans, Louisiana. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/N18-1101"
            }
        },
        {
            "45": {
                "title": "Forecasting sales by exponentially weighted moving averages.",
                "author": "Peter R Winters. 1960.",
                "venue": "Management science, 6(3):324\u2013342.",
                "url": null
            }
        },
        {
            "46": {
                "title": "Efficient streaming language models with attention sinks.",
                "author": "Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2023.",
                "venue": "arXiv preprint arXiv:2309.17453.",
                "url": null
            }
        },
        {
            "47": {
                "title": "S2-mlp: Spatial-shift mlp architecture for vision.",
                "author": "Tan Yu, Xu Li, Yunfeng Cai, Mingming Sun, and Ping Li. 2022.",
                "venue": "In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 297\u2013306.",
                "url": null
            }
        },
        {
            "48": {
                "title": "Kdeformer: Accelerating transformers via kernel density estimation.",
                "author": "Amir Zandieh, Insu Han, Majid Daliri, and Amin Karbasi. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2302.02451"
            }
        },
        {
            "49": {
                "title": "An attention free transformer.",
                "author": "Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and Josh Susskind. 2021.",
                "venue": "arXiv preprint arXiv:2105.14103.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.17445v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "5"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "1",
            "4.3.1",
            "4.3.2",
            "4.3.3"
        ]
    },
    "research_context": {
        "paper_id": "2403.17445v1",
        "paper_title": "Incorporating Exponential Smoothing into MLP: A Simple but Effective Sequence Model",
        "research_background": "### Motivation\n\nThe primary motivation of this paper emerges from the considerable computational and memory burden imposed by the self-attention layer of the transformer architecture, which significantly hinders its application in modeling long sequences. Due to this, the transformers, despite their success, have limitations, particularly when operating under hardware constraints, which is a familiar scenario with large language models.\n\n### Research Problem\n\nThe research problem addressed in this paper is the need for a simpler, more efficient sequence model that can perform comparably or better than existing transformer variants and others like the S4 model while alleviating the associated computational and memory overhead. Specifically, the proposed solution should be accessible, easy to implement, avoid complex initialization requirements, and maintain performance on long-range sequence benchmarks like the Long Range Arena (LRA) and on Natural Language Understanding (NLU) tasks.\n\n### Relevant Prior Work\n\n1. **Transformers and Variants**:\n   * **Vaswani et al. (2017)** introduced the Transformer architecture, which became hugely successful across various deep learning domains but is plagued by high computational and memory demands due to its self-attention mechanism.\n   * **Efforts to Reduce Burden**: Various models have been proposed to address these issues by partially modifying the attention mechanism (Beltagy et al., 2020; Choromanski et al., 2020; Kitaev et al., 2020; and others) but still struggle with long-range sequence benchmarks.\n\n2. **Structured State Space Sequence (S4)**:\n   * **Gu et al. (2021)** introduced the novel S4 framework leveraging State Space Models (SSMs). S4 uses continuous-time SSMs and addresses computational issues via the Normal Plus Low-Rank (NPLR) decomposition of state matrices and HiPPO matrices. S4 showcased superior performance on the LRA benchmark.\n   * **Complexity and Simplifications**: While S4's performance is noteworthy, its complexity in parameterization and initialization schemes presents a challenge for understanding and implementation. Efforts like those from Smith et al. (2022) and Gupta et al. (2022) aimed at simplifying S4 but still relied on HiPPO initialization requirements.\n\n3. **Alternative Approaches**:\n   * Other works established connections between SSMs and recurrent units or global convolutions, proving effectiveness in various tasks (Li et al., 2022; Orvieto et al., 2023). These studies hint at the possibility of simpler yet effective SSM architectures.\n\n### Proposed Approach\n\nThe paper introduces an alternative approach by deviating from the continuous SSM methodology of S4, starting instead with a discrete SSM\u2014Exponential Smoothing (ETS). This approach aims to:\n   1. Enhance accessibility and comprehensibility by avoiding complex continuous SSM simplifications.\n   2. Explore random initialization as opposed to HiPPO initialization.\n   3. Integrate a parameterized ETS into an element-wise MLP to transform a channel-only MLP into a sequence learner with minimal additional parameters.\n\nThe results demonstrate that the proposed model performs comparably or even better than S4 and significantly outperforms transformer variants on long-range sequence tasks while offering better efficiency and memory utilization.",
        "methodology": "Methodology: In this section, we present our ETSMLP. We first introduce a complex exponential smoothing module which is the pivotal component of our architecture. We then describe the full architecture with two proposed versions, ESMLP and ESMLP-Gate.\n\nThe Complex Exponential Smoothing Module: The core of our ETSMLP architecture is the advanced exponential smoothing module designed to handle various aspects of time series data. This module adapts to trends and seasonal patterns, facilitating more accurate predictive capabilities. By incorporating both level and trend components, the module can dynamically adjust its responses to data fluctuations, making it robust against non-stationary time series.\n\nFull Architecture: \n\n1. ESMLP:\n   - In this version, the smoothed signal generated by the complex exponential smoothing module serves as input to a Multi-Layer Perceptron (MLP).\n   - The MLP processes the input through several hidden layers, utilizing non-linear transformations to capture intricate patterns in the smoothed data.\n   - This architecture benefits from the enhanced preprocessing provided by the exponential smoothing module, allowing the MLP to focus on extracting higher-level representations.\n\n2. ESMLP-Gate:\n   - The ESMLP-Gate introduces an additional gating mechanism on top of the basic ESMLP architecture.\n   - This gating mechanism dynamically adjusts the influence of the smoothed signal entering the MLP, giving the model the flexibility to regulate the flow of information based on the relevance and context of the data.\n   - By learning when to amplify or suppress incoming signals, the gating mechanism enhances the model's ability to capture critical patterns while filtering out noise and irrelevant variations.\n\nThese two versions provide a comprehensive suite for handling sequence-based predictive tasks, leveraging the strengths of both exponential smoothing and MLP networks.",
        "main_experiment_and_results": "In the main experiment, the setup involves the evaluation of the proposed ETSMLP model against various baseline models across several sequence modeling tasks, specifically focusing on LRA (Long-Range Arena) and NLU (Natural Language Understanding) benchmarks including MNLI and IMDB.\n\n### Datasets:\n- **LRA (Long-Range Arena)**: A set of tasks designed to assess the ability of models to handle long-range dependencies in sequences.\n- **MNLI (Multi-Genre Natural Language Inference)**: A dataset for evaluating the performance on natural language inference tasks.\n- **IMDB (Internet Movie Database)**: A dataset for sentiment analysis, specifically focusing on movie reviews.\n\n### Baselines:\n- Comparison of the ETSMLP with other standard baseline models used in sequence modeling tasks. Specific baseline models are not detailed in the provided text but typically include models such as Transformers, LSTMs, and other state-of-the-art sequence models.\n\n### Evaluation Metrics:\n- The metrics for evaluating model performance on these tasks typically include accuracy for classification tasks like MNLI and IMDB and other task-specific metrics for LRA tasks.\n\n### Main Experimental Results:\n- The results are divided into two main subsections:\n  - **LRA Benchmarks**: Performance metrics on the tasks under LRA.\n  - **NLU Benchmarks**: Performance metrics on the MNLI and IMDB datasets.\n\nThe main experimental results highlight the effectiveness of the ETSMLP model in comparison to the baselines. The specific evaluation metrics (like accuracy) and the detailed results (quantitative performance numbers) can be found in the tables provided in the full paper, especially in Table 1. For more detailed descriptions of the experimental setup, datasets, and specific baseline comparisons, the reader is referred to Appendix B of the paper."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To investigate if simplifying the Exponential Smoothing (ETS) in fewer parameters affects its performance in sequence modeling tasks.",
            "experiment_process": "An ablation study was conducted by removing the damped factors and changing the parameter fields from complex to real. The performance was evaluated on the Listops dataset by comparing the accuracy results with and without the damped factors and complex fields.",
            "result_discussion": "The results indicate that removing the damped factors (\u03c4_1, \u03c4_2, \u03bc) or changing the parameter fields from complex to real significantly drops performance, especially the removal of \u03c4_1 and the complex field. This highlights the necessity of retaining these components in the architecture for effective sequence modeling.",
            "ablation_id": "2403.17445v1.No1"
        },
        {
            "research_objective": "To determine the impact of different initialization methods on the performance of the model.",
            "experiment_process": "Comparative experiments between different initialization methods were conducted, including using the ring initialization method (uniform sampling on a ring defined by a range around a predetermined value) and fixed-value initialization. The effects were examined on the Listops dataset.",
            "result_discussion": "The model exhibits consistent performance across varying sizes of the ring in the ring initialization method. However, the effectiveness diminishes significantly when using fixed-value initialization. This suggests the importance of the ring initialization method for maintaining performance.",
            "ablation_id": "2403.17445v1.No2"
        },
        {
            "research_objective": "To compare the speed and memory efficiency of the proposed model against the transformer and S4 models.",
            "experiment_process": "Experiments were performed using a synthetic dataset for language modeling tasks with varying sample lengths (maximum length of 8192). Words per second (WPS) and memory usage were measured on an NVIDIA GeForce GTX 2080 GPU, with a uniform batch size of 1.",
            "result_discussion": "The proposed model consistently achieves the highest WPS for all sequence lengths. The transformer shows a decrease in WPS with increased sequence length, whereas the proposed model and S4 maintain constant WPS. In terms of memory usage, the transformer shows an almost quadratic growth, while the proposed model and S4 exhibit linear growth, with the proposed model having a lower slope, highlighting its efficiency.",
            "ablation_id": "2403.17445v1.No3"
        }
    ]
}