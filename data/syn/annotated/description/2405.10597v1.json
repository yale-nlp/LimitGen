{
    "title": "UniCL: A Universal Contrastive Learning Framework for Large Time Series Models",
    "abstract": "Time-series analysis plays a pivotal role across a range of critical applications, from finance to healthcare, which involves various tasks, such as forecasting and classification.\nTo handle the inherent complexities of time-series data, such as high dimensionality and noise,\ntraditional supervised learning methods first annotate extensive labels for time-series data in each task, which is very costly and impractical in real-world applications.\nIn contrast, pre-trained foundation models offer a promising alternative by leveraging unlabeled data to capture general time series patterns, which can then be fine-tuned for specific tasks.\nHowever, existing approaches to pre-training such models typically suffer from high-bias and low-generality issues due to the use of predefined and rigid augmentation operations and domain-specific data training.\nTo overcome these limitations, this paper introduces UniCL, a universal and scalable contrastive learning framework designed for pretraining time-series foundation models across cross-domain datasets.\nSpecifically, we propose a unified and trainable time-series augmentation operation to generate pattern-preserved, diverse, and low-bias time-series data by leveraging spectral information.\nBesides, we introduce a scalable augmentation algorithm capable of handling datasets with varying lengths, facilitating cross-domain pretraining.\nExtensive experiments on two benchmark datasets across eleven domains validate the effectiveness of UniCL, demonstrating its high generalization on time-series analysis across various fields.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1. introduction",
            "text": "Time series data, which consists of sequences of real values recorded over time intervals, are prevalent in numerous real-world applications (Wen\net al., 2022a  ###reference_b65###), including finance, healthcare, environmental monitoring, and manufacturing.\nTime series analysis involves various tasks such as forecasting (Sezer\net al., 2020  ###reference_b58###) and classification (Ismail Fawaz et al., 2019  ###reference_b22###),\nwhich are crucial for decision-making processes.\nHowever, analyzing time series data remains challenging due to their inherent properties, such as high dimensionality, noise, non-stationarity, periodicity, etc. (Trirat et al., 2024  ###reference_b63###).\nThese features pose difficulties in effectively capturing and leveraging the underlying patterns in time series data, thus impacting its practical applications (Ma\net al., 2023  ###reference_b41###), highlighting the ongoing challenge of analyzing time series data across various tasks.\nIn general, existing time-series analysis in deep learning can be categorized into two types, supervised learning (SL)-based approaches (Salehinejad et al., 2017  ###reference_b56###; Wen\net al., 2022b  ###reference_b66###) and pretrained foundation model (PFM)-based approaches (Jin et al., 2023  ###reference_b24###; Zhang et al., 2024  ###reference_b77###).\nFirst, SL-based approaches (Wu\net al., 2021  ###reference_b70###; Liu\net al., 2021  ###reference_b36###; Zhou et al., 2021  ###reference_b83###; Koh\net al., 2021  ###reference_b27###) propose to train a deep learning model, such as transformers (Zhou\net al., 2022  ###reference_b84###; Wu\net al., 2021  ###reference_b70###; Nie\net al., 2023  ###reference_b44###) and CNNs (Zhao\net al., 2017  ###reference_b81###), on labeled time series data. However, these SL-based approaches rely on a large number of annotated time series data (Khaertdinov et al., 2021  ###reference_b25###), which is often impractical in many real-world scenarios where obtaining labeled time-series data is costly or infeasible.\nSecond, current researchers  (Dong\net al., 2024  ###reference_b13###; Yue et al., 2022  ###reference_b73###; Liu et al., 2023  ###reference_b37###; Bian\net al., 2024  ###reference_b4###) propose to first pre-train a foundation model (e.g., large language models) on time-series data, such as those initially large language models (e.g., ChatGPT (OpenAI, 2022  ###reference_b46###)) developed for processing natural language.\nThe basic idea is to first pre-train the foundation model to capture the general intrinsic patterns of time-series data.\nSubsequently, the foundation model can be fine-tuned on specific time-series tasks using a smaller set of labeled data.\nThese PFM-based approaches can leverage the learned general patterns to enhance performance across a variety of tasks.\nDepending on the technique employed to pre-train the foundation model, existing research (Li\net al., 2023  ###reference_b29###; Zerveas et al., 2021  ###reference_b75###; Dong\net al., 2024  ###reference_b13###) first propose to use mask-based approaches to pre-train the foundation model in time series data.\nThey directly use the foundation model to predict the next values in a time series (Bian\net al., 2024  ###reference_b4###; Liu et al., 2023  ###reference_b37###; Rasul et al., 2023  ###reference_b54###) or reconstruct randomly masked values of the data (Dong\net al., 2024  ###reference_b13###), i.e., predictive objectives.\nHowever, these mask-based pre-training approaches require numerous unlabeled time-series data to enable the foundation model to capture general time-series patterns (Wen\net al., 2022b  ###reference_b66###).\nUnlike corpus data, the time-series are scarce in the open-world website (Ansari\net al., 2024  ###reference_b2###; Liu\net al., 2024c  ###reference_b38###). Consequently, these mask-based approaches achieve suboptimal performance on the downstream tasks.\nRecently, to alleviate the heavy reliance on numerous time series data, contrastive learning approaches (Yeh et al., 2023  ###reference_b72###; Eldele et al., 2021  ###reference_b14###; Meng\net al., 2023  ###reference_b42###; Woo\net al., 2022a  ###reference_b67###; Luo et al., 2023  ###reference_b40###) are proposed to augment time-series data and pre-train the foundation model based on augmented data.\nThe basic idea is to first generate positive views for each time series data to preserve its key patterns.\nThen, the foundation model is optimized by maximizing the\nrepresentation similarity between positive view pairs and minimizing the representation similarity between\npositive views and other randomly selected time series data (i.e., negative views), known as contrastive objectives.\nThis enables the pre-trained model to distinguish patterns across different time-series data, thereby facilitating downstream tasks (Ma\net al., 2023  ###reference_b41###).\nNevertheless, existing CL-based pretrained foundation models suffer from two issues, i.e., high-bias and low-generality issues.\nFirst, these models typically predefine and apply a set of time series augmentation operations, such as permutation (Meng\net al., 2023  ###reference_b42###; P\u00f6ppelbaum et al., 2022  ###reference_b48###), random masking (Yue et al., 2022  ###reference_b73###; Wang\net al., 2024  ###reference_b64###),\nand warping (Fan\net al., 2020  ###reference_b15###) to generate positive views.\nHowever, existing predefined operations ignore the time-series intrinsic properties,\nsuch as periodicity (Qiu et al., 2024  ###reference_b49###),\nthereby introducing high bias and noise. For instance, permutation shuffles the sequential information in time series data, potentially resulting in the loss of its inherent patterns.\nConsequently, the foundation models, optimized by maximizing the similarity between the generated positive views, may not capture the patterns of real time-series data, thus degrading the performance of downstream tasks (Zheng et al., 2023  ###reference_b82###; Yue et al., 2022  ###reference_b73###; Eldele et al., 2021  ###reference_b14###).\nSecondly, these models are generally pretrained within a single specific domain (Yue et al., 2022  ###reference_b73###; Woo\net al., 2022a  ###reference_b67###; Luo et al., 2023  ###reference_b40###), such as medicine (Wang\net al., 2024  ###reference_b64###). However, time series data can vary significantly across different domains (e.g., medical, traffic, and weather) in aspects of the number of variables, sequence length, and frequency (e.g., daily, weekly, monthly).\nDue to these variations, foundation models trained on one domain often fail to perform effectively across other domains (Ma\net al., 2023  ###reference_b41###).\nIn other words, existing approaches (Wang\net al., 2024  ###reference_b64###; Yue et al., 2022  ###reference_b73###; Woo\net al., 2022a  ###reference_b67###) need to pre-train a single foundation model for each specific domain, which is time-consuming and very costly.\nTo address the high-bias and low-generality issues, we propose a universal pre-trained foundation model for time series analysis.\nFirstly, to address the high-bias issue, we propose to use effective pattern-preserved augmentation operations to generate positive views for the time series data.\nBy optimizing the foundation model on these augmented views, it becomes capable of learning and retaining the intrinsic patterns present in real-world time series data, thereby reducing bias.\nSecondly, to overcome low-generality issue, we propose to pre-train the foundation model across a diverse array of time series data from various domains.\nIn such a way, this foundation model can learn a wide range of time series patterns specific to different domains.\nAs a result, the foundation model can generalize on various domains and thereby facilitate their downstream tasks.\nHowever, there are three technique challenges to achieving this unified foundation model.\nThere is no theoretical analysis and established metrics for preserving the patterns of time-series data with deep learning. Without these metrics, we cannot design effective augmentation operations to generate positive views that keep the intrinsic patterns.\nDue to the high variations (e.g., variable number and sequence length) in time-series data from different domains, it is challenging to design a scalable and unified augmentation algorithm that is applicable across these diverse settings.\nLast but not least, existing studies train the LLM-based encoder for time-series analysis by optimizing the predictive objective, yet, the exploration of the contrastive objective has received comparatively less attention.\nTo address the above technique challenges, we propose UniCL, a universal contrastive learning framework designed for pre-training time-series foundation models across diverse domains.\nFirstly, we empirically reveal a positive correlation between the bias of time series embeddings and the spectral distance between augmented and raw series. Building on this insight, we propose a unified and trainable time series augmentation operation with theoretical guarantees. Through optimization of two novel losses, our proposed operation generates spectrum-preserved, diverse, and low bias augmented series.\nSecondly, to tackle high variations in datasets and enable large-scale automatic pre-training, we propose a scalable and unified augmentation algorithm. This algorithm utilizes spectrum-preserved time-series segmentation, augmenting each subseries individually. We demonstrate that the difference in convergence loss between the scalable and non-scalable algorithms can be bounded.\nThirdly, to fully leverage the potential of the LLM backbone, we train a transformer-based encoder on 40 cross-domain datasets, initialized with pre-trained weights from the text encoder of CLIP (Radford et al., 2021  ###reference_b50###), owing to our shared contrastive objectives.\nWe summarize the novel contributions of this paper as follows.\nWe present UniCL, an end-to-end general framework for pre-training large foundation time-series models based on contrastive learning, capable of handling high variation time-series datasets.\nWe reveal the factor of representation bias based on a novel metric, and propose a unified and trainable augmentation operation with theoretical guarantees. We then propose two novel losses to facilitate the optimization of this operation.\nWe propose a scalable and unified algorithm to handle data with varied length by pattern-preserved segmentation and concatenation, and demonstrate bounded convergence loss differences between scalable and non-scalable algorithms.\nTo the best of our knowledge, we are the first to train the LLM backbone with contrastive objectives for general time-series analysis. We train the UniCL on 40 cross-domain datasets, and provide a comprehensive evaluation of its performance across two downstream tasks."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2. Preliminaries and related works",
            "text": "In this section, we first introduce the basic concepts in time-series data analysis and then\nintroduce the foundation models for time-series data analysis.\nThe important notations are listed in Tab. 1  ###reference_###."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1. Time-series Data Analysis",
            "text": "Time series data are a set of sequences of observations where each sequence corresponds to a different variable, and all sequences are recorded over the same time periods.\nTime series data plays a crucial role in various fields (Wen\net al., 2022a  ###reference_b65###), such as economics, finance, environmental science, and engineering.\nFormally, a time series data instance with  variables over time  can be denoted as , where  in the observed real value of variable  from time  to .\nIn general, if the number of variable  is 1 (resp, ), this time-series data is called univariate (resp. multivariate) time-series data.\nTime-series analysis involves various tasks such as forecasting (Lim and Zohren, 2021  ###reference_b31###) and classification (Ismail Fawaz et al., 2019  ###reference_b22###).\nWe first give a general time-series data analysis problem definition for various tasks.\nFormally, given a training time-series data  with  instances for a task, where  denotes each time series instance and  is the label of ,\nthe target is to supervised train an encoder model  and a simple decoder model  (e.g., simple linear regression model (Yue et al., 2022  ###reference_b73###)) on data .\nThe encoder model  is used to encode each time-series data  to\nan embedding matrix with  dimensions .\nThen the decoder  will use the  to predict the task labels .\nIn general, the parameters of the encoder model  and the decoder model  can be optimized by\nminimizing the task loss  as follows:\nThen, we discuss how to adapt this definition for each specific task as follows.\nTime-series Classification Task.\nThe target is to predict the label  of each time-series data instance .\nIn general, , where  is multiple-label ground truth of .\nBased on cross-entropy, the classification loss  (Tian Zhou, 2023  ###reference_b60###; Sun\net al., 2023  ###reference_b59###) can be defined as: , where  is the predicted probability of each label  for data .\nTime-series Forecasting Task.\nThe target is to predict the future values  of a time-series data based on the previous observation , where  is the look back window size, and  denotes the number of future prediction values.\nIn general,\n, and forecasting loss  (Jin et al., 2024  ###reference_b23###; Liu et al., 2023  ###reference_b37###) can be defined as , where  is the predicted values by the encoder and decoder models.\nTo handle each specific task in time-series analysis, existing research (Wu\net al., 2021  ###reference_b70###; Liu\net al., 2021  ###reference_b36###; Zhou et al., 2021  ###reference_b83###) propose to prepare the training data set. Then, they train deep learning models, such as transformers and convolutional neural networks (CNNs), on labeled time series data.\nHowever, these approaches need substantial labeled data, which is time-consuming and very costly (Liu\net al., 2024b  ###reference_b32###)."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2. Foundation Models for Time-series Data",
            "text": "To alleviate the reliance on labeled data, current researchers propose to learn a foundation model that captures the general patterns of time-series data and then fine-tune this foundation model with limited labels for the downstream tasks."
        },
        {
            "section_id": "2.2.1",
            "parent_section_id": "2.2",
            "section_name": "2.2.1. Pretrained Foundation Models",
            "text": "In general, existing pretrained foundation models for time-series data analysis can be categorized into the following four types, i.e., pretrained language model-based, mask-based, and contrastive learning (CL)-based foundation models."
        },
        {
            "section_id": "2.2.2",
            "parent_section_id": "2.2",
            "section_name": "2.2.2. Fine-tuning Time-series Foundation Model on Downstream Tasks",
            "text": "Then, the time-series foundation model  pre-trained in Sec. 2.2.1  ###reference_.SSS1### can be used in time-series classification and forecasting tasks.\nIn general, there are two manners to apply the pretrained foundation model for the downstream tasks (Wang\net al., 2024  ###reference_b64###), i.e., Partial Fine-Tuning (P-FT) and Full Fine-Tuning (F-FT). The main difference between P-FT and F-FT is the parameter size of the foundation models and decoders.\nPartial Fine-Tuning (P-FT). P-FT approaches (Yue et al., 2022  ###reference_b73###; Wang\net al., 2024  ###reference_b64###) are to keep the foundation model  frozen. They only train the decoder  in downstream tasks. The objective is defined as follows.\nFull Fine-Tuning (F-FT). F-FT approaches (Tian Zhou, 2023  ###reference_b60###; Chen\net al., 2023  ###reference_b8###; Chang\net al., 2023  ###reference_b5###; Liu et al., 2024a  ###reference_b35###) involve training both the decoder  and the foundation model , encompassing parameters such as positional embeddings and LayerNorm parameters (Tian Zhou, 2023  ###reference_b60###; Chen\net al., 2023  ###reference_b8###), or the parameters within self-attention layers (Chang\net al., 2023  ###reference_b5###; Liu et al., 2024a  ###reference_b35###) in downstream tasks, which is defined as follows.\nCompared with P-FT, F-FT can optimize parameters of the foundation models for downstream tasks, expecting to achieve better performance than P-FT.\nTherefore, in this paper, we use F-FT to apply our model and baselines in the downstream tasks."
        },
        {
            "section_id": "2.2.3",
            "parent_section_id": "2.2",
            "section_name": "2.2.3. Variable independence.",
            "text": "Many transformer-based learning models utilize the variable-mixing (or channel-mixing) configuration (Wu\net al., 2021  ###reference_b70###; Zhou\net al., 2022  ###reference_b84###), where the multivariate time-series  is mapped into a timestamp-wise shared space  via an embedding layer. However, this approach introduces two critical issues: 1) The embedding layer requires the pre-definition of the number of variables, which lacks generality for cross-domain pretraining; 2) A timestamp-wise shared embedding space may not be suitable for all domains, as the mechanism of dependency can vary (e.g., lag-features of financial time-series (Rasul et al., 2023  ###reference_b54###)). To facilitate universal pretraining, our study adopts the recent widely-adopted variable independence configuration (Nie\net al., 2023  ###reference_b44###; Tian Zhou, 2023  ###reference_b60###; Jin et al., 2024  ###reference_b23###), processing  variables independently."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3. Framework Overview",
            "text": "In this study, we introduce UniCL, a universal contrastive learning framework designed for time-series analysis. UniCL is fundamentally general and effective, capable of handling heterogeneous cross-domain time-series data with varying input lengths, based on a unified augmentation operation that generates diverse positive samples, thereby facilitating robust contrastive learning.\nWe provide the overview of UniCL in Fig. 1  ###reference_###. Primarily, there\nare four steps: (1) data generation, (2) unified and scalable data augmentation module, (3) time-series encoder based on LLMs, and (4) embedding contrast. Here, we briefly clarify these steps: (1) to begin with,\nthe time-series datasets from diverse domains are initially partitioned into batches, shuffled, and then randomly fed into the augmentation module ;\n(2) for each batch, the proposed scalable algorithm with bounded convergence loss can deal with varying lengths of inputs with missing values, and the unified and learnable augmentation operation is employed to generate diverse and pattern-preserved positive views for contrastive learning ;\n(3) the CLIP-based encoder generates embeddings for all views, effectively capturing cross-domain and general time-series patterns;\n(4) a typical contrastive loss can be employed to enhance the discriminative power of the learned embeddings."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4. Unified Foundation Model",
            "text": "We first demonstrate our key observations about the bias in time-series representation caused by pre-determined augmentation methods. Then, we summarize existing methods and propose a unified and learnable augmentation operation family with theoretical guarantee. To facilitate the training of such operations, we introduce two novel efficient loss functions. Additionally, we propose a scalable version of this unified operation set to handle datasets from various domains with different lengths and missing values.\nFinally, we introduce the encoder of the UniCL and the whole pre-training paradigm."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1. A Unified Data Augmentation Operation",
            "text": ""
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1. Motivational observation of bias in embedding",
            "text": "As discussed in Sec. 1  ###reference_###, existing pre-defined time-series augmentation methods introduce an inductive bias into representation learning amidst augmentation contrast. We present our key motivational observation: the bias in time-series embedding correlates positively with the spectral distance (SD) between raw and augmented series. To illustrate, we first quantify the bias introduced by the pre-defined data augmentation family  as follows, in line with a previous work (Zhang\net al., 2022c  ###reference_b80###). Let  denotes the augmentation set of  with size , where . Let  symbolize the transformation distribution of . Then,\nLet  denote the Fast Fourier Transform (FFT), and  denote the amplitude operator, which calculates the amplitude as , where  and  represent the real and imaginary part operators, respectively. Due to the conjugate symmetry of the frequency domain, we stipulate that the  operator only generates the first half and removes the zero-frequency component (Wu\net al., 2023  ###reference_b69###), i.e., . Then, the spectral distance between  and  can be defined as:\nWe employ the representative and state-of-the-art contrastive learning method TS2Vec (Yue et al., 2022  ###reference_b73###) to test 4 pre-defined augmentation methods on 23 selected multivariate datasets from the UEA Time Series Classification Archive (Bagnall et al., 2018  ###reference_b3###) and report the average bias and spectral distance. For each dataset, we train the TS2Vec encoder with the same configuration as the original paper but varying the augmentation method. These methods encompass jittering, scaling, time warping, and permutation, each offering diverse variants achieved by adjusting hyper-parameters (e.g., the standard deviation of jittering). The output layer of the encoder has two dimensions to facilitate visualization. Following, we generate  augmented samples randomly for each sample, and compute the average bias  and the average spectral distance , respectively. Fig. 2  ###reference_###(a) illustrates a positive correlation between the average bias and the average spectral distance of augmented and raw series. Meanwhile, Fig. 2  ###reference_###(b) demonstrates that greater bias in the embeddings results in reduced performance on downstream classification tasks. This observation motivates the need for time-series augmentation methods to control the spectral distance between augmented and raw time-series. As depicted in Fig. 2  ###reference_###(c,d), within contrastive learning, a significant bias may hinder the effectiveness of separating augmented embeddings across different instances (e.g., ), thus limiting the discriminative power of learned embeddings for downstream tasks (Zhang\net al., 2022c  ###reference_b80###).\n###figure_2### ###figure_3### ###figure_4### ###figure_5###"
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "4.1.2. Unified Data Augmentation Operation",
            "text": "To mitigate the problem of inductive bias caused by pre-defined augmentation operation , one\nstraightforward way is to employ all augmentation operations (Luo et al., 2023  ###reference_b40###; Zhang\net al., 2022b  ###reference_b79###; Ozyurt\net al., 2023  ###reference_b47###). However, for some augmentation operations, the hyper-parameter space is continuous (e.g., standard deviation of jittering, number of speed changes of time warping), making it infeasible to explore the full augmented view space, especially considering their compositions (Ozyurt\net al., 2023  ###reference_b47###). To address this challenge, we introduce a unified operation family . Formally, given input time-series , the augmentation operation  sampled from , i.e., , is defined as:\nwhere ,\n and . We provide Proposition 1 to demonstrate that the operation  yield an augmented view space equivalent to that of each pre-defined operation and their compositions.\nExisting time-series augmentation operation set  includes jittering, scaling, magnitude warping, masking, pooling, and permutation. Then, the augmented view space of  generated by the unified operation  is the same as the view space generated by  as well as their compositions , where .\nThe proof for each augmentation operation is presented in Table 2  ###reference_###. Therefore, for each , there exist matrix  and vector  such that . Let  be the composite operator. Then,\nThe above concludes that the compositions of  also belongs to the unified operation family .\n\u220e\n###table_1### To introduce randomness into the generation of diverse positive samples, without loss of generality, we can incorporate a random matrix  with the deterministic matrix . Formally, given input time-series with length , the non-scalable unified operation can be expressed as:\nand the matrix form is:\nwhere  is a  deterministic matrix,  is a  random matrix, and  is a  dimensional random vector.\nAs shown in Alg. 2  ###reference_###, we set  to be a Gaussian noise matrix (line 2), where all elements are independent and identically distributed (i.i.d.) random variable following Gaussian distribution, i.e., . Here, both  and  are  trainable matrix. Similarly,  is a Gaussian noise vector (line 3), where all elements are i.i.d. and each  follows a Gaussian distribution  with random vectors  and  as trainable parameters. Therefore, the time and space complexity is , and we will introduce scalable and efficient algorithms in Section 4.2  ###reference_###. Let  symbolize the transformation distribution of , we have the following proposition:\nThe transformation distribution  follows a multivariate normal distribution.\nConsidering the -th random variable , we have:\nNotice that the first term is constant, as the matrix  and input series  are deterministic. The remainder of the equation becomes a linear combination of normal distributions and i.i.d. random variables, i.e.,  and , resulting in  following normal distribution. The above concludes that the transformation distribution\n is a multivariate normal distribution.\n\u220e"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2. Scalable and Diverse Data Augmentation",
            "text": "We first propose a data augmentation objective based on our proposed unified operation to generate spectrum-preserved and diverse time-series data. Then, we propose a scalable algorithm to apply this objective to time-series data with various lengths."
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1. A Spectrum-preserved and Diverse Objective",
            "text": "In contrastive learning, augmented data should satisfy two properties to be effective: pattern preservation and diversity (Luo et al., 2023  ###reference_b40###). Augmented data should preserve the essential pattern of the original data, ensuring that the similarity relationships between instances are maintained. Then, the  optimized by maximizing the similarity\nof positive views are expected to learn representations that capture time-series intrinsic patterns. Additionally, augmented data should introduce diversity into generated time-series, enabling the model to learn robust representations that generalize well to unseen data. We introduce two novel losses to facilitate the learning of our augmentation module: spectrum-preservation loss and spectrum-diversity loss.\n1) Spectrum-preservation loss.  As discussed in Sec. 4.1.1  ###reference_.SSS1###, in order to generate low-bias embeddings, the positive pairs  and  should be close to the original series  in terms of spectral distance. Therefore, the spectral distance in Eq. 9  ###reference_### can serve as the metric to measure the pattern differences of augmented series and raw series. Formally, the spectrum-preservation loss  can be defined as:\n2) Spectrum-diversity loss.  To enhance the diversity of the positive pairs  and , we need to: 1) define a metric to quantify the diversity of positive pairs, and 2) identify which patterns in  are not essential and can therefore be diversified.\nBy Prop. 2  ###reference_p2###, the positive pairs  and  are two random vectors, where the -th values  and  are random variables with i.i.d. normal distribution. One intuitive approach to measure the diversity is to compute the average entropy of all random variables, given by , where  . However, simply increasing the entropy of each point results in large  and , introducing meaningless noise.\nMore generally, suppose the distribution of  and  are unknown. Another intuitive approach to measure the diversity of positive views is to compute the average Kullback-Leibler () divergence between the probability distributions of each pair of random variables at the same time point, given by  , where  denotes the probability density function (PDF). However, in a time-series, only one observation is available at each timestamp. Estimating  and  requires sampling  operators , with  denoting the number of samplings for each view. Since this approximation must be conducted for every time point, the time complexity amounts to , which becomes infeasible when both  and  are large.\nTo tackle the complexity issue, rather than approximating the distribution in the time domain, we transform the positive views  and  into the frequency domain as  and , respectively.\nHere, the -th element  and , denoting the amplitude of the -th frequency component, are also random variables. We convert the amplitude sequence to a probability mass function (PMF) by  and , where  is the temperature parameter. Then, we can measure the diversity by calculating the Jensen-Shannon () divergence between PMF  and PMF , which is more efficient.\nHowever, not all the frequency component should be diversified.\nGiven , the  represent the relative strength of -th frequency within the entire spectrum. The low-frequency component () carries crucial semantic information such as trend and periodicity, while high-frequency component () usually carries meaningless noise (Zhou\net al., 2022  ###reference_b84###). Therefore, we multiply the PMF by a decay factor  using element-wise multiplication , thereby assigning different weights to different components. Formally, the spectrum-diversity loss  can be defined as:\nwhere , and  is used to stabilize the optimization.\n3) Summary.  Formally, we define the objective function  of the data augmentation module as follows:\nwhere  is a hyper-parameter."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2. Scalable operations",
            "text": "With the learnable unified operation and the corresponding loss function, the question is how we can efficiently employ it in time-series datasets exhibiting high variations, including varying sequence lengths and potential missing values.\n###figure_6### 1) Varying sequence lengths. To propose a scalable algorithm for handling inputs with varying lengths, two key issues need to be addressed: 1) Given the input series , the space complexity of the non-scalable algorithm outlined in Alg. 2  ###reference_### is . This complexity becomes impractical when we receive the long sequence. 2) In large-scale pretraining, different datasets contain varying lengths of instances, and employing different size of unified operation for each dataset is inefficient. Thus, we introduce a scalable algorithm that offers efficient implementation, necessitating only a fixed-size unified operation across all datasets. This approach results in a space complexity of , where  is a constant.\nParticularly, with a constant , we define the fix-sized unified operation as follows:\nSuch an operation can only handle input time-series with a fix length , therefore we need to extend (resp. segment) the inputs  when  (resp. ). When , we employ iterative extension of the time series through repetition until its length equals . This iterative repetition aligns with the periodicity assumption of Fourier analysis, ensuring , thereby preserving the amplitude spectrum of . Conversely, alternative methods such as padding (Zerveas et al., 2021  ###reference_b75###) may disrupt the spectrum pattern of .\nAs illustrated in Fig. 3  ###reference_### and Alg. 3  ###reference_###, when , we employ segmentation to the input time-series. Since segmentation may disrupt the intrinsic pattern, we first denote the  as the vulnerable pattern which will be disrupted by segmentation. This pattern should be subtracted prior to the segmentation (line 1) and restored after concatenation (line 6), which is spectrum-preservation due to the linearity of the Fourier transform.\nThen, we segment the time-series into  non-overlapping and contiguous subseries  (line 2), where  is the residual term and . Subsequently, we augment each subseries separately using fix-sized unified operation (line 4) and concatenate them in the same order as , where . Finally, we obtain the augmented series by restoration (line 6).\nTime and Space Complexity. For line 1-3 and 5-6, the time and space complexity is . For line 4, since only a fix-sized unified operation family is needed, the space complexity is  according to Alg. 2  ###reference_### and the time complexity is .\nGiven that 1) ; 2)  is a linear function (i.e., a linear map ): then the augmented view space generated by the scalable algorithm Alg. 3  ###reference_### is a subspace of the view space generated by the non-scalable algorithm Alg. 2  ###reference_###.\nWe can express the equivalent form of the scalable algorithm by introducing a scalable unified operation family . For each scalable unified operation , we have:\nwhere  and   are both block diagonal matrices,\nand . Since  is a linear function, we have , where  is a  matrix. Thus, we get the following Equation:\nThus, we have .\n\u220e\nThe difference in loss convergence between scalable and non-scalable algorithms is bounded by: (1) hyperparameter ; (2) the spectral distance , where .\nBy Prop. 3  ###reference_p3###, the non-scalable operation  is associated with a larger hypothesis space compared to the scalable operation . Without loss of generality, we make the assumption that , where  and  . We derive the upper bound of the difference of the convergence loss:\nTherefore, to lower the upper bound, we can 1) choose small ; 2) choose  by optimizing the below optimization problem:\n\u220e\nThe Lemma 4.1  ###reference_theorem1### suggests that the augmented time-series generated by scalable operation  and non-scalable operation  should exhibit small spectral distance. However, in Alg. 3  ###reference_###, segmenting the time-series into  non-overlapping subseries may disrupt the  lowest frequency components. Therefore, we define the linear function  to extract the lowest  frequency components, thereby preserve such vulnerable patterns. The -th value of function  can be formulated as:\nwhere ,  is the amplitude,  is the angular frequency, and  is the phase of the -th lowest frequency component.\n2) Missing values. Given a minibatch  of time-series data, which may contain missing values requiring imputation, we are motivated by the recognition that the low-frequency components carry essential information, while the high-frequency components often introduce noise. To address this, we initially employ linear interpolation to handle missing values in . Subsequently, to filter out the high-frequency noise introduced by linear interpolation, we apply a moving average with a window size of ."
        },
        {
            "section_id": "4.2.3",
            "parent_section_id": "4.2",
            "section_name": "4.2.3. Training algorithm",
            "text": "We summarize the learning objectives and scalable algorithm of the data augmentation module in Algorithm 4  ###reference_###. To calculate the expectations in Eq.15  ###reference_### and Eq.16  ###reference_###, we introduce two hyper-parameters, denoted as  and .\nHere,  represents the number of samplings for the unified operations in the spectrum-preservation loss (line 8), and  is the number of sampled operation pairs in the spectrum-diversity loss (line 14), where .\nThe outcomes of parameter-sensitive experiments are presented in Sec. 5.4  ###reference_###. Empirically, employing small values for  and  has demonstrated both efficiency and effectiveness.\nTime complexity.\nThe time complexity of Alg. 4  ###reference_### is analyzed as follows. Assuming the length of each time-series is denoted as , the loop from lines 2 to 16 takes  time to traverse all input time-series. Lines 3 to 4 perform linear interpolation and moving average with a small constant window size, which takes  time. Line 8 involves a scalable operation comprising matrix multiplication and addition operations based on Eq. 13  ###reference_### and Eq. 19  ###reference_###. Let the fixed window length be denoted as , then the time complexity is  according to Alg. 3  ###reference_###. Line 10 can be computed using the fast Fourier transform (FFT), which takes  time. Line 13 is not counted since  is a small constant. Line 14 reuses the results from line 8 and aims to calculate the  divergence, which takes  time.\nThus, the total time complexity of Algorithm 4  ###reference_### is . In practice, as  represents the number of time-series and  represents the number of variables, both can be organized into a single matrix to leverage GPU resources for accelerated computation."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "4.3. Pre-train LLM",
            "text": ""
        },
        {
            "section_id": "4.3.1",
            "parent_section_id": "4.3",
            "section_name": "4.3.1. Encoder design",
            "text": "The encoder  comprises two primary components: an input embedding layer and a stack of causal transformer-encoder blocks. Initially, the input embedding layer partitions the input positive views  and  generated by the augmentation module into a series of consecutive patches, which may be overlapping or non-overlapping (Nie\net al., 2023  ###reference_b44###), each with a length of . The total count of patches is , where  represents the sliding stride.\nSubsequently, a reprogramming layer (Liu et al., 2024a  ###reference_b35###) is employed to transform the  into the high-dimensional space .\nThen, both  and  are fed into the causal transformer-decoder blocks for representation learning, which mirror the architecture of the text encoder in ViT-G/14 CLIP, comprising 32 transformer blocks with 1280 hidden dimensions. The weights of these 32 blocks are initialized from ViT-G/14 CLIP as well.\nViT-G/14 CLIP is a large pre-trained model designed for processing visual and textual data, trained on approximately  pairs of text and images using contrastive learning. We opt for ViT-G/14\u2019s text encoder structure due to our shared contrastive-based pre-training paradigm and mutual need for sequence modeling.\nThe output of the encoder is the patch-wise embeddings, denoted as . In summary, the encoder can be represented as a mapping function ."
        },
        {
            "section_id": "4.3.2",
            "parent_section_id": "4.3",
            "section_name": "4.3.2. Contrastive Loss",
            "text": "We leverage the hierarchical contrastive loss  introduced by TS2Vec (Yue et al., 2022  ###reference_b73###), which contrasts learned embeddings at different scales, thereby enhancing fine-grained representation learning for a variety of downstream tasks."
        },
        {
            "section_id": "4.3.3",
            "parent_section_id": "4.3",
            "section_name": "4.3.3. Pre-training paradigm",
            "text": "In the large-scale pre-training, the encoder  is trained in a wide range of time-series data from various domains, denoted as . However, training sequentially (i.e., ) may lead to catastrophic forgetting problem (Goodfellow et al., 2013  ###reference_b19###). To overcome this problem, we preprocess all the time-series data into batches, , where each batch from different domains contain varying length of time-series data. During training, we shuffle batches across all domains to improve the robustness and generality of the learned encoder.\nInstead of optimizing the augmentation module and encoder simultaneously, We train alternatively as described in Alg. 5  ###reference_###. Such approach can effectively tackle the complex optimization problems and avoid problems such as asynchronous optimization. Specifically, for each batch of data, we iteratively train the data augmentation module for  iterations, followed by training the encoder for  iterations with the augmentation module held fixed.\nThe time complexity of Alg. 5  ###reference_### is , and the results of parameter-sensitive experiments are presented in Sec. 5.4  ###reference_###. Empirically, employing small values for  and  has demonstrated both efficiency and effectiveness."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5. Experiments",
            "text": "means that we modify the official code (e.g., modify the input embedding layer) for fair comparison.\nWe evaluate our approach across a diverse set of time-series analysis tasks, as outlined in Sec. 2  ###reference_###, encompassing time-series forecasting and time-series classification. Our comparative study encompasses a comprehensive range of state-of-the-art models, including supervised models and foundational time-series models. Moreover, we conduct additional ablation analyses to assess the scalability and efficacy of the proposed data augmentation methods."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "5.1. Experiment Settings",
            "text": ""
        },
        {
            "section_id": "5.1.1",
            "parent_section_id": "5.1",
            "section_name": "5.1.1. Datasets",
            "text": "In our study, we assess the effectiveness of our model, UniCL, across a diverse array of datasets for various time-series analysis tasks. To ensure fair comparison, we adhere to the experimental settings established in TimesNet (Wu\net al., 2023  ###reference_b69###).\nFor time-series forecasting, we utilize seven well-established real-world datasets: four ETT datasets (Zhou et al., 2021  ###reference_b83###) (ETTh1, ETTh2, ETTm1, ETTm2), Weather111https://www.bgc-jena.mpg.de/wetter/  ###reference_###, Electricity222https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014  ###reference_ectricityLoadDiagrams20112014###, and ILI333https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html  ###reference_ldashboard.html###.\nFor time-series classification, we evaluate the performance across ten multivariate UEA classification datasets (Bagnall et al., 2018  ###reference_b3###)."
        },
        {
            "section_id": "5.1.2",
            "parent_section_id": "5.1",
            "section_name": "5.1.2. Baselines",
            "text": "We thoroughly compare our approach against a wide-range of time-series models, including the followings:\n(1) Transformer-based supervised models: ETSformer (Woo\net al., 2022b  ###reference_b68###), FEDformer (Zhou\net al., 2022  ###reference_b84###), PatchTST (Nie\net al., 2023  ###reference_b44###), Autoformer (Wu\net al., 2021  ###reference_b70###), Informer, Non-stationary Transformer (Liu\net al., 2022  ###reference_b39###), and Flowformer (Huang et al., 2022  ###reference_b21###) ;\n(2) Other supervised models: TimesNet (Wu\net al., 2023  ###reference_b69###), LightTS (Zhang et al., 2022a  ###reference_b78###), DLinear (Zeng\net al., 2023  ###reference_b74###), TCN (Franceschi\net al., 2019  ###reference_b16###), XGBoost (Chen and Guestrin, 2016  ###reference_b6###), LSTNet (Lai\net al., 2018  ###reference_b28###), and Rocket (Dempster\net al., 2020  ###reference_b11###) ;\n(3) LLM-based time-series foundation models: Time-LLM (Jin et al., 2024  ###reference_b23###), GPT4TS (Tian Zhou, 2023  ###reference_b60###), UniTime (Liu et al., 2023  ###reference_b37###), and LLaTA (Liu et al., 2024a  ###reference_b35###)."
        },
        {
            "section_id": "5.1.3",
            "parent_section_id": "5.1",
            "section_name": "5.1.3. Implementation details",
            "text": "With the aid of the scalable algorithm, UniCL undergoes initial pre-training on 40 cross-domain datasets sourced from the Monash Time Series Forecasting Repository (Godahewa et al., 2021  ###reference_b18###). These datasets encompass data with varying lengths, ranging from  to , and include instances with missing values, with missing rate ranging from  to . It should be notice that the training data in the experiments is not involved.\nWe pre-train our model on four NVIDIA A800 GPU for one week.\nSpecifically, the ViT-G/14 Clip encoder utilizes a layer count of , with attention blocks fine-tuned using LoRA (Hu et al., 2021  ###reference_b20###). Other parameters are also fine-tuned during pretraining. The fixed window size  of the scalable algorithm is set to , while  and  in Alg.5  ###reference_### are configured as  and  respectively. The values of  and  in Alg. 4  ###reference_### are chosen as  and  respectively, and  in Eq. 17  ###reference_### is set to . Both the augmentation module and the encoder are trained using the AdamW optimizer with an initial learning rate of .\nAfter pre-training, we integrate our trained encoder into the existing pipeline444https://github.com/thuml/Time-Series-Library  ###reference_ry### for evaluation. We also adhere to the same experimental settings as in Wu et al. (Wu\net al., 2023  ###reference_b69###) for all tasks."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "5.2. Main Experiments",
            "text": "###figure_7###"
        },
        {
            "section_id": "5.2.1",
            "parent_section_id": "5.2",
            "section_name": "5.2.1. Time-series forecasting",
            "text": "Time-series forecasting tasks involve prediction horizons ranging from  to , with evaluation metrics including mean square error (MSE) and mean absolute error (MAE).\nTab. 3  ###reference_### presents a comprehensive overview of the experimental findings.\nWe utilize two vertical lines to demarcate the table. The right part of the table represents state-of-the-art supervised learning models, while the left part pertains to LLM-based time-series foundation models that leverage natural language knowledge for time-series analysis.\nUniCL exhibits superior performance across two evaluation criteria, surpassing all LLM-based time-series foundation models. Notably, UniCL outperforms the state-of-the-art LLM-based model GPT4TS, resulting in a notable  average reduction in MSE and a  average reduction in MAE.\nUniCL demonstrates competitive results when compared to the supervised models PatchTST, significantly bridging the gap between foundation models and supervised models.\n###figure_8### ###figure_9### ###figure_10### ###figure_11###"
        },
        {
            "section_id": "5.2.2",
            "parent_section_id": "5.2",
            "section_name": "5.2.2. Time-series classification",
            "text": "As depicted in Fig. 4  ###reference_###, UniCL attains an average accuracy of , surpassing all baseline methods across 10 multivariate UEA datasets. Notably, UniCL outperforms the state-of-the-art forecasting models TimeLLM and LLaTA by approximately , while also surpassing the previous state-of-the-art transformer-based models PatchTST by . The superior performance of UniCL may be attributed to the good generality of contrastive learning, which effectively discriminates between diverse and unseen positive and negative views."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "5.3. Ablation Study",
            "text": ""
        },
        {
            "section_id": "5.3.1",
            "parent_section_id": "5.3",
            "section_name": "5.3.1. Ablation on the losses and unified operations",
            "text": "To better understand the effectiveness of the data augmentation module designs in UniCL, a comparison between full UniCL and its 8 variants on 128 UCR datasets (Dau et al., 2019  ###reference_b10###) is shown in Tab. 4  ###reference_###, where 1) w/o Spectrum-Preservation Loss removes the spectrum-preservation loss; 2) w/o Spectrum-Diversity Loss removes the spectrum-diversity loss; 3) Unified operation : replaces our proposed unified and learnable operation into 6 different pre-defined augmentation strategies, including jittering, scaling, permutation, masking, pooling, and warping.\nWe observe a significant drop in performance when removing the Spectrum-Preservation Loss, suggesting that by learning spectrum-preserved patterns, the model can effectively capture important patterns within the time-series data.\nThe absence of Spectrum-Diversity Loss resulted in a notable decrease in accuracy, amounting to . This underscores the importance of diverse positive views for ensuring the discriminative power of learned embeddings.\nFurthermore, decreased performance is observed across all augmentation-based variants, with reductions ranging from  (masking) to  (scaling). The relatively minor decrease in performance observed with jittering and masking may be attributed to their ability to generate spectrum-preserved and diverse samples.\nIn summary, the full UniCL leads to the best performance."
        },
        {
            "section_id": "5.3.2",
            "parent_section_id": "5.3",
            "section_name": "5.3.2. Ablation on the scalable algorithm",
            "text": "To better understand the role of the scalable algorithm designs in UniCL, we evaluate the efficiency of the scalable operations and non-scalable operations, as illustrated in Fig. 6  ###reference_###(a). The experiment is conducted on synthetic time-series data with varying lengths, and the time cost per epoch and final average is recorded.\nWe observe that beyond a length of , the time cost of non-scalable operations increases exponentially, whereas the time cost of scalable operations remains linear with the length of the input. This is due to the design of the scalable algorithm.\nTo demonstrate the effectiveness of the scalable algorithm, the corresponding convergence losses of Fig. 6  ###reference_###(a) are illustrated in Fig. 6  ###reference_###(b). Despite the significantly lower time consumption of scalable operations compared to non-scalable ones, we observe that the difference in convergence loss between scalable and non-scalable operations is bounded."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "5.4. Parameter Sensitivity",
            "text": "We evaluate the performance of the UniCL based on different parameters:  in Eq. 17  ###reference_###,  in Alg. 4  ###reference_###, and  in Alg. 5  ###reference_###.\nThe experiment is conducted on two anomaly detection datasets: Yahoo (Nikolay Laptev, 2015  ###reference_b45###) and KPI (Ren et al., 2019  ###reference_b55###), following an evaluation protocol (Ren et al., 2019  ###reference_b55###).\nAs demonstrated in Fig. 5  ###reference_###(a), the\nF1 metric drops with  larger than , suggesting that a wise selection of  should be less than , which is consistent with Lemma 4.1  ###reference_theorem1###.\nAccording to Fig. 5  ###reference_###(b,c), the wise selection for  and  falls within the range of  to  and  to , respectively, underscoring the importance of the spectrum-preserved property over the spectrum-diverse property.\nFurthermore, the choice of  holds critical importance in alternative training, with the results indicating that  should fall within the range of  to . A smaller value of  may result in underfitting of the augmentation module.\n###figure_12### ###figure_13###"
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6. Conclusion",
            "text": "In this paper, we propose UniCL, a universal contrastive learning framework for pre-training time series models across diverse domains, which addresses the limitations of high bias and low generality in existing models.\nFirst, we propose a novel augmentation operation that preserves spectral properties, ensuring diversity and reduced bias in augmented time-series data.\nSecond, we propose a scalable augmentation algorithm to time series data with high variability in terms of domain, sequence length, and variable number, thus facilitating robust cross-domain pre-training.\nThe superior effectiveness of our\nproposed UniCL is demonstrated by extensive experiments\non two benchmarks across eleven domains."
        }
    ],
    "url": "http://arxiv.org/html/2405.10597v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2",
            "2.2.1",
            "2.2.2",
            "2.2.3"
        ],
        "methodology_sections": [
            "3",
            "4",
            "4.1",
            "4.1.1",
            "4.1.2",
            "4.2",
            "4.2.1",
            "4.2.2",
            "4.2.3",
            "4.3",
            "4.3.1",
            "4.3.2",
            "4.3.3"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.1",
            "5.1.1",
            "5.1.2",
            "5.1.3",
            "5.2",
            "5.2.1",
            "5.2.2",
            "5.3",
            "5.3.1",
            "5.3.2",
            "5.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "5.3",
            "5.3.1",
            "5.3.2"
        ]
    },
    "research_context": {
        "paper_id": "2405.10597v1",
        "paper_title": "UniCL: A Universal Contrastive Learning Framework for Large Time Series Models",
        "research_background": "**Motivation:**\n\nThe paper aims to address the pressing challenges in analyzing time series data, which is critical in fields such as finance, healthcare, environmental monitoring, and manufacturing. The inherent properties of time series data, such as high dimensionality, noise, non-stationarity, and periodicity, make it difficult to capture underlying patterns using existing methods. Traditional supervised learning (SL) approaches require large amounts of annotated data, which is often impractical. Meanwhile, Pretrained Foundation Model (PFM)-based approaches, though beneficial, demand extensive unlabeled data and may yield suboptimal performance due to data scarcity. Recent contrastive learning (CL) methods, while promising, struggle with high-bias and low-generality issues, limiting their effectiveness.\n\n**Research Problem:**\n\nThe paper addresses the need for a universal pre-trained foundation model for time-series analysis that can overcome the high-bias and low-generality issues found in existing contrastive learning-based approaches. Specifically, the paper seeks to design a framework that can effectively pre-train time-series models across diverse domains using novel augmentation techniques and contrastive learning objectives while tackling technical challenges related to theoretical analysis, metric development for preserving intrinsic patterns, and scalable augmentation algorithms.\n\n**Relevant Prior Work:**\n\n1. **SL-based Approaches:**\n   - *Transformers and CNNs*: Notable works include those by Wu et al. (2021), Liu et al. (2021), and Zhou et al. (2022) that have used transformer and CNN-based models for time series.\n   - *Annotation Issues*: Challenges of requiring large annotated datasets have been discussed by Khaertdinov et al. (2021).\n\n2. **PFM-based Approaches:**\n   - *Mask-based Pre-training*: Methods have been explored by researchers like Dong et al. (2024) and Wen et al. (2022b), focusing on predictive objectives, such as predicting next values or reconstructing masked values in time series.\n   - *Scarcity of Data*: Emphasized by Ansari et al. (2024) and Liu et al. (2024c), demonstrating that foundation models need vast amounts of data.\n\n3. **Contrastive Learning (CL) Approaches:**\n   - *CL Methods*: Recent approaches by Yeh et al. (2023) and Eldele et al. (2021) involve augmenting data and pre-training models to distinguish between different time series patterns.\n   - *Issues with Existing CL Approaches*: High bias introduced by predefined augmentation operations (Meng et al., 2023; Yue et al., 2022) and lack of generality due to domain-specific training (Woo et al., 2022a; Wang et al., 2024).\n\n4. **Challenges in Time Series Analysis:**\n   - *Intrinsic Properties*: Complexity in preserving intrinsic properties like periodicity and handling high variations across domains is highlighted in works by Qiu et al. (2024) and Ma et al. (2023).\n\nBy addressing these gaps and building upon the limitations identified in previous research, this paper proposes the UniCL framework to provide a more robust and generalizable solution for time-series analysis.",
        "methodology": "**UniCL: A Universal Contrastive Learning Framework for Large Time Series Models**\n\n**Methodology:**  \nIn this study, we introduce UniCL, a universal contrastive learning framework designed for time-series analysis. UniCL is fundamentally general and effective, capable of handling heterogeneous cross-domain time-series data with varying input lengths. It is based on a unified augmentation operation that generates diverse positive samples, thereby facilitating robust contrastive learning.\n\nPrimarily, there are four steps: (1) **data generation**, (2) **unified and scalable data augmentation module**, (3) **time-series encoder based on LLMs**, and (4) **embedding contrast**. Here, we briefly clarify these steps:\n\n1. **Data Generation:**  \n    To begin with, the time-series datasets from diverse domains are initially partitioned into batches, shuffled, and then randomly fed into the augmentation module.\n\n2. **Unified and Scalable Data Augmentation Module:**  \n    For each batch, the proposed scalable algorithm with bounded convergence loss can deal with varying lengths of inputs with missing values. The unified and learnable augmentation operation is employed to generate diverse and pattern-preserved positive views for contrastive learning.\n\n3. **Time-series Encoder Based on LLMs:**  \n    The CLIP-based encoder generates embeddings for all views, effectively capturing cross-domain and general time-series patterns.\n\n4. **Embedding Contrast:**  \n    A typical contrastive loss can be employed to enhance the discriminative power of the learned embeddings.\n\nThrough these steps, UniCL effectively leverages contrastive learning to handle complex and diverse time-series data, providing a robust framework suitable for various time-series analysis tasks.",
        "main_experiment_and_results": "**Main Experiment Setup and Results:**\n\n**Tasks and Datasets:**\n- The main experiment evaluates the proposed method on a variety of time-series analysis tasks, specifically time-series forecasting and time-series classification. Details on the exact datasets are likely outlined in Section 2 of the referenced paper.\n\n**Baseline Models:**\n- The comparative study includes a comprehensive range of state-of-the-art models. This includes both supervised models and foundational time-series models. Examples of these could be Transformer-based models, recurrent neural networks, and traditional autoregressive models, although the exact list is detailed in the referenced section.\n\n**Modifications for Fair Comparison:**\n- For fair comparison, the official code of baseline models is modified. This includes changes to the input embedding layer to ensure a standardized comparison across different models.\n\n**Evaluation Metrics:**\n- The specific evaluation metrics are not detailed in the provided summary, but common choices for time-series tasks include Mean Absolute Error (MAE), Mean Squared Error (MSE) for forecasting tasks, and accuracy, F1-score, and Area Under the Receiver Operating Characteristic Curve (AUC-ROC) for classification tasks.\n\n**Main Experimental Results:**\n- The main experiment results demonstrate the performance of the proposed UniCL framework in comparison to the baseline models across the evaluated tasks. Detailed metrics are discussed in the context of the referenced section, highlighting the improvements or differences observed.\n\nIn summary, the main experiment setup involves evaluating the UniCL framework on diverse time-series datasets for forecasting and classification, comparing its performance against a range of state-of-the-art models while ensuring fair comparison by standardizing the input embedding process. The evaluation metrics likely include common error and accuracy measures relevant to the tasks, and the results underscore the effectiveness of UniCL in enhancing time-series analysis performance."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To understand the effectiveness of the data augmentation module designs in UniCL.",
            "experiment_process": "A comparison between full UniCL and its 8 variants was conducted on 128 UCR datasets. The variants included: 1) w/o Spectrum-Preservation Loss, which removes the spectrum-preservation loss; 2) w/o Spectrum-Diversity Loss, which removes the spectrum-diversity loss; and 3) Unified operation, where our proposed unified and learnable operation is replaced with six different pre-defined augmentation strategies (jittering, scaling, permutation, masking, pooling, and warping).",
            "result_discussion": "Significant performance drops were observed when the Spectrum-Preservation Loss and Spectrum-Diversity Loss were removed, highlighting their importance. The absence of Spectrum-Diversity Loss notably decreased accuracy. Across augmentation-based variants, performance decreases varied from moderate (masking) to severe (scaling). Full UniCL demonstrated the best performance.",
            "ablation_id": "2405.10597v1.No1"
        },
        {
            "research_objective": "To understand the role of the scalable algorithm designs in UniCL.",
            "experiment_process": "The experiment was conducted on synthetic time-series data with varying lengths, evaluating the efficiency of scalable versus non-scalable operations. The time cost per epoch and the final average were recorded. Additionally, convergence losses were analyzed.",
            "result_discussion": "For data lengths beyond a certain point, non-scalable operation costs increased exponentially, while scalable operations remained linear. Despite lower time consumption for scalable operations, the difference in convergence loss between scalable and non-scalable operations was bounded.",
            "ablation_id": "2405.10597v1.No2"
        }
    ]
}