{
    "title": "Mixture-of-LoRAs: An Efficient Multitask Tuning for Large Language Models",
    "abstract": "Instruction Tuning has the potential to stimulate or enhance specific capabilities of large language models (LLMs). However, achieving the right balance of data is crucial to prevent catastrophic forgetting and interference between tasks. To address these limitations and enhance training flexibility, we propose the Mixture-of-LoRAs (MoA) architecture \u2013 a novel and parameter-efficient tuning method designed for multi-task learning with LLMs.\nIn this paper, we start by individually training multiple domain-specific LoRA modules using corresponding supervised corpus data. These LoRA modules can be aligned with the expert design principles seen in Mixture-of-Experts (MoE). Subsequently, we combine the LoRAs using an explicit routing strategy and introduce domain labels to facilitate multi-task learning, which helps prevent interference between tasks and ultimately enhances the performance of each individual task. Furthermore, each LoRA model can be iteratively adapted to new domains, allowing for quick domain-specific adaptation. Experiments on diverse tasks demonstrate superior and robust performance of our approach, which will also further promote the application of domain-specific LLMs.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "###figure_1### Large language models (LLMs) have played a pivotal role in expediting the advancement of natural language processing (NLP), offering a versatile and task-agnostic foundation that underpins an extensive array of applications. The intrinsic diversity found in domain-specific data poses a substantial challenge in training a general-purpose base LLMs. Consequently, there has been a surge in the adoption of domain-specific LLMs to tackle intricate problems within specialized domains, such as SQL-PaLM (Sun et al., 2023  ###reference_b20###), BloombergGPT (Wu et al., 2023  ###reference_b22###), ChatLaw (Cui et al., 2023  ###reference_b2###), pdfGPT (Tripathi, 2023  ###reference_b21###). In real-world application scenarios, the demand often arises for a multitude of customized capabilities.\nLLMs with multiple customized capabilities can efficiently address a diverse range of user problems, and each specific functional module can be optimized individually.\nDomain specification techniques are key to make large language models disruptive in various applications (Zhao et al., 2023  ###reference_b26###). To learn sufficient domain knowledge and not lose basic capability, adapter-based fine-tuning methods (e.g., Adapters (Houlsby et al., 2019  ###reference_b6###), LoRA (Hu et al., 2021  ###reference_b7###)) introduce a limited number of domain-specific parameters to retain domain-related knowledge and do not need to fine-tuning all parameters of the pre-trained model, which can effectively reduce the training cost of LLMs.\nIn order to obtain multiple customized capabilities, the simplest and efficient fine-tuning approach is to directly mix data from multiple domains together and only add one LoRA module for instruction fine-tuning. The second is some two-stage approaches.\nWe first train multiple domain LoRA modules individually, and then introduce a domain classifier to select appropriate LoRA model. In addition, Pfeiffer et al. (2020  ###reference_b17###); Huang et al. (2023  ###reference_b8###) add the AdapterFusion layer and element-wise LoRA composition to implicitly fuse parameter knowledge of multiple task adapters. The MoE (Shen et al., 2023  ###reference_b19###) introduces multiple experts to process different types of input data. Jiang et al. (2023  ###reference_b9###) is to consider the outputs of ensemble LLMs comprehensively.\nHowever, these approaches have two main issues. First, in the practical application scenario, there are often multiple destructive domain tasks with heterogeneous and imbalance training data, as well as limited computing resources. Therefore, the implicit parameter fusion methods exist mutual disturbance, which results in degraded model performance on in-domain tasks, as illustrated in Figure 1  ###reference_###. We often focus more on the domain-specific expertise of a domain-specific LLM than its generalization performance.\nIn addition, the MoE mechanism needs to be trained from scratch based on a new model structure and a large amount of training corpus. The ensemble LLMs require sufficient computing resources to deploy multiple independent LLMs simultaneously.\nIn this paper, we address these limitations and propose an end-to-end parameter-efficient tuning method designed for multi-task learning on LLMs, dubbed MoA. First, we design a routing mechanism within decoder-only model architecture to automatically select LoRA experts, which can be applied to current mainstream LLMs, and can simultaneously deploy LoRA modules of multiple tasks with limited computing resources (using the same LLM). Meanwhile, our comprehensive model can still achieve excellent performance on different types of tasks.\nAdditionally, to improve the efficiency of training and inference, we implement a parallel processing strategy of different domain samples within a batch during the training process, and a LoRA expert selection approach in the inference time.\nOur approach leverages the power of different expert models and the base LLM, and the complementarity of knowledge in different domains.\nIn summary, our contributions are as follows:\nWe propose a MoA architecture for efficient multitask fine-tuning, which can avoid the interference and data imbalance between heterogeneous tasks and easily perform iterative optimization of single task.\nWe implement an explicit routing strategy in the training process, which can leverage the knowledge complementarity to further improve the single task performance and ensures the inference efficiency.\nExtensive experiments on various benchmarks are conducted to verify the effectiveness of our approach. Meanwhile, it is flexible to combine multiple domain-specific LoRAs to form a comprehensive LLM."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Domain Specialization of LLMs. The approaches in LLM domain specialization can be categorized into two corresponding classes of approaches: external augmentation, prompt crafting, and model fine-tuning (Zhao et al., 2023  ###reference_b26###). We focus on the third method, which involves updating the LLM\u2019s parameters to incorporate domain-specific knowledge directly into the model. Because the current LLMs have billions of parameters and the phenomenon of catastrophic forgetting, we use adapter-based fine-tuning (e.g, Adapter, LoRA) to train multiple domain experts in advance on different task data. LoRA is a parameter-efficient fine-tuning method, which facilitates the adaptation of LLMs using a small-scale external module. As such, LoRA tuning presents a resource-efficient technique to quickly adapt LLMs for novel tasks with restricted training data.\nMixture-of-Experts. The Mixture of Experts (MoE) is an ensemble method, often visualized as a collection of sub-modules, or \u2019experts\u2019, each specializing in processing different types of input data. Each expert is controlled by a router that is then selectively activated based on the type of input data. This technique achieves excellent performance in other domains, including computer vision, speech recognition and multi-modal applications (Fedus et al., 2022a  ###reference_b3###). Inspired by the idea of MoE, we regard each task\u2019s LoRA module as a domain expert. Meanwhile, we introduce a routing algorithm to make different domain data automatically choose respective expert, and hence experts in different domains can be combined into a comprehensive model. Furthermore, Shen et al. (2023  ###reference_b19###) also demonstrates that the combination of MoE and instruction tuning can improve task-specific performance.\nMulti-Task Composition.\nThe methods of two-stage learning or end-to-end multi-task learning are commonly used to obtain the combination of multi-task capabilities.\nThe two-stage method requires maintaining a specialized routing model to serve multiple task models, whose overall performance is limited to a single LoRA model. The end-to-end methods introduce new parameter layers or perform implicit parameter fusion. Specifically, Pfeiffer et al. (2020  ###reference_b17###) trains a fusion parameter layer to compose the information stored in the multiple task adapters. Huang et al. (2023  ###reference_b8###) uses a set of learnable weights to integrate multiple LoRA modules into a unified module. Although they can obtain some generalization ability through the combination of parameters trained on various tasks, these fusion methods result in performance degradation on the original task. Zhao et al. (2023  ###reference_b26###) also shows that it is difficult to effectively learn all specialized knowledge of different domains in one LLM."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "###figure_2### The proposed Mixture-of-LoRAs (MoA) can be observed in Figure 2  ###reference_###. Given the variations in data scales and training complexities, we commence by separately training N LoRA modules  for N distinct task types . It is worth noting that the task types mentioned here and the domain-specific data or tasks referred to in the paper have the same meaning, all representing scenario tasks that require certain expertise to solve. Initially, we obtain the optimal LoRA module parameters on each scenario task data.\nThese modules demonstrate commendable performance within their respective domains. Subsequently, a routing mechanism is employed to integrate the N LoRA modules under a shared LLM. Specifically, multiple LoRA modules are simultaneously incorporated alongside each transformer layer. Preceding each transformer layer, a routing mechanism is inserted to guide the selection of distinct LoRA experts."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Learning algorithm",
            "text": "In the first stage, we train a LoRA module for each of the N tasks, which mitigates the problem of catastrophic forgetting of base LLM. Different tasks often have different data scales and training difficulties. Hence, each LoRA module needs to be optimized individually. These LoRA modules can be arbitrarily combined, added, or removed after initial training. This adapter schemes could enable more fine-grained control over which parts of the LLM are domain-specific.\nIn the second stage, the N LoRA modules can be aligned with the MoE\u2019s expert design. We combine the set of N LoRAs using a routing strategy. While keeping the base LLM parameters  fixed, we introduce router parameters  that learn to select the appropriate expert for users\u2019 target tasks (as shown in Figure 2  ###reference_###). The trainable router and LoRA parameters are combined to jointly optimize the autoregressive language modeling tasks. The training data for this stage is evenly sampled from the original data of each task, obviating the need to acquire new, high-quality supervised data. The final loss of the MoA is the summation of the language modeling loss and the MoE routing loss:\nHere,  is a parameter that controls the weight of the routing loss and the convergence rate,  is the cross-entropy loss of expert classification, and  is defined as follows:\nIn this context, the language modeling task (LM) is the commonly used objective for pretraining decoder-only LLMs (e.g., GPT-3 (Brown et al., 2020  ###reference_b1###)). Here,  represents a sequence of tokens . The LM task involves predicting the target tokens  autoregressively, based on the preceding tokens  within the sequence.\nBy dividing the process into two stages: (1) training domain experts in LoRAs, and (2) combining diverse capabilities through a routing strategy, we effectively address concerns such as catastrophic forgetting, task interference, and instability during multi-task training."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Routing Strategy",
            "text": "Prior approaches (Fedus et al., 2022b  ###reference_b4###; Lepikhin et al., 2020  ###reference_b10###) on routing strategy have typically focused on learning token-level weighting functions, often assigning one or two experts per token. This approach necessitates careful load balancing to ensure utilization of all experts, prompting the exploration of explicit balancing mechanisms (Lewis et al., 2021  ###reference_b11###).\nIn contrast, we adopt a sequence-level routing strategy that leverages domain metadata to route data to LoRA experts. While all token sequences traverse the weight matrices in LLM\u2019s transformer layers, during training, each transformer layer employs a distinct router to assign training data labeled with its corresponding domain. The final routing loss enforces precise data-to-expert assignments for each data type. After training with a modest quantity of balanced data, a robust router is obtained. Meanwhile, multi-task training enhances the generalization of the original tasks.\nThroughout the training process, language modeling and router classification tasks complement each other. However, we enhance expert selection during inference by employing techniques such as voting or selecting the last expert (as detailed in  4.4  ###reference_###). This optimization aims to enhance generation efficiency and contextual consistency."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "MoA Architecture",
            "text": "We design an LoRA expert explicitly for each domain (i.e., eight experts for eight training domains in our multi-domain corpus). LoRA updates weights using the formula:\nwhere , ,  and .\n denotes the attention and feed-forward weight matrices of the base LLM, whose parameters are fixed. The\nparameter of router( ) is trainable.  is the parameter of the router, which is implemented by a linear layer. The  defines the LoRA module , which is repeated multiple times within each transformer layer to reduce trainable parameters for adapting to different domain tasks. The multiple  experts are placed in parallel alongside , departing from previous methods (Pfeiffer et al., 2020  ###reference_b17###; Gururangan et al., 2022  ###reference_b5###; Huang et al., 2023  ###reference_b8###) that add shared fusion layers or replace dense feedforward layers with multiple experts\u2019 feedforward networks. The routing algorithm is a key feature in all sparse expert architectures. We adopt an intuitive method to assign different experts to handle tasks in different domains, which helps avoid interference between tasks. Each transformer layer adds a router to select the most appropriate expert. Each router is implemented as a two-layer MLP. This implementation is simple and doesn\u2019t significantly increase the number of training parameters."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experimental Setup",
            "text": "Datasets. To evaluate the effectiveness of MoA, we first conduct experiments on various supervised fine-tuning (SFT) datasets of heterogeneous domains. Finance, Medicine and Leetcode belong to the specialized domain dataset. Exam, Webgpt and Gpt4tools limit the output format of the LLM and allow the model to learn special functions. Other datasets include chain-of-thought, dialog, etc. Meanwhile, both English and Chinese are involved. We show the statistics of all datasets in Table 1  ###reference_###.\nImplementation Details. We use the Qwen-7b as our base LLM, which is the decoder-only architecture. We use the vocabulary of 151,851 BPE types, and train with 4,096-token sequences. We set the total number of training steps based on this allocated runtime, set 10% of these steps to be warmup, and use the AdamW (Loshchilov and Hutter, 2017  ###reference_b15###) optimizer with a cosine learning rate decay. Learning rate is set to . Each worker processes eight sequences of length 4,096, and gradients are accumulated over 4 updates. We clip gradients if their L2 norm exceeds 0.1. In the inference time, we report test perplexity after a single run of training on 8 NVIDIA A100 80GB GPUs."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Models and Metrics",
            "text": "Single-LoRA The first baseline is a LoRA trained on data within the domain, which could be viewed as a specialized model for handling domain tasks.\nSingle-LoRA (mixed) We train a single LoRA on mixed data from all domains. While there is no explicit conditioning on domain, the gradient updates during training average across all domains represented in a batch.\nMoA We add multiple domain LoRAs in the transformer based on the first baselines, and use routing strategy and domain label information for multitask training. Under the routing setting, the test data domain is unknown.\nMoE-LoRA We leverage the idea of sparsely activated Mixture-of-Experts (MoE). We add multiple domain LoRA modules on the bypass of transformer layer. Each input token will be processed by a limited subset of experts. Different from our MoA, this approach can add any number of experts and has no concept of data domain.\nMoE-LoRA (naive) The architecture of this model is exactly the same as MoE-LoRA, and the only difference is that we randomly initialize all LoRA modules.\nTo comprehensively evaluate the performance of different models, we use several evaluation metrics, including the perplexity (PPL) of generated texts, and the bilingual evaluation understudy (BLUE) and the longest common subsequence (ROUGE-L) between the generated answer and the gold answer."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Main results",
            "text": "Classifier+LoRAs The most intuitive method of integrating multiple LoRA experts is to use a specific classifier to act as a distributor, as shown in Figure 3  ###reference_###. We train a classifier based on roberta-base (Liu et al., 2020  ###reference_b13###) using the same training data as MoA. The specific classification performance is shown in Table 3  ###reference_###. This approach is so flexible that we can combine multiple LoRA modules and avoid interference between tasks. However, the performance of this approach is limited to each LoRA expert. From the classification results, our router performs better than the classifier overall.\n###figure_3### ###table_1### ###table_2### End-to-end methods The end-to-end methods mean that one model can directly solve multiple tasks even if the test data domain is unknown. Table 2  ###reference_### shows that test perplexities, blue-4 and rouge-l, averaged across the eight training domains. Training in the mixed domain data is helpful for the overall performance (Perplexity: 4.01283.9450, BLUE: 28.534830.5912, ROUGE-L: 37.787739.0163). However, the performance decreases on data with strict output formats such as Webgpt, Stackoverflow, which also shows that not all additional domain information is complementary. We hypothesize that separate training is advantageous for heterogeneous domains. Therefore, we design an efficient multi-task learning method to avoid interference between partial tasks. Our approach achieves significant improvements across all datasets.\n###table_3### In order to further validate the reliability of the MoA\u2019s performance, we conducts accuracy evaluation experiments on datasets with standard answers. The Exam dataset is utilized for the experiments, as it primarily consists of secondary and university entrance exam questions, with the majority being multiple-choice questions (including single and multiple selections) and a small portion of true/false questions. Accuracy is calculated by directly comparing the answers with the reference answers. Specifically, string processing functions, regular expressions, etc., are used to parse out specific options (A/B/C/D) or judgment results (T/F) from the answers, followed by calculating the accuracy of responses. The final results are presented in Table 5  ###reference_###. Despite the overall low accuracy due to the difficulty of the questions, the accuracy of MoA is significantly higher than the other two models (+5.86%, +5.48%).\n###table_4### In addition, this subsection also introduces GPT-4 as an evaluation expert to assess Finance, Medicine, and Webgpt datasets. As the answers in these datasets do not follow a fixed pattern, this subsection adopts the common evaluation method in the community of LLMs. Evaluation scoring is conducted through a larger model. In this study, GPT-4 is utilized to provide accuracy scores for the model\u2019s responses to questions and standard answers. The complete evaluation prompt is illustrated in Appendix A  ###reference_###.\nApart from the three models: Single-LoRA, Single-LoRA (mixed), and MoA, we also evaluates the individual LoRA modules within the MoA model. These are represented in the table 6  ###reference_### as Single-LoRA of MoA.\nDue to fluctuations in scoring by the large language model, each scoring process is invoked three times and the average is taken. From the experimental results, it is observed that after multi-task learning training within MoA, the performance of each LoRA module surpasses the original Single-LoRA modules in each task. When handling specific professional tasks individually, these modules exhibit outstanding performance. Therefore, through multi-task learning methods, the performance of multiple LoRA modules is further enhanced while effectively collaborating.\n###table_5### Based on the metrics such as PPL, BLUE, ROUGE, and accuracy metrics for specific downstream tasks presented in the paper, we can conclude that our proposed simple and efficient architecture effectively enables learning of various domain-specific competencies within a single large language model, while also avoiding interference between different tasks. Furthermore, each functional module is relatively independent, facilitating efficient consolidation of additional data for further optimization. Additionally, this method significantly saves computational resources during deployment."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Mixing LoRA Experts at Inference Time",
            "text": "The previous section establishes that our multi-task training method improves the performance of the single LoRA expert on test data. In addition, the mixing of multi-domain data is effective in the training process. In practice, however, the original training data scale of the respective tasks is relatively large and textual data to be evaluated may not come with a domain label.\nIn these cases, the mixed training of a large amount of data is unfavorable for our later iterations of single tasks. We propose to treat  as domain experts, transforming the input text into a matter of expert selection. The routing strategy is introducing to solve the problem of interference between tasks. The parameter of router is shown in Table 4  ###reference_###, which only accounts for a very small percentage. Our approach is parameter-efficient and selective complementarity between tasks. The results of evaluation on the test data of unknown domain are shown in Table 2  ###reference_###.\n###figure_4### To further explore the actual effect of the routing module, we test its selection ability in the inference process. The number of our routers is consistent with the number of transformer layers in our LLM. Each router is implemented with a linear layer. Therefore, to improve the efficiency of the inference process and the consistency of generation, we choose to use (1) voting of all routers and (2) the result of last router as the selected LoRA module in the subsequent inference process. The experimental results show that the classification performance of the last router is more stable. Finally, we only use 1.05M routing parameters to achieve an average accuracy of 99.90% in Table 3  ###reference_###. Overall, the router plays two key roles in our model architecture. One is to learn the complementarity of non-heterogeneous domain knowledge and in the training process. The other is to select appropriate LoRA expert to solve problems in unknown domains in the inference process."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Ablation Studies",
            "text": "We further conduct specific experiments to investigate the effectiveness of different components in our model.\nImpact of Domain Label Information In addition to the language modeling loss, we also add the domain label routing loss in the training process. To verify the effectiveness of our proposed mechanism, we remove the routing loss and our model devolved into MoE-LoRA. Therefore, the MoE-LoRA does not introduce explicit domain label information in the training and inference process and guarantees the same number of parameters as MoA. As shown in Table 7  ###reference_###, MoA has achieved an overall improvement over MoE-LoRA, which demonstrates that the domain label information is useful for different tasks. Furthermore, we evaluate the performance of single LoRA module after multi-LoRA joint training in Table 8  ###reference_###. The MoA further improves the perplexity performance of each LoRA module from 4.0128 to 3.7962, while MoE-LoRA causes a slight decrease. Therefore,\nwhen we need to expand to more tasks or combine multiple functions, MoA is more flexible and effective.\n###table_6### Impact of Different Initialization Methods To evaluate the effectiveness of various LoRA initialization methods, we present the MoE-LoRA (naive) approach. In contrast to MoE-LoRA, the LoRA modules in the naive variant are initialized randomly. This approach leverages the complete dataset from all domains, leading to a significant increase in total training duration. However, this approach proves to be highly inefficient when we require customization of different capability combinations. Moreover, the comparative analysis presented in Table 7  ###reference_### demonstrates that both methods yield comparable results across eight tasks. Therefore, conducting further training on the initial multi-domain LoRA parameters emerges as a highly efficient method."
        },
        {
            "section_id": "4.6",
            "parent_section_id": "4",
            "section_name": "Case Study",
            "text": "Figure 4  ###reference_### shows the comparison of the outputs of different models on a specific reasoning question.\nFrom the above results, only the MoA correctly understands the multiple relationships in the query and predicts the right answer, while other approaches are slightly less capable of reasoning. Furthermore, the MoA is superior to the single-LoRA, which also illustrates the advantages of our multitask fine-tuning. This approach can not only avoid the interference between different tasks, but also further improve the performance on a single task, so it has a great application prospect."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusions",
            "text": "We introduce MoA architecture, which provide an efficient multi-task fine-tuning method for LLM, addressing interference among tasks and training instabilities. Each LoRA model can be iterated individually to quickly adapt to new domains. Meanwhile, MoA uses routing strategy to flexibly select the appropriate LoRA expert to solve the problem. It can arbitrarily combine multiple domain-specific LoRA modules to implement a LLM with multiple specific capabilities. Future work may focus on how to flexibility add or remove LoRA modules with unsupervised learning, optimize the current routing algorithm, or reduce the scale of training data in domain specialization of LLMs."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A Evaluation prompt",
            "text": "The complete prompt used for invoking the GPT-4 API is as follows:\neval_prompt = \"\"\" You will receive three parts of content: the questioner\u2019s question, the user\u2019s answer, and the reference answer.\nYour task is to score the accuracy of the user\u2019s answer based on the following criteria.\nPlease ensure that you read and understand these instructions carefully.\nEvaluation Criteria:\nAccuracy - Whether the user\u2019s answer is consistent with the reference answer and has addressed the questioner\u2019s question. We define this dimension as \u2019whether the user\u2019s answer includes all the key points from the reference answer and has addressed the questioner\u2019s question.\u2019\nEvaluation Steps:\n1. Carefully read the questioner\u2019s question, understand the key points of the question.\n2. Carefully read the reference answer, understand the key points related to the question contained in the reference answer.\n3. Check if the user\u2019s answer includes the key points from the reference answer and has addressed the questioner\u2019s question.\n4. Based on the evaluation criteria, score within a range of 0 to 100, where 0 means the user\u2019s answer does not contain any key points from the reference answer and has completely failed to address the questioner\u2019s question; 100 means the user\u2019s answer includes all the key points from the reference answer and has correctly and completely addressed the questioner\u2019s question. \nExample: Questioner\u2019s question: [query] User\u2019s answer: [llm_answer] Reference answer: [fact]\nEvaluation result (score only): \nAccuracy (0-100): \"\"\"\nwhere \"[query]\" represents user input, \"[llm_answer]\" denotes the output of the MoA model, and \"[fact]\" stands for the standard answer provided by the dataset."
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T1\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S3.T1.1\" style=\"width:603.5pt;height:154.8pt;vertical-align:-0.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-15.9pt,4.1pt) scale(0.95,0.95) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.1.1\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S3.T1.1.1.1.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.1.1\">Domain</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.1.1.1.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.2.1\">Source</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.1.1.1.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.3.1\">Language</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.1.1.1.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.4.1\"># Train (Eval.) Tokens</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S3.T1.1.1.2.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T1.1.1.2.1.1\">Finance</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.1.1.2.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Financial related instructions\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Qingyi\u00a0Si, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.03432v1#bib.bib18\" title=\"\">2023</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.1.1.2.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">EN</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.1.1.2.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.2M (0.24M)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.1.3.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T1.1.1.3.1.1\">Medicine</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.3.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">10k real conversations between patients and doctors\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.03432v1#bib.bib12\" title=\"\">2023</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.3.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">EN</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.3.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.4M (0.28M)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.1.4.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T1.1.1.4.1.1\">Leetcode</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.4.2\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T1.1.1.4.2.1\">Chinese Open Instruction Generalist\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.03432v1#bib.bib25\" title=\"\">2023</a>)</cite></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.4.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CN</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.4.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">9.3M (2.09M)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.1.5.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T1.1.1.5.1.1\">Exam</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.5.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CN</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.5.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3.6M (0.71M)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.1.6.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T1.1.1.6.1.1\">webgpt</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.6.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Retrieval question answering dataset\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Nakano et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.03432v1#bib.bib16\" title=\"\">2021</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.6.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">EN</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.6.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">7.4M (1.46M)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.1.7.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T1.1.1.7.1.1\">Gpt4tools</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.7.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">A collection of tool-related instructions\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.03432v1#bib.bib24\" title=\"\">2023</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.7.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">EN</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.7.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">7.5M (1.49M)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.1.8.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T1.1.1.8.1.1\">Cot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.8.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Several Chain-of-Thought datasets\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Longpre et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.03432v1#bib.bib14\" title=\"\">2023</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.8.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">EN</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.8.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.1M (0.22M)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T1.1.1.9.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T1.1.1.9.1.1\">Stackoverflow</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.1.1.9.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">57k dialogs from StackOverFlow questions\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.03432v1#bib.bib23\" title=\"\">2023</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.1.1.9.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">EN</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.1.1.9.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.9M (0.18M)</td>\n</tr>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Statistics of SFT datasets. # Train (Eval.) Tokens denotes the size of our training and evaluation (i.e. validation and test) data, obtained via the Qwen tokenizer. We evenly sample about 10k training data and 2k validation data on each dataset.</figcaption>\n</figure>",
            "capture": "Table 1: Statistics of SFT datasets. # Train (Eval.) Tokens denotes the size of our training and evaluation (i.e. validation and test) data, obtained via the Qwen tokenizer. We evenly sample about 10k training data and 2k validation data on each dataset."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T2\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T2.1\">\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.1.1.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.1.1.1.1\">Domain</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" id=\"S4.T2.1.1.2\">Single-LoRA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" id=\"S4.T2.1.1.3\">Single-LoRA (mixed)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" id=\"S4.T2.1.1.4\">MoA</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.2.1\">PPL</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.2.2\">BLUE</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.2.3\">ROUGE-L</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.2.4\">PPL</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.2.5\">BLUE</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.2.6\">ROUGE-L</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.2.7\">PPL</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.2.8\">BLUE</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.2.9\">ROUGE-L</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.3.1\">\n<span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T2.1.3.1.1\">Finance</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.3.2\">7.8479</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.3.3\">18.5975</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.3.4\">28.6266</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.3.5\">7.7214</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.3.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.3.6.1\">22.4846</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.3.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.3.7.1\">32.5574</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.3.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.3.8.1\">7.5287</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.3.9\">20.5774</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.3.10\">30.6797</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.4.1\">\n<span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T2.1.4.1.1\">Medicine</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.4.2\">9.5097</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.4.3\">13.6096</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.4.4\">18.8911</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.4.5\">9.0499</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.4.6\">13.5373</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.4.7\">19.4425</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.4.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.4.8.1\">8.4561</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.4.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.4.9.1\">13.8811</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.4.10\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.4.10.1\">19.8118</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.5.1\">\n<span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T2.1.5.1.1\">Leetcode</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.5.2\">1.9527</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.5.3\">34.8582</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.5.4\">47.8152</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.5.5\">2.0289</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.5.6\">35.2886</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.5.7\">46.6290</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.5.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.5.8.1\">1.9311</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.5.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.5.9.1\">37.4872</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.5.10\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.5.10.1\">49.3256</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.6.1\">\n<span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T2.1.6.1.1\">Exam</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.6.2\">3.1154</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.6.3\">3.0871</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.6.4\">18.5609</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.6.5\">3.1135</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.6.6\">4.3259</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.6.7\">16.6206</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.6.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.6.8.1\">2.9752</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.6.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.6.9.1\">4.7942</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.6.10\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.6.10.1\">19.1840</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.7.1\">\n<span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T2.1.7.1.1\">Webgpt</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.7.2\">1.7945</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.7.3\">38.8995</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.7.4\">41.4447</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.7.5\">1.8484</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.7.6\">39.6297</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.7.7\">42.0700</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.7.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.7.8.1\">1.7933</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.7.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.7.9.1\">40.2602</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.7.10\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.7.10.1\">43.7395</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.8.1\">\n<span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T2.1.8.1.1\">Gpt4tools</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.8.2\">2.2525</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.8.3\">64.7501</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.8.4\">71.4391</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.8.5\">2.2497</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.8.6\">66.3450</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.8.7\">73.1289</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.8.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.8.8.1\">2.2123</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.8.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.8.9.1\">69.2596</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.8.10\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.8.10.1\">74.5962</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.9\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.9.1\">\n<span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T2.1.9.1.1\">Cot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.9.2\">2.8126</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.9.3\">34.5210</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.9.4\">45.7961</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.9.5\">2.6474</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.9.6\">43.6290</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.9.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.9.7.1\">53.2125</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.9.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.9.8.1\">2.5931</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.9.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.9.9.1\">40.2529</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.9.10\">50.3844</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.10\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.10.1\">\n<span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T2.1.10.1.1\">S.O.</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.10.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.10.2.1\">2.8169</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.10.3\">19.9554</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.10.4\">29.7282</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.10.5\">2.9012</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.10.6\">19.4896</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.10.7\">28.4694</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.10.8\">2.8999</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.10.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.10.9.1\">23.0412</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.10.10\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.10.10.1\">31.9793</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.11\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S4.T2.1.11.1\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.11.1.1\">Average</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T2.1.11.2\">4.0128</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T2.1.11.3\">28.5348</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T2.1.11.4\">37.7877</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T2.1.11.5\">3.9450</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T2.1.11.6\">30.5912</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T2.1.11.7\">39.0163</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T2.1.11.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.11.8.1\">3.7987</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T2.1.11.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.11.9.1\">31.1942</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T2.1.11.10\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.11.10.1\">39.9626</span></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>In-domain test-set performance for different training strategies of LoRA. We report PPL, tokenized BLUE and ROUGE-L above. Both Single-LoRA (mixed) and MoA are tested under the test data of unknown domain labels and Single-LoRA is tested on the test data of the corresponding domain. PPL is short for perplexity. The best value per metric on different tasks is in bold.</figcaption>\n</figure>",
            "capture": "Table 2: In-domain test-set performance for different training strategies of LoRA. We report PPL, tokenized BLUE and ROUGE-L above. Both Single-LoRA (mixed) and MoA are tested under the test data of unknown domain labels and Single-LoRA is tested on the test data of the corresponding domain. PPL is short for perplexity. The best value per metric on different tasks is in bold."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T3\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T3.1\">\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S4.T3.1.1.1\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1\">Domain</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.2.1\">test size</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.3.1\">Classifier</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.1.1.4\">\n<span class=\"ltx_text\" id=\"S4.T3.1.1.4.1\"></span> <span class=\"ltx_text\" id=\"S4.T3.1.1.4.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T3.1.1.4.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T3.1.1.4.2.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.4.2.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.4.2.1.1.1.1\">Router</span></span></span>\n</span></span> <span class=\"ltx_text\" id=\"S4.T3.1.1.4.3\"></span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T3.1.2.1\">\n<span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T3.1.2.1.1\">Finance</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.2.2\">2000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.2.3\">98.80%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.2.4\">99.60%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.1.3.1\">\n<span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T3.1.3.1.1\">Medicine</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.3.2\">1221</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.3.3\">99.92%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.3.4\">99.92%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.1.4.1\">\n<span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T3.1.4.1.1\">Leetcode</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.4.2\">1952</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.4.3\">99.95%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.4.4\">100.00%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.1.5.1\">\n<span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T3.1.5.1.1\">Exam</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.5.2\">1999</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.5.3\">99.95%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.5.4\">100.00%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.1.6.1\">\n<span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T3.1.6.1.1\">Webgpt</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.6.2\">2000</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.6.3\">100.00%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.6.4\">99.85%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.1.7.1\">\n<span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T3.1.7.1.1\">Gpt4tools</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.7.2\">2000</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.7.3\">100.00%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.7.4\">100.00%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.1.8.1\">\n<span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T3.1.8.1.1\">Cot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.8.2\">2000</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.8.3\">99.75%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.8.4\">99.95%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.9\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.1.9.1\">\n<span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T3.1.9.1.1\">Stackoverflow</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.9.2\">2000</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.9.3\">99.00%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.9.4\">99.90%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.10\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S4.T3.1.10.1\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.10.1.1\">Average</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T3.1.10.2\">1896.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T3.1.10.3\">99.67%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T3.1.10.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.10.4.1\">99.90%</span></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>The classification accuracy of MoA router and a specific classifier by domain at inference time.</figcaption>\n</figure>",
            "capture": "Table 3: The classification accuracy of MoA router and a specific classifier by domain at inference time."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T4\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T4.1\">\n<tr class=\"ltx_tr\" id=\"S4.T4.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T4.1.1.1\">Model</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T4.1.1.2\">\n<span class=\"ltx_text\" id=\"S4.T4.1.1.2.1\"></span> <span class=\"ltx_text\" id=\"S4.T4.1.1.2.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T4.1.1.2.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T4.1.1.2.2.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T4.1.1.2.2.1.1.1\">LoRA</span></span>\n</span></span> <span class=\"ltx_text\" id=\"S4.T4.1.1.2.3\"></span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T4.1.1.3\">\n<span class=\"ltx_text\" id=\"S4.T4.1.1.3.1\"></span> <span class=\"ltx_text\" id=\"S4.T4.1.1.3.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T4.1.1.3.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T4.1.1.3.2.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T4.1.1.3.2.1.1.1\">LoRA</span></span>\n<span class=\"ltx_tr\" id=\"S4.T4.1.1.3.2.1.2\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T4.1.1.3.2.1.2.1\">(mixed)</span></span>\n</span></span> <span class=\"ltx_text\" id=\"S4.T4.1.1.3.3\"></span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T4.1.1.4\">\n<span class=\"ltx_text\" id=\"S4.T4.1.1.4.1\"></span> <span class=\"ltx_text\" id=\"S4.T4.1.1.4.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T4.1.1.4.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T4.1.1.4.2.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T4.1.1.4.2.1.1.1\">MoA</span></span>\n</span></span> <span class=\"ltx_text\" id=\"S4.T4.1.1.4.3\"></span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S4.T4.1.2.1\">\n<span class=\"ltx_text\" id=\"S4.T4.1.2.1.1\"></span> <span class=\"ltx_text\" id=\"S4.T4.1.2.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T4.1.2.1.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T4.1.2.1.2.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T4.1.2.1.2.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.2.1.2.1.1.1.1\">trainable</span></span></span>\n<span class=\"ltx_tr\" id=\"S4.T4.1.2.1.2.1.2\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T4.1.2.1.2.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.2.1.2.1.2.1.1\">parameters</span></span></span>\n</span></span> <span class=\"ltx_text\" id=\"S4.T4.1.2.1.3\"></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T4.1.2.2\">143M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T4.1.2.3\">143M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T4.1.2.4\">143M*8+1.05M</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>The trainable parameters under different LoRA combinations. The router module takes up only 1.05M parameters.</figcaption>\n</figure>",
            "capture": "Table 4: The trainable parameters under different LoRA combinations. The router module takes up only 1.05M parameters."
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T5\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T5.1\">\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S4.T5.1.1.1\">Model</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T5.1.1.2\">Total</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T5.1.1.3\">Right</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T5.1.1.4\">Accuracy</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T5.1.2.1\">Single-LoRA (mixed)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.2.2\">1331</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.2.3\">515</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.2.4\">38.69%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.1.3.1\">Single-LoRA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.3.2\">1331</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.3.3\">520</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.3.4\">39.07%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T5.1.4.1\">MoA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T5.1.4.2\">1331</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T5.1.4.3\">593</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T5.1.4.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.4.4.1\">44.55%</span></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>The accuracy of responses on the Exam test dataset.</figcaption>\n</figure>",
            "capture": "Table 5: The accuracy of responses on the Exam test dataset."
        },
        "6": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T6\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T6.1\">\n<tr class=\"ltx_tr\" id=\"S4.T6.1.1\">\n<td class=\"ltx_td ltx_nopad ltx_align_center ltx_border_tt\" id=\"S4.T6.1.1.1\"><svg height=\"24.13\" overflow=\"visible\" version=\"1.1\" width=\"109.28\"><g transform=\"translate(0,24.13) scale(1,-1)\"><path d=\"M 54.64,24.13 109.28,0\" stroke=\"black\" stroke-width=\"0.4\"></path><path d=\"M 0,12.065 109.28,0\" stroke=\"black\" stroke-width=\"0.4\"></path><g class=\"ltx_svg_fog\" transform=\"translate(0,0)\"><g transform=\"translate(0,9.61) scale(1, -1)\"><foreignobject height=\"9.61\" overflow=\"visible\" width=\"37.67\">\n<span class=\"ltx_inline-block\" id=\"S4.T6.1.1.1.pic1.1.1\">\n<span class=\"ltx_inline-block ltx_align_left\" id=\"S4.T6.1.1.1.pic1.1.1.1\">\n<span class=\"ltx_p\" id=\"S4.T6.1.1.1.pic1.1.1.1.1\">Model</span>\n</span>\n</span></foreignobject></g></g><g class=\"ltx_svg_fog\" transform=\"translate(62.5,14.67)\"><g transform=\"translate(0,9.46) scale(1, -1)\"><foreignobject height=\"9.46\" overflow=\"visible\" width=\"46.78\">\n<span class=\"ltx_inline-block\" id=\"S4.T6.1.1.1.pic1.2.1\">\n<span class=\"ltx_inline-block ltx_align_right\" id=\"S4.T6.1.1.1.pic1.2.1.1\">\n<span class=\"ltx_p\" id=\"S4.T6.1.1.1.pic1.2.1.1.1\">Dataset</span>\n</span>\n</span></foreignobject></g></g><g class=\"ltx_svg_fog\" transform=\"translate(0,14.67)\"><g transform=\"translate(0,9.46) scale(1, -1)\"><foreignobject height=\"9.46\" overflow=\"visible\" width=\"46.78\">\n<span class=\"ltx_inline-block\" id=\"S4.T6.1.1.1.pic1.3.1\">\n<span class=\"ltx_inline-block ltx_align_left\" id=\"S4.T6.1.1.1.pic1.3.1.1\">\n<span class=\"ltx_p\" id=\"S4.T6.1.1.1.pic1.3.1.1.1\">Score</span>\n</span>\n</span></foreignobject></g></g></g></svg></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T6.1.1.2\">Finance</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T6.1.1.3\">Medicine</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T6.1.1.4\">Webgpt</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T6.1.2.1\">Single-LoRA (mixed)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.1.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.1.2.2.1\">76.91</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.1.2.3\">57.49</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.1.2.4\">87.92</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T6.1.3.1\">Single-LoRA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.1.3.2\">75.99</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.1.3.3\">57.11</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.1.3.4\">88.59</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T6.1.4.1\">Single-LoRA of MoA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.1.4.2\">76.30</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.1.4.3\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.1.4.3.1\">58.01</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.1.4.4\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.1.4.4.1\">89.00</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T6.1.5.1\">MoA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T6.1.5.2\"><span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.1.5.2.1\">76.56</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T6.1.5.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.1.5.3.1\">60.68</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T6.1.5.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.1.5.4.1\">89.27</span></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 6: </span>The evaluation scoring of the GPT-4 on the Finance, Medicine, and WebGPT datasets. The highest value per column is in bold and the second highest value is <span class=\"ltx_text ltx_framed_underline\" id=\"S4.T6.3.1\">underlined</span>.</figcaption>\n</figure>",
            "capture": "Table 6: The evaluation scoring of the GPT-4 on the Finance, Medicine, and WebGPT datasets. The highest value per column is in bold and the second highest value is underlined."
        },
        "7": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T7\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T7.1\" style=\"width:211.5pt;height:75.6pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(5.0pt,-1.8pt) scale(1.05,1.05) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T7.1.1\">\n<tr class=\"ltx_tr\" id=\"S4.T7.1.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S4.T7.1.1.1.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Methods</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T7.1.1.1.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PPL</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T7.1.1.1.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">BLUE</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T7.1.1.1.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">ROUGE-L</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T7.1.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T7.1.1.2.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MoE-LoRA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T7.1.1.2.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3.8578</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T7.1.1.2.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">29.1640</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T7.1.1.2.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">37.5960</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T7.1.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T7.1.1.3.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MoE-LoRA (naive)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.1.3.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.1.1.3.2.1\">3.7969</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.1.3.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">29.4170</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.1.3.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">37.3917</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T7.1.1.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S4.T7.1.1.4.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MoA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T7.1.1.4.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3.7987</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T7.1.1.4.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.1.1.4.3.1\">31.1942</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T7.1.1.4.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.1.1.4.4.1\">39.9626</span></td>\n</tr>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 7: </span>The averaged test performance comparison on eight tasks. All of the above methods have the same number of LoRA experts and have exactly the same model parameters.</figcaption>\n</figure>",
            "capture": "Table 7: The averaged test performance comparison on eight tasks. All of the above methods have the same number of LoRA experts and have exactly the same model parameters."
        },
        "8": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T8\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T8.1\">\n<tr class=\"ltx_tr\" id=\"S4.T8.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T8.1.1.1\">Domain</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T8.1.1.2\">Single-LoRA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T8.1.1.3\">MoE-LoRA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T8.1.1.4\">MoA</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T8.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T8.1.2.1\">\n<span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T8.1.2.1.1\">Finance</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T8.1.2.2\">7.8479</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T8.1.2.3\">7.6623</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T8.1.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T8.1.2.4.1\">7.5235</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T8.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T8.1.3.1\">\n<span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T8.1.3.1.1\">Medicine</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.3.2\">9.5097</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.3.3\">9.6510</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T8.1.3.4.1\">8.4488</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T8.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T8.1.4.1\">\n<span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T8.1.4.1.1\">Leetcode</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.4.2\">1.9527</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.4.3\">2.0087</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.4.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T8.1.4.4.1\">1.9296</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T8.1.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T8.1.5.1\">\n<span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T8.1.5.1.1\">Exam</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.5.2\">3.1154</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.5.3\">3.1455</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.5.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T8.1.5.4.1\">2.9745</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T8.1.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T8.1.6.1\">\n<span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T8.1.6.1.1\">Webgpt</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.6.2\">1.7945</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.6.3\">1.8080</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.6.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T8.1.6.4.1\">1.7927</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T8.1.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T8.1.7.1\">\n<span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T8.1.7.1.1\">Gpt4tools</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.7.2\">2.2525</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.7.3\">2.2524</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.7.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T8.1.7.4.1\">2.2123</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T8.1.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T8.1.8.1\">\n<span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T8.1.8.1.1\">Cot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.8.2\">2.8126</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.8.3\">2.9205</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.8.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T8.1.8.4.1\">2.5910</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T8.1.9\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T8.1.9.1\">\n<span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T8.1.9.1.1\">S.O.</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.9.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T8.1.9.2.1\">2.8169</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.9.3\">2.8801</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.9.4\">2.8968</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T8.1.10\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S4.T8.1.10.1\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T8.1.10.1.1\">Average</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T8.1.10.2\">4.0128</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T8.1.10.3\">4.0411</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T8.1.10.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T8.1.10.4.1\">3.7962</span></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 8: </span>The test perplexity of corresponding LoRA module in different models on each task dataset.</figcaption>\n</figure>",
            "capture": "Table 8: The test perplexity of corresponding LoRA module in different models on each task dataset."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.03432v1_figure_1.png",
            "caption": "Figure 1: Comparing different LoRA models using multi-task parameter fusion method and corresponding task method."
        },
        "2": {
            "figure_path": "2403.03432v1_figure_2.png",
            "caption": "Figure 2: Overall architecture of our proposed MoA."
        },
        "3": {
            "figure_path": "2403.03432v1_figure_3.png",
            "caption": "Figure 3: The flow diagram of classifier+LoRAs."
        },
        "4": {
            "figure_path": "2403.03432v1_figure_4.png",
            "caption": "Figure 4: Case study of the predicted output of different models."
        }
    },
    "references": [
        {
            "1": {
                "title": "Language models are few-shot learners.",
                "author": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020.",
                "venue": null,
                "url": "http://arxiv.org/abs/2005.14165"
            }
        },
        {
            "2": {
                "title": "Chatlaw: Open-source legal large language model with integrated external knowledge bases.",
                "author": "Jiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and Li Yuan. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2306.16092"
            }
        },
        {
            "3": {
                "title": "A review of sparse expert models in deep learning.",
                "author": "William Fedus, Jeff Dean, and Barret Zoph. 2022a.",
                "venue": "arXiv preprint arXiv:2209.01667.",
                "url": null
            }
        },
        {
            "4": {
                "title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.",
                "author": "William Fedus, Barret Zoph, and Noam Shazeer. 2022b.",
                "venue": "The Journal of Machine Learning Research, 23(1):5232\u20135270.",
                "url": null
            }
        },
        {
            "5": {
                "title": "DEMix layers: Disentangling domains for modular language modeling.",
                "author": "Suchin Gururangan, Mike Lewis, Ari Holtzman, Noah A. Smith, and Luke Zettlemoyer. 2022.",
                "venue": "In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5557\u20135576, Seattle, United States. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2022.naacl-main.407"
            }
        },
        {
            "6": {
                "title": "Parameter-efficient transfer learning for NLP.",
                "author": "Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.",
                "venue": "In Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 2790\u20132799. PMLR.",
                "url": "https://proceedings.mlr.press/v97/houlsby19a.html"
            }
        },
        {
            "7": {
                "title": "Lora: Low-rank adaptation of large language models.",
                "author": "Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021.",
                "venue": "arXiv preprint arXiv:2106.09685.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Lorahub: Efficient cross-task generalization via dynamic lora composition.",
                "author": "Chengsong Huang, Qian Liu, Bill Yuchen Lin, Tianyu Pang, Chao Du, and Min Lin. 2023.",
                "venue": "arXiv preprint arXiv:2307.13269.",
                "url": null
            }
        },
        {
            "9": {
                "title": "Llm-blender: Ensembling large language models with pairwise comparison and generative fusion.",
                "author": "Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. 2023.",
                "venue": "In Proceedings of the 61th Annual Meeting of the Association for Computational Linguistics (ACL 2023).",
                "url": null
            }
        },
        {
            "10": {
                "title": "Gshard: Scaling giant models with conditional computation and automatic sharding.",
                "author": "Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 2020.",
                "venue": "arXiv preprint arXiv:2006.16668.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Base layers: Simplifying training of large, sparse models.",
                "author": "Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. 2021.",
                "venue": "In International Conference on Machine Learning, pages 6265\u20136274. PMLR.",
                "url": null
            }
        },
        {
            "12": {
                "title": "Chatdoctor: A medical chat model fine-tuned on a large language model meta-ai (llama) using medical domain knowledge.",
                "author": "Yunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, Steve Jiang, and You Zhang. 2023.",
                "venue": "Cureus, 15(6).",
                "url": null
            }
        },
        {
            "13": {
                "title": "Ro{bert}a: A robustly optimized {bert} pretraining approach.",
                "author": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2020.",
                "venue": null,
                "url": "https://openreview.net/forum?id=SyxS0T4tvS"
            }
        },
        {
            "14": {
                "title": "The flan collection: Designing data and methods for effective instruction tuning.",
                "author": "Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. 2023.",
                "venue": "arXiv preprint arXiv:2301.13688.",
                "url": null
            }
        },
        {
            "15": {
                "title": "Decoupled weight decay regularization.",
                "author": "Ilya Loshchilov and Frank Hutter. 2017.",
                "venue": "arXiv preprint arXiv:1711.05101.",
                "url": null
            }
        },
        {
            "16": {
                "title": "Webgpt: Browser-assisted question-answering with human feedback.",
                "author": "Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. 2021.",
                "venue": "In arXiv.",
                "url": null
            }
        },
        {
            "17": {
                "title": "Adapterfusion: Non-destructive task composition for transfer learning.",
                "author": "Jonas Pfeiffer, Aishwarya Kamath, Andreas R\u00fcckl\u00e9, Kyunghyun Cho, and Iryna Gurevych. 2020.",
                "venue": "arXiv preprint arXiv:2005.00247.",
                "url": null
            }
        },
        {
            "18": {
                "title": "Alpaca-cot: An instruction fine-tuning platform with instruction data collection and unified large language models interface.",
                "author": "Zheng Lin Qingyi Si. 2023.",
                "venue": "https://github.com/PhoebusSi/alpaca-CoT.",
                "url": null
            }
        },
        {
            "19": {
                "title": "Mixture-of-experts meets instruction tuning:a winning combination for large language models.",
                "author": "Sheng Shen, Le Hou, Yanqi Zhou, Nan Du, Shayne Longpre, Jason Wei, Hyung Won Chung, Barret Zoph, William Fedus, Xinyun Chen, Tu Vu, Yuexin Wu, Wuyang Chen, Albert Webson, Yunxuan Li, Vincent Zhao, Hongkun Yu, Kurt Keutzer, Trevor Darrell, and Denny Zhou. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2305.14705"
            }
        },
        {
            "20": {
                "title": "Sql-palm: Improved large language model adaptation for text-to-sql.",
                "author": "Ruoxi Sun, Sercan O. Arik, Hootan Nakhost, Hanjun Dai, Rajarishi Sinha, Pengcheng Yin, and Tomas Pfister. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2306.00739"
            }
        },
        {
            "21": {
                "title": "Pdf-gpt.",
                "author": "Bhaskar Tripathi. 2023.",
                "venue": "https://github.com/bhaskatripathi/pdfGPT.",
                "url": null
            }
        },
        {
            "22": {
                "title": "Bloomberggpt: A large language model for finance.",
                "author": "Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2303.17564"
            }
        },
        {
            "23": {
                "title": "Baize: An open-source chat model with parameter-efficient tuning on self-chat data.",
                "author": "Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. 2023.",
                "venue": "arXiv preprint arXiv:2304.01196.",
                "url": null
            }
        },
        {
            "24": {
                "title": "Gpt4tools: Teaching llm to use tools via self-instruction.",
                "author": "Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. 2023.",
                "venue": null,
                "url": null
            }
        },
        {
            "25": {
                "title": "Chinese open instruction generalist: A preliminary release.",
                "author": "Ge Zhang, Yemin Shi, Ruibo Liu, Ruibin Yuan, Yizhi Li, Siwei Dong, Yu Shu, Zhaoqun Li, Zekun Wang, Chenghua Lin, Wenhao Huang, and Jie Fu. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2304.07987"
            }
        },
        {
            "26": {
                "title": "Domain specialization as the key to make large language models disruptive: A comprehensive survey.",
                "author": "Xujiang Zhao, Jiaying Lu, Chengyuan Deng, Can Zheng, Junxiang Wang, Tanmoy Chowdhury, Li Yun, Hejie Cui, Zhang Xuchao, Tianjiao Zhao, et al. 2023.",
                "venue": "arXiv preprint arXiv:2305.18703.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.03432v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3"
        ],
        "main_experiment_and_results_sections": [
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.5"
        ]
    },
    "research_context": {
        "paper_id": "2403.03432v1",
        "paper_title": "Mixture-of-LoRAs: An Efficient Multitask Tuning for Large Language Models",
        "research_background": "### Paper's Motivation:\nThe paper is motivated by the need to efficiently tune large language models (LLMs) for multiple specialized tasks, particularly in scenarios where domain-specific expertise is crucial and computational resources are limited. Current approaches either involve implicit parameter fusion, leading to mutual disturbance and degraded performance in domain-specific tasks, or require extensive computational resources, such as training from scratch or deploying multiple models concurrently.\n\n### Research Problem:\nThe research problem addressed in this paper is the development of an efficient multi-task tuning method for large language models that can handle heterogeneous and imbalanced training data across multiple domains without performance degradation or excessive computational requirements. The goal is to design a mechanism that allows LLMs to maintain domain-specific expertise while being adaptable to different tasks within a single architecture.\n\n### Relevant Prior Work:\n1. **Domain-Specific LLMs**: Various domain-specific LLMs like SQL-PaLM, BloombergGPT, ChatLaw, and pdfGPT have been developed to tackle specialized problems.\n2. **Adapter-based Fine-Tuning**: Adapters and LoRA introduce limited domain-specific parameters to retain knowledge while reducing training costs (Houlsby et al., 2019; Hu et al., 2021).\n3. **Two-Stage Approaches**: Some methods train individual domain LoRA modules and then use domain classifiers to select the appropriate model. Others, like AdapterFusion and element-wise LoRA composition, fuse knowledge of multiple task adapters (Pfeiffer et al., 2020; Huang et al., 2023).\n4. **Mixture of Experts (MoE)**: MoE introduces multiple expert models to handle different input types (Shen et al., 2023).\n5. **Ensemble LLMs**: Combining outputs from multiple LLMs to enhance performance (Jiang et al., 2023).\n\n### Limitations of Prior Approaches:\n1. **Implicit Parameter Fusion**: Causes mutual disturbance among tasks due to imbalanced and heterogeneous training data, leading to degraded performance in domain-specific tasks.\n2. **Resource Intensive Methods**: MoE requires new model structures and large training corpora, while ensemble LLMs need sufficient computing resources to deploy multiple models simultaneously.\n\n### Proposed Solution: \nThe authors propose an end-to-end parameter-efficient tuning method called Mixture-of-LoRAs (MoA) to address the limitations of existing approaches. It involves:\n1. A novel routing mechanism within a decoder-only model architecture for automatic selection of LoRA experts, applicable to mainstream LLMs.\n2. Parallel processing during training for different domain samples and an expert selection mechanism during inference.\n3. Leveraging the complementarity of domain-specific knowledge to improve performance.\n\nThe authors claim that their method efficiently allows for multi-task fine-tuning, mitigating mutual interference and data imbalance issues, and enabling independent optimization of each task with limited resources.\n\n### Contributions:\n1. **MoA Architecture**: For efficient multi-task fine-tuning without performance degradation from task interference.\n2. **Explicit Routing Strategy**: Leveraging knowledge complementarity during training to enhance single-task performance and ensure inference efficiency.\n3. **Experimental Validation**: Extensive benchmark experiments confirming the effectiveness and flexibility of combining multiple domain-specific LoRAs to form a comprehensive LLM.",
        "methodology": "### Mixture-of-LoRAs: An Efficient Multitask Tuning for Large Language Models\n\n**Methodology:**\n\nGiven the variations in data scales and training complexities, we start by separately training **N LoRA modules** for **N distinct task types**. The task types mentioned here and the domain-specific data or tasks referred to in the paper carry the same meaning, all representing scenario tasks that require certain expertise to solve.\n\n1. **Training Phase**:\n   - **Separate Training**: We commence by independently training each LoRA module to optimize their performance on specific scenario tasks.\n   - **Optimal Parameters**: Initially, we obtain the optimal LoRA module parameters by training on each distinct scenario task data.\n\n2. **Performance**:\n   - These LoRA modules demonstrate commendable performance within their respective domains.\n\n3. **Integration Phase**:\n   - **Routing Mechanism**: We employ a routing mechanism to integrate the N LoRA modules under a shared Large Language Model (LLM).\n   - **Simultaneous Incorporation**: Multiple LoRA modules are simultaneously incorporated alongside each transformer layer.\n   - **Guidance**: Preceding each transformer layer, the routing mechanism is inserted to guide the selection of distinct LoRA experts.\n\nThe key innovation here lies in the routing mechanism that enables the selection and integration of multiple LoRA experts, which allows the model to efficiently handle various tasks by leveraging task-specific expertise. This results in improved multitask tuning and performance within different domains under a unified LLM framework.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n**Datasets:**\nTo evaluate the effectiveness of the Mixture-of-LoRAs (MoA) approach, the experiments utilize a variety of supervised fine-tuning (SFT) datasets, encompassing heterogeneous domains. The datasets include:\n\n1. **Specialized Domain Datasets:**\n   - **Finance**\n   - **Medicine**\n   - **Leetcode**\n\n2. **Function-specific Datasets:**\n   - **Exam**\n   - **Webgpt**\n   - **Gpt4tools**\n\n3. **Other Datasets:**\n   - **Chain-of-thought**\n   - **Dialog**\n\nBoth English and Chinese datasets are incorporated to assess multilingual performance. Dataset statistics are provided in Table 1.\n\n**Implementation Details:**\n- **Base LLM:** Qwen-7b, a decoder-only architecture.\n- **Vocabulary:** 151,851 BPE types.\n- **Training Token Sequences:** 4,096-token sequences.\n- **Training Steps:** Total number of training steps is determined based on the allocated runtime, with 10% of these steps assigned to warmup.\n- **Optimizer:** AdamW with a cosine learning rate decay.\n- **Learning Rate:** \n- **Batch Processing:** Each worker processes eight sequences of 4,096 tokens. Gradients are accumulated over four updates.\n- **Gradient Clipping:** L2 norm exceeds 0.1.\n- **Hardware:** 8 NVIDIA A100 80GB GPUs.\n\n**Evaluation Metrics:**\nThe primary metric used for evaluation is test perplexity, which is reported after a single run of training.\n\n### Main Experimental Results\n\nResults should include specific metrics on test perplexity across the various datasets mentioned, comparing the performance of the MoA approach with other baselines. This would help in understanding the effectiveness and efficiency of MoA in multitask tuning for large language models, especially in diverse and specialized domains, as well as in handling tasks with specific functional requirements.\n\nAs the description of the main experimental results is not provided in the excerpt, the results section would typically summarize the performance enhancements provided by MoA over existing methods, highlighting any significant improvements in test perplexity on the English and Chinese datasets across the different domains."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Examine the effectiveness of the domain label routing mechanism in the Mixture-of-LoRAs (MoA) framework.",
            "experiment_process": "To verify the effectiveness of the domain label routing loss, the researchers conducted experiments by comparing the MoA model, which includes this loss, with the MoE-LoRA version, which does not include explicit domain label information. Both models were trained with the same number of parameters, but MoA included the domain label routing mechanism while MoE-LoRA did not. The performance was evaluated using metrics such as perplexity performance of each LoRA module and overall improvement in various tasks.",
            "result_discussion": "The findings showed that MoA achieved an overall improvement over MoE-LoRA, demonstrating that domain label information is useful for different tasks. Specifically, MoA improved the perplexity performance of each LoRA module from 4.0128 to 3.7962, while MoE-LoRA caused a slight decrease. This indicates that MoA is more flexible and effective, particularly when expanding to more tasks or combining multiple functions.",
            "ablation_id": "2403.03432v1.No1"
        },
        {
            "research_objective": "Evaluate the effectiveness of different LoRA initialization methods on the performance of the Mixture-of-LoRAs (MoA) framework.",
            "experiment_process": "The researchers created a naive variant of MoE-LoRA where LoRA modules are initialized randomly, contrasting with the MoE-LoRA approach, which uses a specific initialization method. Both models were trained using the complete dataset from all domains, leading to a comparison of the total training duration and efficiency in customizing different capability combinations. The comparative analysis was presented in Table 7, showcasing the performance across eight distinct tasks.",
            "result_discussion": "The comparative analysis demonstrated that both initialization methods produced comparable results across eight tasks. However, the naive initialization method significantly increased the total training duration and proved highly inefficient for customization purposes. Thus, conducting further training on the initial multi-domain LoRA parameters emerged as a highly efficient method.",
            "ablation_id": "2403.03432v1.No2"
        }
    ]
}