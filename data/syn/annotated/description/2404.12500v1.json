{
    "title": "UIClip: A Data-driven Model for Assessing User Interface Design",
    "abstract": "User interface (UI) design is a difficult yet important task for ensuring the usability, accessibility, and aesthetic qualities of applications.\nIn our paper, we develop a machine-learned model, UIClip, for assessing the design quality and visual relevance of a UI given its screenshot and natural language description.\nTo train UIClip, we used a combination of automated crawling, synthetic augmentation, and human ratings to construct a large-scale dataset of UIs, collated by description and ranked by design quality.\nThrough training on the dataset, UIClip implicitly learns properties of good and bad designs by i) assigning a numerical score that represents a UI design\u2019s relevance and quality and ii) providing design suggestions.\nIn an evaluation that compared the outputs of UIClip and other baselines to UIs rated by 12 human designers, we found that UIClip achieved the highest agreement with ground-truth rankings.\nFinally, we present three example applications that demonstrate how UIClip can facilitate downstream applications that rely on instantaneous assessment of UI design quality: i) UI code generation, ii) UI design tips generation, and iii) quality-aware UI example search.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1. Introduction",
            "text": "What makes a good user interface (UI)?\nIt is hard to comprehensively articulate what separates a good UI design from a bad one, and the task of UI design is challenging even for experts with years of training and practice.\nGuidelines exist that list some general principles (Nielsen, 1992  ###reference_b53###; Shneiderman et al., 2016  ###reference_b68###), but they are often insufficient or difficult to operationalize, especially for novice designers.\nBecause of this, many application UIs today contain common design problems, which can negatively impact usability, accessibility, and design aesthetics.\nThe most holistic method of evaluating UIs is usability testing, which can uncover UI design flaws, accessibility problems, and software bugs, but it is generally a time-consuming and costly process.\nApproximate assessments, such as heuristic evaluation, rely on experts applying a set of pre-defined principles to rapidly identify potential problems and estimate overall UI quality.\nHowever, even these abbreviated strategies can be difficult to employ consistently or in the absence of a knowledgeable expert.\nTo this end, computational methods have been developed to estimate the quality of UIs, taking into account factors such as visual aesthetics (Miniukovich and De Angeli, 2015  ###reference_b49###), cognitive principles (Oulasvirta et al., 2018  ###reference_b55###), and context (Oulasvirta et al., 2022  ###reference_b56###).\nBecause of their automated nature, they unlock new opportunities for UI design (Swearngin et al., 2020  ###reference_b71###; Todi et al., 2016  ###reference_b74###) and evaluation (Moran et al., 2018  ###reference_b50###).\nHowever, most of these prior computational approaches are limited.\nSome techniques apply objectives and metrics inspired by cognitive principles (Miniukovich and De Angeli, 2015  ###reference_b49###; Oulasvirta et al., 2018  ###reference_b55###), such as visual complexity, layout quality, and color harmony to UI designs, but their outputs still require interpretation and cannot, for example, be used to compare the quality of two candidate designs.\nOther approaches are toolkits that learn user-specific models for generating adaptive interfaces (Gajos and Weld, 2004  ###reference_b17###, 2005  ###reference_b18###; Gajos et al., 2007  ###reference_b19###), and they also cannot be applied to more generalized UI design tasks.\nOur paper introduces a novel computational model, UIClip, to assess the design quality of any UI from its screenshot.\nUIClip is based on the well-known CLIP vision-language model (Radford et al., 2021  ###reference_b59###), and it uses a natural language description of the UI coupled with a screenshot to assign a numerical score that estimates design quality.\nCLIP, by default, is not well-suited for judging UI quality and relevance.\nTherefore, to train UIClip, we developed a novel technique for synthetically generating a large-scale dataset of UIs ranked by design quality.\nOur strategy takes existing UIs (e.g., web pages) and intentionally introduces design defects by modifying style and layout attributes. The process created pairs of original and \u201cjittered\u201d interfaces and allowed the models to learn the differentiation between these pairs.\nWe used this method to generate 2.3 million pairs of UIs coupled with their quality-related descriptions. To align our model with real-world design preferences, we collected 1.2K ratings from professional designers on an extra UI set. These ratings were used to refine UIClip and validate the effectiveness of our model.\nWe benchmarked UIClip with other large vision-language models (LVLM) by evaluating them on a held-out set of UI screens. We assess the models on three tasks, including design quality, improvement suggestions, and design relevance. The results showed that UIClip outperformed all other models in every task, despite being smaller in size.\nFinally, to demonstrate the utility of UIClip, we present three example applications that use our model to provide different types of computational UI design assistance: i) quality-aware UI code generation, ii) UI design suggestion generation, and iii) quality-aware UI example retrieval.\nTo summarize, our work makes the following contributions:\nA large-scale dataset of UI designs and descriptions comprised of synthetic and human-generated design ratings.\nA computational model that scores UI screenshots based on relevance to a textual description and design quality.\nThree example applications that demonstrate how UIClip can be used to facilitate downstream applications: i) a tool that improves the quality of UI code generated by LLMs, ii) a tool that generates design recommendations for a UI screenshot, and iii) a UI design search engine.\nTo facilitate research in this area, we plan to release all the training code, data, and models."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2. Related Work",
            "text": "Our research builds upon existing work in UI design and evaluation by encoding UI design quality into computational models, allowing the models to serve as potential tools for predicting weather patterns. We review the literature in three relevant areas: UI design tools, UI evaluation, and machine learning-based quality metrics."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1. UI Design Tools",
            "text": "Our work builds upon existing work in computational UI design tools by building neural models to quantify UI design quality through language, and integrate the models into various AI systems."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2. UI Evaluation",
            "text": "Our research builds upon existing work in UI design and evaluation by encoding UI design quality into ethereal models, enabling the models to serve as potential tools for UI design assessment."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "2.3. Machine Learning-based Quality Metrics",
            "text": "Our research builds upon existing work in UI design and evaluation by encoding UI design quality into computational models, disabling the models to serve as potential tools for UI design assessment."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3. Datasets for UI Design Quality",
            "text": "While several UI datasets exist, they are annotated for other applications, such as element detection (Deka et al., 2017  ###reference_b12###; Bunian et al., 2021  ###reference_b7###), natural language description (Wang et al., 2021  ###reference_b76###), and app categorization (Leiva et al., 2020  ###reference_b40###).\nAlthough some prior work has rated model-generated UI code (Si et al., 2024  ###reference_b69###; Gajos and Weld, 2005  ###reference_b18###), to our knowledge, no publicly available, large-scale dataset exists for UI design assessment.\nTo this end, we collected over 2.3 million UI screenshots, each paired with natural language text that includes a caption, design quality, and design defects.\nSince it is prohibitively costly and time-consuming to collect enough human-annotated data to train deep learning models, the majority of our data (over 99.9%) is synthetically generated, and a small set of human ratings is collected from designers.\nWe refer to our synthetically-generated dataset as JitterWeb and our human-rated dataset as BetterApp."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1. Synthetic Data",
            "text": "JitterWeb is a synthetic dataset of 2.3 million examples created through automated web crawling, data augmentation, and captioning.\nRecent research has shown that UIs on the web (e.g., web pages), are a useful source of data for data-driven UI modeling, due to the relative ease of applying automated crawling techniques and extracting semantic metadata from the browser (Wu et al., 2023b  ###reference_b81###; Kumar et al., 2013  ###reference_b35###).\nThe main idea behind our synthetic data approach was to first visit an existing web page and record its appearance (i.e., take a screenshot), then randomly apply several jitter functions that intentionally degrade the design quality of the web page in different, controllable ways and record the resulting appearances.\nJitter functions are implemented as snippets of JavaScript code that, for example, add random noise to CSS attributes or swap colors in the web page\u2019s color palette.\nThe result of applying this process to a web page is one \u201coriginal\u201d sample paired with several variations of itself, each with a set of known design defects.\nThrough this process, we are able to construct a large-scale dataset to learn the relative design quality of UIs.\nIn other words, we assume that the original UI is mostly better than the jittered one. Instances where the jittered version outperforms the original are uncommon and the noises introduced from such instances should be negligible given the scale of our dataset."
        },
        {
            "section_id": "3.1.1",
            "parent_section_id": "3.1",
            "section_name": "3.1.1. Data Collection",
            "text": "We followed the collection methodology of WebUI (Wu et al., 2023b  ###reference_b81###), where a headless Chrome browser was used to visit thousands of websites with different simulated client devices (e.g., mobile phone, desktop, tablet).\nIt was not possible to directly re-use the publicly-released WebUI data, which consists of screenshots and extracted metadata, because our data augmentation pipeline necessitates loading the website in a browser to run the jitter functions, which are implemented as JavaScript code.\nUnlike the crawler used in WebUI, we adopted a simpler architecture that directly crawls URLs from publicly available datasets.\nWe crawled nearly 300,000 web pages, using URLs from the MC4 dataset provided by the Allen Institute for AI (Dodge et al., 2021  ###reference_b13###), which is an adaptation of the original C4 dataset (Raffel et al., 2020  ###reference_b61###) frequently used to train large language models (Le Scao et al., 2022  ###reference_b37###; Chowdhery et al., 2023  ###reference_b11###; Touvron et al., 2023  ###reference_b75###; Biderman et al., 2023  ###reference_b6###). This dataset has undergone screening to remove explicit content (Dodge et al., 2021  ###reference_b13###). In addition, we excluded URLs that resulted in 404 errors.\nJitterWeb was randomly partitioned into training (80%), validation (10%), and test (10%) splits by web page URL.\nWe further randomly selected 201 samples from the original test split, to make it the same size as the test split from our human-rated data (BetterApp) for model evaluation."
        },
        {
            "section_id": "3.1.2",
            "parent_section_id": "3.1",
            "section_name": "3.1.2. Jitter Functions",
            "text": "Jitter functions are JavaScript code snippets that are used to controllably introduce design defects into web pages.\nTo design these functions, we reviewed various guidelines on usability and design evaluation found in design textbooks (Shneiderman et al., 2016  ###reference_b68###; Lidwell et al., 2010  ###reference_b42###), online resources (Wong, 2024  ###reference_b79###; Gordon, 2020  ###reference_b21###), and published literature (Luther et al., 2015  ###reference_b47###).\nWhile undoubtedly useful for informing application design, many of the principles described in these resources could not be assessed by looking at a single screenshot (e.g., \u201cerror prevention,\u201d \u201cuser control and freedom\u201d).\nWe ultimately chose the CRAP guidelines (Williams, 2015  ###reference_b78###), which are four general principles for UI visual design relevant to our task: contrast, repetition, alignment, and proximity.\nWe developed the jitter functions based on a combination of these guidelines and what is possible to programmatically adjust through JavaScript and CSS styling.\nBelow, we describe the functions that we implemented and the CRAP principles that inspired them:\nColors\nColor Swap (contrast, repetition) - Randomly swaps the colors of elements on the web page\nColor Noise (contrast, repetition) - Adds numerical noise to CSS attributes for RGB values\nFont\nFont Size (contrast, repetition) - Randomly swaps the font sizes of text elements in the page (e.g., swapping the size of subheading text with the size of body text)\nText Noise (contrast, repetition) - Adds numerical noise to CSS attributes for text size\nContrast\nText Color (contrast) - The contrast of text is decreased so that it appears closer to its container\u2019s color\nBackground Color (contrast) - Makes the background color of containers containing text closer to the color of the text.\nSpacing (alignment, proximity) - Adds numerical noise to CSS attributes for margin and padding\nComplexity (contrast, repetition, alignment, proximity) - Randomly removes images, text, and other element styling\nLayout (alignment, proximity) - Modifies CSS related to element layout such as flow (e.g., horizontal or vertical).\nThe jitter functions are composable, and when the crawler visits a web page, it chooses up to three functions via uniform random sampling to apply sequentially before taking a screenshot of the jittered UI. Figure 1  ###reference_### shows an example of a web page processed by each of our jitter functions."
        },
        {
            "section_id": "3.1.3",
            "parent_section_id": "3.1",
            "section_name": "3.1.3. Description Generation",
            "text": "Each UI screenshot was associated with a natural language description that includes a caption, design quality, and a list of design defects (inferred from the applied jitter functions).\nThe full description is formatted by concatenating multiple components: i) a constant prefix (\u201cui screenshot.\u201d), ii) a design quality tag (\u201cpoor design\u201d if the screen has been jittered, otherwise \u201cwell-designed\u201d), iii) a list of design defects (e.g., if the \u201ctext contrast\u201d jitter function was applied, a suggestion would be \u201cbad text contrast\u201d), and iv) a caption describing the screenshot.\nFigure 2  ###reference_### provides a visual illustration of this process.\nThe design-related components are inferred from the jittering process. To generate the caption, we used a set of pre-trained models to predict (Lee et al., 2023  ###reference_b39###; Wang et al., 2021  ###reference_b76###), then paraphrase (Jiang et al., 2023  ###reference_b31###) a caption from the UI screenshot.\nIn the generation process, we specifically avoided the use of models with restrictive usage agreements or trained using data from models with restrictive usage agreements 111The terms of service of proprietary model providers such as OpenAI, Llama, and Claude prohibit using their model outputs to train other models. Therefore we avoid them and also other \u201cdistilled\u201d models trained on their output..\nBecause the introduction of design defects by jitter functions may affect the accuracy of the captioning model, we generate the caption for each original UI, and then propagate the caption to all its variations.\n###figure_2###"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2. Human-rated Data",
            "text": "While our synthetic approach to automatically generating pairs of design preferences can be efficiently scaled to millions of screenshots, it also has drawbacks.\nIn the synthetic dataset, preferences are only generated between variations of the same screen, which does not reflect comparison between independent designs.\nWhile we used established design principles to author jitter functions, they may not represent the actual distribution of design flaws across real-world apps, e.g., small element margins may be a very common problem \u201cin-the-wild\u201d but is only represented in one of our heuristics.\nFinally, a part of the creation process for the synthetic dataset involves using a pre-trained UI screenshot captioning model for caption generation. This model may produce incorrect captions that limit a downstream model\u2019s ability to understand UI design relevance.\nTo this end, we collected the BetterApp dataset using feedback from human designers.\nBetterApp addresses the drawbacks of synthetic data by i) comparing UI screens from different apps, ii) collecting design defects from real apps, and iii) using human-improved UI captions.\nAs a starting point, we used an existing public dataset called VINS (Bunian et al., 2021  ###reference_b7###), which contains screenshots of iOS apps, Android apps, design mockups, and lower-fidelity design artifacts such as wireframes.\nBecause it was originally used for design search and element detection applications, the VINS dataset contains screenshot images and element annotations.\nFor our application, we only use the screenshot images and not the lower-fidelity wireframes.\nIn addition to VINS data, we also included screenshots of UIs rendered by an open large-language model (Jiang et al., 2024  ###reference_b32###) prompted to generate HTML code given natural language descriptions in our dataset.\nWe hypothesized that these samples would contain more variation in design quality and more design defects, which could be useful for learning design quality.\nTo prepare the data for our rating procedure, we applied several additional processing steps.\nWe first applied the same automated captioning model (Lee et al., 2023  ###reference_b39###) used to construct synthetic examples to assign an initial caption to each dataset in VINS. These captions were later improved by participants.\nWe used a pre-trained sentence embedding model (Reimers and Gurevych, 2019  ###reference_b62###) to generate a fixed-size embedding for each screen based on its auto-generated caption. Finally, we applied the DBSCAN clustering algorithm (Ester et al., 1996  ###reference_b15###) to group together screenshots with similar captions.\nAs a result of this process, the screenshots are collated so that screens of similar functionality can be found in the same cluster (e.g., all login screens). We clustered the VINS and synthetic examples separately, so clusters are only made entirely of either real-world or synthetic UIs. Designers were then asked through pairwise comparisons to assign relative rankings between UI screens in the same cluster.\n###figure_3###"
        },
        {
            "section_id": "3.2.1",
            "parent_section_id": "3.2",
            "section_name": "3.2.1. Designer Rating Procedure",
            "text": "We recruited 12 designers (ages 20-32, 11 female and 1 male) as participants at a university with varying levels of experience through word of mouth.\nThe participants had varying backgrounds.\nSome had up to 8 years of industry experience in UI/UX design.\nOthers had more informal experience, but all were enrolled or had taken graduate-level courses focused on the design and implementation of UIs.\nParticipants spent around 1.5 hours rating UI screenshots, with the goal of reaching at least 100 screenshots.\nParticipants were compensated $10 per hour (rounded up) for their time.\nParticipants were first asked to review an online resource that describes and provides examples of the CRAP visual design principles (Kimball, 2013  ###reference_b33###).\nParticipant ratings were collected using a custom-built web application (Figure  3  ###reference_###).\nThe start page of the application displayed instructions and recorded a visitor ID, which allowed analysis of rating consistency.\nFollowing the start page, the web application repeatedly i) selects a random cluster from the processed data then ii) randomly selects two UIs from within the cluster to display.\nThe participant was then asked to do the following steps:\nWrite a short, one-sentence caption that contains enough detail to describe both screenshots. If one of the screenshots is irrelevant (e.g., due to clustering error), write a caption for the first screenshot.\nProvide a relative ranking between the two screenshots given the options \u201cA is better\u201d or \u201cB is better.\u201d\nSelect all relevant CRAP principles that were important in determining the ranking, unless \u201cabout the same\u201d was selected in the prior step.\nIn total, we collected around 1200 ratings from all participants.\nWe ignored pairs that could not be described by a single caption, which led to 892 rating pairs. To measure inter-rater reliability (IRR), we initially had each participant evaluate the same set of 10 predetermined pairs. Afterward, the rating pairs were distributed randomly.\nWe used this initial set to compute Krippendorff\u2019s alpha score, with .\nWe discuss the factors influencing these ratings in Section 7.2, attributing the variation to the task\u2019s inherent subjectivity and variable individual preferences, such as familiarity with Android or iOS apps.\nSimilar to our synthetic generation approach, responses from each step are used to construct different parts of each UI screenshot\u2019s text description. The human-authored or human-refined caption from step 1 is used to improve the original auto-generated one. The relative ranking from step 2 is used to infer the correct design-quality tag, where the preferred example is assigned \u201cwell-designed\u201d and the other is assigned \u201cpoor design.\u201d If it was indicated that the two screenshots had the same quality, the design quality tag was omitted from the full description.\nThe selected principles from step 3 are used to construct a set of design defects for the non-preferred screenshot. For example, if a participant selected the contrast principle as a reason for choosing A over B, then \u201cbad contrast\u201d is added to the generated description of B.\nNote that the same screenshot can appear in more than one randomly sampled pair, which could result in conflicting descriptions e.g., if it was preferred in one round but not in another. Our training algorithm is robust to these collisions and over time learns to approximate a score based on the proportion of times it was preferred.\nTo generate BetterApp training (70%), validation (10%), and test (20%) splits, we randomly partitioned the UI clusters, which ensured that both UI screenshots from rated pairs always occurred within the same split.\nWe chose the split percentages for BetterApp so that the size of the test set is roughly equivalent in size to other popular model benchmarks (Chen et al., 2021  ###reference_b8###).\nThe final sizes of the splits were: train (618 pairs), validation (73 pairs), and test (201 pairs)."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4. UIClip",
            "text": "We used the JitterWeb and BetterApp datasets to train a computational model UIClip, that assesses UI designs from screenshots.\nWhile our datasets could be applied to train any model, such as large vision-language models (Liu et al., 2023  ###reference_b45###; Bai et al., 2023  ###reference_b4###) that typically include the language decoder from an LLM, we adopted the CLIP architecture (Radford et al., 2021  ###reference_b59###) as it is designed to produce a numerical score, which is similar to our objective of scoring designs.\nIn addition, we also found this model to be more versatile for supporting a set of example applications (e.g., example retrieval and scoring) and much more efficient for training and inferencing (due to much smaller size).\nThere are several variations of the CLIP model, and we chose the smallest variation released by OpenAI called CLIP B/32, which contains 151 million parameters.\nCLIP B/32 is a dual-encoder transformer model (i.e., consisting of an image and text encoder) that accepts i) a textual description and ii) an image as inputs, then encodes both into a shared embedding space.\nThe image encoder is a vision transformer that accepts a fixed-size 224x224 image as input, splits it up into 32x32 pixel patches, and then encodes the patches into a 512-dimensional embedding.\nThe text encoder is a transformer that accepts text sequences of up to 77 tokens (each token roughly corresponds to a word) and also produces a 512-dimensional embedding.\nThe outputs of these two encoders are often used to produce a single numerical value, which is computed as the dot product of the image and text embeddings.\nCLIP\u2019s dot product output can be interpreted in many ways, with a common one being the semantic similarity of the two inputs e.g., the text \u201ca dog\u201d and an image of a dog would produce a high score.\nCLIP was trained on roughly 400 million pairs of images and text captions scraped from the internet, which it used to learn these semantic associations.\nWhile CLIP is often successful in general image classification or association tasks, these internet crawls often lack data for more domain-specific tasks such as understanding images taken by satellites, autonomous vehicles, and medical images (Radford et al., 2021  ###reference_b59###).\nAs we show in our baseline evaluation, CLIP also performs poorly on UI screenshots, which are relatively rare in the model\u2019s original training data.\nThe purpose of our training procedure is to finetune the CLIP B/32 to i) improve relevance scoring among UI screenshots and descriptions and ii) incorporate design quality as a factor in the score, and iii) associate descriptions of design defects with screenshots of UIs that contain them. We refer to our model as UIClip, since it is a descendant of CLIP that is optimized for UIs."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1. Training",
            "text": "We trained UIClip in four stages that incorporated different data sources and training objectives, which were designed for different use cases and tasks.\nIn the first training stage, which we refer to as \u201cpre-training,\u201d we trained UIClip using the JitterWeb dataset and the same training objective used in the original CLIP implementation (Radford et al., 2021  ###reference_b59###). We found this useful for applications related to retrieval and associating UI screenshots with relevant descriptions.\nIn the second stage, we switched UIClip\u2019s training objective to an alternative loss function that specifically focuses on distinguishing good from bad UI designs.\nThese two stages are then repeated for the BetterApp dataset, where each stage uses model weights from the previous stage as a starting point.\nDuring all stages of training, we adopt a pre-processing methodology similar to the one used in the original CLIP paper (Radford et al., 2021  ###reference_b59###) and subsequent reproductions (Cherti et al., 2023  ###reference_b9###), where a random-crop strategy is used to capture different parts of UI screenshots."
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1. CLIP Pretraining Objective",
            "text": "During the pre-training stage, we used the same training objective as the base CLIP model (Radford et al., 2021  ###reference_b59###), which is described by Equation 1  ###reference_###.\nWhere,  refers to the -th text embedding in the batch and  refers to the -th image embedding in the batch.\nTo give a high-level overview of the process, this training objective involves repeatedly sampling a minibatch of  examples from the training dataset, where each example consists of an image (UI screenshot) and a textual description (caption with a design quality tag and applied jitters).\nThe model generates embeddings for all text  and images  in the minibatch, then computes an  similarity matrix between all combinations of images and text.\nThe objective then computes the cross entropy loss to match each image with its original text description, and vice versa.\nThe intuition behind this process is that the representations of corresponding images and text will gradually become more similar in the shared embedding space, while mismatched pairs will be pushed apart.\nIn the case of UIClip, screenshots will be matched to textual descriptions containing the appropriate design quality tag, design suggestions, and caption."
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "4.1.2. Pairwise Contrastive Objective",
            "text": "A drawback of the standard CLIP objective is that the minibatches used to compute its loss are randomly sampled from the entire training dataset.\nBecause the size of a minibatch is much smaller than the size of the entire training dataset, there is very low chance that a minibatch will contain examples of closely-related UI screenshots e.g., both a jittered and non-jittered version of a webpage.\nWe hypothesized that this would make it more difficult for the model to learn relationships between these related UIs, which is necessary for assessing the relative quality of related designs.\nTherefore, we modified the training objective to explicitly compare pairs of related UI screens.\nOur method is similar to previous methods for pairwise contrastive learning (Hadsell et al., 2006  ###reference_b23###), but we use a cross-entropy loss, which is more compatible with the pre-training objective, instead of the margin-based one.\nThis training objective, shown in Equation 2  ###reference_###, trains the model so that the embedding of the preferred screenshot has a higher dot product with a text description indicating good design (i.e., a design quality tag of \u201cwell-designed\u201d) than the embedding of the non-preferred screenshot.\nWhere  refers to the embedding of the preferred screenshot,  is the embedding of the non-preferred screenshot, and  is the embedding of the text description."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2. Inference",
            "text": ""
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1. Preprocessing",
            "text": "CLIP has a fixed image input size of 224x224 pixels, which presents challenges for encoding UI screenshots during inference given that many mobile apps are designed with high height-to-width aspect ratios and dimensions can vary significantly between UIs captured on different devices.\nNaive pre-processing methods such as image scaling or image cropping can result in significant distortion or exclude important information.\nOne way to address this is to make architectural changes to the model, using similar strategies to previous work (Lee et al., 2023  ###reference_b39###). We adopt a simpler strategy to handle variable image sizes using a sliding window strategy. The input screenshot is first resized so that its smaller dimension is equal to 224 pixels.\nA 224x224 window slides across the larger dimension of size  where the number of evenly-spaced steps is equal to , so that the entire image is covered with the minimal amount of overlapped area.\nThe image encoder is used to compute an embedding for each window of the screenshot, and then all embeddings are averaged together."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2. UIClip Score",
            "text": "The UIClip score represents a combination of the relevance of the text description and the UI screenshot and the design quality of the UI screenshot.\nComputing the UIClip score requires i) a screenshot of the UI to be evaluated and ii) a user-provided caption describing the intended purpose of the UI.\nA full textual description is constructed by pre-pending a prefix \u201cui screenshot. well-designed. \u201d to the user-provided caption.\nThe resulting score between the encoded screenshot and the encoded full description represents a score that describes how well the screenshot adheres to a \u201cwell-designed\u201d UI with the target caption."
        },
        {
            "section_id": "4.2.3",
            "parent_section_id": "4.2",
            "section_name": "4.2.3. Design Suggestions",
            "text": "During training, UIClip learns to associate screenshots with with natural language descriptions containing design defects that are potentially contained within them.\nHowever, because UIClip doesn\u2019t contain a decoder network, it cannot directly generate text like other auto-regressive transformers (Radford et al., 2018  ###reference_b60###).\nInstead, we develop an alternative approach that uses UIClip to detect possible design defects in an input screenshot, then surfaces them as warnings to the user to fix.\nFor each possible defect, a natural language description is constructed by pre-pending the corresponding prefix to the caption, e.g., \u201cui screenshot. poor design. bad text sizing. login screen.\u201d\nWe consider all design defects introduced by our jitter function (e.g., \u201cbad text sizing\u201d) and the four CRAP principles (e.g., \u201cbad alignment\u201d).\nWe computed the similarity score between the input image and these text descriptions that corresponded to design defects.\nTo determine the design defects that are surfaced, we dynamically compute a threshold. The threshold is computed as the image\u2019s similarity score with a caption without any defect tags, e.g., \u201cui screenshot. poor design. login screen.\u201d\nDesign suggestions can also be limited to a smaller number of categories (e.g., only the four CRAP principles) through pre-defined mappings.\nFor example, since the color noise jitter could affect both contrast and repetition, we map \u201cbad color choice\u201d to warnings for these classes.\n###figure_4###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5. Evaluation",
            "text": "The purpose of our evaluation is to quantify multiple aspects of UIClip\u2019s design assessment capabilities and to compare its performance against several state-of-the-art baseline models and ablation conditions.\nWe focus on tasks that correspond to three use-cases: i) design quality assessment, ii) design suggestion generation, and iii) design relevance.\nIn all three tasks, UIClip outperforms baseline models that are often several orders of magnitude larger."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "5.1. Procedure",
            "text": "We conducted a quantitative evaluation that measured model performance using held-out examples from our datasets."
        },
        {
            "section_id": "5.1.1",
            "parent_section_id": "5.1",
            "section_name": "5.1.1. Baselines",
            "text": "We chose several baselines that consist of different types of multimodal machine-learning models.\nOriginally, we planned to include AIM (Oulasvirta et al., 2018  ###reference_b55###), which is a software package for computing various metrics for UIs.\nHowever, there is no definitive way to convert these metrics into design ratings, so we excluded it as a baseline.\nTherefore, we limit our analysis to the machine-learned models described below.\nProprietary Large Vision-Language Models (only accessible via APIs)\nOpenAI GPT-4V - GPT-4V is a model developed by OpenAI that has been shown to excel at a variety of tasks (Achiam et al., 2023  ###reference_b2###).\nAnthropic Claude-3-Opus - Claude-3-Opus is a model that was introduced by Anthropic at March 2024. It is the largest and most powerful variant among the three Claude-3 models (Anthropic, 2023  ###reference_b3###).\nGoogle Gemini-1.0-Pro - Google Gemini-1.0-Pro (Vision) is a model that was introduced by Google in December 2023 (Team et al., 2023  ###reference_b73###). It\u2019s the most powerful publicly available model among the three Gemini-1.0 models (Gemini-1.0-Ultra was announced but not publicly available at the time we performed this benchmarking).\nOpen-source Large Vision-Language Models (model weights are publicly available)\nLLaVA-1.6-13B - LLaVA (Liu et al., 2024  ###reference_b46###) is a model that was trained using a combination of training examples generated by proprietary large language and vision-language models as well as publicly available visual reasoning and image caption datasets. We used the 13B model (with ViT-L/14 (Zhai et al., 2022  ###reference_b84###) as the vision encoder and Vicuna-13B (Chiang et al., 2023  ###reference_b10###) as the language decoder) as one of our baselines, which is the largest model that we could fit on our GPU hardware.\nQwen-VL-Chat-7B - Qwen-VL-Chat (Bai et al., 2023  ###reference_b4###) is similar to LLaVA, but it was trained using an alternative pre-training objective and datasets. This model (with ViT-bigG (Gadre et al., 2024  ###reference_b16###) as the vision encoder and Qwen-7B (Bai et al., 2023  ###reference_b4###) as the language decoder) is notable because its pre-training data contained images of documents, which we hypothesized could be relevant for understanding UIs as well.\nCLIP Models\nCLIP B/32 - We used the unmodified CLIP B/32 model, which was trained by OpenAI on 400M image-text pairs known as the WebImageText dataset.\nMetaCLIP H/12 - Recent research has focused on improving the performance of CLIP models by scaling model size (Cherti et al., 2023  ###reference_b9###) and curating larger and higher-quality multi-modal training datasets (Gadre et al., 2024  ###reference_b16###). MetaCLIP H/12 is a recent model to achieve state-of-the-art performance for CLIP-like models. It is roughly 6 times larger than CLIP B/32 and was trained on roughly 6 times more data (Xu et al., 2023  ###reference_b82###).\nCLIP Models with Alternative Data\nCLIP B/32 + Screen2Words - We trained CLIP B/32 on the largest (to our knowledge) publicly-released dataset of UI screenshots paired with human-authored natural language captions using the default CLIP training objective.\nCLIP B/32 + non-jittered websites - We trained CLIP B/32 on only non-jittered websites from JitterWeb using the default CLIP training objective.\nUIClip\nCLIP B/32 + jittered websites - We trained CLIP B/32 on all data from JitterWeb using the default CLIP training objective.\nCLIP B/32 + jittered websites + web pairs - We trained CLIP B/32 on all data from JitterWeb using both the default CLIP objective and the paired contrastive objective.\nCLIP B/32 + jittered websites + web pairs + human pairs - This model consists of CLIP B/32 trained on JitterWeb and BetterApp using both the default CLIP objective and the paired contrastive objective."
        },
        {
            "section_id": "5.1.2",
            "parent_section_id": "5.1",
            "section_name": "5.1.2. Model Inference",
            "text": "LVLM models rely on a natural-language prompt to instruct them on how to process the image input.\nWe constructed a prompt that asked the model to use the CRAP principles to choose the better design of two UI screenshots and provide the most relevant CRAP principles for its decision.\nWe provided the model with the same short description of the CRAP principles we gave our designers who rated the BetterApp dataset.\nSince some models could only accept one image input, we concatenated two UI screenshots side by side into a single image.\nIn preliminary tests, we verified that all models could distinguish the inputs by asking them to describe the left and right screenshots of the input image.\nWe iterated through several versions of prompts which included well-known strategies for eliciting step-by-step reasoning (Kojima et al., 2022  ###reference_b34###).\nWe chose the best natural language prompt from our tests and used it for all models. The format of the prompt is provided in the appendix (We also included an example of GPT-4V\u2019s output when accessed through the web interface in Figure 6  ###reference_###, with a slightly modified prompt that allowed it to provide additional reasoning). We used the default parameters (e.g., temperature and top-p) for all the LVLMs we compared.\nUIClip and other CLIP-based models used the inference strategies described in Section 4.2  ###reference_###.\n###figure_5###"
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "5.2. Results",
            "text": "We focused on evaluating three aspects of design assessment: i) UI design quality assessment, ii) design suggestion generation, and iii) design relevance."
        },
        {
            "section_id": "5.2.1",
            "parent_section_id": "5.2",
            "section_name": "5.2.1. Design Quality",
            "text": "We evaluated a model\u2019s design quality assessment by measuring its accuracy in identifying the \u201cpreferred\u201d UI from an example pair.\nThe results of our experiments are shown in Figure 4  ###reference_###.\nIn general, design quality assessment is a difficult task for all tested models, especially when evaluating human-rated pairs of real app screens.\nOur results are shown in Figure 5  ###reference_###.\nFor both BetterApp and JitterWeb, the UIClip variant trained with web pairs performed the best, with an average overall accuracy of 75.12%.\nIn particular, we see large improvements in detecting design defects in web pages (87.1%), which is what the majority of the training process and data focused on.\nThese improvements are notable because CLIP B/32, which was the base model of all UIClip variants, performed the worst out of all baselines. CLIP B/32 performed especially poorly for jittered websites, where a further analysis revealed it erroneously associated certain types of jitters (e.g., dark, unreadable backgrounds) with better design. This suggests that our training procedure and data are effective for learning design quality, especially when compared to other publicly available sources of captioned UIs (e.g., Screen2Words) or general-purpose multi-modal data (e.g., MetaCLIP H/12).\nIncorporating human ratings appeared to lead to slightly degraded performance (overall accuracy of 73.88%), possibly due to insufficient data.\nAs noted previously, the UIClip with CLIP pre-training objective alone was less effective at improving design quality assessment capabilities because paired UI examples are often not found in randomly sampled minibatches. Nevertheless, it had the third-highest overall accuracy of 65.42%.\nDespite their much larger size, all LVLMs performed very poorly on design quality assessment, often around the level of random guessing. Interestingly, GPT-4V (overall 51.58%) refused to provide a response for around 10% of examples, stating \u201cI\u2019m sorry, I can\u2019t help you with that.\u201d\nIn this regard, open models such as LLaVA performed better than GPT-4V, even though LLaVA was trained by distilling GPT-4V output. However, since LLaVA was not trained to refuse requests (Liu et al., 2023  ###reference_b45###), it ended up with a higher overall performance for design choice prediction (with 54.59% prediction accuracy).\n###figure_6###"
        },
        {
            "section_id": "5.2.2",
            "parent_section_id": "5.2",
            "section_name": "5.2.2. Design Suggestions",
            "text": "We evaluated all models\u2019 design suggestion capabilities by comparing the model-generated output to the CRAP principles selected by designers when rating UI quality in BetterApp.\nThere were four possible CRAP principles that could have been chosen for each comparison, which we formulate as a multi-label classification problem with four labels.\nSince designers were allowed to omit reasoning for comparisons, we ignored comparisons where none of the CRAP principles were selected.\nAgain, design suggestion was a challenging task for all tested models.\nSome LVLM baselines listed all four CRAP principles for almost every single example, despite being prompted to only choose the most relevant principles.\nThis appears to be consistent with prior work on using LLMs for heuristic evaluation (Duan et al., 2024  ###reference_b14###), where similar models often provided a large number of irrelevant design suggestions.\nIn our case, this phenomena led to artificially high recall for models such as Gemini (87.11% recall) and GPT-4V (84.57% recall).\nThus, we introduced a choice-adjusted F1 metric that ignored models\u2019 design suggestions if it led to choosing the wrong preferred UI, i.e., right reasoning but wrong answer.\nThis lowered all models\u2019 recall to more realistic levels, e.g., Gemini\u2019s recall was lowered to 49.17% and GPT-4V was lowered to 46.58%.\nSome open LVLM baselines, such as Qwen-VL-Chat, also had trouble following our prompt and often ignored instructions that asked them to provide reasoning for their answers.\nUnder both methods of calculation, UIClip variants had the best performance, with the web pre-trained variant performing the best when all examples were considered and the full UIClip variant performing the best when adjusted for choice accuracy.\nCLIP variants that were trained on other sources of data did not achieve high performance, since their training data did not include information about present design defects that would help them make suggestions."
        },
        {
            "section_id": "5.2.3",
            "parent_section_id": "5.2",
            "section_name": "5.2.3. Design Relevance",
            "text": "Finally, we also evaluated a model\u2019s ability to compute UI relevance, which is useful for assessing designs and for various applications that require example retrieval (Bunian et al., 2021  ###reference_b7###; Huang et al., 2019  ###reference_b28###).\nTo measure UI relevance, we adopted a metric commonly used in information retrieval known as mean reciprocal rank (MRR).\nAn embedding is computed for the preferred screenshot in BetterApp and JitterWeb.\nFor each description in the evaluation set, we appended the prefix \u201cui screenshot. well designed. \u201d and computed its text embedding.\nThe text embedding is used the calculate similarity scores with all screenshots, which is used to sort them in descending order (i.e., highest similarity first).\nThe rank of the first element with the same description was recorded.\nSince a lower rank is desirable (indicating higher similarity with the corresponding image), MRR (higher is better) is computed as the average of all reciprocal ranks.\nA higher MRR indicates better retrieval performance.\nBecause there is no straightforward way to generate rankings from LVLMs, we only evaluate models based on the CLIP architecture.\nThe results of our retrieval evaluation are shown in Table 1  ###reference_###.\nThe variant of UIClip pretrained on JitterWeb using the default CLIP objective achieves the highest MRR score for both BetterApp (0.3851) and JitterWeb (0.4085).\nUIClip variants trained using pairwise loss were the worst performers, with MRRs lower than the original CLIP B/32 base model, because the objective focuses on the design-comparison task.\nIn our discussion, we provide more detailed reasoning for this phenomenon.\nNevertheless, our evaluation still shows that our datasets are useful for learning design relevance, especially when training objectives are closely aligned.\nFor example, applying the CLIP objective to JitterWeb is much more effective than alternate data sources and nearly doubles () the overall retrieval performance of the base CLIP B/32 model."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6. Example Applications",
            "text": "Based on the three capabilities of UIClip that we evaluated, we present a suite of example applications that illustrate how common user-facing design tools can be enhanced with our model."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "6.1. Improving UI Code Generation",
            "text": "###figure_7### We built a web application that allows users to generate rendered UI screenshots from a natural language description of a UI.\nTo use the interface (Figure 7  ###reference_###), users enter their descriptions in a textbox, which is formulated in a prompt. The prompt is fed into an external LLM (OpenAI GPT-3.5), which generates web code (HTML/CSS) using the description. We sampled  different outputs and rendered each into a screenshot using the script that programmatically controlled a browser. If the web code referenced external images, we replaced them with a placeholder image to render. Screenshots were fed into UIClip and were scored against the input prompt. The screenshot of the results ranked in descending score order is returned to the user.\nThis is a simple example of how UIClip could be used to improve the output of generative models, most similar to existing \u201cbest-of-n sampling\u201d approaches.\nThe method can also be incorporated into the additional vision checkup (Sharma et al., 2024  ###reference_b67###) and used for feedback in self-improving generative model outputs (Madaan et al., 2024  ###reference_b48###).\nOur technique is simple to implement and does not require access to the underlying model\u2019s weights; however, it is computationally expensive during inference because multiple candidate solutions must be generated.\nIf model weights are available, this process could be further improved by applying UIClip\u2019s filtering during the training process of generative models, or if UIClip was used as a reward model in reinforcement learning fine-tuning approaches (Ouyang et al., 2022  ###reference_b57###; Gulcehre et al., 2023  ###reference_b22###).\nWe leave these investigations to future work."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "6.2. UI Design Tips",
            "text": "###figure_8### We built a tool that allows users to upload screenshots of UI designs to generate design tips using our model\u2019s design suggestion capabilities.\nWe developed a web application (Figure 8  ###reference_###) that allows users to upload a screenshot of an app or UI design, and our system generates tips that are surfaced to the user, although a similar idea could be better integrated into, for example, UI design applications for improved ease-of-use. One limitation of our current application is that it might suggest improving text contrast, but it is unable to provide additional information for which part of the UI led to the recommendation. This is due to our problem formulation that pairs descriptions with entire screenshots and doesn\u2019t contain location information. Future improvements can help address this by collecting the relevant data and incorporating that into text descriptions, or by sliding a smaller window across the UI screenshot and associating generated design suggestions to the location of the window.\nWe leave these additional features to future work."
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "6.3. UI Example Retrieval",
            "text": "###figure_9### UI design search has been explored by many prior works (Kumar et al., 2013  ###reference_b35###; Deka et al., 2017  ###reference_b12###; Bunian et al., 2021  ###reference_b7###), and it has the potential to accelerate the design process by providing relevant examples that designers use to seek inspiration during early phases of the design process. Existing systems built for UI example retrieval have focused on querying and indexing UI screenshots by their layout (e.g., to support finding designs similar to an exemplar) or captions (e.g., to support natural-language or free-form search).\nSince UIClip contains both an image and text encoder, it is possible to support both of these use cases, although our example application focuses on handling text-based queries.\nOur application uses a similar procedure to our UI relevance evaluation, where model-computed embeddings are used to retrieve and sort screenshots based on the user\u2019s query.\nUIClip\u2019s score can take into account both the relevance and quality of retrieved examples, and we incorporate a negative prompt that biases the query vector away from simple or ambiguous designs (Sanchez et al., 2023  ###reference_b65###).\nWe built a web application that contains a search box where the user enters their query. Figure 9  ###reference_### shows examples of screens retrieved for a set of queries indexed by UIClip and the vanilla CLIP model."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7. Discussion",
            "text": "Our experiments and example applications show that UIClip\u2019s design assessment capabilities can improve many machine-assisted design tools. In this section, we discuss UIClip\u2019s implications, limitations, and directions for future work."
        },
        {
            "section_id": "7.1",
            "parent_section_id": "7",
            "section_name": "7.1. Data-driven Learning of UI Design",
            "text": "Our paper introduces techniques for machine-learning a generalized scoring function (c.f. personalized functions (Gajos and Weld, 2005  ###reference_b18###)) that quantifies aspects UI design.\nWe discuss our work\u2019s data and algorithm contributions.\nWe hypothesized that a large volume of data (millions of examples) is important for effectively learning to assess designs, similar to how seasoned human designers develop their intuition through years of experience.\nThis hypothesis was largely supported by our experimental results.\nWe showed the substantial benefits of training on our large-scale dataset of UI designs, leading to improvements over alternate datasets (e.g., Screen2Words (Wang et al., 2021  ###reference_b76###)) that more human-authored descriptions but fewer overall samples.\nWhen we incorporated our own human-rated BetterApp dataset, we found that in most cases, it did not result in substantial changes, most likely due to insufficient data volume.\nAt the same time, dataset size alone is not enough to ensure good design assessment performance.\nFor example, MetaCLIP H/12 was trained on a total of 2.5 billion pairs.\nOur paper introduces training objectives targeted at different aspects of design quality.\nWe employed two objectives for training UIClip, a batch-wise contrastive objective (i.e., CLIP\u2019s pretraining objective) and a pairwise contrastive objective, designed specifically for quality comparisons.\nBased on our results, the pairwise objective represents a tradeoff where it achieves higher focus on design-comparison tasks by focusing on pairs of relevant screens but incurs a penalty on retrieval-related tasks, since it is not trained to distinguish irrelevant examples from a diverse minibatch.\nWe leave further investigation of how to optimally combine these two training objectives to future work; although given the relatively small size of our model, we believe it would be feasible to use different variations for application-specific scenarios."
        },
        {
            "section_id": "7.2",
            "parent_section_id": "7",
            "section_name": "7.2. Formulating UI Design Quality",
            "text": "UIClip\u2019s current model architecture is designed around the assumption that design quality can be represented by a numerical score.\nHowever, there are many nuances that cannot be captured by this formulation.\nWithin the context of our collected data, we distinguish between assessing UIs for design defects and understanding more subtle design preferences.\nJitterWeb was constructed by introducing \u201cjitters\u201d into web pages, that intentionally violate design guidelines.\nIn these cases, one might expect to more objectively identify the preferred screen, since the alternative screen would contain a defect.\nWe found this case well captured by our formulation, as shown by our models\u2019 higher performance on the JitterWeb test data.\nIn contrast, examples from BetterApp are more representative of design preferences. Many of its samples were real-world apps, which are often designed professionally and less likely to contain design defects.\nTo verify this, we analyzed design quality performance on the subset of synthetic, LLM-generated screens within BetterApp and compared it with the app screens from VINS.\nMany of the LLM-generated screens (as shown in Figure 9  ###reference_###) contain design defects, which potentially led to easier comparisons.\nUIClip\u2019s accuracy for rating the quality of synthetic screens (67.65%) was much higher than for real app screens (57.89%).\nThis trend was true for almost all other tested models, where the average of all models\u2019 accuracy on synthetic screens (56.05%) was higher than real apps (51.50%).\nIt is also possible that UIClip is not trained to detect certain types of design defects present in real-world apps, e.g., violations that require the semantic understanding of content, such as information flow hierarchy.\nBesides the nature of design defects in real-world apps, design preferences could also vary by person.\nFor example, it is reasonable to expect that someone who frequently uses iOS devices might feel more familiar with iOS screenshots over Android ones.\nThe design language of the same platform can change over time, causing corresponding shifts in user perception, e.g., some screenshots in VINS were from the older Rico dataset (Deka et al., 2017  ###reference_b12###).\nWhile UIClip is currently not designed to support more personalized use cases, we envision that it could be finetuned with user-provided preference paired (Gajos and Weld, 2005  ###reference_b18###) or augmented so that it incorporates platform-specific design into its prompt, e.g., \u201candroid material design screenshot. well-designed.\u201d"
        },
        {
            "section_id": "7.3",
            "parent_section_id": "7",
            "section_name": "7.3. Supporting UI Design Applications",
            "text": "Related to our problem formulation is the types of design-assistance tasks that UIClip can support.\nDespite only producing a numerical score as output, we introduced inference techniques that extend beyond simple UI scoring and allow for a limited generation of natural language design suggestions.\nWe developed three example applications that demonstrate how UIClip could facilitate some forms of automated design assistance.\nWhile we did not conduct formal user evaluations of our example applications, we built these applications based on validated systems described in the literature, which suggest they would provide value to users.\nHowever, there are many types of design assistance that are not yet supported by UIClip.\nFor example, while UIClip can infer the presence of design defects in a screenshot, there is currently no straightforward method to localize them (e.g., footer bar has poor color contrast).\nWe believe this capability is important for practical use since it provides cues for designers to address the detected flaws. One promising approach, previously applied to other visual design tasks (Schoop et al., 2022  ###reference_b66###), is to augment our current model with model explainability frameworks to understand which parts of the image contribute to predictions.\nFuture iterations of the UIClip could also be trained on sub-windows of a UI for finer-grain inference of fault location, similar to how object detection architectures work.\nFinally, UIClip could be fine-tuned with more detailed natural language descriptions that associate spatial information with predicted flaws (e.g., \u201cbad color contrast in footer bar\u201d) or even provide suggested fixes (e.g., \u201cbad color contrast. make footer darker\u201d), although this would necessitate a more complex inference algorithm.\nRecent trends in machine learning suggest that model architectures that generate free-form text can be more easily scaled and provide more flexible feedback.\nOur evaluation found that VLMs generally performed poorly, and prior work suggests that LLMs are prone to providing irrelevant design suggestions (Duan et al., 2024  ###reference_b14###).\nA qualitative assessment of current LLM responses (Figure 6  ###reference_###) suggests that current LLMs produce realistic-sounding but inaccurate reasoning.\nHowever, we believe that our work could be useful in improving foundation models such as LVLMs.\nFor example, the UIClip model could be used as a \u201creward model\u201d that guides their UI generation (Section 6.1  ###reference_###) and design assessment capabilities.\nFurthermore, our datasets could be reformatted and used to fine-tune LVLMs for UI-related tasks (Li and Li, 2022  ###reference_b41###).\nFinally, most machine learning models, including UIClip and LVLMs are limited in that they can only process a single state of the UI (i.e., a screenshot) when responding to text prompts.\nBecause of this limitation, our current approach focuses on assessing the visual design of a single screen using the CRAP visual design principles.\nA more holistic evaluation of both UI design and usability depends on a deeper understanding of interface functionality and app navigation flows, which requires both observation and interaction.\nTo support this, we envision that models like UIClip, could be integrated into interactive systems, such as crawlers, that can interact with and explore different parts of an entire app (Wu et al., 2023a  ###reference_b80###; Swearngin et al., 2023  ###reference_b72###)."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "8. Conclusion",
            "text": "In this paper, we introduce a computational model called UIClip that automatically assesses various aspects of UI design: i) design quality, ii) UI relevance, and iii) is capable of generating design suggestions based on predicted defects. Our model is trained from a large-scale dataset of 2.3 million UIs that we collected and augmented with synthetic and human ratings of design quality. In an evaluation with several strong baselines, we demonstrate our model\u2019s performance in UI design understanding in our three target UI tasks, showing that UIClip outperforms all other baselines in all tasks. Finally, we introduce three example applications that demonstrate how UIClip can facilitate novel applications through its automated design assessment capabilities: i) UI code generation, ii) UI design tips generation, and iii) quality-aware UI example search. Overall, our work demonstrates the process of effectively encoding design awareness into the computational models."
        }
    ],
    "url": "http://arxiv.org/html/2404.12500v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2",
            "2.3"
        ],
        "methodology_sections": [
            "4",
            "4.1",
            "4.1.1",
            "4.1.2",
            "4.2",
            "4.2.1",
            "4.2.2",
            "4.2.3"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.1",
            "5.1.1",
            "5.1.2",
            "5.2",
            "5.2.1",
            "5.2.2",
            "5.2.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.2.1",
            "4.1",
            "4.1.1",
            "4.1.2",
            "5.1.1",
            "5.1.2",
            "5.2.1",
            "5.2",
            "7.1"
        ]
    },
    "research_context": {
        "paper_id": "2404.12500v1",
        "paper_title": "UIClip: A Data-driven Model for Assessing User Interface Design",
        "research_background": "The paper \"UIClip: A Data-driven Model for Assessing User Interface Design\" is motivated by the inherent challenges in defining and evaluating good UI design, particularly for novice designers. Traditional guidelines and principles are often hard to apply, and usability testing, though comprehensive, is costly and time-intensive. Even heuristic evaluations, which are more feasible, require expert knowledge and consistency in application. This gap has led to persistent design flaws in many modern UIs, affecting usability, accessibility, and aesthetics.\n\nThe research problem addressed in the paper is the need for a robust and scalable method to automatically assess and enhance UI design quality. Existing computational approaches either focus on specific aesthetic and cognitive metrics, requiring interpretation without providing direct comparisons between designs, or are too user-specific, lacking general applicability.\n\nRelevant prior work includes guiding principles by Nielsen (1992) and Shneiderman et al. (2016), as well as computational approaches by Miniukovich and De Angeli (2015) and Oulasvirta et al. (2018, 2022) that assess visual aesthetics and cognitive principles in UI design. Previous toolkits, like those developed by Gajos and Weld (2004, 2005), aim at user-specific adaptive interfaces but do not generalize well to broader UI design tasks. The CLIP vision-language model (Radford et al., 2021) is a foundational tool leveraged by the authors, albeit modified to suit UI quality assessments.\n\nTo address these limitations, the paper introduces UIClip, a novel computational model based on CLIP, that evaluates UI design quality from screenshots paired with natural language descriptions. The model is trained using a large synthetically generated dataset of UIs ranked by design quality, supplemented with real-world ratings from professional designers. The paper benchmarks UIClip against other large vision-language models, demonstrating superior performance in design quality evaluation, improvement suggestion generation, and design relevance tasks. UIClip's utility is showcased through three applications: enhancing UI code quality, generating UI design suggestions, and a UI design search engine.\n\nUltimately, the contributions of the paper include a large-scale UI design dataset, a robust computational model for UI quality assessment, and demonstrated applications facilitating downstream UI design improvements. To promote further research, the authors plan to make their training code, data, and models publicly available.",
        "methodology": "The proposed method, UIClip, is a data-driven computational model designed to assess user interface (UI) designs from screenshots. This model is trained using the JitterWeb and BetterApp datasets. While the datasets can technically be used to train various models, such as large vision-language models that involve language decoders from large language models (LLMs), the chosen architecture for UIClip is the CLIP (Contrastive Language-Image Pre-training) model developed by OpenAI.\n\nKey components and innovations of UIClip include:\n\n1. **CLIP Architecture**: \n   - **Model Selection**: The model is based on the smallest variation of CLIP, specifically CLIP B/32. This model includes 151 million parameters, making it relatively efficient for both training and inferencing.\n   - **Dual-encoder Transformer Model**: CLIP B/32 consists of two main components\u2014an image encoder and a text encoder:\n     - **Image Encoder**: A vision transformer that processes a fixed-size 224x224 image, splits it into 32x32 pixel patches, and encodes each patch into a 512-dimensional embedding.\n     - **Text Encoder**: A transformer that processes text sequences up to 77 tokens long, converting them into a 512-dimensional embedding.\n   - **Shared Embedding Space**: The outputs of the image and text encoders are projected into a shared embedding space. The dot product of these embeddings produces a numerical score, which represents the semantic similarity between the image and text inputs.\n\n2. **Training and Fine-tuning UIClip**:\n   - **Data Sources**: The JitterWeb and BetterApp datasets are used to train the model specifically on UI screenshots, as these are underrepresented in the original CLIP training data.\n   - **Training Objectives**:\n     - **Relevance Scoring**: Improving the model's ability to score the relevance of UI screenshots and their corresponding textual descriptions.\n     - **Design Quality Evaluation**: Incorporating an assessment of design quality into the scoring mechanism.\n     - **Defect Associations**: Associating textual descriptions of design defects with the corresponding UI screenshots that display these issues.\n\n3. **Efficiency and Versatility**: The adoption of the CLIP architecture, specifically the smaller B/32 variation, ensures that UIClip is both efficient and versatile enough to support various application scenarios such as example retrieval and scoring.\n\nUIClip is essentially a finetuned version of CLIP, optimized for evaluating user interfaces by focusing on relevance, design quality, and defect associations within the specific domain of UI screenshots.",
        "main_experiment_and_results": "**Main Experiment Setup:**\n\n**Tasks:**\n1. **Design Quality Assessment**\n2. **Design Suggestion Generation**\n3. **Design Relevance**\n\n**Datasets:**\n- The specifics of the datasets used aren't provided in the provided text. Typically, datasets for these tasks would include vast collections of user interface designs annotated for quality, suggestions, and relevance.\n\n**Baselines:**\n- Several state-of-the-art baseline models are used for comparison. Though the exact models are not detailed, they can be inferred to be models typically employed for similar design tasks and are noted to be significantly larger in size compared to UIClip.\n\n**Evaluation Metrics:**\n- The precise metrics used aren't mentioned, but common metrics might include accuracy, precision, recall, F1-score, and possibly some form of relevance or quality scores specific to UI design.\n\n**Main Experimental Results:**\n- UIClip outperforms the baseline models across all three tasks (design quality assessment, design suggestion generation, and design relevance). This performance is noted despite the baseline models being several orders of magnitude larger, indicating UIClip's efficiency and effectiveness in UI design assessment.\n\nThis description captures the setup and results for the main experiment without detailing the ablation studies meant to evaluate specific module contributions."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the impact of training objectives on the performance of UIClip in assessing the design quality of user interfaces.",
            "experiment_process": "UIClip was trained using two different objectives: the standard CLIP pretraining objective and a pairwise contrastive objective. The pre-training stage utilized the JitterWeb dataset and the same training objective as the original CLIP implementation. In the second stage, the objective was switched to a loss function that focuses on distinguishing good from bad UI designs. Both stages were repeated for the BetterApp dataset. An evaluation was then performed comparing UIClip variants trained with each objective to other baselines, including various large vision-language models and other improved CLIP models.",
            "result_discussion": "The UIClip variant trained with web pairs performed the best overall in assessing design quality, particularly excelling in detecting design defects in web pages with an overall accuracy of 75.12%. This suggests that the training procedure and data significantly aid in learning design quality. The standard CLIP pre-training objective was less effective, and incorporating human ratings slightly degraded performance due to insufficient data. Larger vision-language models performed poorly compared to UIClip, showcasing the advantage of the specialized training objectives and datasets used for UIClip.",
            "ablation_id": "2404.12500v1.No1"
        },
        {
            "research_objective": "To compare the effectiveness of different datasets and data preprocessing methods for training UIClip on design quality assessment.",
            "experiment_process": "Different datasets such as JitterWeb and BetterApp were used to train UIClip. The BetterApp dataset involved collecting UI screenshots rated by human designers, implementing a custom-built web application to randomly select UI pairs for ranking based on design quality. Training splits for BetterApp were made with 70% for training, 10% for validation, and 20% for testing. The pre-processing included methods like random-crop strategy, drawing from approaches in the original CLIP implementation and subsequent reproductions.",
            "result_discussion": "Training on the large-scale JitterWeb dataset led to significant improvements in UIClip's performance compared to other datasets like Screen2Words, which has more human-authored descriptions but fewer samples. The addition of the BetterApp dataset did not substantially change performance due to insufficient data volume. These results indicate that a large volume of data is crucial for effectively learning to assess designs, supporting the hypothesis.",
            "ablation_id": "2404.12500v1.No2"
        },
        {
            "research_objective": "To determine the impact of pairwise contrastive learning on the design quality assessment capabilities of UIClip.",
            "experiment_process": "A pairwise contrastive objective was implemented to compare related UI screens, wherein the preferred screenshot was embedded to have a higher dot product with a description indicating good design. This method was evaluated against the traditional CLIP objective, which uses randomly sampled minibatches from the entire training dataset. Performance comparison was then conducted between models trained with these objectives across design quality, suggestion generation, and relevance tasks.",
            "result_discussion": "The pairwise contrastive objective resulted in a better performance for design quality assessment tasks, especially in cases where related pairs of UI screenshots were necessary for learning relative design quality. UIClip trained with this objective had the highest accuracy, substantially outperforming the standard CLIP objective. This highlights the advantage of pair-wise training for tasks involving direct comparisons between related designs, despite incurring some penalties on retrieval-related tasks.",
            "ablation_id": "2404.12500v1.No3"
        }
    ]
}