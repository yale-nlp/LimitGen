{
    "title": "Learning Diverse Attacks on Large Language Models for Robust Red-Teaming and Safety Tuning",
    "abstract": "Red-teaming, or identifying prompts that elicit harmful responses, is a critical step in ensuring the safe and responsible deployment of large language models (LLMs). Developing effective protection against many modes of attack prompts requires discovering diverse attacks. Automated red-teaming typically uses reinforcement learning to fine-tune an attacker language model to generate prompts that elicit undesirable responses from a target LLM, as measured, for example, by an auxiliary toxicity classifier. We show that even with explicit regularization to favor novelty and diversity, existing approaches suffer from mode collapse or fail to generate effective attacks. As a flexible and probabilistically principled alternative, we propose to use GFlowNet fine-tuning, followed by a secondary smoothing phase, to train the attacker model to generate diverse and effective attack prompts. We find that the attacks generated by our method are effective against a wide range of target LLMs, both with and without safety tuning, and transfer well between target LLMs. Finally, we demonstrate that models safety-tuned using a dataset of red-teaming prompts generated by our method are robust to attacks from other RL-based red-teaming approaches.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The deployment of large language models (LLMs) in the wild has raised concerns about their potential harmful impacts for nearly a decade [31  ###reference_b31###, 59  ###reference_b59###]. These concerns have grown with the increasing capabilities of LLMs: even models fine-tuned to satisfy certain safety constraints can be manipulated to produce toxic outputs [57  ###reference_b57###]. Red-teaming, or identification of \u2018attack\u2019 prompts that elicit undesirable responses, gives model developers a chance to address such vulnerabilities before deployment [42  ###reference_b42###]. This paper studies the problem of automatically generating diverse attack prompts for LLMs and argues for the potential of robust automated red-teaming in the development of effective defenses.\nEffective red-teaming requires identifying many modes of attack [20  ###reference_b20###]. Methods for automated red-teaming based on stochastic optimization of attack prompts [68  ###reference_b68###, 65  ###reference_b65###] have been proposed, while others have used reinforcement learning (RL) to train an attacker language model (LM), allowing to generate novel prompts efficiently at test time [42  ###reference_b42###, 20  ###reference_b20###]. However, even when regularized to favor diversity, these methods struggle to balance between diversity and effective attacks (Fig. 2  ###reference_###). They often suffer from mode collapse, where the attacker LLM generates a small set of similar prompts, or overly focus on diversity and fail to generate effective attacks (Fig. 3  ###reference_###). Moreover, we have empirically found that they also fail to discover attacks that persist across different target LLMs (Table 2  ###reference_###).\nThis paper takes an amortized inference perspective on red-teaming: we view the problem of generating an attack prompt as one of sampling a latent variable in a probabilistic model. Using the off-policy RL approach of GFlowNet fine-tuning, proposed for inference of linguistic latent variables in [23  ###reference_b23###], we fine-tune attack LMs to sample the full posterior distribution over attack prompts.\nHowever, controlling the \u2018peakiness\u2019 of the posterior distribution \u2013 the preference of attack quality to attack diversity \u2013 is challenging, especially when red-teaming a target LLM that has been safety-tuned to resist some modes of attack, leading to a sparser landscape of attack prompts. Inspired by the success of behavior cloning in offline RL [16  ###reference_b16###, 27  ###reference_b27###] and reinforcement learning for human feedback [RLHF; 34  ###reference_b34###], we propose a two-stage GFlowNet fine-tuning procedure with MLE smoothing. As illustrated in Fig. 1  ###reference_###, we first fine-tune a pretrained attacker LM with a GFlowNet objective and collect high-reward attack prompts discovered in the course of training (Step 1). The collected prompts form an offline dataset. Subsequently, the pretrained attacker model is fine-tuned again to maximize the likelihood of the offline dataset (Step 2). The first stage, GFlowNet fine-tuning, enables us to collect a set of diverse and effective attack prompts using exploratory off-policy training. In the second phase, we obtain a smooth distribution over high-reward attack prompts, since all the collected attack prompts in the offline dataset are considered equally important and the attacker LM is trained to maximize their log-likelihood uniformly. Consequently, we find that the attacker LM is able to sample attack prompts that are both diverse and effective.\nWe empirically evaluate the efficacy of our proposed method in red-teaming four target LLMs: GPT-2 [43  ###reference_b43###], dolly-v2-7b [13  ###reference_b13###], Gemma-2b-it [39  ###reference_b39###], and Llama-2-7b-chat [51  ###reference_b51###]. Our approach is found to sample more diverse and effective attack prompts than other relevant baselines. Moreover, many of our attack prompts effectively transfer to other target LLMs that are not used for training the attacker model, such as Llama-2-13b/70b-chat, Llama-3-8b/70b-instruct [1  ###reference_b1###], Starling-7b-beta [66  ###reference_b66###], and Mistral-7b-instruct-v0.2 [28  ###reference_b28###]. Lastly, we fine-tune a target LLM to generate refusal responses to the discovered attack prompts and find that the model fine-tuned with our red-teaming prompts is more robust than the models safety-tuned with other RL-based red-teaming methods.\nIt is important to note that while we study an approximate measure of toxicity as a proxy for harmfulness, following past works [42  ###reference_b42###, 20  ###reference_b20###], the true harmful impact of an LLM output is often subjective and dependent on the social context of deployment [59  ###reference_b59###]. We nonetheless believe that the methods we propose will be useful in practice and can be extended to other measures of harmfulness.\nOur contributions and findings are summarized below:\nTo generate diverse and effective attack prompts, we take a probabilistic perspective on red-teaming and demonstrate the usefulness of the off-policy RL approach of GFlowNet fine-tuning.\nWe propose a smoothing and reranking step that can be used to generalize from high-reward samples found during GFlowNet fine-tuning, improving the attacker model and allowing efficient adaptation to new target LLMs.\nAttacker LMs trained with GFlowNet-finetuning followed by MLE generate more diverse and effective attack prompts that also transfer to other target LLMs.\nWhen safety-tuned on attack prompts generated by our method, target LLMs become robust to attacks generated with other RL-based methods without performance degradation on other tasks.\n###figure_1###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related work",
            "text": "As LLMs increase in general capabilities and performance, so does the risk associated to potential misuse of LLMs. To mitigate this, LLMs are often trained to refuse to generate content given prompts that are considered boring, offensive, or harmful. This is done at various stages of the training process such as filtering out harmful training data [39  ###reference_b39###] or fine-tuning on \u2018safe\u2019 responses to harmful prompts [51  ###reference_b51###]. This process is often augmented by red-teaming, which proactively looks for ways to elicit harmful behavior from models. Prior works [15  ###reference_b15###, 61  ###reference_b61###, 54  ###reference_b54###] rely on a large amount of human annotation to identify vulnerabilities of LMs. To automate red-teaming, Perez et al. [42  ###reference_b42###] formulate red teaming as an RL problem and train an LM to sample toxic prompts. However, most RL algorithms are not suitable for sampling diverse objects since they tend to converge to a single reward-maximizing trajectory. To overcome this limitation,  Hong et al. [20  ###reference_b20###] propose using a novelty-based reward to encourage a policy to explore diverse samples during RL training. Instead of generating a prompt from scratch, Lee et al. [30  ###reference_b30###] replace words of prompts from a predefined user input pool to attack LMs using Bayesian optimization in a sample-efficient manner. As a concurrent work, Rainbow Teaming [47  ###reference_b47###] sample an attack prompt from a pool and iteratively mutate the prompt with auxiliary LLMs. Jailbreaking and red-teaming are closely related in that red-teaming proactively tries to discover vulnerabilities for the purpose of improving model safety, whereas jailbreaking generally refers to circumventing the built-in safeguards of models. Initially, jailbreaks were found manually through trial and error, taking advantage of the different objectives models were trained against [57  ###reference_b57###]. Recently, automated jailbreak attacks are becoming increasingly popular. They utilize techniques such as genetic algorithms [35  ###reference_b35###], iterative gradient-based methods [68  ###reference_b68###], or automated prompting via auxiliary LLMs [9  ###reference_b9###] to optimize query prompts. Mazeika et al. [37  ###reference_b37###] propose a method defending against GCG [68  ###reference_b68###], which is one of the most popular gradient based jailbreak methods. A drawback of these methods is the computational cost since the optimization has to be performed separately for each new query prompt. Another drawback is the transferability of jailbreaks. Meade et al. [38  ###reference_b38###] have shown that prompts optimized by GCG to jailbreak one target LLM do not transfer to jailbreak other target LLMs. Generative flow networks [GFlowNets; 5  ###reference_b5###] are a probabilistic framework to train stochastic policies to sample discrete compositional objects (e.g., graphs, sequences) proportionally to a reward. Sampling objects proportionally to a reward results in diverse high-reward samples. Consequently, GFlowNets have found applications in a wide variety of problems including biological sequence generation [25  ###reference_b25###], combinatorial optimization [63  ###reference_b63###, 64  ###reference_b64###], Bayesian structure learning [14  ###reference_b14###], variational EM with discrete latent variables [22  ###reference_b22###], and probablistic neurosymbolic inference [52  ###reference_b52###]. Most closely related to our work is [23  ###reference_b23###], which uses the GFlowNet objective to fine-tune LMs for solving intractable inference problems such as sampling chains of thought [58  ###reference_b58###]. We use GFlowNet fine-tuning as a part of our approach for learning policies which generate diverse prompts that elicit toxic responses from target LLMs."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Sampling diverse attacks with GFlowNet fine-tuning",
            "text": "GFlowNets are diversity-seeking RL algorithms that learn a policy  which samples prompts with a probability proportional to the reward associated with the prompt111In the case of generating sequences, GFlowNets are equivalent to MaxEnt RL algorithms [18  ###reference_b18###].. We define the reward for a prompt  as follows:\nwhere  and  are positive constants that control the \u201cpeakiness\u201d (tempering) of toxicity score  and the reference LM likelihood , respectively. The prompt , consisting of  tokens with a special token  indicating the beginning of a sentence, is generated autoregressively from a behavior policy, which is a mix of  and a tempered variant of it. We define  as a state in the generative process and the token sampled from the policy at each step is the action. To learn the parameters , we use the trajectory balance learning objective [36  ###reference_b36###]:\nwhere  is a learnable scalar approximating the partition function.\nOne distinction of the red-teaming setting, compared to other GFlowNet tasks, is that the reward is stochastic as it depends on the response sampled from the LLM. In practice, we approximate the log reward  with an empirical mean over  samples from the target LLM:\nAs we illustrate in \u00a74  ###reference_###, simply using GFlowNet fine-tuning to sample effective and diverse red-teaming prompts can be challenging in practice due to non-trivial choice of the temperature parameters  and . While in principle there are choices of  and  which can balance the reward and diversity well, in practice GFlowNet fine-tuning can be overly sensitive to the peakiness of reward [29  ###reference_b29###]. Moreover, balancing between  and  is non-trivial. The first two examples, shown in \u00a73.2  ###reference_.SSS0.Px1###, get a low total reward compared to the last one, even though they are grammatically valid sentences and achieve high toxicity rewards since they are assigned numerically low likelihood by .\nIf we try much smaller  to focus on toxicity reward , the policy  will likely generate prompts from potentially spurious modes of the toxicity classifier, which will have high perplexity. On the other hand, if we set  to a small value, the model would merely focus on  and not generate toxic prompts.\nTo reduce sensitivity to the aforementioned parameters of the reward distribution, while preserving the mode coverage and ability of the training procedure to generalize to new modes, we propose an inexpensive retraining step following GFlowNet fine-tuning. This second step akin to behavior cloning [10  ###reference_b10###, 16  ###reference_b16###, 27  ###reference_b27###] in RL, where a policy is trained to imitate expert trajectories. First, we store all prompts sampled by the policy  during GFlowNet fine-tuning in Stage 1. We expect this set to contain a diverse and high reward prompts discovered by off-policy exploration during GFlowNet fine-tuning.\nSubsequently, we filter the prompts in this set based on the toxicity score  and language model likelihood  being larger than some thresholds. The collected examples form an offline dataset, and the reference policy is fine-tuned again (from the same initial state as in Stage 1) to maximize log-likelihood of samples from the offline dataset. We outline our complete method in Alg. 1  ###reference_###. This procedure results in a smooth approximation to the distribution over high-reward prompts found during exploratory training."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Preliminaries",
            "text": "We denote the target large language model (LLM) as , which samples a text response  for a given prompt , with probability of . The goal of red-teaming an LLM is to identify prompts  that elicit toxic responses from the target LLM. A binary toxicity classifier, denoted as , is used to quantify the effectiveness of a prompt . Specifically, the likelihood of the response being classified as toxic by the classifier , where  is a binary variable denoting toxicity, is used to measure the effectiveness of a prompt . Moreover, for the attack to be effective, the prompts  should appear natural, as unnatural prompts (with high perplexity) are easy to defend against with simple filters [26  ###reference_b26###]. Red-teaming can often be a time-consuming process if done manually as the space of prompts is quite large. Perez et al. [42  ###reference_b42###], Hong et al. [20  ###reference_b20###] formulate red-teaming as an RL problem, to automate the discovery of these prompts. This involves training an autoregressive language model (LM) as a policy, , parameterized by , to generate prompts that maximize the expected reward (as measured by the toxicity of the response generated by the target LLM):\nwhere  and  is a KL-divergence between the policy  and a pretrained LM  is the constraint which enforces the prompt  to be close to natural language text. However, most RL algorithms are not suitable for discovering diverse prompts since they generally concentrate most of probability mass of the policy  on actions with highest reward, often resulting in a deterministic policy that generates a single prompt [5  ###reference_b5###]. While Hong et al. [20  ###reference_b20###] propose adding a novelty-based reward term along with entropy bonus [48  ###reference_b48###] as a regularization to encourage the policy to generate diverse prompts, empirically we find that it is challenging to find an optimal trade-off between diversity and toxicity rate even with the regularization."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "GFlowNet fine-tuning and smoothing with MLE on collected high-reward prompts",
            "text": "We propose a two-stage approach designed to learn a stochastic policy to sample diverse and effective prompts for red-teaming. The first stage consists of fine-tuning a pretrained LM  as a generative flow network [GFlowNet; 5  ###reference_b5###, 6  ###reference_b6###, 23  ###reference_b23###] policy in order to collect sample prompts, and the second stage restarts fine-tuning from the original pretrained LM policy but this time with maximum likelihood estimation (MLE) on the high-reward prompts collected during GFlowNet training in the first stage.\nGFlowNets are diversity-seeking RL algorithms that learn a policy  which samples prompts with a probability proportional to the reward associated with the prompt111In the case of generating sequences, GFlowNets are equivalent to MaxEnt RL algorithms [18  ###reference_b18###  ###reference_b18###].. We define the reward for a prompt  as follows:\nwhere  and  are positive constants that control the \u201cpeakiness\u201d (tempering) of toxicity score  and the reference LM likelihood , respectively. The prompt , consisting of  tokens with a special token  indicating the beginning of a sentence, is generated autoregressively from a behavior policy, which is a mix of  and a tempered variant of it. We define  as a state in the generative process and the token sampled from the policy at each step is the action. To learn the parameters , we use the trajectory balance learning objective [36  ###reference_b36###  ###reference_b36###]:\nwhere  is a learnable scalar approximating the partition function.\nOne distinction of the red-teaming setting, compared to other GFlowNet tasks, is that the reward is stochastic as it depends on the response sampled from the LLM. In practice, we approximate the log reward  with an empirical mean over  samples from the target LLM:\nAs we illustrate in \u00a74  ###reference_###  ###reference_###, simply using GFlowNet fine-tuning to sample effective and diverse red-teaming prompts can be challenging in practice due to non-trivial choice of the temperature parameters  and . While in principle there are choices of  and  which can balance the reward and diversity well, in practice GFlowNet fine-tuning can be overly sensitive to the peakiness of reward [29  ###reference_b29###  ###reference_b29###]. Moreover, balancing between  and  is non-trivial. The first two examples, shown in \u00a73.2  ###reference_.SSS0.Px1###  ###reference_.SSS0.Px1###, get a low total reward compared to the last one, even though they are grammatically valid sentences and achieve high toxicity rewards since they are assigned numerically low likelihood by .\nIf we try much smaller  to focus on toxicity reward , the policy  will likely generate prompts from potentially spurious modes of the toxicity classifier, which will have high perplexity. On the other hand, if we set  to a small value, the model would merely focus on  and not generate toxic prompts.\nTo reduce sensitivity to the aforementioned parameters of the reward distribution, while preserving the mode coverage and ability of the training procedure to generalize to new modes, we propose an inexpensive retraining step following GFlowNet fine-tuning. This second step akin to behavior cloning [10  ###reference_b10###  ###reference_b10###, 16  ###reference_b16###  ###reference_b16###, 27  ###reference_b27###  ###reference_b27###] in RL, where a policy is trained to imitate expert trajectories. First, we store all prompts sampled by the policy  during GFlowNet fine-tuning in Stage 1. We expect this set to contain a diverse and high reward prompts discovered by off-policy exploration during GFlowNet fine-tuning.\nSubsequently, we filter the prompts in this set based on the toxicity score  and language model likelihood  being larger than some thresholds. The collected examples form an offline dataset, and the reference policy is fine-tuned again (from the same initial state as in Stage 1) to maximize log-likelihood of samples from the offline dataset. We outline our complete method in Alg. 1  ###reference_###  ###reference_###. This procedure results in a smooth approximation to the distribution over high-reward prompts found during exploratory training."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We validate the efficacy of our method by red-teaming a variety of open source LMs with varying levels of saferty-tuning. Recall that the goal of red-teaming is to generate diverse prompts that elicit harmful responses from a target LLM . In our experiments, we use GPT-2  ###reference_a### [43  ###reference_b43###], dolly-v2-7b  ###reference_-7b### [13  ###reference_b13###], Llama-2-7b-chat  ###reference_7b-chat-hf### [51  ###reference_b51###], and Gemma-2b-it  ###reference_### [39  ###reference_b39###] as target LLMs. Note that we only require black-box access, i.e., we can only query a prompt and get a text response.\nTo measure effectiveness of each method, we compute the toxicity rate which is the percentage of generated prompts that are toxic.\nWe consider a prompt toxic if the toxicity classifier  assigns a score greater than 0.5 to an input pair consisting of the prompt  and response  generated by the target LLM. For , we use a RoBERTa hate speech classifier [53  ###reference_b53###] for GPT-2 and dolly-v2-7b, and Llama-Guard [24  ###reference_b24###] for Llama-2-7b-chat and Gemma-2b-it. To measure the diversity of the generated prompts, we embed the generated prompts using MiniLMv2  ###reference_rs/all-MiniLM-L6-v2### [55  ###reference_b55###] encoder and compute the average pairwise cosine distance between embeddings of the prompts.\nWe compare our proposed method against some relevant red-teaming baselines:\nSupervised Fine-tuning (SFT): We fine-tune the pretrained LM  with a maximum likelihood objective on 3,003 toxic prompts from SafetyDataset [7  ###reference_b7###] and AdvBench [68  ###reference_b68###].\nIn-Context Learning (ICL) [8  ###reference_b8###]: We sample 5-shot demonstrations from toxic prompt datasets (SafetyDataset and AdvBench) and prompt GPT-2 to generate a prompt.\nREINFORCE [60  ###reference_b60###]: We fine-tune the pretrained LM  as an RL policy with policy gradients to optimize the reward in Equation 1  ###reference_###.\nPPO + Novelty [20  ###reference_b20###]: This method adds entropy bonus [48  ###reference_b48###] along with a novelty-based term to the reward in Equation 1  ###reference_### and train the policy  with proximal policy optimization (PPO) [49  ###reference_b49###]. For novelty-based reward, it utilizes self-BLEU [67  ###reference_b67###] and pairwise cosine similarity between embeddings of all the past generated prompts.\nGFlowNet [36  ###reference_b36###]: We fine-tune the pretrained LM  with Equation 3  ###reference_###.\nGFlowNet + MLE: This is our full method for collecting high-reward prompts during GFlowNet fine-tuning and re-train the pretrained LM  with maximum likelihood estimation (MLE) on the collected prompts as described in Alg. 1  ###reference_###.\n###figure_2### ###figure_3### ###figure_4### ###figure_5### As the number of prompts which would elicit toxic responses occupy a small subset of all possible sequences, there is a natural trade-off between diversity and toxicity. We start by investigating how each method handles this trade-off. Fig. 2  ###reference_### illustrates the cosine distance plotted against the toxicity rate for 1,024 red-teaming prompts generated by each method across four different target LLMs. We find that our GFlowNet + MLE is the only method which manages to balance a high toxicity rate with the diversity of generated prompts across all four target LLMs. Qualitative assessment of examples generated by GFlowNet + MLE, included in Table B.3  ###reference_###, Table B.4  ###reference_###, Table B.5  ###reference_###, and Table B.6  ###reference_###, supports the numerical results. While the GFlowNet achieves both high diversity and toxicity rate for red-teaming GPT-2 (2(a)  ###reference_sf1###) and dolly-v2-7b (2(b)  ###reference_sf2###), the toxicity rate drops significantly for the target LLMs with safety fine-tuning: Gemma-2b-it (2(c)  ###reference_sf3###) and Llama-2-7b-chat (2(d)  ###reference_sf4###). We hypothesize this drop comes from the reward signal (toxicity of responses from the target) becoming sparse with safety-tuned models. Similarly, PPO + Novelty fails to find a balance between diversity and toxicity. When it is able to find effective prompts (2(a)  ###reference_sf1### and 2(b)  ###reference_sf2###) they are not as diverse and for safety fine-tuned models it fails to find any prompts which elicit a toxic response (2(c)  ###reference_sf3### and 2(d)  ###reference_sf4###). (For context, a random policy would have the highest diversity but would have a low toxicity rate).\nOn the other hand, REINFORCE, which does not take account diversity into the reward function, collapses to deterministically generating a single reward-maximizing prompt. Finally, SFT and ICL generate diverse but ineffective prompts.\n###figure_6### Gemma-2b-it\nLlama-2-7b-chat\nLlama-2-13b-chat\nLlama-2-70b-chat\nLlama-3-8b-instruct\nLlama-3-70b-instruct\nGemma-7b-it\nGemma-1.1-2b-it\nGemma-1.1-7b-it\nMistral-7b-instruct-v0.2\nStarling-7b-beta\nTo further understand the behavior of each method beyond the toxicity rate (which depends on the  decision boundary), we illustrate the distribution over the toxicity scores and corresponding average pairwise cosine distances for the generated prompts in Fig. 3  ###reference_###, obtained from the experiment for red-teaming the Llama-2-7b-chat target LLM. Results for the other target LLMs are illustrated in Fig. B.1  ###reference_###, Fig. B.2  ###reference_###, and Fig. B.3  ###reference_### in \u00a7B.1  ###reference_###. GFlowNet + MLE achieves consistently high diversity across different toxicity score bins. On the other hand, all other methods fail to achieve high diversity and toxicity at the same time. GFlowNet generates fewer toxic prompts compared to GFlowNet + MLE.\nNotably, PPO + Novelty does not generate prompts with the toxicity score greater than  at all for Gemma-2b-it and Llama-2-7b-chat. While REINFORCE generates a single highly toxic prompt achieving a much lower diversity, SFT and ICL generate few toxic prompts.\nA potential advantage of generating diverse attack prompts is that prompts generated for red-teaming a given target LLM can potentially transfer to other LLMs, since some of the failure modes of a target LLM might be shared by other models, for instance, due to using similar web-filtered data or similar safety alignment recipes.\nTo study this empirically, we train an attacker policy  for red-teaming the Gemma-2b-it as the target LLM. We then sample  prompts from the trained attacker LM and evaluate the number of prompts which transfer to other LLMs, i.e., elicit toxic responses from unseen LLMs: Llama-2-7b-chat  ###reference_7b-chat-hf###, Llama-2-13b-chat  ###reference_13b-chat-hf###,\nLlama-2-70b-chat  ###reference_70b-chat-hf###,\nLlama-3-8b-instruct  ###reference_ma-3-8B-Instruct### [1  ###reference_b1###],\nLlama-3-70b-instruct  ###reference_ma-3-70B-Instruct###,\nGemma-7b-it  ###reference_###, Gemma-1.1-2b-it  ###reference_-it###, Gemma-1.1-7b-it  ###reference_-it###, Mistral-7b-instruct-v0.2  ###reference_-it### [28  ###reference_b28###], and Starling-7b-beta  ###reference_LM-7B-beta### [66  ###reference_b66###]. As shown in Table 2  ###reference_###, we find that many prompts generated by GFlowNet + MLE transfer to unseen target LLMs, outperforming all other methods across all the target LLMs except Mistral-7b-instruct-v0.2. REINFORCE generates almost identical prompts, tailored to the Gemma-2b-it target it was trained with, which consequently do not transfer to other target LLMs. This highlights a drawback of methods which do not generate diverse attacks. On the other extreme, PPO + Novelty is unable to discover any prompt that is effective in eliciting toxic responses and consequently none of the prompts transfer to any other LLM. These results further highlight the efficacy and usefulness of GFlowNet + MLE, which can generate both diverse and effective red-teaming prompts that can be transferred to red-team other LLMs.\n###figure_7### ###figure_8### ###figure_9### Another advantage of our two stage approach is that it can enable fast adaptation of an attacker LM policy to red-team a different target LLM. Concretely, we can compute reward of the stored attack prompts, sampled during GFlowNet fine-tuning, with a different target LLM and rerank the prompts (instead of scoring them with the same target LLM). The offline dataset can be constructed by filtering the prompts with the newly computed  and pre-computed  based on the corresponding thresholds  and . The initial pre-trained attacker LM policy  is fine-tuned with supervised learning on this dataset. For this experiment, we consider the the prompts stored during the red-teaming of Gemma-2b-it and adapt the attacker LM to red-team Gemma-1.1-2b-it, Gemma-7b-it, Gemma-1.1-7b-it, Llama-2-7b-chat, and Llama-3-8b-instruct target LLMs. As shown in \u00a74.2  ###reference_.SSS0.Px3###, adaptation of the attack LM policy with this reranking procedure is effective and significantly improves toxicity rate over direct transfer from an attacker trained to red-team the initial target LLM, Gemma-2b-it. Note that a considerable amount of computational cost and wall-clock time can be saved since we skip the GFlowNet fine-tuning stage and simply re-use the stored prompts.\nIn this experiment, we demonstrate empirically the challenges in tuning the temperature  in Equation 2  ###reference_### and how second phase of MLE smoothing provides a better trade-off between toxicity rate and diversity. We fine-tune the pretrained initial policy  as a GFlowNet by setting the temperature  to each value in  and fine-tune again the initial attacker LM policy with MLE on each of the high-reward prompts discovered during GFlowNet fine-tuning with the corresponding . As shown in Fig. 5  ###reference_###, the GFlowNet objective sacrifices diversity (cosine distance) considerably to obtain high toxicity rate, or it significantly degrades the toxicity rate resulting in diverse prompts. On the other hand, smoothing with MLE is robust to this choice of  and enables the attacker policy to sample effective attack prompts while retaining diversity.\nWe perform an ablation study to demonstrate the importance of the off-policy exploration ability of GFlowNets for collecting the offline dataset for the second phase of MLE smoothing as described in Alg. 1  ###reference_###. To show this, we train the initial pretrained policy  with PPO + Novelty, instead of GFlowNet fine-tuning, for red-teaming Llama-2-7b-chat target LLM and attempt to collect high reward prompts sampled by the policy during training. However, PPO + Novelty does not generate any high-reward prompts at all during training, as shown in \u00a74.2  ###reference_.SSS0.Px5###. Thus, we cannot collect any samples for the offline dataset and consequently cannot fine-tune the policy with MLE.\nThis indicates that a diversity-seeking approach like GFlowNet fine-tuning is crucial for collecting effective red-teaming prompts.\nTo demonstrate that attack prompts generated by red-teaming methods can enhance the safety alignment of target LLMs, we fine-tune Gemma-2b-it target LLM with these attack prompts and evaluate its robustness. Specifically, we generate 1,024 prompts with each red-teaming method and generate refusal responses to the attack prompts with the target LLM using the prompt: \u201cRefuse answering to the following instruction. {attack_prompt}\u201d. Subsequently, the target model is fine-tuned with LoRA [21  ###reference_b21###] to maximize the conditional log-likelihood of the refusal responses to the attack prompts, resulting in six different fine-tuned target LLMs corresponding to each red-teaming method. Finally, each fine-tuned model generates responses to the attack prompts generated by each red-teaming method, and we measure the toxicity rate of the responses with Llama-Guard as the toxicity classifier .\nAs shown in Fig. 6  ###reference_###, the target LLM fine-tuned on the attack prompts generated by GFlowNet + MLE is the most robust to unseen attack prompts generated by the other RL-based red-teaming methods. On the other hand, even after safety fine-tuning, all the other target LLMs cannot defend against the attack prompts generated by GFlowNet + MLE. We also confirm that our safety-tuned model still preserves general instruction-following capabilities: as shown in Table B.2  ###reference_###, the performance on the six tasks in the Open LLM Leaderboard  ###reference_4/open_llm_leaderboard### changes insignificantly with safety tuning. These experimental results verify the importance of generating diverse and effective prompts for robust red-teaming."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experimental setup",
            "text": "We validate the efficacy of our method by red-teaming a variety of open source LMs with varying levels of saferty-tuning. Recall that the goal of red-teaming is to generate diverse prompts that elicit harmful responses from a target LLM . In our experiments, we use GPT-2  ###reference_a###  ###reference_a### [43  ###reference_b43###  ###reference_b43###], dolly-v2-7b  ###reference_-7b###  ###reference_-7b### [13  ###reference_b13###  ###reference_b13###], Llama-2-7b-chat  ###reference_7b-chat-hf###  ###reference_7b-chat-hf### [51  ###reference_b51###  ###reference_b51###], and Gemma-2b-it  ###reference_###  ###reference_### [39  ###reference_b39###  ###reference_b39###] as target LLMs. Note that we only require black-box access, i.e., we can only query a prompt and get a text response.\nTo measure effectiveness of each method, we compute the toxicity rate which is the percentage of generated prompts that are toxic.\nWe consider a prompt toxic if the toxicity classifier  assigns a score greater than 0.5 to an input pair consisting of the prompt  and response  generated by the target LLM. For , we use a RoBERTa hate speech classifier [53  ###reference_b53###  ###reference_b53###] for GPT-2 and dolly-v2-7b, and Llama-Guard [24  ###reference_b24###  ###reference_b24###] for Llama-2-7b-chat and Gemma-2b-it. To measure the diversity of the generated prompts, we embed the generated prompts using MiniLMv2  ###reference_rs/all-MiniLM-L6-v2###  ###reference_rs/all-MiniLM-L6-v2### [55  ###reference_b55###  ###reference_b55###] encoder and compute the average pairwise cosine distance between embeddings of the prompts.\nWe compare our proposed method against some relevant red-teaming baselines:\nSupervised Fine-tuning (SFT): We fine-tune the pretrained LM  with a maximum likelihood objective on 3,003 toxic prompts from SafetyDataset [7  ###reference_b7###  ###reference_b7###] and AdvBench [68  ###reference_b68###  ###reference_b68###].\nIn-Context Learning (ICL) [8  ###reference_b8###  ###reference_b8###]: We sample 5-shot demonstrations from toxic prompt datasets (SafetyDataset and AdvBench) and prompt GPT-2 to generate a prompt.\nREINFORCE [60  ###reference_b60###  ###reference_b60###]: We fine-tune the pretrained LM  as an RL policy with policy gradients to optimize the reward in Equation 1  ###reference_###  ###reference_###.\nPPO + Novelty [20  ###reference_b20###  ###reference_b20###]: This method adds entropy bonus [48  ###reference_b48###  ###reference_b48###] along with a novelty-based term to the reward in Equation 1  ###reference_###  ###reference_### and train the policy  with proximal policy optimization (PPO) [49  ###reference_b49###  ###reference_b49###]. For novelty-based reward, it utilizes self-BLEU [67  ###reference_b67###  ###reference_b67###] and pairwise cosine similarity between embeddings of all the past generated prompts.\nGFlowNet [36  ###reference_b36###  ###reference_b36###]: We fine-tune the pretrained LM  with Equation 3  ###reference_###  ###reference_###.\nGFlowNet + MLE: This is our full method for collecting high-reward prompts during GFlowNet fine-tuning and re-train the pretrained LM  with maximum likelihood estimation (MLE) on the collected prompts as described in Alg. 1  ###reference_###  ###reference_###.\n###figure_10### ###figure_11### ###figure_12### ###figure_13###"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Results: Robust red-teaming",
            "text": "As the number of prompts which would elicit toxic responses occupy a small subset of all possible sequences, there is a natural trade-off between diversity and toxicity. We start by investigating how each method handles this trade-off. Fig. 2  ###reference_###  ###reference_### illustrates the cosine distance plotted against the toxicity rate for 1,024 red-teaming prompts generated by each method across four different target LLMs. We find that our GFlowNet + MLE is the only method which manages to balance a high toxicity rate with the diversity of generated prompts across all four target LLMs. Qualitative assessment of examples generated by GFlowNet + MLE, included in Table B.3  ###reference_###  ###reference_###, Table B.4  ###reference_###  ###reference_###, Table B.5  ###reference_###  ###reference_###, and Table B.6  ###reference_###  ###reference_###, supports the numerical results. While the GFlowNet achieves both high diversity and toxicity rate for red-teaming GPT-2 (2(a)  ###reference_sf1###  ###reference_sf1###) and dolly-v2-7b (2(b)  ###reference_sf2###  ###reference_sf2###), the toxicity rate drops significantly for the target LLMs with safety fine-tuning: Gemma-2b-it (2(c)  ###reference_sf3###  ###reference_sf3###) and Llama-2-7b-chat (2(d)  ###reference_sf4###  ###reference_sf4###). We hypothesize this drop comes from the reward signal (toxicity of responses from the target) becoming sparse with safety-tuned models. Similarly, PPO + Novelty fails to find a balance between diversity and toxicity. When it is able to find effective prompts (2(a)  ###reference_sf1###  ###reference_sf1### and 2(b)  ###reference_sf2###  ###reference_sf2###) they are not as diverse and for safety fine-tuned models it fails to find any prompts which elicit a toxic response (2(c)  ###reference_sf3###  ###reference_sf3### and 2(d)  ###reference_sf4###  ###reference_sf4###). (For context, a random policy would have the highest diversity but would have a low toxicity rate).\nOn the other hand, REINFORCE, which does not take account diversity into the reward function, collapses to deterministically generating a single reward-maximizing prompt. Finally, SFT and ICL generate diverse but ineffective prompts.\n###figure_14### Gemma-2b-it\nLlama-2-7b-chat\nLlama-2-13b-chat\nLlama-2-70b-chat\nLlama-3-8b-instruct\nLlama-3-70b-instruct\nGemma-7b-it\nGemma-1.1-2b-it\nGemma-1.1-7b-it\nMistral-7b-instruct-v0.2\nStarling-7b-beta\nTo further understand the behavior of each method beyond the toxicity rate (which depends on the  decision boundary), we illustrate the distribution over the toxicity scores and corresponding average pairwise cosine distances for the generated prompts in Fig. 3  ###reference_###  ###reference_###, obtained from the experiment for red-teaming the Llama-2-7b-chat target LLM. Results for the other target LLMs are illustrated in Fig. B.1  ###reference_###  ###reference_###, Fig. B.2  ###reference_###  ###reference_###, and Fig. B.3  ###reference_###  ###reference_### in \u00a7B.1  ###reference_###  ###reference_###. GFlowNet + MLE achieves consistently high diversity across different toxicity score bins. On the other hand, all other methods fail to achieve high diversity and toxicity at the same time. GFlowNet generates fewer toxic prompts compared to GFlowNet + MLE.\nNotably, PPO + Novelty does not generate prompts with the toxicity score greater than  at all for Gemma-2b-it and Llama-2-7b-chat. While REINFORCE generates a single highly toxic prompt achieving a much lower diversity, SFT and ICL generate few toxic prompts.\nA potential advantage of generating diverse attack prompts is that prompts generated for red-teaming a given target LLM can potentially transfer to other LLMs, since some of the failure modes of a target LLM might be shared by other models, for instance, due to using similar web-filtered data or similar safety alignment recipes.\nTo study this empirically, we train an attacker policy  for red-teaming the Gemma-2b-it as the target LLM. We then sample  prompts from the trained attacker LM and evaluate the number of prompts which transfer to other LLMs, i.e., elicit toxic responses from unseen LLMs: Llama-2-7b-chat  ###reference_7b-chat-hf###  ###reference_7b-chat-hf###, Llama-2-13b-chat  ###reference_13b-chat-hf###  ###reference_13b-chat-hf###,\nLlama-2-70b-chat  ###reference_70b-chat-hf###  ###reference_70b-chat-hf###,\nLlama-3-8b-instruct  ###reference_ma-3-8B-Instruct###  ###reference_ma-3-8B-Instruct### [1  ###reference_b1###  ###reference_b1###],\nLlama-3-70b-instruct  ###reference_ma-3-70B-Instruct###  ###reference_ma-3-70B-Instruct###,\nGemma-7b-it  ###reference_###  ###reference_###, Gemma-1.1-2b-it  ###reference_-it###  ###reference_-it###, Gemma-1.1-7b-it  ###reference_-it###  ###reference_-it###, Mistral-7b-instruct-v0.2  ###reference_-it###  ###reference_-it### [28  ###reference_b28###  ###reference_b28###], and Starling-7b-beta  ###reference_LM-7B-beta###  ###reference_LM-7B-beta### [66  ###reference_b66###  ###reference_b66###]. As shown in Table 2  ###reference_###  ###reference_###, we find that many prompts generated by GFlowNet + MLE transfer to unseen target LLMs, outperforming all other methods across all the target LLMs except Mistral-7b-instruct-v0.2. REINFORCE generates almost identical prompts, tailored to the Gemma-2b-it target it was trained with, which consequently do not transfer to other target LLMs. This highlights a drawback of methods which do not generate diverse attacks. On the other extreme, PPO + Novelty is unable to discover any prompt that is effective in eliciting toxic responses and consequently none of the prompts transfer to any other LLM. These results further highlight the efficacy and usefulness of GFlowNet + MLE, which can generate both diverse and effective red-teaming prompts that can be transferred to red-team other LLMs.\n###figure_15### ###figure_16### ###figure_17### Another advantage of our two stage approach is that it can enable fast adaptation of an attacker LM policy to red-team a different target LLM. Concretely, we can compute reward of the stored attack prompts, sampled during GFlowNet fine-tuning, with a different target LLM and rerank the prompts (instead of scoring them with the same target LLM). The offline dataset can be constructed by filtering the prompts with the newly computed  and pre-computed  based on the corresponding thresholds  and . The initial pre-trained attacker LM policy  is fine-tuned with supervised learning on this dataset. For this experiment, we consider the the prompts stored during the red-teaming of Gemma-2b-it and adapt the attacker LM to red-team Gemma-1.1-2b-it, Gemma-7b-it, Gemma-1.1-7b-it, Llama-2-7b-chat, and Llama-3-8b-instruct target LLMs. As shown in \u00a74.2  ###reference_.SSS0.Px3###  ###reference_.SSS0.Px3###, adaptation of the attack LM policy with this reranking procedure is effective and significantly improves toxicity rate over direct transfer from an attacker trained to red-team the initial target LLM, Gemma-2b-it. Note that a considerable amount of computational cost and wall-clock time can be saved since we skip the GFlowNet fine-tuning stage and simply re-use the stored prompts.\nIn this experiment, we demonstrate empirically the challenges in tuning the temperature  in Equation 2  ###reference_###  ###reference_### and how second phase of MLE smoothing provides a better trade-off between toxicity rate and diversity. We fine-tune the pretrained initial policy  as a GFlowNet by setting the temperature  to each value in  and fine-tune again the initial attacker LM policy with MLE on each of the high-reward prompts discovered during GFlowNet fine-tuning with the corresponding . As shown in Fig. 5  ###reference_###  ###reference_###, the GFlowNet objective sacrifices diversity (cosine distance) considerably to obtain high toxicity rate, or it significantly degrades the toxicity rate resulting in diverse prompts. On the other hand, smoothing with MLE is robust to this choice of  and enables the attacker policy to sample effective attack prompts while retaining diversity.\nWe perform an ablation study to demonstrate the importance of the off-policy exploration ability of GFlowNets for collecting the offline dataset for the second phase of MLE smoothing as described in Alg. 1  ###reference_###  ###reference_###. To show this, we train the initial pretrained policy  with PPO + Novelty, instead of GFlowNet fine-tuning, for red-teaming Llama-2-7b-chat target LLM and attempt to collect high reward prompts sampled by the policy during training. However, PPO + Novelty does not generate any high-reward prompts at all during training, as shown in \u00a74.2  ###reference_.SSS0.Px5###  ###reference_.SSS0.Px5###. Thus, we cannot collect any samples for the offline dataset and consequently cannot fine-tune the policy with MLE.\nThis indicates that a diversity-seeking approach like GFlowNet fine-tuning is crucial for collecting effective red-teaming prompts."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Results: Safety fine-tuning",
            "text": "To demonstrate that attack prompts generated by red-teaming methods can enhance the safety alignment of target LLMs, we fine-tune Gemma-2b-it target LLM with these attack prompts and evaluate its robustness. Specifically, we generate 1,024 prompts with each red-teaming method and generate refusal responses to the attack prompts with the target LLM using the prompt: \u201cRefuse answering to the following instruction. {attack_prompt}\u201d. Subsequently, the target model is fine-tuned with LoRA [21  ###reference_b21###  ###reference_b21###] to maximize the conditional log-likelihood of the refusal responses to the attack prompts, resulting in six different fine-tuned target LLMs corresponding to each red-teaming method. Finally, each fine-tuned model generates responses to the attack prompts generated by each red-teaming method, and we measure the toxicity rate of the responses with Llama-Guard as the toxicity classifier .\nAs shown in Fig. 6  ###reference_###  ###reference_###, the target LLM fine-tuned on the attack prompts generated by GFlowNet + MLE is the most robust to unseen attack prompts generated by the other RL-based red-teaming methods. On the other hand, even after safety fine-tuning, all the other target LLMs cannot defend against the attack prompts generated by GFlowNet + MLE. We also confirm that our safety-tuned model still preserves general instruction-following capabilities: as shown in Table B.2  ###reference_###  ###reference_###, the performance on the six tasks in the Open LLM Leaderboard  ###reference_4/open_llm_leaderboard###  ###reference_4/open_llm_leaderboard### changes insignificantly with safety tuning. These experimental results verify the importance of generating diverse and effective prompts for robust red-teaming."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "As LMs become increasingly more capable and widely used, red-teaming them for a wide variety of potential attacks becomes more critical for safe and responsible deployment. We have proposed an approach to generate diverse and effective red-teaming prompts using a novel two-stage procedure consisting of GFlowNet fine-tuning followed by MLE smoothing. Through our experiments, we showed that our approach is effective for red-teaming a wide variety of target LMs with varying levels of safety-tuning. An interesting observation is the transferability of the generated prompts to different target LLMs, which reveals shared failure modes of current approaches for aligning LMs and opens interesting direction for future work. In particular, our reranking-based adaptation procedure can serve as a quick way to red-team new target LLMs during development.\nOur approach is not limited to text tokens and future work can explore the applicability to red-team multimodal models (e.g., text-to-image models [44  ###reference_b44###, 45  ###reference_b45###]). Further, an interesting area of future work is extending the approach to the jailbreaking setting, where an attacker language model generates a suffix for an adversarial query prompt. Finally, in addition to red-teaming, it would be interesting to apply our method to generate prompts which can improve model performance on different tasks [32  ###reference_b32###].\nWhile our approach shows promising performance for red-teaming various target language models, the performance is still limited by the classifier used to quantify the harmfulness of a response.\nThe true harm that an LM output causes is often subjective and depends on the social context of deployment [59  ###reference_b59###]. As with other RL-based approaches, our approach is trained online (i.e., requires iteratively sampling the current model) and, consequently, requires the ability to sample several responses from the target LLM to compute the reward during training, which can be costly.\nOur proposed red-teaming framework is useful for automatically discovering diverse ways to induce undesirable responses from LLMs. Before deployment of the LLM, we can perform safety fine-tuning of the model to prevent generation of harmful responses. However, our method can be misused to attack commercial LLMs at scale, since it can generate harmful prompts that transfer to other target LLMs. This necessitates precautions for the deployment of LLMs. We can defend against such attacks by filtering harmful responses with the toxicity classifier employed for training the attacker model."
        }
    ],
    "url": "http://arxiv.org/html/2405.18540v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.2"
        ]
    },
    "research_context": {
        "paper_id": "2405.18540v1",
        "paper_title": "Learning Diverse Attacks on Large Language Models for Robust Red-Teaming and Safety Tuning",
        "research_background": "**Introduction**\nThe deployment of large language models (LLMs) has raised longstanding concerns about their potential harmful impacts, especially as these models become increasingly capable. Even LLMs that have been fine-tuned for safety can still produce toxic outputs under certain manipulations. The practice of red-teaming, which involves identifying 'attack' prompts that cause undesirable responses, aims to address these vulnerabilities before deploying the models. Existing methods for automated red-teaming use stochastic optimization or reinforcement learning (RL) to generate such prompts but often struggle to balance between diversity and effectiveness. This paper focuses on automatically generating diverse attack prompts for LLMs, proposing robust automated red-teaming as a solution to develop more effective defenses.\n\n**Research Problem**\nThe primary research problem tackled by this paper is the automatic generation of diverse and effective attack prompts for LLMs in the context of red-teaming. Specifically, it aims to overcome the limitations of existing methods, which either suffer from mode collapse (generating too similar prompts) or overly prioritize diversity at the expense of effectiveness. Additionally, these methods often fail to identify attack prompts that generalize across different LLMs.\n\n**Relevant Prior Work**\n1. **Concerns and Capabilities of LLMs**: The potential harmful impacts of LLMs have been documented for nearly a decade, highlighting concerns about their misuse [31, 59]. Even models fine-tuned for safety can be exploited to produce toxic outputs [57].\n2. **Red-Teaming Approaches**: The concept of red-teaming in machine learning to identify attack prompts has been explored as a method to preemptively address model vulnerabilities [42]. Effective red-teaming necessitates the identification of varied attack strategies [20].\n3. **Automated Red-Teaming Methods**: Previous approaches have utilized stochastic optimization [68, 65] and reinforcement learning [42, 20] to automate the generation of attack prompts. However, these methods often fail to balance diversity with effectiveness, leading to issues like mode collapse or ineffective attacks.\n4. **GFlowNet and RLHF**: The off-policy RL approach of GFlowNet fine-tuning, which has been proposed for inference of linguistic latent variables [23], provides a promising framework for this task. Additionally, techniques from behavior cloning in offline RL [16, 27] and reinforcement learning with human feedback [RLHF; 34] offer useful insights into the training process.\n\nIn summary, this paper leverages these research insights to propose an advanced method for generating diverse and effective attack prompts while ensuring they generalize across different LLMs.",
        "methodology": "In their methodology for \"Learning Diverse Attacks on Large Language Models for Robust Red-Teaming and Safety Tuning,\" the authors leverage GFlowNets, diversity-seeking reinforcement learning (RL) algorithms, to generate diverse and effective attack prompts. The key components and innovations of their proposed method are outlined as follows:\n\n1. **GFlowNets**: \n   - **Diversity-Seeking Policy**: These algorithms seek to sample prompts with probabilities proportional to their associated rewards. In the context of generating sequences, GFlowNets align with MaxEnt RL algorithms.\n   - **Reward Definition**: For a given prompt, the reward integrates a toxicity score (T) and a reference language model likelihood (L), modulated by constants \\( \\beta \\) and \\( \\gamma \\) which control the influence (or \u201cpeakiness\u201d) of each component.\n\n2. **Autoregressive Prompt Generation**:\n   - Prompts are generated token-by-token from a mixed behavior policy comprising the initial policy \\( \\pi \\) and a tempered variant. Each token generated corresponds to an action while each intermediate state in this process reflects the sequence generated so far.\n\n3. **Trajectory Balance Learning Objective**:\n   - The learning objective utilizes the trajectory balance measure to optimize the parameters \\( \\theta \\). The inclusion of a learnable scalar \\( Z_\\theta \\) helps approximate the partition function necessary for computing the reward distribution.\n\n4. **Stochastic Reward Approximations**:\n   - Given the stochastic nature of rewards due to dependency on model responses, the log reward is approximated using an empirical mean over multiple LLM samples.\n\n5. **Balancing Rewards and Diversity**:\n   - Challenges arise from the non-trivial selection of temperature parameters \\( \\beta \\) and \\( \\gamma \\), influencing reward peaking and ensuring diversity. Earlier attempts can result in reduced total rewards for prompts even if they exhibit high toxicity scores but low likelihood, necessitating a balanced approach.\n\n6. **Behavior Cloning and Retraining**:\n   - To address sensitivity issues and enhance robustness, an additional retraining step analogous to behavior cloning in RL is proposed. \n   - **Offline Dataset**: Prompts sampled during initial GFlowNet fine-tuning are stored. These prompts are envisaged to possess high reward and diversity due to the exploration phase.\n   - **Filtering and Fine-Tuning**: Prompts are filtered based on threshold values for toxicity score \\( T \\) and LLM likelihood \\( L \\). The reference policy is fine-tuned with this filtered dataset to maximize their log-likelihood, starting from the same initial state as in the first stage.\n   - This retraining yields a smoother approximation to the distribution of high-reward prompts, preserving mode coverage and enhancing generalization.\n\nIn summary, the proposed method combines the initial GFlowNet fine-tuning for exploring diverse high-reward prompts with a secondary retraining step to mitigate sensitivity issues, ensuring a robust policy for generating effective red-team prompts. The methodology underscores the need to balance toxicity and language model likelihood to optimize performance.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n**Datasets and Target Models:**\n- **Datasets:** \n  - SafetyDataset \n  - AdvBench\n- **Target LLMs:** \n  - GPT-2\n  - Dolly-v2-7b\n  - Llama-2-7b-chat\n  - Gemma-2b-it\n\n**Baselines:**\n1. **Supervised Fine-Tuning (SFT):** Fine-tuning pretrained LM with 3,003 toxic prompts.\n2. **In-Context Learning (ICL):** Sampling 5-shot demonstrations from toxic prompt datasets.\n3. **REINFORCE:** Fine-tuning LM as an RL policy with policy gradients.\n4. **PPO + Novelty:** Includes entropy bonus and novelty-based term in reward, trained with proximal policy optimization.\n5. **GFlowNet:** Fine-tuning pretrained LM.\n6. **GFlowNet + MLE:** GFlowNet fine-tuning followed by retraining with maximum likelihood estimation (MLE).\n\n**Evaluation Metrics:**\n1. **Toxicity Rate:** Percentage of generated prompts that are toxic, measured using:\n   - RoBERTa hate speech classifier for GPT-2 and Dolly-v2-7b.\n   - Llama-Guard for Llama-2-7b-chat and Gemma-2b-it.\n2. **Diversity:** Average pairwise cosine distance between embeddings of the generated prompts using MiniLMv2 encoder.\n\n### Main Experimental Results\n\n**Effectiveness in Balancing Toxicity and Diversity:**\n- **GFlowNet + MLE:**\n  - Managed to balance high toxicity rate and diversity across all four target LLMs.\n  - Achieved high diversity and toxicity for GPT-2 and Dolly-v2-7b.\n  - Lower toxicity rates for safety-tuned models (Gemma-2b-it and Llama-2-7b-chat) due to sparse reward signals.\n  \n**Overall Conclusion:**\n- **GFlowNet + MLE:**\n  - Demonstrated the importance of generating diverse and effective prompts for robust red-teaming.\n  - Provided both high performance and adaptability across various LLMs, reinforcing its efficacy in real-world scenarios."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To investigate the balance between diversity and toxicity in prompts generated by different red-teaming methods for large language models (LLMs).",
            "experiment_process": "Four different strategies\u2014GFlowNet + MLE, PPO + Novelty, REINFORCE, SFT, and ICL\u2014were employed to generate 1,024 red-teaming prompts for four different target LLMs: GPT-2, dolly-v2-7b, Gemma-2b-it, and Llama-2-7b-chat. The diversity and toxicity rate of the generated prompts were measured, and results were visually illustrated through cosine distance against toxicity rate graphs. These methods were also evaluated on other target LLMs such as Llama-2-13b-chat, Llama-2-70b-chat, Llama-3-8b-instruct, Llama-3-70b-instruct, and several Gemma models.",
            "result_discussion": "GFlowNet + MLE achieved a balance between high toxicity and diversity across all four target LLMs, while PPO + Novelty and REINFORCE failed to maintain both criteria. SFT and ICL generated diverse but ineffective prompts. The study underscored the capacity of GFlowNet + MLE to transfer effective and diverse red-teaming prompts to other LLMs, affirming its superiority over other methods.",
            "ablation_id": "2405.18540v1.No1"
        },
        {
            "research_objective": "To assess the transferability of red-teaming prompts generated for one LLM to other unseen LLMs.",
            "experiment_process": "An attacker policy was trained on the Gemma-2b-it target LLM, and 1,024 prompts were sampled to evaluate their toxicity when applied to unseen LLMs: Llama-2-7b-chat, Llama-2-13b-chat, Llama-2-70b-chat, Llama-3-8b-instruct, Llama-3-70b-instruct, Gemma-7b-it, Gemma-1.1-2b-it, Gemma-1.1-7b-it, Mistral-7b-instruct-v0.2, and Starling-7b-beta.",
            "result_discussion": "Prompts generated by GFlowNet + MLE transferred more effectively to other unseen target LLMs compared to those generated by REINFORCE and PPO + Novelty. This indicates the advantage of generating diverse attack prompts that span failure modes common across various models.",
            "ablation_id": "2405.18540v1.No2"
        },
        {
            "research_objective": "To evaluate the efficacy of reranking stored attack prompts and adapting the attacker LM policy to different LLMs.",
            "experiment_process": "Stored prompts from the Gemma-2b-it red-teaming experiment were re-evaluated using different target LLMs, such as Gemma-1.1-2b-it, Gemma-7b-it, Gemma-1.1-7b-it, Llama-2-7b-chat, and Llama-3-8b-instruct. Reranked prompts were filtered and used to fine-tune the initial attacker LM policy.",
            "result_discussion": "The reranking procedure effectively improved the toxicity rate over direct transfer from the initial attacker policy, demonstrating that reranking pre-stored prompts enables efficient adaptation without the computational cost of re-running GFlowNet fine-tuning.",
            "ablation_id": "2405.18540v1.No3"
        },
        {
            "research_objective": "To illustrate the benefits of MLE smoothing after GFlowNet fine-tuning in balancing diversity and toxicity.",
            "experiment_process": "The pretrained initial policy was fine-tuned as a GFlowNet with varying temperatures and then further fine-tuned using MLE on high-reward prompts. This was compared against PPO + Novelty based approach for gathering high-reward prompts.",
            "result_discussion": "GFlowNet alone sacrifices either diversity or toxicity, but MLE smoothing retains diversity while ensuring an effective attack rate. PPO + Novelty failed to generate high-reward prompts necessary for effective MLE fine-tuning, highlighting the importance of GFlowNet's off-policy exploration capabilities.",
            "ablation_id": "2405.18540v1.No4"
        }
    ]
}