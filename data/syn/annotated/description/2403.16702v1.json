{
    "title": "ProCQA: A Large-scale Community-based Programming Question Answering Dataset for Code Search",
    "abstract": "Retrieval-based code question answering seeks to match user queries in natural language to relevant code snippets. Previous approaches typically rely on pretraining models using crafted bi-modal and uni-modal datasets to align text and code representations. In this paper, we introduce ProCQA, a large-scale programming question answering dataset extracted from the StackOverflow community, offering naturally structured mixed-modal QA pairs. To validate its effectiveness, we propose a modality-agnostic contrastive pre-training approach to improve the alignment of text and code representations of current code language models. Compared to previous models that primarily employ bimodal and unimodal pairs extracted from CodeSearchNet for pre-training, our model exhibits significant performance improvements across a wide range of code retrieval benchmarks.\n\n\n\nKeywords:\u2009Code QA Dataset, Code Search, Contrastive Pretraining",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "Code Question Answering (Code QA) represents a pivotal research area in software intelligence.\nOne popular task formulation is retrieval-based QA (Gu et al., 2018  ###reference_b9###), in which the primary objective is to effectively match user queries expressed in natural language to relevant code snippets from an existing corpus.\nThe prevailing approach for retrieval-based Code QA has been the utilization of dual-encoder-based representation models.\nThe core idea underlying this approach is to map natural language queries and code snippets into a shared representation space, where closely located vectors correspond to semantically similar meanings.\nTo learn a shared representation space for text and code, early research efforts adopted masked language modeling (MLM) objective on paired text-code dataset to align different modalities (Kanade et al., 2020  ###reference_b17###; Feng et al., 2020  ###reference_b8###), similar to the monolingual and cross-lingual pre-training approaches (Devlin et al., 2019  ###reference_b6###; Conneau and Lample, 2019  ###reference_b5###).\nSubsequent work discovered the potential of contrastive pre-training, and applied it to code representation learning by constructing large-scale paired datasets (Jain et al., 2021  ###reference_b16###; Li et al., 2022  ###reference_b20###).\nCurrent contrastive code representation learning methods such as CodeRetriever (Li et al., 2022  ###reference_b20###) typically rely on curated uni-modal (code-code pairs) or bi-modal data (text-code pairs).\nFew work even uses distinct encoder for text and code (Heyman and Cutsem, 2020  ###reference_b12###; Salza et al., 2023  ###reference_b29###).\nSuch pre-training design emphasizes the concept of modality distinction, diverging from the goal of establishing a unified representation space for different modalities.\nIn Figure 1  ###reference_###, we illustrate different data formats used for contrastive pre-training and analyse their chunk-level matching patterns.\nUni-modal data offers code-code matching patterns, whereas bi-modal data implies code-text matching patterns.\nWhile a combination of both data types during pre-training can enable models to learn both matching signals, a more data-efficient approach to capture all matching patterns is through mixed-modal data.\n###figure_1### Segmentation fault while copying a string to the memory allocated array\nFollowing is a program I am practicing;\nint main()\nint i = 0;\nchar **grid = (char **) malloc(5*sizeof(int));\nfor (i = 0 ; i < 5 ; i++)\ngrid[i] = (char *) malloc(6);\nstrcpy(grid[0], \"eabcd\");\nstrcpy(grid[1], \"fghij\");\nstrcpy(grid[2], \"olkmn\");\nstrcpy(grid[3], \"trpqs\");\nstrcpy(grid[4], \"xywuv\"); /*Segmentation Fault at this line*/\nreturn 0;\n\nI am getting a segmentation fault at the line strcpy(grid[4], \"xywuv\"); . What could be the reason? I have allocated the array to have 5 strings(rows) of 6 characters each(columns).\nYou are allocating the wrong type at line 3\nchar **grid = (char **) malloc(5*sizeof(int));\nShould be\nchar **grid = (char **) malloc(5*sizeof(char*));\nThis is because you are declaring string-array. Therefore, the malloc should be char* (string / character pointer)\nAlso the same if you were trying to declare 2-D integer array. It will be\nint **grid = (int **) malloc(5*sizeof(int*));\nBesides, the majority of code embedding models (Jain et al., 2021  ###reference_b16###; Wang et al., 2021a  ###reference_b30###; Li et al., 2022  ###reference_b20###) have primarily relied on CodeSearchNet (Husain et al., 2019  ###reference_b14###) as the main pre-training corpus. While CodeSearchNet is a valuable resource, its size and data distribution have inherent limitations that may impact the quality and diversity of learned code representations.\nRecent work has proposed to curate large-scale code datasets from GitHub (Allal et al., 2023  ###reference_b2###).\nYet their efforts mainly focus on training large-scale generative language models (LMs).\nIn parallel, some research endeavors have aimed to create code-related question-answering datasets from diverse sources, as evidenced by Huang et al. (2021  ###reference_b13###); Lee et al. (2022  ###reference_b18###).\nNevertheless, most of these datasets remain constrained by their scale, rendering them more suitable for stand-alone evaluation benchmarks rather than comprehensive pre-training corpus.\nTherefore in this research, we try to bridge these gaps by proposing ProCQA, a large-scale community-based programming question answering dataset mined from StackOverflow.\nProCQA encompasses an extensive collection of approximately 5 million QA pairs, spanning 11 different programming languages.\nThis dataset is distinguished by its comprehensive language coverage, the diversity of user queries, and its code-mixing data format111Please refer to Table 1  ###reference_### for an illustrative example.. It can be used as both an evaluation benchmark and a pre-training corpus.\nWe provide strict rule-based filtering and data decontamination procedure to ensure its quality and fairness.\nDifferent types of baseline models are trained and compared on this dataset to test its suitability as an evaluation benchmark.\nTo assess the efficacy of our proposed dataset as a pre-training corpus, we conducted large-scale modality-agnostic contrastive pretraining (MACP) on the code-mixing dataset, without making distinctions between text and code modalities.\nTo demonstrate whether MACP can learn a better aligned representation space, we evaluated it on extensive code retrieval benchmarks, covering supervised, zero-shot, and out-of-domain scenarios.\nExperiments reveal that compared to previous pre-trained code language models, MACP achieves substantial improvements on most tasks we considered, advancing the previous best code retrieval model CodeRetriever (Li et al., 2022  ###reference_b20###) by 110% points across different evaluation benchmarks.\nComprehensive ablation and analysis demonstrates the effectiveness of our proposed approach.\nThe contributions of this paper can be summarized as follows:\nWe create ProCQA, a large-scale dataset for programming question answering.\nProCQA is characterized by its practicality, diversity and mixed-modal data format.\nWe demonstrate its potential as an evaluation benchmark for comparing different code language models.\nBased on ProCQA, we present MACP, a code representation model pre-trained with modality-agnostic contrastive learning on the large-scale code-mixing dataset. MACP demonstrates remarkable performance gains over prior approaches across a wide range of code retrieval tasks."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1.   Code QA",
            "text": "Recent work has been focused on constructing code QA dataset from fictional scenarios."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2.   Code Language Models",
            "text": "Recent work has been focused on constructing code QA dataset from simulated environments."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   ProCQA",
            "text": "In this section, we outline the methodologies employed in the creation of the ProCQA dataset, along with the filtering strategies applied to ensure data quality and fairness. Additionally, we present an analysis of various dataset statistics and define two tasks utilizing this dataset to evaluate different baseline models. The source code is available at https://github.com/jordane95/procqa  ###reference_###.\nThis task is defined as finding the correct answer from a large-scale answer corpus.\nWe use answers from all splits of the dataset to form retrieval corpus.\nThe query is the concatenation of question and description.\nWe choose BM25 and some recent neural language models, such as BERT (Devlin et al., 2019  ###reference_b6###), CodeBERT (Feng et al., 2020  ###reference_b8###) and UniXCoder (Guo et al., 2022  ###reference_b10###).\nNeural LMs are fine-tuned with the contrastive learning objective (i.e., InfoNCE loss) on the question answer pairs from the training set.\nAll models are trained for 3 epochs with the batch size of 32 and the learning rate of 2e-5.\nBoth questions and answers are truncated to be maximum of 256 tokens.\nWe choose MRR@10, Recall@10 and Recall@100 as main evaluation metrics.\nResults are demonstrated in Table 4  ###reference_###.\nWe observe that text-only language models such as BERT are even inferior to unsupervised BM25, in terms of MRR@10.\nWith code-specific pre-training, CodeBERT can outperform the strong BM25 baseline.\nMore recent code language models such as UniXCoder performs best on this task.\nWe also consider a generative task formulation, in which the model is required to directly generate the answer to the question without additional reference.\nSimilarly, we benchmark several generative language models on this task.\nSelected baseline models include T5 (Raffel et al., 2020  ###reference_b28###), CodeT5 (Wang et al., 2021b  ###reference_b31###), PLBART (Ahmad et al., 2021  ###reference_b1###).\nModels are trained in a sequence-to-sequence manner by optimizing the cross-entropy loss of the answer sequence given question sequence with the same training hyperparameters as stated above.\nDuring inference, beam search decoding is used with a beam size of 5.\nWe use ROUGE Lin (2004  ###reference_b21###) as main evaluation metrics for this task and demonstrate results in Table 5  ###reference_###.\nIt is found that code language models is better than text-only models, indicating the effectiveness of code-specific pre-training.\nEven the best model struggles on this task because the answers are relatively long (mostly 100-200 words, see Figure 2  ###reference_###).\nThis indicates that ProCQA is a challenging dataset for long-form generative QA task.\nHow to improve the long-form question-answering performance of language models with limited parameters is also an interesting direction for future research."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   Data Acquisition",
            "text": "To ensure the diversity and reflect real world user problems, we crawl our dataset from StackOverflow, a question answering community focusing on solving programming problems.\nUsers can post their problems on the website and wait for others\u2019 answers.\nOne characteristic of this dataset is that both the question and answer are code-mixing, i.e., text and code are interleaved within these fields.\nSuch data format is very useful to indoctrinate and evaluate the model\u2019s matching ability of different patterns.\nWe use the public dumps as of 2022/12 for raw data downloading222https://archive.org/details/stackexchange.\nWe extract the textual content consisting of code and texts from XML files.\nThree fields (title, question, answer) are kept.\nHTML tags are removed and only text content are kept using BeautifulSoup library."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   Data Cleaning",
            "text": "A critical problem with these QA communities is that there are many unanswered questions and wrong answers.\nTo handle this issue, we apply some rule-based approaches to filter out low-quality questions and answers.\nMore specifically, we filter out questions/answers that are either too short (< 20 characters) or too long (> 4096 characters).\nWe only keep questions that have answers marked as accepted by the questioner since it is a natural annotation signal indicating the answer is helpful for the user."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "3.3.   Data Format",
            "text": "Data in ProCQA is formatted as triples illustrated in Table 1  ###reference_###.\nThe question is a concise user request.\nIt is coupled with a detailed description which explains the problem in more detail.\nThe answer is posted by other user and is the one accepted by the questioner.\nNote that in all data fields, code and text are interleaved, which provides a natural supervision signal for aligning the two modalities."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "3.4.   Data Statistics",
            "text": "We partition the dataset into different programming language subsets according to their tags contained in meta information.\nWe consider the following eleven languages based on their popularity: Python, Java, JavaScript, Ruby, C, C++, C#, Rust, PHP, Lisp and Go.\nDataset statistics are shown in Table 2  ###reference_###.\nWe split the dataset into train / valid / test set by a proportion of 80%:10%:10% following chronological order of posting date.\nIn addition, we analyse the question and answer length distribution of our ProCQA dataset in Figure 2  ###reference_###.\nMost of the QA pairs in ProCQA contain dozens or hundreds of words, which are much closer to real user questions.\n###figure_2###"
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "3.5.   Decontamination",
            "text": "Since ProCQA is crawled from StackOverflow, it many overlap with some evaluation sets constructed from the same source.\nTo avoid data contamination, we perform evaluation data deduplication for our ProCQA training set.\nSpecifically, we employ two methods for deduplication.\nThe first one is based on substring matching.\nTraining example in ProCQA dataset is dropped if it contains any substring that is part of the queries in the evaluation set.\nWe use three evaluation sets to perform deduplication (CoNaLa, SO-DS and StaQC).\nAfter this step, about 0.5% examples from the Python subset are dropped.\nOther subsets are influenced lightly.\nWe also apply fuzzy deduplication method based on MinHash but no additional duplicate is found."
        },
        {
            "section_id": "3.6",
            "parent_section_id": "3",
            "section_name": "3.6.   Comparison to previous datasets",
            "text": "To better understand the difference with previous dataset, we summarize some key factors of our ProCQA and previous ones in Table 3  ###reference_###, including the number of supported programming languages (PLs), data format, size and data source.\nCodeNN (Iyer et al., 2016  ###reference_b15###) is also a dataset mined from StackOverflow for code summarization but contains much smaller amount of training examples and languages.\nCodeSearchNet (CSN) is on pair with ProCQA in terms of languages and size but drawn from a different data distribution (GitHub). Its queries are either documentation strings or comments rather than natural language questions, limiting its practicality in real scenarios.\nCoSQA and CS1QA contain some real user queries collected from Bing logs and classrooms but only cover Python and are limited in size.\nIn summary, ProCQA differs from previous work in the following main aspects:\n1. More diverse language distribution at a larger scale.\n2. Long-form questions and answers more aligned with real-world scenarios."
        },
        {
            "section_id": "3.7",
            "parent_section_id": "3",
            "section_name": "3.7.   Tasks",
            "text": "We define two tasks based on the collected dataset for pilot exploration, including answer retrieval and generation.\nWe choose C subset as a test bed for comparing multiple language models.\nThis task is defined as finding the correct answer from a large-scale answer corpus.\nWe use answers from all splits of the dataset to form retrieval corpus.\nThe query is the concatenation of question and description.\nWe choose BM25 and some recent neural language models, such as BERT (Devlin et al., 2019  ###reference_b6###  ###reference_b6###), CodeBERT (Feng et al., 2020  ###reference_b8###  ###reference_b8###) and UniXCoder (Guo et al., 2022  ###reference_b10###  ###reference_b10###).\nNeural LMs are fine-tuned with the contrastive learning objective (i.e., InfoNCE loss) on the question answer pairs from the training set.\nAll models are trained for 3 epochs with the batch size of 32 and the learning rate of 2e-5.\nBoth questions and answers are truncated to be maximum of 256 tokens.\nWe choose MRR@10, Recall@10 and Recall@100 as main evaluation metrics.\nResults are demonstrated in Table 4  ###reference_###  ###reference_###.\nWe observe that text-only language models such as BERT are even inferior to unsupervised BM25, in terms of MRR@10.\nWith code-specific pre-training, CodeBERT can outperform the strong BM25 baseline.\nMore recent code language models such as UniXCoder performs best on this task.\nWe also consider a generative task formulation, in which the model is required to directly generate the answer to the question without additional reference.\nSimilarly, we benchmark several generative language models on this task.\nSelected baseline models include T5 (Raffel et al., 2020  ###reference_b28###  ###reference_b28###), CodeT5 (Wang et al., 2021b  ###reference_b31###  ###reference_b31###), PLBART (Ahmad et al., 2021  ###reference_b1###  ###reference_b1###).\nModels are trained in a sequence-to-sequence manner by optimizing the cross-entropy loss of the answer sequence given question sequence with the same training hyperparameters as stated above.\nDuring inference, beam search decoding is used with a beam size of 5.\nWe use ROUGE Lin (2004  ###reference_b21###  ###reference_b21###) as main evaluation metrics for this task and demonstrate results in Table 5  ###reference_###  ###reference_###.\nIt is found that code language models is better than text-only models, indicating the effectiveness of code-specific pre-training.\nEven the best model struggles on this task because the answers are relatively long (mostly 100-200 words, see Figure 2  ###reference_###  ###reference_###).\nThis indicates that ProCQA is a challenging dataset for long-form generative QA task.\nHow to improve the long-form question-answering performance of language models with limited parameters is also an interesting direction for future research."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Experiments",
            "text": "To assess the quality and utility of our proposed dataset, we evaluate its benefits to other code search benchmarks when acting as a pre-training corpus.\nWe also conduct ablation experiments to demonstrate the effectiveness of ProCQA over existing pre-training corpus CSN.\nWe first present evaluation results on six programming language subsets of CodeSearchNet in Table 7  ###reference_###.\nMACP trained with the newly proposed dataset outperforms previous best model CodeRetriever on all language subsets, by an average of 2.1 points.\nNext, we look at results on several challenging benchmarks, all collected from real-world user queries instead of docstrings.\nAs shown in Table 8  ###reference_###, our model significantly outperforms prior state-of-the-art models by up to 10 points on average.\nWe attribute the improvement to real-world user queries from ProCQA.\nAfter evaluating the cross-modal search ability of our embedding model, we zoom into the intra-modal retrieval performance by evaluating on a code clone detection benchmark, POJ-104.\nResults are illustrated in Table 9  ###reference_###.\nOur model outperforms previous best baseline CodeRetriever by +1.38 points.\nWe list the cross-lingual code retrieval performance of our model MACP and other baselines from  Guo et al. (2022  ###reference_b10###) in Table 10  ###reference_###.\nUniXCoder has significantly better zero-shot code retrieval performance, owing to its contrastive objective during pre-training.\nMACP consistently outperforms previous baselines by a large margin, setting new state-of-the-art performance on this task.\nIn Table 11  ###reference_###, we compare different models\u2019 performance on Solidity and SQL, two languages unseen during pre-training.\nPrevious best model MAML (Chai et al., 2022  ###reference_b4###) applied model-agnostic meta learning on CodeBERT (Feng et al., 2020  ###reference_b8###) using Java and Python subsets from CSN for pre-training.\nIn addition, we also report the performance of GraphCodeBERT using our codebase as another baseline for comparison.\nOur model significantly improves the cross-domain code search performance on unseen languages.\nOne possible reason is that the diversity of language coverage in ProCQA equips the model with better language adaptation ability.\nWe first investigate the effect of different pre-training corpus by doing a series of controlled experiments where only the pre-training data distribution is changed.\nWe run two additional experiments by using the CSN and ProCQA dataset individually for pre-training.\nDue to space limitation, we report downstream fine-tuned retrieval performance on CodeSearchNet in Figure 3  ###reference_###.\nResults on other evaluation benchmarks follow the same trends.\nDespite CSN belongs to in-domain data for this evaluation benchmark, it still underperforms ProCQA when being used as a pre-training corpus.\nCombining two datasets gives better results.\nThis showcases the effectiveness of ProCQA dataset being used as a mixed-modal corpus for retrieval-oriented contrastive pre-training.\n###figure_3### We ablate on the choice of modality-agnostic contrastive learning by comparing to another setting which we explicitly distinguish text and code in data design.\nDue to the high difficulty of parsing incomplete code snippets in ProCQA, we conduct this ablation on the CSN pre-training corpus where code are well formed and can be parsed by existing tools. Bi-modal setting removes all comments in the code while mixed-modal setting keeps them.\nWe list results in Table 12  ###reference_###.\nThe evaluation set Adv Test only requires code-text matching, yet training with mixed-modal data formats still has benefits.\nTo avoid data contamination and ensure fairness, we performed de-duplication for the ProCQA dataset with respect to the relevant evaluation benchmarks from the same source, including CoNaLa, SO-DS and StaQC.\nIn Table 13  ###reference_###, we provide a quantitative analysis on the proportion of contaminated data for each evaluation set and the performance using raw and filtered version of the ProCQA dataset for pre-training.\nAlthough a large-proportion of the evaluation set is included in the raw pre-training data, removing them raises a limited degradation of model performance, as they only make up a small portion of the large-scale pre-training data."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1.   Settings",
            "text": "Our model basically follows the two-tower architecture in which vector representations for code and text are produced by mean pooling over the last layer hidden representations of the language models. It is trained via the contrastive objective using the InfoNCE loss\nwhere  denotes the question,  denotes the corresponding answer,  is a set of negative samples,  is the temperature.\n is also enlarged with other examples from the same batch.\nThe main baselines we compare to are GraphCodeBERT (Guo et al., 2021  ###reference_b11###) and CodeRetriever (Li et al., 2022  ###reference_b20###). In addition to the text-code pairs in CSN used by GraphCodeBERT, CodeRetriever also employs sophisticated rules and learned models for mining high-quality code-code and code-text pairs from the raw CSN code corpus.\nInstead we use commonly available QA pairs from ProCQA mined by weak supervision.\nWe use the training split across all languages to construct different types of mixed-modal positive pairs.\nWe apply modality-agnostic contrastive pre-training on the ProCQA and CSN dataset and compare our model to previous code embedding models on various retrieval tasks.\nOur model is denoted as MACP."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2.   Implementation Details",
            "text": "For fair comparison, our model is initialized with GraphCodeBERT, same as CodeRetriever.\nMACP is pre-trained with the contrastive objective in Equation 1  ###reference_### using cosine similarity and .\nTo balance low-resource languages, we sample each data batch from a multi-nominal distribution over different language subsets\nwith  equal to the size of subset  and smoothing parameter .\nWe run the contrastive pre-training for 10k steps with a global batch size of 6192.\nIn-batch negatives are used and shared across different GPUs.\nEach sequence is truncated at a maximum length of 128.\nThe learning rate is initially warmed up to 2e-4 for the first 10% steps, followed by a linear decay.\nWe utilize the same contrastive loss during fine-tuning on each downstream dataset.\nEach fine-tuning experiment only involves one dataset so we directly sample data after shuffling it.\nModels are trained using a peak learning rate of 2e-5 with the same scheduler as pre-training.\nThe maximum sequence length is 512.\nBatch size is 128 and each sample is accompanied with 7 randomly sampled negatives.\nTraining epochs is 3.\nOther hyperparameters are same as pre-training.\nWe only consider in-batch negatives for contrastive learning so we compare models under this setting.\nWe conduct all experiments on two NVIDIA A100 GPUs with 40G memory.\nWe use DeepSpeed, gradient checkpointing and mixed precision (FP16) encoding to reduce memory cost.\nThe pre-training process takes about 18 hours. Fine-tuning on all datasets is finished in one day."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "4.3.   Evaluation Benchmarks",
            "text": "To provide an extensive evaluation of the generalization ability of our pre-trained models, we select a large variety of code retrieval tasks from different domains under different settings.\nWe first evaluate on the CodeSearchNet benchmark (Husain et al., 2019  ###reference_b14###), which is widely used for evaluating the text-code search ability of code retrieval models.\nOne drawback of CodeSearchNet is the queries are not aligned to real user questions.\nSo, we also evaluate on some more challenging datasets, Adv Test (Lu et al., 2021  ###reference_b24###), CoSQA (Huang et al., 2021  ###reference_b13###), CoNaLa (Yin et al., 2018  ###reference_b34###), SO-DS (Heyman and Cutsem, 2020  ###reference_b12###), StaQC (Yao et al., 2018  ###reference_b33###).\nThe last three evaluation datasets follow the setting of  Heyman and Cutsem (2020  ###reference_b12###), where during inference both text description and code snippet are used for matching.\nThe main evaluation metric is MRR.\nThen, code-code search results on POJ-104 (Mou et al., 2016  ###reference_b25###) is also reported to evaluate the intra-modal retrieval ability.\nIn this dataset, Python program solutions of the same problem is regarded as positive pairs.\nThe objective is to retrieve relevant code snippets which answer the same problem.\nTo investigate the cross-lingual code retrieval ability, we use CodeNet (Puri et al., 2021  ###reference_b27###) as an evaluation benchmark, which is also a problem-solution dataset similar to POJ-104 but covers more languages.\nOn CodeNet, we consider the zero-shot retrieval of three programming languages (Ruby, Python and Java) following Guo et al. (2022  ###reference_b10###), where code is pre-processed by removing comments and replacing all separators with whitespace.\nPerformance is evaluated by MAP.\nFinally, to test whether our model can generalize to out-of-domain languages, we choose two text-code search datasets with languages unseen during pre-training. Smart Contracts (SC) (Yang et al., 2021  ###reference_b32###) contains Solidity programming language and Spider (Yu et al., 2018  ###reference_b35###) consists of SQL-query pairs.\nWe use the dataset split released by  Chai et al. (2022  ###reference_b4###).\nModels are evaluated by Recall@{1,5,10} and MRR@1000. The statistics of downstream evaluation benchmarks are illustrated in Table 6  ###reference_###."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "4.4.   Results",
            "text": "In this section, we report and discuss the performance of MACP on the evaluation benchmarks introduced in the previous section, spanning both supervised and zero-shot settings.\nIn the supervised setting, MACP is directly fine-tuned on full training set and the last checkpoint is evaluated on the test set.\nIn the zero-shot setting, it is directly evaluated on the test set.\nWe first present evaluation results on six programming language subsets of CodeSearchNet in Table 7  ###reference_###  ###reference_###.\nMACP trained with the newly proposed dataset outperforms previous best model CodeRetriever on all language subsets, by an average of 2.1 points.\nNext, we look at results on several challenging benchmarks, all collected from real-world user queries instead of docstrings.\nAs shown in Table 8  ###reference_###  ###reference_###, our model significantly outperforms prior state-of-the-art models by up to 10 points on average.\nWe attribute the improvement to real-world user queries from ProCQA.\nAfter evaluating the cross-modal search ability of our embedding model, we zoom into the intra-modal retrieval performance by evaluating on a code clone detection benchmark, POJ-104.\nResults are illustrated in Table 9  ###reference_###  ###reference_###.\nOur model outperforms previous best baseline CodeRetriever by +1.38 points.\nWe list the cross-lingual code retrieval performance of our model MACP and other baselines from  Guo et al. (2022  ###reference_b10###  ###reference_b10###) in Table 10  ###reference_###  ###reference_###.\nUniXCoder has significantly better zero-shot code retrieval performance, owing to its contrastive objective during pre-training.\nMACP consistently outperforms previous baselines by a large margin, setting new state-of-the-art performance on this task.\nIn Table 11  ###reference_###  ###reference_###, we compare different models\u2019 performance on Solidity and SQL, two languages unseen during pre-training.\nPrevious best model MAML (Chai et al., 2022  ###reference_b4###  ###reference_b4###) applied model-agnostic meta learning on CodeBERT (Feng et al., 2020  ###reference_b8###  ###reference_b8###) using Java and Python subsets from CSN for pre-training.\nIn addition, we also report the performance of GraphCodeBERT using our codebase as another baseline for comparison.\nOur model significantly improves the cross-domain code search performance on unseen languages.\nOne possible reason is that the diversity of language coverage in ProCQA equips the model with better language adaptation ability."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "4.5.   Analysis",
            "text": "We first investigate the effect of different pre-training corpus by doing a series of controlled experiments where only the pre-training data distribution is changed.\nWe run two additional experiments by using the CSN and ProCQA dataset individually for pre-training.\nDue to space limitation, we report downstream fine-tuned retrieval performance on CodeSearchNet in Figure 3  ###reference_###  ###reference_###.\nResults on other evaluation benchmarks follow the same trends.\nDespite CSN belongs to in-domain data for this evaluation benchmark, it still underperforms ProCQA when being used as a pre-training corpus.\nCombining two datasets gives better results.\nThis showcases the effectiveness of ProCQA dataset being used as a mixed-modal corpus for retrieval-oriented contrastive pre-training.\n###figure_4### We ablate on the choice of modality-agnostic contrastive learning by comparing to another setting which we explicitly distinguish text and code in data design.\nDue to the high difficulty of parsing incomplete code snippets in ProCQA, we conduct this ablation on the CSN pre-training corpus where code are well formed and can be parsed by existing tools. Bi-modal setting removes all comments in the code while mixed-modal setting keeps them.\nWe list results in Table 12  ###reference_###  ###reference_###.\nThe evaluation set Adv Test only requires code-text matching, yet training with mixed-modal data formats still has benefits.\nTo avoid data contamination and ensure fairness, we performed de-duplication for the ProCQA dataset with respect to the relevant evaluation benchmarks from the same source, including CoNaLa, SO-DS and StaQC.\nIn Table 13  ###reference_###  ###reference_###, we provide a quantitative analysis on the proportion of contaminated data for each evaluation set and the performance using raw and filtered version of the ProCQA dataset for pre-training.\nAlthough a large-proportion of the evaluation set is included in the raw pre-training data, removing them raises a limited degradation of model performance, as they only make up a small portion of the large-scale pre-training data."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Conclusion",
            "text": "In this work we introduce ProCQA, a large-scale community-based programming question answering dataset mined from StackOverflow with strict filtering strategies for quality and fairness control.\nProCQA is featured by its practicality, diversity and code-mixing data format.\nFurthermore, through modality-agnostic contrastive pre-training on interleaved code and text data, our new dataset yields a language model that has a better aligned representation space between code and text, achieving state-of-the-art performance on a large spectrum of code retrieval tasks.\nIn future work, it would be interesting to explore the benefit of ProCQA to other generative code QA tasks."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Acknowledgements",
            "text": "This work was supported by the National Natural Science Foundation of China (No.61977003) and the State Key Laboratory of Complex & Critical Software Environment (CCSE-2024ZX-16)."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7.   Bibliographical References",
            "text": ""
        }
    ],
    "appendix": [],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S1.T1\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S1.T1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" id=\"S1.T1.1.1.1.1\">Question</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\" id=\"S1.T1.1.1.1.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S1.T1.1.1.1.2.1\">Segmentation fault while copying a string to the memory allocated array</p>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S1.T1.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S1.T1.1.2.1.1\">Description</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S1.T1.1.2.1.2\">\n<div class=\"ltx_block ltx_align_top\" id=\"S1.T1.1.2.1.2.1\">\n<p class=\"ltx_p\" id=\"S1.T1.1.2.1.2.1.1\">Following is a program I am practicing;\nint main()</p>\n<p class=\"ltx_p\" id=\"S1.T1.1.2.1.2.1.2\">int i = 0;\nchar **grid = (char **) malloc(5*sizeof(int));\nfor (i = 0 ; i &lt; 5 ; i++)\ngrid[i] = (char *) malloc(6);\nstrcpy(grid[0], \"eabcd\");\nstrcpy(grid[1], \"fghij\");\nstrcpy(grid[2], \"olkmn\");\nstrcpy(grid[3], \"trpqs\");\nstrcpy(grid[4], \"xywuv\"); /*Segmentation Fault at this line*/\nreturn 0;\n\nI am getting a segmentation fault at the line strcpy(grid[4], \"xywuv\"); . What could be the reason? I have allocated the array to have 5 strings(rows) of 6 characters each(columns).</p>\n</div>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" id=\"S1.T1.1.3.2.1\">Answer</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_t\" id=\"S1.T1.1.3.2.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S1.T1.1.3.2.2.1\">You are allocating the wrong type at line 3\nchar **grid = (char **) malloc(5*sizeof(int));\nShould be\nchar **grid = (char **) malloc(5*sizeof(char*));\nThis is because you are declaring string-array. Therefore, the malloc should be char* (string / character pointer)\nAlso the same if you were trying to declare 2-D integer array. It will be\nint **grid = (int **) malloc(5*sizeof(int*));</p>\n</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>An example sampled from the C programming language subset of ProCQA. Text and code are interleaved in these QA pairs.</figcaption>\n</figure>",
            "capture": "Table 1: An example sampled from the C programming language subset of ProCQA. Text and code are interleaved in these QA pairs."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T2\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S3.T2.1\" style=\"width:455.2pt;height:42.4pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(50.3pt,-4.7pt) scale(1.28378042224783,1.28378042224783) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S3.T2.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S3.T2.1.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.1.1.1.1\">PL</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.1.1.1.1.2\">C</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.1.1.1.1.3\">C++</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.1.1.1.1.4\">Java</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.1.1.1.1.5\">Python</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.1.1.1.1.6\">Ruby</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.1.1.1.1.7\">Lisp</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.1.1.1.1.8\">JavaScript</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.1.1.1.1.9\">C#</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.1.1.1.1.10\">Go</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.1.1.1.1.11\">Rust</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.1.1.1.1.12\">PHP</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.2.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t\" id=\"S3.T2.1.1.2.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.2.1.1.1\">Size</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T2.1.1.2.1.2\">204746</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T2.1.1.2.1.3\">418346</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T2.1.1.2.1.4\">831697</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T2.1.1.2.1.5\">1008478</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T2.1.1.2.1.6\">131218</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T2.1.1.2.1.7\">4612</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T2.1.1.2.1.8\">1217095</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T2.1.1.2.1.9\">817970</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T2.1.1.2.1.10\">36011</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T2.1.1.2.1.11\">15514</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T2.1.1.2.1.12\">567357</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Number of QA pairs for each programming language in ProCQA.</figcaption>\n</figure>",
            "capture": "Table 2: Number of QA pairs for each programming language in ProCQA."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T3\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T3.6\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T3.6.7.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.6.7.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.6.7.1.1.1\">Dataset</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.6.7.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.6.7.1.2.1\"># of PLs</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.6.7.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.6.7.1.3.1\">Data Format</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.6.7.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.6.7.1.4.1\">Size</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.6.7.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.6.7.1.5.1\">Data Source</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T3.1.1.2\">CodeNN</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.1.1.3\">2</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T3.1.1.4\">Title, code</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.1.1.1\">\n187K pairs</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.1.1.5\">StackOverflow</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.2.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.2.2.2\">CodeSearchNet</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.2.2.3\">6</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.2.2.4\">Comment, code</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.2.2.1\">\n2M pairs</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.2.2.5\">GitHub</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.3.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.3.3.2\">CodeQA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.3.3.3\">2</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.3.3.4\">Question, answer, code</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.3.3.1\">\n190K pairs</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.3.3.5\">GitHub</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.4.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.4.4.2\">CoSQA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.4.4.3\">1</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.4.4.4\">Query, code</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.4.4.1\">\n20K pairs</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.4.4.5\">Web search</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.5.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.5.5.2\">CS1QA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.5.5.3\">1</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.5.5.4\">Chat log, question, answer, type, code</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.5.5.1\">\n9K pairs</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.5.5.5\">Classroom</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.6.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S3.T3.6.6.2\">ProCQA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T3.6.6.3\">11</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S3.T3.6.6.4\">Question, description, answer</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T3.6.6.1\">\n5M pairs</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T3.6.6.5\">StackOverflow</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Comparison between different code-based datasets.</figcaption>\n</figure>",
            "capture": "Table 3: Comparison between different code-based datasets."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T4\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T4.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T4.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S3.T4.1.1.1.1\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T4.1.1.1.2\">MRR@10</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T4.1.1.1.3\">R@10</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T4.1.1.1.4\">R@100</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T4.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T4.1.2.1.1\">BM25</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T4.1.2.1.2\">51.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T4.1.2.1.3\">61.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T4.1.2.1.4\">73.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T4.1.3.2.1\">BERT</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T4.1.3.2.2\">48.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T4.1.3.2.3\">62.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T4.1.3.2.4\">79.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T4.1.4.3.1\">CodeBERT</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T4.1.4.3.2\">53.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T4.1.4.3.3\">66.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T4.1.4.3.4\">83.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.1.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S3.T4.1.5.4.1\">UniXCoder</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T4.1.5.4.2\">58.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T4.1.5.4.3\">71.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T4.1.5.4.4\">86.1</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>Answer retrieval performance of different language models on the C subset of ProCQA.</figcaption>\n</figure>",
            "capture": "Table 4: Answer retrieval performance of different language models on the C subset of ProCQA."
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T5\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T5.3\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T5.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S3.T5.3.3.4\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T5.1.1.1\">ROUGE\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T5.2.2.2\">ROUGE\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T5.3.3.3\">ROUGE\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T5.3.4.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T5.3.4.1.1\">T5</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T5.3.4.1.2\">14.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T5.3.4.1.3\">2.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T5.3.4.1.4\">11.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T5.3.5.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T5.3.5.2.1\">CodeT5</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T5.3.5.2.2\">17.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T5.3.5.2.3\">4.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T5.3.5.2.4\">14.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T5.3.6.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S3.T5.3.6.3.1\">PLBART</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T5.3.6.3.2\">19.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T5.3.6.3.3\">5.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T5.3.6.3.4\">15.3</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>Answer generation results of different baselines on ProCQA (C subset).</figcaption>\n</figure>",
            "capture": "Table 5: Answer generation results of different baselines on ProCQA (C subset)."
        },
        "6": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T6\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T6.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T6.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" id=\"S4.T6.1.1.1.1\">Dataset</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" id=\"S4.T6.1.1.1.2\">Lang</th>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S4.T6.1.1.1.3\">Train</td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S4.T6.1.1.1.4\">Valid</td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S4.T6.1.1.1.5\">Test</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T6.1.2.2.1\">CSN</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T6.1.2.2.2\">Ruby</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T6.1.2.2.3\">24.9K</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T6.1.2.2.4\">1.4K</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T6.1.2.2.5\">1.3K</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T6.1.3.3.1\">CSN</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T6.1.3.3.2\">JS</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.1.3.3.3\">58K</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.1.3.3.4\">3.9K</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.1.3.3.5\">3.3K</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T6.1.4.4.1\">CSN</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T6.1.4.4.2\">Go</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.1.4.4.3\">167K</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.1.4.4.4\">7.3K</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.1.4.4.5\">8.1K</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T6.1.5.5.1\">CSN</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T6.1.5.5.2\">Python</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.1.5.5.3\">252K</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.1.5.5.4\">13.9K</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.1.5.5.5\">14.9K</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.6.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T6.1.6.6.1\">CSN</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T6.1.6.6.2\">Java</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.1.6.6.3\">165K</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.1.6.6.4\">5.2K</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.1.6.6.5\">10.9K</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.7.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T6.1.7.7.1\">CSN</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T6.1.7.7.2\">PHP</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.1.7.7.3\">241K</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.1.7.7.4\">13.0K</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.1.7.7.5\">14.0K</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.8.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T6.1.8.8.1\">Adv</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T6.1.8.8.2\">Python</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T6.1.8.8.3\">28.0K</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T6.1.8.8.4\">9.6K</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T6.1.8.8.5\">19.2K</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.9.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T6.1.9.9.1\">CoSQA</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T6.1.9.9.2\">Python</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.1.9.9.3\">19K</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.1.9.9.4\">0.5K</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.1.9.9.5\">0.5K</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.10.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T6.1.10.10.1\">CoNaLa</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T6.1.10.10.2\">Python</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.1.10.10.3\">2.8K</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.1.10.10.4\">-</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.1.10.10.5\">0.8K</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.11.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T6.1.11.11.1\">SO-DS</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T6.1.11.11.2\">Python</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.1.11.11.3\">14.2K</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.1.11.11.4\">0.9K</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.1.11.11.5\">1.1K</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.12.12\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T6.1.12.12.1\">StaQC</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T6.1.12.12.2\">Python</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.1.12.12.3\">20.4K</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.1.12.12.4\">2.6K</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.1.12.12.5\">2.7K</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.13.13\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T6.1.13.13.1\">POJ104</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T6.1.13.13.2\">Python</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T6.1.13.13.3\">32K</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T6.1.13.13.4\">8K</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T6.1.13.13.5\">12K</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.14.14\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T6.1.14.14.1\">CodeNet</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T6.1.14.14.2\">Ruby</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T6.1.14.14.3\">-</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T6.1.14.14.4\">-</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T6.1.14.14.5\">11.7K</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.15.15\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T6.1.15.15.1\">CodeNet</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T6.1.15.15.2\">Python</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.1.15.15.3\">-</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.1.15.15.4\">-</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.1.15.15.5\">15.6K</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.16.16\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T6.1.16.16.1\">CodeNet</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T6.1.16.16.2\">Java</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.1.16.16.3\">-</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.1.16.16.4\">-</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.1.16.16.5\">23.5K</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.17.17\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T6.1.17.17.1\">SC</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T6.1.17.17.2\">Solidity</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T6.1.17.17.3\">57K</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T6.1.17.17.4\">4.1K</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T6.1.17.17.5\">1K</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.18.18\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S4.T6.1.18.18.1\">Spider</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S4.T6.1.18.18.2\">SQL</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S4.T6.1.18.18.3\">14K</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S4.T6.1.18.18.4\">2.1K</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S4.T6.1.18.18.5\">1K</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 6: </span>Statistics of downstream evaluation datasets.</figcaption>\n</figure>",
            "capture": "Table 6: Statistics of downstream evaluation datasets."
        },
        "7": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T7\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T7.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T7.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S4.T7.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.1.1.1.1.1\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T7.1.1.1.2\">Ruby</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T7.1.1.1.3\">Javascript</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T7.1.1.1.4\">Go</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T7.1.1.1.5\">Python</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T7.1.1.1.6\">Java</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T7.1.1.1.7\">PHP</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T7.1.1.1.8\">Overall</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T7.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T7.1.2.1.1\">ContraCode\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Jain et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16702v1#bib.bib16\" title=\"\">2021</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T7.1.2.1.2\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T7.1.2.1.3\">30.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T7.1.2.1.4\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T7.1.2.1.5\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T7.1.2.1.6\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T7.1.2.1.7\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T7.1.2.1.8\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T7.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T7.1.3.2.1\">SyncoBERT\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16702v1#bib.bib30\" title=\"\">2021a</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.3.2.2\">72.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.3.2.3\">67.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.3.2.4\">91.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.3.2.5\">72.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.3.2.6\">72.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.3.2.7\">67.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.3.2.8\">74.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T7.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T7.1.4.3.1\">CodeBERT\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Feng et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16702v1#bib.bib8\" title=\"\">2020</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.4.3.2\">67.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.4.3.3\">62.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.4.3.4\">88.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.4.3.5\">67.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.4.3.6\">67.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.4.3.7\">62.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.4.3.8\">69.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T7.1.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T7.1.5.4.1\">GraphCodeBERT\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Guo et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16702v1#bib.bib11\" title=\"\">2021</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.5.4.2\">70.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.5.4.3\">64.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.5.4.4\">89.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.5.4.5\">69.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.5.4.6\">69.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.5.4.7\">64.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.5.4.8\">71.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T7.1.6.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T7.1.6.5.1\">UniXcoder\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Guo et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16702v1#bib.bib10\" title=\"\">2022</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.6.5.2\">74.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.6.5.3\">68.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.6.5.4\">91.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.6.5.5\">72.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.6.5.6\">72.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.6.5.7\">67.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.6.5.8\">74.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T7.1.7.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T7.1.7.6.1\">CodeRetriever\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Li et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16702v1#bib.bib20\" title=\"\">2022</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.7.6.2\">75.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.7.6.3\">69.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.7.6.4\">91.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.7.6.5\">73.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.7.6.6\">74.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.7.6.7\">68.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.7.6.8\">75.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T7.1.8.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" id=\"S4.T7.1.8.7.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T7.1.8.7.1.1\">MACP</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T7.1.8.7.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.1.8.7.2.1\">77.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T7.1.8.7.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.1.8.7.3.1\">72.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T7.1.8.7.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.1.8.7.4.1\">92.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T7.1.8.7.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.1.8.7.5.1\">76.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T7.1.8.7.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.1.8.7.6.1\">75.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T7.1.8.7.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.1.8.7.7.1\">70.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T7.1.8.7.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.1.8.7.8.1\">77.4</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 7: </span>MRR@1k on six programming language test sets of the CodeSearchNet.</figcaption>\n</figure>",
            "capture": "Table 7: MRR@1k on six programming language test sets of the CodeSearchNet."
        },
        "8": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T8\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T8.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T8.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S4.T8.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T8.1.1.1.1.1\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T8.1.1.1.2\">Adv</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T8.1.1.1.3\">CoSQA</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T8.1.1.1.4\">CoNaLa</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T8.1.1.1.5\">SO-DS</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T8.1.1.1.6\">StaQC</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T8.1.1.1.7\">Overall</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T8.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T8.1.2.1.1\">SyncoBERT\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16702v1#bib.bib30\" title=\"\">2021a</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T8.1.2.1.2\">38.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T8.1.2.1.3\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T8.1.2.1.4\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T8.1.2.1.5\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T8.1.2.1.6\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T8.1.2.1.7\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T8.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T8.1.3.2.1\">CodeBERT\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Feng et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16702v1#bib.bib8\" title=\"\">2020</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.3.2.2\">27.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.3.2.3\">64.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.3.2.4\">20.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.3.2.5\">23.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.3.2.6\">23.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.3.2.7\">31.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T8.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T8.1.4.3.1\">GraphCodeBERT\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Guo et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16702v1#bib.bib11\" title=\"\">2021</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.4.3.2\">35.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.4.3.3\">67.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.4.3.4\">23.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.4.3.5\">25.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.4.3.6\">23.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.4.3.7\">35.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T8.1.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T8.1.5.4.1\">UniXcoder\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Guo et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16702v1#bib.bib10\" title=\"\">2022</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.5.4.2\">41.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.5.4.3\">70.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.5.4.4\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.5.4.5\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.5.4.6\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.5.4.7\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T8.1.6.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T8.1.6.5.1\">CodeRetriever\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Li et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16702v1#bib.bib20\" title=\"\">2022</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.6.5.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T8.1.6.5.2.1\">43.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.6.5.3\">70.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.6.5.4\">29.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.6.5.5\">27.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.6.5.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T8.1.6.5.6.1\">25.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.6.5.7\">39.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T8.1.7.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" id=\"S4.T8.1.7.6.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T8.1.7.6.1.1\">MACP</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T8.1.7.6.2\">39.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T8.1.7.6.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T8.1.7.6.3.1\">72.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T8.1.7.6.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T8.1.7.6.4.1\">61.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T8.1.7.6.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T8.1.7.6.5.1\">48.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T8.1.7.6.6\">23.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T8.1.7.6.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T8.1.7.6.7.1\">49.0</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 8: </span>Text-code search performance (MRR@1k) on datasets that are closer to the real scenario.</figcaption>\n</figure>",
            "capture": "Table 8: Text-code search performance (MRR@1k) on datasets that are closer to the real scenario."
        },
        "9": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T9\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T9.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T9.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S4.T9.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T9.1.1.1.1.1\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T9.1.1.1.2\">MAP</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T9.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T9.1.2.1.1\">RoBERTa\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16702v1#bib.bib23\" title=\"\">2019</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T9.1.2.1.2\">76.67</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T9.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T9.1.3.2.1\">CodeBERT\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Feng et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16702v1#bib.bib8\" title=\"\">2020</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T9.1.3.2.2\">82.67</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T9.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T9.1.4.3.1\">GraphCodeBERT\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Guo et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16702v1#bib.bib11\" title=\"\">2021</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T9.1.4.3.2\">85.16</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T9.1.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T9.1.5.4.1\">SynCoBERT\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16702v1#bib.bib30\" title=\"\">2021a</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T9.1.5.4.2\">88.24</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T9.1.6.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T9.1.6.5.1\">DISCO\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Ding et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16702v1#bib.bib7\" title=\"\">2022</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T9.1.6.5.2\">82.77</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T9.1.7.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T9.1.7.6.1\">Corder\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Bui et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16702v1#bib.bib3\" title=\"\">2021</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T9.1.7.6.2\">84.10</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T9.1.8.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T9.1.8.7.1\">CodeRetriever\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Li et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16702v1#bib.bib20\" title=\"\">2022</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T9.1.8.7.2\">88.85</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T9.1.9.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" id=\"S4.T9.1.9.8.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T9.1.9.8.1.1\">MACP</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T9.1.9.8.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T9.1.9.8.2.1\">90.23</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 9: </span>Performance of Python code-to-code retrieval task on POJ-104.</figcaption>\n</figure>",
            "capture": "Table 9: Performance of Python code-to-code retrieval task on POJ-104."
        },
        "10": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T10\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T10.1\" style=\"width:455.2pt;height:172.7pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(53.6pt,-20.3pt) scale(1.30801268621941,1.30801268621941) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T10.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T10.1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S4.T10.1.1.1.1.1\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T10.1.1.1.1.1.1\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" id=\"S4.T10.1.1.1.1.2\">Ruby</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" id=\"S4.T10.1.1.1.1.3\">Python</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" id=\"S4.T10.1.1.1.1.4\">Java</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T10.1.1.1.1.5\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T10.1.1.1.1.5.1\">Overall</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T10.1.1.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T10.1.1.2.2.1\">Ruby</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T10.1.1.2.2.2\">Python</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T10.1.1.2.2.3\">Java</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T10.1.1.2.2.4\">Ruby</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T10.1.1.2.2.5\">Python</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T10.1.1.2.2.6\">Java</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T10.1.1.2.2.7\">Ruby</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T10.1.1.2.2.8\">Python</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T10.1.1.2.2.9\">Java</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T10.1.1.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T10.1.1.3.1.1\">CodeBERT</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T10.1.1.3.1.2\">13.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T10.1.1.3.1.3\">3.18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T10.1.1.3.1.4\">0.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T10.1.1.3.1.5\">3.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T10.1.1.3.1.6\">14.39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T10.1.1.3.1.7\">0.96</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T10.1.1.3.1.8\">0.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T10.1.1.3.1.9\">0.42</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T10.1.1.3.1.10\">7.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T10.1.1.3.1.11\">4.94</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T10.1.1.4.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T10.1.1.4.2.1\">GraphCodeBERT</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T10.1.1.4.2.2\">17.01</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T10.1.1.4.2.3\">9.29</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T10.1.1.4.2.4\">6.38</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T10.1.1.4.2.5\">5.01</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T10.1.1.4.2.6\">19.34</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T10.1.1.4.2.7\">6.92</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T10.1.1.4.2.8\">1.77</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T10.1.1.4.2.9\">3.50</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T10.1.1.4.2.10\">13.31</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T10.1.1.4.2.11\">9.17</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T10.1.1.5.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T10.1.1.5.3.1\">PLBART</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T10.1.1.5.3.2\">18.60</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T10.1.1.5.3.3\">10.76</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T10.1.1.5.3.4\">1.90</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T10.1.1.5.3.5\">8.27</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T10.1.1.5.3.6\">19.55</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T10.1.1.5.3.7\">1.98</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T10.1.1.5.3.8\">1.47</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T10.1.1.5.3.9\">1.27</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T10.1.1.5.3.10\">10.41</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T10.1.1.5.3.11\">8.25</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T10.1.1.6.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T10.1.1.6.4.1\">CodeT5</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T10.1.1.6.4.2\">18.22</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T10.1.1.6.4.3\">10.02</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T10.1.1.6.4.4\">1.81</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T10.1.1.6.4.5\">8.74</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T10.1.1.6.4.6\">17.83</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T10.1.1.6.4.7\">1.58</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T10.1.1.6.4.8\">1.13</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T10.1.1.6.4.9\">0.81</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T10.1.1.6.4.10\">10.18</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T10.1.1.6.4.11\">7.81</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T10.1.1.7.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T10.1.1.7.5.1\">UniXcoder</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T10.1.1.7.5.2\">29.05</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T10.1.1.7.5.3\">26.36</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T10.1.1.7.5.4\">15.16</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T10.1.1.7.5.5\">23.96</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T10.1.1.7.5.6\">30.15</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T10.1.1.7.5.7\">15.07</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T10.1.1.7.5.8\">13.61</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T10.1.1.7.5.9\">14.53</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T10.1.1.7.5.10\">16.12</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T10.1.1.7.5.11\">20.45</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T10.1.1.8.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" id=\"S4.T10.1.1.8.6.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T10.1.1.8.6.1.1\">MACP</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T10.1.1.8.6.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T10.1.1.8.6.2.1\">44.74</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T10.1.1.8.6.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T10.1.1.8.6.3.1\">43.11</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T10.1.1.8.6.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T10.1.1.8.6.4.1\">31.26</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T10.1.1.8.6.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T10.1.1.8.6.5.1\">40.59</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T10.1.1.8.6.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T10.1.1.8.6.6.1\">45.77</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T10.1.1.8.6.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T10.1.1.8.6.7.1\">29.75</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T10.1.1.8.6.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T10.1.1.8.6.8.1\">32.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T10.1.1.8.6.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T10.1.1.8.6.9.1\">33.75</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T10.1.1.8.6.10\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T10.1.1.8.6.10.1\">30.74</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T10.1.1.8.6.11\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T10.1.1.8.6.11.1\">36.95</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 10: </span>MAP score (%) of zero-shot code-to-code search task on CodeNet.</figcaption>\n</figure>",
            "capture": "Table 10: MAP score (%) of zero-shot code-to-code search task on CodeNet."
        },
        "11": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T11\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T11.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T11.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S4.T11.1.1.1.1\" rowspan=\"2\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T11.1.1.1.1.1\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"4\" id=\"S4.T11.1.1.1.2\">Solidity</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"4\" id=\"S4.T11.1.1.1.3\">SQL</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T11.1.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T11.1.2.2.1\">R@1</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T11.1.2.2.2\">R@5</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T11.1.2.2.3\">R@10</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T11.1.2.2.4\">MRR</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T11.1.2.2.5\">R@1</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T11.1.2.2.6\">R@5</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T11.1.2.2.7\">R@10</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T11.1.2.2.8\">MRR</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T11.1.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T11.1.3.1.1\">CodeBERT\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Feng et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16702v1#bib.bib8\" title=\"\">2020</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T11.1.3.1.2\">53.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T11.1.3.1.3\">77.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T11.1.3.1.4\">84.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T11.1.3.1.5\">64.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T11.1.3.1.6\">67.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T11.1.3.1.7\">92.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T11.1.3.1.8\">96.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T11.1.3.1.9\">78.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T11.1.4.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T11.1.4.2.1\">MAML\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Chai et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16702v1#bib.bib4\" title=\"\">2022</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T11.1.4.2.2\">65.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T11.1.4.2.3\">82.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T11.1.4.2.4\">87.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T11.1.4.2.5\">73.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T11.1.4.2.6\">74.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T11.1.4.2.7\">95.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T11.1.4.2.8\">97.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T11.1.4.2.9\">83.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T11.1.5.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T11.1.5.3.1\">GraphCodeBERT\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Guo et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16702v1#bib.bib11\" title=\"\">2021</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T11.1.5.3.2\">72.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T11.1.5.3.3\">85.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T11.1.5.3.4\">89.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T11.1.5.3.5\">78.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T11.1.5.3.6\">78.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T11.1.5.3.7\">94.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T11.1.5.3.8\">96.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T11.1.5.3.9\">85.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T11.1.6.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" id=\"S4.T11.1.6.4.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T11.1.6.4.1.1\">MACP</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T11.1.6.4.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T11.1.6.4.2.1\">75.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T11.1.6.4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T11.1.6.4.3.1\">87.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T11.1.6.4.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T11.1.6.4.4.1\">90.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T11.1.6.4.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T11.1.6.4.5.1\">80.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T11.1.6.4.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T11.1.6.4.6.1\">85.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T11.1.6.4.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T11.1.6.4.7.1\">96.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T11.1.6.4.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T11.1.6.4.8.1\">97.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T11.1.6.4.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T11.1.6.4.9.1\">90.3</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 11: </span>\nResults of cross-domain code retrieval on programming languages unseen during pretraining.</figcaption>\n</figure>",
            "capture": "Table 11: \nResults of cross-domain code retrieval on programming languages unseen during pretraining."
        },
        "12": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T12\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T12.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T12.1.1.1\">\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S4.T12.1.1.1.1\">Setting</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T12.1.1.1.2\">Adv Test</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T12.1.2.1\">\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t\" id=\"S4.T12.1.2.1.1\">bi-modal</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T12.1.2.1.2\">38.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T12.1.3.2\">\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb\" id=\"S4.T12.1.3.2.1\">mixed-modal</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T12.1.3.2.2\">39.1</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 12: </span>Effect of the data format used in CodeSearchNet corpus pre-training. We report MRR@1k on Adv Test set.</figcaption>\n</figure>",
            "capture": "Table 12: Effect of the data format used in CodeSearchNet corpus pre-training. We report MRR@1k on Adv Test set."
        },
        "13": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T13\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T13.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T13.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S4.T13.1.1.1.1\">Dataset</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T13.1.1.1.2\">Proportion</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T13.1.1.1.3\">Unfiltered</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T13.1.1.1.4\">Filtered</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T13.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T13.1.2.1.1\">CoNaLa</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T13.1.2.1.2\">83.7%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T13.1.2.1.3\">62.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T13.1.2.1.4\">61.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T13.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T13.1.3.2.1\">SO-DS</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T13.1.3.2.2\">48.4%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T13.1.3.2.3\">49.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T13.1.3.2.4\">48.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T13.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S4.T13.1.4.3.1\">StaQC</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T13.1.4.3.2\">31.4%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T13.1.4.3.3\">23.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T13.1.4.3.4\">23.3</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 13: </span>Analysis on the influence of data contamination on three evaluation datasets.</figcaption>\n</figure>",
            "capture": "Table 13: Analysis on the influence of data contamination on three evaluation datasets."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.16702v1_figure_1.png",
            "caption": "Figure 1: Illustration of different data formats used for contrastive representation alignment. Color represents chunk modality. Unimodal data focuses on code-to-code matching, while bimodal data emphasizes cross-modal matching. The mixed-modal data in ProCQA enables simultaneous learning of all matching patterns."
        },
        "2": {
            "figure_path": "2403.16702v1_figure_2.png",
            "caption": "Figure 2: Question and answer length distribution in ProCQA (C subset)."
        },
        "3": {
            "figure_path": "2403.16702v1_figure_3.png",
            "caption": "Figure 3: Ablation of the pre-training corpus. Results compared on test sets of CodeSearchNet."
        }
    },
    "references": [
        {
            "1": {
                "title": "Unified pre-training for program understanding and generation.",
                "author": "Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021.",
                "venue": "In Proceedings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language\nTechnologies, pages 2655\u20132668.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Santacoder: Don\u2019t reach for the stars!",
                "author": "Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki,\nCarlos Mu\u00f1oz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu,\nManan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel\nLamy-Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel\nRomero, Michael Lappert, Francesco De Toni, Bernardo Garc\u00eda del\nR\u00edo, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian\nYu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen,\nDanish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean\nHughes, Daniel Fried, Arjun Guha, Harm de Vries, and Leandro von Werra. 2023.",
                "venue": "CoRR, abs/2301.03988.",
                "url": null
            }
        },
        {
            "3": {
                "title": "Self-supervised contrastive learning for code retrieval and\nsummarization via semantic-preserving transformations.",
                "author": "Nghi D. Q. Bui, Yijun Yu, and Lingxiao Jiang. 2021.",
                "venue": "In Proceedings of the 44th International ACM SIGIR\nConference on Research and Development in Information Retrieval, pages\n511\u2013521.",
                "url": null
            }
        },
        {
            "4": {
                "title": "Cross-domain deep code search with meta learning.",
                "author": "Yitian Chai, Hongyu Zhang, Beijun Shen, and Xiaodong Gu. 2022.",
                "venue": "In Proceedings of the 44th IEEE/ACM International Conference\non Software Engineering, pages 487\u2013498.",
                "url": null
            }
        },
        {
            "5": {
                "title": "Cross-lingual language model pretraining.",
                "author": "Alexis Conneau and Guillaume Lample. 2019.",
                "venue": "In Proceedings of the 2019 Annual Conference on Neural\nInformation Processing Systems, pages 7057\u20137067.",
                "url": null
            }
        },
        {
            "6": {
                "title": "BERT: Pre-training of deep bidirectional transformers for language\nunderstanding.",
                "author": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.",
                "venue": "In Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language\nTechnologies, pages 4171\u20134186.",
                "url": null
            }
        },
        {
            "7": {
                "title": "Towards learning (dis)-similarity of source code from program\ncontrasts.",
                "author": "Yangruibo Ding, Luca Buratti, Saurabh Pujar, Alessandro Morari, Baishakhi Ray,\nand Saikat Chakraborty. 2022.",
                "venue": "In Proceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics, pages 6300\u20136312.",
                "url": null
            }
        },
        {
            "8": {
                "title": "CodeBERT: A pre-trained model for programming and natural\nlanguages.",
                "author": "Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun\nShou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020.",
                "venue": "In Findings of the Association for Computational Linguistics:\nEMNLP 2020, pages 1536\u20131547.",
                "url": null
            }
        },
        {
            "9": {
                "title": "Deep code search.",
                "author": "Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018.",
                "venue": "In Proceedings of the 40th International Conference on Software\nEngineering, pages 933\u2013944.",
                "url": null
            }
        },
        {
            "10": {
                "title": "UniXcoder: Unified cross-modal pre-training for code\nrepresentation.",
                "author": "Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, and Jian Yin. 2022.",
                "venue": "In Proceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics, pages 7212\u20137225.",
                "url": null
            }
        },
        {
            "11": {
                "title": "GraphCodeBERT: Pre-training code representations with data flow.",
                "author": "Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou,\nNan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun Deng,\nColin B. Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, and\nMing Zhou. 2021.",
                "venue": "In Proceedings of the 9th International Conference on Learning\nRepresentations.",
                "url": null
            }
        },
        {
            "12": {
                "title": "Neural code search revisited: Enhancing code snippet retrieval\nthrough natural language intent.",
                "author": "Geert Heyman and Tom Van Cutsem. 2020.",
                "venue": "CoRR, abs/2008.12193.",
                "url": null
            }
        },
        {
            "13": {
                "title": "CoSQA: 20, 000+ web queries for code search and question answering.",
                "author": "Junjie Huang, Duyu Tang, Linjun Shou, Ming Gong, Ke Xu, Daxin Jiang, Ming Zhou,\nand Nan Duan. 2021.",
                "venue": "In Proceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International Joint Conference on\nNatural Language Processing, pages 5690\u20135700.",
                "url": null
            }
        },
        {
            "14": {
                "title": "CodeSearchNet challenge: Evaluating the state of semantic code\nsearch.",
                "author": "Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc\nBrockschmidt. 2019.",
                "venue": "CoRR, abs/1909.09436.",
                "url": null
            }
        },
        {
            "15": {
                "title": "Summarizing source code using a neural attention model.",
                "author": "Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016.",
                "venue": "In Proceedings of the 54th Annual Meeting of the Association\nfor Computational Linguistics, pages 2073\u20132083.",
                "url": null
            }
        },
        {
            "16": {
                "title": "Contrastive code representation learning.",
                "author": "Paras Jain, Ajay Jain, Tianjun Zhang, Pieter Abbeel, Joseph Gonzalez, and Ion\nStoica. 2021.",
                "venue": "In Proceedings of the 2021 Conference on Empirical Methods in\nNatural Language Processing, pages 5954\u20135971.",
                "url": null
            }
        },
        {
            "17": {
                "title": "Learning and evaluating contextual embedding of source code.",
                "author": "Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. 2020.",
                "venue": "In Proceedings of the 37th International Conference on Machine\nLearning, pages 5110\u20135121.",
                "url": null
            }
        },
        {
            "18": {
                "title": "CS1QA: A dataset for assisting code-based question answering in\nan introductory programming course.",
                "author": "Changyoon Lee, Yeon Seonwoo, and Alice Oh. 2022.",
                "venue": "In Proceedings of the 2022 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language\nTechnologies, pages 2026\u20132040.",
                "url": null
            }
        },
        {
            "19": {
                "title": "StarCoder: May the source be with you!",
                "author": "Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov,\nChenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu,\nEvgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig\nDavaadorj, Joel Lamy-Poirier, Jo\u00e3o Monteiro, Oleh Shliazhko, Nicolas\nGontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi,\nJian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy V,\nJason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan\nDey, Zhihan Zhang, Nour Moustafa-Fahmy, Urvashi Bhattacharyya, Wenhao Yu,\nSwayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov,\nManuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger,\nHailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer\nRobinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor,\nSiva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite,\nCarlos Mu\u00f1oz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro\nvon Werra, and Harm de Vries. 2023.",
                "venue": "CoRR, abs/2305.06161.",
                "url": null
            }
        },
        {
            "20": {
                "title": "CodeRetriever: A large scale contrastive pre-training method for\ncode search.",
                "author": "Xiaonan Li, Yeyun Gong, Yelong Shen, Xipeng Qiu, Hang Zhang, Bolun Yao, Weizhen\nQi, Daxin Jiang, Weizhu Chen, and Nan Duan. 2022.",
                "venue": "In Proceedings of the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 2898\u20132910.",
                "url": null
            }
        },
        {
            "21": {
                "title": "ROUGE: A package for automatic evaluation of summaries.",
                "author": "Chin-Yew Lin. 2004.",
                "venue": "In Proceedings of ACL Workshop on Text Summarization Branches\nOut, pages 74\u201381.",
                "url": null
            }
        },
        {
            "22": {
                "title": "CodeQA: A question answering dataset for source code\ncomprehension.",
                "author": "Chenxiao Liu and Xiaojun Wan. 2021.",
                "venue": "In Findings of the Association for Computational Linguistics:\nEMNLP 2021, pages 2618\u20132632.",
                "url": null
            }
        },
        {
            "23": {
                "title": "Roberta: A robustly optimized BERT pretraining approach.",
                "author": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.",
                "venue": "CoRR, abs/1907.11692.",
                "url": null
            }
        },
        {
            "24": {
                "title": "CodeXGLUE: A machine learning benchmark dataset for code\nunderstanding and generation.",
                "author": "Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio\nBlanco, Colin B. Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong\nZhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan,\nNeel Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie Liu. 2021.",
                "venue": "CoRR, abs/2102.04664.",
                "url": null
            }
        },
        {
            "25": {
                "title": "Convolutional neural networks over tree structures for programming\nlanguage processing.",
                "author": "Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin. 2016.",
                "venue": "In Proceedings of the 30th AAAI Conference on Artificial\nIntelligence, pages 1287\u20131293.",
                "url": null
            }
        },
        {
            "26": {
                "title": "Text and code embeddings by contrastive pre-training.",
                "author": "Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry\nTworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, Johannes\nHeidecke, Pranav Shyam, Boris Power, Tyna Eloundou Nekoul, Girish Sastry,\nGretchen Krueger, David Schnurr, Felipe Petroski Such, Kenny Hsu, Madeleine\nThompson, Tabarak Khan, Toki Sherbakov, Joanne Jang, Peter Welinder, and\nLilian Weng. 2022.",
                "venue": "CoRR, abs/2201.10005.",
                "url": null
            }
        },
        {
            "27": {
                "title": "CodeNet: A large-scale AI for code dataset for learning a\ndiversity of coding tasks.",
                "author": "Ruchir Puri, David S. Kung, Geert Janssen, Wei Zhang, Giacomo Domeniconi,\nVladimir Zolotov, Julian Dolby, Jie Chen, Mihir R. Choudhury, Lindsey Decker,\nVeronika Thost, Luca Buratti, Saurabh Pujar, Shyam Ramji, Ulrich Finkler,\nSusan Malaika, and Frederick Reiss. 2021.",
                "venue": "In Proceedings of the 35th Annual Conference on Neural\nInformation Processing Systems Track on Datasets and Benchmarks.",
                "url": null
            }
        },
        {
            "28": {
                "title": "Exploring the limits of transfer learning with a unified text-to-text\ntransformer.",
                "author": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael\nMatena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020.",
                "venue": "Journal of Machine Learning Research, 21:140:1\u2013140:67.",
                "url": null
            }
        },
        {
            "29": {
                "title": "On the effectiveness of transfer learning for code search.",
                "author": "Pasquale Salza, Christoph Schwizer, Jian Gu, and Harald C. Gall. 2023.",
                "venue": "IEEE Transactions on Software Engineering, 49(4):1804\u20131822.",
                "url": null
            }
        },
        {
            "30": {
                "title": "SynCoBERT: Syntax-guided multi-modal contrastive pre-training for\ncode representation.",
                "author": "Xin Wang, Yasheng Wang, Fei Mi, Pingyi Zhou, Yao Wan, Xiao Liu, Li Li, Hao Wu,\nJin Liu, and Xin Jiang. 2021a.",
                "venue": "CoRR, abs/2108.04556.",
                "url": null
            }
        },
        {
            "31": {
                "title": "CodeT5: Identifier-aware unified pre-trained encoder-decoder models\nfor code understanding and generation.",
                "author": "Yue Wang, Weishi Wang, Shafiq R. Joty, and Steven C. H. Hoi.\n2021b.",
                "venue": "In Proceedings of the 2021 Conference on Empirical Methods in\nNatural Language Processing, pages 8696\u20138708.",
                "url": null
            }
        },
        {
            "32": {
                "title": "A multi-modal transformer-based code summarization approach for smart\ncontracts.",
                "author": "Zhen Yang, Jacky Keung, Xiao Yu, Xiaodong Gu, Zhengyuan Wei, Xiaoxue Ma, and\nMiao Zhang. 2021.",
                "venue": "In Proceedings of the 29th IEEE/ACM International Conference\non Program Comprehension, pages 1\u201312.",
                "url": null
            }
        },
        {
            "33": {
                "title": "StaQC: A systematically mined question-code dataset from stack\noverflow.",
                "author": "Ziyu Yao, Daniel S. Weld, Wei-Peng Chen, and Huan Sun. 2018.",
                "venue": "In Proceedings of the 2018 World Wide Web Conference, pages\n1693\u20131703.",
                "url": null
            }
        },
        {
            "34": {
                "title": "Learning to mine aligned code and natural language pairs from stack\noverflow.",
                "author": "Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, and Graham Neubig.\n2018.",
                "venue": "In Proceedings of the 15th International Conference on Mining\nSoftware Repositories, pages 476\u2013486.",
                "url": null
            }
        },
        {
            "35": {
                "title": "Spider: A large-scale human-labeled dataset for complex and\ncross-domain semantic parsing and Text-to-SQL task.",
                "author": "Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James\nMa, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir R.\nRadev. 2018.",
                "venue": "In Proceedings of the 2018 Conference on Empirical Methods in\nNatural Language Processing, pages 3911\u20133921.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.16702v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.4",
            "3.5",
            "3.6",
            "3.7",
            "4.1",
            "4.2"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.3",
            "4.4",
            "4.5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "4.5"
        ]
    },
    "research_context": {
        "paper_id": "2403.16702v1",
        "paper_title": "ProCQA: A Large-scale Community-based Programming Question Answering Dataset for Code Search",
        "research_background": "The paper titled \"ProCQA: A Large-scale Community-based Programming Question Answering Dataset for Code Search\" addresses the domain of Code Question Answering (Code QA), a significant area within software intelligence.\n\n### Motivation:\nThe primary motivation for this research stems from limitations in existing datasets and methods for training and evaluating models in Code QA. Although methods such as dual-encoder-based representation models have been employed for retrieval-based Code QA, these rely heavily on curated datasets like CodeSearchNet, which come with inherent limitations in size and data distribution that could impact the quality and diversity of code representations. Additionally, existing code-related QA datasets from various sources are generally constrained by their scale, making them more suitable for standalone evaluation rather than comprehensive pre-training.\n\n### Research Problem:\nThe main research problem being tackled is the lack of large-scale, diverse, and high-quality datasets that can serve both as pre-training corpora and evaluation benchmarks for code representation models. The paper aims to fill this gap by proposing ProCQA, a substantial, community-based programming QA dataset sourced from StackOverflow. This dataset is intended to offer a broad coverage of programming languages and a variety of user queries, helping to develop and benchmark more effective code representation models.\n\n### Relevant Prior Work:\nThe introduction cites several influential works in the field:\n1. **Retrieval-based QA models**: Gu et al. (2018) laid foundational work in retrieval-based QA by focusing on matching natural language queries with relevant code snippets.\n2. **Masked Language Modeling (MLM) and dual-encoder models**: Kanade et al. (2020) and Feng et al. (2020) explored aligning text and code using MLM, similar to monolingual and cross-lingual pre-training approaches suggested by Devlin et al. (2019) and Conneau and Lample (2019).\n3. **Contrastive pre-training**: Jain et al. (2021) and Li et al. (2022) developed contrastive pre-training techniques for code representation by constructing large-scale paired datasets.\n4. **Specialized Encoders**: Some works like Heyman and Cutsem (2020) and Salza et al. (2023) utilized distinct encoders for text and code, emphasizing modality distinction.\n5. **CodeSearchNet**: Husain et al. (2019) created CodeSearchNet, heavily relied upon by many code embedding models but limited by size and data distribution.\n6. **Large-scale generative models**: Allal et al. (2023) focused on large-scale datasets from GitHub for training generative language models.\n7. **Smaller QA datasets**: Huang et al. (2021) and Lee et al. (2022) developed QA datasets suitable for evaluation but limited by their scale.\n\n### Summary:\nThe paper proposes a novel ProCQA dataset designed to address the inadequacies of current resources by offering a large-scale, community-based corpus. The dataset intends to assist in both pre-training and evaluating varied code representation models, leveraging a modality-agnostic contrastive pre-training approach (MACP). The results indicate substantial improvements over previous models, thus advancing the domain of code retrieval.",
        "methodology": "### Proposed Methodology for ProCQA Dataset\n\nThe development of the ProCQA dataset follows a detailed and structured methodology designed to ensure high data quality and fairness. Here's a breakdown of the proposed method and model:\n\n1. **Dataset Creation and Filtering**:\n    - The ProCQA dataset is constructed from a large collection of community-based programming question-answering pairs.\n    - Rigorous filtering strategies are applied to enforce data quality and fairness, though specific filtering techniques are not detailed in this section.\n\n2. **Dataset Analysis**:\n    - Various dataset statistics are analyzed to better understand its characteristics, such as the length of answers and distribution of questions.\n\n3. **Tasks and Baseline Models**:\n    Two main tasks are defined to evaluate the dataset and different baseline models:\n\n    a. **Retrieval Task**:\n        - **Objective**: To find the correct answer from a large corpus of answers.\n        - **Dataset Usage**: Answers from all splits form the retrieval corpus.\n        - **Query Formation**: The concatenation of the question and description forms the query.\n        - **Models Used**: \n            - BM25 (an unsupervised baseline)\n            - Neural language models including BERT, CodeBERT, and UniXCoder.\n        - **Training**:\n            - Neural LMs are fine-tuned using contrastive learning with InfoNCE loss on question-answer pairs from the training set.\n            - Training is conducted for 3 epochs with a batch size of 32 and a learning rate of 2e-5.\n            - Questions and answers are truncated to a maximum length of 256 tokens.\n        - **Evaluation Metrics**: MRR@10, Recall@10, and Recall@100.\n        - **Results**:\n            - BM25 outperforms text-only language models like BERT in MRR@10.\n            - CodeBERT and UniXCoder, with code-specific pre-training, outperform BM25, with UniXCoder showing the best performance.\n\n    b. **Generative Task**:\n        - **Objective**: The model generates the answer to the question directly without relying on references.\n        - **Models Used**: \n            - Generative language models including T5, CodeT5, and PLBART.\n        - **Training**:\n            - Models are trained in a sequence-to-sequence manner by optimizing the cross-entropy loss of the answer sequence given the question sequence.\n            - Training uses the same hyperparameters as the retrieval task.\n        - **Inference**: Beam search decoding with a beam size of 5.\n        - **Evaluation Metrics**: ROUGE scores.\n        - **Results**:\n            - Code-specific language models outperform text-only ones, suggesting effective code-specific pre-training.\n            - The challenge of generating long answers (mostly 100-200 words) is highlighted, marking ProCQA as a difficult dataset for long-form generative QA tasks.\n\n4. **Future Directions**:\n    - The difficulty of the generative task with long-form answers points to potential research into improving long-form question-answering performance, especially with models constrained by limited parameters.\n\nThis methodology lays a comprehensive foundation for utilizing the ProCQA dataset and provides insights into model performance on retrieval and generative tasks, while identifying gaps and areas for future research.",
        "main_experiment_and_results": "**Main Experiment Setup and Results:**\n\n**Setup:**\n1. **Datasets:**\n   - ProCQA: The newly proposed large-scale community-based programming question answering dataset.\n   - CodeSearchNet (CSN) subsets across six programming languages.\n   - Other real-world query benchmarks.\n   - POJ-104: A code clone detection benchmark.\n   - Cross-lingual code retrieval data.\n   - Additional evaluation on unseen languages, specifically Solidity and SQL.\n\n2. **Baselines:**\n   - CodeRetriever: Previous best model for code search.\n   - UniXCoder: Previous state-of-the-art for zero-shot code retrieval.\n   - MAML: Model applying Meta Learning on CodeBERT.\n   - GraphCodeBERT: A baseline for comparison using the authors' codebase.\n\n3. **Evaluation Metrics:**\n   - Retrieval performance across various benchmarks.\n   - Cross-lingual and cross-domain code search performance.\n\n**Results:**\n1. **Performance on CodeSearchNet (Table 7):**\n   - MACP trained with ProCQA outperforms CodeRetriever on all language subsets by an average of 2.1 points.\n\n2. **Real-world query benchmarks (Table 8):**\n   - The proposed model shows a significant improvement over prior state-of-the-art models by up to 10 points on average, due to real-world user queries incorporated from ProCQA.\n\n3. **Intra-modal retrieval performance on POJ-104 (Table 9):**\n   - Outperforms previous best baseline CodeRetriever by +1.38 points, showcasing superior clone detection performance.\n\n4. **Cross-lingual code retrieval (Table 10):**\n   - MACP surpasses previous baselines, establishing new state-of-the-art performance, with UniXCoder noted for its zero-shot retrieval capabilities.\n\n5. **Performance on unseen languages (Solidity and SQL in Table 11):**\n   - The model significantly improves performance for cross-domain code search on unseen languages compared to MAML and GraphCodeBERT, attributed to the diverse language coverage of ProCQA.\n\nIn summary, the main experimental results consistently demonstrate that the newly proposed ProCQA dataset enhances pre-training for code search, surpassing existing state-of-the-art on multiple benchmarks, including both seen and unseen programming languages. This validates the dataset's utility in improving retrieval-oriented contrastive pre-training."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To compare the effectiveness of different pre-training corpora on downstream code retrieval performance.",
            "experiment_process": "A series of controlled experiments were conducted where only the pre-training data distribution was changed. Two experiments were run using the CodeSearchNet (CSN) and ProCQA datasets individually for pre-training. Due to space limitations, downstream fine-tuned retrieval performance on CodeSearchNet was reported, as shown in Figure 3.",
            "result_discussion": "Despite CSN being in-domain data for the evaluation benchmark, it underperforms when used as a pre-training corpus compared to ProCQA. Combining the two datasets yields better results, showcasing the effectiveness of the ProCQA dataset as a mixed-modal corpus for retrieval-oriented contrastive pre-training.",
            "ablation_id": "2403.16702v1.No1"
        },
        {
            "research_objective": "To evaluate the impact of modality-agnostic contrastive learning compared to an explicitly distinguished text and code design.",
            "experiment_process": "An ablation study was conducted on the CSN pre-training corpus, as the code within this corpus is well-formed and can be parsed by existing tools. The bi-modal setting removes all comments in the code, while the mixed-modal setting retains them. Results are listed in Table 12.",
            "result_discussion": "The evaluation set 'Adv Test' requires only code-text matching; however, training with mixed-modal data formats still shows benefits over explicitly distinguishing text and code in the data design.",
            "ablation_id": "2403.16702v1.No2"
        },
        {
            "research_objective": "To ensure the fairness of the evaluation by removing data contamination in the ProCQA dataset.",
            "experiment_process": "De-duplication was performed on the ProCQA dataset with respect to relevant evaluation benchmarks from the same source, including CoNaLa, SO-DS, and StaQC. The proportion of contaminated data for each evaluation set and the performance using raw and filtered versions of the ProCQA dataset for pre-training were listed in Table 13.",
            "result_discussion": "Although a large proportion of the evaluation set is included in the raw pre-training data, removing them results in limited degradation of model performance, implying that the contaminated data make up a small portion of the large-scale pre-training data.",
            "ablation_id": "2403.16702v1.No3"
        }
    ]
}