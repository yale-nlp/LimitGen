{
    "title": "Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era",
    "abstract": "Explainable AI (XAI) refers to techniques that provide human-understandable insights into the workings of AI models.\nRecently, the focus of XAI is being extended towards Large Language Models (LLMs) which are often criticized for their lack of transparency.\nThis extension calls for a significant transformation in XAI methodologies because of two reasons. First, many existing XAI methods cannot be directly applied to LLMs due to their complexity and advanced capabilities. Second, as LLMs are increasingly deployed across diverse industry applications, the role of XAI shifts from merely opening the \u201cblack box\u201d to actively enhancing the productivity and applicability of LLMs in real-world settings.\nMeanwhile, unlike traditional machine learning models that are passive recipients of XAI insights, the distinct abilities of LLMs can reciprocally enhance XAI.\nTherefore, in this paper, we introduce Usable XAI in the context of LLMs by analyzing (1) how XAI can benefit LLMs and AI systems, and (2) how LLMs can contribute to the advancement of XAI. We introduce 10 strategies, introducing the key techniques for each and discussing their associated challenges. We also provide case studies to demonstrate how to obtain and leverage explanations.\nThe code used in this paper can be found at: https://github.com/JacksonWuxs/UsableXAI_LLM.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Explainability holds great promise in understanding machine learning models and providing directions for improvement.\nIn practice, users have high expectations for model explainability:\n1. Through explanation, can we know if a model works properly?\n2. Does explainability help developing better models?\nFirst, explanations are expected to illuminate whether a model operates in accordance with human expectations. For example, does the model leverage reliable evidence and domain knowledge in its decision-making? Does the model contain bias and discrimination? Does the model show any vulnerabilities to potential attacks? Will the model output harmful information?\nSecond, in recognition of model imperfections, we aspire for explainability to inform the development of better models. For example, how to adjust the behaviors of a model if we find it is using unreliable or unreasonable features in making predictions? Can we improve the performance of a model by aligning its behavior with human preferences?\nTherefore, the question arises: Have these expectations been met?\nIn recent years, the body of literature on Explainable AI (XAI) has expanded rapidly to improve model transparency (Du et al., 2019a  ###reference_b62###; Murdoch et al., 2019  ###reference_b175###; Tjoa & Guan, 2020  ###reference_b225###; Do\u0161ilovi\u0107 et al., 2018  ###reference_b61###; Rudin et al., 2022  ###reference_b195###), encompassing a wide array of methods customized for different data modalities, including visual (Zhang & Zhu, 2018  ###reference_b282###), textual (Danilevsky et al., 2020  ###reference_b50###), graph (Yuan et al., 2022  ###reference_b279###), and time-series data (Zhao et al., 2023c  ###reference_b288###). Some literature delves into specific techniques, such as attention methods, generalized additive models, and causal models. Additionally, some offer reviews on general principles and categorizations or initiate discussions on evaluating the faithfulness of explanations (Yang et al., 2019  ###reference_b266###).\nDespite the progress, the last mile of XAI \u2013 making use of explanations \u2013 has not received enough attention.\nIn many cases, we seem to be satisfied with just acquiring explanations and their associated visualizations, sometimes followed by qualitative analysis of the model\u2019s strengths and weaknesses. While these explanations can reveal a model\u2019s imperfections, quantifying model properties (e.g., fairness, security, rationality) or taking the next concrete steps toward model improvement remains a difficult task.\nThe challenges in achieving usable explainability are twofold.\nFirst, there is an inherent conflict between AI automation and human engagement in XAI.\nOn one hand, humans need to define explainability that the model should follow, or scrutinize explanation to identify if any vulnerabilities exist in the model. On the other hand, the requirement for human oversight introduces substantial costs, posing challenges to the scalability and practical implementation of model debugging and improvement in AI workflows.\nSecond, many of the current approaches view explainability as a purely technical matter, ignoring the needs of practitioners and non-technical stakeholders.\nExisting XAI methods are mainly developed as statistical and mathematical tools. However, there exists a noticeable disparity between the objectives of these tools and the expectations of practitioners across various application domains (Malizia & Patern\u00f2, 2023  ###reference_b164###). An explanation that satisfies a technical audience might offer little value to a non-technical audience.\nWhile the opacity issues have not yet been fully resolved for traditional deep models (e.g., multi-layer perceptrons, convolutional and recurrent neural network), the recent advancements of Large Language Models (LLMs) (Brown et al., 2020  ###reference_b26###; Achiam et al., 2023  ###reference_b1###; Touvron et al., 2023b  ###reference_b228###; Chiang et al., 2023  ###reference_b40###) appear to have exacerbated the challenge we are facing.\nFirstly, LLMs typically possess a significantly larger model size and a greater number of parameters. This increased model complexity intensifies the difficulty of explaining their inner workings.\nSecond, different from traditional ML models that primarily focus on low-level pattern recognition tasks such as classification and parsing, LLMs can handle more complex tasks such as generation, reasoning and question answering. Understanding the exclusive abilities of LLMs presents novel challenges for XAI techniques.\nConsidering the transformative impact of LLMs across various applications, ensuring the explainability and ethical use of LLMs has become an imminent and pressing need.\nMeanwhile, the emergent capabilities of LLMs also present new opportunities for XAI research. Their human-like communication and commonsense reasoning skills offer prospects for achieving explainability in ways that could potentially augment or replace human involvement.\n###figure_1### Defining \u201cUsable XAI\u201d. In light of the above considerations, in the context of LLMs, we define Usable XAI which includes two aspects as follows.\n(1) Utilizing Explainability to Enhance LLM and AI Systems. Beyond just producing explanations or enhancing the transparency of LLMs, we explore whether these explanations can pinpoint issues for model debugging or improve the overall performance of LLMs or AI models at large, such as accuracy, controllability, fairness, and truthfulness.\n(2) Utilizing LLMs to Enhance XAI Frameworks. The human-like communication ability of LLMs can enhance model explanations in terms of user-friendliness, by converting the numerical values into understandable language. Also, the commonsense knowledge stored in LLMs can significantly boost the practicality of existing XAI frameworks, by playing the role of humans and alleviating the need for real human involvement in AI workflows.\nContribution of this paper.\nIn this paper, we investigate 10 strategies towards usable XAI techniques in the context of LLMs.\nThese strategies are organized into two major categories: (1) Usable XAI for LLMs; (2) LLM for Usable XAI, as shown in Figure 1  ###reference_###. Additionally, we conduct case studies to substantiate the discussion on selected techniques.\nFor each strategy, we also explore the open challenges and areas that require further investigation in future work.\nUsable XAI for LLMs. We introduce how interpretation can be utilized to enhance AI pipelines, including LLMs and small models.\nFirst, we investigate how explanations could be utilized to diagnose and enhance LLMs in terms of utility. We study three types of post-hoc explanation methods, targeting LLM predictions (Section 2  ###reference_###), LLM components (Section 3  ###reference_###), and training samples (Section 4  ###reference_###), respectively.\nSecond, we focus on how explanations could be leveraged to scrutinize and boost model trustworthiness (Section 5  ###reference_###), including security, fairness, toxicity, and truthfulness, which is crucial to achieving human alignment.\nThird, we discuss how explainability could guide the augmentation of data, including both inference data (i.e., prompts) and training data. Specifically, we discuss two strategies of crafting explainable prompts for LLMs: Chain-of-Thought prompts (Section 6  ###reference_###) and knowledge-enhanced prompts (Section 7  ###reference_###). Furthermore, we introduce leveraging LLM explanations to augment training data for improving small models (Section 8  ###reference_###).\nLLM for Usable XAI. In this part, we investigate strategies for leveraging the advanced capabilities of LLMs to address the challenges in traditional XAI domains, thus enhancing the usability of XAI in practice.\nFirst, we examine ways to enhance the user-friendliness of explanations through the generative capabilities of LLMs (Section 9  ###reference_###). Second, we introduce how to automate the design of interpretable AI workflows by leveraging the planning abilities of LLMs (Section 10  ###reference_###).\nThird, we introduce how to facilitate the evaluation of XAI methods by utilizing the unique property of LLMs in emulating human cognition processes (Section 11  ###reference_###).\nDifferences between this paper and existing surveys.\nMany surveys have been conducted to examine Explainable AI (Du et al., 2019a  ###reference_b62###; Tjoa & Guan, 2020  ###reference_b225###; Do\u0161ilovi\u0107 et al., 2018  ###reference_b61###) or Interpretable Machine Learning (Murdoch et al., 2019  ###reference_b175###). This paper differs from existing work as we focus on explanation methods for large language models.\nMeanwhile, different from the existing survey (Zhao et al., 2023b  ###reference_b285###) that mainly reviews explanation methods for LLMs, our paper puts an emphasis on the XAI usability in LLM studies.\nTo the best of our knowledge, the most related paper to our survey is (Luo & Specia, 2024  ###reference_b159###), which also discusses several aspects where explanations can improve LLM performance. Nevertheless, this light-weight investigation lacks a thorough examination of XAI methods (e.g., sample-based explanation, interpretable workflows, explainable prompts) and how LLMs can benefit existing XAI frameworks (e.g., data augmentation, improving user-friendliness, XAI evaluation).\nFinally, our paper contributes further by providing detailed case studies and open-sourced codes, fostering future research in applying explanations effectively within the LLM context."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "LLM Diagnosis via Attribution Methods",
            "text": "This section introduces attribution methods as post-hoc explanations for LLMs, and how we can discover model defects with attribution scores.\nWe start with revisiting existing attribution methods, and then discuss which methods are still suitable for explaining LLMs.\nSince LLMs widely serve both classification and generation tasks, our discussion categorizes the attribution methods accordingly.\nAfter that, we explore case studies of applying attribution methods to assess LLM-generated output quality. Finally, we discuss future work of designing novel post-hoc explanation methods for LLMs.\nWe employ the Multi-Sentence Reading Comprehension (MultiRC) dataset (Khashabi et al., 2018  ###reference_b112###) in this case study.\nMultiRC presents a more significant challenge than other datasets: it requires the system to answer questions based on multiple sentences from a given paragraph. The answers may not directly correspond to specific sentence spans, reflecting a more realistic scenario of modern LLMs. Each MultiRC instance is associated with a human-annotated rationale indicating which sentences are relevant for answering the question. Our study specifically employs its test subset with 950 samples in total. We consider 80% in which are the training set and we report the results evaluated on the rest samples.\nWe apply the Importance Density Score method (Wu et al., 2023  ###reference_b257###) to estimate the importance of each word in the provided paragraph. Specifically, given -word paragraph  and the -word response , the attribution from each input word  to each output word  is defined as , where  indicates the formatted input prompt concatenated with the first  response word,  is the language model, and  indicates the input static embedding of . This pairwise attribution score is then normalized as . Any normalized attribution  is equal or less than  will is forced to 0. The overall attribution score on word  is defined as its attribution density, i.e., . In our experiments, we let , , , and consider Vicuna-7B-v1.1 as our language model . By averaging these word attribution scores, we reach the importance of each sentence. The top-K sentences with the highest importance scores are selected as the explanation for each instance.\nThe explanation is then concatenated with the output response and fed to a classifier to predict whether the output is correct or not. We initialize the classifier with DistillBERT-base (Sanh et al., 2019  ###reference_b197###), and train it over 3 epochs with learning rate  and weight decay . To compare with this, we also consider the Full Paragraph or the human-annotated rationales as the explanation part to train the classifier.\nFollowing previous studies (Khashabi et al., 2018  ###reference_b112###; DeYoung et al., 2019  ###reference_b55###), we evaluate the accuracy of the generated responses by exactly matching all correct answers with the generated responses.\nTable 2  ###reference_### reports the macro precision, recall, F1, and AUC scores on the test set over five random seeds.\nIn Table 2  ###reference_###, we observe that isolating the rationales from the full context could best help the classifier identify the response quality.\nIt is evident that explanations align more closely with human-annotated rationales when associated with correct responses. In particular, the precision and recall metrics for explanations derived from correct answers surpass those associated with incorrect responses. This finding serves as empirical evidence underscoring the effectiveness of attribution-based explanations in estimating the accuracy of responses generated by LLMs.\nWe use the Hallucination Evaluation Benchmark (Li et al., 2024a  ###reference_b135###) in this case study. Each instance from this dataset includes an input prompt, a response generated by ChatGPT (Ouyang et al., 2022  ###reference_b180###), and the knowledge related to the response. Each piece of knowledge has a human annotation about whether the knowledge is valid or not. Each instance having at least one invalid knowledge is considered a hallucination response. This study focuses on 632 less confused examples from the benchmark, each of which has almost all knowledge correct or almost all wrong. We randomly select 80% of the samples to form a training set and the rest form the testing set.\nGiven a query prompt and its ChatGPT response, we aim to build a classifier to detect if the response contains hallucination. Since the gradients of ChatGPT is inaccessible, we apply the Vicuna-7B model as the substitute to compute the attribution scores. Specifically, we adopt Importance Density Scores (Wu et al., 2023  ###reference_b257###) to compute the attribution scores of Vicuna.\nWe then use the NLTK package to identify the part-of-speech (POS) tag of each query word.\nFinally, each query-response pair is represented with an 82-dimensional vector, where each dimension indicates the average attribution scores over a certain type of POS tagging.\nWe develop a Support Vector Machine classifier based on the POS tagging attribution scores on the training set and report the precision, recall, F1 scores on the hallucination samples, as well as accuracy over all samples in Table  3  ###reference_###.\nTo compare with this, we also leverage a fine-tuned model and a prompting-based method to serve as the hallucination detector baselines.\nIn the table, we first observe that all methods have demonstrated a greater performance than the Random strategy, indicating that the proposed methods and the baselines are valid hallucination detectors.\nIn addition, we observe that the proposed methods achieve a competitive performance with FacTool using GPT-4, establishing a new benchmark in this domain. It is worth noting that, instead of using GPT-4, the attribution score for our method is calculated using a smaller 7-billion-parameter language model. This demonstrates the practicality and efficiency of our approach in weak-to-strong generalization as we could diagnose large language models with smaller models.\nFuture work could consider extracting more effective features and using more powerful classifiers."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Literature Review of Attribution Methods",
            "text": "The attribution-based explanation quantifies the importance of each input feature that contributes to making predictions.\nGiven a language model  with a prediction  according to the -words input prompt , the explainer  assesses the influence of input words in  as . Typically, the sign of  indicates word  positively or negatively influences , and a greater value of  indicates a stronger impact.\nIn text classification,  denotes a specific class label. In text generation,  represents a varying length of generated text.\nMany existing attribution-based explanation methods focus on classification tasks and cannot be directly applied to the generation task.\nThe primary distinction between them is that: classification is limited to a specific set of predictions, while generation encompasses an endless array of possibilities.\nFor instance, in sentiment analysis, a language model can be instructed to output a number between 0 and 1 that indicates the positivity of input text by adding a linear layer and a sigmoid function on top of the language model.\nHowever, in the generative setting, the model can express this positivity in numerous expressions, such as \u201cthe reviewer definitely loves this movie\u201d and \u201cit is a strong positive movie review\u201d.\nThis distinction poses a unique challenge in adapting explanation methods from classification to generation tasks.\nIn the following, we review related works based on the scenarios they are applicable to."
        },
        {
            "section_id": "2.1.1",
            "parent_section_id": "2.1",
            "section_name": "2.1.1 Attributing Inputs for Label Classification",
            "text": "Common attribution methods (Du et al., 2019a  ###reference_b62###; Murdoch et al., 2019  ###reference_b175###) developed for traditional deep models include gradient-based methods, perturbation-based methods, surrogate methods, and decomposition methods.\nWe introduce the general idea and representative examples for each category, followed by the analysis of their suitability for explaining large language models.\nPerturbation-based Explanation.\nPerturbation-based methods assess the importance of input features by perturbing them and monitoring changes in prediction confidence, i.e., , where  refers to the input sequence with the -th feature being perturbed.\nEach feature could refer to a word (Li et al., 2016a  ###reference_b132###), a phrase (Wu et al., 2020b  ###reference_b259###), or a word embedding (Li et al., 2016b  ###reference_b133###).\nThe underlying principle is that perturbing a more important feature should result in a more pronounced alteration in the model\u2019s prediction confidence.\nHowever, this method has limitations, particularly in its assumption that features are independent, which is not always the case with textual data due to word inter-dependencies. Additionally, it is computationally intensive for explaining LLMs, requiring  inferences for an input of  words.\nGradient-based Explanation.\nGradient-based methods offer a computationally efficient approach for estimating model sensitivity to input features based on gradients , where  refers to the embedding of word .\nSome methods employ the -norm of gradients to assess word importance (Li et al., 2016a  ###reference_b132###), i.e., . This approach only requires a single inference and one backpropagation pass.\nSome extended methods multiply the gradient with the word embedding (Kindermans et al.,  ###reference_b116###; Ebrahimi et al., 2018  ###reference_b66###; Mohebbi et al., 2021  ###reference_b172###), i.e., .\nThese methods may yield explanations with limited faithfulness for deep models (Shrikumar et al., 2017  ###reference_b208###), as gradients only reflect the local relationship between input variation and output variation.\nTo address this, Integrated Gradients (IG) has been proposed (Sundararajan et al., 2017  ###reference_b219###; Sikdar et al., 2021  ###reference_b210###; Sanyal & Ren, 2021  ###reference_b199###; Enguehard, 2023  ###reference_b69###), which accumulates gradients as input transitions from a reference point to the actual data point. Nevertheless, IG entails multiple rounds of inference and backpropagation, thus significantly increasing computational demands.\nSurrogate-based Explanation.\nSurrogate-based explanation methods understand complex models by constructing a simpler model  trained on , where  denotes a dataset constructed for the target instance ;  is usually obtained by perturbing , and . The surrogate model , ranging from basic linear models to sophisticated decision trees, serves as a proxy to approximate the decision boundary of the target model  for a specific instance .\nNotable examples include LIME (Ribeiro et al., 2016  ###reference_b191###), SHAP (Lundberg & Lee, 2017  ###reference_b158###), and TransSHAP (Kokalj et al., 2021  ###reference_b119###), where the first two are designed for general deep neural networks and the last one is tailored for Transformer-based language models.\nNevertheless, a significant limitation of them is their intensive reliance on repeated interactions with the target model, a process that is impractical for LLMs.\nDecomposition-based Explanation.\nDecomposition-based methods assign linearly additive relevance scores to inputs, effectively breaking down the model\u2019s prediction. Layer-wise Relevance Propagation (Montavon et al., 2019  ###reference_b174###) and Taylor-type Decomposition (Montavon et al., 2017  ###reference_b173###) are well-known techniques for computing these relevance scores. These methods have been adapted for Transformer-based language models in various research (Voita et al., 2019  ###reference_b232###; 2020  ###reference_b233###; Wu & Ong, 2021  ###reference_b258###). However, a primary challenge in implementing decomposition-based explanations is the need for tailored decomposition strategies to accommodate different model architectures. Although many large language models are based on the Transformer framework, there are key variations between them, such as LLaMA (Touvron et al., 2023a  ###reference_b227###) and GPT (OpenAI, 2023  ###reference_b179###), particularly in aspects like positional encoding strategy and feed-forward network design. This challenge poses a limitation on the universal applicability of decomposition methods for general-purpose interpretation.\nTo summarize, the traditional explanation methods are not always suitable for LLMs.\nIn particular, the perturbation-based and gradient-based explanations are relatively easy to extend for attributing LLM responses to the input prompts, while the surrogate-based and decomposition-based methods become significantly challenging to do so.\nSpecifically, surrogate-based methods suppose that an explainable small model could approximate the decision boundary of the target model around a local example, but there are limited explainable models for the text generation task.\nMeanwhile, decomposition-based methods require designing decomposition strategies for different layers, which is challenging for big LLM architectures.\nAnother primary concern is their significant demand for computing resources.\nGiven an -words input prompt and an -words output response, we present the time complexity of several representative explanation methods in Table 1  ###reference_###.\nIt demonstrates that existing methods either require a large number of forward operations or backward operations.\nTherefore, improving the efficiency of the attribution-based explanation is an important direction for future research and development."
        },
        {
            "section_id": "2.1.2",
            "parent_section_id": "2.1",
            "section_name": "2.1.2 Attributing Inputs for Text Generation",
            "text": "###figure_2### The explanation of generative models can be defined as attributing the overall confidence  to the input , where  denotes the generated response  with  words.\nOne method to achieve this is by treating the text generation process as a sequence of word-level classification tasks. This perspective allows for the application of existing classification-based explanation techniques to assess the influence of each input word  in relation to each output word , resulting in a corresponding attribution score .\nAfter gathering the attributions  for , we perform an aggregation to determine the overall contribution of each input word . This is accomplished by aggregating the individual attributions for all output words corresponding to the input word, denoted as .\nThe simplest approach for this aggregation is to average the attributions assigned to each input word across the different output words (Selvaraju et al., 2016  ###reference_b204###).\nHowever, Wu et al. (2023  ###reference_b257###) observe that attribution scores from different output words are not inherently comparable. For example, the attribution scores for function words (e.g., \u201cthe\u201d, \u201cis\u201d, \u201chave\u201d) are often disproportionately larger than the scores for content words with clear semantic meaning (e.g., verbs and nouns).\nTherefore, it is necessary to normalize the scores prior to the aggregation, so that the scores  become comparable for .\nFigure 2  ###reference_### plots the normalized scores of an example case, where each index in the Y-axis refers to an input prompt token, while that in the X-axis is an output response token.\nA greater normalized attribution score is brighter.\nIn this example, the user attempts to direct the model to output information that does not exist, namely the French president in 1250. The model successfully realizes that this thing does not exist and refuses to answer. The model response can be realized as three parts, \u201cThere was no\u201d, \u201cpresident in France\u201d, and \u201cin 1250\u201d. According to the figure, the first span is generated heavily because of the tokens \u201cWho\u201d and \u201cpresident\u201d, while the model uses both \u201cFrance\u201d and \u201c1250\u201d to respond to the second span \u201cpresident in France\u201d. Finally, the model emphasizes the date \u201c1250\u201d again by referencing the same information from the prompt. Overall, these explanations align with human understanding and highlight the usage of this method in the future.\nHowever, current research on attribution-based explaining for generative LLMs is still in its early stages, and only a limited number of methods have been proposed."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Case Studies: Usability of Attribution Methods for LLMs",
            "text": "###figure_3### The attribution map offers a partial insight into the operational mechanics of LLMs (Chandrasekaran et al., 2018  ###reference_b32###; Hase & Bansal, 2020  ###reference_b88###; Ye & Durrett, 2022a  ###reference_b272###). Accordingly, we propose a general pipeline that leverages attribution scores to analyze LLM behaviors, as shown in Figure 3  ###reference_###.\nFirst, given the target LLM and an input prompt, we compute attribution scores of input tokens relative to the output tokens.\nSecond, we extract a feature vector from the attribution map, tailored to the requirements of the diagnostic task at hand.\nThird, we train a light-weight predictor (e.g., a classifier) to diagnose whether the model behaves appropriately based on the feature vector.\nIn the following, we provide case studies to illustrate how attribution scores could be utilized to assess LLM response quality (Adlakha et al., 2023  ###reference_b3###).\nWe employ the Multi-Sentence Reading Comprehension (MultiRC) dataset (Khashabi et al., 2018  ###reference_b112###  ###reference_b112###) in this case study.\nMultiRC presents a more significant challenge than other datasets: it requires the system to answer questions based on multiple sentences from a given paragraph. The answers may not directly correspond to specific sentence spans, reflecting a more realistic scenario of modern LLMs. Each MultiRC instance is associated with a human-annotated rationale indicating which sentences are relevant for answering the question. Our study specifically employs its test subset with 950 samples in total. We consider 80% in which are the training set and we report the results evaluated on the rest samples.\nWe apply the Importance Density Score method (Wu et al., 2023  ###reference_b257###  ###reference_b257###) to estimate the importance of each word in the provided paragraph. Specifically, given -word paragraph  and the -word response , the attribution from each input word  to each output word  is defined as , where  indicates the formatted input prompt concatenated with the first  response word,  is the language model, and  indicates the input static embedding of . This pairwise attribution score is then normalized as . Any normalized attribution  is equal or less than  will is forced to 0. The overall attribution score on word  is defined as its attribution density, i.e., . In our experiments, we let , , , and consider Vicuna-7B-v1.1 as our language model . By averaging these word attribution scores, we reach the importance of each sentence. The top-K sentences with the highest importance scores are selected as the explanation for each instance.\nThe explanation is then concatenated with the output response and fed to a classifier to predict whether the output is correct or not. We initialize the classifier with DistillBERT-base (Sanh et al., 2019  ###reference_b197###  ###reference_b197###), and train it over 3 epochs with learning rate  and weight decay . To compare with this, we also consider the Full Paragraph or the human-annotated rationales as the explanation part to train the classifier.\nFollowing previous studies (Khashabi et al., 2018  ###reference_b112###  ###reference_b112###; DeYoung et al., 2019  ###reference_b55###  ###reference_b55###), we evaluate the accuracy of the generated responses by exactly matching all correct answers with the generated responses.\nTable 2  ###reference_###  ###reference_### reports the macro precision, recall, F1, and AUC scores on the test set over five random seeds.\nIn Table 2  ###reference_###  ###reference_###, we observe that isolating the rationales from the full context could best help the classifier identify the response quality.\nIt is evident that explanations align more closely with human-annotated rationales when associated with correct responses. In particular, the precision and recall metrics for explanations derived from correct answers surpass those associated with incorrect responses. This finding serves as empirical evidence underscoring the effectiveness of attribution-based explanations in estimating the accuracy of responses generated by LLMs.\nWe use the Hallucination Evaluation Benchmark (Li et al., 2024a  ###reference_b135###  ###reference_b135###) in this case study. Each instance from this dataset includes an input prompt, a response generated by ChatGPT (Ouyang et al., 2022  ###reference_b180###  ###reference_b180###), and the knowledge related to the response. Each piece of knowledge has a human annotation about whether the knowledge is valid or not. Each instance having at least one invalid knowledge is considered a hallucination response. This study focuses on 632 less confused examples from the benchmark, each of which has almost all knowledge correct or almost all wrong. We randomly select 80% of the samples to form a training set and the rest form the testing set.\nGiven a query prompt and its ChatGPT response, we aim to build a classifier to detect if the response contains hallucination. Since the gradients of ChatGPT is inaccessible, we apply the Vicuna-7B model as the substitute to compute the attribution scores. Specifically, we adopt Importance Density Scores (Wu et al., 2023  ###reference_b257###  ###reference_b257###) to compute the attribution scores of Vicuna.\nWe then use the NLTK package to identify the part-of-speech (POS) tag of each query word.\nFinally, each query-response pair is represented with an 82-dimensional vector, where each dimension indicates the average attribution scores over a certain type of POS tagging.\nWe develop a Support Vector Machine classifier based on the POS tagging attribution scores on the training set and report the precision, recall, F1 scores on the hallucination samples, as well as accuracy over all samples in Table  3  ###reference_###  ###reference_###.\nTo compare with this, we also leverage a fine-tuned model and a prompting-based method to serve as the hallucination detector baselines.\nIn the table, we first observe that all methods have demonstrated a greater performance than the Random strategy, indicating that the proposed methods and the baselines are valid hallucination detectors.\nIn addition, we observe that the proposed methods achieve a competitive performance with FacTool using GPT-4, establishing a new benchmark in this domain. It is worth noting that, instead of using GPT-4, the attribution score for our method is calculated using a smaller 7-billion-parameter language model. This demonstrates the practicality and efficiency of our approach in weak-to-strong generalization as we could diagnose large language models with smaller models.\nFuture work could consider extracting more effective features and using more powerful classifiers."
        },
        {
            "section_id": "2.2.1",
            "parent_section_id": "2.2",
            "section_name": "2.2.1 LLM Response Quality Evaluation with Explanations",
            "text": "This case study explores the use of attribution-based explanations as evidence for assessing the quality of LLM-generated responses.\nHere, \u201cquality\u201d is specifically measured by the responses\u2019 accuracy.\nWe hypothesize that responses generated from correct rationales are likely to be more accurate.\nOur method involves comparing the model\u2019s explanations with underlying rationales as a means to evaluate response accuracy.\nThis approach could enhance the reliability of conversational systems, which is crucial for applications such as medical question answering.\nWe employ the Multi-Sentence Reading Comprehension (MultiRC) dataset (Khashabi et al., 2018  ###reference_b112###  ###reference_b112###  ###reference_b112###) in this case study.\nMultiRC presents a more significant challenge than other datasets: it requires the system to answer questions based on multiple sentences from a given paragraph. The answers may not directly correspond to specific sentence spans, reflecting a more realistic scenario of modern LLMs. Each MultiRC instance is associated with a human-annotated rationale indicating which sentences are relevant for answering the question. Our study specifically employs its test subset with 950 samples in total. We consider 80% in which are the training set and we report the results evaluated on the rest samples.\nWe apply the Importance Density Score method (Wu et al., 2023  ###reference_b257###  ###reference_b257###  ###reference_b257###) to estimate the importance of each word in the provided paragraph. Specifically, given -word paragraph  and the -word response , the attribution from each input word  to each output word  is defined as , where  indicates the formatted input prompt concatenated with the first  response word,  is the language model, and  indicates the input static embedding of . This pairwise attribution score is then normalized as . Any normalized attribution  is equal or less than  will is forced to 0. The overall attribution score on word  is defined as its attribution density, i.e., . In our experiments, we let , , , and consider Vicuna-7B-v1.1 as our language model . By averaging these word attribution scores, we reach the importance of each sentence. The top-K sentences with the highest importance scores are selected as the explanation for each instance.\nThe explanation is then concatenated with the output response and fed to a classifier to predict whether the output is correct or not. We initialize the classifier with DistillBERT-base (Sanh et al., 2019  ###reference_b197###  ###reference_b197###  ###reference_b197###), and train it over 3 epochs with learning rate  and weight decay . To compare with this, we also consider the Full Paragraph or the human-annotated rationales as the explanation part to train the classifier.\nFollowing previous studies (Khashabi et al., 2018  ###reference_b112###  ###reference_b112###  ###reference_b112###; DeYoung et al., 2019  ###reference_b55###  ###reference_b55###  ###reference_b55###), we evaluate the accuracy of the generated responses by exactly matching all correct answers with the generated responses.\nTable 2  ###reference_###  ###reference_###  ###reference_### reports the macro precision, recall, F1, and AUC scores on the test set over five random seeds.\nIn Table 2  ###reference_###  ###reference_###  ###reference_###, we observe that isolating the rationales from the full context could best help the classifier identify the response quality.\nIt is evident that explanations align more closely with human-annotated rationales when associated with correct responses. In particular, the precision and recall metrics for explanations derived from correct answers surpass those associated with incorrect responses. This finding serves as empirical evidence underscoring the effectiveness of attribution-based explanations in estimating the accuracy of responses generated by LLMs."
        },
        {
            "section_id": "2.2.2",
            "parent_section_id": "2.2",
            "section_name": "2.2.2 Hallucination Detection with Attribution Explanations",
            "text": "This case study explores a different aspect of LLM generation quality, focusing on the presence of hallucinations in LLM-generated responses. We show that attribution-based explanations can serve as indicators to detect LLM hallucinations.\nHallucinations are defined as responses that contain information conflicting with or unverifiable by factual knowledge (Li et al., 2023c  ###reference_b134###; Ji et al., 2023  ###reference_b107###).\nFor instance, if a model is asked about a fictitious entity like \u201cKing Renoit\u201d and responds with a narrative about \u201cThe Three Musketeers\u201d, claiming it pertains to the nonexistent king, it illustrates a hallucination.\nThis tendency, particularly pronounced in instruction-tuned models, stems from their earnest endeavor to fulfill user requests. The issue often arises when direct commands (\u201ctell me a story\u201d) significantly influence the generation process, while the instruction\u2019s subject (\u201cabout King Renoit\u201d) is neglected.\nBased on this insight, we develop a hallucination detector according to the distribution of attribution scores over different types of prompting words.\nWe use the Hallucination Evaluation Benchmark (Li et al., 2024a  ###reference_b135###  ###reference_b135###  ###reference_b135###) in this case study. Each instance from this dataset includes an input prompt, a response generated by ChatGPT (Ouyang et al., 2022  ###reference_b180###  ###reference_b180###  ###reference_b180###), and the knowledge related to the response. Each piece of knowledge has a human annotation about whether the knowledge is valid or not. Each instance having at least one invalid knowledge is considered a hallucination response. This study focuses on 632 less confused examples from the benchmark, each of which has almost all knowledge correct or almost all wrong. We randomly select 80% of the samples to form a training set and the rest form the testing set.\nGiven a query prompt and its ChatGPT response, we aim to build a classifier to detect if the response contains hallucination. Since the gradients of ChatGPT is inaccessible, we apply the Vicuna-7B model as the substitute to compute the attribution scores. Specifically, we adopt Importance Density Scores (Wu et al., 2023  ###reference_b257###  ###reference_b257###  ###reference_b257###) to compute the attribution scores of Vicuna.\nWe then use the NLTK package to identify the part-of-speech (POS) tag of each query word.\nFinally, each query-response pair is represented with an 82-dimensional vector, where each dimension indicates the average attribution scores over a certain type of POS tagging.\nWe develop a Support Vector Machine classifier based on the POS tagging attribution scores on the training set and report the precision, recall, F1 scores on the hallucination samples, as well as accuracy over all samples in Table  3  ###reference_###  ###reference_###  ###reference_###.\nTo compare with this, we also leverage a fine-tuned model and a prompting-based method to serve as the hallucination detector baselines.\nIn the table, we first observe that all methods have demonstrated a greater performance than the Random strategy, indicating that the proposed methods and the baselines are valid hallucination detectors.\nIn addition, we observe that the proposed methods achieve a competitive performance with FacTool using GPT-4, establishing a new benchmark in this domain. It is worth noting that, instead of using GPT-4, the attribution score for our method is calculated using a smaller 7-billion-parameter language model. This demonstrates the practicality and efficiency of our approach in weak-to-strong generalization as we could diagnose large language models with smaller models.\nFuture work could consider extracting more effective features and using more powerful classifiers."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Challenges",
            "text": ""
        },
        {
            "section_id": "2.3.1",
            "parent_section_id": "2.3",
            "section_name": "2.3.1 How to Identify and Explain the Semantics of Output?",
            "text": "The attribution function  is no longer faithfully attributing the model predictions in the human-interested semantic level since the model could express the same semantic meaning with various responses.\nSpecifically, the model could assign a lower confidence to its original response than the new one, while both responses share the same semantic meaning.\nThis is a significant difference compared with the traditional classification problem, where target label sets are manually designed so that a lower  indicates the model is less confident in predicting a specific semantic concept.\nTaking the sentiment analysis task as an example, an LLM may generate two different responses sharing the same predicted concepts, such as \u201cit is a positive review\u201d and \u201cthe audience thinks the movie is great\u201d.\nCurrent attribution-based explanations concentrate on the literal changes in generated responses, but they do not study how the semantic meanings of these responses change.\nTherefore, they do not provide sufficient explanations of model-generated responses at a semantic level. In this case, the semantic level is which words of the input review lead the model to believe it is positive.\nFuture work may tackle this challenge by proposing metrics to evaluate the semantic differences in responses.\n###figure_4###"
        },
        {
            "section_id": "2.3.2",
            "parent_section_id": "2.3",
            "section_name": "2.3.2 Explaining LLM Predictions Beyond Attribution",
            "text": "The versatility of LLM generation will inspire a variety of explanation paradigms beyond traditional attribution methods.\nAttribution methods aim to explain model output with the contribution of input features. This explanation task is meaningful for conventional machine learning (ML) models whose outputs are usually individual decisions with clear formats (e.g., classification, regression, object detection). The decisions are highly dependent on the input features.\nHowever, LLMs differ from traditional ML models in two aspects. First, the generative process of LLMs is stochastic, e.g., we may obtain different outputs by running an LLM twice even with the same input. Second, LLMs encode rich knowledge within their parameters, which are independent of inputs.\nThese unique properties give rise to novel explanation paradigms.\nFor the first aspect, an explanation task of interest would be to understand the uncertainty of LLM generation. For example, researchers (Ahdritz et al., 2024  ###reference_b4###; Varshney et al., 2023  ###reference_b230###; Su et al., 2024  ###reference_b217###) leverage the prediction perplexity to check whether the LLM is confident during generation, identifying potential errors in less confident predictions.\nSecond, attributing LLM predictions to their encoded knowledge instead of input patterns could provide a new perspective. Some researchers (Yin et al., 2024a  ###reference_b274###) propose the knowledge-boundary detection task to detect whether a model has specific knowledge for a given question. Nevertheless, it did not attribute the prediction to specific knowledge, so humans cannot verify the prediction process with their results yet."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "LLM Diagnosis and Enhancement via Interpreting Model Components",
            "text": "This section discusses the XAI methods that interpret the internal components of large language models. Additionally, it delves into the insights these methodologies offer, which can be instrumental in refining and enhancing the design of language models.\nLLMs adopt transformers as the basic architecture, which typically comprises two types of major components: self-attention layers and feed-forward layers. In the following, we review the research that focuses on interpreting each of these components respectively."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Understanding the Self-Attention Module",
            "text": "A multi-head self-attention module contains multiple self-attention heads capturing different kinds of word-word relations, which are modeled with weights .\nSpecifically, the relation of words  and  is computed as , where  are contextual embeddings of the words.\nThe most straightforward interpretation is analyzing the attention score matrix  given an input sequence to study the relations between words (Vig, 2019  ###reference_b231###; Hoover et al., 2020  ###reference_b96###).\nIn practice, these intuitive explanations would be majorly used to present case studies via visualization.\nWith this strategy, Wang et al. (2023b  ###reference_b240###) conduct case studies on in-context sentiment analysis, where they find that the label words from the in-context examples serve as anchors of the final prediction. In specific, these anchors aggregate information from the examples to generate informative representations from the lower layers, while the deeper layers utilize these representations to make final predictions.\nThis insight motivates them to reweight the attention scores of these anchors to achieve better inference accuracy.\nSome researchers (Dar et al., 2023  ###reference_b51###; Wu et al., 2023  ###reference_b257###) extend this framework to globally analyze the attention weights  and  by feeding the static word embeddings of words from an interested vocabulary, instead of their contextual embeddings.\nFor example, with this approach, Wu et al. (2023  ###reference_b257###) find that instruction tuning empowers LLMs to follow human intentions by encouraging them to encode more word-word relations related to instruction words.\nOn the other hand, some mathematical models are proposed to theoretically explain the self-attention mechanism, such as Sparse Distributed Memory (Bricken & Pehlevan, 2021  ###reference_b24###) and Transformer Circuits (Elhage et al., 2021  ###reference_b67###). In particular, the Transformer Circuits provide mechanistic interpretability to transformer-based models, which breaks models down into human-understandable pieces.\nAlthough these theoretical analyses on self-attention solids a foundation for future research, their direct application is largely underexplored."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Understanding the Feed-Forward Module",
            "text": "A feed-forward network is formalized as , where  is the intermediate contextual representation of an input word,  is a non-linear operation, and  are model parameters.\nFeed-forward networks can be understood as key-value memories (Sukhbaatar et al., 2015  ###reference_b218###; Geva et al., 2021  ###reference_b75###), where each key or value is defined as  and , respectively. That is, each feed-forward network obtains  key-value pairs, called memories.\nOne simple way to interpret the semantic meaning of memory is collecting the words that could maximally activate the key or value vector of that piece of the memory (Geva et al., 2021  ###reference_b75###; Dar et al., 2023  ###reference_b51###), which has demonstrated strong interpretability of the extracted word lists.\nHowever, it is critical to be aware that the key or value vectors are polysemantic (Arora et al., 2018  ###reference_b10###; Scherlis et al., 2022  ###reference_b200###; Bricken et al., 2023  ###reference_b25###), indicating that this simple approach might not provide concise explanations for each key-value pair.\nIt has been shown that the word list of each key-value pair has 3.6 human interpretable patterns on average (Geva et al., 2021  ###reference_b75###).\nTo alleviate the limited interpretability caused by the nature of polysemantic, Wu et al. (2023  ###reference_b257###) propose to interpret the principal components of these key or value vectors, leading to a more concise explanation for each word list, such as \u201cmedical abbreviations\u201d and \u201cprogramming tasks and actions\u201d.\nOther work examines individual memories by measuring the changes of predictions after perturbing their corresponding activations, where it reveals that some memories encode specific knowledge (Dai et al., 2022  ###reference_b46###) and some others capture general concepts (Wang et al., 2022b  ###reference_b244###).\nBy leveraging the explanations of key-value memories, we could locate and update the memories associated with a specific piece of knowledge to perform model editing (Dai et al., 2022  ###reference_b46###; Meng et al., 2022a  ###reference_b167###; b  ###reference_b168###; Hase et al., 2024  ###reference_b89###), i.e., modifying outdated or incorrect knowledge.\nAnother usage of these weight explanations is model pruning, where LLMs can be condensed up to 66.6% of their initial parameters by exclusively maintaining redundant neurons, resulting in an approximate 1.4x increase in inference speed (Dalvi et al., 2020  ###reference_b49###).\nBesides interpreting and analyzing model weights, some work studies model activations (e.g.,  or ) to interpret their functionality.\nThe probing technique is the most popular way for this purpose (Belinkov et al., 2018  ###reference_b18###; Tenney et al., 2018  ###reference_b223###; Jawahar et al., 2019  ###reference_b106###; Rogers et al., 2021  ###reference_b193###), identifying whether a specific concept is encoded within the representations. The basic idea is developing an auxiliary classifier  to map from the representations  to the interested concept space , such as syntax and part-of-speech knowledge, and the performance of  interprets how much information encoded in  is related to the concepts in .\nThis technique motivates developing better parameter-efficient (Chen et al., 2022  ###reference_b35###), domain-specific (Das et al., 2023  ###reference_b52###), and robust (Bai et al., 2021  ###reference_b13###; Wang et al., 2023a  ###reference_b238###) LLMs.\nRecent studies (Chen et al., 2023a  ###reference_b34###; Ahdritz et al., 2024  ###reference_b4###) also apply the probing method to detect the knowledge boundary of a LLM so that the hallucinated responses could be reduced.\nSome researchers (Bricken et al., 2023  ###reference_b25###; Cunningham et al., 2023  ###reference_b44###) point out another direction to interpret the model hidden activations, called dictionary learning, which is motivated by the assumption of superposition (Elhage et al., 2022  ###reference_b68###; Sharkey et al., 2022  ###reference_b205###).\nThe superposition assumes that LLMs would learn an over-complete set of non-orthogonal features, thereby transcending the limitations imposed by the dimensionality of the representation space.\nTherefore, the researchers aim to reconstruct and interpret these features to understand the internals of the model.\nPractically, they develop a sparse auto-encoder  to reconstruct the representations , which shows that humans could well interpret the learned sparse features of  according to their most activation words.\nTheir research shows that this method could be used for more controllable generation.\nSpecifically, if forcing a sparse feature to be activated, then the language model  would change its response to perform the particular behavior of that sparse feature.\nFor example, given \u201c1,2,3,4,5,6,7,8,9,10\u201d as input, the model originally generates numbers as output.\nHowever, when they are forced to magnify the activations of a sparse feature called \u201cDNA\u201d, the model changes its output to \u201cAGACCAGAGAGAAC\u201d.\nIn general, while the explanation techniques for feed-forward networks primarily offer insights for model development, they have also demonstrated promising applications in areas such as model editing and controllable generation."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Challenges",
            "text": "Interpreting the functionality of internal modules is still in its infancy, and we identify two challenges to be tackled in this direction."
        },
        {
            "section_id": "3.3.1",
            "parent_section_id": "3.3",
            "section_name": "3.3.1 Complexity of Individual Models and Their Interactions",
            "text": "The transformer-based language model contains two types of modules that collaborate based on the residual mechanism (He et al., 2016  ###reference_b90###), which enables later modules to utilize, enhance, and/or discard outputs from preceding modules.\nFormally, the output of the -th module is denoted as , where  could be a self-attention module or a feed-forward network.\nResearch in this area aims to interpret how different modules  and  work together for .\nPilot studies (Elhage et al., 2021  ###reference_b67###; Olsson et al., 2022  ###reference_b178###) find that stacked self-attention modules could form Induction Heads, which demonstrate a strong correlation with the in-context learning capability.\nSpecifically, the induction head encourages the model to predict the word \u201cB\u201d followed by a sequence \u201cAB\u2026A\u201d.\nTheir study finds a specific phase during pre-training LLMs where both induction heads and the in-context learning capability emerge from the model.\nFollowing this track, researchers observe diverse functional heads within LLMs for different tasks, such as \u201cName Mover Head\u201d and \u201cDuplicate Token Head\u201d for the object identification task (Wang et al., 2022a  ###reference_b239###), \u201cSingle Letter Head\u201d and \u201cCorrect Letter Head\u201d for the multiple-choice question answering task (Lieberum et al., 2023  ###reference_b146###), and \u201cCapitalize Head\u201d as well as \u201cAntonym Head\u201d for the general purpose tasks (Todd et al., 2023  ###reference_b226###).\nAlthough these studies have indeed deepened our understanding of cross-module effects, their analyses are grounded on specific tasks or scenarios, leaving it uncertain whether these findings possess broad generalizability."
        },
        {
            "section_id": "3.3.2",
            "parent_section_id": "3.3",
            "section_name": "3.3.2 Nature of Polysemantic and Superposition Assumption",
            "text": "Interpreting the functionality of a single neuron (one row/column vector of a weight matrix) fails in analyzing large language models since a single neuron could be activated by multiple and diverse meanings, called polysemantic (Arora et al., 2018  ###reference_b10###; Scherlis et al., 2022  ###reference_b200###; Bricken et al., 2023  ###reference_b25###).\nThis nature leads to poor interpretability: explaining a single neuron usually does not reflect a concise human concept.\nSome researchers (Elhage et al., 2022  ###reference_b68###; Sharkey et al., 2022  ###reference_b205###) assume that this phenomenon is caused by the superposition of an over-complete set of features learned by the models.\nBased on this assumption, we may reach another level of explanation by decomposing the model weights to reconstruct a large number of features.\nHowever, the two critical problems of this approach are still unclear: (1) How do we ensure our reconstructed features faithfully represent the latent features learned by the models? (2) How do we interpret our reconstructed features with human language?"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "LLM Debugging with Sample-based Explanation",
            "text": "In this section, we discuss sample-based explanation strategies for LLMs, which aim to trace back the answers generated by LLMs to specific training samples (i.e., documents) or document fragments in the corpora. The utility of sample-based explanations for LLMs is multifaceted. First, tracing back the predictions of LLM to the training samples can provide evidence for the generation results, which facilitates model debugging in cases of errors and increases the trustworthiness of the model from users when the outcomes are accurate. In addition, it can also help researchers understand how LLMs generalize from training samples. If the outputs of LLMs can be traced back to exact subsequences directly spliced from the training data, it might suggest that the LLM is simply memorizing the data. In contrast, if the generation results and the influencing training samples are abstractly related, it could indicate that LLMs can understand the concepts and generate responses by reasoning from input prompts.\nIn this section, we start by systematically reviewing traditional sample-based explanation strategies, including gradient-based methods and embedding-based methods, as well as some preliminary explorations to generalize them to LLMs. We then analyze the challenges associated with generalizing the above strategies to LLMs with unique transformer structures and unprecedented numbers of parameters. Finally, we discuss the insights to address the challenges, as well as open challenges worthy of further investigation."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Literature Review of Sample-based Explanation",
            "text": "In this section, we denote the input space and output space as  and , respectively. In the context of large language models (LLMs),  is the space of token sequences known as the prompts, and  could be the space of discrete labels in classification tasks or the space of token sequences as output in generation tasks***Here, we should note that, in the pretraining phase of LLM with language modeling, the model either predicts masked tokens in  (i.e., masked language modeling), or autoregressively predicts the next token (i.e., causal language modeling) in . Therefore, in some works,  is omitted, and only  is included for discussions..\nAccordingly, we have a training dataset  with  samples drawn from the joint space , on which an LLM model  is trained with pretrained parameters . We also have a test sample  of interest, where we want to explain the generation of  from  based on training samples in  (which can be viewed as the information source). The goal of sample-based explanation is to measure the influence of a training sample  or a certain segment within , such that the generation of LLMs can be well-explained and backed up by the selected training samples."
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1 Influence Function-based Methods",
            "text": "One strategy to quantify the influence of a training sample  in the dataset  to a test sample  is through the influence function (Koh & Liang, 2017  ###reference_b117###; Han et al., 2020  ###reference_b86###). It measures the change of the prediction loss  for the test sample , when the training sample  undergoes a hypothetical modification in the dataset  during model training. This modification results in an altered set of optimal model parameters, denoted as . The most common modification of a training sample is to remove it from the dataset, where the influence of the removal of a training sample  on the loss at test sample  can be computed as follows:\nwhere  is the gradient of the loss function  on the test sample  evaluated at the optimal parameters , and  denotes the Hessian matrix of the LLM model at parameter .\nIf we denote the number of parameters in  as , the na\u00efve inversion of the Hessian matrix  leads to  time complexity and  space complexity (Schioppa et al., 2022  ###reference_b201###), which is clearly infeasible for large models. To improve efficiency, Koh & Liang (2017  ###reference_b117###) adopt an iterative approximation process, i.e., LiSSA (Linear time Stochastic Second-Order Algorithm), to calculate the Hessian-Vector Product (HVP) in Eq. (1  ###reference_###), where the memory complexity can be reduced to  and time complexity to  ( is the number of iterations). To further reduce the complexity, Pruthi et al. (2020  ###reference_b183###) propose an alternative to Eq. (1  ###reference_###), i.e., TracIn, which measures the influence of  on  by calculating the total reduction of the loss on  whenever  is included in the minibatch during model training. The TracIn measurement can be formulated as follows:\nwhere  is the -th mini-batch fed into the model during training,  is the parameter checkpoint at the -th step,  is the step size, and  is the size of the mini-batch. According to the above equation, TracIn only leverages gradient terms, where Hessian  is removed from the influence measurement. This substantially improves the efficiency. However, such complexity is still prohibitive for large models from both the computational and memory perspectives. In addition, TracIn can only estimate the influence of adding/removing the sample to the loss, where variants of the vanilla influence function defined in Eq. (1  ###reference_###) can measure the influence of other modifications of the training sample , such as perturbation (e.g., masking out a segment of a document ). To adapt the vanilla influence function of Eq. (1  ###reference_###) to explain transformers, Schioppa et al. (2022  ###reference_b201###) propose to use Alnordi iteration (Arnoldi, 1951  ###reference_b9###) to find the dominant eigenvalues and eigenvectors of the Hessian matrix on randomly sampled subsets , with . In such a case, the diagonalized Hessian can be cheaply cached and inverted, where the computational and memory complexity can be substantially reduced.\nPrevious work mainly focuses on reducing the complexity of calculating the influence of a single training sample. Observing that finding the most influential training sample on  needs to iterate Eq. (1  ###reference_###) overall  training samples, Guo et al. (2021  ###reference_b80###) propose to use fast KNN to pre-filter a small subset of influence-worthy data points from  as candidates to explain small pretrained language models, whereas Han & Tsvetkov (2022  ###reference_b85###) propose to iteratively find a small subset  whose gradient is the most similar to that of the downstream task examples.\nRecently, Grosse et al. (2023  ###reference_b79###) propose to use the Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation to scale influence functions to LLMs up to 52B parameters. For adaptation, only influences mediated by the multi-layer perceptron (MLP) layers are considered, whereas weights from the self-attention layers are fixed, as previous work has demonstrated that knowledge is mainly encoded in the MLP layers (Wang et al., 2023d  ###reference_b242###). In addition, based on the assumption that weights from different MLP layers are independent, the EK-FAC approximated influence can be formulated as the sum of influences mediated by each layer:\nwhere  denotes the weights of the -th MLP layer, and  is the EK-FAC approximated Gauss-Newton Hessian for . Since the inversion of  small  matrices (i.e., ) is substantially more efficient than the inversion of a large  matrix (i.e., ),  can be adaptable to very large models, let alone the HVP can be further simplified by utilizing the EK-FAC properties.\nRecently, influence function-based explanation has been used in efficient finetuning of LLMs, where influence function has been used to select a small subset of training samples given few-shot validation samples for a specific downstream task, where the training overhead can be substantially improved (Xia et al., 2024  ###reference_b260###)."
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "4.1.2 Embedding-based Methods",
            "text": "Another strategy for sample-based explanation involves leveraging the hidden representations within the transformer architecture, which is recognized for encoding low-level semantics from textual data, to calculate the semantic similarity between training and test samples. The similarity can also be used to measure the influence of  on  as explanations (Rajani et al., 2019  ###reference_b188###). Specifically, Akyurek et al. (2022  ###reference_b5###) propose to represent the training sample  and test sample  by concatenating the input and output as , . The concatenation is feasible for generation tasks where the output  lies in the same token sequence space as the input prompt . The similarity between  and  can then be calculated as follows:\nwhere  is the sub-network that outputs the -th layer intermediate activation of the pretrained LLM . The Eq. (4  ###reference_###) has a similar form as the vanilla influence function defined in Eq. (1  ###reference_###) as well as its TracIn alternative defined in Eq. (2  ###reference_###), which assigns a score  for the explainee  for each training sample  in the dataset  as the explanation confidence of the sample .\nCompared with the influence function methods introduced in the previous part, embedding-based methods are computationally efficient, as for each explainee , the explanation score from a training sample  requires only one forward pass of the transformer network. In addition, the calculation can be easily paralleled for different training samples. However, the disadvantage is also evident: These methods lack a theoretical foundation and may fail to identify important training samples that may not be semantically similar to the test sample. Consider the following toy example: Training samples  = (\u201c1+1=\u201d, \u201c2\u201d) and  = (\u201c2+2=\u201d, \u201c4\u201d) make the LLM gain the ability to conduct arithmetic calculation, which explains why prompting the model with  = \u201c100+100\u201d gives the results =\u201c200\u201d. However, the embeddings between the test sample  and the two training samples  and  can be very different when calculated via Eq. (4  ###reference_###) (Akyurek et al., 2022  ###reference_b5###). Therefore, embedding-based methods may not be able to faithfully find the training samples where the explanations require generalization ability beyond semantic similarity."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Case Study: EK-FAC-based Influence Estimation",
            "text": "In this part, we implement the EK-FAC-approximated influence function proposed in Grosse et al. (2023  ###reference_b79###), and verify its scalability and effectiveness on LLMs with billions of parameters, including GPT2-1.5B (Radford et al., 2019  ###reference_b185###), LLaMA2-7B (Touvron et al., 2023b  ###reference_b228###), Mistral-7B (Jiang et al., 2023  ###reference_b108###), and LLaMA2-13B."
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1 Experimental Design",
            "text": "We use the SciFact dataset (Wadden et al., 2020  ###reference_b235###) as the corpora, which contains the abstract of 5,183 papers from the domain of basic science and medicine. The explainee LLMs are obtained by finetuning the pretrained LLMs for 20,000 iterations, where AdamW (Loshchilov & Hutter, 2018  ###reference_b155###) is used as the optimizer, and the learning rate and weight decay are set to 1e-5, 1e-3, respectively. Then, we use 500 samples from the corpora to estimate the (i) uncentered covariance matrices of the activations and pre-activation pseudo-gradients , , and (ii) the variances of the projected pseudo-gradient  for each selected dense layer , and cache them on the hard disk (details see Eqs. (16) and (20) in Grosse et al. (2023  ###reference_b79###)). We select the c_fc layer for GPT2-1.5B, and gate_proj layer for LLaMA2-7B, Mistral-7B, and LLaMA2-13B***All the implementation and layer names are based on the huggingface transformers, where the details can be found in https://huggingface.co/docs/transformers/en/index  ###reference_n/index###..\nFor evaluation, we randomly select 200 samples from the corpora to construct the test set, which we name SciFact-Inf. Specifically, for the -th selected sample  (here  as label equals the input in language modeling), we use the first three sentences in , i.e., , to generate a completion  with the finetuned LLM (here,  does not equal the remaining sentences in ), and we aim to explain the generation of  from  with the finetuned LLM with the training samples via EK-FAC approximated influence scores defined in Eq. (3  ###reference_###). Ideally, the -th training sample  itself should be the most influential sample w.r.t. the generation of  for test sample , which facilitates quantitative analysis of the effectiveness of Eq. (3  ###reference_###).\nIn our implementation, for each test sample , we first calculate the EK-FAC approximated HVP part of the influence , i.e., , which is shared for all training samples . Specifically, we record the layer-wise gradient  and calculate the HVP with the cached ,  as Eq. (21) in Grosse et al. (2023  ###reference_b79###). We then go through candidate training samples (1 positive and 99 negative), calculate the gradient , and take inner-product with the approximate HVP as the layer-wise influence. Finally, the layer-wise influences are summed up as Eq. (3  ###reference_###) as the total influence . We rank the influence and calculate the top- hit rate of the positive training sample."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2 Results and Analysis",
            "text": "The experimental results are summarized in Table 4  ###reference_###. From Table 4  ###reference_### we can find that, the EK-FAC approximated influence function achieves a good accuracy in finding the training sample that has the greatest influence on the generation of a test sample, even if only the influences mediated by a small part of dense layers are considered. In addition, we find that the main computational bottleneck in calculating the EK-FAC-based influence is to estimate the covariances ,  and variance , which can take hours when 500 training samples are used for the estimation. However, after the estimation, it is relatively cheap to calculate the influence of 500 training samples for each test sample, which takes around 3 minutes for a 13B LLaMA2 model on 8 A100 GPUs. This demonstrates the scalability of EK-FAC-based influence by assuming the independence of different dense layers and using EK-FAC to simplify the computation."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Challenges",
            "text": "Overall, explaining the generation of LLMs by tracing back to the training samples is still an emerging area. Open questions need to be addressed to further advance the field. In this section, we identify three main challenges as follows, which can serve as directions for future explorations."
        },
        {
            "section_id": "4.3.1",
            "parent_section_id": "4.3",
            "section_name": "4.3.1 Strong Assumptions for Scalability",
            "text": "The unprecedented number of parameters in modern LLMs causes severe scalability issues for sample-based explanation strategies. This is especially evident for the gradient-based methods, as the HVP in Eq. (1  ###reference_###) induces both high computational and space complexity. To address the bottleneck, strong assumptions are usually required to make it feasible for large models. For example, TracIn (Pruthi et al., 2020  ###reference_b183###) simplifies the second-order term in Eq. (1  ###reference_###) via first-order approximation. Schioppa et al. (2022  ###reference_b201###) assume the Hessian to be low rank. Grosse et al. (2023  ###reference_b79###) that assume that the weights from different layers of the LLMs are independent, as well as the tokens in different steps, such that EK-FAC can be appropriately applied to approximate the influence function. From the above analysis, we can find that while the method from Grosse et al. (2023  ###reference_b79###) has the best scalability, it also has the strongest assumption, which may fail to hold in practice. While highly efficient to compute, embedding-based methods make the implicit assumption that semantics similarly implies explainability, which we have demonstrated may not always be the case. Therefore, how to improve the scalability with weak assumptions needs to be investigated in the future."
        },
        {
            "section_id": "4.3.2",
            "parent_section_id": "4.3",
            "section_name": "4.3.2 Explainability v.s. Understandability",
            "text": "Despite the advantage of influence/embedding similarity to provide a qualitative measurement of a specific training sample as the explanation for LLM generation, the understandability of the identified sample can still be weak, where the connection between the selected training samples and the generation may not be understandable to human beings. Specifically, Grosse et al. (2023  ###reference_b79###) cautions that the sign of influence score of the training tokens may be difficult for humans to connect to the positive or negative influence on the generation results. This severely jeopardizes the usability of the identified training samples. In addition, Grosse et al. (2023  ###reference_b79###) also found that, since LLMs are usually not trained to the minimum to avoid overfitting (and due to overparameterization, the number of local minimums may be large), the connection between influence defined in Eq. (1  ###reference_###) with the counterfactual loss of removing the sample  at  is also weak. For the embedding-based methods, since most LLM models are black box transformer models, the similarity of embeddings can also be hard to interpret by human beings; therefore, it is imperative to improve the interpretability of the identified training samples, such that tracing back becomes more meaningful."
        },
        {
            "section_id": "4.3.3",
            "parent_section_id": "4.3",
            "section_name": "4.3.3 LLM-Oriented Sample-based Explanations",
            "text": "Finally, we observed that both gradient-based and embedding-based methods are loosely connected to the LLM, as well as the backbone transformer networks. For example, algorithms like TracIn (Pruthi et al., 2020  ###reference_b183###) are designed to scale up influence functions to large models, which are not specific for LLMs. Similarly, the embedding-based method proposed in Akyurek et al. (2022  ###reference_b5###) is applicable to most machine learning models with latent representations. Grosse et al. (2023  ###reference_b79###) considers the specialty of LLMs by utilizing the knowledge neuron assumption of the backbone transformers (Wang et al., 2023d  ###reference_b242###) to simplify the influence function, where the weights considered are constrained to the MLP layers, which may not fully utilize the property of transformers. Therefore, how to further utilize the property of the LLM and the backbone transformer to design LLM-tailored sample-based influence/similarity (either to reduce the computational/space overhead or to improve the explanation quality) is highly promising for future work."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Explainability for Trustworthy LLMs and Human Alignment",
            "text": "In previous sections, we explore the use of explanation techniques for assessing and improving the performance of LLMs.\nIn this section, we shift the focus towards examining LLM trustworthiness.\nAs LLMs are increasingly integrated into various applications of daily life, including high-stakes areas like healthcare, finance, and legal advice, it is crucial that their responses not only are accurate but also align with human ethical standards and safety protocols (Liu et al., 2023b  ###reference_b152###; Li et al., 2023f  ###reference_b144###).\nThus, the need arises to extend the scope of explanations from merely evaluating the accuracy of LLMs to scrutinizing their trustworthiness. Herein, we delve into how explanation techniques, discussed in the previous sections, can be instrumental in assessing LLMs across key aspects of trustworthiness like security, privacy, fairness, toxicity, and honesty.\nIt is worth noting that while explainability itself is an aspect of trustworthiness, it holds the promise of serving as a foundational tool for addressing other trustworthiness concerns."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Security",
            "text": "LLMs are known to be vulnerable to attacks and exploitation, such as spreading misinformation, launching phishing attacks, and poisoning training data (Derner et al., 2023  ###reference_b54###).\nFor enhanced safety, LLMs are designed to reject certain types of prompts that may result in the generation of harmful content, e.g., by precluding prompts likely to elicit unsafe outputs from the training phase. However, jailbreak techniques can circumvent these restriction measures and manipulate LLMs into producing malicious contents.\nMalevolent users (i.e., attackers) can craft special prompts that compel or induce LLMs to prioritize instruction following over rejections (Liu et al., 2023c  ###reference_b153###; Li et al., 2023a  ###reference_b130###).\nFor example, through Prefix Injection, attackers can use out-of-distribution prompt prefixes that are less likely to be rejected (walkerspider, 2022  ###reference_b236###; Wei et al., 2023  ###reference_b251###). Another approach, called Refuse Suppression, involves directing or persuading models to ignore established safety protocols (Wei et al., 2023  ###reference_b251###; Zeng et al., 2024  ###reference_b281###), where the instruction following ability is then employed to perform the attack.\nExisting methods mainly rely on prompt engineering for attacking LLMs, but they usually have low attack success rates and significant time costs (Li et al., 2024c  ###reference_b141###).\nThus, by understanding and engineering latent representations of LLMs, explanation methods provide a viable way to design advanced attacks and discover the potential vulnerabilities of LLMs (Liu et al., 2021  ###reference_b150###).\nFor example, a recent work extracts \u201csafety patterns\u201d via explaining the latent space of LLMs using representation engineering. Specifically, these patterns can be captured from the activation differences between malicious queries and benign queries. The salient portion of difference vectors\u2019 dimensions is localized and utilized to generate features of safety patterns. The safety patterns reflect the internal protection mechanisms within LLMs. Circumventing these patterns leads to novel attacks, which helps exploring potential vulnerabilities of LLMs (Li et al., 2024c  ###reference_b141###).\nBesides, a deeper understanding of fine-tuning can shed light on the reliability of existing safety measures. In particular, Jain et al. (2023  ###reference_b104###) use networking pruning, attention map activation, and probing classifiers to track the changes of model capabilities from pre-training to fine-tuning. These tools are helpful in finding salient weights that identify crucial neurons to restore pre-training capabilities. These neurons have demonstrated that the capabilities gained during fine-tuning can be removed easily through fine-tuning on other unrelated tasks. This finding casts doubt on the robustness of current safety alignments in LLMs."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Privacy",
            "text": "Recent studies have revealed that LLMs such as ChatGPT can leak extensive amounts of training data through a method known as divergence attacks. These attacks utilize specially crafted prompts to lead the model away from its standard chatbot-style generation (Nasr et al., 2023  ###reference_b177###). The risk of private data exposure through such means poses a serious challenge to the development of ethically responsible models. This issue is compounded by strategies similar to jailbreak attacks, where misalignment is exploited to induce LLMs into operating in an unconventional \u201cdeveloper mode\u201d via out-of-distribution prompts (Li et al., 2023a  ###reference_b130###).\nTraditional data pre-processing techniques, such as data sanitization (Lison et al., 2021  ###reference_b147###), are impractical as defenses for LLMs due to the enormous scale of training data.\nEnhancing LLM privacy involves two strategic approaches: (1) preventing the models from memorizing sensitive data, and (2) establishing safeguards against the release of sensitive information during content generation.\nThe latter can employ techniques used in jailbreak defenses, treating prompts that solicit private information as potentially malicious.\nThe former approach requires identifying whether LLMs possess specific knowledge, which is traditionally tested by crafting question-answering (QA) tasks to determine if LLMs can provide answers. However, this approach faces limitations due to LLMs\u2019 sensitivity to the phrasing of QA prompts, while the optimal prompt is usually unknown.\nTo tackle the challenge, explanatory techniques can serve as a tool to confirm whether LLMs have internalized certain knowledge.\nFor instance, via explaining the relation between factual knowledge and neuron activations (Meng et al., 2022a  ###reference_b167###; Dai et al., 2022  ###reference_b46###; Hase et al., 2024  ###reference_b89###), we may investigate whether and where a piece of factual knowledge is stored within transformers.\nIn addition, Yin et al. (2024a  ###reference_b274###) recently proposes the concept of \u201cknowledge boundary\u201d and develops a gradient-based method to explore whether LLMs master certain knowledge independent of the input prompt."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Fairness",
            "text": "Despite LLMs\u2019 powerful generation capabilities, their widespread applications also bring concerns about exacerbating bias issues in society, as LLMs are able to learn social biases within human-generated corpus (Gallegos et al., 2023  ###reference_b71###). For example, in a gender bias case, \u201c[He] is a doctor\u201d is much more likely than \u201c[She] is a doctor\u201d because of the gender stereotype. In this subsection, we focus on fairness issues that refer to biases related to race, gender, and age within human communities (Li et al., 2023g  ###reference_b145###).\nThere is a rich body of literature quantifying fairness issues within LLMs through various tests (Adebayo et al., 2023  ###reference_b2###). Interpretation complements these methods by providing a unique perspective for bias mitigation, focusing on unraveling the mechanisms through which biases are embedded into LLMs.\nA research direction within this domain is the examination of biased attention heads. For instance, Ma et al. (2023  ###reference_b162###) detect stereotype encodings through probing attention heads and evaluating their attributions via Shapley values (Lundberg & Lee, 2017  ###reference_b158###). The results unveil that approximately 15% to 30% of attention heads across six transformer-based models are linked to stereotypes. These attention heads tend to specialize in maintaining various stereotypes, offering a potential pathway for developing effective de-biasing techniques. Moreover, scoring head biases with gradient-based metrics offers another way for identifying biased heads (Yang et al., 2023a  ###reference_b267###).\nFurthermore, recent work has placed LLM representations under scrutiny (Zou et al., 2023a  ###reference_b295###). Typically, specific templates relevant to a given concept or function are designed beforehand. Then, representations closely aligned with the concepts or functions are examined using principal component analysis (PCA). From this analysis, a vector is derived from the first principal component to predict a certain bias.\nTo achieve fair model predictions, a diverse range of mitigation techniques have been proposed to debias models. One stream of work proposes to debias LLMs at the embedding level, refining embeddings through various approaches. For example, a recent work attempts to alter biased embeddings with minimal alterations to make them orthogonal to neutral embeddings (Rakshit et al., 2024  ###reference_b189###). Additionally, some studies concentrate on removing biases at the level of attention heads. Ma et al. (2023  ###reference_b162###) address this by pruning attention heads that significantly contribute to certain biases. Similarly, another study adapts movement pruning (Sanh et al., 2020  ###reference_b198###) to dynamically choose an optimal subset of low-bias attention heads (Joniak & Aizawa, 2022  ###reference_b109###). Beyond modifying embeddings and pruning attention heads, another strategy involves targeting a specific group of neurons known to propagate biases. It unlearns the biases by retraining weight vectors for these neurons (Yu et al., 2023  ###reference_b277###). Besides, bias mitigation can also be approached from a data-centric perspective using a few training samples (Thakur et al., 2023  ###reference_b224###). This work uses a pre-trained model to find the most biased training examples, and then modifies these examples to fine-tune the model."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Toxicity",
            "text": "Toxicity is another form of harmful content that LLMs may produce. This issue arises because LLMs are often trained on extensive online text corpora that have not been thoroughly filtered, containing elements of toxicity that can hardly be fully eliminated.\nToxicity can be identified by interpreting LLM components like the feed-forward layers and attention heads. For instance, recent work reveals how toxicity is represented within LLMs by identifying multiple vectors promoting toxicity within the MLP layers and further recognizes relevant dimensions through singular value decomposition (Lee et al., 2024  ###reference_b124###).\nFurthermore, the exploration of geometric structures in per-layer representations offers another way to detect toxicity. Balestriero et al. (2023  ###reference_b14###) applied spline formulations within MLPs to extract seven characteristic input features, demonstrating their utility in describing the domain of prompts and classifying toxic remarks.\nThe insights into how toxicity manifests within LLMs also shed light on mitigation strategies. Motivated by the finding that toxicity can be reduced by manipulating relevant vectors, Lee et al. (2024  ###reference_b124###) develops a method called direct preference optimization (DPO), where paired toxic and non-toxic samples are utilized to fine-tune models so that non-toxic content is promoted. By examining the changes in the parameter matrices during the fine-tuning process, it substantiates that even minor adjustments to these critical vectors can reduce toxicity. Built on the observation that LLMs\u2019 representations are updated by outputs from attention layers (Elhage et al., 2021  ###reference_b67###), another work attempts to reduce toxicity by identifying the \u201ctoxicity direction\u201d and then adjusting representations in the opposite direction (Leong et al., 2023  ###reference_b127###)."
        },
        {
            "section_id": "5.5",
            "parent_section_id": "5",
            "section_name": "Truthfulness",
            "text": "One prominent drawback of LLMs is their tendency to confidently produce false statements. These statements fall into two main categories: 1) statements that contradict learned knowledge within models, a problem often related to models\u2019 honesty; 2) statements that are factually incorrect and appear to be fabricated by models, a phenomenon commonly referred to as hallucination. In the following, we delve into various approaches that aim to understand aforementioned two behaviors by leveraging explainability tools."
        },
        {
            "section_id": "5.5.1",
            "parent_section_id": "5.5",
            "section_name": "5.5.1 Honesty",
            "text": "Honesty of LLMs describes models\u2019 ability to produce true statements based on their learned information, where dishonest behaviors significantly compromise the perceived trustworthiness of LLMs. Numerous studies have been undertaken to understand how and why dishonest behaviors happen by looking inside LLMs. One notable work attempts to distinguish dishonesty by training a classifier to predict the accuracy of statements (Azaria & Mitchell, 2023  ###reference_b12###). The classifier is simply trained on top of activations from the hidden layers of LLMs. These activations are generated from true and false statements. The classifier reaches an accuracy range between 60% and 80%, which suggests that LLMs may be internally aware of the truthfulness of their outputs (Azaria & Mitchell, 2023  ###reference_b12###). Furthermore, research by Campbell et al. (2023  ###reference_b29###) localizes dishonesty behaviors at the level of attention heads. This study employs specially crafted prompts to induce dishonest responses, and then trains logistic classifiers on models\u2019 activations over true/false statements. It also employs activation patching to substitute lying activations with honesty ones. Intriguingly, both approaches have witnessed the importance of layer 2329 in flipping dishonesty behaviors. Besides, another popular method tries to study the geometric structure of true/false statements(Marks & Tegmark, 2023  ###reference_b166###). Typically, these structures are visualized by projecting representations of statements onto two principal components. A clear linear structure and the truth directions can be derived to mitigate the dishonest behaviors."
        },
        {
            "section_id": "5.5.2",
            "parent_section_id": "5.5",
            "section_name": "5.5.2 Hallucinations",
            "text": "Hallucinations in LLMs can arise due to poor data quality, biases, outdated knowledge, and the lack of explicit knowledge (Xu et al., 2024  ###reference_b265###; Zhu et al., 2023b  ###reference_b293###). However, whether LLMs are aware of their hallucination behaviors remains an open question. Recent work investigates this question by examining models\u2019 hidden representation space (Duan et al., 2024  ###reference_b65###). It examines three hidden states involving a question, its correct answer and its incorrect answer, which are used to compute an \u201cawareness\u201d score. This metric quantifies the uncertainty of LLMs regarding their own answers, finding that adversarially induced hallucination can increase models\u2019 awareness.\nAdditionally, Li et al. (2024b  ###reference_b136###) illustrates the major differences between models\u2019 output and their inner activations, identifying these discrepancies as a potential source of hallucination. By training linear probing classifiers on each attention head\u2019s activations, the most specialized attention head is identified. An orthogonal probe is subsequently trained with the identified specialized probe. The experiments reveal that \u201ctruth\u201d might exist in a subspace instead of a single direction (Li et al., 2024b  ###reference_b136###).\nAnother work investigates the source of hallucination by analyzing patterns of source token contributions through perturbations (Xu et al., 2023c  ###reference_b264###). Their findings suggest that hallucinations may stem from the models\u2019 excessive dependence on a restricted set of source tokens. Besides, the static distribution of source token contribution, termed as \u201csource contribution staticity\u201d, can be used as another indicator of hallucinations.\nBuilding on the above insights into LLM hallucinations, Duan et al. (2024  ###reference_b65###) apply PCA to derive the direction of the correct answer\u2019s final hidden state, and enhance the hidden representations with this direction to reduce hallucinations. In contrast, Li et al. (2024b  ###reference_b136###) adopts a different approach, by intervening on top- specialized attention heads, while minimizing the influence of the rest attention heads within models. Different from PCA that identifies a single principle direction, this work adopts two distinct techniques to find multiple directions of intervention. First, they use orthogonal vectors of each probe\u2019s hyperplane, which is similar to PCA. Second, they leverage vectors that connect the mean of the true and false distributions (Li et al., 2024b  ###reference_b136###). The vectors derived from mean shift has been demonstrated more effective than those from probe classifiers, which presents another feasible strategy for identifying directions of truth."
        },
        {
            "section_id": "5.6",
            "parent_section_id": "5",
            "section_name": "Challenges",
            "text": "We discuss the challenges in employing explanations to improve models\u2019 trustworthiness and enhance alignment from two aspects: 1) limitations of existing detection techniques, and 2) shortcomings of mitigation strategies based on explanations."
        },
        {
            "section_id": "5.6.1",
            "parent_section_id": "5.6",
            "section_name": "5.6.1 Challenges of Existing Detection Methods",
            "text": "Current detection methods primarily focus on the level of LLM modules, such as layers, attention heads, and representations. However, we still lack a finer-grained understanding of how knowledge is encoded within LLMs, such as at the level of individual neurons and weight parameters. Furthermore, we lack general and robust strategies to identify these knowledge reliably. For instance, to identify gender biases, attention heads might be examined and then the related heads pruned (Li et al., 2024b  ###reference_b136###). This approach requires analyzing each model individually, rather than adopting a general approach. Moreover, existing localization approaches rely either on probing classifiers or casual scrubbing, which might not be reliable. For probing classifiers, the pre-designed biases used to train these classifiers are crucial to their performance. On the other hand, casual cleaning usually introduces new variables that complicate the analysis."
        },
        {
            "section_id": "5.6.2",
            "parent_section_id": "5.6",
            "section_name": "5.6.2 Challenges of Mitigation Strategies",
            "text": "Since LLMs are trained on vast corpora of data, it is impractical to mitigate the aforementioned trustworthiness issues from the data perspective. It is widely believed that XAI, i.e. understanding the inner mechanisms of LLMs, is a fundamental way to solve these issues. Thus, mitigation methods for LLMs are typically developed based on explanations. Existing explanations are implemented using techniques from mechanistic interpretability and representation engineering (Zhao et al., 2024  ###reference_b286###). While both streams of methods can alleviate these issues, they fail to fully address them. For example, principal component analysis (PCA) is often used to find geometric structures related to these issues, but cannot fix these issues. Another popular mitigation method is casual scrubbing, which introduces certain \"positive\" activations to replace those demonstrated to be responsible for specific issues. However, the identified directions and patched activations can only mitigate issues to a certain extent. Moreover, the changes to either representations or activations could also influence other aspects of models\u2019 capabilities, which we are yet unable to evaluate."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "LLM Enhancement via Explainable Prompting",
            "text": "A key distinction between LLMs and traditional machine learning models lies in the LLMs\u2019 ability to accept flexibly manipulated input data, namely prompts, during model inference (Liu et al., 2023a  ###reference_b151###).\nLLMs generally give precedence to the information presented in these prompts when generating outputs.\nTherefore, to mitigate the opacity issue in LLM predictions, we can enhance prompts with understandable content, which is then prioritized over the LLMs\u2019 inherent and implicit knowledge.\nThese enriched prompts can include domain-specific insights, contextual information, or a step-by-step reasoning chain.\nIn response, LLMs might reveal their decision-making processes during inference, which improves the explainability of their behaviors."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Chain of Thoughts (CoT) Prompting",
            "text": "The Chain of Thought (CoT) approach significantly enhances LLMs in tackling complex tasks (Wei et al., 2022  ###reference_b252###). While LLMs are adept at generating human-like responses, they often lack transparency in their reasoning processes. This limitation makes it difficult for users to assess the credibility of the responses, especially for questions that require detailed reasoning.\nTo bridge this gap, recent efforts have incorporated in-context learning with human-crafted explanations directly into prompts (Wei et al., 2022  ###reference_b252###; Huang et al., 2023a  ###reference_b98###; Yao et al., 2023b  ###reference_b271###; Besta et al., 2023  ###reference_b19###). Among these approaches, the CoT method stands out by employing explicit knowledge to guide the reasoning process. Formally, we define the language model as , and input prompt as , where  denote the example question-response pairs for in-context learning, and  is the actual question. In a standard question-answering scenario, we have the model output as . This approach, however, does not provide insights into the reasoning process behind the answer . Therefore, the CoT method proposes to include human-crafted explanations  for the -th in-context example, resulting in a modified input format . Given the input, the model will output not only  but also the generated explanation :\nBesides allowing for a more transparent and understandable interaction with LLMs, the CoT approach is also practically useful as it augments LLMs\u2019 functionality by opening a window for users to control the models\u2019 thought processes. Specifically, the usefulness of CoT methods lies in several key aspects:\nReducing Errors in Reasoning: By breaking down complex problems into a series of smaller tasks, CoT reduces errors in complex calculations and logic-oriented tasks, leading to a more precise resolution of intricate problems (Wei et al., 2022  ###reference_b252###; Qin et al., 2023  ###reference_b184###; Zhang et al., 2023  ###reference_b283###; Wang & Zhou, 2024b  ###reference_b246###).\nProviding Adjustable Intermediate Steps: CoT enables the outlining of traceable intermediate steps within the problem-solving process. This feature enables users to trace the model\u2019s thought process from inception to conclusion, and to adjust the prompts if undesirable model behaviors are observed (Lyu et al., 2023  ###reference_b161###; Wang et al., 2023d  ###reference_b242###).\nFacilitating Knowledge Distillation: The step-by-step reasoning processes derived from larger LLMs can serve as a specialized fine-tuning dataset for smaller LLMs. It allows smaller models to learn complex problem-solving by following explanations, effectively teaching them to tackle intricate questions with enhanced reasoning capabilities (Magister et al., 2022  ###reference_b163###)."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Extended Methods of Explainable Prompting",
            "text": "Advanced techniques beyond CoT methods have been developed to broaden the range of reasoning paths available to LLMs towards enhancing the transparency and understandability of the decision-making process (Yao et al., 2023b  ###reference_b271###; Besta et al., 2023  ###reference_b19###; Yao et al., 2023a  ###reference_b270###; Dhuliawala et al., 2023  ###reference_b57###; Lyu et al., 2023  ###reference_b161###).\nWe introduce several notable examples below.\nTree-of-Thoughts (ToT).\nProposed by Yao et al. (2023b  ###reference_b271###), ToT advances beyond the traditional linear Chain of Thought reasoning, offering a more versatile structure that allows models to navigate through multiple reasoning paths.\nToT makes the reasoning process of LLMs more interpretable by closely aligning it with human thought processes, as humans naturally consider multiple options and possible outcomes in both forward planning and retrospective analysis to reach conclusions (Sloman, 1996  ###reference_b213###; Stanovich, 1999  ###reference_b216###).\nThis capability enhances the capacity of LLMs to tackle complex challenges that require the ability to consider and reevaluate different strategies, such as devising game strategies or generating creative content. By simulating the way humans think and make decisions, ToT not only makes their thought process more understandable to human users, but also improves the models\u2019 effectiveness in handling complex tasks.\nGraph of Thoughts (GoT).\nProposed by Besta et al. (2023  ###reference_b19###), GoT transforms the output of LLMs into a graph format. This format visualizes information pieces as nodes and their connections as edges, enabling a more intricate and connected form of reasoning compared to previous methods CoT and ToT.\nBy organizing data into nodes (individual concepts or pieces of information) and edges (relationship between these concepts), GoT makes the logical connections within complex systems more understandable (Yao et al., 2023a  ###reference_b270###).\nThis graphical representation brings several benefits for understanding complex information. Firstly, it enables dynamic modification of relationships between concepts, offering a clear visualization of how changing one element affects the others. This is crucial in fields like legal reasoning (Cui et al., 2023  ###reference_b42###; Boche et al., 2024  ###reference_b22###), scientific research (Ding et al., 2023  ###reference_b59###; Choudhury et al., 2023  ###reference_b41###), and policy analysis (Chen et al., 2023c  ###reference_b38###), where the inter-dependencies between various factors can be intricate and subtle. Secondly, GoT enables an assessment of the significance of each node within the graph, providing insights into which pieces of information are most critical to the task. This level of adaptability and clarity makes GoT exceptionally powerful for analyzing and navigating complex information networks."
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "Case Study: Is CoT Really Making LLM Inferences Explainable?",
            "text": ""
        },
        {
            "section_id": "6.3.1",
            "parent_section_id": "6.3",
            "section_name": "6.3.1 Background and Experimental Settings",
            "text": "Despite the apparent intuitiveness of the CoT prompt design, a critical question remains unanswered: Does CoT really make LLM inferences explainable? In other words, can the information provided through CoT faithfully reflect the underlying generation process of LLMs?\nWe use multi-hop question-answering (QA) as the scenario to investigate this problem.\nIn QA systems, answering multi-hop questions remains a significant challenge. Instead of leveraging a single information source, multi-hop questions require synthesizing information from multiple pieces or sources of data into a coherent and logical sequence.\nWhile LLMs show good performance in single-hop QA tasks (Radford et al., 2019  ###reference_b185###), their efficacy significantly declines in multi-hop situations (Tan et al., 2023  ###reference_b220###; Kim et al., 2023a  ###reference_b114###; Zhong et al., 2023  ###reference_b289###). This discrepancy highlights the need for more advanced methods to effectively handle the intricacy of multi-hop reasoning.\nCoT Prompts for Multi-hop QA. To address the above challenge, our case study applies the CoT technique. CoT relies on high-quality templates as the in-context learning prompt, where we give an example as below. Here,  denotes the test question. The \u201cThoughts\u201d following each \u201cQuestion\u201d are step-by-step problem-solving statements for the multi-hop questions. The thoughts in the templates align the generation process of LLMs with human cognitive problem-solving patterns.\nCoT Faithfulness for Explanation:\nTo quantitatively measure the faithfulness of CoTs, we select fidelity as the corresponding metrics (Zhao et al., 2023b  ###reference_b285###; Wachter et al., 2017  ###reference_b234###):\nwhere  denotes the ground truth label,  denotes the original model output with CoT, while  denotes the model output with misleading information inserted in the \"Thoughts\" section.\nIn the following, we give an example. Given the target question, the correct step-by-step thoughts should be: \u201cEllie Kemper is a citizen of the United States of America. The president of the United States of America is Joe Biden.\u201d\nTo mislead the model, we replace the thoughts with incorrect information (the underlined text) and ask the model to generate a new answer based on incorrect thoughts. If the model still generates the correct answer after the modification, we believe that the CoT information does not faithfully reflect the true process of the answer generation. On the other hand, if it generates an answer corresponding to the incorrect thoughts, then we claim the thoughts are faithful.\nExperimental Settings.\nWe evaluate the performance on the MQUAKE-CF dataset Zhong et al. (2023  ###reference_b289###), which includes 1,000 cases for each -hop questions, , which totally consists of 3,000 questions. Our evaluation applies various language models, including GPT-2 (Radford et al., 2019  ###reference_b185###) with 1.5 billion parameters, GPT-J (Wang & Komatsuzaki, 2021  ###reference_b237###) with 6 billion parameters, LLaMA (Touvron et al., 2023a  ###reference_b227###) with 7 billion parameters, Vicuna-v1.5 (Chiang et al., 2023  ###reference_b40###) with 7 billion parameters, LLaMA2-chat-hf (Touvron et al., 2023b  ###reference_b228###) with 7 billion parameters, Falcon (Almazrouei et al., 2023  ###reference_b7###) with 7 billion parameters, Mistral-v0.1 (Jiang et al., 2023  ###reference_b108###) with 7 billion parameters, and Mistral-Instruct-v0.2 (Jiang et al., 2023  ###reference_b108###) with 7 billion parameters.\nThese models have demonstrated proficiency in both language generation and comprehension."
        },
        {
            "section_id": "6.3.2",
            "parent_section_id": "6.3",
            "section_name": "6.3.2 Experiment Results",
            "text": "Performance Improvement.\nThe performance reported in Table 5  ###reference_### for multi-hop question answering highlights the effectiveness of the CoT across various models. GPT-J exhibits the most dramatic improvement, particularly in 3-hop questions with over a 200% increase in accuracy, suggesting that CoT\u2019s coherent reasoning greatly enhances LLMs\u2019 question-answering ability. While GPT-2 shows modest gains, the performance of GPT-J and other models, such as LLaMA, Vicuna-v1.5, LLaMA2, Falcon, Mistral-v0.1, and Mistral-v0.2, indicates that the CoT method may be more beneficial for more advanced models. LLaMA2 and Mistral-v0.2 display considerable improvements of 78.4% and 63.8%, respectively, in 3-hop questions, further supporting this observation. Overall, the results underscore the potential of coherent reasoning techniques, i.e., CoT, in advancing the question-answering capabilities of LLMs across different model architectures and sizes.\nFaithfulness Evaluation of CoT.\nTable 6  ###reference_### illustrates the impact of accurate versus misleading CoTs on the performance of LLMs. The Fidelity metric indicates how faithfully the model\u2019s output reflects the reasoning process described in the CoT. Ideally, a high Fidelity score suggests that the model\u2019s final response is directly based on the provided CoT, validating it as a faithful explanation of the model\u2019s reasoning pathway. However, as we will discuss below, a low Fidelity may not always imply a lack of faithfulness in the model\u2019s reasoning, which calls for developing more effective evaluation methods in future research.\nGPT-J and LLaMA exhibit high fidelity scores across different question types, indicating a strong adherence to the given reasoning paths. Conversely, models such as Vicuna-v1.5, LLaMA2, Falcon, Mistral-v0.1, and Mistral-v0.2 show relatively high mislead accuracy scores with lower fidelity scores. In the experiments, we observe that these models usually rely on their own generated thoughts instead of using incorrect information provided in the CoT. Mistral-v0.2, in particular, demonstrates the lowest fidelity scores and highest misleading accuracy scores, suggesting a potential self-defense ability against false information. The lower fidelity scores of later models may be attributed to their improved training processes on more diverse and high-quality datasets, enabling them to develop a better understanding of context and reasoning. As a result, they are more likely to generate their own correct reasoning paths.\nIn conclusion, the generated thoughts can be generally viewed as faithful explanations of their output answer. While high fidelity scores generally indicate a model\u2019s adherence to the provided CoT, low fidelity scores do not necessarily imply a lack of faithfulness, especially when the model demonstrates the ability to reject misleading information. Further research on CoT faithfulness and the development of more sophisticated evaluation metrics could contribute to the advancement of interpretable and reliable language models."
        },
        {
            "section_id": "6.4",
            "parent_section_id": "6",
            "section_name": "Challenges",
            "text": "Within machine learning, explanation faithfulness refers to the degree to which an explanation accurately reflects the decision-making process of the model it is intended to elucidate (Li et al., 2022c  ###reference_b142###). An explanation is considered as faithful if it causes the model to make the same decision as the original input.\nIn this context, the challenge faced by explainable prompting (e.g., CoT prompt) lies in two aspects: (1) directing language models to generate explanations that are genuinely representative of the models\u2019 internal decision-making processes, and (2) preventing language models from depending on potentially biased CoT templates.\nRegarding the first challenge, our case study has revealed that relatively small language models may generate answers that do not align with the provided CoT rationales. Therefore, these rationales do not accurately represent the decision-making process within these models. Some efforts have been made to bolster the CoT capabilities of smaller language models by implementing instruction tuning with CoT rationales (Kim et al., 2023b  ###reference_b115###; Ho et al., 2022  ###reference_b95###). These methods can help improve the explanation faithfulness of CoT for small language models, thereby addressing this issue to some extent.\nNevertheless, it remains a challenging problem of how to ensure the generated explanations (i.e., \u201cwhat the model says\u201d) are faithful to the internal mechanism (i.e., \u201cwhat the model thinks\u201d) of language models.\nRegarding the second challenge, recent research shows that explanations in the CoT can be heavily influenced by the introduction of biasing prompt templates into model input (Turpin et al., 2024  ###reference_b229###). This is because existing CoT requires carefully designed templates to prompt language models to produce explanations. If incorrect or biased information is encoded in such templates, the generated explanations could be misleading. Recently, Wang & Zhou (2024a  ###reference_b245###) propose a novel decoding strategy to implement CoT with prompting, which could mitigate this issue. However, how to effectively help language models get rid of the template reliance still remains to be underexplored."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "LLM Enhancement via Knowledge-Augmented Prompting",
            "text": "Enhancing models with external knowledge can significantly improve the control and interpretability of decision-making processes. While LLMs acquire extensive knowledge through pre-training on web-scale data, this knowledge is embedded implicitly within the model parameters, making it challenging to explain or control how this knowledge is utilized during inference. Additionally, LLMs may not always encompass the unique knowledge specific to certain domains, nor keep pace with the constantly evolving information in the world.\nTo address these limitations, this section discusses Retrieval-Augmented Generation (RAG) for the explicit integration of external knowledge into the mechanism of LLM-based AI systems, aiming to yield more interpretable predictions."
        },
        {
            "section_id": "7.1",
            "parent_section_id": "7",
            "section_name": "Preliminaries: Retrieval-Augmented Generation",
            "text": "By fetching relevant information from external databases or the internet, RAG ensures that LLM outputs are accurate and up-to-date. It addresses LLMs\u2019 limitation of relying on fixed and potentially outdated knowledge bases.\nRAG operates in two steps: (1) Retrieval: It locates and fetches pertinent information from an external source based on the user\u2019s query; (2) Generation: It incorporates this information into the model\u2019s generated response. Given an input query  and the desired output , the objective function of RAG can be formulated as (Guu et al., 2020  ###reference_b83###):\nwhere  stands for the external knowledge retrieved from a knowledge base . Thus, the target distribution is jointly modeled by a knowledge retriever  and an answer reasoning module .\nThe knowledge  serves as a latent variable. An RAG model is trained to optimize the parameters, so that it learns to retrieve relevant knowledge  and to produce correct answers  based on  and . As LLMs possess stronger text comprehension and reasoning abilities, they can directly serve as the reasoning module  without further training. In this case, RAG can be treated as a data-centric problem:\nwhere the goal is to find appropriate knowledge that supports the desired output.\nThe interpretability of RAG-based models comes from the information in : (1)  usually elucidates or supplements the task-specific information in ; (2)  could explain the generation of output .\nUnlike other deep models that directly estimate  in an end-to-end manner, where the decision process is not comprehensible, the RAG process provides justification or rationale  that supports the result.\nExisting Retrieval-Augmented Generation (RAG) approaches can be categorized based on when they integrate external knowledge into the model\u2019s workflow.\nThe first category incorporates external knowledge at the inference stage. For instance, Karpukhin et al. (2020  ###reference_b111###) employ dense vectors to identify related documents or text passages, enhancing the data retrieval step of RAG. Similarly, Lewis et al. (2020  ###reference_b128###) refine the data retrieval process to ensure only the most pertinent information influences the model\u2019s output.\nThe second category integrates external knowledge during the model tuning stage. Some representative approaches include Guu et al. (2020  ###reference_b83###); Borgeaud et al. (2022  ###reference_b23###); Nakano et al. (2021  ###reference_b176###). Generally, these methods embed a retrieval mechanism into the model\u2019s training phase, enabling the model to utilize external data more efficiently from the outset."
        },
        {
            "section_id": "7.2",
            "parent_section_id": "7",
            "section_name": "Enhancing Decision-Making Control with Explicit Knowledge",
            "text": "The incorporation of explicit external knowledge through RAG enhances the precision and controllability of decision-making in LLMs. This method leverages real-time information from external databases to produce responses that are not only accurate but also tailored to the specific requirements of each query. Below, we explore the mechanisms by which RAG achieves a more controllable and directed content generation process, with references to key papers that have contributed to these advancements."
        },
        {
            "section_id": "7.2.1",
            "parent_section_id": "7.2",
            "section_name": "7.2.1 Reducing Hallucinations in Response",
            "text": "\u201cHallucination\" in the context of LLMs refers to instances where these models generate information that, while coherent and contextually appropriate, is not based on factual accuracy or real-world evidence (Huang et al., 2023c  ###reference_b101###). This issue can lead to the production of misleading or entirely fabricated content, posing a significant challenge to the reliability and trustworthiness of LLMs\u2019 outputs.\nRAG offers a powerful solution to mitigate the problem of hallucinations in LLMs. By actively incorporating up-to-date, verified external knowledge at the point of generating responses, RAG ensures that the information produced by the model is anchored in reality. This process significantly enhances the factual basis of the model\u2019s outputs, thereby reducing the occurrence of hallucinations. Shuster et al. (2021  ###reference_b209###) applies neural-retrieval-in-the-loop architectures to knowledge-grounded dialogue, which significantly reduces factual inaccuracies in chatbots, as confirmed by human evaluations. Siriwardhana et al. (2023  ###reference_b212###) introduces RAG-end2end, which joint trains retriever and generator components together. Their method demonstrates notable performance improvements across specialized domains like healthcare and news while reducing knowledge hallucination."
        },
        {
            "section_id": "7.2.2",
            "parent_section_id": "7.2",
            "section_name": "7.2.2 Dynamic Responses to Knowledge Updating",
            "text": "RAG empowers LLMs with the ability to incorporate the most current information, keeping their decision-making processes aligned with the latest developments. This feature is especially vital in fast-evolving fields such as medicine and technology, where the need for timely and accurate information is paramount (Meng et al., 2022b  ###reference_b168###). For example, research by (Izacard & Grave, 2020  ###reference_b103###) demonstrates significant enhancements in output relevance and accuracy through real-time information retrieval. Similarly, Han et al. (2023  ###reference_b87###) suggest using retrieved factual data to correct and update the knowledge within pre-trained LLMs efficiently. Additionally, Wang et al. (2023e  ###reference_b243###) introduce a method for integrating newly retrieved knowledge from a multilingual database directly into the model prompts, facilitating updates in a multilingual context."
        },
        {
            "section_id": "7.2.3",
            "parent_section_id": "7.2",
            "section_name": "7.2.3 Domain-specific Customization",
            "text": "RAG enhances LLMs by incorporating knowledge from specialized sources, enabling the creation of models tailored to specific domains. Research by Guu et al. (2020  ###reference_b83###) illustrates how integrating databases specific to certain fields into the retrieval process can empower models to deliver expert-level responses, boosting their effectiveness in both professional and academic contexts. Shi et al. (2023  ###reference_b207###) have applied this concept in the medical domain with MedEdit, utilizing an in-context learning strategy to merge relevant medical knowledge into query prompts for more accurate medical advice. Moreover, recent research finds that LLMs struggle to capture specific knowledge that is not widely discussed in the pre-training data. Specifically, Mallen et al. (2023  ###reference_b165###) observe that LLMs often fail to learn long-tail factual knowledge with relatively low popularity, finding that simply increasing model size does not significantly enhance the recall of such information. However, they note that retrieval-augmented LLMs surpass much larger models in accuracy, particularly for questions on well-known subjects, suggesting that this method can effectively bridge knowledge gaps. Similarly, Kandpal et al. (2023  ###reference_b110###) highlights LLMs\u2019 challenges with acquiring rare knowledge and proposes that retrieval augmentation offers a viable solution, minimizing reliance on extensive pre-training for capturing nuanced, less common information."
        },
        {
            "section_id": "7.3",
            "parent_section_id": "7",
            "section_name": "Challenges",
            "text": "We discuss the challenges in RAG that are relevant to its explainability aspects: (1) In the retrieval stage , does the retrieved information  always elucidate the task-specific information contained in the input ? (2) In the generation stage , does  effectively serve as an explanation for the generation of output ?\nPlease note that our goal is not to exhaustively discuss all the limitations of RAG in this paper as RAG itself is a broad topic in NLP research. For a more detailed examination of the broader limitations of RAG, we direct readers to other reviews (Gao et al., 2023  ###reference_b72###)."
        },
        {
            "section_id": "7.3.1",
            "parent_section_id": "7.3",
            "section_name": "7.3.1 Retrieval Accuracy Bottlenecks",
            "text": "Existent RAG methods typically rely on similarity search to pinpoint relevant information (Lewis et al., 2020  ###reference_b128###; Gao et al., 2023  ###reference_b72###), which represents a substantial improvement over basic keyword searches (Robertson et al., 2009  ###reference_b192###). However, these methods may struggle with complex queries that demand deeper comprehension and nuanced reasoning.\nThe recent \u201clost-in-the-middle\u201d phenomenon (Liu et al., 2024  ###reference_b148###) has revealed that an ineffective retrieval can result in the accumulation of extraneous or conflicting information, negatively affecting the generation quality.\nTo address this challenge, recent RAG approaches have integrated adaptive learning processes (Asai et al., 2023  ###reference_b11###). This advancement enables the retrieval system to refine their performance over time through feedback, adapting to evolving language use and information updates, ensuring their responses remain relevant and accurate. Nonetheless, efficiently handling intricate and multi-hop questions remains a significant challenge, highlighting the need for ongoing research to enhance the capabilities of RAG systems."
        },
        {
            "section_id": "7.3.2",
            "parent_section_id": "7.3",
            "section_name": "7.3.2 Controllable Generation Bottlenecks",
            "text": "In-context learning stands out as the premier method for incorporating external knowledge to boost the capabilities of LLMs such as GPT-4 (Asai et al., 2023  ###reference_b11###; Gao et al., 2023  ###reference_b72###). Despite its effectiveness, there\u2019s no surefire way to ensure that these models consistently leverage the provided external knowledge within the prompts for their decision-making processes.\nIn practice, to achieve thorough coverage, commonly used dense retrieval usually returns a large volume of content, including both relevant and redundant information to the input question. Unfortunately, redundant information in the model prompt raises the computational cost and can mislead LLMs to generate incorrect answers. Recent research shows the retrieved information can degrade the question-answering task performance (Yoran et al., 2023  ###reference_b276###; Petroni et al., 2020  ###reference_b182###; Li et al., 2022a  ###reference_b129###). Some recent work proposes to fine-tune the LLM to improve resilience to noise and reduce hallucinations. However, such approach still cannot prevent oversized retrieval information decrease the system interpretability (Yoran et al., 2023  ###reference_b276###; Xu et al., 2023b  ###reference_b263###).\nThe challenge of optimizing the use of external explanations to achieve more precise and controlled decision-making in LLMs is an ongoing issue that has yet to be fully addressed."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Training Data Augmentation with Explanation",
            "text": "This section explores the generation of synthetic data from explanations using large language models, a technique poised to enhance various machine learning tasks.\nIn machine learning, limited data availability often constrains model performance, presenting a significant challenge across many domains. A viable solution is data augmentation, where LLMs, with their robust generative capabilities, can be utilized for text data augmentation (Whitehouse et al., 2023  ###reference_b255###), such as transforming existing text samples into new variants (Dai et al., 2023  ###reference_b47###).\nNevertheless, there are several challenges to be tackled for effective text augmentation. First, for utility, the generated samples need to exhibit diversity compared to the original data. Second, these samples should be exhibit useful patterns relevant to the downstream tasks.\nTo address these challenges, explanation methods offer a valuable tool to guide data augmentation by providing supplemental contexts and useful rationales (Carton et al., 2021  ###reference_b30###).\nUsing LLMs for explanation-guided data augmentation is a nascent but promising field.\nIn this section, we aim to outline feasible frameworks and discuss potential applications, offering directions for future research in this field.\nExplanations can be particularly beneficial in data augmentation within two scenarios.\nIn the first scenario, explanations are used to delineate desired model behaviors or to identify existing deficiencies, which effectively guides the data augmentation process of LLMs. The second scenario involves employing LLMs to directly produce explanatory texts, which serve as supplementary information to enrich the dataset."
        },
        {
            "section_id": "8.1",
            "parent_section_id": "8",
            "section_name": "Explanation-guided Data Augmentation for Mitigating Shortcuts",
            "text": "Machine learning models are prone to make predictions with spurious correlations, also known as shortcuts (Geirhos et al., 2020  ###reference_b73###), which are misaligned with human reasoning processes. This dependency on shortcuts underlies various challenges in machine learning, notably diminishing a model\u2019s ability to generalize to out-of-distribution samples and weakening its resilience against adversarial attacks (Xu et al., 2020  ###reference_b262###).\nThe extent to which a model depends on shortcut features is intrinsically linked to its interpretability. Essentially, if a model\u2019s predictions are predominantly based on such unreliable features, it indicates that the underlying mechanisms of the model are not fully comprehensible or interpretable from a human perspective.\nTherefore, post-hoc explanation techniques usually play a crucial role in detecting undesirable correlations between input and predictions within deep models (Liu et al., 2018  ###reference_b149###; 2021  ###reference_b150###). For example, Du et al. (2021  ###reference_b64###) adopt Integrated Gradient (IG) to attribute a model\u2019s predictions to its input features, showing that the model tends to treat functional words, numbers, and negation words as shortcuts and strongly rely on these shortcut features for prediction in natural language understanding tasks.\nData augmentation can be applied to training better models that are less sensitive to shortcut features. Explanatory information such as counterfactuals (Wang & Culotta, 2021  ###reference_b247###) has been incorporated in data augmentation to improve model robustness. It generates counterfactual samples by first identifying critical features (e.g., word tokens) and then replacing these features with their antonyms, along with reversing their associated labels. Subsequently, the generated samples are combined with the original ones to train downstream models.\nFurthermore, these techniques can be extended to enhance the out-of-distribution performance of smaller models (Sachdeva et al., 2023  ###reference_b196###; Wen et al., 2022  ###reference_b254###). Namely, large language models could serve as an effective tool to augment data. For example, LLMs are able to synthesize examples that represent outlier cases or rare occurrences, which helps smaller models generalize better on unseen data (Xu et al., 2023a  ###reference_b261###). This could be helpful in building robust models in scenarios where data are scarce or confidential (Tang et al., 2023a  ###reference_b221###).\nBesides, LLMs are promising in improving models\u2019 safety by generating adversarial examples that are more valid and natural compared to conventional approaches (Wang et al., 2023f  ###reference_b249###). First, the most vulnerable words are identified with attribution-based methods. Then, these words are replaced by LLMs in a manner that maintains the integrity of the original text. The quality of these examples can be examined with an external classifier. Subsequently, these adversarial examples are employed to train downstream models, effectively fortifying them against potential attacks and boosting their security.\nSimilarly, LLMs are also helpful in mitigating biases such as fairness issues in models (He et al., 2023  ###reference_b92###). This work claims that it automatically identifies underrepresented subgroups, and chooses an optimal augmentation strategy that avoids hurting other groups. New group examples are generated using LLMs with human-providing labels. The experiments observe improvements on both underrepresented groups and overall model performance. This approach prevents skewed datasets from leading to unfair outcomes for a certain demographic, thereby potentially promoting fairness in society."
        },
        {
            "section_id": "8.2",
            "parent_section_id": "8",
            "section_name": "Explanation-enhanced Data Enrichment",
            "text": "As a powerful generative tool, LLMs have been leveraged to directly generate natural language explanations as augmented data. This strategy relies on LLMs\u2019 understanding abilities to assist smaller models in their learning tasks. One objective in such work is to add natural language explanation generated by LLMs to training data, so as to enhance the performance of small models. Li et al. (2022b  ###reference_b140###) introduce explanations from LLMs that facilitate the training of smaller models to enhance their reasoning capabilities and acquire explanation generation abilities. Specifically, three explanation generation approaches from LLM are utilized to train smaller models, including (1) explanations generated through chain of thought prompting, (2) explanations by rationalization prompting conditioned on golden labels and (3) hybrid approach that combines the first two approaches. With the best case scenario in a reasoning case, a 9.5% improvement on accuracy compared to 60x larger GPT-3 has been seen (Li et al., 2022b  ###reference_b140###). It is worth noting that LLMs including ChatGPT and GPT-4 are competitive in generating coherent and logical explanations, except for limited languages (Whitehouse et al., 2023  ###reference_b255###).\nExplanations from LLMs have also been utilized to mitigate spurious correlations in aspect-based sentiment analysis tasks (Wang et al., 2023c  ###reference_b241###). This study proposes using LLMs to generate explanations for the sentiment labels of aspects in sentences (Wang et al., 2023c  ###reference_b241###). These explanations provide reasoning grounded in contextual semantics rather than relying on spurious correlations between words and labels. The explanations are integrated into the training of aspect-based sentiment analysis models through two methods: augmenting the training data with the explanations or distilling knowledge from the explanations through mimicking behaviors. By focusing on the explanations instead of superficial textual cues, the models can better learn the true associations between text and sentiment and become more robust, improving both in-domain performance and generalization ability (Wang et al., 2023c  ###reference_b241###).\nAnother line of work involves integrating LLM rationales as additional supervision to guide the training of smaller models. Experiments have shown that this approach not only requires fewer training data but also outperforms traditional fine-tuning and distillation methods (Hsieh et al., 2023  ###reference_b97###).\nIn addition to the existing application of augmentation techniques summarized above, we envision that incorporating additional text information can also be practical and efficient in enhancing the performance of various models. For example, one promising application lies in the realm of guiding the parameter learning process of small models by using automatically generated explanations. Previous research has investigated this avenue by directing the attention of natural language inference models towards human-crafted explanations (Stacey et al., 2022  ###reference_b215###). As human-crafted explanations are both arduous and non-transferable, utilizing LLMs as generators presents a more economic and versatile alternation. Another potential application is to enhance model performance on complex tasks using natural language explanations from LLMs. For instance, code translation generation tasks incorporate explanations as an intermediate step, improving model performance by 12% on average (Tang et al., 2023b  ###reference_b222###). The result shows that explanations are particularly useful in zero-shot settings. Apart from assisting in smaller models, LLMs have demonstrated their ability to improve their own reasoning abilities by generating reliable rationales (Huang et al., 2022  ###reference_b100###).\nFurther, Krishna et al. (2023  ###reference_b120###) embed post-hoc explanations, attributing scores to all input features, into natural language rationales. This approach improves model accuracy by 10-25% across multiple tasks via in-context learning. Another study explicitly investigates LLMs\u2019 ability to generate post-hoc explanations in natural language. The experiments show that LLMs spot key features with 75% accuracy (Kroeger et al., 2023  ###reference_b121###). These studies present a novel strategy for leveraging post-hoc explanations, beyond traditional natural language explanations, to enrich training data so as to bolster model performance."
        },
        {
            "section_id": "8.3",
            "parent_section_id": "8",
            "section_name": "Challenges",
            "text": ""
        },
        {
            "section_id": "8.3.1",
            "parent_section_id": "8.3",
            "section_name": "8.3.1 Computational Overhead",
            "text": "Conventional post-hoc explanations, built on well-trained models, are often resource-intensive tasks. The first scenario mentioned above leverages interpretability techniques to accurately diagnose dataset issues. This process typically requires multiple rounds of model training and applying interpretability methods to develop fair and robust models. Consequently, the crafting process can be both time and energy-consuming. Given these challenges, exploring the development of data-centric evaluation metrics is crucial. These metrics can offer a more efficient way to assess data issues, bypassing traditional, cumbersome explanation methods. By focusing on these data-centric measurements, data issues can be diagnosed and fixed before training. The number of training rounds needed is then significantly reduced. This shift not only streamlines model development but also helps reduce computational overhead, making the whole process more practical and efficient."
        },
        {
            "section_id": "8.3.2",
            "parent_section_id": "8.3",
            "section_name": "8.3.2 Data Quality and Volume",
            "text": "Despite their advanced capabilities, LLMs still have limitations when dealing with highly specialized or niche contexts. For example, one of the most prominent issues is \u201challucination\u201d, where models generate plausible but incorrect or misleading responses. This could adversely affect the quality of augmented data, potentially introducing more biases to which LLMs are also vulnerable. Another challenge is controlling the relevance of LLM-generated content. That is, the explanations or data points may seem reasonable but often lack factual accuracy or nuances specific to a domain. Currently, we lack robust metrics to effectively measure the quality and relevance of these generated data relative to the original tasks. Determining the precise amount of data required is also challenging, often leading to new dataset imbalances. Managing the quality of LLM-generated data is an immense challenge, as augmented data can introduce other biases (Zhao & Vydiswaran, 2021  ###reference_b287###). This stems from LLMs\u2019 limited ability to accurately control the quantity and distribution of generated data. Moreover, crafting effective prompts is more of an art than a science, adding uncertainty around generated data quality. Together, these factors underscore the complexities and challenges in fully harnessing LLMs\u2019 potential for data augmentation and related tasks."
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "Generating User-Friendly Explanation for XAI",
            "text": "Previous sections mainly focused on quantitative explanations with LLM via numerical values. For example, sample-based explanation discussed in Section 4  ###reference_### aims to assign each training sample an influence score (see Eqs.1  ###reference_###-4  ###reference_###) that measures the confidence that we can use that training sample to explain the prediction of a test sample. However, using numerical values for explanations is not intuitive, which can be difficult to understand by practitioners with little domain knowledge (Latif & Zhai, 2024  ###reference_b123###; Lee et al., 2023  ###reference_b125###; Li et al., 2020  ###reference_b137###). User-friendly explanations, on the contrary, aim to generate human-understandable explanations, e.g., natural language-based descriptions, regarding certain data, the reason why a model makes certain predictions, or what role a neuron plays in the network, such that the generated explanations can be well-understood by both researchers and practitioners.\nGiven an explainee , which can be a data sample , a neuron  from a pretrained model , or a prediction result  based on the input , generating user-friendly explanation aims to map the explainee  to a sequence of natural language tokens as the explanation for the explainee , such that the generated explanations can be easily comprehended by human beings."
        },
        {
            "section_id": "9.1",
            "parent_section_id": "9",
            "section_name": "User-friendly Data Explanation with LLMs",
            "text": "Data explanation refers to the process of translating difficult materials (e.g., program codes, long documents) into concise and straightforward language so that they are easy to understand by humans.\nLanguage models have long been used to generate explanations for textual data (Dai & Callan, 2019  ###reference_b48###). Since modern LLMs are trained on corpora composed of codes, math, and papers, they can be leveraged to explain data beyond pure textual content. For example, Chen et al. (2021  ###reference_b36###) have demonstrated that pretrained GPT models possess the ability to understand and generate codes, where explanatory comments are generated simultaneously that facilitate the understanding of programmers. In addition, Welleck et al. (2022  ###reference_b253###) propose to explain math theorems by providing detailed derivations, so that the theorems are easier to understand. Recently, LLMs have also been used to elucidate academic papers (Castillo-Gonz\u00e1lez et al., 2022  ###reference_b31###), making difficult content to be easily understood by individuals with little domain knowledge."
        },
        {
            "section_id": "9.2",
            "parent_section_id": "9",
            "section_name": "Explaining Small Models with LLMs",
            "text": "Recently, there has been growing interest in leveraging LLMs to generate free-text explanations for small models. For example, to explain black-box text classifiers, Bhattacharjee et al. (2023  ###reference_b20###) propose a prompting-based strategy to identify keywords  in the input texts  with pretrained LLMs that are informative for the label , and ask LLMs to substitute them with another set of keywords , such that changed text  changes the label prediction to . They view the textual mapping rule \u201cif we change  into  in , then  will be classified as \u201d as the counterfactual explanation for the model. In addition, to explain the neuron of a pretrained language model (e.g., GPT2), Bills et al. (2023  ###reference_b21###) propose to summarize the neuron activation patterns into textual phrases with a larger language model (e.g., GPT4), where the neuron activation patterns are expressed as a sequence of (token, attribution score) pairs. To verify the identified patterns, they generate activation patterns according to the phrases via the same LLM and compare their similarity with the true activation patterns of the neuron, where the phrases with high scores are considered more confident to serve as the explanation for the neuron.\nThe explaining ability of LLMs is not necessarily limited to text models. For example, Zhao et al. (2023a  ###reference_b284###) propose using pretrained vision-language models to generate explanations for a neuron  of an image classification model. Specifically, for each class , they first find regions in images with label  that have maximum activation of the neuron  as the surrogate explainees for , and prompt LLMs such as ChatGPT to generate candidate explanations (words, short phrases) for the class label . Then, they use the pretrained vision-language model CLIP (Radford et al., 2021  ###reference_b186###) to match the candidate explanations with the surrogate explainees as the explanations for the neuron . Recently, LLMs have also found applications in explaining recommender systems Zhu et al. (2023a  ###reference_b291###). Specifically, Yang et al. (2023c  ###reference_b269###) found that LLMs can well interpret the latent space of sequential recommendation model after alignment, whereas Lei et al. (2023  ###reference_b126###) propose to align user tokens of LLMs with the learned user embeddings of small recommendation model to generate explanations of user preferences encoded in the embeddings. Recently,  Schwettmann et al. (2024  ###reference_b203###) propose a unified framework to explain all models where inputs and outputs can be converted to textual strings. Specifically, the explainer LLM is used as an agent to interact with the explainee model by iteratively creating inputs and observing outputs from the model, where the textual explanations are generated by viewing all the interactions as the context."
        },
        {
            "section_id": "9.3",
            "parent_section_id": "9",
            "section_name": "Self-Explanation of LLMs",
            "text": "Due to the black-box nature of LLMs, it is promising to generate user-friendly explanations for the LLMs themselves, such that the operational mechanics and the predictions of LLMs can be well-understood by human experts. Based on whether the LLM needs to be retrained to generate explanations for themselves, the self-explanation of LLM can be categorized into two classes: fine-tuning based approach and in-context based approach, which will be introduced in the following parts.\nFine-tuning based approaches.\nGiven sufficient exemplar explanations on the labels of the training data (e.g., in recommendation datasets such as the Amazon Review datasets (He et al., 2017  ###reference_b91###) or the Yelp dataset (Zhou et al., 2020  ###reference_b290###), users have provided explanations on why they have purchased certain items, which can be viewed as explanations for the ratings), LLMs can learn to generate explanations for their predictions as an auxiliary task through supervised learning. One exemplar method is P5 (Geng et al., 2022  ###reference_b74###), which fine-tunes the pre-trained language model T5 (Raffel et al., 2020  ###reference_b187###) on both the rating and explanation data to generate an explanation alongside the recommendations. Recently, several works have improved upon P5 (Cui et al., 2022  ###reference_b43###; Zhu et al., 2024  ###reference_b292###), which fine-tunes different LLMs such as GPT2, LLaMA, Vicuna, etc., and propose different prompt learning strategies (Li et al., 2023d  ###reference_b138###) with generating explanation as the auxiliary task.\nWith explanations introduced as additional supervision signals to fine-tune pretrained LLMs for recommendations, the performance can be improved with good explainability.\nIn-context based approaches. In many applications, there is often a lack of sufficient exemplar explanations. However, the unique capability of modern LLMs to reason and provide answers through human-like prompts introduces the potential for in-context based explanations. Here, explanations for predictions are crafted solely based on the information within the prompt. A leading approach in this domain is the Chain-of-Thoughts (CoT) prompting (Wei et al., 2022  ###reference_b252###), which provides few-shot examples (with or without explanations) in the prompt and asks the LLM to generate answers after reasoning step-by-step, where the intermediate reasoning steps that provide more context for generating the final answer can be viewed as explanations.\nHowever, CoT generates reasoning first and then based on which generates predictions, where the reasoning steps can influence prediction results (Lyu et al., 2023  ###reference_b161###). If explanations are generated after the prediction, since the explanation is conditioned on the predicted label, it can provide a more faithful post-hoc explanation of why the model makes certain decisions (Lanham et al., 2023  ###reference_b122###).\nThe application of in-context based self-explanation of LLMs is broad. For example, Huang et al. (2023d  ###reference_b102###) explore generating zero-shot self-explanation of sentiment analysis with LLMs by directly asking them to generate explanations alongside the predictions. In addition, Huang et al. (2023a  ###reference_b98###) propose a chain-of-explanation strategy that aims to explain how LLMs can detect hate speech from the textual input. Lu et al. (2022  ###reference_b157###) find that CoT can generate well-supported explanations for question answering with scientific knowledge."
        },
        {
            "section_id": "9.4",
            "parent_section_id": "9",
            "section_name": "Challenges",
            "text": ""
        },
        {
            "section_id": "9.4.1",
            "parent_section_id": "9.4",
            "section_name": "9.4.1 Usability v.s. Reliability",
            "text": "Many existing methods rely on prompts to generate user-friendly explanations, which are not as reliable as numerical methods with good theoretical foundations. Ye & Durrett (2022b  ###reference_b273###) find that the explanations by CoT may not be factually grounded in the inputs. Therefore, they believe that these explanations are more suitable as post-hoc explanations regarding why the LLM makes certain predictions (no matter whether the predictions are right or wrong).\nHowever, the validity of viewing CoT explanations as post-hoc justifications has been questioned by recent findings from Turpin et al. (2024  ###reference_b229###), which uses biased datasets (e.g., the few-shot examples in the prompt always answer \u201cA\u201d for multiple choice questions) to show that the generated explanations may be plausible, but systematically unfaithful to represent the true reasoning process of the LLMs. This issue arises from biased reasoning steps which inadvertently skew the predictions. Thus, there\u2019s a growing need for more theoretical scrutiny of user-friendly explanations to ensure faithfulness and credibility."
        },
        {
            "section_id": "9.4.2",
            "parent_section_id": "9.4",
            "section_name": "9.4.2 Constrained Application Scenarios",
            "text": "Currently, the utilization of LLMs to explain smaller black-box models is mainly limited to those that deal with data with rich textual information (Bhattacharjee et al., 2023  ###reference_b20###; Lei et al., 2023  ###reference_b126###). Although Zhao et al. (2023b  ###reference_b285###) propose a strategy to explain image classifiers, the ability to match candidate textual explanations with image patterns still relies on the pretrained vision-language model CLIP. This method may not be applicable to other domains such as graph machine learning (e.g., graph neural networks), or time-series analysis (recurrent neural networks), where large pretrained models have demonstrated little progress compared to natural language processing and computer vision. Therefore, there is a compelling need to devise more versatile strategies for explaining models across a wider range of fields. This endeavor could depend on the fundamental research on combining LLM with other domain-specific tasks, such as the development of Graph-Language Models that are applicable to unseen graphs in a zero-shot manner."
        }
    ],
    "appendix": [],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S2.T1\">\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Time complexity analysis on different attribution methods for the generative task.</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S2.T1.10\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S2.T1.10.11.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S2.T1.10.11.1.1\"><span class=\"ltx_text\" id=\"S2.T1.10.11.1.1.1\" style=\"font-size:90%;\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S2.T1.10.11.1.2\"><span class=\"ltx_text\" id=\"S2.T1.10.11.1.2.1\" style=\"font-size:90%;\">Forward</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S2.T1.10.11.1.3\"><span class=\"ltx_text\" id=\"S2.T1.10.11.1.3.1\" style=\"font-size:90%;\">Backward</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S2.T1.10.11.1.4\"><span class=\"ltx_text\" id=\"S2.T1.10.11.1.4.1\" style=\"font-size:90%;\">Notes</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S2.T1.1.1.2\"><span class=\"ltx_text\" id=\"S2.T1.1.1.2.1\" style=\"font-size:90%;\">Mask Perturbation</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S2.T1.1.1.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S2.T1.1.1.3\"><span class=\"ltx_text\" id=\"S2.T1.1.1.3.1\" style=\"font-size:90%;\">0</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S2.T1.1.1.4\"><span class=\"ltx_text\" id=\"S2.T1.1.1.4.1\" style=\"font-size:90%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.4.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S2.T1.2.2.1\">\n<span class=\"ltx_text\" id=\"S2.T1.2.2.1.1\" style=\"font-size:90%;\">Gradient</span><span class=\"ltx_text\" id=\"S2.T1.2.2.1.2\" style=\"font-size:90%;\">Input</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S2.T1.3.3.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S2.T1.4.4.3\"></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S2.T1.4.4.4\"><span class=\"ltx_text\" id=\"S2.T1.4.4.4.1\" style=\"font-size:90%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.7.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S2.T1.7.7.4\"><span class=\"ltx_text\" id=\"S2.T1.7.7.4.1\" style=\"font-size:90%;\">Integrated Gradients</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S2.T1.5.5.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S2.T1.6.6.2\"></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S2.T1.7.7.3\">\n<span class=\"ltx_text\" id=\"S2.T1.7.7.3.1\" style=\"font-size:90%;\"> is the number of steps for integrating gradients.</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.9.9\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S2.T1.9.9.3\"><span class=\"ltx_text\" id=\"S2.T1.9.9.3.1\" style=\"font-size:90%;\">LIME</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S2.T1.8.8.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S2.T1.9.9.4\"><span class=\"ltx_text\" id=\"S2.T1.9.9.4.1\" style=\"font-size:90%;\">0</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S2.T1.9.9.2\">\n<span class=\"ltx_text\" id=\"S2.T1.9.9.2.1\" style=\"font-size:90%;\"> is the number of augmented samples.</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.10.10\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" id=\"S2.T1.10.10.2\"><span class=\"ltx_text\" id=\"S2.T1.10.10.2.1\" style=\"font-size:90%;\">SHAP</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" id=\"S2.T1.10.10.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" id=\"S2.T1.10.10.3\"><span class=\"ltx_text\" id=\"S2.T1.10.10.3.1\" style=\"font-size:90%;\">0</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S2.T1.10.10.4\"><span class=\"ltx_text\" id=\"S2.T1.10.10.4.1\" style=\"font-size:90%;\">-</span></td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 1: Time complexity analysis on different attribution methods for the generative task."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table ltx_align_floatright\" id=\"S2.T2\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Utilizing attribution for response quality evaluation</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S2.T2.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S2.T2.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S2.T2.1.1.1.1\">Setting</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S2.T2.1.1.1.2\">Precision</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S2.T2.1.1.1.3\">Recall</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S2.T2.1.1.1.4\">F1</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S2.T2.1.1.1.5\">AUC</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T2.1.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" id=\"S2.T2.1.2.2.1\">Random</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S2.T2.1.2.2.2\">49.40</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S2.T2.1.2.2.3\">51.79</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S2.T2.1.2.2.4\">49.61</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S2.T2.1.2.2.5\">49.03</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S2.T2.1.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S2.T2.1.3.1.1\">Human Rationale</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T2.1.3.1.2\">68.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T2.1.3.1.3\">66.88</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T2.1.3.1.4\">67.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T2.1.3.1.5\">73.11</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T2.1.4.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S2.T2.1.4.2.1\">Full Paragraph</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T2.1.4.2.2\">58.02</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T2.1.4.2.3\">58.47</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T2.1.4.2.4\">56.89</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T2.1.4.2.5\">63.44</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T2.1.5.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S2.T2.1.5.3.1\">Attribution (ours)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S2.T2.1.5.3.2\">63.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S2.T2.1.5.3.3\">67.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S2.T2.1.5.3.4\">64.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S2.T2.1.5.3.5\">71.53</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 2: Utilizing attribution for response quality evaluation"
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S2.T3\">\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Utilizing attribution for hallucination detection on ChatGPT responses.</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S2.T3.3\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S2.T3.3.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\" id=\"S2.T3.3.1.1.1\"><span class=\"ltx_text\" id=\"S2.T3.3.1.1.1.1\" style=\"font-size:90%;\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\" id=\"S2.T3.3.1.1.2\"><span class=\"ltx_text\" id=\"S2.T3.3.1.1.2.1\" style=\"font-size:90%;\">Language Model</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S2.T3.3.1.1.3\"><span class=\"ltx_text\" id=\"S2.T3.3.1.1.3.1\" style=\"font-size:90%;\">Precision</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S2.T3.3.1.1.4\"><span class=\"ltx_text\" id=\"S2.T3.3.1.1.4.1\" style=\"font-size:90%;\">Recall</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S2.T3.3.1.1.5\"><span class=\"ltx_text\" id=\"S2.T3.3.1.1.5.1\" style=\"font-size:90%;\">F1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S2.T3.3.1.1.6\"><span class=\"ltx_text\" id=\"S2.T3.3.1.1.6.1\" style=\"font-size:90%;\">Accuracy</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T3.3.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S2.T3.3.2.2.1\"><span class=\"ltx_text\" id=\"S2.T3.3.2.2.1.1\" style=\"font-size:90%;\">Random</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S2.T3.3.2.2.2\"><span class=\"ltx_text\" id=\"S2.T3.3.2.2.2.1\" style=\"font-size:90%;\">-</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.3.2.2.3\"><span class=\"ltx_text\" id=\"S2.T3.3.2.2.3.1\" style=\"font-size:90%;\">88.41</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.3.2.2.4\"><span class=\"ltx_text\" id=\"S2.T3.3.2.2.4.1\" style=\"font-size:90%;\">50.34</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.3.2.2.5\"><span class=\"ltx_text\" id=\"S2.T3.3.2.2.5.1\" style=\"font-size:90%;\">64.11</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.3.2.2.6\"><span class=\"ltx_text\" id=\"S2.T3.3.2.2.6.1\" style=\"font-size:90%;\">50.59</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T3.3.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S2.T3.3.3.3.1\"><span class=\"ltx_text\" id=\"S2.T3.3.3.3.1.1\" style=\"font-size:90%;\">FacTool</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S2.T3.3.3.3.2\"><span class=\"ltx_text\" id=\"S2.T3.3.3.3.2.1\" style=\"font-size:90%;\">GPT-4</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.3.3.3.3\"><span class=\"ltx_text\" id=\"S2.T3.3.3.3.3.1\" style=\"font-size:90%;\">95.30</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.3.3.3.4\"><span class=\"ltx_text\" id=\"S2.T3.3.3.3.4.1\" style=\"font-size:90%;\">72.93</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.3.3.3.5\"><span class=\"ltx_text\" id=\"S2.T3.3.3.3.5.1\" style=\"font-size:90%;\">82.62</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.3.3.3.6\"><span class=\"ltx_text\" id=\"S2.T3.3.3.3.6.1\" style=\"font-size:90%;\">73.04</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T3.3.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S2.T3.3.4.4.1\"><span class=\"ltx_text\" id=\"S2.T3.3.4.4.1.1\" style=\"font-size:90%;\">Vectara</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S2.T3.3.4.4.2\"><span class=\"ltx_text\" id=\"S2.T3.3.4.4.2.1\" style=\"font-size:90%;\">DeBERTa-base</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.3.4.4.3\"><span class=\"ltx_text\" id=\"S2.T3.3.4.4.3.1\" style=\"font-size:90%;\">90.29</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.3.4.4.4\"><span class=\"ltx_text\" id=\"S2.T3.3.4.4.4.1\" style=\"font-size:90%;\">60.54</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.3.4.4.5\"><span class=\"ltx_text\" id=\"S2.T3.3.4.4.5.1\" style=\"font-size:90%;\">72.40</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.3.4.4.6\"><span class=\"ltx_text\" id=\"S2.T3.3.4.4.6.1\" style=\"font-size:90%;\">59.45</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T3.3.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" id=\"S2.T3.3.5.5.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S2.T3.3.5.5.1.1\" style=\"font-size:90%;\">AttrScore (ours)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S2.T3.3.5.5.2\"><span class=\"ltx_text\" id=\"S2.T3.3.5.5.2.1\" style=\"font-size:90%;\">Vicuna-7B</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.3.5.5.3\"><span class=\"ltx_text\" id=\"S2.T3.3.5.5.3.1\" style=\"font-size:90%;\">90.15</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.3.5.5.4\"><span class=\"ltx_text\" id=\"S2.T3.3.5.5.4.1\" style=\"font-size:90%;\">74.21</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.3.5.5.5\"><span class=\"ltx_text\" id=\"S2.T3.3.5.5.5.1\" style=\"font-size:90%;\">81.36</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.3.5.5.6\"><span class=\"ltx_text\" id=\"S2.T3.3.5.5.6.1\" style=\"font-size:90%;\">70.20</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T3.3.6.6\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S2.T3.3.6.6.1\"><span class=\"ltx_text\" id=\"S2.T3.3.6.6.1.1\" style=\"font-size:90%;\">Mistral-7B</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S2.T3.3.6.6.2\"><span class=\"ltx_text\" id=\"S2.T3.3.6.6.2.1\" style=\"font-size:90%;\">88.74</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S2.T3.3.6.6.3\"><span class=\"ltx_text\" id=\"S2.T3.3.6.6.3.1\" style=\"font-size:90%;\">75.04</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S2.T3.3.6.6.4\"><span class=\"ltx_text\" id=\"S2.T3.3.6.6.4.1\" style=\"font-size:90%;\">81.26</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S2.T3.3.6.6.5\"><span class=\"ltx_text\" id=\"S2.T3.3.6.6.5.1\" style=\"font-size:90%;\">69.57</span></td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 3: Utilizing attribution for hallucination detection on ChatGPT responses."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T4\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>Effectiveness of EK-FAC approximated influence function on the established SciFact-Inf dataset. Time (Pre.) stands for the time for precomputing the , , and . Time (Inf.) stands for the time for calculating the influence of 100 training samples per test sample. Experiments for GPT2-1.5B, LLaMA2-7B, Mistral-7B are done on 4 A100 GPUs, whereas experiments for LLaMA2-13B are done with 8 A100 GPUs.</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T4.7\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T4.7.1.1\">\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S4.T4.7.1.1.1\">Strategy</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S4.T4.7.1.1.2\">LLM</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T4.7.1.1.3\">Recall@5</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T4.7.1.1.4\">Recall@10</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T4.7.1.1.5\">Time (Pre.)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T4.7.1.1.6\">Time (Inf.)</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.7.2.2\">\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_t\" id=\"S4.T4.7.2.2.1\">Random</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" id=\"S4.T4.7.2.2.2\">-</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T4.7.2.2.3\">0.0100</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T4.7.2.2.4\">0.0200</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T4.7.2.2.5\">-</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T4.7.2.2.6\">-</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T4.7.3.1\">\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_t\" id=\"S4.T4.7.3.1.1\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S4.T4.7.3.1.1.1\">Inf. Func.</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T4.7.3.1.2\">GPT2-1.5B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.7.3.1.3\">0.6368</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.7.3.1.4\">0.7363</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.7.3.1.5\">0h 27min</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.7.3.1.6\">0min 28sec</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.7.4.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T4.7.4.2.1\">Mistral-7B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.7.4.2.2\">0.6418</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.7.4.2.3\">0.6866</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.7.4.2.4\">2h 05min</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.7.4.2.5\">1min 47sec</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.7.5.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T4.7.5.3.1\">LLaMA2-7B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.7.5.3.2\">0.8063</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.7.5.3.3\">0.8308</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.7.5.3.4\">1h 37min</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.7.5.3.5\">1min 34sec</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.7.6.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S4.T4.7.6.4.1\">LLaMA2-13B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T4.7.6.4.2\">0.7811</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T4.7.6.4.3\">0.8940</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T4.7.6.4.4\">3h 11min</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T4.7.6.4.5\">3min 08sec</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 4: Effectiveness of EK-FAC approximated influence function on the established SciFact-Inf dataset. Time (Pre.) stands for the time for precomputing the , , and . Time (Inf.) stands for the time for calculating the influence of 100 training samples per test sample. Experiments for GPT2-1.5B, LLaMA2-7B, Mistral-7B are done on 4 A100 GPUs, whereas experiments for LLaMA2-13B are done with 8 A100 GPUs."
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"S6.T5\">\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>Multi-hop question answering performance on MQUAKE-CF.</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S6.T5.24\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S6.T5.24.25.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" id=\"S6.T5.24.25.1.1\"><span class=\"ltx_text\" id=\"S6.T5.24.25.1.1.1\" style=\"font-size:90%;\">Datasets</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"9\" id=\"S6.T5.24.25.1.2\"><span class=\"ltx_text\" id=\"S6.T5.24.25.1.2.1\" style=\"font-size:90%;\">MQUAKE-CF</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T5.24.26.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" id=\"S6.T5.24.26.2.1\"><span class=\"ltx_text\" id=\"S6.T5.24.26.2.1.1\" style=\"font-size:90%;\">Question Type</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"3\" id=\"S6.T5.24.26.2.2\"><span class=\"ltx_text\" id=\"S6.T5.24.26.2.2.1\" style=\"font-size:90%;\">2-hops</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"3\" id=\"S6.T5.24.26.2.3\"><span class=\"ltx_text\" id=\"S6.T5.24.26.2.3.1\" style=\"font-size:90%;\">3-hops</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"3\" id=\"S6.T5.24.26.2.4\"><span class=\"ltx_text\" id=\"S6.T5.24.26.2.4.1\" style=\"font-size:90%;\">4-hops</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T5.24.27.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" id=\"S6.T5.24.27.3.1\"><span class=\"ltx_text\" id=\"S6.T5.24.27.3.1.1\" style=\"font-size:90%;\">Edited Instances</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S6.T5.24.27.3.2\"><span class=\"ltx_text\" id=\"S6.T5.24.27.3.2.1\" style=\"font-size:90%;\">Base</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S6.T5.24.27.3.3\"><span class=\"ltx_text\" id=\"S6.T5.24.27.3.3.1\" style=\"font-size:90%;\">Enhanced</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S6.T5.24.27.3.4\"><span class=\"ltx_text\" id=\"S6.T5.24.27.3.4.1\" style=\"font-size:90%;\">Improve</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S6.T5.24.27.3.5\"><span class=\"ltx_text\" id=\"S6.T5.24.27.3.5.1\" style=\"font-size:90%;\">Base</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S6.T5.24.27.3.6\"><span class=\"ltx_text\" id=\"S6.T5.24.27.3.6.1\" style=\"font-size:90%;\">Enhanced</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S6.T5.24.27.3.7\"><span class=\"ltx_text\" id=\"S6.T5.24.27.3.7.1\" style=\"font-size:90%;\">Improve</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S6.T5.24.27.3.8\"><span class=\"ltx_text\" id=\"S6.T5.24.27.3.8.1\" style=\"font-size:90%;\">Base</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S6.T5.24.27.3.9\"><span class=\"ltx_text\" id=\"S6.T5.24.27.3.9.1\" style=\"font-size:90%;\">Enhanced</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S6.T5.24.27.3.10\"><span class=\"ltx_text\" id=\"S6.T5.24.27.3.10.1\" style=\"font-size:90%;\">Improve</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S6.T5.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S6.T5.3.3.4\"><span class=\"ltx_text\" id=\"S6.T5.3.3.4.1\" style=\"font-size:90%;\">GPT-2 (1.5B)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T5.3.3.5\"><span class=\"ltx_text\" id=\"S6.T5.3.3.5.1\" style=\"font-size:90%;\">13.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T5.3.3.6\"><span class=\"ltx_text\" id=\"S6.T5.3.3.6.1\" style=\"font-size:90%;\">15.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S6.T5.1.1.1\">\n<span class=\"ltx_text\" id=\"S6.T5.1.1.1.1\" style=\"font-size:90%;\">16.9% </span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T5.3.3.7\"><span class=\"ltx_text\" id=\"S6.T5.3.3.7.1\" style=\"font-size:90%;\">11.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T5.3.3.8\"><span class=\"ltx_text\" id=\"S6.T5.3.3.8.1\" style=\"font-size:90%;\">8.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S6.T5.2.2.2\">\n<span class=\"ltx_text\" id=\"S6.T5.2.2.2.1\" style=\"font-size:90%;\">23.3% </span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T5.3.3.9\"><span class=\"ltx_text\" id=\"S6.T5.3.3.9.1\" style=\"font-size:90%;\">7.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T5.3.3.10\"><span class=\"ltx_text\" id=\"S6.T5.3.3.10.1\" style=\"font-size:90%;\">8.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T5.3.3.3\">\n<span class=\"ltx_text\" id=\"S6.T5.3.3.3.1\" style=\"font-size:90%;\">20.0% </span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T5.6.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S6.T5.6.6.4\"><span class=\"ltx_text\" id=\"S6.T5.6.6.4.1\" style=\"font-size:90%;\">GPT-J (6B)</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.6.6.5\"><span class=\"ltx_text\" id=\"S6.T5.6.6.5.1\" style=\"font-size:90%;\">23.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.6.6.6\"><span class=\"ltx_text\" id=\"S6.T5.6.6.6.1\" style=\"font-size:90%;\">51.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S6.T5.4.4.1\">\n<span class=\"ltx_text\" id=\"S6.T5.4.4.1.1\" style=\"font-size:90%;\">124.7% </span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.6.6.7\"><span class=\"ltx_text\" id=\"S6.T5.6.6.7.1\" style=\"font-size:90%;\">10.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.6.6.8\"><span class=\"ltx_text\" id=\"S6.T5.6.6.8.1\" style=\"font-size:90%;\">30.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S6.T5.5.5.2\">\n<span class=\"ltx_text\" id=\"S6.T5.5.5.2.1\" style=\"font-size:90%;\">202.0% </span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.6.6.9\"><span class=\"ltx_text\" id=\"S6.T5.6.6.9.1\" style=\"font-size:90%;\">21.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.6.6.10\"><span class=\"ltx_text\" id=\"S6.T5.6.6.10.1\" style=\"font-size:90%;\">49.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.6.6.3\">\n<span class=\"ltx_text\" id=\"S6.T5.6.6.3.1\" style=\"font-size:90%;\">133.8% </span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T5.9.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S6.T5.9.9.4\"><span class=\"ltx_text\" id=\"S6.T5.9.9.4.1\" style=\"font-size:90%;\">LLaMA (7B)</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.9.9.5\"><span class=\"ltx_text\" id=\"S6.T5.9.9.5.1\" style=\"font-size:90%;\">47.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.9.9.6\"><span class=\"ltx_text\" id=\"S6.T5.9.9.6.1\" style=\"font-size:90%;\">65.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S6.T5.7.7.1\">\n<span class=\"ltx_text\" id=\"S6.T5.7.7.1.1\" style=\"font-size:90%;\">36.4% </span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.9.9.7\"><span class=\"ltx_text\" id=\"S6.T5.9.9.7.1\" style=\"font-size:90%;\">29.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.9.9.8\"><span class=\"ltx_text\" id=\"S6.T5.9.9.8.1\" style=\"font-size:90%;\">39.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S6.T5.8.8.2\">\n<span class=\"ltx_text\" id=\"S6.T5.8.8.2.1\" style=\"font-size:90%;\">32.8% </span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.9.9.9\"><span class=\"ltx_text\" id=\"S6.T5.9.9.9.1\" style=\"font-size:90%;\">52.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.9.9.10\"><span class=\"ltx_text\" id=\"S6.T5.9.9.10.1\" style=\"font-size:90%;\">62.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.9.9.3\">\n<span class=\"ltx_text\" id=\"S6.T5.9.9.3.1\" style=\"font-size:90%;\">20.0% </span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T5.12.12\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S6.T5.12.12.4\"><span class=\"ltx_text\" id=\"S6.T5.12.12.4.1\" style=\"font-size:90%;\">Vicuna-v1.5 (7B)</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.12.12.5\"><span class=\"ltx_text\" id=\"S6.T5.12.12.5.1\" style=\"font-size:90%;\">41.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.12.12.6\"><span class=\"ltx_text\" id=\"S6.T5.12.12.6.1\" style=\"font-size:90%;\">56.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S6.T5.10.10.1\">\n<span class=\"ltx_text\" id=\"S6.T5.10.10.1.1\" style=\"font-size:90%;\">36.3% </span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.12.12.7\"><span class=\"ltx_text\" id=\"S6.T5.12.12.7.1\" style=\"font-size:90%;\">22.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.12.12.8\"><span class=\"ltx_text\" id=\"S6.T5.12.12.8.1\" style=\"font-size:90%;\">29.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S6.T5.11.11.2\">\n<span class=\"ltx_text\" id=\"S6.T5.11.11.2.1\" style=\"font-size:90%;\">30.9% </span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.12.12.9\"><span class=\"ltx_text\" id=\"S6.T5.12.12.9.1\" style=\"font-size:90%;\">31.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.12.12.10\"><span class=\"ltx_text\" id=\"S6.T5.12.12.10.1\" style=\"font-size:90%;\">53.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.12.12.3\">\n<span class=\"ltx_text\" id=\"S6.T5.12.12.3.1\" style=\"font-size:90%;\">68.2% </span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T5.15.15\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S6.T5.15.15.4\"><span class=\"ltx_text\" id=\"S6.T5.15.15.4.1\" style=\"font-size:90%;\">LLaMA2 (7B)</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.15.15.5\"><span class=\"ltx_text\" id=\"S6.T5.15.15.5.1\" style=\"font-size:90%;\">36.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.15.15.6\"><span class=\"ltx_text\" id=\"S6.T5.15.15.6.1\" style=\"font-size:90%;\">58.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S6.T5.13.13.1\">\n<span class=\"ltx_text\" id=\"S6.T5.13.13.1.1\" style=\"font-size:90%;\">60.0% </span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.15.15.7\"><span class=\"ltx_text\" id=\"S6.T5.15.15.7.1\" style=\"font-size:90%;\">17.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.15.15.8\"><span class=\"ltx_text\" id=\"S6.T5.15.15.8.1\" style=\"font-size:90%;\">30.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S6.T5.14.14.2\">\n<span class=\"ltx_text\" id=\"S6.T5.14.14.2.1\" style=\"font-size:90%;\">78.4% </span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.15.15.9\"><span class=\"ltx_text\" id=\"S6.T5.15.15.9.1\" style=\"font-size:90%;\">29.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.15.15.10\"><span class=\"ltx_text\" id=\"S6.T5.15.15.10.1\" style=\"font-size:90%;\">49.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.15.15.3\">\n<span class=\"ltx_text\" id=\"S6.T5.15.15.3.1\" style=\"font-size:90%;\">68.1% </span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T5.18.18\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S6.T5.18.18.4\"><span class=\"ltx_text\" id=\"S6.T5.18.18.4.1\" style=\"font-size:90%;\">Falcon (7B)</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.18.18.5\"><span class=\"ltx_text\" id=\"S6.T5.18.18.5.1\" style=\"font-size:90%;\">42.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.18.18.6\"><span class=\"ltx_text\" id=\"S6.T5.18.18.6.1\" style=\"font-size:90%;\">61.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S6.T5.16.16.1\">\n<span class=\"ltx_text\" id=\"S6.T5.16.16.1.1\" style=\"font-size:90%;\">45.7% </span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.18.18.7\"><span class=\"ltx_text\" id=\"S6.T5.18.18.7.1\" style=\"font-size:90%;\">23.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.18.18.8\"><span class=\"ltx_text\" id=\"S6.T5.18.18.8.1\" style=\"font-size:90%;\">31.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S6.T5.17.17.2\">\n<span class=\"ltx_text\" id=\"S6.T5.17.17.2.1\" style=\"font-size:90%;\">35.7% </span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.18.18.9\"><span class=\"ltx_text\" id=\"S6.T5.18.18.9.1\" style=\"font-size:90%;\">33.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.18.18.10\"><span class=\"ltx_text\" id=\"S6.T5.18.18.10.1\" style=\"font-size:90%;\">48.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.18.18.3\">\n<span class=\"ltx_text\" id=\"S6.T5.18.18.3.1\" style=\"font-size:90%;\">45.7% </span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T5.21.21\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S6.T5.21.21.4\"><span class=\"ltx_text\" id=\"S6.T5.21.21.4.1\" style=\"font-size:90%;\">Mistral-v0.1 (7B)</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.21.21.5\"><span class=\"ltx_text\" id=\"S6.T5.21.21.5.1\" style=\"font-size:90%;\">49.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.21.21.6\"><span class=\"ltx_text\" id=\"S6.T5.21.21.6.1\" style=\"font-size:90%;\">69.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S6.T5.19.19.1\">\n<span class=\"ltx_text\" id=\"S6.T5.19.19.1.1\" style=\"font-size:90%;\">41.5% </span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.21.21.7\"><span class=\"ltx_text\" id=\"S6.T5.21.21.7.1\" style=\"font-size:90%;\">30.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.21.21.8\"><span class=\"ltx_text\" id=\"S6.T5.21.21.8.1\" style=\"font-size:90%;\">42.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S6.T5.20.20.2\">\n<span class=\"ltx_text\" id=\"S6.T5.20.20.2.1\" style=\"font-size:90%;\">41.1% </span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.21.21.9\"><span class=\"ltx_text\" id=\"S6.T5.21.21.9.1\" style=\"font-size:90%;\">48.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.21.21.10\"><span class=\"ltx_text\" id=\"S6.T5.21.21.10.1\" style=\"font-size:90%;\">63.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.21.21.3\">\n<span class=\"ltx_text\" id=\"S6.T5.21.21.3.1\" style=\"font-size:90%;\">29.9% </span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T5.24.24\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S6.T5.24.24.4\"><span class=\"ltx_text\" id=\"S6.T5.24.24.4.1\" style=\"font-size:90%;\">Mistral-v0.2 (7B)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T5.24.24.5\"><span class=\"ltx_text\" id=\"S6.T5.24.24.5.1\" style=\"font-size:90%;\">44.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T5.24.24.6\"><span class=\"ltx_text\" id=\"S6.T5.24.24.6.1\" style=\"font-size:90%;\">56.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S6.T5.22.22.1\">\n<span class=\"ltx_text\" id=\"S6.T5.22.22.1.1\" style=\"font-size:90%;\">28.0% </span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T5.24.24.7\"><span class=\"ltx_text\" id=\"S6.T5.24.24.7.1\" style=\"font-size:90%;\">23.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T5.24.24.8\"><span class=\"ltx_text\" id=\"S6.T5.24.24.8.1\" style=\"font-size:90%;\">37.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S6.T5.23.23.2\">\n<span class=\"ltx_text\" id=\"S6.T5.23.23.2.1\" style=\"font-size:90%;\">63.8% </span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T5.24.24.9\"><span class=\"ltx_text\" id=\"S6.T5.24.24.9.1\" style=\"font-size:90%;\">32.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T5.24.24.10\"><span class=\"ltx_text\" id=\"S6.T5.24.24.10.1\" style=\"font-size:90%;\">56.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T5.24.24.3\">\n<span class=\"ltx_text\" id=\"S6.T5.24.24.3.1\" style=\"font-size:90%;\">70.9% </span>\n</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 5: Multi-hop question answering performance on MQUAKE-CF."
        },
        "6": {
            "table_html": "<figure class=\"ltx_table\" id=\"S6.T6\">\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 6: </span>CoT Faithfulness Evaluation on MQUAKE-CF.</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S6.T6.3\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S6.T6.3.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\" id=\"S6.T6.3.1.1.1\"><span class=\"ltx_text\" id=\"S6.T6.3.1.1.1.1\" style=\"font-size:90%;\">Datasets</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"9\" id=\"S6.T6.3.1.1.2\"><span class=\"ltx_text\" id=\"S6.T6.3.1.1.2.1\" style=\"font-size:90%;\">MQUAKE-CF</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.3.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S6.T6.3.2.2.1\"><span class=\"ltx_text\" id=\"S6.T6.3.2.2.1.1\" style=\"font-size:90%;\">Question Type</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\" id=\"S6.T6.3.2.2.2\"><span class=\"ltx_text\" id=\"S6.T6.3.2.2.2.1\" style=\"font-size:90%;\">2-hops</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\" id=\"S6.T6.3.2.2.3\"><span class=\"ltx_text\" id=\"S6.T6.3.2.2.3.1\" style=\"font-size:90%;\">3-hops</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"3\" id=\"S6.T6.3.2.2.4\"><span class=\"ltx_text\" id=\"S6.T6.3.2.2.4.1\" style=\"font-size:90%;\">4-hops</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.3.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S6.T6.3.3.3.1\"><span class=\"ltx_text\" id=\"S6.T6.3.3.3.1.1\" style=\"font-size:90%;\">Edited Instances</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S6.T6.3.3.3.2\"><span class=\"ltx_text\" id=\"S6.T6.3.3.3.2.1\" style=\"font-size:90%;\">Correct</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S6.T6.3.3.3.3\"><span class=\"ltx_text\" id=\"S6.T6.3.3.3.3.1\" style=\"font-size:90%;\">Mislead</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S6.T6.3.3.3.4\"><span class=\"ltx_text\" id=\"S6.T6.3.3.3.4.1\" style=\"font-size:90%;\">Fidelity</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S6.T6.3.3.3.5\"><span class=\"ltx_text\" id=\"S6.T6.3.3.3.5.1\" style=\"font-size:90%;\">Correct</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S6.T6.3.3.3.6\"><span class=\"ltx_text\" id=\"S6.T6.3.3.3.6.1\" style=\"font-size:90%;\">Mislead</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S6.T6.3.3.3.7\"><span class=\"ltx_text\" id=\"S6.T6.3.3.3.7.1\" style=\"font-size:90%;\">Fidelity</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S6.T6.3.3.3.8\"><span class=\"ltx_text\" id=\"S6.T6.3.3.3.8.1\" style=\"font-size:90%;\">Correct</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S6.T6.3.3.3.9\"><span class=\"ltx_text\" id=\"S6.T6.3.3.3.9.1\" style=\"font-size:90%;\">Mislead</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.3.3.3.10\"><span class=\"ltx_text\" id=\"S6.T6.3.3.3.10.1\" style=\"font-size:90%;\">Fidelity</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.3.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S6.T6.3.4.4.1\"><span class=\"ltx_text\" id=\"S6.T6.3.4.4.1.1\" style=\"font-size:90%;\">GPT-2 (1.5B)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.3.4.4.2\"><span class=\"ltx_text\" id=\"S6.T6.3.4.4.2.1\" style=\"font-size:90%;\">15.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.3.4.4.3\"><span class=\"ltx_text\" id=\"S6.T6.3.4.4.3.1\" style=\"font-size:90%;\">5.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S6.T6.3.4.4.4\"><span class=\"ltx_text\" id=\"S6.T6.3.4.4.4.1\" style=\"font-size:90%;\">67.3%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.3.4.4.5\"><span class=\"ltx_text\" id=\"S6.T6.3.4.4.5.1\" style=\"font-size:90%;\">8.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.3.4.4.6\"><span class=\"ltx_text\" id=\"S6.T6.3.4.4.6.1\" style=\"font-size:90%;\">2.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S6.T6.3.4.4.7\"><span class=\"ltx_text\" id=\"S6.T6.3.4.4.7.1\" style=\"font-size:90%;\">67.4%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.3.4.4.8\"><span class=\"ltx_text\" id=\"S6.T6.3.4.4.8.1\" style=\"font-size:90%;\">8.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.3.4.4.9\"><span class=\"ltx_text\" id=\"S6.T6.3.4.4.9.1\" style=\"font-size:90%;\">1.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.3.4.4.10\"><span class=\"ltx_text\" id=\"S6.T6.3.4.4.10.1\" style=\"font-size:90%;\">84.5%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.3.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S6.T6.3.5.5.1\"><span class=\"ltx_text\" id=\"S6.T6.3.5.5.1.1\" style=\"font-size:90%;\">GPT-J (6B)</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.3.5.5.2\"><span class=\"ltx_text\" id=\"S6.T6.3.5.5.2.1\" style=\"font-size:90%;\">51.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.3.5.5.3\"><span class=\"ltx_text\" id=\"S6.T6.3.5.5.3.1\" style=\"font-size:90%;\">7.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S6.T6.3.5.5.4\"><span class=\"ltx_text\" id=\"S6.T6.3.5.5.4.1\" style=\"font-size:90%;\">85.9%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.3.5.5.5\"><span class=\"ltx_text\" id=\"S6.T6.3.5.5.5.1\" style=\"font-size:90%;\">30.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.3.5.5.6\"><span class=\"ltx_text\" id=\"S6.T6.3.5.5.6.1\" style=\"font-size:90%;\">1.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S6.T6.3.5.5.7\"><span class=\"ltx_text\" id=\"S6.T6.3.5.5.7.1\" style=\"font-size:90%;\">94.1%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.3.5.5.8\"><span class=\"ltx_text\" id=\"S6.T6.3.5.5.8.1\" style=\"font-size:90%;\">49.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.3.5.5.9\"><span class=\"ltx_text\" id=\"S6.T6.3.5.5.9.1\" style=\"font-size:90%;\">2.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.3.5.5.10\"><span class=\"ltx_text\" id=\"S6.T6.3.5.5.10.1\" style=\"font-size:90%;\">96.0%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.3.6.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S6.T6.3.6.6.1\"><span class=\"ltx_text\" id=\"S6.T6.3.6.6.1.1\" style=\"font-size:90%;\">LLaMA (6B)</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.3.6.6.2\"><span class=\"ltx_text\" id=\"S6.T6.3.6.6.2.1\" style=\"font-size:90%;\">65.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.3.6.6.3\"><span class=\"ltx_text\" id=\"S6.T6.3.6.6.3.1\" style=\"font-size:90%;\">9.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S6.T6.3.6.6.4\"><span class=\"ltx_text\" id=\"S6.T6.3.6.6.4.1\" style=\"font-size:90%;\">84.8%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.3.6.6.5\"><span class=\"ltx_text\" id=\"S6.T6.3.6.6.5.1\" style=\"font-size:90%;\">39.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.3.6.6.6\"><span class=\"ltx_text\" id=\"S6.T6.3.6.6.6.1\" style=\"font-size:90%;\">6.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S6.T6.3.6.6.7\"><span class=\"ltx_text\" id=\"S6.T6.3.6.6.7.1\" style=\"font-size:90%;\">84.5%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.3.6.6.8\"><span class=\"ltx_text\" id=\"S6.T6.3.6.6.8.1\" style=\"font-size:90%;\">62.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.3.6.6.9\"><span class=\"ltx_text\" id=\"S6.T6.3.6.6.9.1\" style=\"font-size:90%;\">6.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.3.6.6.10\"><span class=\"ltx_text\" id=\"S6.T6.3.6.6.10.1\" style=\"font-size:90%;\">90.5%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.3.7.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S6.T6.3.7.7.1\"><span class=\"ltx_text\" id=\"S6.T6.3.7.7.1.1\" style=\"font-size:90%;\">Vicuna-v1.5 (7B)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.3.7.7.2\"><span class=\"ltx_text\" id=\"S6.T6.3.7.7.2.1\" style=\"font-size:90%;\">56.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.3.7.7.3\"><span class=\"ltx_text\" id=\"S6.T6.3.7.7.3.1\" style=\"font-size:90%;\">21.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S6.T6.3.7.7.4\"><span class=\"ltx_text\" id=\"S6.T6.3.7.7.4.1\" style=\"font-size:90%;\">61.5%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.3.7.7.5\"><span class=\"ltx_text\" id=\"S6.T6.3.7.7.5.1\" style=\"font-size:90%;\">29.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.3.7.7.6\"><span class=\"ltx_text\" id=\"S6.T6.3.7.7.6.1\" style=\"font-size:90%;\">12.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S6.T6.3.7.7.7\"><span class=\"ltx_text\" id=\"S6.T6.3.7.7.7.1\" style=\"font-size:90%;\">57.3%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.3.7.7.8\"><span class=\"ltx_text\" id=\"S6.T6.3.7.7.8.1\" style=\"font-size:90%;\">53.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.3.7.7.9\"><span class=\"ltx_text\" id=\"S6.T6.3.7.7.9.1\" style=\"font-size:90%;\">16.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.3.7.7.10\"><span class=\"ltx_text\" id=\"S6.T6.3.7.7.10.1\" style=\"font-size:90%;\">69.7%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.3.8.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S6.T6.3.8.8.1\"><span class=\"ltx_text\" id=\"S6.T6.3.8.8.1.1\" style=\"font-size:90%;\">LLaMA2 (7B)</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.3.8.8.2\"><span class=\"ltx_text\" id=\"S6.T6.3.8.8.2.1\" style=\"font-size:90%;\">58.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.3.8.8.3\"><span class=\"ltx_text\" id=\"S6.T6.3.8.8.3.1\" style=\"font-size:90%;\">17.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S6.T6.3.8.8.4\"><span class=\"ltx_text\" id=\"S6.T6.3.8.8.4.1\" style=\"font-size:90%;\">71.0%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.3.8.8.5\"><span class=\"ltx_text\" id=\"S6.T6.3.8.8.5.1\" style=\"font-size:90%;\">30.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.3.8.8.6\"><span class=\"ltx_text\" id=\"S6.T6.3.8.8.6.1\" style=\"font-size:90%;\">8.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S6.T6.3.8.8.7\"><span class=\"ltx_text\" id=\"S6.T6.3.8.8.7.1\" style=\"font-size:90%;\">72.5%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.3.8.8.8\"><span class=\"ltx_text\" id=\"S6.T6.3.8.8.8.1\" style=\"font-size:90%;\">49.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.3.8.8.9\"><span class=\"ltx_text\" id=\"S6.T6.3.8.8.9.1\" style=\"font-size:90%;\">12.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.3.8.8.10\"><span class=\"ltx_text\" id=\"S6.T6.3.8.8.10.1\" style=\"font-size:90%;\">75.6%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.3.9.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S6.T6.3.9.9.1\"><span class=\"ltx_text\" id=\"S6.T6.3.9.9.1.1\" style=\"font-size:90%;\">Falcon (7B)</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.3.9.9.2\"><span class=\"ltx_text\" id=\"S6.T6.3.9.9.2.1\" style=\"font-size:90%;\">61.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.3.9.9.3\"><span class=\"ltx_text\" id=\"S6.T6.3.9.9.3.1\" style=\"font-size:90%;\">24.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S6.T6.3.9.9.4\"><span class=\"ltx_text\" id=\"S6.T6.3.9.9.4.1\" style=\"font-size:90%;\">61.1%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.3.9.9.5\"><span class=\"ltx_text\" id=\"S6.T6.3.9.9.5.1\" style=\"font-size:90%;\">31.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.3.9.9.6\"><span class=\"ltx_text\" id=\"S6.T6.3.9.9.6.1\" style=\"font-size:90%;\">15.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S6.T6.3.9.9.7\"><span class=\"ltx_text\" id=\"S6.T6.3.9.9.7.1\" style=\"font-size:90%;\">52.6%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.3.9.9.8\"><span class=\"ltx_text\" id=\"S6.T6.3.9.9.8.1\" style=\"font-size:90%;\">48.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.3.9.9.9\"><span class=\"ltx_text\" id=\"S6.T6.3.9.9.9.1\" style=\"font-size:90%;\">23.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.3.9.9.10\"><span class=\"ltx_text\" id=\"S6.T6.3.9.9.10.1\" style=\"font-size:90%;\">52.4%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.3.10.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S6.T6.3.10.10.1\"><span class=\"ltx_text\" id=\"S6.T6.3.10.10.1.1\" style=\"font-size:90%;\">Mistral-v0.1 (7B)</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.3.10.10.2\"><span class=\"ltx_text\" id=\"S6.T6.3.10.10.2.1\" style=\"font-size:90%;\">69.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.3.10.10.3\"><span class=\"ltx_text\" id=\"S6.T6.3.10.10.3.1\" style=\"font-size:90%;\">24.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S6.T6.3.10.10.4\"><span class=\"ltx_text\" id=\"S6.T6.3.10.10.4.1\" style=\"font-size:90%;\">65.4%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.3.10.10.5\"><span class=\"ltx_text\" id=\"S6.T6.3.10.10.5.1\" style=\"font-size:90%;\">42.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.3.10.10.6\"><span class=\"ltx_text\" id=\"S6.T6.3.10.10.6.1\" style=\"font-size:90%;\">13.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S6.T6.3.10.10.7\"><span class=\"ltx_text\" id=\"S6.T6.3.10.10.7.1\" style=\"font-size:90%;\">69.3%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.3.10.10.8\"><span class=\"ltx_text\" id=\"S6.T6.3.10.10.8.1\" style=\"font-size:90%;\">63.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.3.10.10.9\"><span class=\"ltx_text\" id=\"S6.T6.3.10.10.9.1\" style=\"font-size:90%;\">18.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.3.10.10.10\"><span class=\"ltx_text\" id=\"S6.T6.3.10.10.10.1\" style=\"font-size:90%;\">70.8%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.3.11.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S6.T6.3.11.11.1\"><span class=\"ltx_text\" id=\"S6.T6.3.11.11.1.1\" style=\"font-size:90%;\">Mistral-v0.2 (7B)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T6.3.11.11.2\"><span class=\"ltx_text\" id=\"S6.T6.3.11.11.2.1\" style=\"font-size:90%;\">56.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T6.3.11.11.3\"><span class=\"ltx_text\" id=\"S6.T6.3.11.11.3.1\" style=\"font-size:90%;\">47.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S6.T6.3.11.11.4\"><span class=\"ltx_text\" id=\"S6.T6.3.11.11.4.1\" style=\"font-size:90%;\">14.8%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T6.3.11.11.5\"><span class=\"ltx_text\" id=\"S6.T6.3.11.11.5.1\" style=\"font-size:90%;\">37.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T6.3.11.11.6\"><span class=\"ltx_text\" id=\"S6.T6.3.11.11.6.1\" style=\"font-size:90%;\">22.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S6.T6.3.11.11.7\"><span class=\"ltx_text\" id=\"S6.T6.3.11.11.7.1\" style=\"font-size:90%;\">41.6%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T6.3.11.11.8\"><span class=\"ltx_text\" id=\"S6.T6.3.11.11.8.1\" style=\"font-size:90%;\">56.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T6.3.11.11.9\"><span class=\"ltx_text\" id=\"S6.T6.3.11.11.9.1\" style=\"font-size:90%;\">37.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T6.3.11.11.10\"><span class=\"ltx_text\" id=\"S6.T6.3.11.11.10.1\" style=\"font-size:90%;\">33.6%</span></td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 6: CoT Faithfulness Evaluation on MQUAKE-CF."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.08946v1_figure_1.png",
            "caption": "Figure 1: The contributions and outline of this paper. We define Usable XAI in the context of LLMs with seven strategies of enhancing LLMs with XAI, and three strategies of enhancing XAI with LLMs."
        },
        "2": {
            "figure_path": "2403.08946v1_figure_2.png",
            "caption": "Figure 2: An example of attribution saliency map between input and output tokens."
        },
        "3": {
            "figure_path": "2403.08946v1_figure_3.png",
            "caption": "Figure 3: A general pipeline of model diagnosis with attribution explanations."
        },
        "4": {
            "figure_path": "2403.08946v1_figure_4.png",
            "caption": "Figure 4: Review of interpretation methods for LLM components and their applications. We categorize methods according to the target LLM modules: self-attention layers and feed-forward layers."
        }
    },
    "references": [
        {
            "1": {
                "title": "Gpt-4 technical report.",
                "author": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.",
                "venue": "arXiv preprint arXiv:2303.08774, 2023.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Quantifying and mitigating the impact of label errors on model disparity metrics.",
                "author": "Julius Adebayo, Melissa Hall, Bowen Yu, and Bobbie Chern.",
                "venue": "arXiv preprint arXiv:2310.02533, 2023.",
                "url": null
            }
        },
        {
            "3": {
                "title": "Evaluating correctness and faithfulness of instruction-following models for question answering.",
                "author": "Vaibhav Adlakha, Parishad BehnamGhader, Xing Han Lu, Nicholas Meade, and Siva Reddy.",
                "venue": "arXiv preprint arXiv:2307.16877, 2023.",
                "url": null
            }
        },
        {
            "4": {
                "title": "Distinguishing the knowable from the unknowable with language models.",
                "author": "Gustaf Ahdritz, Tian Qin, Nikhil Vyas, Boaz Barak, and Benjamin L Edelman.",
                "venue": "arXiv preprint arXiv:2402.03563, 2024.",
                "url": null
            }
        },
        {
            "5": {
                "title": "Towards tracing knowledge in language models back to the training data.",
                "author": "Ekin Akyurek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, and Kelvin Guu.",
                "venue": "In Findings of EMNLP, pp.  2429\u20132446, December 2022.",
                "url": null
            }
        },
        {
            "6": {
                "title": "Where is your evidence: Improving fact-checking by justification modeling.",
                "author": "Tariq Alhindi, Savvas Petridis, and Smaranda Muresan.",
                "venue": "In Proceedings of the first workshop on fact extraction and verification (FEVER), pp.  85\u201390, 2018.",
                "url": null
            }
        },
        {
            "7": {
                "title": "The falcon series of open language models.",
                "author": "Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, M\u00e9rouane Debbah, \u00c9tienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al.",
                "venue": "arXiv preprint arXiv:2311.16867, 2023.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Rationalization through concepts.",
                "author": "Diego Matteo Antognini and Boi Faltings.",
                "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp.  761\u2013775, 2021.",
                "url": null
            }
        },
        {
            "9": {
                "title": "The principle of minimized iterations in the solution of the matrix eigenvalue problem.",
                "author": "Walter Edwin Arnoldi.",
                "venue": "Quarterly of applied mathematics, 9(1):17\u201329, 1951.",
                "url": null
            }
        },
        {
            "10": {
                "title": "Linear algebraic structure of word senses, with applications to polysemy.",
                "author": "Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski.",
                "venue": "Transactions of the Association for Computational Linguistics, 6:483\u2013495, 2018.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Self-rag: Learning to retrieve, generate, and critique through self-reflection.",
                "author": "Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi.",
                "venue": "arXiv preprint arXiv:2310.11511, 2023.",
                "url": null
            }
        },
        {
            "12": {
                "title": "The internal state of an llm knows when its lying.",
                "author": "Amos Azaria and Tom Mitchell.",
                "venue": "arXiv preprint arXiv:2304.13734, 2023.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Syntax-bert: Improving pre-trained transformers with syntax trees.",
                "author": "Jiangang Bai, Yujing Wang, Yiren Chen, Yaming Yang, Jing Bai, Jing Yu, and Yunhai Tong.",
                "venue": "arXiv preprint arXiv:2103.04350, 2021.",
                "url": null
            }
        },
        {
            "14": {
                "title": "Characterizing large language model geometry solves toxicity detection and generation.",
                "author": "Randall Balestriero, Romain Cosentino, and Sarath Shekkizhar.",
                "venue": "arXiv preprint arXiv:2312.01648, 2023.",
                "url": null
            }
        },
        {
            "15": {
                "title": "Network dissection: Quantifying interpretability of deep visual representations.",
                "author": "David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba.",
                "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.  6541\u20136549, 2017.",
                "url": null
            }
        },
        {
            "16": {
                "title": "Gan dissection: Visualizing and understanding generative adversarial networks.",
                "author": "David Bau, Jun-Yan Zhu, Hendrik Strobelt, Bolei Zhou, Joshua B Tenenbaum, William T Freeman, and Antonio Torralba.",
                "venue": "In International Conference on Learning Representations, 2018.",
                "url": null
            }
        },
        {
            "17": {
                "title": "Leveraging chatgpt as text annotation tool for sentiment analysis.",
                "author": "Mohammad Belal, James She, and Simon Wong.",
                "venue": "arXiv preprint arXiv:2306.17177, 2023.",
                "url": null
            }
        },
        {
            "18": {
                "title": "Evaluating layers of representation in neural machine translation on part-of-speech and semantic tagging tasks.",
                "author": "Yonatan Belinkov, Llu\u00eds M\u00e0rquez, Hassan Sajjad, Nadir Durrani, Fahim Dalvi, and James Glass.",
                "venue": "arXiv preprint arXiv:1801.07772, 2018.",
                "url": null
            }
        },
        {
            "19": {
                "title": "Graph of thoughts: Solving elaborate problems with large language models.",
                "author": "Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, et al.",
                "venue": "arXiv preprint arXiv:2308.09687, 2023.",
                "url": null
            }
        },
        {
            "20": {
                "title": "Llms as counterfactual explanation modules: Can chatgpt explain black-box text classifiers?",
                "author": "Amrita Bhattacharjee, Raha Moraffah, Joshua Garland, and Huan Liu.",
                "venue": "arXiv preprint arXiv:2309.13340, 2023.",
                "url": null
            }
        },
        {
            "21": {
                "title": "Language models can explain neurons in language models.",
                "author": "Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, Jan Leike, Jeff Wu, and William Saunders.",
                "venue": "URL https://openaipublic. blob. core. windows. net/neuron-explainer/paper/index. html.(Date accessed: 14.05. 2023), 2023.",
                "url": null
            }
        },
        {
            "22": {
                "title": "Mathematical algorithm design for deep learning under societal and judicial constraints: The algorithmic transparency requirement.",
                "author": "Holger Boche, Adalbert Fono, and Gitta Kutyniok.",
                "venue": "arXiv preprint arXiv:2401.10310, 2024.",
                "url": null
            }
        },
        {
            "23": {
                "title": "Improving language models by retrieving from trillions of tokens.",
                "author": "Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al.",
                "venue": "In International conference on machine learning, pp.  2206\u20132240. PMLR, 2022.",
                "url": null
            }
        },
        {
            "24": {
                "title": "Attention approximates sparse distributed memory.",
                "author": "Trenton Bricken and Cengiz Pehlevan.",
                "venue": "Advances in Neural Information Processing Systems, 34:15301\u201315315, 2021.",
                "url": null
            }
        },
        {
            "25": {
                "title": "Towards monosemanticity: Decomposing language models with dictionary learning. transformer circuits thread, 2023, 2023.",
                "author": "Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, et al.",
                "venue": null,
                "url": null
            }
        },
        {
            "26": {
                "title": "Language models are few-shot learners.",
                "author": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.",
                "venue": "Advances in neural information processing systems, 33:1877\u20131901, 2020.",
                "url": null
            }
        },
        {
            "27": {
                "title": "Rationale-inspired natural language explanations with commonsense.",
                "author": "Bodhisattwa Prasad Majumder1 Oana-Maria Camburu, Thomas Lukasiewicz, and Julian McAuley.",
                "venue": "arXiv preprint arXiv:2106.13876, 2021.",
                "url": null
            }
        },
        {
            "28": {
                "title": "e-snli: Natural language inference with natural language explanations.",
                "author": "Oana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, and Phil Blunsom.",
                "venue": "Advances in Neural Information Processing Systems, 31, 2018.",
                "url": null
            }
        },
        {
            "29": {
                "title": "Localizing lying in llama: Understanding instructed dishonesty on true-false questions through prompting, probing, and patching.",
                "author": "James Campbell, Richard Ren, and Phillip Guo.",
                "venue": "arXiv preprint arXiv:2311.15131, 2023.",
                "url": null
            }
        },
        {
            "30": {
                "title": "What to learn, and how: Toward effective learning from rationales.",
                "author": "Samuel Carton, Surya Kanoria, and Chenhao Tan.",
                "venue": "arXiv preprint arXiv:2112.00071, 2021.",
                "url": null
            }
        },
        {
            "31": {
                "title": "Chat gpt: a promising tool for academic editing.",
                "author": "William Castillo-Gonz\u00e1lez, Carlos Oscar Lepez, and Mabel Cecilia Bonardi.",
                "venue": "Data Metadata, 1:23, 2022.",
                "url": null
            }
        },
        {
            "32": {
                "title": "Do explanations make vqa models more predictable to a human?",
                "author": "Arjun Chandrasekaran, Viraj Prabhu, Deshraj Yadav, Prithvijit Chattopadhyay, and Devi Parikh.",
                "venue": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp.  1036\u20131042, 2018.",
                "url": null
            }
        },
        {
            "33": {
                "title": "Interpretable deep models for icu outcome prediction.",
                "author": "Zhengping Che, Sanjay Purushotham, Robinder Khemani, and Yan Liu.",
                "venue": "In AMIA annual symposium proceedings, volume 2016, pp.  371. American Medical Informatics Association, 2016.",
                "url": null
            }
        },
        {
            "34": {
                "title": "Inside: Llms\u2019 internal states retain the power of hallucination detection.",
                "author": "Chao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu, Mingyuan Tao, Zhihang Fu, and Jieping Ye.",
                "venue": "In The Twelfth International Conference on Learning Representations, 2023a.",
                "url": null
            }
        },
        {
            "35": {
                "title": "Parameter-efficient fine-tuning design spaces.",
                "author": "Jiaao Chen, Aston Zhang, Xingjian Shi, Mu Li, Alex Smola, and Diyi Yang.",
                "venue": "In The Eleventh International Conference on Learning Representations, 2022.",
                "url": null
            }
        },
        {
            "36": {
                "title": "Evaluating large language models trained on code.",
                "author": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al.",
                "venue": "arXiv preprint arXiv:2107.03374, 2021.",
                "url": null
            }
        },
        {
            "37": {
                "title": "Zara: Improving few-shot self-rationalization for small language models.",
                "author": "Wei-Lin Chen, An-Zi Yen, Hen-Hsen Huang, Cheng-Kuang Wu, and Hsin-Hsi Chen.",
                "venue": "arXiv preprint arXiv:2305.07355, 2023b.",
                "url": null
            }
        },
        {
            "38": {
                "title": "Can large language models provide security & privacy advice? measuring the ability of llms to refute misconceptions.",
                "author": "Yufan Chen, Arjun Arunasalam, and Z Berkay Celik.",
                "venue": "In Proceedings of the 39th Annual Computer Security Applications Conference, pp.  366\u2013378, 2023c.",
                "url": null
            }
        },
        {
            "39": {
                "title": "Explainable recommendation with personalized review retrieval and aspect learning.",
                "author": "Hao Cheng, Shuo Wang, Wensheng Lu, Wei Zhang, Mingyang Zhou, Kezhong Lu, and Hao Liao.",
                "venue": "arXiv preprint arXiv:2306.12657, 2023.",
                "url": null
            }
        },
        {
            "40": {
                "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.",
                "author": "Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al.",
                "venue": "See https://vicuna. lmsys. org (accessed 14 April 2023), 2023.",
                "url": null
            }
        },
        {
            "41": {
                "title": "Benefits and harms of large language models in digital mental health, 2023.",
                "author": "Munmun De Choudhury, Sachin R. Pendse, and Neha Kumar.",
                "venue": null,
                "url": null
            }
        },
        {
            "42": {
                "title": "Chatlaw: Open-source legal large language model with integrated external knowledge bases.",
                "author": "Jiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and Li Yuan.",
                "venue": "arXiv preprint arXiv:2306.16092, 2023.",
                "url": null
            }
        },
        {
            "43": {
                "title": "M6-rec: Generative pretrained language models are open-ended recommender systems.",
                "author": "Zeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang.",
                "venue": "arXiv preprint arXiv:2205.08084, 2022.",
                "url": null
            }
        },
        {
            "44": {
                "title": "Sparse autoencoders find highly interpretable features in language models.",
                "author": "Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey.",
                "venue": "arXiv preprint arXiv:2309.08600, 2023.",
                "url": null
            }
        },
        {
            "45": {
                "title": "Dynamic planning with a llm.",
                "author": "Gautier Dagan, Frank Keller, and Alex Lascarides.",
                "venue": "arXiv preprint arXiv:2308.06391, 2023.",
                "url": null
            }
        },
        {
            "46": {
                "title": "Knowledge neurons in pretrained transformers.",
                "author": "Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei.",
                "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.  8493\u20138502, 2022.",
                "url": null
            }
        },
        {
            "47": {
                "title": "Chataug: Leveraging chatgpt for text data augmentation.",
                "author": "Haixing Dai, Zhengliang Liu, Wenxiong Liao, Xiaoke Huang, Zihao Wu, Lin Zhao, Wei Liu, Ninghao Liu, Sheng Li, Dajiang Zhu, et al.",
                "venue": "arXiv preprint arXiv:2302.13007, 2023.",
                "url": null
            }
        },
        {
            "48": {
                "title": "Deeper text understanding for ir with contextual neural language modeling.",
                "author": "Zhuyun Dai and Jamie Callan.",
                "venue": "In Proceedings of the 42nd international ACM SIGIR conference on research and development in information retrieval, pp.  985\u2013988, 2019.",
                "url": null
            }
        },
        {
            "49": {
                "title": "Analyzing redundancy in pretrained transformer models.",
                "author": "Fahim Dalvi, Hassan Sajjad, Nadir Durrani, and Yonatan Belinkov.",
                "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp.  4908\u20134926, 2020.",
                "url": null
            }
        },
        {
            "50": {
                "title": "A survey of the state of explainable ai for natural language processing.",
                "author": "Marina Danilevsky, Kun Qian, Ranit Aharonov, Yannis Katsis, Ban Kawas, and Prithviraj Sen.",
                "venue": "In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pp.  447\u2013459, 2020.",
                "url": null
            }
        },
        {
            "51": {
                "title": "Analyzing transformers in embedding space.",
                "author": "Guy Dar, Mor Geva, Ankit Gupta, and Jonathan Berant.",
                "venue": "In Annual Meeting of the Association for Computational Linguistics, 2023.",
                "url": null
            }
        },
        {
            "52": {
                "title": "Improving pretraining techniques for code-switched nlp.",
                "author": "Richeek Das, Sahasra Ranjan, Shreya Pathak, and Preethi Jyothi.",
                "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.  1176\u20131191, 2023.",
                "url": null
            }
        },
        {
            "53": {
                "title": "Unsupervised learning of disentangled representations from video.",
                "author": "Emily L Denton et al.",
                "venue": "Advances in neural information processing systems, 30, 2017.",
                "url": null
            }
        },
        {
            "54": {
                "title": "A security risk taxonomy for large language models.",
                "author": "Erik Derner, Kristina Batisti\u010d, Jan Zah\u00e1lka, and Robert Babu\u0161ka.",
                "venue": "arXiv preprint arXiv:2311.11415, 2023.",
                "url": null
            }
        },
        {
            "55": {
                "title": "Eraser: A benchmark to evaluate rationalized nlp models.",
                "author": "Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and Byron C Wallace.",
                "venue": "arXiv preprint arXiv:1911.03429, 2019.",
                "url": null
            }
        },
        {
            "56": {
                "title": "Eraser: A benchmark to evaluate rationalized nlp models.",
                "author": "Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and Byron C Wallace.",
                "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp.  4443\u20134458, 2020.",
                "url": null
            }
        },
        {
            "57": {
                "title": "Chain-of-verification reduces hallucination in large language models.",
                "author": "Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston.",
                "venue": "arXiv preprint arXiv:2309.11495, 2023.",
                "url": null
            }
        },
        {
            "58": {
                "title": "Is gpt-3 a good data annotator?",
                "author": "Bosheng Ding, Chengwei Qin, Linlin Liu, Yew Ken Chia, Shafiq Joty, Boyang Li, and Lidong Bing.",
                "venue": "arXiv preprint arXiv:2212.10450, 2022.",
                "url": null
            }
        },
        {
            "59": {
                "title": "Everything of thoughts: Defying the law of penrose triangle for thought generation.",
                "author": "Ruomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu, Minghua Ma, Wei Zhang, Si Qin, Saravan Rajmohan, Qingwei Lin, and Dongmei Zhang.",
                "venue": "arXiv preprint arXiv:2311.04254, 2023.",
                "url": null
            }
        },
        {
            "60": {
                "title": "Towards a rigorous science of interpretable machine learning.",
                "author": "Finale Doshi-Velez and Been Kim.",
                "venue": "arXiv preprint arXiv:1702.08608, 2017.",
                "url": null
            }
        },
        {
            "61": {
                "title": "Explainable artificial intelligence: A survey.",
                "author": "Filip Karlo Do\u0161ilovi\u0107, Mario Br\u010di\u0107, and Nikica Hlupi\u0107.",
                "venue": "In 2018 41st International convention on information and communication technology, electronics and microelectronics (MIPRO), pp.  0210\u20130215. IEEE, 2018.",
                "url": null
            }
        },
        {
            "62": {
                "title": "Techniques for interpretable machine learning.",
                "author": "Mengnan Du, Ninghao Liu, and Xia Hu.",
                "venue": "Communications of the ACM, 63(1):68\u201377, 2019a.",
                "url": null
            }
        },
        {
            "63": {
                "title": "Learning credible deep neural networks with rationale regularization.",
                "author": "Mengnan Du, Ninghao Liu, Fan Yang, and Xia Hu.",
                "venue": "In 2019 IEEE International Conference on Data Mining (ICDM), pp.  150\u2013159. IEEE, 2019b.",
                "url": null
            }
        },
        {
            "64": {
                "title": "Towards interpreting and mitigating shortcut learning behavior of nlu models.",
                "author": "Mengnan Du, Varun Manjunatha, Rajiv Jain, Ruchi Deshpande, Franck Dernoncourt, Jiuxiang Gu, Tong Sun, and Xia Hu.",
                "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2021.",
                "url": null
            }
        },
        {
            "65": {
                "title": "Do llms know about hallucination? an empirical investigation of llm\u2019s hidden states.",
                "author": "Hanyu Duan, Yi Yang, and Kar Yan Tam.",
                "venue": "arXiv preprint arXiv:2402.09733, 2024.",
                "url": null
            }
        },
        {
            "66": {
                "title": "Hotflip: White-box adversarial examples for text classification.",
                "author": "Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou.",
                "venue": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp.  31\u201336, 2018.",
                "url": null
            }
        },
        {
            "67": {
                "title": "A mathematical framework for transformer circuits.",
                "author": "Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et al.",
                "venue": "Transformer Circuits Thread, 1, 2021.",
                "url": null
            }
        },
        {
            "68": {
                "title": "Toy models of superposition.",
                "author": "Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, et al.",
                "venue": "arXiv preprint arXiv:2209.10652, 2022.",
                "url": null
            }
        },
        {
            "69": {
                "title": "Sequential integrated gradients: a simple but effective method for explaining language models.",
                "author": "Joseph Enguehard.",
                "venue": "arXiv preprint arXiv:2305.15853, 2023.",
                "url": null
            }
        },
        {
            "70": {
                "title": "Knowledge card: Filling llms\u2019 knowledge gaps with plug-in specialized language models.",
                "author": "Shangbin Feng, Weijia Shi, Yuyang Bai, Vidhisha Balachandran, Tianxing He, and Yulia Tsvetkov.",
                "venue": "In The Twelfth International Conference on Learning Representations, 2023.",
                "url": null
            }
        },
        {
            "71": {
                "title": "Bias and fairness in large language models: A survey.",
                "author": "Isabel O Gallegos, Ryan A Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen K Ahmed.",
                "venue": "arXiv preprint arXiv:2309.00770, 2023.",
                "url": null
            }
        },
        {
            "72": {
                "title": "Retrieval-augmented generation for large language models: A survey.",
                "author": "Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang.",
                "venue": "arXiv preprint arXiv:2312.10997, 2023.",
                "url": null
            }
        },
        {
            "73": {
                "title": "Shortcut learning in deep neural networks.",
                "author": "Robert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A Wichmann.",
                "venue": "Nature Machine Intelligence, 2(11):665\u2013673, 2020.",
                "url": null
            }
        },
        {
            "74": {
                "title": "Recommendation as language processing (rlp): A unified pretrain, personalized prompt & predict paradigm (p5).",
                "author": "Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang.",
                "venue": "In Proceedings of the 16th ACM Conference on Recommender Systems, pp.  299\u2013315, 2022.",
                "url": null
            }
        },
        {
            "75": {
                "title": "Transformer feed-forward layers are key-value memories.",
                "author": "Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy.",
                "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp.  5484\u20135495, 2021.",
                "url": null
            }
        },
        {
            "76": {
                "title": "Towards automatic concept-based explanations.",
                "author": "Amirata Ghorbani, James Wexler, James Y Zou, and Been Kim.",
                "venue": "Advances in neural information processing systems, 32, 2019.",
                "url": null
            }
        },
        {
            "77": {
                "title": "Chatgpt outperforms crowd-workers for text-annotation tasks.",
                "author": "Fabrizio Gilardi, Meysam Alizadeh, and Ma\u00ebl Kubli.",
                "venue": "arXiv preprint arXiv:2303.15056, 2023.",
                "url": null
            }
        },
        {
            "78": {
                "title": "Inductive biases for deep learning of higher-level cognition.",
                "author": "Anirudh Goyal and Yoshua Bengio.",
                "venue": "Proceedings of the Royal Society A, 478(2266):20210068, 2022.",
                "url": null
            }
        },
        {
            "79": {
                "title": "Studying large language model generalization with influence functions.",
                "author": "Roger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner, Dustin Li, Esin Durmus, Ethan Perez, et al.",
                "venue": "arXiv preprint arXiv:2308.03296, 2023.",
                "url": null
            }
        },
        {
            "80": {
                "title": "Fastif: Scalable influence functions for efficient model interpretation and debugging.",
                "author": "Han Guo, Nazneen Rajani, Peter Hase, Mohit Bansal, and Caiming Xiong.",
                "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp.  10333\u201310350, 2021.",
                "url": null
            }
        },
        {
            "81": {
                "title": "How do multimodal llms really fare in classical vision few-shot challenges? a deep dive.",
                "author": "Qing Guo, Prashan Wanigasekara, Skyler Zheng, Jacob Zhiyuan Fang, Xinwei Deng, and Chenyang Tao.",
                "venue": "2023.",
                "url": null
            }
        },
        {
            "82": {
                "title": "Rationalization for explainable nlp: A survey.",
                "author": "Sai Gurrapu, Ajay Kulkarni, Lifu Huang, Ismini Lourentzou, and Feras A Batarseh.",
                "venue": "Frontiers in Artificial Intelligence, 6, 2023.",
                "url": null
            }
        },
        {
            "83": {
                "title": "Retrieval augmented language model pre-training.",
                "author": "Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang.",
                "venue": "In International conference on machine learning, pp.  3929\u20133938. PMLR, 2020.",
                "url": null
            }
        },
        {
            "84": {
                "title": "Which explanation should i choose? a function approximation perspective to characterizing post hoc explanations.",
                "author": "Tessa Han, Suraj Srinivas, and Himabindu Lakkaraju.",
                "venue": "Advances in Neural Information Processing Systems, 35:5256\u20135268, 2022.",
                "url": null
            }
        },
        {
            "85": {
                "title": "Orca: Interpreting prompted language models via locating supporting data evidence in the ocean of pretraining data.",
                "author": "Xiaochuang Han and Yulia Tsvetkov.",
                "venue": "arXiv preprint arXiv:2205.12600, 2022.",
                "url": null
            }
        },
        {
            "86": {
                "title": "Explaining black box predictions and unveiling data artifacts through influence functions.",
                "author": "Xiaochuang Han, Byron C Wallace, and Yulia Tsvetkov.",
                "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp.  5553\u20135563, 2020.",
                "url": null
            }
        },
        {
            "87": {
                "title": "Improving sequential model editing with fact retrieval.",
                "author": "Xiaoqi Han, Ru Li, Hongye Tan, Wang Yuanlong, Qinghua Chai, and Jeff Pan.",
                "venue": "In Findings of the Association for Computational Linguistics: EMNLP 2023, pp.  11209\u201311224, 2023.",
                "url": null
            }
        },
        {
            "88": {
                "title": "Evaluating explainable ai: Which algorithmic explanations help users predict model behavior?",
                "author": "Peter Hase and Mohit Bansal.",
                "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp.  5540\u20135552, 2020.",
                "url": null
            }
        },
        {
            "89": {
                "title": "Does localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models.",
                "author": "Peter Hase, Mohit Bansal, Been Kim, and Asma Ghandeharioun.",
                "venue": "Advances in Neural Information Processing Systems, 36, 2024.",
                "url": null
            }
        },
        {
            "90": {
                "title": "Deep residual learning for image recognition.",
                "author": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.",
                "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.  770\u2013778, 2016.",
                "url": null
            }
        },
        {
            "91": {
                "title": "Translation-based recommendation.",
                "author": "Ruining He, Wang-Cheng Kang, and Julian McAuley.",
                "venue": "In Proceedings of the eleventh ACM conference on recommender systems, pp.  161\u2013169, 2017.",
                "url": null
            }
        },
        {
            "92": {
                "title": "Targeted data generation: Finding and fixing model weaknesses.",
                "author": "Zexue He, Marco Tulio Ribeiro, and Fereshte Khani.",
                "venue": "arXiv preprint arXiv:2305.17804, 2023.",
                "url": null
            }
        },
        {
            "93": {
                "title": "Support vector machines.",
                "author": "Marti A. Hearst, Susan T Dumais, Edgar Osuna, John Platt, and Bernhard Scholkopf.",
                "venue": "IEEE Intelligent Systems and their applications, 13(4):18\u201328, 1998.",
                "url": null
            }
        },
        {
            "94": {
                "title": "beta-vae: Learning basic visual concepts with a constrained variational framework.",
                "author": "Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner.",
                "venue": "In International conference on learning representations, 2016.",
                "url": null
            }
        },
        {
            "95": {
                "title": "Large language models are reasoning teachers.",
                "author": "Namgyu Ho, Laura Schmid, and Se-Young Yun.",
                "venue": "arXiv preprint arXiv:2212.10071, 2022.",
                "url": null
            }
        },
        {
            "96": {
                "title": "exbert: A visual analysis tool to explore learned representations in transformer models.",
                "author": "Benjamin Hoover, Hendrik Strobelt, and Sebastian Gehrmann.",
                "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp.  187\u2013196, 2020.",
                "url": null
            }
        },
        {
            "97": {
                "title": "Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes.",
                "author": "Cheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister.",
                "venue": "In Findings of the Association for Computational Linguistics: ACL 2023, pp.  8003\u20138017. Association for Computational Linguistics, July 2023.",
                "url": null
            }
        },
        {
            "98": {
                "title": "Chain of explanation: New prompting method to generate quality natural language explanation for implicit hate speech.",
                "author": "Fan Huang, Haewoon Kwak, and Jisun An.",
                "venue": "In Companion Proceedings of the ACM Web Conference 2023, pp.  90\u201393, 2023a.",
                "url": null
            }
        },
        {
            "99": {
                "title": "Is chatgpt better than human annotators? potential and limitations of chatgpt in explaining implicit hate speech.",
                "author": "Fan Huang, Haewoon Kwak, and Jisun An.",
                "venue": "In Companion Proceedings of the ACM Web Conference 2023, pp.  294\u2013297, 2023b.",
                "url": null
            }
        },
        {
            "100": {
                "title": "Large language models can self-improve.",
                "author": "Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han.",
                "venue": "arXiv preprint arXiv:2210.11610, 2022.",
                "url": null
            }
        },
        {
            "101": {
                "title": "A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions.",
                "author": "Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al.",
                "venue": "arXiv preprint arXiv:2311.05232, 2023c.",
                "url": null
            }
        },
        {
            "102": {
                "title": "Can large language models explain themselves? a study of llm-generated self-explanations.",
                "author": "Shiyuan Huang, Siddarth Mamidanna, Shreedhar Jangam, Yilun Zhou, and Leilani H Gilpin.",
                "venue": "arXiv preprint arXiv:2310.11207, 2023d.",
                "url": null
            }
        },
        {
            "103": {
                "title": "Leveraging passage retrieval with generative models for open domain question answering.",
                "author": "Gautier Izacard and Edouard Grave.",
                "venue": "arXiv preprint arXiv:2007.01282, 2020.",
                "url": null
            }
        },
        {
            "104": {
                "title": "Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks.",
                "author": "Samyak Jain, Robert Kirk, Ekdeep Singh Lubana, Robert P Dick, Hidenori Tanaka, Edward Grefenstette, Tim Rockt\u00e4schel, and David Scott Krueger.",
                "venue": "arXiv preprint arXiv:2311.12786, 2023.",
                "url": null
            }
        },
        {
            "105": {
                "title": "Hugging face.",
                "author": "Shashank Mohan Jain.",
                "venue": "In Introduction to Transformers for NLP: With the Hugging Face Library and Models to Solve Problems, pp.  51\u201367. Springer, 2022.",
                "url": null
            }
        },
        {
            "106": {
                "title": "What does bert learn about the structure of language?",
                "author": "Ganesh Jawahar, Beno\u00eet Sagot, and Djam\u00e9 Seddah.",
                "venue": "In ACL 2019-57th Annual Meeting of the Association for Computational Linguistics, 2019.",
                "url": null
            }
        },
        {
            "107": {
                "title": "Survey of hallucination in natural language generation.",
                "author": "Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung.",
                "venue": "ACM Computing Surveys, 55(12):1\u201338, 2023.",
                "url": null
            }
        },
        {
            "108": {
                "title": "Mistral 7b.",
                "author": "Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al.",
                "venue": "arXiv preprint arXiv:2310.06825, 2023.",
                "url": null
            }
        },
        {
            "109": {
                "title": "Gender biases and where to find them: Exploring gender bias in pre-trained transformer-based language models using movement pruning.",
                "author": "Przemyslaw Joniak and Akiko Aizawa.",
                "venue": "arXiv preprint arXiv:2207.02463, 2022.",
                "url": null
            }
        },
        {
            "110": {
                "title": "Large language models struggle to learn long-tail knowledge.",
                "author": "Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel.",
                "venue": "In International Conference on Machine Learning, pp.  15696\u201315707. PMLR, 2023.",
                "url": null
            }
        },
        {
            "111": {
                "title": "Dense passage retrieval for open-domain question answering.",
                "author": "Vladimir Karpukhin, Barlas O\u011fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.",
                "venue": "arXiv preprint arXiv:2004.04906, 2020.",
                "url": null
            }
        },
        {
            "112": {
                "title": "Looking beyond the surface: A challenge set for reading comprehension over multiple sentences.",
                "author": "Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth.",
                "venue": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp.  252\u2013262, 2018.",
                "url": null
            }
        },
        {
            "113": {
                "title": "Decomposed prompting: A modular approach for solving complex tasks.",
                "author": "Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal.",
                "venue": "In The Eleventh International Conference on Learning Representations, 2022.",
                "url": null
            }
        },
        {
            "114": {
                "title": "Sure: Improving open-domain question answering of llms via summarized retrieval.",
                "author": "Jaehyung Kim, Jaehyun Nam, Sangwoo Mo, Jongjin Park, Sang-Woo Lee, Minjoon Seo, Jung-Woo Ha, and Jinwoo Shin.",
                "venue": "In The Twelfth International Conference on Learning Representations, 2023a.",
                "url": null
            }
        },
        {
            "115": {
                "title": "The cot collection: Improving zero-shot and few-shot learning of language models via chain-of-thought fine-tuning.",
                "author": "Seungone Kim, Se June Joo, Doyoung Kim, Joel Jang, Seonghyeon Ye, Jamin Shin, and Minjoon Seo.",
                "venue": "arXiv preprint arXiv:2305.14045, 2023b.",
                "url": null
            }
        },
        {
            "116": {
                "title": "The (un) reliability of saliency methods.",
                "author": "Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T Sch\u00fctt, Sven D\u00e4hne, Dumitru Erhan, and Been Kim.",
                "venue": "In Explainable AI: Interpreting, Explaining and Visualizing Deep Learning, pp.  267\u2013280.",
                "url": null
            }
        },
        {
            "117": {
                "title": "Understanding black-box predictions via influence functions.",
                "author": "Pang Wei Koh and Percy Liang.",
                "venue": "In International conference on machine learning, pp.  1885\u20131894. PMLR, 2017.",
                "url": null
            }
        },
        {
            "118": {
                "title": "Concept bottleneck models.",
                "author": "Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and Percy Liang.",
                "venue": "In International conference on machine learning, pp.  5338\u20135348. PMLR, 2020.",
                "url": null
            }
        },
        {
            "119": {
                "title": "Bert meets shapley: Extending shap explanations to transformer-based classifiers.",
                "author": "Enja Kokalj, Bla\u017e \u0160krlj, Nada Lavra\u010d, Senja Pollak, and Marko Robnik-\u0160ikonja.",
                "venue": "In Proceedings of the EACL Hackashop on News Media Content Analysis and Automated Report Generation, pp.  16\u201321, 2021.",
                "url": null
            }
        },
        {
            "120": {
                "title": "Post hoc explanations of language models can improve language models.",
                "author": "Satyapriya Krishna, Jiaqi Ma, Dylan Slack, Asma Ghandeharioun, Sameer Singh, and Himabindu Lakkaraju.",
                "venue": "arXiv preprint arXiv:2305.11426, 2023.",
                "url": null
            }
        },
        {
            "121": {
                "title": "Are large language models post hoc explainers?",
                "author": "Nicholas Kroeger, Dan Ley, Satyapriya Krishna, Chirag Agarwal, and Himabindu Lakkaraju.",
                "venue": "arXiv preprint arXiv:2310.05797, 2023.",
                "url": null
            }
        },
        {
            "122": {
                "title": "Measuring faithfulness in chain-of-thought reasoning.",
                "author": "Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, et al.",
                "venue": "arXiv preprint arXiv:2307.13702, 2023.",
                "url": null
            }
        },
        {
            "123": {
                "title": "Fine-tuning chatgpt for automatic scoring.",
                "author": "Ehsan Latif and Xiaoming Zhai.",
                "venue": "Computers and Education: Artificial Intelligence, pp.  100210, 2024.",
                "url": null
            }
        },
        {
            "124": {
                "title": "A mechanistic understanding of alignment algorithms: A case study on dpo and toxicity.",
                "author": "Andrew Lee, Xiaoyan Bai, Itamar Pres, Martin Wattenberg, Jonathan K Kummerfeld, and Rada Mihalcea.",
                "venue": "arXiv preprint arXiv:2401.01967, 2024.",
                "url": null
            }
        },
        {
            "125": {
                "title": "Multimodality of ai for education: Towards artificial general intelligence.",
                "author": "Gyeong-Geon Lee, Lehong Shi, Ehsan Latif, Yizhu Gao, Arne Bewersdorf, Matthew Nyaaba, Shuchen Guo, Zihao Wu, Zhengliang Liu, Hui Wang, et al.",
                "venue": "arXiv preprint arXiv:2312.06037, 2023.",
                "url": null
            }
        },
        {
            "126": {
                "title": "Recexplainer: Aligning large language models for recommendation model interpretability.",
                "author": "Yuxuan Lei, Jianxun Lian, Jing Yao, Xu Huang, Defu Lian, and Xing Xie.",
                "venue": "arXiv preprint arXiv:2311.10947, 2023.",
                "url": null
            }
        },
        {
            "127": {
                "title": "Self-detoxifying language models via toxification reversal.",
                "author": "Chak Tou Leong, Yi Cheng, Jiashuo Wang, Jian Wang, and Wenjie Li.",
                "venue": "arXiv preprint arXiv:2310.09573, 2023.",
                "url": null
            }
        },
        {
            "128": {
                "title": "Retrieval-augmented generation for knowledge-intensive nlp tasks.",
                "author": "Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al.",
                "venue": "Advances in Neural Information Processing Systems, 33:9459\u20139474, 2020.",
                "url": null
            }
        },
        {
            "129": {
                "title": "Large language models with controllable working memory.",
                "author": "Daliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin Wang, Michal Lukasik, Andreas Veit, Felix Yu, and Sanjiv Kumar.",
                "venue": "arXiv preprint arXiv:2211.05110, 2022a.",
                "url": null
            }
        },
        {
            "130": {
                "title": "Multi-step jailbreaking privacy attacks on chatgpt.",
                "author": "Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, and Yangqiu Song.",
                "venue": "arXiv preprint arXiv:2304.05197, 2023a.",
                "url": null
            }
        },
        {
            "131": {
                "title": "Ucepic: Unifying aspect planning and lexical constraints for generating explanations in recommendation.",
                "author": "Jiacheng Li, Zhankui He, Jingbo Shang, and Julian McAuley.",
                "venue": "In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp.  1248\u20131257, 2023b.",
                "url": null
            }
        },
        {
            "132": {
                "title": "Visualizing and understanding neural models in nlp.",
                "author": "Jiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky.",
                "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp.  681\u2013691, 2016a.",
                "url": null
            }
        },
        {
            "133": {
                "title": "Understanding neural networks through representation erasure.",
                "author": "Jiwei Li, Will Monroe, and Dan Jurafsky.",
                "venue": "arXiv preprint arXiv:1612.08220, 2016b.",
                "url": null
            }
        },
        {
            "134": {
                "title": "Halueval: A large-scale hallucination evaluation benchmark for large language models.",
                "author": "Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen.",
                "venue": "In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp.  6449\u20136464, 2023c.",
                "url": null
            }
        },
        {
            "135": {
                "title": "The dawn after the dark: An empirical study on factuality hallucination in large language models.",
                "author": "Junyi Li, Jie Chen, Ruiyang Ren, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen.",
                "venue": "arXiv preprint arXiv:2401.03205, 2024a.",
                "url": null
            }
        },
        {
            "136": {
                "title": "Inference-time intervention: Eliciting truthful answers from a language model.",
                "author": "Kenneth Li, Oam Patel, Fernanda Vi\u00e9gas, Hanspeter Pfister, and Martin Wattenberg.",
                "venue": "Advances in Neural Information Processing Systems, 36, 2024b.",
                "url": null
            }
        },
        {
            "137": {
                "title": "Generate neural template explanations for recommendation.",
                "author": "Lei Li, Yongfeng Zhang, and Li Chen.",
                "venue": "In Proceedings of the 29th ACM International Conference on Information & Knowledge Management, pp.  755\u2013764, 2020.",
                "url": null
            }
        },
        {
            "138": {
                "title": "Personalized prompt learning for explainable recommendation.",
                "author": "Lei Li, Yongfeng Zhang, and Li Chen.",
                "venue": "ACM Transactions on Information Systems, 41(4):1\u201326, 2023d.",
                "url": null
            }
        },
        {
            "139": {
                "title": "Vqa-e: Explaining, elaborating, and enhancing your answers for visual questions.",
                "author": "Qing Li, Qingyi Tao, Shafiq Joty, Jianfei Cai, and Jiebo Luo.",
                "venue": "In Proceedings of the European Conference on Computer Vision (ECCV), pp.  552\u2013567, 2018.",
                "url": null
            }
        },
        {
            "140": {
                "title": "Explanations from large language models make small reasoners better.",
                "author": "Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian, Baolin Peng, Yi Mao, et al.",
                "venue": "arXiv preprint arXiv:2210.06726, 2022b.",
                "url": null
            }
        },
        {
            "141": {
                "title": "Open the pandora\u2019s box of llms: Jailbreaking llms through representation engineering.",
                "author": "Tianlong Li, Xiaoqing Zheng, and Xuanjing Huang.",
                "venue": "arXiv preprint arXiv:2401.06824, 2024c.",
                "url": null
            }
        },
        {
            "142": {
                "title": "Faithfulness in natural language generation: A systematic survey of analysis, evaluation and optimization methods.",
                "author": "Wei Li, Wenhao Wu, Moye Chen, Jiachen Liu, Xinyan Xiao, and Hua Wu.",
                "venue": "arXiv preprint arXiv:2203.05227, 2022c.",
                "url": null
            }
        },
        {
            "143": {
                "title": "Deepinception: Hypnotize large language model to be jailbreaker.",
                "author": "Xuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang Liu, and Bo Han.",
                "venue": "arXiv preprint arXiv:2311.03191, 2023e.",
                "url": null
            }
        },
        {
            "144": {
                "title": "A survey on fairness in large language models.",
                "author": "Yingji Li, Mengnan Du, Rui Song, Xin Wang, and Ying Wang.",
                "venue": "arXiv preprint arXiv:2308.10149, 2023f.",
                "url": null
            }
        },
        {
            "145": {
                "title": "Prompt tuning pushes farther, contrastive learning pulls closer: A two-stage approach to mitigate social biases.",
                "author": "Yingji Li, Mengnan Du, Xin Wang, and Ying Wang.",
                "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.  14254\u201314267, 2023g.",
                "url": null
            }
        },
        {
            "146": {
                "title": "Does circuit analysis interpretability scale? evidence from multiple choice capabilities in chinchilla.",
                "author": "Tom Lieberum, Matthew Rahtz, J\u00e1nos Kram\u00e1r, Geoffrey Irving, Rohin Shah, and Vladimir Mikulik.",
                "venue": "arXiv preprint arXiv:2307.09458, 2023.",
                "url": null
            }
        },
        {
            "147": {
                "title": "Anonymisation models for text data: State of the art, challenges and future directions.",
                "author": "Pierre Lison, Ildik\u00f3 Pil\u00e1n, David S\u00e1nchez, Montserrat Batet, and Lilja \u00d8vrelid.",
                "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp.  4188\u20134203, 2021.",
                "url": null
            }
        },
        {
            "148": {
                "title": "Lost in the middle: How language models use long contexts.",
                "author": "Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.",
                "venue": "Transactions of the Association for Computational Linguistics, 12:157\u2013173, 2024.",
                "url": null
            }
        },
        {
            "149": {
                "title": "Adversarial detection with model interpretation.",
                "author": "Ninghao Liu, Hongxia Yang, and Xia Hu.",
                "venue": "In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp.  1803\u20131811, 2018.",
                "url": null
            }
        },
        {
            "150": {
                "title": "Adversarial attacks and defenses: An interpretation perspective.",
                "author": "Ninghao Liu, Mengnan Du, Ruocheng Guo, Huan Liu, and Xia Hu.",
                "venue": "ACM SIGKDD Explorations Newsletter, 23(1):86\u201399, 2021.",
                "url": null
            }
        },
        {
            "151": {
                "title": "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing.",
                "author": "Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig.",
                "venue": "ACM Computing Surveys, 55(9):1\u201335, 2023a.",
                "url": null
            }
        },
        {
            "152": {
                "title": "Trustworthy llms: a survey and guideline for evaluating large language models\u2019 alignment.",
                "author": "Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, and Hang Li.",
                "venue": "arXiv preprint arXiv:2308.05374, 2023b.",
                "url": null
            }
        },
        {
            "153": {
                "title": "Jailbreaking chatgpt via prompt engineering: An empirical study.",
                "author": "Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida Zhao, Tianwei Zhang, and Yang Liu.",
                "venue": "arXiv preprint arXiv:2305.13860, 2023c.",
                "url": null
            }
        },
        {
            "154": {
                "title": "Controlllm: Augment language models with tools by searching on graphs.",
                "author": "Zhaoyang Liu, Zeqiang Lai, Zhangwei Gao, Erfei Cui, Zhiheng Li, Xizhou Zhu, Lewei Lu, Qifeng Chen, Yu Qiao, Jifeng Dai, et al.",
                "venue": "arXiv preprint arXiv:2310.17796, 2023d.",
                "url": null
            }
        },
        {
            "155": {
                "title": "Decoupled weight decay regularization.",
                "author": "Ilya Loshchilov and Frank Hutter.",
                "venue": "In International Conference on Learning Representations, 2018.",
                "url": null
            }
        },
        {
            "156": {
                "title": "Intelligible models for classification and regression.",
                "author": "Yin Lou, Rich Caruana, and Johannes Gehrke.",
                "venue": "In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, pp.  150\u2013158, 2012.",
                "url": null
            }
        },
        {
            "157": {
                "title": "Learn to explain: Multimodal reasoning via thought chains for science question answering.",
                "author": "Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan.",
                "venue": "Advances in Neural Information Processing Systems, 35:2507\u20132521, 2022.",
                "url": null
            }
        },
        {
            "158": {
                "title": "A unified approach to interpreting model predictions.",
                "author": "Scott M Lundberg and Su-In Lee.",
                "venue": "Advances in neural information processing systems, 30, 2017.",
                "url": null
            }
        },
        {
            "159": {
                "title": "From understanding to utilization: A survey on explainability for large language models.",
                "author": "Haoyan Luo and Lucia Specia.",
                "venue": "arXiv preprint arXiv:2401.12874, 2024.",
                "url": null
            }
        },
        {
            "160": {
                "title": "Xal: Explainable active learning makes classifiers better low-resource learners.",
                "author": "Yun Luo, Zhen Yang, Fandong Meng, Yingjie Li, Fang Guo, Qinglin Qi, Jie Zhou, and Yue Zhang.",
                "venue": "arXiv preprint arXiv:2310.05502, 2023.",
                "url": null
            }
        },
        {
            "161": {
                "title": "Faithful chain-of-thought reasoning.",
                "author": "Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris Callison-Burch.",
                "venue": "arXiv preprint arXiv:2301.13379, 2023.",
                "url": null
            }
        },
        {
            "162": {
                "title": "Deciphering stereotypes in pre-trained language models.",
                "author": "Weicheng Ma, Henry Scheible, Brian Wang, Goutham Veeramachaneni, Pratim Chowdhary, Alan Sun, Andrew Koulogeorge, Lili Wang, Diyi Yang, and Soroush Vosoughi.",
                "venue": "In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp.  11328\u201311345, 2023.",
                "url": null
            }
        },
        {
            "163": {
                "title": "Teaching small language models to reason.",
                "author": "Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn.",
                "venue": "arXiv preprint arXiv:2212.08410, 2022.",
                "url": null
            }
        },
        {
            "164": {
                "title": "Why is the current xai not meeting the expectations?",
                "author": "Alessio Malizia and Fabio Patern\u00f2.",
                "venue": "Communications of the ACM, 66(12):20\u201323, 2023.",
                "url": null
            }
        },
        {
            "165": {
                "title": "When not to trust language models: Investigating effectiveness of parametric and non-parametric memories.",
                "author": "Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi.",
                "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.  9802\u20139822, 2023.",
                "url": null
            }
        },
        {
            "166": {
                "title": "The geometry of truth: Emergent linear structure in large language model representations of true/false datasets.",
                "author": "Samuel Marks and Max Tegmark.",
                "venue": "arXiv preprint arXiv:2310.06824, 2023.",
                "url": null
            }
        },
        {
            "167": {
                "title": "Locating and editing factual associations in gpt.",
                "author": "Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov.",
                "venue": "Advances in Neural Information Processing Systems, 35:17359\u201317372, 2022a.",
                "url": null
            }
        },
        {
            "168": {
                "title": "Mass-editing memory in a transformer.",
                "author": "Kevin Meng, Arnab Sen Sharma, Alex J Andonian, Yonatan Belinkov, and David Bau.",
                "venue": "In The Eleventh International Conference on Learning Representations, 2022b.",
                "url": null
            }
        },
        {
            "169": {
                "title": "Visual classification via description from large language models.",
                "author": "Sachit Menon and Carl Vondrick.",
                "venue": "In The Eleventh International Conference on Learning Representations, 2022.",
                "url": null
            }
        },
        {
            "170": {
                "title": "Selfcheck: Using llms to zero-shot check their own step-by-step reasoning.",
                "author": "Ning Miao, Yee Whye Teh, and Tom Rainforth.",
                "venue": "arXiv preprint arXiv:2308.00436, 2023.",
                "url": null
            }
        },
        {
            "171": {
                "title": "Can llms keep a secret? testing privacy implications of language models via contextual integrity theory.",
                "author": "Niloofar Mireshghallah, Hyunwoo Kim, Xuhui Zhou, Yulia Tsvetkov, Maarten Sap, Reza Shokri, and Yejin Choi.",
                "venue": "In The Twelfth International Conference on Learning Representations, 2023.",
                "url": null
            }
        },
        {
            "172": {
                "title": "Exploring the role of bert token representations to explain sentence probing results.",
                "author": "Hosein Mohebbi, Ali Modarressi, and Mohammad Taher Pilehvar.",
                "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp.  792\u2013806, 2021.",
                "url": null
            }
        },
        {
            "173": {
                "title": "Explaining nonlinear classification decisions with deep taylor decomposition.",
                "author": "Gr\u00e9goire Montavon, Sebastian Lapuschkin, Alexander Binder, Wojciech Samek, and Klaus-Robert M\u00fcller.",
                "venue": "Pattern recognition, 65:211\u2013222, 2017.",
                "url": null
            }
        },
        {
            "174": {
                "title": "Layer-wise relevance propagation: an overview.",
                "author": "Gr\u00e9goire Montavon, Alexander Binder, Sebastian Lapuschkin, Wojciech Samek, and Klaus-Robert M\u00fcller.",
                "venue": "Explainable AI: interpreting, explaining and visualizing deep learning, pp.  193\u2013209, 2019.",
                "url": null
            }
        },
        {
            "175": {
                "title": "Definitions, methods, and applications in interpretable machine learning.",
                "author": "W James Murdoch, Chandan Singh, Karl Kumbier, Reza Abbasi-Asl, and Bin Yu.",
                "venue": "Proceedings of the National Academy of Sciences, 116(44):22071\u201322080, 2019.",
                "url": null
            }
        },
        {
            "176": {
                "title": "Webgpt: Browser-assisted question-answering with human feedback.",
                "author": "Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al.",
                "venue": "arXiv preprint arXiv:2112.09332, 2021.",
                "url": null
            }
        },
        {
            "177": {
                "title": "Scalable extraction of training data from (production) language models.",
                "author": "Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A Feder Cooper, Daphne Ippolito, Christopher A Choquette-Choo, Eric Wallace, Florian Tram\u00e8r, and Katherine Lee.",
                "venue": "arXiv preprint arXiv:2311.17035, 2023.",
                "url": null
            }
        },
        {
            "178": {
                "title": "In-context learning and induction heads.",
                "author": "Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al.",
                "venue": "arXiv preprint arXiv:2209.11895, 2022.",
                "url": null
            }
        },
        {
            "179": {
                "title": "Gpt-4 technical report.",
                "author": "R OpenAI.",
                "venue": "arXiv, pp.  2303\u201308774, 2023.",
                "url": null
            }
        },
        {
            "180": {
                "title": "Training language models to follow instructions with human feedback.",
                "author": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al.",
                "venue": "Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.",
                "url": null
            }
        },
        {
            "181": {
                "title": "Language models as knowledge bases?",
                "author": "Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller.",
                "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp.  2463\u20132473, 2019.",
                "url": null
            }
        },
        {
            "182": {
                "title": "How context affects language models\u2019 factual predictions.",
                "author": "Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rockt\u00e4schel, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel.",
                "venue": "arXiv preprint arXiv:2005.04611, 2020.",
                "url": null
            }
        },
        {
            "183": {
                "title": "Estimating training data influence by tracing gradient descent.",
                "author": "Garima Pruthi, Frederick Liu, Satyen Kale, and Mukund Sundararajan.",
                "venue": "Advances in Neural Information Processing Systems, 33:19920\u201319930, 2020.",
                "url": null
            }
        },
        {
            "184": {
                "title": "Is chatgpt a general-purpose natural language processing task solver?",
                "author": "Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang.",
                "venue": "arXiv preprint arXiv:2302.06476, 2023.",
                "url": null
            }
        },
        {
            "185": {
                "title": "Language models are unsupervised multitask learners.",
                "author": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.",
                "venue": "OpenAI blog, 1(8):9, 2019.",
                "url": null
            }
        },
        {
            "186": {
                "title": "Learning transferable visual models from natural language supervision.",
                "author": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.",
                "venue": "In International conference on machine learning, pp.  8748\u20138763. PMLR, 2021.",
                "url": null
            }
        },
        {
            "187": {
                "title": "Exploring the limits of transfer learning with a unified text-to-text transformer.",
                "author": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu.",
                "venue": "The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.",
                "url": null
            }
        },
        {
            "188": {
                "title": "Explain yourself! leveraging language models for commonsense reasoning.",
                "author": "Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher.",
                "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp.  4932\u20134942, 2019.",
                "url": null
            }
        },
        {
            "189": {
                "title": "From prejudice to parity: A new approach to debiasing large language model word embeddings.",
                "author": "Aishik Rakshit, Smriti Singh, Shuvam Keshari, Arijit Ghosh Chowdhury, Vinija Jain, and Aman Chadha.",
                "venue": "arXiv preprint arXiv:2402.11512, 2024.",
                "url": null
            }
        },
        {
            "190": {
                "title": "Investigating the factual knowledge boundary of large language models with retrieval augmentation.",
                "author": "Ruiyang Ren, Yuhao Wang, Yingqi Qu, Wayne Xin Zhao, Jing Liu, Hao Tian, Hua Wu, Ji-Rong Wen, and Haifeng Wang.",
                "venue": "arXiv preprint arXiv:2307.11019, 2023.",
                "url": null
            }
        },
        {
            "191": {
                "title": "\" why should i trust you?\" explaining the predictions of any classifier.",
                "author": "Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.",
                "venue": "In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pp.  1135\u20131144, 2016.",
                "url": null
            }
        },
        {
            "192": {
                "title": "The probabilistic relevance framework: Bm25 and beyond.",
                "author": "Stephen Robertson, Hugo Zaragoza, et al.",
                "venue": "Foundations and Trends\u00ae in Information Retrieval, 3(4):333\u2013389, 2009.",
                "url": null
            }
        },
        {
            "193": {
                "title": "A primer in bertology: What we know about how bert works.",
                "author": "Anna Rogers, Olga Kovaleva, and Anna Rumshisky.",
                "venue": "Transactions of the Association for Computational Linguistics, 8:842\u2013866, 2021.",
                "url": null
            }
        },
        {
            "194": {
                "title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead.",
                "author": "Cynthia Rudin.",
                "venue": "Nature machine intelligence, 1(5):206\u2013215, 2019.",
                "url": null
            }
        },
        {
            "195": {
                "title": "Interpretable machine learning: Fundamental principles and 10 grand challenges.",
                "author": "Cynthia Rudin, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova, and Chudi Zhong.",
                "venue": "Statistic Surveys, 16:1\u201385, 2022.",
                "url": null
            }
        },
        {
            "196": {
                "title": "Catfood: Counterfactual augmented training for improving out-of-domain performance and calibration.",
                "author": "Rachneet Sachdeva, Martin Tutek, and Iryna Gurevych.",
                "venue": "arXiv preprint arXiv:2309.07822, 2023.",
                "url": null
            }
        },
        {
            "197": {
                "title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter.",
                "author": "Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.",
                "venue": "arXiv preprint arXiv:1910.01108, 2019.",
                "url": null
            }
        },
        {
            "198": {
                "title": "Movement pruning: Adaptive sparsity by fine-tuning.",
                "author": "Victor Sanh, Thomas Wolf, and Alexander Rush.",
                "venue": "Advances in Neural Information Processing Systems, 33:20378\u201320389, 2020.",
                "url": null
            }
        },
        {
            "199": {
                "title": "Discretized integrated gradients for explaining language models.",
                "author": "Soumya Sanyal and Xiang Ren.",
                "venue": "arXiv preprint arXiv:2108.13654, 2021.",
                "url": null
            }
        },
        {
            "200": {
                "title": "Polysemanticity and capacity in neural networks.",
                "author": "Adam Scherlis, Kshitij Sachan, Adam S Jermyn, Joe Benton, and Buck Shlegeris.",
                "venue": "arXiv preprint arXiv:2210.01892, 2022.",
                "url": null
            }
        },
        {
            "201": {
                "title": "Scaling up influence functions.",
                "author": "Andrea Schioppa, Polina Zablotskaia, David Vilar, and Artem Sokolov.",
                "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp.  8179\u20138186, 2022.",
                "url": null
            }
        },
        {
            "202": {
                "title": "Bridging the human-ai knowledge gap: Concept discovery and transfer in alphazero.",
                "author": "Lisa Schut, Nenad Tomasev, Tom McGrath, Demis Hassabis, Ulrich Paquet, and Been Kim.",
                "venue": "arXiv preprint arXiv:2310.16410, 2023.",
                "url": null
            }
        },
        {
            "203": {
                "title": "Find: A function description benchmark for evaluating interpretability methods.",
                "author": "Sarah Schwettmann, Tamar Shaham, Joanna Materzynska, Neil Chowdhury, Shuang Li, Jacob Andreas, David Bau, and Antonio Torralba.",
                "venue": "Advances in Neural Information Processing Systems, 36, 2024.",
                "url": null
            }
        },
        {
            "204": {
                "title": "Grad-cam: Why did you say that?",
                "author": "Ramprasaath R Selvaraju, Abhishek Das, Ramakrishna Vedantam, Michael Cogswell, Devi Parikh, and Dhruv Batra.",
                "venue": "arXiv preprint arXiv:1611.07450, 2016.",
                "url": null
            }
        },
        {
            "205": {
                "title": "Taking features out of superposition with sparse autoencoders.",
                "author": "Lee Sharkey, Dan Braun, and Beren Millidge.",
                "venue": "In AI Alignment Forum, 2022.",
                "url": null
            }
        },
        {
            "206": {
                "title": "Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face.",
                "author": "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang.",
                "venue": "Advances in Neural Information Processing Systems, 36, 2024.",
                "url": null
            }
        },
        {
            "207": {
                "title": "Mededit: Model editing for medical question answering with external knowledge bases.",
                "author": "Yucheng Shi, Shaochen Xu, Zhengliang Liu, Tianming Liu, Xiang Li, and Ninghao Liu.",
                "venue": "arXiv preprint arXiv:2309.16035, 2023.",
                "url": null
            }
        },
        {
            "208": {
                "title": "Learning important features through propagating activation differences.",
                "author": "Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje.",
                "venue": "In International conference on machine learning, pp.  3145\u20133153. PMLR, 2017.",
                "url": null
            }
        },
        {
            "209": {
                "title": "Retrieval augmentation reduces hallucination in conversation.",
                "author": "Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston.",
                "venue": "arXiv preprint arXiv:2104.07567, 2021.",
                "url": null
            }
        },
        {
            "210": {
                "title": "Integrated directional gradients: Feature interaction attribution for neural nlp models.",
                "author": "Sandipan Sikdar, Parantapa Bhattacharya, and Kieran Heese.",
                "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp.  865\u2013878, 2021.",
                "url": null
            }
        },
        {
            "211": {
                "title": "Augmenting interpretable models with large language models during training.",
                "author": "Chandan Singh, Armin Askari, Rich Caruana, and Jianfeng Gao.",
                "venue": "Nature Communications, 14(1):7913, 2023.",
                "url": null
            }
        },
        {
            "212": {
                "title": "Improving the domain adaptation of retrieval augmented generation (rag) models for open domain question answering.",
                "author": "Shamane Siriwardhana, Rivindu Weerasekera, Elliott Wen, Tharindu Kaluarachchi, Rajib Rana, and Suranga Nanayakkara.",
                "venue": "Transactions of the Association for Computational Linguistics, 11:1\u201317, 2023.",
                "url": null
            }
        },
        {
            "213": {
                "title": "The empirical case for two systems of reasoning.",
                "author": "Steven A Sloman.",
                "venue": "Psychological bulletin, 119(1):3, 1996.",
                "url": null
            }
        },
        {
            "214": {
                "title": "Decision tree methods: applications for classification and prediction.",
                "author": "Yan-Yan Song and LU Ying.",
                "venue": "Shanghai archives of psychiatry, 27(2):130, 2015.",
                "url": null
            }
        },
        {
            "215": {
                "title": "Supervising model attention with human explanations for robust natural language inference.",
                "author": "Joe Stacey, Yonatan Belinkov, and Marek Rei.",
                "venue": "In Proceedings of the AAAI conference on artificial intelligence, volume 36, pp.  11349\u201311357, 2022.",
                "url": null
            }
        },
        {
            "216": {
                "title": "Who is rational?: Studies of individual differences in reasoning.",
                "author": "Keith E Stanovich.",
                "venue": "Psychology Press, 1999.",
                "url": null
            }
        },
        {
            "217": {
                "title": "Api is enough: Conformal prediction for large language models without logit-access.",
                "author": "Jiayuan Su, Jing Luo, Hongwei Wang, and Lu Cheng.",
                "venue": "arXiv preprint arXiv:2403.01216, 2024.",
                "url": null
            }
        },
        {
            "218": {
                "title": "End-to-end memory networks.",
                "author": "Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.",
                "venue": "Advances in neural information processing systems, 28, 2015.",
                "url": null
            }
        },
        {
            "219": {
                "title": "Axiomatic attribution for deep networks.",
                "author": "Mukund Sundararajan, Ankur Taly, and Qiqi Yan.",
                "venue": "In International conference on machine learning, pp.  3319\u20133328. PMLR, 2017.",
                "url": null
            }
        },
        {
            "220": {
                "title": "Can chatgpt replace traditional kbqa models? an in-depth analysis of the question answering performance of the gpt llm family.",
                "author": "Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu, Yongrui Chen, and Guilin Qi.",
                "venue": "In International Semantic Web Conference, pp.  348\u2013367. Springer, 2023.",
                "url": null
            }
        },
        {
            "221": {
                "title": "Does synthetic data generation of llms help clinical text mining?",
                "author": "Ruixiang Tang, Xiaotian Han, Xiaoqian Jiang, and Xia Hu.",
                "venue": "arXiv preprint arXiv:2303.04360, 2023a.",
                "url": null
            }
        },
        {
            "222": {
                "title": "Explain-then-translate: an analysis on improving program translation with self-generated explanations.",
                "author": "Zilu Tang, Mayank Agarwal, Alex Shypula, Bailin Wang, Derry Wijaya, Jie Chen, and Yoon Kim.",
                "venue": "arXiv preprint arXiv:2311.07070, 2023b.",
                "url": null
            }
        },
        {
            "223": {
                "title": "What do you learn from context? probing for sentence structure in contextualized word representations.",
                "author": "Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R Bowman, Dipanjan Das, et al.",
                "venue": "In International Conference on Learning Representations, 2018.",
                "url": null
            }
        },
        {
            "224": {
                "title": "Language models get a gender makeover: Mitigating gender bias with few-shot data interventions.",
                "author": "Himanshu Thakur, Atishay Jain, Praneetha Vaddamanu, Paul Pu Liang, and Louis-Philippe Morency.",
                "venue": "arXiv preprint arXiv:2306.04597, 2023.",
                "url": null
            }
        },
        {
            "225": {
                "title": "A survey on explainable artificial intelligence (xai): Toward medical xai.",
                "author": "Erico Tjoa and Cuntai Guan.",
                "venue": "IEEE transactions on neural networks and learning systems, 32(11):4793\u20134813, 2020.",
                "url": null
            }
        },
        {
            "226": {
                "title": "Function vectors in large language models.",
                "author": "Eric Todd, Millicent L Li, Arnab Sen Sharma, Aaron Mueller, Byron C Wallace, and David Bau.",
                "venue": "arXiv preprint arXiv:2310.15213, 2023.",
                "url": null
            }
        },
        {
            "227": {
                "title": "Llama: Open and efficient foundation language models.",
                "author": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al.",
                "venue": "arXiv preprint arXiv:2302.13971, 2023a.",
                "url": null
            }
        },
        {
            "228": {
                "title": "Llama 2: Open foundation and fine-tuned chat models.",
                "author": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.",
                "venue": "arXiv preprint arXiv:2307.09288, 2023b.",
                "url": null
            }
        },
        {
            "229": {
                "title": "Language models don\u2019t always say what they think: unfaithful explanations in chain-of-thought prompting.",
                "author": "Miles Turpin, Julian Michael, Ethan Perez, and Samuel Bowman.",
                "venue": "Advances in Neural Information Processing Systems, 36, 2024.",
                "url": null
            }
        },
        {
            "230": {
                "title": "A stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation.",
                "author": "Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu.",
                "venue": "arXiv preprint arXiv:2307.03987, 2023.",
                "url": null
            }
        },
        {
            "231": {
                "title": "Bertviz: A tool for visualizing multihead self-attention in the bert model.",
                "author": "Jesse Vig.",
                "venue": "In ICLR workshop: Debugging machine learning models, volume 23, 2019.",
                "url": null
            }
        },
        {
            "232": {
                "title": "Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned.",
                "author": "Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov.",
                "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp.  5797\u20135808, 2019.",
                "url": null
            }
        },
        {
            "233": {
                "title": "Analyzing the source and target contributions to predictions in neural machine translation.",
                "author": "Elena Voita, Rico Sennrich, and Ivan Titov.",
                "venue": "arXiv preprint arXiv:2010.10907, 2020.",
                "url": null
            }
        },
        {
            "234": {
                "title": "Counterfactual explanations without opening the black box: Automated decisions and the gdpr.",
                "author": "Sandra Wachter, Brent Mittelstadt, and Chris Russell.",
                "venue": "Harv. JL & Tech., 31:841, 2017.",
                "url": null
            }
        },
        {
            "235": {
                "title": "Fact or fiction: Verifying scientific claims.",
                "author": "David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi.",
                "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp.  7534\u20137550, 2020.",
                "url": null
            }
        },
        {
            "236": {
                "title": "Dan is my new friend.",
                "author": "walkerspider.",
                "venue": "https://www.reddit.com/r/ChatGPT/comments/zlcyr9/dan_is_my_new_friend/, 2022.",
                "url": null
            }
        },
        {
            "237": {
                "title": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.",
                "author": "Ben Wang and Aran Komatsuzaki.",
                "venue": "https://github.com/kingoflolz/mesh-transformer-jax, May 2021.",
                "url": null
            }
        },
        {
            "238": {
                "title": "Robust natural language understanding with residual attention debiasing.",
                "author": "Fei Wang, James Y Huang, Tianyi Yan, Wenxuan Zhou, and Muhao Chen.",
                "venue": "arXiv preprint arXiv:2305.17627, 2023a.",
                "url": null
            }
        },
        {
            "239": {
                "title": "Interpretability in the wild: a circuit for indirect object identification in gpt-2 small.",
                "author": "Kevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt.",
                "venue": "In The Eleventh International Conference on Learning Representations, 2022a.",
                "url": null
            }
        },
        {
            "240": {
                "title": "Label words are anchors: An information flow perspective for understanding in-context learning.",
                "author": "Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun.",
                "venue": "arXiv preprint arXiv:2305.14160, 2023b.",
                "url": null
            }
        },
        {
            "241": {
                "title": "Reducing spurious correlations in aspect-based sentiment analysis with explanation from large language models.",
                "author": "Qianlong Wang, Keyang Ding, Bin Liang, Min Yang, and Ruifeng Xu.",
                "venue": "In Findings of the Association for Computational Linguistics: EMNLP 2023, pp.  2930\u20132941, 2023c.",
                "url": null
            }
        },
        {
            "242": {
                "title": "Knowledge editing for large language models: A survey.",
                "author": "Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, et al.",
                "venue": "arXiv preprint arXiv:2310.16218, 2023d.",
                "url": null
            }
        },
        {
            "243": {
                "title": "Retrieval-augmented multilingual knowledge editing.",
                "author": "Weixuan Wang, Barry Haddow, and Alexandra Birch.",
                "venue": "arXiv preprint arXiv:2312.13040, 2023e.",
                "url": null
            }
        },
        {
            "244": {
                "title": "Finding skill neurons in pre-trained transformer-based language models.",
                "author": "Xiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou, Zhiyuan Liu, and Juanzi Li.",
                "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp.  11132\u201311152, 2022b.",
                "url": null
            }
        },
        {
            "245": {
                "title": "Chain-of-thought reasoning without prompting.",
                "author": "Xuezhi Wang and Denny Zhou.",
                "venue": "arXiv preprint arXiv:2402.10200, 2024a.",
                "url": null
            }
        },
        {
            "246": {
                "title": "Chain-of-thought reasoning without prompting, 2024b.",
                "author": "Xuezhi Wang and Denny Zhou.",
                "venue": null,
                "url": null
            }
        },
        {
            "247": {
                "title": "Robustness to spurious correlations in text classification via automatically generated counterfactuals.",
                "author": "Zhao Wang and Aron Culotta.",
                "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp.  14024\u201314031, 2021.",
                "url": null
            }
        },
        {
            "248": {
                "title": "Describe, explain, plan and select: interactive planning with llms enables open-world multi-task agents.",
                "author": "Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Shawn Ma, and Yitao Liang.",
                "venue": "Advances in Neural Information Processing Systems, 36, 2024.",
                "url": null
            }
        },
        {
            "249": {
                "title": "Generating valid and natural adversarial examples with large language models.",
                "author": "Zimu Wang, Wei Wang, Qi Chen, Qiufeng Wang, and Anh Nguyen.",
                "venue": "arXiv preprint arXiv:2311.11861, 2023f.",
                "url": null
            }
        },
        {
            "250": {
                "title": "Learning from explanations with neural execution tree.",
                "author": "Ziqi Wang, Yujia Qin, Wenxuan Zhou, Jun Yan, Qinyuan Ye, Leonardo Neves, Zhiyuan Liu, and Xiang Ren.",
                "venue": "In International Conference on Learning Representations, 2019.",
                "url": null
            }
        },
        {
            "251": {
                "title": "Jailbroken: How does llm safety training fail?",
                "author": "Alexander Wei, Nika Haghtalab, and Jacob Steinhardt.",
                "venue": "arXiv preprint arXiv:2307.02483, 2023.",
                "url": null
            }
        },
        {
            "252": {
                "title": "Chain-of-thought prompting elicits reasoning in large language models.",
                "author": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.",
                "venue": "Advances in Neural Information Processing Systems, 35:24824\u201324837, 2022.",
                "url": null
            }
        },
        {
            "253": {
                "title": "Naturalprover: Grounded mathematical proof generation with language models.",
                "author": "Sean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi, and Yejin Choi.",
                "venue": "Advances in Neural Information Processing Systems, 35:4913\u20134927, 2022.",
                "url": null
            }
        },
        {
            "254": {
                "title": "Autocad: Automatically generating counterfactuals for mitigating shortcut learning.",
                "author": "Jiaxin Wen, Yeshuang Zhu, Jinchao Zhang, Jie Zhou, and Minlie Huang.",
                "venue": "arXiv preprint arXiv:2211.16202, 2022.",
                "url": null
            }
        },
        {
            "255": {
                "title": "Llm-powered data augmentation for enhanced crosslingual performance.",
                "author": "Chenxi Whitehouse, Monojit Choudhury, and Alham Fikri Aji.",
                "venue": "arXiv preprint arXiv:2305.14288, 2023.",
                "url": null
            }
        },
        {
            "256": {
                "title": "Improving vqa and its explanationsby comparing competing explanations.",
                "author": "Jialin Wu, Liyan Chen, and Raymond J Mooney.",
                "venue": "arXiv preprint arXiv:2006.15631, 2020a.",
                "url": null
            }
        },
        {
            "257": {
                "title": "From language modeling to instruction following: Understanding the behavior shift in llms after instruction tuning, 2023.",
                "author": "Xuansheng Wu, Wenlin Yao, Jianshu Chen, Xiaoman Pan, Xiaoyang Wang, Ninghao Liu, and Dong Yu.",
                "venue": null,
                "url": null
            }
        },
        {
            "258": {
                "title": "On explaining your explanations of bert: An empirical study with sequence classification.",
                "author": "Zhengxuan Wu and Desmond C Ong.",
                "venue": "arXiv preprint arXiv:2101.00196, 2021.",
                "url": null
            }
        },
        {
            "259": {
                "title": "Perturbed masking: Parameter-free probing for analyzing and interpreting bert.",
                "author": "Zhiyong Wu, Yun Chen, Ben Kao, and Qun Liu.",
                "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp.  4166\u20134176, 2020b.",
                "url": null
            }
        },
        {
            "260": {
                "title": "Less: Selecting influential data for targeted instruction tuning.",
                "author": "Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen.",
                "venue": "arXiv preprint arXiv:2402.04333, 2024.",
                "url": null
            }
        },
        {
            "261": {
                "title": "Contrastive novelty-augmented learning: Anticipating outliers with large language models.",
                "author": "Albert Xu, Xiang Ren, and Robin Jia.",
                "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.  11778\u201311801, 2023a.",
                "url": null
            }
        },
        {
            "262": {
                "title": "Adversarial attacks and defenses in images, graphs and text: A review.",
                "author": "Han Xu, Yao Ma, Hao-Chen Liu, Debayan Deb, Hui Liu, Ji-Liang Tang, and Anil K Jain.",
                "venue": "International Journal of Automation and Computing, 17:151\u2013178, 2020.",
                "url": null
            }
        },
        {
            "263": {
                "title": "Retrieval meets long context large language models.",
                "author": "Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro.",
                "venue": "arXiv preprint arXiv:2310.03025, 2023b.",
                "url": null
            }
        },
        {
            "264": {
                "title": "Understanding and detecting hallucinations in neural machine translation via model introspection.",
                "author": "Weijia Xu, Sweta Agrawal, Eleftheria Briakou, Marianna J Martindale, and Marine Carpuat.",
                "venue": "Transactions of the Association for Computational Linguistics, 11:546\u2013564, 2023c.",
                "url": null
            }
        },
        {
            "265": {
                "title": "Hallucination is inevitable: An innate limitation of large language models.",
                "author": "Ziwei Xu, Sanjay Jain, and Mohan Kankanhalli.",
                "venue": "arXiv preprint arXiv:2401.11817, 2024.",
                "url": null
            }
        },
        {
            "266": {
                "title": "Evaluating explanation without ground truth in interpretable machine learning.",
                "author": "Fan Yang, Mengnan Du, and Xia Hu.",
                "venue": "arXiv preprint arXiv:1907.06831, 2019.",
                "url": null
            }
        },
        {
            "267": {
                "title": "Bias a-head? analyzing bias in transformer-based language model attention heads.",
                "author": "Yi Yang, Hanyu Duan, Ahmed Abbasi, John P Lalor, and Kar Yan Tam.",
                "venue": "arXiv preprint arXiv:2311.10395, 2023a.",
                "url": null
            }
        },
        {
            "268": {
                "title": "Language in a bottle: Language model guided concept bottlenecks for interpretable image classification.",
                "author": "Yue Yang, Artemis Panagopoulou, Shenghao Zhou, Daniel Jin, Chris Callison-Burch, and Mark Yatskar.",
                "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.  19187\u201319197, 2023b.",
                "url": null
            }
        },
        {
            "269": {
                "title": "Large language model can interpret latent space of sequential recommender.",
                "author": "Zhengyi Yang, Jiancan Wu, Yanchen Luo, Jizhi Zhang, Yancheng Yuan, An Zhang, Xiang Wang, and Xiangnan He.",
                "venue": "arXiv preprint arXiv:2310.20487, 2023c.",
                "url": null
            }
        },
        {
            "270": {
                "title": "Beyond labels: Empowering human annotators with natural language explanations through a novel active-learning architecture.",
                "author": "Bingsheng Yao, Ishan Jindal, Lucian Popa, Yannis Katsis, Sayan Ghosh, Lihong He, Yuxuan Lu, Shashank Srivastava, Yunyao Li, James Hendler, et al.",
                "venue": "In Findings of the Association for Computational Linguistics: EMNLP 2023, pp.  11629\u201311643, 2023a.",
                "url": null
            }
        },
        {
            "271": {
                "title": "Tree of thoughts: Deliberate problem solving with large language models.",
                "author": "Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan.",
                "venue": "arXiv preprint arXiv:2305.10601, 2023b.",
                "url": null
            }
        },
        {
            "272": {
                "title": "Can explanations be useful for calibrating black box models?",
                "author": "Xi Ye and Greg Durrett.",
                "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.  6199\u20136212, 2022a.",
                "url": null
            }
        },
        {
            "273": {
                "title": "The unreliability of explanations in few-shot prompting for textual reasoning.",
                "author": "Xi Ye and Greg Durrett.",
                "venue": "Advances in neural information processing systems, 35:30378\u201330392, 2022b.",
                "url": null
            }
        },
        {
            "274": {
                "title": "Benchmarking knowledge boundary for large language model: A different perspective on model evaluation.",
                "author": "Xunjian Yin, Xu Zhang, Jie Ruan, and Xiaojun Wan.",
                "venue": "arXiv preprint arXiv:2402.11493, 2024a.",
                "url": null
            }
        },
        {
            "275": {
                "title": "Should we respect llms? a cross-lingual study on the influence of prompt politeness on llm performance.",
                "author": "Ziqi Yin, Hao Wang, Kaito Horio, Daisuke Kawahara, and Satoshi Sekine.",
                "venue": "arXiv preprint arXiv:2402.14531, 2024b.",
                "url": null
            }
        },
        {
            "276": {
                "title": "Making retrieval-augmented language models robust to irrelevant context.",
                "author": "Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant.",
                "venue": "arXiv preprint arXiv:2310.01558, 2023.",
                "url": null
            }
        },
        {
            "277": {
                "title": "Unlearning bias in language models by partitioning gradients.",
                "author": "Charles Yu, Sullam Jeoung, Anish Kasi, Pengfei Yu, and Heng Ji.",
                "venue": "In Findings of the Association for Computational Linguistics: ACL 2023, pp.  6032\u20136048, 2023.",
                "url": null
            }
        },
        {
            "278": {
                "title": "Learning diverse and discriminative representations via the principle of maximal coding rate reduction.",
                "author": "Yaodong Yu, Kwan Ho Ryan Chan, Chong You, Chaobing Song, and Yi Ma.",
                "venue": "Advances in Neural Information Processing Systems, 33:9422\u20139434, 2020.",
                "url": null
            }
        },
        {
            "279": {
                "title": "Explainability in graph neural networks: A taxonomic survey.",
                "author": "Hao Yuan, Haiyang Yu, Shurui Gui, and Shuiwang Ji.",
                "venue": "IEEE transactions on pattern analysis and machine intelligence, 45(5):5782\u20135799, 2022.",
                "url": null
            }
        },
        {
            "280": {
                "title": "Post-hoc concept bottleneck models.",
                "author": "Mert Yuksekgonul, Maggie Wang, and James Zou.",
                "venue": "In The Eleventh International Conference on Learning Representations, 2022.",
                "url": null
            }
        },
        {
            "281": {
                "title": "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms.",
                "author": "Yi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, and Weiyan Shi.",
                "venue": "arXiv preprint arXiv:2401.06373, 2024.",
                "url": null
            }
        },
        {
            "282": {
                "title": "Visual interpretability for deep learning: a survey.",
                "author": "Quan-shi Zhang and Song-Chun Zhu.",
                "venue": "Frontiers of Information Technology & Electronic Engineering, 19(1):27\u201339, 2018.",
                "url": null
            }
        },
        {
            "283": {
                "title": "Multimodal chain-of-thought reasoning in language models.",
                "author": "Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola.",
                "venue": "arXiv preprint arXiv:2302.00923, 2023.",
                "url": null
            }
        },
        {
            "284": {
                "title": "Automated natural language explanation of deep visual neurons with large models.",
                "author": "Chenxu Zhao, Wei Qian, Yucheng Shi, Mengdi Huai, and Ninghao Liu.",
                "venue": "arXiv preprint arXiv:2310.10708, 2023a.",
                "url": null
            }
        },
        {
            "285": {
                "title": "Explainability for large language models: A survey.",
                "author": "Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, and Mengnan Du.",
                "venue": "ACM Transactions on Intelligent Systems and Technology (TIST), 2023b.",
                "url": null
            }
        },
        {
            "286": {
                "title": "Opening the black box of large language models: Two views on holistic interpretability.",
                "author": "Haiyan Zhao, Fan Yang, Himabindu Lakkaraju, and Mengnan Du.",
                "venue": "arXiv preprint arXiv:2402.10688, 2024.",
                "url": null
            }
        },
        {
            "287": {
                "title": "Lirex: Augmenting language inference with relevant explanations.",
                "author": "Xinyan Zhao and VG Vinod Vydiswaran.",
                "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp.  14532\u201314539, 2021.",
                "url": null
            }
        },
        {
            "288": {
                "title": "Interpretation of time-series deep models: A survey.",
                "author": "Ziqi Zhao, Yucheng Shi, Shushan Wu, Fan Yang, Wenzhan Song, and Ninghao Liu.",
                "venue": "arXiv preprint arXiv:2305.14582, 2023c.",
                "url": null
            }
        },
        {
            "289": {
                "title": "Mquake: Assessing knowledge editing in language models via multi-hop questions.",
                "author": "Zexuan Zhong, Zhengxuan Wu, Christopher D Manning, Christopher Potts, and Danqi Chen.",
                "venue": "arXiv preprint arXiv:2305.14795, 2023.",
                "url": null
            }
        },
        {
            "290": {
                "title": "S3-Rec: Self-supervised learning for sequential recommendation with mutual information maximization.",
                "author": "Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, and Ji-Rong Wen.",
                "venue": "In CIKM, pp.  1893\u20131902, 2020.",
                "url": null
            }
        },
        {
            "291": {
                "title": "Causal inference in recommender systems: A survey of strategies for bias mitigation, explanation, and generalization.",
                "author": "Yaochen Zhu, Jing Ma, and Jundong Li.",
                "venue": "arXiv preprint arXiv:2301.00910, 2023a.",
                "url": null
            }
        },
        {
            "292": {
                "title": "Collaborative large language model for recommender systems.",
                "author": "Yaochen Zhu, Liang Wu, Qi Guo, Liangjie Hong, and Jundong Li.",
                "venue": "In The Web Conference, 2024.",
                "url": null
            }
        },
        {
            "293": {
                "title": "Large language models can learn rules.",
                "author": "Zhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang, Dale Schuurmans, and Hanjun Dai.",
                "venue": "arXiv preprint arXiv:2310.07064, 2023b.",
                "url": null
            }
        },
        {
            "294": {
                "title": "Interpretable ranking with generalized additive models.",
                "author": "Honglei Zhuang, Xuanhui Wang, Michael Bendersky, Alexander Grushetsky, Yonghui Wu, Petr Mitrichev, Ethan Sterling, Nathan Bell, Walker Ravina, and Hai Qian.",
                "venue": "In WSDM, 2021.",
                "url": null
            }
        },
        {
            "295": {
                "title": "Representation engineering: A top-down approach to ai transparency.",
                "author": "Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, et al.",
                "venue": "arXiv preprint arXiv:2310.01405, 2023a.",
                "url": null
            }
        },
        {
            "296": {
                "title": "Universal and transferable adversarial attacks on aligned language models.",
                "author": "Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson.",
                "venue": "arXiv preprint arXiv:2307.15043, 2023b.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.08946v1",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "2",
            "2.1",
            "2.1.1",
            "2.1.2",
            "2.2",
            "2.2.1",
            "2.2.2",
            "2.3",
            "2.3.1",
            "2.3.2",
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.3.1",
            "3.3.2",
            "4",
            "4.1",
            "4.1.1",
            "4.1.2",
            "4.2",
            "4.2.1",
            "4.2.2",
            "4.3",
            "4.3.1",
            "4.3.2",
            "4.3.3"
        ],
        "main_experiment_and_results_sections": [
            "2.2",
            "2.2.1",
            "2.2.2",
            "4.2",
            "4.2.1",
            "4.2.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "2",
            "2.2",
            "2.2.1",
            "2.2.2"
        ]
    },
    "research_context": {
        "paper_id": "2403.08946v1",
        "paper_title": "Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era",
        "research_background": "**Paper's Motivation:**\n\nThe motivation of this paper centers on the promise of explainability in understanding and improving machine learning models. The paper aims to scrutinize whether contemporary explainability methods meet high user expectations, specifically in determining if models work properly and if these explanations aid in developing better models. The advent of Large Language Models (LLMs), which have considerably larger model sizes and more complex functionalities, exacerbates the challenges in achieving effective explainability. Given LLMs' transformative impact and potential risks, there is an imminent need to ensure their ethical use and explainability. Moreover, LLMs\u2019 human-like communication and commonsense reasoning abilities present unique opportunities for enhancing Explainable AI (XAI).\n\n**Research Problem:**\n\nThe core research problem addressed by the paper is twofold:\n\n1. Many existing explanation approaches fail to bridge the gap between producing explanations and taking actionable steps to improve model performance and trustworthiness (e.g., fairness, security, rationality).\n2. Current XAI methods predominantly adopt a technical perspective, overlooking the practical needs of non-technical stakeholders and practitioners. This disparity hinders the usability and effectiveness of XAI in real-world applications, particularly in the context of LLMs which introduce new challenges and opportunities.\n\nThe paper seeks to devise strategies that make XAI more usable in the era of LLMs, focusing on both utilizing explainability to enhance AI systems and leveraging LLM capabilities to improve XAI frameworks.\n\n**Relevant Prior Work:**\n\n1. **Expansion of XAI Methods:** A substantial body of literature has been developed to improve model transparency, covering a variety of methods for different data types, such as visual, textual, graph, and time-series data (e.g., Du et al., 2019a; Murdoch et al., 2019; Tjoa & Guan, 2020; Do\u0161ilovi\u0107 et al., 2018; Rudin et al., 2022; Zhang & Zhu, 2018; Danilevsky et al., 2020; Yuan et al., 2022; Zhao et al., 2023c).\n\n2. **Specific Techniques in XAI:** Certain studies focus on specific techniques like attention methods and causal models, as well as general principles and categorizations, or evaluate the faithfulness of explanations (e.g., Yang et al., 2019).\n\n3. **Challenges in XAI Usability:** The disconnect between acquiring explanations and employing them to make concrete improvements to AI models is underscored. Existing approaches often see XAI merely as a technical issue, missing practical usability for non-technical stakeholders (Malizia & Patern\u00f2, 2023).\n\n4. **Existing Surveys on XAI and LLMs:** Prior surveys have examined general XAI techniques (e.g., Du et al., 2019a; Tjoa & Guan, 2020; Do\u0161ilovi\u0107 et al., 2018; Murdoch et al., 2019), and some have specifically reviewed explainability methods for LLMs (e.g., Zhao et al., 2023b). However, there's a lack of emphasis on the usability of XAI in LLM studies.\n\n5. **Related Work by Luo & Specia:** The most related work discusses how explanations can enhance LLM performance but lacks a thorough examination of methods and practical implementation to make XAI frameworks more effective, as well as the role LLMs can play in improving existing XAI frameworks.",
        "methodology": "This section outlines our approach to leverage attribution methods as post-hoc explanations for large language models (LLMs), and how these methods can be used to identify model defects. We start by revisiting existing attribution methods and categorizing them based on their suitability for explaining LLMs. Given that LLMs serve both classification and generation tasks, we organize our discussion around these functionalities.\nWe utilize two primary datasets for our case studies:\n1. **Multi-Sentence Reading Comprehension (MultiRC) Dataset:** This dataset is challenging because it requires answering questions based on multiple sentences from a paragraph, akin to real-world scenarios involving LLMs. Human-annotated rationales indicate which sentences are relevant for each question. We use the test subset of 950 samples, with 80% allocated for training. \n2. **Hallucination Evaluation Benchmark:** This dataset includes input prompts, responses generated by ChatGPT, and knowledge related to these responses, annotated for validity. Instances with at least one invalid knowledge point are considered hallucinations. The study focuses on 632 examples, split into 80% for training and 20% for testing.\n**Importance Density Score Method:**\nWe apply the Importance Density Score method to estimate the importance of each word in the provided paragraph. Attribution from each input word \\( w \\) to each output word \\( o \\) is defined as:\n\\[ A(i,j) = LM(concat(P, R_{1:j})) - emb(w_i) \\]\nwhere:\n- \\( P \\) is the input paragraph,\n- \\( R_{1:j} \\) is the input prompt concatenated with the first \\( j \\) response words,\n- \\( LM \\) is the language model,\n- \\( emb(w_i) \\) is the static embedding of the input word \\( w_i \\).\nThis pairwise attribution score is normalized:\n\\[ A' (i,j) = \\frac{A(i,j) - \\min(A(i,:))}{\\max(A(i,:)) - \\min(A(i,:))} \\]\nAttribution scores less or equal to a threshold \\( \\Delta \\) are set to 0. The overall attribution score on word \\( i \\) is its density:\n\\[ Density(w_i) = \\sum_{j=1}^L A'(i,j) \\]\nWe use \\( L = 80 \\), \\( \\Delta = 0.5 \\), and the Vicuna-7B-v1.1 model. The importance of each sentence is determined by averaging these word scores. Top-K sentences with the highest importance are selected as explanations.\nGiven ChatGPT's inaccessibility for computing gradients, we employ Vicuna-7B as a substitute to compute attribution scores. We utilize the NLTK package for POS tagging each query word. Each query-response pair is represented by an 82-dimensional vector based on average attribution scores over POS types. We develop an SVM classifier and assess performance using precision, recall, F1, and accuracy metrics.\nThe results indicate our methods outperform random strategies in detecting hallucinations and exhibit competitive performance compared to GPT-4-based tools while using a smaller model (7B parameters). This highlights the practicality and efficiency of our approach, demonstrating its capacity to generalize from weaker to stronger models.\nFuture research could explore more effective feature extraction and advanced classifiers to enhance performance.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\nThe main experiment focuses on leveraging attribution scores to analyze Large Language Models (LLMs) behaviors and diagnose response quality. The overall experiment setup includes two primary case studies: \n\n1. **Multi-Sentence Reading Comprehension (MultiRC) Case Study**\n2. **Hallucination Detection Case Study**\n\n**Datasets:**\n1. **MultiRC Dataset:** Employs the Multi-Sentence Reading Comprehension (MultiRC) dataset, which requires the system to answer questions based on multiple sentences from a paragraph. The dataset includes 950 test samples, with 80% allocated to the training set.\n2. **Hallucination Evaluation Benchmark:** Involves a dataset that includes input prompts, ChatGPT-generated responses, and human annotations of the validity of related knowledge. The study focuses on 632 examples, split into 80% for training and 20% for testing.\n\n**Baselines:**\n1. For the MultiRC case study, compared the model using attribution-based explanations with models using either the full paragraph or human-annotated rationales.\n2. For the Hallucination Detection case study, compared the proposed method with a fine-tuned model and a prompting-based method.\n\n**Evaluation Metrics:**\n1. For the MultiRC case study, evaluated the accuracy of generated responses using macro precision, recall, F1 score, and AUC score.\n2. For the Hallucination Detection case study, reported precision, recall, F1 score on hallucination samples, and overall accuracy.\n\n**Main Experimental Results:**\n1. **MultiRC Case Study:**\n   - Attribution-based explanations yielded better results than using the full paragraph.\n   - Explanations more closely aligned with human-annotated rationales, particularly for correct responses.\n\n2. **Hallucination Detection Case Study:**\n   - The proposed method showed competitive performance with FacTool using GPT-4, but with a smaller 7-billion-parameter model.\n   - The results indicated that all methods, including the proposed one, performed better than a random strategy.\n   - The attribution scores effectively diagnosed hallucinations, achieving significant precision, recall, and F1 scores.\n\nThe overall findings underscore the effectiveness and practicality of using smaller models to diagnose large LLM behaviors through attribution score-based features. Future work could aim to extract more effective features and use more powerful classifiers."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "The study aims to assess how effectively attribution methods can identify defects in LLMs by using different datasets and metrics.",
            "experiment_process": "The MultiRC dataset, known for its need to answer questions based on multiple sentences, is selected. They use a subset of 950 samples, split 80%-20% into training and test sets. The Importance Density Score method is employed to estimate the importance of each word in a paragraph. These scores are normalized and used to determine the overall attribution score for each word. Vicuna-7B-v1.1 is the language model employed. The top-K sentences with the highest importance scores are selected as explanations. These explanations are concatenated with the output response and fed to a DistillBERT-base classifier. Results are compared with full paragraphs and human-annotated rationales, evaluated by matching correct answers. The Hallucination Evaluation Benchmark is also used for samples with potentially incorrect knowledge, training an SVM classifier based on POS tagging attribution scores.",
            "result_discussion": "The study finds that isolating the rationales from the full context helps the classifier to identify response quality better. Explanations align more closely with human-annotated rationales for correct responses. The precision and recall metrics for correct responses are higher, proving the effectiveness of attribution-based explanations. For hallucination detection, all methods outperform random strategy, showing competitive performance with methods using GPT-4. The practical efficiency of using a smaller language model for attribution scores is highlighted.",
            "ablation_id": "2403.08946v1.No1"
        },
        {
            "research_objective": "To evaluate the quality of LLM-generated responses by using attribution-based explanations, specifically measuring response accuracy related to human rationales.",
            "experiment_process": "Using the MultiRC dataset, a comparison is made between attribution-based explanations and human-annotated rationales. The Importance Density Score method is applied to estimate word importance in paragraphs. The top-K sentences by importance are used as explanations, fed to a DistillBERT-base classifier, trained over 3 epochs, and compared against full paragraphs and human rationales. Accuracy is evaluated by exact matches of correct responses.",
            "result_discussion": "Isolating rationales helps the classifier in effectively identifying response quality. Explanations match closely with human rationales for correct responses, exhibiting higher precision and recall. The effectiveness of attribution-based explanations in estimating LLM response accuracy is empirically supported.",
            "ablation_id": "2403.08946v1.No2"
        },
        {
            "research_objective": "To detect hallucinations in LLM-generated responses using attribution-based explanations.",
            "experiment_process": "The Hallucination Evaluation Benchmark is used to identify hallucination instances. The VICUNA-7B model is applied to compute attribution scores, due to ChatGPT's gradients being inaccessible. POS tagging of query words is performed using NLTK, creating an 82-dimensional vector representing average attribution scores by POS tagging type. An SVM classifier is developed based on these POS tagging scores, trained on 80% of 632 benchmark samples, with results compared against fine-tuned model and prompting-based method baselines.",
            "result_discussion": "The methods outperform random strategies, showing validity as hallucination detectors. The proposed methods, using a smaller language model for attribution scores, demonstrate competitive performance with GPT-4 baselines. This illustrates practical advantages in weak-to-strong generalization for diagnosing large language models.",
            "ablation_id": "2403.08946v1.No3"
        }
    ]
}