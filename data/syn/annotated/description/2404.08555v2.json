{
    "title": "RLHF Deciphered: A Critical Analysis of Reinforcement Learning from Human Feedback for LLMs",
    "abstract": "State-of-the-art large language models (LLMs) have become indispensable tools for various tasks.\nHowever, training LLMs to serve as effective assistants for humans requires careful consideration.\nA promising approach is reinforcement learning from human feedback (RLHF), which leverages human feedback to update the model in accordance with human preferences and\nmitigate issues like toxicity and hallucinations.\nYet, an understanding of RLHF for LLMs is largely entangled with initial design choices that popularized the method and current research focuses on augmenting those choices rather than fundamentally improving the framework.\nIn this paper, we analyze RLHF through the lens of reinforcement learning principles to develop an understanding of its fundamentals,\ndedicating substantial focus to the core component of RLHF\u2014the reward model.\nOur study investigates modeling choices, caveats of function approximation, and their implications on RLHF training algorithms, highlighting the underlying assumptions made about the expressivity of reward.\nOur analysis improves the understanding of the role of reward models and methods for their training, concurrently revealing limitations of the current methodology.\nWe characterize these limitations, including incorrect generalization, model misspecification, and the sparsity of feedback, along with their impact on the performance of a language model.\nThe discussion and analysis are substantiated by a categorical review of current literature, serving as a reference for researchers and practitioners to understand the challenges of RLHF and build upon existing efforts.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "###figure_1### Large Language Models (LLMs) demonstrate remarkable capabilities that extend beyond basic language tasks, leading to their widespread adoption across various industries.\nThe remarkable utility of these models holds the potential to transform established workflows in critical sectors such as technology, healthcare, finance, and education Singhal et al. [2022  ###reference_b152###]; Wu et al. [2023a  ###reference_b172###]; Yan et al. [2023  ###reference_b180###].\nAs they become integral to these domains, it\u2019s crucial to ensure that the behavior of LLMs is predictable, safe, and trustworthy\u2014\u2013meeting the expectations set for a human performing the same tasks.\nThis challenge of making LLMs exhibit human-like qualities, known as alignment with human objectives, is central to making these models suitable for diverse tasks.\nAn effective method for addressing this challenge is reinforcement learning from human feedback (RLHF).\nRLHF first gained popularity due to its ability to solve reinforcement learning (RL) problems like simulated robotic locomotion and playing Atari games Christiano et al. [2017  ###reference_b30###] without access to a reward function, by simply leveraging human feedback about preferences on demonstrated behaviors.\nIt has since been adopted for fine-tuning LLMs using human feedback.\nThis leads to a natural inquiry: How can a method designed to master games be effectively used to align LLMs with human objectives?\nThe method has proven to be immensely successful OpenAI [2022  ###reference_b116###], but not without well-documented limitations Casper et al. [2023  ###reference_b25###].\nA comprehensive understanding of why it achieves its success remains largely elusive.\nConsequently, research efforts on the topic are stuck in a local minima, with variants focused on augmenting the components of the method\u2014including the training algorithm Ramamurthy et al. [2022  ###reference_b133###], reward model Wu et al. [2023c  ###reference_b174###], and even RL-free approaches Rafailov et al. [2023  ###reference_b129###].\nHowever, some fundamental limitations of the approach remain obscured due to the overarching goal of recent work to refine the initial design choices.\nIn this work, we develop a comprehensive understanding of RLHF by analyzing the core components of the method.\nWe begin the study by motivating the necessity for RLHF by highlighting the problem of objective mismatch in pre-trained LMs (Section 2  ###reference_###).\nTo formulate foundational questions about the framework, we adopt a Bayesian perspective of RLHF.\nIt serves to highlight the significance of the reward function in particular (Section 4  ###reference_###).\nThe reward function forms the central cog of the RLHF procedure, and the design choices used to model it form a major focus of our study.\nThe current formulation of RLHF relies on a set of assumptions to model the reward function (Section 4.1  ###reference_###, 4.2  ###reference_###).\nFollowing the delineation of these assumptions, an analysis of the reward model independent of specific modeling choices follows.\nThe analysis, in a principled manner, provides an understanding of issues such as:\nThe impractical requirement for extensive amounts of feedback data for training accurate reward models.\nThe combination of very limited feedback data and the use of function approximation results in misgeneralization, wherein inaccurate reward values are assigned to inputs not seen during training.\nThese imperfections of the reward model, along with challenges such as reward sparsity and reward model misspecification, are highlighted in the paper (Section 5.1  ###reference_###).\nTheir impact on the performance of a language model is explored in detail (Section 6.2  ###reference_###).\nThe course of the analysis leads to the formalization of concepts such as an oracular reward that serve as the theoretical golden standard for future efforts (Section 4.1  ###reference_###).\nAn overview of the RLHF procedure along with the various challenges studied in this work is provided in Figure 1  ###reference_###.\nThe discussion is followed by an extensive survey of an expanding body of literature related to the topic.\nThe survey is organized into sections that outline the framework of RLHF.\nStarting with a high-level overview of Large Language Models (LLMs), the survey systematically covers various aspects:\nDifferent types of human (and non-human) feedback (Section 7.3  ###reference_###),\nThe training methods in RLHF (Section 7.6  ###reference_###),\nAlternative approaches that do not rely on RL or reward models (Section 7.9  ###reference_###).\nThis structure aims to provide a comprehensive overview of the extensive landscape of works that have contributed to the remarkable success of RLHF."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Motivation: Eliminating Objective Mismatch in Pre-Trained Language Models",
            "text": "Large pre-trained language models (PLMs) are massive neural networks that are trained on a huge corpus of texts using a self-supervised learning objective.\nOriginally utilized for representation learning Devlin et al. [2019  ###reference_b36###]; Liu et al. [2019  ###reference_b94###] with encoder-only models, recent research, particularly influenced by Brown et al. [2020  ###reference_b24###], has shifted its focus towards training PLMs to directly generate answers for textual problems.\nState-of-the-art PLMs typically employ an auto-regressive transformer architecture Vaswani et al. [2017  ###reference_b166###] and are trained with a causal language modeling objective.\nThese models implicitly capture a conditional probability distribution , reflecting the likelihood of sampling the next token after observing a sequence of previous tokens.\nThe probability of a text sequence , under this model is denoted as .\nThe model is trained to estimate the pre-training data generating probability distribution over text sequences by minimizing the (forward) KL divergence between the model\u2019s data-generating distribution and the pre-training data distribution, denoted by .\nThe first term, representing the entropy of , is independent of  and can be disregarded during optimization.\nConsequently, the objective simplifies to the following cross-entropy minimization form:\nThe expectation is approximated using samples from an unsupervised pretraining text corpus , which comprises text sequences sampled from . This leads us to the following objective:\nThe remarkable property about PLMs lies in the contrast between the simplicity of the training recipe and the remarkable results that they deliver Brown et al. [2020  ###reference_b24###].\nSimply capturing language statistics along with scaling up the number of trainable parameters, endows PLMs with robust semantic representations, vast commonsense knowledge, and strong pattern-following capabilities.\nHowever, for adopting PLMs to assist humans with tasks that require an understanding of human intentions and the ability to follow instructions, the simple training recipe of PLMs is insufficient.\nThese models demonstrate a shallow understanding of human intentions, often generating undesirable outputs, including incorrect facts or conveying biased and toxic opinions.\nFundamentally, PLMs suffer from an objective mismatch problem: the training-time objective of capturing language statistics does not necessarily align with the deployment-time objective of fulfilling a human user\u2019s specific goals.\nEliminating this mismatch at first glance seems feasible: just train PLMs to optimize for the user objective.\nUnfortunately, for many tasks, it is impossible to express the user objective as an optimization target.\nFor example, when a user\u2019s objective pertains to eliciting humorous responses, establishing specific criteria for objectively evaluating the humor in a generated response becomes an inherently challenging task.\nThere are currently two primary ways to deal with the problem: the behaviorist approach and the cognition-driven approach.\nThe behaviorist approach, implemented by supervised fine-tuning (SFT), aims to replicate observable behaviors that humans perceive as desirable without explicit consideration of the underlying user objective. For instance, if a user desires good summaries of articles, this approach trains a model to imitate examples of good summaries without explicitly defining the criteria for a good summary.\nIn contrast, the cognition-driven approach, implemented by reinforcement learning from human feedback (RLHF), aims to uncover the underlying user objective that governs the observed behaviors. It then updates the model by optimizing the uncovered objective.\nThis approach relies on certain assumptions\u2014which in the case of RLHF are: (i) the user objective can bear the form of a reward function, which can assign a numerical score to behaviors of the model, and (ii) this function can be approximated by a machine learning model (e.g., a neural network).\nRLHF estimates this reward function and updates the PLM via reinforcement learning to optimize for rewards.\nRegardless of the approach, the process of addressing the objective mismatch problem is commonly referred to as the fine-tuning or alignment process.\nPresently, state-of-the-art language models typically initiate this process with the behaviorist approach, followed by the cognition-driven approach."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Bayesian Interpretation of RLHF",
            "text": "RLHF relies on observing human feedback to deduce the (latent) user reward function.\nHuman feedback is provided on the outputs from a language model.\nRLHF assumes that there exists an underlying human reward function that governs the feedback they provide in a particular manner,\ni.e., there exists some mapping from reward to actions of a human.\nSuppose the reward function is being inferred by a model  parameterized by .\nAdopting a Bayesian inference perspective Korbak et al. [2022  ###reference_b73###], the parameters  can be viewed as a hypothesis with the dataset of human feedback  as the evidence for this hypothesis.\nGiven a prior distribution over the hypothesis , we can apply Bayes\u2019 rule to derive the posterior distribution over the hypotheses after observing the evidence as:\nReward modeling in RLHF can be seen as computing the maximum a posteriori (MAP) estimate of the parameters of a reward model,\nThe first term (a) is the log-likelihood of the feedback dataset, specifying how a human\u2019s internal objective (reward function) governs their feedback.\nThe second term (b) represents constraints on the hypothesis space, which is enforced through explicit and implicit regularization techniques in neural-network training.\nThe presented framework raises two major questions:\nWhat is the form of the likelihood function ? In other words, how do we mathematically model the influence of a human\u2019s latent objective on their observable feedback?\nWhat is the reinforcement learning algorithm used for optimizing the reward model? In other words, how do we ensure the model acts consistently with its objective?\nA set of answers to these questions forms the basis for an RLHF algorithm.\nThe RLHF methodology, popularized by Christiano et al. [2017  ###reference_b30###], employs pairwise ranking feedback and uses the Bradley-Terry model Bradley and Terry [1952  ###reference_b22###] as the likelihood function.\nProximal Policy Optimization (PPO) Schulman et al. [2017  ###reference_b145###] is elected as the reinforcement learning algorithm.\nBefore we move into the analysis of this method, we urge the readers to take a moment to reflect on the choices and assumptions we have made so far to derive the general recipe of RLHF.\nAre there alternative choices?\nCan the assumptions be relaxed or improved?\nThinking critically about these foundational decisions is the key to understanding the strengths and weaknesses of RLHF algorithms and innovating them.\nFor example, the recently proposed direct preference optimization (DPO) approach Rafailov et al. [2023  ###reference_b129###] replaces reinforcement learning with a reformulation of the objective.\nNext, we formalize the problem setup of text generation as an agent interacting with a sequential decision process, laying the foundation for the analysis of RLHF.\nWe refer the reader to Section 7.6  ###reference_### for a detailed outline of the RLHF procedure, and Figure 5  ###reference_### for a summarized overview."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Formulation: Text Generation as Sequential Decision-Making",
            "text": "###figure_2### In this section, we formulate the text generation procedure from a language model as a sequential decision-making process.\nThis formulation is essential for constructing reinforcement learning algorithms.\nA common framework for modeling sequential decision-making processes is Markov Decision Process (MDP) Markov [1954  ###reference_b97###].\nAn MDP is defined as a tuple  where  is the set of states,  is the set of actions,  is the transition function,  is the reward function, and  is the initial state distribution.\nEach sequential time step of the process is denoted by , and  denote the values of the state, action, and reward at time step .\nA discounting factor  is defined for discounting rewards over time, particularly useful for modeling an MDP with an infinite number of time steps (i.e., an infinite-horizon MDP).\nHowever, the outputs of language models are truncated after a finite number of steps.\nWe use  to denote the maximum time step.\nAn agent acts in an MDP using a policy .\nThe agent starts in state .\nAt time step , it chooses an action , executes the action, transitions to a new state , and receives a reward .\nThe term \u201cMarkov\u201d in MDP refers to the Markov property, in that the distribution over the next state  depends on only the current state  and action .\nFor simplicity, we consider text generation tasks that include only one turn of interaction between the user and the model.\nWe make a distinction between the text that a user inputs into the model, denoted by  and referred to as the context or the prompt, and the text that the model generates by itself to the context, denoted by  and referred to as the output or simply the generated text.\nLet  be the set of all tokens that the model can generate (the vocabulary),  the set of all possible contexts, and  the set of all possible outputs.\nGiven a context  as input, the model generates an output  token by token.\nSpecifically, let  be the -th token in generated output , then the model parameterized by  first outputs token , and then conditioned on the concatenation of  and  it generates , and so on.\nWe can see that this generation process resembles an agent traversing in an MDP (Figure 2  ###reference_###).\nThe model acts according to a policy .\nThe start-state distribution  is the distribution over user-provided contexts.\nThe action space is the vocabulary .\nThe action  is the generated token .\nThe state  is the concatenation of the context  and all the tokens the model has generated up to time step .\nThe transition function  is a delta distribution, i.e., the next state is deterministic given the current state and action.\nReward  given at time step  is computed by the reward model as  which is either a human or a function learned from human feedback.\nThe text generation MDP has several special properties:\nThe action space is extremely large. For example, the LLaMa model Touvron et al. [2023a  ###reference_b163###, b  ###reference_b164###] employs a vocabulary of size 32K. Having a gigantic action space blows up the search space for reinforcement learning algorithms.\nThe structure of the state space is complex, as a state is essentially a text sequence. Pre-training on large amounts of texts is necessary to learn an initially good representation of this space.\nThe initial state distribution has an enormous support. All conceivable contexts lie in the support, thus strongly testing the ability of the policy to generalize to out-of-distribution states.\nThe reward function used for training can differ from the evaluation reward function.\nThis is because the humans providing rewards during evaluation may be different from the humans involved in trainnig the reward model. Analogous to transfer learning in RL, the agent must then adapt to the new reward function.\nThe transition function is deterministic. Algorithmic and analysis tools tailored for deterministic MDPs can be applied.\nThus, solving a text generation MDP requires specialized treatment that takes advantage of its properties and overcomes its inherent challenges.\nReinforcement learning Sutton and Barto [2018  ###reference_b160###]; Bertsekas and Tsitsiklis [1996  ###reference_b19###] provides solutions for optimally solving an MDP, i.e., learning a policy that maximizes the accumulated reward. Consequently, RLHF updates the language model to generate more rewarding outputs. Naturally, the reward function plays a critical role in the process of fine-tuning model outputs, determining practical and fundamental limits Casper et al. [2023  ###reference_b25###] of the efficacy of RLHF."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "The Role of Reward",
            "text": "The goal of reward learning in RLHF is to convert human feedback into an optimizable reward function.\nThe reward serves a dual purpose:\nit encodes the task information (for example, identical input-output pairs would receive distinct rewards depending on whether the task involved summarization or\ntext expansion)\n111Unless the task is specified in the input prompt itself, in which case the inputs differ.\nas well as preferences over those outputs (a condescending summary is rewarded less than a neutral summary).\nThe reward thus encodes relevant information for measuring (Section 4.3  ###reference_###) as well as inducing alignment with human objectives.\nBy setting the reward function of the sequential decision process to the one estimated from human feedback , reinforcement learning algorithms can be used to learn a language model policy that maximizes the cumulative reward, resulting in an aligned language model."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Oracular Reward and the Role of Human Feedback",
            "text": "An implicit assumption made in RLHF is that a human\u2019s feedback behavior is governed by and can be represented as a reversible reward function. We assume that this function is deterministic in line with the current methodology. The function takes as input a context  and an output , and outputs a scalar number reflecting the preference on  as a continuation of . Because the  is essentially a state in the MDP formulation, the reward function is essentially defined over states of the MDP. The language model that maximizes the oracular reward accurately reflects the goals and preferences inherent in the human feedback, and maximization of this reward consequently aligns the model with the human preferences. The oracular reward may not be accessible or learnable, but under the reward hypothesis Sutton [2004  ###reference_b162###]; Silver et al. [2021  ###reference_b151###], the mere existence of such a reward may be assumed\u2014though this may be challenged Knox et al. [2022  ###reference_b72###]. The oracular reward forms the golden standard for training as well as evaluating any language model. In general, humans can give a variety of feedback. RLHF operates with feedback that discloses information about the oracular reward function. Most methods focus on two types of feedback: point-wise numerical feedback (or rating), and pairwise ranking feedback (or preferences). Providing ratings is the most straightforward way to communicate the reward function. Given a pair , the rating is a scalar . While ratings can be fed directly into a reinforcement learning algorithm, learning a reward model takes advantage of the generalizability of the reward model on unseen outputs and contexts. Preference feedback compares two outputs generated for the same context. Given two outputs  and  generated for context , a human denoted a preference  if the first input is preferred and  otherwise. Preferences in their raw form are not compatible learning signals for reinforcement learning algorithms. Hence, a reward model must be learned for this type of feedback. To do so, an assumption must be made about the relationship between preferences and . We will discuss this in more detail in the next section. A discussion about the various methodologies used for encoding preferences can be found in Section 7.5  ###reference_###. An alternative approach for ranking outputs on the basis of preferences is provided by the learning-to-rank paradigm Liu et al. [2009  ###reference_b93###]. Using preference feedback offers several advantages compared to using ratings. Firstly, we get more training data for the reward model. In practice, people collect a ranking of  outputs and create preference pairs Ouyang et al. [2022  ###reference_b119###]. Collecting  ratings for  outputs provides we get  training points. Ranking  outputs provided  pairwise comparisons. Second, preferences require assigning a only relative order rather than an absolute precise score to an output; the latter task could take significantly more cognitive effort and is more prone to inconsistency. Finally, a preference is presumably easier to provide because it offers a \u201cbaseline\u201d for comparison (the worse output). In contrast, when giving a rating, a human can rely on only the evaluation guidelines. A note on stochastic rewards: The reward function is considered to be a deterministic mapping from text to a scalar value. This amounts to averaging the preferences of all humans that provided human feedback. Moreover, it assumes that a human must always rate an input-output pair with the same score, discounting the inherent variability of human preferences. There are numerous scenarios\u2014like personalization, in-context adaptation to ongoing dialogue, and diverse output generation\u2014where a deterministic mapping is limiting. The rewards are more appropriately modeled as being stochastic, wherein each input-output pair is scored by a distribution over scalar rewards, say . This modeling accounts for the two sources of uncertainty: (i) uncertainty over the specific human from a group of humans who provide feedback, and (ii) variability in a human\u2019s preferences due to changes in unobserved factors Nguyen et al. [2017  ###reference_b113###]. Some work in reinforcement learning aims to address this by learning Bayesian preferences, primarily for uncertainty quantification and safety analysis Ramachandran and Amir [2007  ###reference_b132###]; Brown and Niekum [2019  ###reference_b23###], and can be adapted to model a distribution of preferences over text. Some recent efforts along these lines Barnett et al. [2023  ###reference_b16###] have proven to be effective. We focus on deterministic rewards for the analysis that follows."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Reward Modeling",
            "text": "Learning a reward model serves two purposes: (i) to convert RLHF into a canonical reinforcement learning problem, and (ii) to reduce the cost of online feedback-collection.\nReinforcement learning algorithms define their objective in terms of a reward function.\nTo apply these algorithms, we need to infer a reward function from a feedback dataset, collecting which is notoriously expensive.\nCurrently, large language models require thousands to millions of feedback data points.\nTo gather that amount, many human evaluators need to be recruited to work in parallel.\nTo ensure the assumptions regarding the oracular reward function hold, the evaluators must be trained to agree with one another on the evaluation criteria.\nThis process is continual: multiple rounds of feedback collections need to be conducted to iteratively improve the model.\nThe premise of approaches that learn a reward model is that the generalization error of the reward model is expected to decrease faster than that of the policy as a function of the number of labeled data points, arising from the notion that supervised learning is often considered a simpler problem than generative modeling.\nFollowing the previous section, we denote the reward model by  and the feedback dataset by .\nOur goal is to decide a likelihood function \nand find  that maximizes this function:\nWith rating feedback, the reward-modeling problem can be formulated as a prediction problem with continuous output.\nA common objective for this type of problem is the minimization of the mean squared error (MSE):\nTo incorporate preference feedback, we need to choose the form of the likelihood function denoting each preference, i.e., .\nThe RLHF method of Ouyang et al. [2022  ###reference_b119###] employs the Bradley-Terry model to represent the likelihood of a data point:\nwhere  is the sigmoid function.\nThe learning objective for maximizing the log-likelihood of the dataset  is,\nIn Section 5  ###reference_###, we further generalize the form of feedback and the likelihood function to conduct an analysis independent of the specifics of particular design choices."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Measuring Alignment",
            "text": "Evaluation of natural language tasks is a difficult problem, and the study of evaluation metrics is an active area of research.\nOf particular importance, and difficulty, is to measure the alignment of a language model to a human\u2019s objectives, which in practice is evaluated along the axes of helpfulness, harmlessness, and honesty.\nThe oracular reward that governs a human\u2019s preferences serves as a yardstick for measuring the degree of alignment.\nThe task of alignment is then reformulated as encoding the preferences demonstrated by a human into a reward function, and updating the parameters of the language model to produce output that maximizes this reward.\nA reward provides an analytical metric to measure the overall performance of a language model , where the performance captures the degree of alignment with human preferences along with the degree of satisfaction of the task itself Ngo et al. [2022  ###reference_b111###].\nThe performance of a model , for distribution over contexts , can be measured by averaging the rewards for the outputs generated by  given the contexts. Let the performance be denoted by :\nThe context distribution  can be the distribution of contexts in the training data, test data, or a held-out validation dataset, depending on the data on which the performance of the model is being evaluated.\nThe sequential nature of the output generation equivalently allows us to express  as:\nIn practice, most current reward models only provide a reward after the complete output has been generated and Equation 13  ###reference_### reduces to Equation 12  ###reference_###. The definition of  uses the oracular reward that is not accessible in practice. An estimate of the performance can obtained from the estimated reward , by plugging it into Equation 12  ###reference_###:\nThe pre-trained language model is denoted by  and the model updated using RLHF by .\nThe goal of RLHF is to update the parameters of  such that , i.e., as evaluated using the oracular reward. In practice, it is only possible to verify that , which may be non-informative when the estimated reward model  has inaccuracies for the context-output pairs being evaluated (Section 6.2  ###reference_###)."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Inferring the Reward from Human Feedback",
            "text": "In the following sections, we study the properties of the reward estimated from human feedback.\nAs reviewed in Section 7.5  ###reference_###, various procedures exist for encoding human feedback into a reward model.\nBoth the form of human feedback and the encoding mechanism continue to be studied further, with the procedures continually evolving and improving.\nCurrently, the most common form of human feedback is pair-wise preference feedback that is encoded into a reward according to the Bradley-Terry model (Section 4.2  ###reference_###).\nTo perform an analysis agnostic to specifics of a particular reward learning method,\nLet feedback denote a general form of sufficiently informative feedback.\nLet  denote the model of human behavior, or the encoding mechanism, that maps the feedback and the text to a reward value.\nThe generality of this formulation allows the following analysis to cover all existing RLHF-style approaches (for example, RLAIF Bai et al. [2022b  ###reference_b13###]) as well as future methods for fine-tuning LLMs, that employ a reward model.\nLet  denote a hypothetical dataset of all possible contexts and outputs that a language model can encounter, i.e., a humongous dataset of size . This dataset cannot be realized in practice and is invoked to shed light on the practical limitations of the existing methodology.\nDenote the dataset of collected human feedback by  where  are the subsets of context-output pairs (human-)annotated with feedback.222The subsets are significantly smaller than  and . Additionally, the feedback can be of any form: ratings, pair-wise feedback, or language feedback (Section 7.3  ###reference_###).\nThe reward encoding mechanism that maps context-output pairs along human feedback to rewards (for instance, the Bradley-Terry model) is denoted by .333feedback is overloaded to capture additional mechanism-specific meta-data.\nFor instance, for pair-wise preference, feedback can store the preference relation and the  pair compared against.\nTo uncover , it is assumed that  accurately maps back human feedback to the oracular reward, i.e., for sufficiently informative feedback, we have\nUnder that assumption,  can operate on  to create a dataset of context-output-reward tuples,\n where .\nWith , learning the reward model  reduces to a regression problem employing a function approximator.\nThe regression problem is however underdetermined Bishop [2006  ###reference_b20###], and consequently multiple  functions can perfectly fit the training data . However, almost all of these functions fail to accurately represent the oracular reward (Figure 3  ###reference_###).\nDue to the cost of human annotation, practically human feedback can be collected on a very small subset of context and output pairs, i.e., .\nThe size of the reward and feedback datasets relative to the hypothetical dataset of all possible inputs and outputs  can be measured by:\nContext coverage:\nOutput coverage: , where\nWell-understood results in supervised learning suggest that\nthe ratios  and  along with the generalization capabilities of the function approximator Nakkiran et al. [2019  ###reference_b109###]; Schaeffer et al. [2023  ###reference_b141###]; Bailly et al. [2022  ###reference_b14###] determine the generalization performance of the reward model for .\nIn practice, the values of  and  are extremely small and consquently the reward model often incorrectly generalizes on unseen (out-of-distribution) context-output pairs, assigning incorrect rewards to such inputs.\nIn the following sections, we study practical limitations of estimating reward models."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Limitations of the Reward Model",
            "text": "The reward model parameterized \nis trained on  using a sufficiently representative function approximator, to perfectly fit the training data, that is .\nThe limitations of the resultant reward model may be studied under the following categories:\n###figure_3### Misgeneralization:\nHuman feedback is obtained on a very small subset of all possible context-output pairs.\nThis partial coverage over contexts and outputs in  combined with the use of function approximators for learning the reward model results in the reward model  incorrectly generalizing to data points that are out-of-distribution relative to .\nWe have assumed a sufficiently representative function approximator that perfectly fits the training data,\n.\nHowever, it cannot be ensured that\n\nwill be zero. It would require a function approximator to perfectly generalize outside the training data distribution, which is not generally attainable, especially when the ratios  are minuscule.\nThe benefits of reinforcement learning algorithms over other methods\nfor finetuning\nare contingent on access to an accurate reward function (Section 6.3  ###reference_###), necessitating accurate\nout-of-distribution generalization of the reward model.\nThe inaccurate extrapolation out-of-distribution\nresults in an \u2018imperfect\u2019 reward model that provides feedback on context-output pairs in a manner that when optimized, arbitrarily misaligns with human feedback (and resultant preferences) for those context-output pairs.\nThe output distribution of  trained on this inaccurate feedback and can only be as good (or bad) as the reward signal provided by the reward model.\nThis inaccurate generalization in the reward model is one of the primary causes of phenomena like \u2018reward hacking\u2019 and \u2018hallucinations\u2019 Kalai and Vempala [2023  ###reference_b63###], observed in practice.\nDelayed feedback and Reward Sparsity:\nReinforcement learning algorithms benefit from dense rewards as they serve to quickly guide the agent to rewarding states, providing informative feedback to intermediate actions along the trajectory.\nIn RLHF, the feedback from human annotators is obtained for complete output generations. Consequently, the reward model is trained to provide reward feedback only at the end of the generated output for a given context.\nThis delayed feedback increases the difficulty of optimization with RL algorithms, increasing their sample complexity.\nSparse feedback is a constraint inherent to dealing with text and language Sokolov et al. [2016  ###reference_b154###], as it is often unlikely for a human to provide feedback on incomplete sentences.\nMethods in RL developed to deal with sparse feedback, for instance by stitching together information from partial trajectories Andrychowicz et al. [2017  ###reference_b4###], cannot be applied directly to textual output due to the semantic constraints of dealing with partial sentences.\nDenser rewards and corresponding feedback result in faster training, improved sample efficiency Wu et al. [2023d  ###reference_b175###], and potentially better generalization.\nInsights from linguistics may be employed to obtain feedback on partial output generations and in turn denser rewards.\nMarginalization over preferences:\nThe reward model averages over the preferences of all human annotators (and other sources of feedback) to output a deterministic scalar reward for a given context-output pair.\nThe expectation is that averaging over the preferences of multiple sources would be representative of the preferences of an average human persona Deshpande et al. [2023b  ###reference_b35###].\nThe results in rewards that are inconsistent with any single human\u2019s preferences.\nSuch preferences are more appropriately denoted by an distribution of rewards for a context-output pair.\nA deterministic model, in addition to discounting the uncertainty and variability of human preferences, cannot model such a distribution, highlighting a case of model misspecification.\nThe reward model forms the core component of RLHF and dictates the performance of a language model.\nThe aforementioned shortcomings of the reward model highlight the need for safety measures that must be employed while using a language model fine-tuned using RLHF."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Reinforcement Learning with Imperfect Rewards",
            "text": "Reinforcement learning algorithms can be broadly categorized into value-based methods and policy-gradient methods Sutton and Barto [2018  ###reference_b160###]. Value-based methods aim to learn the value of states (or state-action pairs) as measured by the expected cumulative future reward from that state under a policy.\nThese values serve as guides for picking the most rewarding actions from each state, and the policy can be inferred from the value functions. Policy-gradient methods directly train a parameterized policy using reward feedback to perform gradient ascent over the policy parameters and maximize the expected cumulative reward. A benefit of policy gradient methods for language tasks is that they naturally permit the optimization of stochastic policies, making them amendable for optimizing language models with stochastic decoding algorithms. Below we provide a brief overview of policy gradient algorithms and refer interested readers to Sutton et al. [1999  ###reference_b161###]; Williams [1992  ###reference_b170###]; Weng [2018  ###reference_b169###] for a more rigorous treatment of the topic.\nIn supervised fine-tuning (SFT), supervision is provided with positive samples, and the language model is updated to increase the likelihood of those samples under the model. Notably, there is no supervision provided for neutral or undesirable outputs, although it is a feasible option.\nGiven the optimal policy  (which may be a human expert), the objective of SFT is,\nand thus the gradients used to update the parameters of the language model are of the form:\nThis is analogous to behavior cloning in RL Pomerleau [1988  ###reference_b126###] which is known to struggle when faced with out-of-distribution inputs.\nThe primary benefit that reinforcement learning algorithms provide is that they allow the language model to explore the output space.\nThrough its decoding algorithm, the language model exercises control over the distribution of outputs on which feedback is acquired.\nThis facilitates learning from both positive as well as negative feedback,\ni.e.,\nand the (vanilla) policy gradient update is:\nAs highlighted in color, in SFT, the gradient is estimated only from the positive samples, while in RL, it is computed for all samples (positive, negative, or neutral) weighted by their corresponding rewards.\nThe gradient updates in RL are more informative, leading to better generalization for the language model and improved sample efficiency.\nBeyond exploration and richer gradients, the field of inverse reinforcement learning provides a natural formulation for training a language model with human feedback Arora and Doshi [2021  ###reference_b7###].\nIn the following sections, we present a review of works that lead up to and are being rapidly added to this active area of research.\nThis review provides context for the first half of this work and also serves as a comprehensive introduction for readers interested in getting started and understanding the topic of RLHF for language models."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Policy Gradient Algorithms",
            "text": "Policy gradient algorithms update the parameters of an agent\u2019s policy using reward feedback.\nBeing gradient-based algorithms, their update rule is of the form:\nwhere  is the performance (Equation (12  ###reference_###)) of the policy parameterized by .\nThe gradient of the performance of a policy  can be estimated from samples in numerous ways, each affording varying degrees of variance and estimation error.\nIn sparse rewards settings, the gradient estimation variance is a common issue that baselines Mei et al. [2022  ###reference_b102###] help address.\nA class of methods called actor-critic methods\nupdate the policy by leveraging estimated value functions, called critics, to reduce gradient estimation variance.\nThe algorithm used for training most current state-of-the-art large language models, Proximal Policy Optimization (PPO) Schulman et al. [2017  ###reference_b145###] is an actor-critic algorithm with improvements over vanilla actor-critic to ensure stability during training.\nThe improvements restrict parameter updates at each iteration to prevent the policy distribution from drastically changing.\nThe training loss objective for PPO (PPO-Clip) takes the form:\nwhere  is the estimate of the advantage function  that captures the advantage obtained in terms of cumulative reward by taking an action \nfrom state  and then following the current policy, relative to following the policy starting from state . While this background suffices for the discussion in this paper, we urge the reader to refer to Weng [2018  ###reference_b169###] for a more in-depth explanation of the topic."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Misalignment due to Imperfect Rewards",
            "text": "In practice, a KL penalty  with some weight  is added to the PPO training objective.\nThis can be interpreted either as a regularizer or a prior which helps prevent overoptimization of an imperfect reward model.\nUsing a reward model ,\nthe policy at convergence learnt by training with the updated PPO objective can expressed directly as a function of the reward Scheurer et al. [2023  ###reference_b143###]; Rafailov et al. [2023  ###reference_b129###] as,\nwhere  is the weight on the KL penalty.\nLet  be the set of contexts in .444Note that  is the same as . The subscript is used for clarity under the current context.\nAfter training,  must generate desirable (most rewarding) outputs when prompted with .\nBut for out-of-distribution contexts, where the reward estimation may be erroneous, the output distribution of  may be arbitrarily misaligned with human preferences and generate undesirable output.\nThis misalignment can be quantified by comparing against the policy trained with the oracular reward.\nThe set of contexts on which the performance of  is evaluated is denoted by  with  being the distribution over those contexts. Let  be the set of contexts in the evaluation set that are not present in . The performance of  is given by:\nwhere (a) is permitted\nby the following:  and (b) follows from Equation (17  ###reference_###).\nLet  be the policy trained using the oracular reward with RLHF. It can be expressed as:\nThe performance of  can be written as:\nThe performance gap  caused by the imperfections in the reward model can be quantified as,\nFor out-of-distribution contexts and outputs, the reward model is known to misgeneralize. The performance gap increases with increasing discrepancy from the oracular reward, and the discrepancy is further weighted by the likelihood of that  pair and its oracular reward value.\nSome observations from the above analysis:\nassigns high probability to highly rewarding outputs (Equation (17  ###reference_###)), which is beneficial in-distribution contexts but can be harmful for out-of-distribution contexts when the reward model is erroneous.\nThe deviation of the estimated reward from the oracular reward on unseen contexts exacerbates misalignment, which can be mitigated by increasing the weight on the KL penalty due to the  dependence in the exponent.\nHowever, there is a trade-off. Increasing the value of  results in  and  being closer to  and have a lowered performance\u2014due to increased weight in the KL penalty."
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "Why use Reinforcement Learning Algorithms?",
            "text": "The efficacy of RLHF heavily relies on the quality of the reward model, and thus a large fraction of future research must focus on improving the reward model.\nBefore allocating resources to that effort, it is essential to evaluate the merits and downsides of employing reinforcement learning as the fine-tuning paradigm.\nIn comparison to supervised learning as an alternative approach, examining the gradient updates of a (vanilla) policy gradient algorithm alongside those of a supervised learning algorithm (such as supervised fine-tuning) offers some insights.\nIn supervised fine-tuning (SFT), supervision is provided with positive samples, and the language model is updated to increase the likelihood of those samples under the model. Notably, there is no supervision provided for neutral or undesirable outputs, although it is a feasible option.\nGiven the optimal policy  (which may be a human expert), the objective of SFT is,\nand thus the gradients used to update the parameters of the language model are of the form:\nThis is analogous to behavior cloning in RL Pomerleau [1988  ###reference_b126###  ###reference_b126###] which is known to struggle when faced with out-of-distribution inputs.\nThe primary benefit that reinforcement learning algorithms provide is that they allow the language model to explore the output space.\nThrough its decoding algorithm, the language model exercises control over the distribution of outputs on which feedback is acquired.\nThis facilitates learning from both positive as well as negative feedback,\ni.e.,\nand the (vanilla) policy gradient update is:\nAs highlighted in color, in SFT, the gradient is estimated only from the positive samples, while in RL, it is computed for all samples (positive, negative, or neutral) weighted by their corresponding rewards.\nThe gradient updates in RL are more informative, leading to better generalization for the language model and improved sample efficiency.\nBeyond exploration and richer gradients, the field of inverse reinforcement learning provides a natural formulation for training a language model with human feedback Arora and Doshi [2021  ###reference_b7###  ###reference_b7###].\nIn the following sections, we present a review of works that lead up to and are being rapidly added to this active area of research.\nThis review provides context for the first half of this work and also serves as a comprehensive introduction for readers interested in getting started and understanding the topic of RLHF for language models."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Review of Reinforcement Learning from Human Feedback for Language Models",
            "text": "To address the shortcomings faced by PLMs, a straightforward approach is to fine-tune them on a set of high-quality downstream datasets that are indicative of the intended task and behavior. For example, for instruction-following, human annotations can be collected on a set of input prompts, or input instances of existing public datasets can be re-formatted for instruction-following format. The model is then simply fine-tuned on these human demonstrations, often with the same pretraining objective.\nThis increases the likelihood of generating desirable text and makes the model less biased and harmful.\nNonetheless, in order to generate high-quality text, it is crucial to note that the task of distinguishing between high and low-quality text is inherently subjective and challenging, with end users being humans. Thus, quality assessment rests on human judgment and varies significantly based on the individual evaluator\u2019s perspective Yi et al. [2019  ###reference_b184###]; Fan et al. [2022  ###reference_b41###]; Ziegler et al. [2019  ###reference_b197###]. Incorporating human feedback into such a process can be challenging, and collecting high-quality human demonstrations can be expensive and not scalable.\nforked edges,\nfor tree=\ngrow=east,\nreversed=true,\nanchor=base west,\nparent anchor=east,\nchild anchor=west,\nbase=left,\nfont=,\nrectangle,\ndraw=hidden-draw,\nrounded corners,\nalign=left,\nminimum width=4em,\nedge+=darkgray, line width=1pt,\ns sep=3pt,\ninner xsep=2pt,\ninner ysep=3pt,\nver/.style=rotate=90, child anchor=north, parent anchor=south, anchor=center,\n,\nwhere level=1text width=3em,font=,,\nwhere level=2text width=5.6em,font=,,\nwhere level=3text width=5.5em,font=,,\nwhere level=4text width=6.6em,font=,,\n[\nReinforcement Learning from Human Feedback in Language Models, ver\n[\nPretrained \nLanguage \nModels \n(\u00a77.1  ###reference_###)\n[\nGPT-3 Brown et al. [2020  ###reference_b24###],\nPALM Chowdhery et al. [2022  ###reference_b29###],\nOPT Zhang et al. [2022  ###reference_b187###], \nLLaMA Touvron et al. [2023a  ###reference_b163###]\n, leaf, text width=25em\n]\n]\n[\nHuman \nFeedback \n(\u00a77.3  ###reference_###)\n[\nPreference Based\n[\nKreutzer et al. [2018b  ###reference_b76###],\nInstructGPT Ouyang et al. [2022  ###reference_b119###],\nBai et al. [2022a  ###reference_b12###], \nSparrow Glaese et al. [2022  ###reference_b49###]\n, leaf, text width=25em\n]\n]\n[\nRating Based\n[\nKreutzer et al. [2018a  ###reference_b75###],\nLiu et al. [2018  ###reference_b88###],\nFan et al. [2022  ###reference_b41###]\n, leaf, text width=25em\n]\n]\n[\nLanguage Based\n[\nLi et al. [2016  ###reference_b82###],\nScheurer et al. [2023  ###reference_b143###],\nNguyen et al. [2021  ###reference_b114###], leaf, text width=25em\n]\n]\n[\nMiscellaneous \nFeedback\n[\nSparrow Glaese et al. [2022  ###reference_b49###],\nUesato et al. [2022  ###reference_b165###],\nWu et al. [2023c  ###reference_b174###],\n, leaf, text width=25em\n]\n]\n]\n[\nSupervised \nFine-Tuning \n(\u00a77.4  ###reference_###)\n[\nWei et al. [2021  ###reference_b168###],\nZhou et al. [2023  ###reference_b192###],\nChiang et al. [2023  ###reference_b27###]\n, leaf, text width=25em\n]\n]\n[\nReward \nModels\n[\nRL-Training \n(\u00a77.6  ###reference_###)\n[\nAlgorithm \n\n]\n[\nActor-critic \nBahdanau et al. [2016  ###reference_b11###]\nNguyen et al. [2017  ###reference_b113###]\n[\nSparrow Glaese et al. [2022  ###reference_b49###],\nGopherCite Menick et al. [2022  ###reference_b103###],\nPerez et al. [2022  ###reference_b124###]\n, leaf, text width=25em\n]\n]\n[\nOthers\n[\nRamamurthy et al. [2022  ###reference_b133###],\nScheurer et al. [2023  ###reference_b143###],\nMunos et al. [2023  ###reference_b105###]\n, leaf, text width=25em\n]\n]\n]\n[\nTask\n[\nTranslation\n[\nNguyen et al. [2017  ###reference_b113###],\nKreutzer et al. [2018a  ###reference_b75###],\nKiegeland and Kreutzer [2021  ###reference_b68###]\n, leaf, text width=25em\n]\n]\n[\nSummarization\n[\nStiennon et al. [2020  ###reference_b156###],\nNguyen et al. [2022  ###reference_b112###],\nZiegler et al. [2019  ###reference_b197###],\n, leaf, text width=25em\n]\n]\n[\nDialogue\n[\nInstructGPT Ouyang et al. [2022  ###reference_b119###],\nBai et al. [2022a  ###reference_b12###],\nNano Fan et al. [2022  ###reference_b41###],\n, leaf, text width=25em\n]\n]\n[\nCiting \nAnswers\n[\nMenick et al. [2022  ###reference_b103###],\nNakano et al. [2021  ###reference_b108###]\n, leaf, text width=25em\n]\n]\n]\n]\n[\nNon-RL \nTraining\n[\nDong et al. [2023  ###reference_b38###],\nYuan et al. [2023  ###reference_b186###],\nScheurer et al. [2022  ###reference_b142###]\n, leaf, text width=25em\n]\n]\n]\n[\nReward \nAlternatives \n(\u00a77.9  ###reference_###)\n[\nZhao et al. [2023  ###reference_b190###],\nLiu et al. [2023a  ###reference_b90###],\nBai et al. [2022b  ###reference_b13###],\n, leaf, text width=25em\n]\n]\n]\n###figure_4### The alignment of a model with the user\u2019s intentions and preferences is critical, and incorporating human feedback in model training is a key step towards achieving this (Section 2  ###reference_###). However, the process of obtaining high-quality human feedback, particularly in the form of human demonstrations, can be a resource-intensive process, both in terms of time and cost. A more efficient approach is to collect feedback on the outputs generated by the model and train the language model to incorporate this feedback. However, collecting such a large amount of feedback is also costly and impractical for real-time/online collection during training.\nReinforcement Learning from Human Feedback (RLHF) offers a solution to these challenges. In RLHF, human feedback is collected offline and used to train a reward model. This reward model then acts as a surrogate for human feedback during training, providing reward signals to the Language Model.\nReinforcement learning algorithms form the natural candidates for training a model from scalar evaluative feedback, as provided by the reward model.\nThis forms the essence of Reinforcement Learning from Human Feedback (RLHF) Christiano et al. [2017  ###reference_b30###] as used to train Language Models. This approach is more sample-efficient and has shown more promising results compared to supervised fine-tuning alone Ouyang et al. [2022  ###reference_b119###].\nIn early works, RL has been used in training Language models across various domains such as dialogue generation Yi et al. [2019  ###reference_b184###]; Li et al. [2016  ###reference_b82###]; Jaques et al. [2019  ###reference_b62###], machine translation Kreutzer et al. [2018a  ###reference_b75###]; Nguyen et al. [2017  ###reference_b113###]; Fernandes et al. [2022  ###reference_b42###]; Sokolov et al. [2016  ###reference_b154###], text generation  Li et al. [2017  ###reference_b83###]; Shi et al. [2018  ###reference_b148###]; Zhou and Xu [2020  ###reference_b193###]; Ziebart et al. [2008  ###reference_b196###], semantic parsing Lawrence and Riezler [2018  ###reference_b79###], summarization Stiennon et al. [2020  ###reference_b156###]; Ziegler et al. [2019  ###reference_b197###]; Wu et al. [2021  ###reference_b171###].\nMore commonly, these methods were trained using non-differentiable automated evaluation metrics such as BLEU, ROUGE Ranzato et al. [2015  ###reference_b135###]; Shen et al. [2015  ###reference_b147###]; Keneshloo et al. [2018  ###reference_b66###], or simulated feedback Nguyen et al. [2017  ###reference_b113###].\nHowever, while the combination of RL and human feedback has been extensively studied Knox and Stone [2008  ###reference_b71###]; Christiano et al. [2017  ###reference_b30###], it is only recently that RLHF with LLMs has achieved significant success in sequence-to-sequence tasks such as Summarization Stiennon et al. [2020  ###reference_b156###]; Ziegler et al. [2019  ###reference_b197###]; Wu et al. [2021  ###reference_b171###], providing reliable answers with citations to queries Nakano et al. [2021  ###reference_b108###]; Glaese et al. [2022  ###reference_b49###], creating Helpful, Harmless and Honest dialogue agents aligned with broad human values Ouyang et al. [2022  ###reference_b119###]; Bai et al. [2022a  ###reference_b12###].\nReinforcement Learning (RL) is a learning paradigm\nfor a setting where an agent must make a sequence of decisions while interacting with an environment and obtaining evaluative feedback in the form of rewards. The agent\u2019s objective is to maximize the total reward it receives over time. In the context of language models, the agent is the language model itself, and its actions consist of generating tokens from its vocabulary. The agent\u2019s policy, which maps states to actions, is represented by the language model\u2019s parameters. The agent receives rewards from the environment, which in this case is a reward function that forms a surrogate from human feedback (Section 4  ###reference_###). The agent\u2019s objective is to optimize its actions (by updating its policy) to maximize the cumulative reward. A thorough mathematical formulation can be found Section 3  ###reference_###, and has been summarized in\nTable 2  ###reference_### and Figure 2  ###reference_###. While these details are sufficient for further discussion in the paper, we refer interested readers to Arulkumaran et al. [2017  ###reference_b8###]; Sutton and Barto [2018  ###reference_b160###] for more details about reinforcement learning.\nRLHF, as first popularized by Christiano et al. [2017  ###reference_b30###] for mastering Atari Games consists of three crucial stages.\nAn overview of standard RLHF workflow is highlighted in Figure 5  ###reference_###.\nThe first stage involves the collection of human feedback on a set of <input, output> pairs. These pairs can be sourced from existing datasets or generated by the pre-trained model for a given set of input prompts. The second stage involves learning a reward model from the collected human feedback. The reward model is trained to output a scalar reward for a given <input, output> pair, indicating the favorability of the pair. In essence, the reward model is trained to mimic human feedback, such that for a given input, desirable outputs are scored higher than undesirable outputs.\nThe final stage involves the RLHF training of the language model, where the reward model provides reward signals on model outputs, usually in the form of scalar reward. The parameters of the language model are then updated based on these reward signals using an appropriate policy-gradient RL algorithm, updating the model to produce more rewarding outputs.\nThese stages can be performed iteratively, with the intermediately trained model generating more prompts to collect additional human feedback. This feedback is then used to train the reward model, and the process is repeated multiple times Stiennon et al. [2020  ###reference_b156###]; Bai et al. [2022a  ###reference_b12###]; Menick et al. [2022  ###reference_b103###].\nIn the following sections, we discuss each of these stages in detail. We start with Human Feedback Collection (Section 7.3  ###reference_###), followed by training the Initial Policy (Section 7.4  ###reference_###), Reward Model Training (Section 7.5  ###reference_###), and finally RLHF Training (Section 7.6  ###reference_###).\nFinally, we discuss the properties of RLHF-trained models and their limitations in Section 7.7  ###reference_###.\nThe simplest form of rating feedback is binary feedback, where the labeler is asked to provide a binary response (yes/no) to a given input Li et al. [2016  ###reference_b82###]; Scheurer et al. [2023  ###reference_b143###].\nBinary feedback is easy to collect and interpret. Some works have used binary responses to get feedback on multiple questions (such as if the generated text is coherent) Yi et al. [2019  ###reference_b184###].\nA richer form of feedback is to ask labelers to provide a rating on a scale.\nThe scale can be continuous Graham et al. [2013  ###reference_b50###], or be similar to Likert Scale Likert [1932  ###reference_b86###] (where user rate using an integer from 1 to k) Kreutzer et al. [2018a  ###reference_b75###]; Jaques et al. [2019  ###reference_b62###]. A different variant of rating feedback is to provide categorical feedback such as \u2018incorrect\u2019, \u2018partially-correct\u2019, and \u2018correct\u2019 Gao et al. [2023  ###reference_b47###].\nWhile rating feedback is easy to specify, often inter-annotator agreement is low because of the subjective nature of the task Kreutzer et al. [2018b  ###reference_b76###]. Further, the order of examples presented to the annotator may bias the results Yannakakis and Hallam [2011  ###reference_b183###].\nMoreover, it is challenging to differentiate between data points with outputs of similar quality since feedback is provided individually to each output without comparison.\nRanking feedback or Preference-based feedback has been extensively used in the recent development of AI assistants and found to be both convenient to collect and performative. Specifically, the labeler is offered with binary Stiennon et al. [2020  ###reference_b156###] or multiple choice options Ziegler et al. [2019  ###reference_b197###], and asked to select the most appropriate response based on a certain set of instructions (directions).\nRecently, Zhu et al. [2023a  ###reference_b194###] has shown convergence guarantees for reward models trained using this feedback form.\nMoreover, given an input prompt, it is common to ask labelers to rank k (> 2) generated responses, which are then repurposed as pairwise comparisons for the reward model Ouyang et al. [2022  ###reference_b119###].\nHowever, collecting pairwise feedback might still be difficult for near similar responses and may result in much time spent by the labelers even on single input Scheurer et al. [2023  ###reference_b143###].\nAdditionally, preference-based feedback provides a very sparse signal, conveying limited information about the reasoning behind the provided feedback. Moreover, it is provided only on the complete text generated by the model (trajectory) and not on specific parts of the text (particular state) Pang et al. [2022  ###reference_b120###]; Lewis et al. [2017  ###reference_b81###].\nMoreover, preference-based feedback provides no further improvement in terms of inter-annotator agreement when compared to rating feedback Kreutzer et al. [2018b  ###reference_b76###].\nA more informative way to provide feedback is in free-form language. This provides a dense reward signal, specifying more precisely where the model goes wrong or needs improvement.\nFor example, consider the case where the output generated by the model is \u201cA humorous story about a specific profession involving person A and person B.\u201c The previous feedback forms would provide only sparse signals, such as indicating that the output is inappropriate. However, this feedback alone will not help the model identify the cause of inappropriateness, and the single example alone can imply that the text is inappropriate because: \u201cit is wrong to create humor in general,\u201c \u201cit is wrong to create humor about specific professions\u201c or \u201cit is wrong to involve individuals in humorous stories\u201c and so on. On the other hand, free-form feedback can provide more precise feedback, such as \u201cIt is inappropriate to create humor that targets specific professions.\u201c This enables the model to understand the issue from a single example better and generalize to similar cases without learning from more examples.\nLanguage Feedback has been extensively used in various domains such as Dialogue models Li et al. [2016  ###reference_b82###]; Hancock et al. [2019  ###reference_b55###], Summarization Scheurer et al. [2023  ###reference_b143###], Question-Answering Li et al. [2022  ###reference_b84###], Code generation Chen [2023  ###reference_b26###]. Recently, Scheurer et al. [2023  ###reference_b143###] has shown that language feedback is more effective than preference-based feedback in the context of summarization systems.\nAlso, as Hancock et al. [2019  ###reference_b55###] discusses, getting preference-based feedback is plausible for paid labelers but not for real users using real deployed systems. Real users interact with the system through free-form language; hence, getting human feedback in the free-form language is more natural.\nAlthough task-dependent, Scheurer et al. [2023  ###reference_b143###] further find that labelers take only 3x times to provide language feedback compared to preference-based feedback, despite providing much granular information.\nHowever, incorporating language feedback in the RLHF pipeline is not straightforward, and there has been limited work in this direction.\nApart from providing single feedback, methods have experimented with using a combination of feedback types or altogether different types. For example, Glaese et al. [2022  ###reference_b49###] uses a combination of rule violation feedback (binary), preference-based feedback, and rating of evidence.\nUesato et al. [2022  ###reference_b165###]; Korbak et al. [2023  ###reference_b74###] provide segment-level feedback instead of the whole text, and Wu et al. [2023c  ###reference_b174###] provide feedback at the token level. Moreover, some studies employ indirect methods for collecting feedback. For example, Kreutzer et al. [2018a  ###reference_b75###] uses human interactions on translated eBay titles to find more preferred translations.\nFurther, it is also possible to provide computational feedback, for example, from automated metrics Bahdanau et al. [2016  ###reference_b11###], forms of synthetic feedback Kim et al. [2023  ###reference_b69###]; Black et al. [2023  ###reference_b21###], web descriptions Hanjie et al. [2022  ###reference_b56###]; Aggarwal et al. [2023  ###reference_b1###], LLM generated feeedback Shinn et al. [2023  ###reference_b149###]; Madaan et al. [2023  ###reference_b96###]; Yang et al. [2022a  ###reference_b181###], which might, in turn, be generated based on certain human requisites or instructions Bai et al. [2022b  ###reference_b13###]; Sun et al. [2023  ###reference_b158###]; Kundu et al. [2023  ###reference_b77###]. However, these methods still use little to no human feedback and may have several unexplored limitations such as instability and lack of robustness Shumailov et al. [2023  ###reference_b150###]; Alemohammad et al. [2023  ###reference_b3###]; Gudibande et al. [2023  ###reference_b52###] and are not the focus of this survey. We refer readers to Fernandes et al. [2023  ###reference_b43###] for discussion on different type of feedback used in Natural Language Generation.\nWhile finetuning on supervised data enhances the model beyond its pretrained version in following instructions and intended tasks, it suffers from numerous limitations.\nFor instance, it does not penalize the model for hallucinating or permit it to learn from neutral or negative feedback. This can lead to harmful and unintended behavior, making it easier to prompt such models to elicit them Ganguli et al. [2022  ###reference_b46###]; Perez et al. [2022  ###reference_b124###].\nFurthermore, behavior cloning is likely to perform poorly in out-of-distribution prompts Pomerleau [1988  ###reference_b126###]. These limitations may stem from the fact that during behavior cloning, the model is not allowed to explore the vast space of possible actions, i.e., the model is not allowed to generate outputs that are not present in the demonstrations and, in turn, get feedback for them. We refer readers to Section 6.3  ###reference_### for theoretical discussion on the limitations of SFT.\nDespite its caveats, supervised fine-tuning plays a pivotal role in RLHF as it provides a robust initial policy, which allows RLHF methods to work well.\nFrom an RL perspective, learning algorithms such as the widely used Proximal Policy Optimization (PPO) in training sequence-to-sequence models, struggle to improve from poor initializations, especially when the action space is large, as in the case of text generation. This is because these methods use model-based exploration, which is ineffective when the transition probabilities over many actions are similar Nguyen et al. [2017  ###reference_b113###] i.e., different text outputs have similar probabilities of generation.\nFurthermore, as we discuss in Section 7.6  ###reference_###, usually a KL penalty is applied to ensure the output text generated by our RL-tuned model is close to the initial model. Thus, during RL training, it is preferable to start with an initial model that already generates decent-quality text.\nEmpirical studies have demonstrated that starting with fine-tuning on high-quality human demonstrations results in significant improvements over starting with pretrained language models Stiennon et al. [2020  ###reference_b156###]; Ouyang et al. [2022  ###reference_b119###]. For instance, InstructGPT collects API customer and labeler written prompts and outputs to fine-tune their model before initiating with the RLHF training Ouyang et al. [2022  ###reference_b119###].\nGlaese et al. [2022  ###reference_b49###] has also shown that starting with a prompted model (dialogue) instead of fine-tuning on label demonstrations is possible. However, they start with a large model (70B parameters) and do not perform a comparative study starting with fine-tuning on human demonstrations. Thus, it cannot be definitively concluded that starting with a prompted model is equivalent to fine-tuning on human demonstrations. Moreover, prompting has the limitation of using up a major portion of the context length of the model, which, apart from the computational burden, can also be crucial for some tasks because of limited context length. Askell et al. [2021  ###reference_b9###] propose using context distillation by training the model to generate output similar to its prompted counterpart using KL divergence loss. They find similar performance to the prompted model, and the method has been used in their subsequent works Bai et al. [2022a  ###reference_b12###].\nIn conclusion, while supervised fine-tuning can be utilized independently of the RLHF pipeline, it still suffers from several significant limitations. However, it still serves as an integral step in the RLHF pipeline, providing a robust initial policy crucial for subsequent RL training.\nAfter the collection of human feedback, the next challenge is training the language model effectively. Although supervised fine-tuning offers a straightforward method, its effectiveness is limited by the volume of human feedback.\nIn contrast, RLHF introduces a reward model to emulate human feedback, thereby acting as a stand-in for the true reward function, i.e., the actual human feedback. This reward model, usually much smaller than the language model, facilitates fine-tuning the language model using feedback generated by it on new model outputs, avoiding the need for additional costly human annotation. In practice, using a reward model over supervised fine-tuning has been found more data-efficient Ramamurthy et al. [2022  ###reference_b133###].\nThe reward model is a fine-tuned language model that assigns a scalar reward score to an input-output pair. The last embedding layer is replaced with a single projection layer that outputs this scalar reward.\nWhile the reward model can learn from various types of feedback, recent studies highlight the simplicity and effectiveness of preference-based feedback Ouyang et al. [2022  ###reference_b119###]; Bai et al. [2022a  ###reference_b12###].\nThis approach involves fine-tuning the initialized reward model to predict the preference between two trajectories (output text) given the same input prompt or context. The reward is typically modeled as a Bradley-Terry-Luce (BTL) model Bradley and Terry [1952  ###reference_b22###], where the probability of preferring one trajectory over another is a function of the difference in their reward scores.\nMathematically, this can be represented as:\nwhere  is the sigmoid function, o and o\u2019 represent the two trajectories, and their rewards are represented as  and  respectively.\nThis form of reward modeling has been found to provide smoother rewards and is less noisy Christiano et al. [2017  ###reference_b30###]. A similar method can then be used for ranking between k trajectories (k > 2), where the reward is modeled as a Plackett-Luce (PL) model Plackett [1975  ###reference_b125###]; Luce [1979  ###reference_b95###]. Moreover, Zhu et al. [2023a  ###reference_b194###] provides theoretical proof of convergence guarantees under the Maximum Likelihood estimate of both BTL and PL models.\nThe size and initialization of the reward model are critical determinants of its performance.\nWhile smaller reward models are easier to train, scaling laws suggest that larger models yield better agreement with actual human preferences Askell et al. [2021  ###reference_b9###].\nHowever, Ouyang et al. [2022  ###reference_b119###] found that training very large reward models can be unstable and result in overfitting. Instead, they report good performance even when using a reward model that is 30 times smaller than the policy model.\nRegarding initialization, multiple methods have been proposed. While Ziegler et al. [2019  ###reference_b197###] fine-tunes a pretrained language model on preference data collected on model-generated outputs, Ouyang et al. [2022  ###reference_b119###] trains a GPT-3 based reward model on publicly available datasets. However, only a slight advantage was found over using pretrained language models or supervised-fine-tuned models.\nLeveraging publicly available preference datasets (such as ranked answers from StackOverflow), as suggested by Askell et al. [2021  ###reference_b9###], notably enhances reward model performance, especially for smaller models and datasets.\nThe reward model is initially trained on a selected set of input prompts and corresponding initial model outputs. As the model training progresses, it is crucial for the reward model to generalize to new model outputs and potentially new input prompts. We refer readers to Section 5.1  ###reference_### for a deeper theoretical exploration of this aspect.\nRegarding the generalization capabilities of reward models, Ouyang et al. [2022  ###reference_b119###] presents findings that demonstrate high generalization to held-out test labelers. This capability is of paramount importance since a majority of the inputs encountered during language model training would be out-of-distribution w.r.t. the reward model training phase. Generalization capability depends on various factors such as the dataset\u2019s size, the amount of noise in the feedback dataset, and the characteristics of the pretrained reward model.\nMoreover, the robustness and calibration of the reward models with respect to actual human preferences are essential for their effectiveness. A well-calibrated reward model should accurately predict the probability of a human preferring one output over another.\nBai et al. [2022a  ###reference_b12###] discovered that when training solely on a helpfulness feedback dataset, their model exhibits strong calibration. However, when trained on a mixture of helpfulness and harmlessness datasets, the model is underconfident in its predictions. To assess robustness, a common practice involves evaluating the policy model trained using the reward model.\nTo assess robustness, a common practice involves evaluating the policy model trained using the reward model.\nInterestingly, Bai et al. [2022a  ###reference_b12###] discerned that smaller reward models and higher rewards correlate with decreased robustness. This phenomenon arises from the reward model\u2019s initial training on model outputs with naturally low rewards. To address this distribution shift, an approach involving iterated training of the reward model is proposed (see Section 7.6  ###reference_###). In summation, the discussion underscores that the trained reward model on preferences is an imperfect proxy of human feedback, especially in out-of-domain cases.\nApart from providing a single scalar reward at the end of a trajectory (complete text output), several methods model a more fine-grained approach. Uesato et al. [2022  ###reference_b165###]; Korbak et al. [2023  ###reference_b74###] provides a segment-level reward during training, a method also known as process supervision. Interestingly, while Uesato et al. [2022  ###reference_b165###] did not find any major downstream performance improvement with their method, Lightman et al. [2023  ###reference_b85###] used similar methodology but instead trained larger models on a larger feedback dataset coupled with evaluation on a more difficult task found segment-level feedback to be significantly more useful. Scheurer et al. [2023  ###reference_b143###] uses language feedback from another LLM that implicitly acts like a reward model for the training of the policy model.\nWhile ideally, as discussed in Section 4  ###reference_###, the reward model provides a dual-purpose reward taking into account both the task information (eg, summarization task) and the task-specific evaluation ((a condescending summary is rewarded less than a neutral summary). However, diversifying the approach, some strategies involve the use of multiple reward models, each specializing in distinct characteristics or specific tasks. Wu et al. [2023c  ###reference_b174###]; Ram\u00e9 et al. [2023  ###reference_b134###] demonstrate the efficacy of training separate reward models for specific attributes such as coherency and factuality. Similarly, Glaese et al. [2022  ###reference_b49###] introduces two reward models\u2014one for preference and another for rule violation in dialogue generation. They found using two models over one to be more effective, likely because of a smaller feedback dataset.\nFurther, since the preference-based reward model provides a delayed reward (reward is provided only at the end of the whole trajectory), the A2C algorithm, when used for sequence modeling Bahdanau et al. [2016  ###reference_b11###] proposes potential-based reward shaping, where intermediate generations (states) are also rewarded.\nIn conclusion, the reward modeling process is a critical component of RLHF which involves the training of a model to emulate human feedback, thereby acting as a surrogate for the true reward function. The size, initialization, and generalization capabilities of the reward model are all crucial factors that influence its performance. The reward model must be robust, well-calibrated, and additionally can provide more fine-grained feedback to the policy model training.\nDespite the impressive results achieved by RLHF in practice, it is an unstable training process Choshen et al. [2019  ###reference_b28###]. Moreover, it is highly sensitive to hyperparameters, necessitating a significant amount of hyperparameter tuning Rafailov et al. [2023  ###reference_b129###]; Yuan et al. [2023  ###reference_b186###]. Furthermore, the generalization capabilities of RLHF and other issues, such as underperformance on metrics not captured by the reward model, warrant further investigation. A comprehensive examination of these aspects is discussed in Section 7.7  ###reference_###.\nOne intriguing property, referred to as the Alignment Tax, was identified by Ouyang et al. [2022  ###reference_b119###]. The phenomenon reveals that RLHF-trained chat models sometimes perform poorly compared to initial policy in downstream tasks, suggesting a cost linked to aligning human preferences.\nTo mitigate this, they propose incorporating the pre-training objective into RLHF-finetuning, which substantially reduces the Alignment Tax.\nMoreover, Bai et al. [2022a  ###reference_b12###] indicates that larger models tend to exhibit lower alignment tax.\nBai et al. [2022a  ###reference_b12###] also observed that RLHF models better align with human preferences as the scales of both the reward model and policy model increase. It is noteworthy, however, that a similar scaling effect could be seen in instruction-finetuned SFT models. A comprehensive comparison of the scaling effects on RLHF versus SFT models is currently lacking in the literature and would make for an intriguing future study.\nRLHF models have exhibited impressive generalization capabilities beyond their training data, including generalization on new prompts and human feedback.\nFor instance, Ouyang et al. [2022  ###reference_b119###] demonstrates RLHF-tuned models answering coding questions and following instructions in multiple languages despite being finetuned only in English and with limited code-related prompts.\nThis suggests that the majority of a language model\u2019s capabilities are acquired during pre-training, and RLHF merely aligns these capabilities to elicit desired behavior.\nHowever, this generalization can be a double-edged sword, potentially leading to undesirable outcomes, especially when the feedback signal is sparse. For instance, the initial LLaMA2 Chat Model555https://huggingface.co/meta-llama/Llama-2-70b-chat-hf  ###reference_70b-chat-hf###, when prompted \"How to kill a process?\" refused to answer, drawing ethical concerns, though the intended answer was about terminating a computer process. This behavior likely stems from the model\u2019s extended generalization from examples that trained it to reject violent queries. The example further highlights the problems of imperfect rewards leading to misgeneralization, as discussed in Section 6.2  ###reference_###.\nFurther, a distributional shift between prompts used for reward model finetuning and RLHF training can result in the policy model misaligning with human preferences Bai et al. [2022a  ###reference_b12###].\nFurther, during RL training, outputs are sampled from the language model, which is evaluated using the reward model. However, deviations in parameters used for sampling outputs from the model during inference from those in training can yield poor results Ramamurthy et al. [2022  ###reference_b133###].\nAnother characteristic of RLHF models is their low entropy in output distribution Bai et al. [2022a  ###reference_b12###], which challenges generating diverse responses Kirk et al. [2023  ###reference_b70###]. This holds true for both seen and unseen datasets. To address this, entropy regularization techniques are introduced Jaques et al. [2019  ###reference_b62###]; Li et al. [2016  ###reference_b82###] to amplify diversity in the action space, albeit not always resolving the issue Raichuk et al. [2021  ###reference_b131###].\nWhile not conclusive, Bai et al. [2022a  ###reference_b12###] found that while RLHF models exhibit better sentiment towards all classes, they display similar biases to underlying LLMs when sampling with temperature < 1 (i.e., with low diversity samples). This could be attributed to their lower entropy. Furthermore, while pre-trained models often generate probabilities that are well-calibrated, RLHF models may lose this calibration. For instance, OpenAI [2023  ###reference_b117###] found that for pre-trained GPT-4, the probability of generating an answer is often directly proportional to the probability of it being correct. However, in the case of RLHF models, the distribution is skewed towards more likely answers.\nWhile RLHF aims to align language models with human preferences and intentions, reward model misalignment is frequently possible. For instance,  Singhal et al. [2023  ###reference_b153###] finds that reward models provide higher rewards to longer outputs. Further,\nIt is imperative to note that the reward model is merely an imperfect proxy for real human preferences/feedback. Due to the lack of calibration and robustness of reward models Bai et al. [2022a  ###reference_b12###], over-optimizing against the reward model can render it an ineffective measure (Goodhart\u2019s Law). This phenomenon, known as Reward Overoptimization, has been studied in the context of language models by Gao et al. [2022  ###reference_b48###]; Coste et al. [2023  ###reference_b32###].\nFurther, training RLHF models in practice is very difficult for practitioners owing to unstable training Choshen et al. [2019  ###reference_b28###], hyperparameter sensitivity Yuan et al. [2023  ###reference_b186###]; Rafailov et al. [2023  ###reference_b129###], loading multiple models leading to high memory usage Santacroce et al. [2023  ###reference_b139###]. As a result, there have been significant efforts to simplify the training process by learning directly from the available feedback using simpler supervised finetuning objectives, as we discuss in Section 7.9  ###reference_###.\nIn conclusion, while RLHF substantially enhances the performance of LLMs and aligns them with human preferences, it is not without its limitations. These include, but are not limited to, issues such as text hallucination McKenna et al. [2023  ###reference_b101###], bias and toxicity Deshpande et al. [2023a  ###reference_b34###]; Ferrara [2023  ###reference_b44###]; Gupta et al. [2023  ###reference_b53###], and the generation of harmful text when probed Perez et al. [2022  ###reference_b124###]; Wei et al. [2023  ###reference_b167###]. Despite significant improvements, these models are not fully aligned with human preferences, underscoring the need for continued research and development in this field.\nReinforcement learning (RL) has conventionally employed delayed and sparse rewards, where agents receive scalar feedback at the end of a trajectory or episode Sutton and Barto [2005  ###reference_b159###]. While this approach is straightforward to implement and aligns with the task objective, it is not without its drawbacks. Sparse rewards can lead to sample-inefficient learning due to extensive exploration requirements Bellemare et al. [2016  ###reference_b18###]. Additionally, they may result in reward hacking, where agents exploit unintended strategies to maximize rewards without solving the intended task Ibarz et al. [2018  ###reference_b60###]. Underspecified rewards, which do not fully capture the desired behavior, can also yield suboptimal or degenerate solutions Hadfield-Menell et al. [2017  ###reference_b54###].\nTo mitigate the limitations of sparse rewards, researchers have explored various methods for providing richer feedback in environments with inherently sparse rewards. These approaches include reward shaping, where the original reward signal is augmented with additional feedback Ng et al. [1999  ###reference_b110###]; Grzes [2017  ###reference_b51###]; intrinsic motivation, which encourages exploration and learning through internal rewards based on novelty, curiosity, or learning progress Oudeyer et al. [2007  ###reference_b118###]; Bellemare et al. [2016  ###reference_b18###]; Pathak et al. [2017  ###reference_b122###]; and multi-objective optimization with multiple reward signals Roijers et al. [2013  ###reference_b138###]; Roijers [2016  ###reference_b137###]. Hierarchical RL, which decomposes complex tasks into simpler subtasks with their own reward structures, has also been investigated Dietterich [1999  ###reference_b37###]; Barto and Mahadevan [2003  ###reference_b17###]. Moreover, richer forms of feedback, such as learning from corrections Jain et al. [2015  ###reference_b61###]; Bajcsy et al. [2017  ###reference_b15###], demonstrations Rengarajan et al. [2022  ###reference_b136###], and language feedback Matuszek et al. [2012  ###reference_b98###]; Fried et al. [2017  ###reference_b45###], have proven beneficial.\nCurrent RLHF pipelines for LLMs primarily rely on sparse rewards provided at the end of an episode, with reward models trained using sparse preference-based feedback. Similar challenges observed in traditional RL have also been identified in RLHF-tuned LLMs. Some progress has been made in learning from feedback for multi-objective optimization Ram\u00e9 et al. [2023  ###reference_b134###], language feedback Scheurer et al. [2022  ###reference_b142###], corrective feedback Madaan et al. [2023  ###reference_b96###]; Shinn et al. [2023  ###reference_b149###], and denser rewards Wu et al. [2023c  ###reference_b174###]. Future research should explore the integration of these techniques to address the unique challenges in training LLMs with RLHF, potentially improving generalization and robustness."
        },
        {
            "section_id": "7.1",
            "parent_section_id": "7",
            "section_name": "Language Model Pre-Training: Foundation for Large Language Models",
            "text": "Language Models (LMs) have gained significant attention in recent years due to their impressive abilities to model language and retain textual knowledge. The Transformer architecture, characterized by its use of self-attention mechanisms, has become the standard for LMs Vaswani et al. [2017  ###reference_b166###]. It is employed in a range of models, including BERT, T5, LLaMA, GPT-3, PALM, GLaM Devlin et al. [2019  ###reference_b36###]; Raffel et al. [2019  ###reference_b130###]; Touvron et al. [2023a  ###reference_b163###]; Brown et al. [2020  ###reference_b24###]; Chowdhery et al. [2022  ###reference_b29###]; Du et al. [2021  ###reference_b39###].\nPre-training has played an important role in the development of Large Language Models (LLMs), significantly contributing to their remarkable performance across a myriad of downstream tasks Brown et al. [2020  ###reference_b24###]; Chowdhery et al. [2022  ###reference_b29###]; Zhang et al. [2022  ###reference_b187###].\nThis process involves training models with an unsupervised training objective on extensive datasets, often comprised of a diverse mix of web content, literary works, scientific documents, and code repositories Rae et al. [2021  ###reference_b128###]; Xie et al. [2023  ###reference_b178###]. The scale of these datasets is critical, with studies highlighting the superior performance of smaller models trained on larger datasets Kaplan et al. [2020  ###reference_b64###]; Hoffmann et al. [2022  ###reference_b59###]; Touvron et al. [2023a  ###reference_b163###]. In addition to scale, the quality of training data, ensured through deduplication and filtering of low-quality content, is a key determinant of model performance Rae et al. [2021  ###reference_b128###]; Du et al. [2021  ###reference_b39###]; Hernandez et al. [2022  ###reference_b57###]; Lee et al. [2021  ###reference_b80###]. Masked Language Modeling (MLM) Devlin et al. [2019  ###reference_b36###] and Causal Language Modeling Radford and Narasimhan [2018  ###reference_b127###] are the most common objectives used for pretraining, with latter showing notable success in recent Large Language Model series such as GPT, PaLM, OPT Anil et al. [2023  ###reference_b5###]; OpenAI [2023  ###reference_b117###]; Zhang et al. [2022  ###reference_b187###].\nStudies demonstrate that pre-training by itself is responsible for the bulk of the observed capabilities even in downstream tasks Brown et al. [2020  ###reference_b24###]; Raffel et al. [2019  ###reference_b130###].\nThe simple pre-training objective of next, or masked, token prediction imbibes the LMs with a range of capabilities.\nThey are few-task learners, without the need for fine-tuning. This applies to a variety of tasks from text generation, reasoning, question answering, summarization, and translation to name a few.\nHowever, though scaling pretrained language models (PLMs) exhibit remarkable performance across a variety of tasks,\nthey suffer from several limitations, such as the inability to follow human instructions Ouyang et al. [2022  ###reference_b119###].\n\nThis is because PLMs suffer from objective mismatch problems (See Section 2  ###reference_###), as they are trained on generic internet data. As a result, PLMs need to learn to mimic the conflicting behavior of billions of humans.\nFurther, the Maximum Likelihood Estimate on the next token prediction for such data doesn\u2019t explicitly penalize the model for hallucinating concepts, i.e., generating concepts not encapsulated within its internal representation, and even important & unimportant errors are given equal weightage.\nMoreover, pretrained models often show unintended behavior such as generating harmful, biased, untruthful, and low-quality content Perez et al. [2022  ###reference_b124###].\nTo address the shortcomings faced by PLMs, a straightforward approach is to fine-tune them on a set of high-quality downstream datasets that are indicative of the intended task and behavior. For example, for instruction-following, human annotations can be collected on a set of input prompts, or input instances of existing public datasets can be re-formatted for instruction-following format. The model is then simply fine-tuned on these human demonstrations, often with the same pretraining objective.\nThis increases the likelihood of generating desirable text and makes the model less biased and harmful.\nNonetheless, in order to generate high-quality text, it is crucial to note that the task of distinguishing between high and low-quality text is inherently subjective and challenging, with end users being humans. Thus, quality assessment rests on human judgment and varies significantly based on the individual evaluator\u2019s perspective Yi et al. [2019  ###reference_b184###  ###reference_b184###]; Fan et al. [2022  ###reference_b41###  ###reference_b41###]; Ziegler et al. [2019  ###reference_b197###  ###reference_b197###]. Incorporating human feedback into such a process can be challenging, and collecting high-quality human demonstrations can be expensive and not scalable.\nforked edges,\nfor tree=\ngrow=east,\nreversed=true,\nanchor=base west,\nparent anchor=east,\nchild anchor=west,\nbase=left,\nfont=,\nrectangle,\ndraw=hidden-draw,\nrounded corners,\nalign=left,\nminimum width=4em,\nedge+=darkgray, line width=1pt,\ns sep=3pt,\ninner xsep=2pt,\ninner ysep=3pt,\nver/.style=rotate=90, child anchor=north, parent anchor=south, anchor=center,\n,\nwhere level=1text width=3em,font=,,\nwhere level=2text width=5.6em,font=,,\nwhere level=3text width=5.5em,font=,,\nwhere level=4text width=6.6em,font=,,\n[\nReinforcement Learning from Human Feedback in Language Models, ver\n[\nPretrained \nLanguage \nModels \n(\u00a77.1  ###reference_###  ###reference_###)\n[\nGPT-3 Brown et al. [2020  ###reference_b24###  ###reference_b24###],\nPALM Chowdhery et al. [2022  ###reference_b29###  ###reference_b29###],\nOPT Zhang et al. [2022  ###reference_b187###  ###reference_b187###], \nLLaMA Touvron et al. [2023a  ###reference_b163###  ###reference_b163###]\n, leaf, text width=25em\n]\n]\n[\nHuman \nFeedback \n(\u00a77.3  ###reference_###  ###reference_###)\n[\nPreference Based\n[\nKreutzer et al. [2018b  ###reference_b76###  ###reference_b76###],\nInstructGPT Ouyang et al. [2022  ###reference_b119###  ###reference_b119###],\nBai et al. [2022a  ###reference_b12###  ###reference_b12###], \nSparrow Glaese et al. [2022  ###reference_b49###  ###reference_b49###]\n, leaf, text width=25em\n]\n]\n[\nRating Based\n[\nKreutzer et al. [2018a  ###reference_b75###  ###reference_b75###],\nLiu et al. [2018  ###reference_b88###  ###reference_b88###],\nFan et al. [2022  ###reference_b41###  ###reference_b41###]\n, leaf, text width=25em\n]\n]\n[\nLanguage Based\n[\nLi et al. [2016  ###reference_b82###  ###reference_b82###],\nScheurer et al. [2023  ###reference_b143###  ###reference_b143###],\nNguyen et al. [2021  ###reference_b114###  ###reference_b114###], leaf, text width=25em\n]\n]\n[\nMiscellaneous \nFeedback\n[\nSparrow Glaese et al. [2022  ###reference_b49###  ###reference_b49###],\nUesato et al. [2022  ###reference_b165###  ###reference_b165###],\nWu et al. [2023c  ###reference_b174###  ###reference_b174###],\n, leaf, text width=25em\n]\n]\n]\n[\nSupervised \nFine-Tuning \n(\u00a77.4  ###reference_###  ###reference_###)\n[\nWei et al. [2021  ###reference_b168###  ###reference_b168###],\nZhou et al. [2023  ###reference_b192###  ###reference_b192###],\nChiang et al. [2023  ###reference_b27###  ###reference_b27###]\n, leaf, text width=25em\n]\n]\n[\nReward \nModels\n[\nRL-Training \n(\u00a77.6  ###reference_###  ###reference_###)\n[\nAlgorithm \n\n]\n[\nActor-critic \nBahdanau et al. [2016  ###reference_b11###  ###reference_b11###]\nNguyen et al. [2017  ###reference_b113###  ###reference_b113###]\n[\nSparrow Glaese et al. [2022  ###reference_b49###  ###reference_b49###],\nGopherCite Menick et al. [2022  ###reference_b103###  ###reference_b103###],\nPerez et al. [2022  ###reference_b124###  ###reference_b124###]\n, leaf, text width=25em\n]\n]\n[\nOthers\n[\nRamamurthy et al. [2022  ###reference_b133###  ###reference_b133###],\nScheurer et al. [2023  ###reference_b143###  ###reference_b143###],\nMunos et al. [2023  ###reference_b105###  ###reference_b105###]\n, leaf, text width=25em\n]\n]\n]\n[\nTask\n[\nTranslation\n[\nNguyen et al. [2017  ###reference_b113###  ###reference_b113###],\nKreutzer et al. [2018a  ###reference_b75###  ###reference_b75###],\nKiegeland and Kreutzer [2021  ###reference_b68###  ###reference_b68###]\n, leaf, text width=25em\n]\n]\n[\nSummarization\n[\nStiennon et al. [2020  ###reference_b156###  ###reference_b156###],\nNguyen et al. [2022  ###reference_b112###  ###reference_b112###],\nZiegler et al. [2019  ###reference_b197###  ###reference_b197###],\n, leaf, text width=25em\n]\n]\n[\nDialogue\n[\nInstructGPT Ouyang et al. [2022  ###reference_b119###  ###reference_b119###],\nBai et al. [2022a  ###reference_b12###  ###reference_b12###],\nNano Fan et al. [2022  ###reference_b41###  ###reference_b41###],\n, leaf, text width=25em\n]\n]\n[\nCiting \nAnswers\n[\nMenick et al. [2022  ###reference_b103###  ###reference_b103###],\nNakano et al. [2021  ###reference_b108###  ###reference_b108###]\n, leaf, text width=25em\n]\n]\n]\n]\n[\nNon-RL \nTraining\n[\nDong et al. [2023  ###reference_b38###  ###reference_b38###],\nYuan et al. [2023  ###reference_b186###  ###reference_b186###],\nScheurer et al. [2022  ###reference_b142###  ###reference_b142###]\n, leaf, text width=25em\n]\n]\n]\n[\nReward \nAlternatives \n(\u00a77.9  ###reference_###  ###reference_###)\n[\nZhao et al. [2023  ###reference_b190###  ###reference_b190###],\nLiu et al. [2023a  ###reference_b90###  ###reference_b90###],\nBai et al. [2022b  ###reference_b13###  ###reference_b13###],\n, leaf, text width=25em\n]\n]\n]\n###figure_5###"
        },
        {
            "section_id": "7.2",
            "parent_section_id": "7",
            "section_name": "Reinforcement Learning from Human Feedback (RLHF): Overview and Motivation",
            "text": "The alignment of a model with the user\u2019s intentions and preferences is critical, and incorporating human feedback in model training is a key step towards achieving this (Section 2  ###reference_###  ###reference_###). However, the process of obtaining high-quality human feedback, particularly in the form of human demonstrations, can be a resource-intensive process, both in terms of time and cost. A more efficient approach is to collect feedback on the outputs generated by the model and train the language model to incorporate this feedback. However, collecting such a large amount of feedback is also costly and impractical for real-time/online collection during training.\nReinforcement Learning from Human Feedback (RLHF) offers a solution to these challenges. In RLHF, human feedback is collected offline and used to train a reward model. This reward model then acts as a surrogate for human feedback during training, providing reward signals to the Language Model.\nReinforcement learning algorithms form the natural candidates for training a model from scalar evaluative feedback, as provided by the reward model.\nThis forms the essence of Reinforcement Learning from Human Feedback (RLHF) Christiano et al. [2017  ###reference_b30###  ###reference_b30###] as used to train Language Models. This approach is more sample-efficient and has shown more promising results compared to supervised fine-tuning alone Ouyang et al. [2022  ###reference_b119###  ###reference_b119###].\nIn early works, RL has been used in training Language models across various domains such as dialogue generation Yi et al. [2019  ###reference_b184###  ###reference_b184###]; Li et al. [2016  ###reference_b82###  ###reference_b82###]; Jaques et al. [2019  ###reference_b62###  ###reference_b62###], machine translation Kreutzer et al. [2018a  ###reference_b75###  ###reference_b75###]; Nguyen et al. [2017  ###reference_b113###  ###reference_b113###]; Fernandes et al. [2022  ###reference_b42###  ###reference_b42###]; Sokolov et al. [2016  ###reference_b154###  ###reference_b154###], text generation  Li et al. [2017  ###reference_b83###  ###reference_b83###]; Shi et al. [2018  ###reference_b148###  ###reference_b148###]; Zhou and Xu [2020  ###reference_b193###  ###reference_b193###]; Ziebart et al. [2008  ###reference_b196###  ###reference_b196###], semantic parsing Lawrence and Riezler [2018  ###reference_b79###  ###reference_b79###], summarization Stiennon et al. [2020  ###reference_b156###  ###reference_b156###]; Ziegler et al. [2019  ###reference_b197###  ###reference_b197###]; Wu et al. [2021  ###reference_b171###  ###reference_b171###].\nMore commonly, these methods were trained using non-differentiable automated evaluation metrics such as BLEU, ROUGE Ranzato et al. [2015  ###reference_b135###  ###reference_b135###]; Shen et al. [2015  ###reference_b147###  ###reference_b147###]; Keneshloo et al. [2018  ###reference_b66###  ###reference_b66###], or simulated feedback Nguyen et al. [2017  ###reference_b113###  ###reference_b113###].\nHowever, while the combination of RL and human feedback has been extensively studied Knox and Stone [2008  ###reference_b71###  ###reference_b71###]; Christiano et al. [2017  ###reference_b30###  ###reference_b30###], it is only recently that RLHF with LLMs has achieved significant success in sequence-to-sequence tasks such as Summarization Stiennon et al. [2020  ###reference_b156###  ###reference_b156###]; Ziegler et al. [2019  ###reference_b197###  ###reference_b197###]; Wu et al. [2021  ###reference_b171###  ###reference_b171###], providing reliable answers with citations to queries Nakano et al. [2021  ###reference_b108###  ###reference_b108###]; Glaese et al. [2022  ###reference_b49###  ###reference_b49###], creating Helpful, Harmless and Honest dialogue agents aligned with broad human values Ouyang et al. [2022  ###reference_b119###  ###reference_b119###]; Bai et al. [2022a  ###reference_b12###  ###reference_b12###].\nReinforcement Learning (RL) is a learning paradigm\nfor a setting where an agent must make a sequence of decisions while interacting with an environment and obtaining evaluative feedback in the form of rewards. The agent\u2019s objective is to maximize the total reward it receives over time. In the context of language models, the agent is the language model itself, and its actions consist of generating tokens from its vocabulary. The agent\u2019s policy, which maps states to actions, is represented by the language model\u2019s parameters. The agent receives rewards from the environment, which in this case is a reward function that forms a surrogate from human feedback (Section 4  ###reference_###  ###reference_###). The agent\u2019s objective is to optimize its actions (by updating its policy) to maximize the cumulative reward. A thorough mathematical formulation can be found Section 3  ###reference_###  ###reference_###, and has been summarized in\nTable 2  ###reference_###  ###reference_### and Figure 2  ###reference_###  ###reference_###. While these details are sufficient for further discussion in the paper, we refer interested readers to Arulkumaran et al. [2017  ###reference_b8###  ###reference_b8###]; Sutton and Barto [2018  ###reference_b160###  ###reference_b160###] for more details about reinforcement learning.\nRLHF, as first popularized by Christiano et al. [2017  ###reference_b30###  ###reference_b30###] for mastering Atari Games consists of three crucial stages.\nAn overview of standard RLHF workflow is highlighted in Figure 5  ###reference_###  ###reference_###.\nThe first stage involves the collection of human feedback on a set of <input, output> pairs. These pairs can be sourced from existing datasets or generated by the pre-trained model for a given set of input prompts. The second stage involves learning a reward model from the collected human feedback. The reward model is trained to output a scalar reward for a given <input, output> pair, indicating the favorability of the pair. In essence, the reward model is trained to mimic human feedback, such that for a given input, desirable outputs are scored higher than undesirable outputs.\nThe final stage involves the RLHF training of the language model, where the reward model provides reward signals on model outputs, usually in the form of scalar reward. The parameters of the language model are then updated based on these reward signals using an appropriate policy-gradient RL algorithm, updating the model to produce more rewarding outputs.\nThese stages can be performed iteratively, with the intermediately trained model generating more prompts to collect additional human feedback. This feedback is then used to train the reward model, and the process is repeated multiple times Stiennon et al. [2020  ###reference_b156###  ###reference_b156###]; Bai et al. [2022a  ###reference_b12###  ###reference_b12###]; Menick et al. [2022  ###reference_b103###  ###reference_b103###].\nIn the following sections, we discuss each of these stages in detail. We start with Human Feedback Collection (Section 7.3  ###reference_###  ###reference_###), followed by training the Initial Policy (Section 7.4  ###reference_###  ###reference_###), Reward Model Training (Section 7.5  ###reference_###  ###reference_###), and finally RLHF Training (Section 7.6  ###reference_###  ###reference_###).\nFinally, we discuss the properties of RLHF-trained models and their limitations in Section 7.7  ###reference_###  ###reference_###."
        },
        {
            "section_id": "7.3",
            "parent_section_id": "7",
            "section_name": "Human Feedback",
            "text": "In this section, we discuss the nature, objectives, and different types of human feedback, followed by the challenges and strategies associated with collecting high-quality feedback.\nThe simplest form of rating feedback is binary feedback, where the labeler is asked to provide a binary response (yes/no) to a given input Li et al. [2016  ###reference_b82###  ###reference_b82###]; Scheurer et al. [2023  ###reference_b143###  ###reference_b143###].\nBinary feedback is easy to collect and interpret. Some works have used binary responses to get feedback on multiple questions (such as if the generated text is coherent) Yi et al. [2019  ###reference_b184###  ###reference_b184###].\nA richer form of feedback is to ask labelers to provide a rating on a scale.\nThe scale can be continuous Graham et al. [2013  ###reference_b50###  ###reference_b50###], or be similar to Likert Scale Likert [1932  ###reference_b86###  ###reference_b86###] (where user rate using an integer from 1 to k) Kreutzer et al. [2018a  ###reference_b75###  ###reference_b75###]; Jaques et al. [2019  ###reference_b62###  ###reference_b62###]. A different variant of rating feedback is to provide categorical feedback such as \u2018incorrect\u2019, \u2018partially-correct\u2019, and \u2018correct\u2019 Gao et al. [2023  ###reference_b47###  ###reference_b47###].\nWhile rating feedback is easy to specify, often inter-annotator agreement is low because of the subjective nature of the task Kreutzer et al. [2018b  ###reference_b76###  ###reference_b76###]. Further, the order of examples presented to the annotator may bias the results Yannakakis and Hallam [2011  ###reference_b183###  ###reference_b183###].\nMoreover, it is challenging to differentiate between data points with outputs of similar quality since feedback is provided individually to each output without comparison.\nRanking feedback or Preference-based feedback has been extensively used in the recent development of AI assistants and found to be both convenient to collect and performative. Specifically, the labeler is offered with binary Stiennon et al. [2020  ###reference_b156###  ###reference_b156###] or multiple choice options Ziegler et al. [2019  ###reference_b197###  ###reference_b197###], and asked to select the most appropriate response based on a certain set of instructions (directions).\nRecently, Zhu et al. [2023a  ###reference_b194###  ###reference_b194###] has shown convergence guarantees for reward models trained using this feedback form.\nMoreover, given an input prompt, it is common to ask labelers to rank k (> 2) generated responses, which are then repurposed as pairwise comparisons for the reward model Ouyang et al. [2022  ###reference_b119###  ###reference_b119###].\nHowever, collecting pairwise feedback might still be difficult for near similar responses and may result in much time spent by the labelers even on single input Scheurer et al. [2023  ###reference_b143###  ###reference_b143###].\nAdditionally, preference-based feedback provides a very sparse signal, conveying limited information about the reasoning behind the provided feedback. Moreover, it is provided only on the complete text generated by the model (trajectory) and not on specific parts of the text (particular state) Pang et al. [2022  ###reference_b120###  ###reference_b120###]; Lewis et al. [2017  ###reference_b81###  ###reference_b81###].\nMoreover, preference-based feedback provides no further improvement in terms of inter-annotator agreement when compared to rating feedback Kreutzer et al. [2018b  ###reference_b76###  ###reference_b76###].\nA more informative way to provide feedback is in free-form language. This provides a dense reward signal, specifying more precisely where the model goes wrong or needs improvement.\nFor example, consider the case where the output generated by the model is \u201cA humorous story about a specific profession involving person A and person B.\u201c The previous feedback forms would provide only sparse signals, such as indicating that the output is inappropriate. However, this feedback alone will not help the model identify the cause of inappropriateness, and the single example alone can imply that the text is inappropriate because: \u201cit is wrong to create humor in general,\u201c \u201cit is wrong to create humor about specific professions\u201c or \u201cit is wrong to involve individuals in humorous stories\u201c and so on. On the other hand, free-form feedback can provide more precise feedback, such as \u201cIt is inappropriate to create humor that targets specific professions.\u201c This enables the model to understand the issue from a single example better and generalize to similar cases without learning from more examples.\nLanguage Feedback has been extensively used in various domains such as Dialogue models Li et al. [2016  ###reference_b82###  ###reference_b82###]; Hancock et al. [2019  ###reference_b55###  ###reference_b55###], Summarization Scheurer et al. [2023  ###reference_b143###  ###reference_b143###], Question-Answering Li et al. [2022  ###reference_b84###  ###reference_b84###], Code generation Chen [2023  ###reference_b26###  ###reference_b26###]. Recently, Scheurer et al. [2023  ###reference_b143###  ###reference_b143###] has shown that language feedback is more effective than preference-based feedback in the context of summarization systems.\nAlso, as Hancock et al. [2019  ###reference_b55###  ###reference_b55###] discusses, getting preference-based feedback is plausible for paid labelers but not for real users using real deployed systems. Real users interact with the system through free-form language; hence, getting human feedback in the free-form language is more natural.\nAlthough task-dependent, Scheurer et al. [2023  ###reference_b143###  ###reference_b143###] further find that labelers take only 3x times to provide language feedback compared to preference-based feedback, despite providing much granular information.\nHowever, incorporating language feedback in the RLHF pipeline is not straightforward, and there has been limited work in this direction.\nApart from providing single feedback, methods have experimented with using a combination of feedback types or altogether different types. For example, Glaese et al. [2022  ###reference_b49###  ###reference_b49###] uses a combination of rule violation feedback (binary), preference-based feedback, and rating of evidence.\nUesato et al. [2022  ###reference_b165###  ###reference_b165###]; Korbak et al. [2023  ###reference_b74###  ###reference_b74###] provide segment-level feedback instead of the whole text, and Wu et al. [2023c  ###reference_b174###  ###reference_b174###] provide feedback at the token level. Moreover, some studies employ indirect methods for collecting feedback. For example, Kreutzer et al. [2018a  ###reference_b75###  ###reference_b75###] uses human interactions on translated eBay titles to find more preferred translations.\nFurther, it is also possible to provide computational feedback, for example, from automated metrics Bahdanau et al. [2016  ###reference_b11###  ###reference_b11###], forms of synthetic feedback Kim et al. [2023  ###reference_b69###  ###reference_b69###]; Black et al. [2023  ###reference_b21###  ###reference_b21###], web descriptions Hanjie et al. [2022  ###reference_b56###  ###reference_b56###]; Aggarwal et al. [2023  ###reference_b1###  ###reference_b1###], LLM generated feeedback Shinn et al. [2023  ###reference_b149###  ###reference_b149###]; Madaan et al. [2023  ###reference_b96###  ###reference_b96###]; Yang et al. [2022a  ###reference_b181###  ###reference_b181###], which might, in turn, be generated based on certain human requisites or instructions Bai et al. [2022b  ###reference_b13###  ###reference_b13###]; Sun et al. [2023  ###reference_b158###  ###reference_b158###]; Kundu et al. [2023  ###reference_b77###  ###reference_b77###]. However, these methods still use little to no human feedback and may have several unexplored limitations such as instability and lack of robustness Shumailov et al. [2023  ###reference_b150###  ###reference_b150###]; Alemohammad et al. [2023  ###reference_b3###  ###reference_b3###]; Gudibande et al. [2023  ###reference_b52###  ###reference_b52###] and are not the focus of this survey. We refer readers to Fernandes et al. [2023  ###reference_b43###  ###reference_b43###] for discussion on different type of feedback used in Natural Language Generation."
        },
        {
            "section_id": "7.3.1",
            "parent_section_id": "7.3",
            "section_name": "7.3.1 Nature and Objectives of Human Feedback",
            "text": "Tasks such as summarization and providing helpful answers are inherently ambiguous and require human judgment to evaluate the quality of the generated text.\nAutomated metrics like BLEU and ROUGE Lin and Och [2004  ###reference_b87###] often do not correlate with human judgment Liu et al. [2016  ###reference_b89###]; Schluter [2017  ###reference_b144###]; Sellam et al. [2020  ###reference_b146###]; Stiennon et al. [2020  ###reference_b156###], making them unreliable for evaluation and training.\nThus, acquiring high-quality human feedback to align the model with human behavior becomes crucial.\nFeedback is typically provided on the outputs generated by the model (or input-output pairs from the dataset), and subsequently, the model is trained to learn from this feedback.\nHowever, capturing diverse human preferences is a challenging task.\nOne approach to encapsulate subjective human preferences is to approximate them using \u201cmodels of human behavior\u201d.\nThis concept of human behavior models has roots in diverse fields such as econometrics McFadden [1981  ###reference_b100###], psychology O\u2019Connor [1989  ###reference_b115###], and inverse reinforcement learning.\nA notable example is the Bradley-Terry model Bradley and Terry [1952  ###reference_b22###], a probabilistic model that encodes the preference of one output over another in pairwise competitions. In the context of RLHF, reward models that form surrogates for human preferences serve as such models of human behavior.\nThe type of feedback collected depends on the intended objective to be displayed by the fine-tuned language model.\nAskell et al. [2021  ###reference_b9###] proposes three objectives for an aligned Language Model: Helpfulness, Honesty, and Harmlessness (HHH). These objectives can be broadly defined as follows:\n- Helpful: A Language Model is considered helpful if it can efficiently complete tasks or answer questions (while being harmless), ask relevant follow-up questions when necessary, and appropriately redirect ill-informed requests. Helpfulness includes context-dependent aspects such as informativeness, coherence, relevance, creativity, and specificity.\n- Honest: Honesty in a Language Model implies providing accurate information, expressing appropriate levels of uncertainty, and honestly conveying its capabilities, knowledge, and internal state.\nLanguage Models are particularly susceptible to hallucination Khandelwal et al. [2019  ###reference_b67###]; Maynez et al. [2020  ###reference_b99###], making it essential to penalize such behavior. Unlike helpfulness, honesty is more objectively evaluated.\n- Harmless: A harmless Language Model should avoid offensive or biased behavior, refuse to aid in dangerous acts, recognize disguised nefarious attempts, and act with modesty and care when providing advice with potentially sensitive or consequential impacts.\nThese broad objectives, as mentioned above, encompass specific objectives, which can be considered subcategories.\nFor example, in the case of summarization, the summary should be helpful to the reader and should not contain any false or harmful information.\nSimilarly, the goal of reducing bias in a dialogue agent\u2019s responses can be considered a subset of the Harmless objective. At the same time, coherence and creativity in the generated text are aspects of being helpful.\nThese objectives are not mutually exclusive and are context and task-dependent. Even human labelers and researchers have shown disagreements in annotation Kreutzer et al. [2018b  ###reference_b76###]."
        },
        {
            "section_id": "7.3.2",
            "parent_section_id": "7.3",
            "section_name": "7.3.2 Types of Human Feedback",
            "text": "Human Feedback is usually collected on model-generated outputs. Good feedback should incorporate information on where the model output is lacking and how to improve it. A simple process is to let human labelers provide feedback on a set of model outputs generated from a dataset of prompts or inputs.\nAlternatively, existing datasets can be repurposed to incorporate implicit feedback, such as rating different user choices Kreutzer et al. [2018a  ###reference_b75###].\nRegardless of the process, human feedback can be collected in various forms, such as binary responses, preference ranking, language feedback, etc. While the choice of feedback type depends on the downstream task, it is essential to note that the feedback should be collected in a way that is easy for humans (labelers) to provide; there is high agreement among the labelers, and it is also informative.\nIn this section, we classify the feedback into four different categories: rating feedback, ranking feedback, language feedback, and miscellaneous feedback.\nThe simplest form of rating feedback is binary feedback, where the labeler is asked to provide a binary response (yes/no) to a given input Li et al. [2016  ###reference_b82###  ###reference_b82###  ###reference_b82###]; Scheurer et al. [2023  ###reference_b143###  ###reference_b143###  ###reference_b143###].\nBinary feedback is easy to collect and interpret. Some works have used binary responses to get feedback on multiple questions (such as if the generated text is coherent) Yi et al. [2019  ###reference_b184###  ###reference_b184###  ###reference_b184###].\nA richer form of feedback is to ask labelers to provide a rating on a scale.\nThe scale can be continuous Graham et al. [2013  ###reference_b50###  ###reference_b50###  ###reference_b50###], or be similar to Likert Scale Likert [1932  ###reference_b86###  ###reference_b86###  ###reference_b86###] (where user rate using an integer from 1 to k) Kreutzer et al. [2018a  ###reference_b75###  ###reference_b75###  ###reference_b75###]; Jaques et al. [2019  ###reference_b62###  ###reference_b62###  ###reference_b62###]. A different variant of rating feedback is to provide categorical feedback such as \u2018incorrect\u2019, \u2018partially-correct\u2019, and \u2018correct\u2019 Gao et al. [2023  ###reference_b47###  ###reference_b47###  ###reference_b47###].\nWhile rating feedback is easy to specify, often inter-annotator agreement is low because of the subjective nature of the task Kreutzer et al. [2018b  ###reference_b76###  ###reference_b76###  ###reference_b76###]. Further, the order of examples presented to the annotator may bias the results Yannakakis and Hallam [2011  ###reference_b183###  ###reference_b183###  ###reference_b183###].\nMoreover, it is challenging to differentiate between data points with outputs of similar quality since feedback is provided individually to each output without comparison.\nRanking feedback or Preference-based feedback has been extensively used in the recent development of AI assistants and found to be both convenient to collect and performative. Specifically, the labeler is offered with binary Stiennon et al. [2020  ###reference_b156###  ###reference_b156###  ###reference_b156###] or multiple choice options Ziegler et al. [2019  ###reference_b197###  ###reference_b197###  ###reference_b197###], and asked to select the most appropriate response based on a certain set of instructions (directions).\nRecently, Zhu et al. [2023a  ###reference_b194###  ###reference_b194###  ###reference_b194###] has shown convergence guarantees for reward models trained using this feedback form.\nMoreover, given an input prompt, it is common to ask labelers to rank k (> 2) generated responses, which are then repurposed as pairwise comparisons for the reward model Ouyang et al. [2022  ###reference_b119###  ###reference_b119###  ###reference_b119###].\nHowever, collecting pairwise feedback might still be difficult for near similar responses and may result in much time spent by the labelers even on single input Scheurer et al. [2023  ###reference_b143###  ###reference_b143###  ###reference_b143###].\nAdditionally, preference-based feedback provides a very sparse signal, conveying limited information about the reasoning behind the provided feedback. Moreover, it is provided only on the complete text generated by the model (trajectory) and not on specific parts of the text (particular state) Pang et al. [2022  ###reference_b120###  ###reference_b120###  ###reference_b120###]; Lewis et al. [2017  ###reference_b81###  ###reference_b81###  ###reference_b81###].\nMoreover, preference-based feedback provides no further improvement in terms of inter-annotator agreement when compared to rating feedback Kreutzer et al. [2018b  ###reference_b76###  ###reference_b76###  ###reference_b76###].\nA more informative way to provide feedback is in free-form language. This provides a dense reward signal, specifying more precisely where the model goes wrong or needs improvement.\nFor example, consider the case where the output generated by the model is \u201cA humorous story about a specific profession involving person A and person B.\u201c The previous feedback forms would provide only sparse signals, such as indicating that the output is inappropriate. However, this feedback alone will not help the model identify the cause of inappropriateness, and the single example alone can imply that the text is inappropriate because: \u201cit is wrong to create humor in general,\u201c \u201cit is wrong to create humor about specific professions\u201c or \u201cit is wrong to involve individuals in humorous stories\u201c and so on. On the other hand, free-form feedback can provide more precise feedback, such as \u201cIt is inappropriate to create humor that targets specific professions.\u201c This enables the model to understand the issue from a single example better and generalize to similar cases without learning from more examples.\nLanguage Feedback has been extensively used in various domains such as Dialogue models Li et al. [2016  ###reference_b82###  ###reference_b82###  ###reference_b82###]; Hancock et al. [2019  ###reference_b55###  ###reference_b55###  ###reference_b55###], Summarization Scheurer et al. [2023  ###reference_b143###  ###reference_b143###  ###reference_b143###], Question-Answering Li et al. [2022  ###reference_b84###  ###reference_b84###  ###reference_b84###], Code generation Chen [2023  ###reference_b26###  ###reference_b26###  ###reference_b26###]. Recently, Scheurer et al. [2023  ###reference_b143###  ###reference_b143###  ###reference_b143###] has shown that language feedback is more effective than preference-based feedback in the context of summarization systems.\nAlso, as Hancock et al. [2019  ###reference_b55###  ###reference_b55###  ###reference_b55###] discusses, getting preference-based feedback is plausible for paid labelers but not for real users using real deployed systems. Real users interact with the system through free-form language; hence, getting human feedback in the free-form language is more natural.\nAlthough task-dependent, Scheurer et al. [2023  ###reference_b143###  ###reference_b143###  ###reference_b143###] further find that labelers take only 3x times to provide language feedback compared to preference-based feedback, despite providing much granular information.\nHowever, incorporating language feedback in the RLHF pipeline is not straightforward, and there has been limited work in this direction.\nApart from providing single feedback, methods have experimented with using a combination of feedback types or altogether different types. For example, Glaese et al. [2022  ###reference_b49###  ###reference_b49###  ###reference_b49###] uses a combination of rule violation feedback (binary), preference-based feedback, and rating of evidence.\nUesato et al. [2022  ###reference_b165###  ###reference_b165###  ###reference_b165###]; Korbak et al. [2023  ###reference_b74###  ###reference_b74###  ###reference_b74###] provide segment-level feedback instead of the whole text, and Wu et al. [2023c  ###reference_b174###  ###reference_b174###  ###reference_b174###] provide feedback at the token level. Moreover, some studies employ indirect methods for collecting feedback. For example, Kreutzer et al. [2018a  ###reference_b75###  ###reference_b75###  ###reference_b75###] uses human interactions on translated eBay titles to find more preferred translations.\nFurther, it is also possible to provide computational feedback, for example, from automated metrics Bahdanau et al. [2016  ###reference_b11###  ###reference_b11###  ###reference_b11###], forms of synthetic feedback Kim et al. [2023  ###reference_b69###  ###reference_b69###  ###reference_b69###]; Black et al. [2023  ###reference_b21###  ###reference_b21###  ###reference_b21###], web descriptions Hanjie et al. [2022  ###reference_b56###  ###reference_b56###  ###reference_b56###]; Aggarwal et al. [2023  ###reference_b1###  ###reference_b1###  ###reference_b1###], LLM generated feeedback Shinn et al. [2023  ###reference_b149###  ###reference_b149###  ###reference_b149###]; Madaan et al. [2023  ###reference_b96###  ###reference_b96###  ###reference_b96###]; Yang et al. [2022a  ###reference_b181###  ###reference_b181###  ###reference_b181###], which might, in turn, be generated based on certain human requisites or instructions Bai et al. [2022b  ###reference_b13###  ###reference_b13###  ###reference_b13###]; Sun et al. [2023  ###reference_b158###  ###reference_b158###  ###reference_b158###]; Kundu et al. [2023  ###reference_b77###  ###reference_b77###  ###reference_b77###]. However, these methods still use little to no human feedback and may have several unexplored limitations such as instability and lack of robustness Shumailov et al. [2023  ###reference_b150###  ###reference_b150###  ###reference_b150###]; Alemohammad et al. [2023  ###reference_b3###  ###reference_b3###  ###reference_b3###]; Gudibande et al. [2023  ###reference_b52###  ###reference_b52###  ###reference_b52###] and are not the focus of this survey. We refer readers to Fernandes et al. [2023  ###reference_b43###  ###reference_b43###  ###reference_b43###] for discussion on different type of feedback used in Natural Language Generation."
        },
        {
            "section_id": "7.3.3",
            "parent_section_id": "7.3",
            "section_name": "7.3.3 Collection of High-Quality Human Feedback",
            "text": "Collecting high-quality human feedback is a challenging task that has been the focus of extensive research.\nThe quality of feedback is pivotal; subpar or noisy feedback can significantly hamper the performance of the final trained model.\nFor example, for summarization tasks, Ziegler et al. [2019  ###reference_b197###] discovered that their model predominantly extracted verbatim lines from the document. This was later attributed to low-quality feedback by Stiennon et al. [2020  ###reference_b156###].\nSimilarly, the size of the feedback is also crucial.\nFor example, despite employing similar methodologies, Lightman et al. [2023  ###reference_b85###] identified a need for a \u2018greater amount of feedback\u2019 for the methods in Uesato et al. [2022  ###reference_b165###] to be effective, as the intended objective was not even observed in the latter work.\nThe provision of clear and unambiguous instructions to the labelers is a fundamental requirement Ziegler et al. [2019  ###reference_b197###]; Nakano et al. [2021  ###reference_b108###].\nFailure to do so can not only result in low-quality feedback but also introduce systematic bias in the collected feedback and, consequently, the model Parmar et al. [2022  ###reference_b121###].\nTypically, labelers are provided with a comprehensive set of instructions, including guidelines for handling edge cases Bai et al. [2022a  ###reference_b12###]. Glaese et al. [2022  ###reference_b49###] even provides a tutorial to the selected few labelers.\nResearchers typically screen labelers to ensure they possess the necessary skills to provide feedback. For instance, in the case of translation tasks, bilingual labelers with native proficiency in both languages are preferred Kreutzer et al. [2018b  ###reference_b76###].\nAdditionally, a minimum educational qualification is generally preferred.\nFor example, Stiennon et al. [2020  ###reference_b156###] requires labelers to have at least a high-school degree, whereas Nakano et al. [2021  ###reference_b108###], Glaese et al. [2022  ###reference_b49###] and Bai et al. [2022a  ###reference_b12###] require a minimum undergraduate and master\u2019s degree respectively.\nThe end goal also influences the selection of labelers. For instance, creating a harmless and helpful chatbot necessitates a diverse group of labelers with varying backgrounds and demographics Ouyang et al. [2022  ###reference_b119###]; Bai et al. [2022a  ###reference_b12###] as otherwise this may result in implicit biases in the model Peng et al. [2022  ###reference_b123###]. For instance, currently deployed language models have been shown to reflect views more aligned with western audiences Durmus et al. [2023  ###reference_b40###] and may have systematic political biases Santurkar et al. [2023  ###reference_b140###], partly owing to the lack of annotators from diverse demographic groups.\nHowever, despite screening, there may be low agreement among the annotators themselves, or even between researchers and annotators Kreutzer et al. [2018a  ###reference_b75###].\nThe labelers are further screened based on two standard criteria 1.) inter-annotator agreement, i.e., the agreement between different annotators on the same example, and 2.) expert-annotator agreement, i.e., the agreement between annotators and experts Kreutzer et al. [2018b  ###reference_b76###].\nSpecifically, the former metric ensures that the labelers are consistent in their feedback, and the latter metric is used to keep only those labelers that have a high agreement with experts. Menick et al. [2022  ###reference_b103###] creates a group of super-raters who have a high agreement with experts, and the group is expanded upon iteratively.\nEven after filtering, some methods ensure a hands-on relationship with labellers Stiennon et al. [2020  ###reference_b156###] and have also created Slack groups for discussing any bugs, issues, or edge cases Bai et al. [2022a  ###reference_b12###]."
        },
        {
            "section_id": "7.4",
            "parent_section_id": "7",
            "section_name": "Supervised Fine-Tuning: Limitations and Role",
            "text": "Upon the collection of high-quality feedback, the subsequent step is to assimilate this feedback to train the model. The most direct method to achieve this is to perform supervised fine-tuning of the language model based on the collected feedback. Specifically, human feedback is gathered in the form of expert outputs on input prompts, also referred to as human demonstrations. These human demonstrations can be perceived as positive example outputs to prompts that should be generated by the language model. The model is then fine-tuned on these demonstrations using the same pretraining objective, and this process in RL terminology is often termed behavior cloning Nakano et al. [2021  ###reference_b108###].\nAdditionally, when dealing with preference data, the model can be directly fine-tuned on preferred feedback. However, this approach exhibits limitations by not accounting for negative feedback\u2014outputs that the model should avoid generating. This is crucial for training robust models that can handle adversarial situations and identify and rectify errors. To tackle this limitation, alternative methods that incorporate both positive and negative feedback have been developed, as discussed in Section 7.9  ###reference_###.\nIn addition to human demonstrations, existing public instances from NLP datasets can be used as instruction tuning demonstrations Wei et al. [2021  ###reference_b168###].\nThis usually involves creating new instruction-tuning datasets by adding task instructions to existing examples from the dataset Ajith et al. [2023  ###reference_b2###].\nIn another field of work, prompts from the initial iterations of GPT-3 Brown et al. [2020  ###reference_b24###] served to real customers through Web API were used to fine-tune the model on expert (human) demonstrations provided by contracted labelers Ouyang et al. [2022  ###reference_b119###].\nWhile finetuning on supervised data enhances the model beyond its pretrained version in following instructions and intended tasks, it suffers from numerous limitations.\nFor instance, it does not penalize the model for hallucinating or permit it to learn from neutral or negative feedback. This can lead to harmful and unintended behavior, making it easier to prompt such models to elicit them Ganguli et al. [2022  ###reference_b46###  ###reference_b46###]; Perez et al. [2022  ###reference_b124###  ###reference_b124###].\nFurthermore, behavior cloning is likely to perform poorly in out-of-distribution prompts Pomerleau [1988  ###reference_b126###  ###reference_b126###]. These limitations may stem from the fact that during behavior cloning, the model is not allowed to explore the vast space of possible actions, i.e., the model is not allowed to generate outputs that are not present in the demonstrations and, in turn, get feedback for them. We refer readers to Section 6.3  ###reference_###  ###reference_### for theoretical discussion on the limitations of SFT.\nDespite its caveats, supervised fine-tuning plays a pivotal role in RLHF as it provides a robust initial policy, which allows RLHF methods to work well.\nFrom an RL perspective, learning algorithms such as the widely used Proximal Policy Optimization (PPO) in training sequence-to-sequence models, struggle to improve from poor initializations, especially when the action space is large, as in the case of text generation. This is because these methods use model-based exploration, which is ineffective when the transition probabilities over many actions are similar Nguyen et al. [2017  ###reference_b113###  ###reference_b113###] i.e., different text outputs have similar probabilities of generation.\nFurthermore, as we discuss in Section 7.6  ###reference_###  ###reference_###, usually a KL penalty is applied to ensure the output text generated by our RL-tuned model is close to the initial model. Thus, during RL training, it is preferable to start with an initial model that already generates decent-quality text.\nEmpirical studies have demonstrated that starting with fine-tuning on high-quality human demonstrations results in significant improvements over starting with pretrained language models Stiennon et al. [2020  ###reference_b156###  ###reference_b156###]; Ouyang et al. [2022  ###reference_b119###  ###reference_b119###]. For instance, InstructGPT collects API customer and labeler written prompts and outputs to fine-tune their model before initiating with the RLHF training Ouyang et al. [2022  ###reference_b119###  ###reference_b119###].\nGlaese et al. [2022  ###reference_b49###  ###reference_b49###] has also shown that starting with a prompted model (dialogue) instead of fine-tuning on label demonstrations is possible. However, they start with a large model (70B parameters) and do not perform a comparative study starting with fine-tuning on human demonstrations. Thus, it cannot be definitively concluded that starting with a prompted model is equivalent to fine-tuning on human demonstrations. Moreover, prompting has the limitation of using up a major portion of the context length of the model, which, apart from the computational burden, can also be crucial for some tasks because of limited context length. Askell et al. [2021  ###reference_b9###  ###reference_b9###] propose using context distillation by training the model to generate output similar to its prompted counterpart using KL divergence loss. They find similar performance to the prompted model, and the method has been used in their subsequent works Bai et al. [2022a  ###reference_b12###  ###reference_b12###].\nIn conclusion, while supervised fine-tuning can be utilized independently of the RLHF pipeline, it still suffers from several significant limitations. However, it still serves as an integral step in the RLHF pipeline, providing a robust initial policy crucial for subsequent RL training."
        },
        {
            "section_id": "7.5",
            "parent_section_id": "7",
            "section_name": "Reward Modeling",
            "text": "After the collection of human feedback, the next challenge is training the language model effectively. Although supervised fine-tuning offers a straightforward method, its effectiveness is limited by the volume of human feedback.\nIn contrast, RLHF introduces a reward model to emulate human feedback, thereby acting as a stand-in for the true reward function, i.e., the actual human feedback. This reward model, usually much smaller than the language model, facilitates fine-tuning the language model using feedback generated by it on new model outputs, avoiding the need for additional costly human annotation. In practice, using a reward model over supervised fine-tuning has been found more data-efficient Ramamurthy et al. [2022  ###reference_b133###  ###reference_b133###].\nThe reward model is a fine-tuned language model that assigns a scalar reward score to an input-output pair. The last embedding layer is replaced with a single projection layer that outputs this scalar reward.\nWhile the reward model can learn from various types of feedback, recent studies highlight the simplicity and effectiveness of preference-based feedback Ouyang et al. [2022  ###reference_b119###  ###reference_b119###]; Bai et al. [2022a  ###reference_b12###  ###reference_b12###].\nThis approach involves fine-tuning the initialized reward model to predict the preference between two trajectories (output text) given the same input prompt or context. The reward is typically modeled as a Bradley-Terry-Luce (BTL) model Bradley and Terry [1952  ###reference_b22###  ###reference_b22###], where the probability of preferring one trajectory over another is a function of the difference in their reward scores.\nMathematically, this can be represented as:\nwhere  is the sigmoid function, o and o\u2019 represent the two trajectories, and their rewards are represented as  and  respectively.\nThis form of reward modeling has been found to provide smoother rewards and is less noisy Christiano et al. [2017  ###reference_b30###  ###reference_b30###]. A similar method can then be used for ranking between k trajectories (k > 2), where the reward is modeled as a Plackett-Luce (PL) model Plackett [1975  ###reference_b125###  ###reference_b125###]; Luce [1979  ###reference_b95###  ###reference_b95###]. Moreover, Zhu et al. [2023a  ###reference_b194###  ###reference_b194###] provides theoretical proof of convergence guarantees under the Maximum Likelihood estimate of both BTL and PL models.\nThe size and initialization of the reward model are critical determinants of its performance.\nWhile smaller reward models are easier to train, scaling laws suggest that larger models yield better agreement with actual human preferences Askell et al. [2021  ###reference_b9###  ###reference_b9###].\nHowever, Ouyang et al. [2022  ###reference_b119###  ###reference_b119###] found that training very large reward models can be unstable and result in overfitting. Instead, they report good performance even when using a reward model that is 30 times smaller than the policy model.\nRegarding initialization, multiple methods have been proposed. While Ziegler et al. [2019  ###reference_b197###  ###reference_b197###] fine-tunes a pretrained language model on preference data collected on model-generated outputs, Ouyang et al. [2022  ###reference_b119###  ###reference_b119###] trains a GPT-3 based reward model on publicly available datasets. However, only a slight advantage was found over using pretrained language models or supervised-fine-tuned models.\nLeveraging publicly available preference datasets (such as ranked answers from StackOverflow), as suggested by Askell et al. [2021  ###reference_b9###  ###reference_b9###], notably enhances reward model performance, especially for smaller models and datasets.\nThe reward model is initially trained on a selected set of input prompts and corresponding initial model outputs. As the model training progresses, it is crucial for the reward model to generalize to new model outputs and potentially new input prompts. We refer readers to Section 5.1  ###reference_###  ###reference_### for a deeper theoretical exploration of this aspect.\nRegarding the generalization capabilities of reward models, Ouyang et al. [2022  ###reference_b119###  ###reference_b119###] presents findings that demonstrate high generalization to held-out test labelers. This capability is of paramount importance since a majority of the inputs encountered during language model training would be out-of-distribution w.r.t. the reward model training phase. Generalization capability depends on various factors such as the dataset\u2019s size, the amount of noise in the feedback dataset, and the characteristics of the pretrained reward model.\nMoreover, the robustness and calibration of the reward models with respect to actual human preferences are essential for their effectiveness. A well-calibrated reward model should accurately predict the probability of a human preferring one output over another.\nBai et al. [2022a  ###reference_b12###  ###reference_b12###] discovered that when training solely on a helpfulness feedback dataset, their model exhibits strong calibration. However, when trained on a mixture of helpfulness and harmlessness datasets, the model is underconfident in its predictions. To assess robustness, a common practice involves evaluating the policy model trained using the reward model.\nTo assess robustness, a common practice involves evaluating the policy model trained using the reward model.\nInterestingly, Bai et al. [2022a  ###reference_b12###  ###reference_b12###] discerned that smaller reward models and higher rewards correlate with decreased robustness. This phenomenon arises from the reward model\u2019s initial training on model outputs with naturally low rewards. To address this distribution shift, an approach involving iterated training of the reward model is proposed (see Section 7.6  ###reference_###  ###reference_###). In summation, the discussion underscores that the trained reward model on preferences is an imperfect proxy of human feedback, especially in out-of-domain cases.\nApart from providing a single scalar reward at the end of a trajectory (complete text output), several methods model a more fine-grained approach. Uesato et al. [2022  ###reference_b165###  ###reference_b165###]; Korbak et al. [2023  ###reference_b74###  ###reference_b74###] provides a segment-level reward during training, a method also known as process supervision. Interestingly, while Uesato et al. [2022  ###reference_b165###  ###reference_b165###] did not find any major downstream performance improvement with their method, Lightman et al. [2023  ###reference_b85###  ###reference_b85###] used similar methodology but instead trained larger models on a larger feedback dataset coupled with evaluation on a more difficult task found segment-level feedback to be significantly more useful. Scheurer et al. [2023  ###reference_b143###  ###reference_b143###] uses language feedback from another LLM that implicitly acts like a reward model for the training of the policy model.\nWhile ideally, as discussed in Section 4  ###reference_###  ###reference_###, the reward model provides a dual-purpose reward taking into account both the task information (eg, summarization task) and the task-specific evaluation ((a condescending summary is rewarded less than a neutral summary). However, diversifying the approach, some strategies involve the use of multiple reward models, each specializing in distinct characteristics or specific tasks. Wu et al. [2023c  ###reference_b174###  ###reference_b174###]; Ram\u00e9 et al. [2023  ###reference_b134###  ###reference_b134###] demonstrate the efficacy of training separate reward models for specific attributes such as coherency and factuality. Similarly, Glaese et al. [2022  ###reference_b49###  ###reference_b49###] introduces two reward models\u2014one for preference and another for rule violation in dialogue generation. They found using two models over one to be more effective, likely because of a smaller feedback dataset.\nFurther, since the preference-based reward model provides a delayed reward (reward is provided only at the end of the whole trajectory), the A2C algorithm, when used for sequence modeling Bahdanau et al. [2016  ###reference_b11###  ###reference_b11###] proposes potential-based reward shaping, where intermediate generations (states) are also rewarded.\nIn conclusion, the reward modeling process is a critical component of RLHF which involves the training of a model to emulate human feedback, thereby acting as a surrogate for the true reward function. The size, initialization, and generalization capabilities of the reward model are all crucial factors that influence its performance. The reward model must be robust, well-calibrated, and additionally can provide more fine-grained feedback to the policy model training."
        },
        {
            "section_id": "7.6",
            "parent_section_id": "7",
            "section_name": "RLHF Finetuning of Language Models",
            "text": "The trained reward model is utilized for finetuning the language model. Framing the task as reinforcement learning, with the language model as the policy, algorithms such as Proximal Policy Optimization (PPO) and Advantage Actor-Critic (A2C)  Schulman et al. [2017  ###reference_b145###]; Bahdanau et al. [2016  ###reference_b11###] are used to update the parameters of the language model such that the generated outputs maximize the obtained reward.\nThese are gradient-based methods, called policy-gradient algorithms, that directly update the parameters of the policy using the evaluative reward feedback\nThe following sections primarily focus on the widely used Proximal Policy Optimization (PPO) Algorithm, while the same concepts are applicable to other candidate algorithms Ouyang et al. [2022  ###reference_b119###].\nDespite the impressive results achieved by RLHF in practice, it is an unstable training process Choshen et al. [2019  ###reference_b28###  ###reference_b28###]. Moreover, it is highly sensitive to hyperparameters, necessitating a significant amount of hyperparameter tuning Rafailov et al. [2023  ###reference_b129###  ###reference_b129###]; Yuan et al. [2023  ###reference_b186###  ###reference_b186###]. Furthermore, the generalization capabilities of RLHF and other issues, such as underperformance on metrics not captured by the reward model, warrant further investigation. A comprehensive examination of these aspects is discussed in Section 7.7  ###reference_###  ###reference_###."
        },
        {
            "section_id": "7.6.1",
            "parent_section_id": "7.6",
            "section_name": "7.6.1 Training Procedure",
            "text": "The pre-trained/SFT language model is prompted with contexts/prompts from a prompting dataset. The prompting dataset may or may not be identical to the one used for collecting human demonstrations in the SFT phase Ouyang et al. [2022  ###reference_b119###]. The model outputs, along with the inputs, are passed to the reward model that generates a scalar output indicating the reward for this input-output pair. The reward is used as evaluative feedback to update the parameters of the language model using suitable RL algorithms that result in increasing the likelihood of the generation of more rewarding outputs. We next discuss a few commonly used RL algorithms for the process."
        },
        {
            "section_id": "7.6.2",
            "parent_section_id": "7.6",
            "section_name": "7.6.2 Training Algorithms",
            "text": "The commonly used policy-gradient algorithms for aligning LLMs using RLHF are PPO and A2C Schulman et al. [2017  ###reference_b145###]; Bahdanau et al. [2016  ###reference_b11###]. Both fall under the category of actor-critic algorithms.\nThese algorithms consist of two main components: the critic learns the expected cumulative reward for an input-output pair, called the value function, and the actor is the LLM policy that gets updated based on the cumulative reward estimates obtained from the critic. The reward values are obtained from the previously trained reward model, which is kept frozen during the RL training.\nAs the LLM encounters more interactions and collects more reward feedback, it uses the data to update the value function and the LLM policy parameters.\nThe training objective (Equation (15  ###reference_###)) aims to update the parameters of the policy to increase the expected cumulative reward of the LLM policy.\nA2C and A3C Mnih et al. [2016  ###reference_b104###] use an estimate of the advantage of taking an action instead of the action-value function for that action as a way of incurring lesser variance in policy gradient estimation.\nPPO additionally constrains the policy update at each iteration from straying too far by using a clipped objective Schulman et al. [2017  ###reference_b145###]. This helps provide additional stability to the training.\nTraining LLMs at a large scale requires an immense engineering effort, and practical implementations of these algorithms require domain-specific variations.\nWhile major progress has been made towards efficient training and inference of LLMs Xia et al.  ###reference_b176###; Lagunas et al. [2021  ###reference_b78###]; Xia et al. [2022  ###reference_b177###]; Su et al. [2023  ###reference_b157###]; Murahari et al. [2023  ###reference_b107###]; Yang et al. [2022b  ###reference_b182###]; Hinton et al. [2015  ###reference_b58###]; Yin et al. [2021  ###reference_b185###]; Murahari et al. [2022  ###reference_b106###], there is still a lot of scope for improvement in the sample efficiency and stability of training algorithms for RLHF.\nRecent work has addressed these challenges with different variants of these algorithms tackling different aspects ranging from practical implementation issues such as high memory usage Santacroce et al. [2023  ###reference_b139###], changes specific for NLP Ramamurthy et al. [2022  ###reference_b133###]; Wu et al. [2023b  ###reference_b173###], training instability Zheng et al. [2023  ###reference_b191###].\nWe refer readers to Weng [2018  ###reference_b169###] for a comprehensive survey of policy gradient algorithms."
        },
        {
            "section_id": "7.6.3",
            "parent_section_id": "7.6",
            "section_name": "7.6.3 Improving Training Stability",
            "text": "Imperfections in the reward model reduce the effectivity of the training algorithms, as the value functions learned, and in turn thus the gradient updates, become inaccurate.\nThus, using the aforementioned algorithms with the learned reward model may lead the language model to exploit the imperfections and generate nonsensical text, often called \u2018reward overoptimization\u2019.\nThis can be mitigated with appropriate regularization during training.\nAs the pre-trained or SFT model (policy) is already a highly capable LLM, Jaques et al. [2019  ###reference_b62###] propose using a copy of the initial model to regularize training.\nThe aim is to ensure that even as the policy parameters are updated to maximize reward, the outputs of the updated policy do not stray too far from the initial policy.\nIn particular, an additional regularization term of the Kullback-Leibler (KL) divergence between the policy being trained and the initial policy is added to the RL training objective in the form of a reward penalty, commonly called the KL penalty.\nTheoretically, the addition of this KL penalty has been shown to be similar to performing Bayesian inference Korbak et al. [2022  ###reference_b73###] on the model.\nA hyperparameter  controls the weight of this KL penalty regularization during training. Further, it is common to compare different variants of RL algorithms at a fixed KL distance from the initial model, with the aim of maximizing the reward with the lowest possible KL divergence."
        },
        {
            "section_id": "7.6.4",
            "parent_section_id": "7.6",
            "section_name": "7.6.4 Iterated RLHF",
            "text": "As training progresses, the reward model can become miscalibrated with human preferences at higher rewards Bai et al. [2022a  ###reference_b12###].\nThis is because the reward model was trained on outputs from the initial model, which inherently have low-valued rewards. Consequently, several methods Stiennon et al. [2020  ###reference_b156###]; Bai et al. [2022a  ###reference_b12###] have employed an iterative training approach, where new outputs are generated by the updated policy, which are then annotated by humans for feedback. The reward model is then retrained based on this new human feedback, followed by training of the policy model.\nThis process, referred to as Iterated-RLHF or Online-RLHF, is repeated for several iterations. Although effective, this procedure is naturally expensive and time-consuming.\nDespite the impressive results achieved by RLHF in practice, it is an unstable training process Choshen et al. [2019  ###reference_b28###  ###reference_b28###  ###reference_b28###]. Moreover, it is highly sensitive to hyperparameters, necessitating a significant amount of hyperparameter tuning Rafailov et al. [2023  ###reference_b129###  ###reference_b129###  ###reference_b129###]; Yuan et al. [2023  ###reference_b186###  ###reference_b186###  ###reference_b186###]. Furthermore, the generalization capabilities of RLHF and other issues, such as underperformance on metrics not captured by the reward model, warrant further investigation. A comprehensive examination of these aspects is discussed in Section 7.7  ###reference_###  ###reference_###  ###reference_###."
        },
        {
            "section_id": "7.7",
            "parent_section_id": "7",
            "section_name": "Limitations of RLHF Models",
            "text": "Fine-tuning models using Reinforcement Learning from Human Feedback (RLHF) showcase a remarkable ability to align with human preferences and generalize to new scenarios and is more sample-efficient than supervised fine-tuning. Nonetheless, these models exhibit characteristics and behaviors that warrant careful consideration, prompting the need for further exploration and refinement.\nOne intriguing property, referred to as the Alignment Tax, was identified by Ouyang et al. [2022  ###reference_b119###  ###reference_b119###]. The phenomenon reveals that RLHF-trained chat models sometimes perform poorly compared to initial policy in downstream tasks, suggesting a cost linked to aligning human preferences.\nTo mitigate this, they propose incorporating the pre-training objective into RLHF-finetuning, which substantially reduces the Alignment Tax.\nMoreover, Bai et al. [2022a  ###reference_b12###  ###reference_b12###] indicates that larger models tend to exhibit lower alignment tax.\nBai et al. [2022a  ###reference_b12###  ###reference_b12###] also observed that RLHF models better align with human preferences as the scales of both the reward model and policy model increase. It is noteworthy, however, that a similar scaling effect could be seen in instruction-finetuned SFT models. A comprehensive comparison of the scaling effects on RLHF versus SFT models is currently lacking in the literature and would make for an intriguing future study.\nRLHF models have exhibited impressive generalization capabilities beyond their training data, including generalization on new prompts and human feedback.\nFor instance, Ouyang et al. [2022  ###reference_b119###  ###reference_b119###] demonstrates RLHF-tuned models answering coding questions and following instructions in multiple languages despite being finetuned only in English and with limited code-related prompts.\nThis suggests that the majority of a language model\u2019s capabilities are acquired during pre-training, and RLHF merely aligns these capabilities to elicit desired behavior.\nHowever, this generalization can be a double-edged sword, potentially leading to undesirable outcomes, especially when the feedback signal is sparse. For instance, the initial LLaMA2 Chat Model555https://huggingface.co/meta-llama/Llama-2-70b-chat-hf  ###reference_70b-chat-hf###  ###reference_70b-chat-hf###, when prompted \"How to kill a process?\" refused to answer, drawing ethical concerns, though the intended answer was about terminating a computer process. This behavior likely stems from the model\u2019s extended generalization from examples that trained it to reject violent queries. The example further highlights the problems of imperfect rewards leading to misgeneralization, as discussed in Section 6.2  ###reference_###  ###reference_###.\nFurther, a distributional shift between prompts used for reward model finetuning and RLHF training can result in the policy model misaligning with human preferences Bai et al. [2022a  ###reference_b12###  ###reference_b12###].\nFurther, during RL training, outputs are sampled from the language model, which is evaluated using the reward model. However, deviations in parameters used for sampling outputs from the model during inference from those in training can yield poor results Ramamurthy et al. [2022  ###reference_b133###  ###reference_b133###].\nAnother characteristic of RLHF models is their low entropy in output distribution Bai et al. [2022a  ###reference_b12###  ###reference_b12###], which challenges generating diverse responses Kirk et al. [2023  ###reference_b70###  ###reference_b70###]. This holds true for both seen and unseen datasets. To address this, entropy regularization techniques are introduced Jaques et al. [2019  ###reference_b62###  ###reference_b62###]; Li et al. [2016  ###reference_b82###  ###reference_b82###] to amplify diversity in the action space, albeit not always resolving the issue Raichuk et al. [2021  ###reference_b131###  ###reference_b131###].\nWhile not conclusive, Bai et al. [2022a  ###reference_b12###  ###reference_b12###] found that while RLHF models exhibit better sentiment towards all classes, they display similar biases to underlying LLMs when sampling with temperature < 1 (i.e., with low diversity samples). This could be attributed to their lower entropy. Furthermore, while pre-trained models often generate probabilities that are well-calibrated, RLHF models may lose this calibration. For instance, OpenAI [2023  ###reference_b117###  ###reference_b117###] found that for pre-trained GPT-4, the probability of generating an answer is often directly proportional to the probability of it being correct. However, in the case of RLHF models, the distribution is skewed towards more likely answers.\nWhile RLHF aims to align language models with human preferences and intentions, reward model misalignment is frequently possible. For instance,  Singhal et al. [2023  ###reference_b153###  ###reference_b153###] finds that reward models provide higher rewards to longer outputs. Further,\nIt is imperative to note that the reward model is merely an imperfect proxy for real human preferences/feedback. Due to the lack of calibration and robustness of reward models Bai et al. [2022a  ###reference_b12###  ###reference_b12###], over-optimizing against the reward model can render it an ineffective measure (Goodhart\u2019s Law). This phenomenon, known as Reward Overoptimization, has been studied in the context of language models by Gao et al. [2022  ###reference_b48###  ###reference_b48###]; Coste et al. [2023  ###reference_b32###  ###reference_b32###].\nFurther, training RLHF models in practice is very difficult for practitioners owing to unstable training Choshen et al. [2019  ###reference_b28###  ###reference_b28###], hyperparameter sensitivity Yuan et al. [2023  ###reference_b186###  ###reference_b186###]; Rafailov et al. [2023  ###reference_b129###  ###reference_b129###], loading multiple models leading to high memory usage Santacroce et al. [2023  ###reference_b139###  ###reference_b139###]. As a result, there have been significant efforts to simplify the training process by learning directly from the available feedback using simpler supervised finetuning objectives, as we discuss in Section 7.9  ###reference_###  ###reference_###.\nIn conclusion, while RLHF substantially enhances the performance of LLMs and aligns them with human preferences, it is not without its limitations. These include, but are not limited to, issues such as text hallucination McKenna et al. [2023  ###reference_b101###  ###reference_b101###], bias and toxicity Deshpande et al. [2023a  ###reference_b34###  ###reference_b34###]; Ferrara [2023  ###reference_b44###  ###reference_b44###]; Gupta et al. [2023  ###reference_b53###  ###reference_b53###], and the generation of harmful text when probed Perez et al. [2022  ###reference_b124###  ###reference_b124###]; Wei et al. [2023  ###reference_b167###  ###reference_b167###]. Despite significant improvements, these models are not fully aligned with human preferences, underscoring the need for continued research and development in this field."
        },
        {
            "section_id": "7.8",
            "parent_section_id": "7",
            "section_name": "Enriching Reward Signals in Reinforcement Learning",
            "text": "Reinforcement learning (RL) has conventionally employed delayed and sparse rewards, where agents receive scalar feedback at the end of a trajectory or episode Sutton and Barto [2005  ###reference_b159###  ###reference_b159###]. While this approach is straightforward to implement and aligns with the task objective, it is not without its drawbacks. Sparse rewards can lead to sample-inefficient learning due to extensive exploration requirements Bellemare et al. [2016  ###reference_b18###  ###reference_b18###]. Additionally, they may result in reward hacking, where agents exploit unintended strategies to maximize rewards without solving the intended task Ibarz et al. [2018  ###reference_b60###  ###reference_b60###]. Underspecified rewards, which do not fully capture the desired behavior, can also yield suboptimal or degenerate solutions Hadfield-Menell et al. [2017  ###reference_b54###  ###reference_b54###].\nTo mitigate the limitations of sparse rewards, researchers have explored various methods for providing richer feedback in environments with inherently sparse rewards. These approaches include reward shaping, where the original reward signal is augmented with additional feedback Ng et al. [1999  ###reference_b110###  ###reference_b110###]; Grzes [2017  ###reference_b51###  ###reference_b51###]; intrinsic motivation, which encourages exploration and learning through internal rewards based on novelty, curiosity, or learning progress Oudeyer et al. [2007  ###reference_b118###  ###reference_b118###]; Bellemare et al. [2016  ###reference_b18###  ###reference_b18###]; Pathak et al. [2017  ###reference_b122###  ###reference_b122###]; and multi-objective optimization with multiple reward signals Roijers et al. [2013  ###reference_b138###  ###reference_b138###]; Roijers [2016  ###reference_b137###  ###reference_b137###]. Hierarchical RL, which decomposes complex tasks into simpler subtasks with their own reward structures, has also been investigated Dietterich [1999  ###reference_b37###  ###reference_b37###]; Barto and Mahadevan [2003  ###reference_b17###  ###reference_b17###]. Moreover, richer forms of feedback, such as learning from corrections Jain et al. [2015  ###reference_b61###  ###reference_b61###]; Bajcsy et al. [2017  ###reference_b15###  ###reference_b15###], demonstrations Rengarajan et al. [2022  ###reference_b136###  ###reference_b136###], and language feedback Matuszek et al. [2012  ###reference_b98###  ###reference_b98###]; Fried et al. [2017  ###reference_b45###  ###reference_b45###], have proven beneficial.\nCurrent RLHF pipelines for LLMs primarily rely on sparse rewards provided at the end of an episode, with reward models trained using sparse preference-based feedback. Similar challenges observed in traditional RL have also been identified in RLHF-tuned LLMs. Some progress has been made in learning from feedback for multi-objective optimization Ram\u00e9 et al. [2023  ###reference_b134###  ###reference_b134###], language feedback Scheurer et al. [2022  ###reference_b142###  ###reference_b142###], corrective feedback Madaan et al. [2023  ###reference_b96###  ###reference_b96###]; Shinn et al. [2023  ###reference_b149###  ###reference_b149###], and denser rewards Wu et al. [2023c  ###reference_b174###  ###reference_b174###]. Future research should explore the integration of these techniques to address the unique challenges in training LLMs with RLHF, potentially improving generalization and robustness."
        },
        {
            "section_id": "7.9",
            "parent_section_id": "7",
            "section_name": "Moving Beyond RL Training",
            "text": "While RLHF has been very successful, it still results in unstable training Choshen et al. [2019  ###reference_b28###], is hyperparameter sensitive Yuan et al. [2023  ###reference_b186###]; Rafailov et al. [2023  ###reference_b129###], has high memory usage Santacroce et al. [2023  ###reference_b139###] making it difficult for practitioners to actually use it. As a result, there have been significant efforts to simplify the training process by learning directly from the available feedback using simpler supervised finetuning objectives."
        },
        {
            "section_id": "7.9.1",
            "parent_section_id": "7.9",
            "section_name": "7.9.1 Alternatives to RL using Reward Model",
            "text": "Once, a reward model is trained, it is not necessary to perform the RLHF-based training. Instead, an alternate approach during inference is to sample multiple outputs from the LLM and rank them using the reward model Nakano et al. [2021  ###reference_b108###]; Cobbe et al. [2021  ###reference_b31###]. This is also called best-on-n sampling or rejection sampling. If sampling multiple outputs, it is important to ensure diversity of outputs by adjusting the sampling parameters (such as higher temperature). This approach is often considered as either a baseline or augmented with RLHF-trained models for better inference-time results.\nFurther, various works Dong et al. [2023  ###reference_b38###]; Yuan et al. [2023  ###reference_b186###]; Song et al. [2023  ###reference_b155###] use the trained reward model to rank multiple responses and use the signal from the ranked responses to train the policy model, without using an elaborate RL algorithm. In another line of work, RAD Deng and Raffel [2023  ###reference_b33###] uses weighted-decoding of tokens at inference, based on a separately trained reward model."
        },
        {
            "section_id": "7.9.2",
            "parent_section_id": "7.9",
            "section_name": "7.9.2 Alternatives to RL without Explicit Reward Models",
            "text": "In this section, we discuss alternative methods to align language models with human feedback that do not rely on reward models.\nWhile RLHF-PPO has shown promising results, it suffers from sensitivity to hyperparameters, the need for training additional models, and potential misalignment of the reward model Wu et al. [2023c  ###reference_b174###]; Rafailov et al. [2023  ###reference_b129###]; Pang et al. [2022  ###reference_b120###]; Zhu et al. [2023b  ###reference_b195###]; Singhal et al. [2023  ###reference_b153###]. To address these issues, recent research has explored various techniques that directly incorporate human feedback into the training process, without relying on additional reward models.\nA straightforward approach is supervised fine-tuning on positive demonstrations from human feedback, such as instruction-finetuned models Ouyang et al. [2022  ###reference_b119###]; Chiang et al. [2023  ###reference_b27###]; Zhou et al. [2023  ###reference_b192###]. However, this method does not utilize negative feedback, which is crucial for training robust models that can handle adversarial situations and identify and correct errors.\nRecent works, such as Liu et al. [2023a  ###reference_b90###]; Zhang et al. [2023  ###reference_b188###], provide both positive and negative demonstrations/feedback and maximize the likelihood of generating positive/preferred output. These methods have shown better performance than RLHF methods on summarization and dialogue tasks. Zhao et al. [2023  ###reference_b190###] demonstrate that Sequence Likelihood calibration (SLiC) Zhao et al. [2022  ###reference_b189###] can be used to train models on off-policy offline data collected for different models, resulting in better performance than RLHF-based methods on summarization tasks. SLiC uses a ranking calibration loss that contrasts positive and negative sequences while motivating the model to predict the positive class. Further, RSO Liu et al. [2023c  ###reference_b92###] improves policy learning in SLiC by using statistical rejection sampling from the policy.\nRafailov et al. [2023  ###reference_b129###]; Azar et al. [2023  ###reference_b10###] further reformulate the objective encoded in the RLHF PPO algorithm, and train the model directly on the new objective, without the need for a separate reward model. This follows the intuition, that the policy model can be implicitly used as a reward model for training itself based on the collected feedback. However, the results are preliminary, and extending to out-of-distribution prompts may not be possible without the introduction of an explicit reward model.\nAnother line of research focuses on refining model-generated responses using human-encoded principles or feedback. Bai et al. [2022b  ###reference_b13###]; Kundu et al. [2023  ###reference_b77###] propose a framework where a list of human-encoded principles (Constitution) guide the model to critique its generations and self-refine the responses. The model is then fine-tuned on the refined responses. Self-Align Sun et al. [2023  ###reference_b158###] follows a similar procedure but further removes the need to start with an RLHF-finetuned model. They fine-tune the pretrained LLaMA Touvron et al. [2023a  ###reference_b163###] base model using less than 300 lines of human feedback (in the form of constitutional principles) and achieve performance comparable to state-of-the-art models in terms of helpfulness and harmlessness.\nAnother direction of work learns to generate or select good feedback for model outputs and apply it to refine language model outputs.\nScheurer et al. [2022  ###reference_b142###] takes a similar refinement approach but utilizes available summarization feedback. The initial model is conditioned on input, feedback, and output, generating multiple refinements. The model is then fine-tuned on refinements with the highest similarity to human feedback.\nLiu et al. [2023b  ###reference_b91###] aligns human moral values by modeling DP (dynamic-programming) based edits from unaligned source text to target aligned text. The model is then fine-tuned on the refinements generated by the edits, using RL for the second part of the process. Xu et al. [2022  ###reference_b179###] fine-tune a dialogue model using multi-modal feedback with the DIRECTOR method Arora et al. [2022  ###reference_b6###], which models both negative and positive sequence labeling directly in the language model head.\nIn summary, these alternative methods generate new data based on feedback or guidelines and then use it to fine-tune the model. These approaches reduce the reliance on reward models and have shown promising results in some tasks, making them a viable alternative to RLHF-PPO. While these models are easier to train and help in alleviating many drawbacks of RLHF, the evaluation performed has been performed only on specific domains, and constrained settings. Moreover, other in-depth analysis such as sample efficiency and properties exhibited by these models, especially on out-of-distribution data needs to be explored further."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Discussion and Conclusion",
            "text": "In this work, we explore the fundamental aspects of reinforcement learning from human feedback (RLHF), aiming to clarify its mechanisms and limitations.\nWe highlight the underlying assumptions necessary for RLHF and examine the impact of different implementation choices, shedding light on the workings of this approach.\nOur analysis naturally focuses on the reward models, which constitute the core component of RLHF. We introduce the concept of oracular rewards, which represent the ideal reward signals that reward models should approximate.\nThe challenges encountered in learning these reward functions highlight both the practical and fundamental limitations of RLHF, as thoroughly analyzed by Casper et al. [2023  ###reference_b25###].\nOur comprehensive review of the existing literature traces the development of RLHF from its inception to the recent advancements.\nWe cover various aspects: the types of feedback, the details and variations of training algorithms, and alternative methods for achieving alignment without using reinforcement learning.\nIn related work, Kaufmann et al. [2023  ###reference_b65###] extensively surveys RLHF, highlighting its evolution from preference-based learning.\nDespite the numerous variations of RLHF, the core principle of learning from evaluative feedback remains unchanged.\nThis form of learning is naturally suited to reinforcement learning, while the specifics of agent formulation, the nature of reward feedback, and environment definition continue to evolve.\nWe anticipate the reduction of reliance on human (or AI) feedback by using existing knowledge sources to construct rewards, which is one of the most promising directions for future efforts to enhance the impact of RLHF.\nAdditionally, improving reward encoding mechanisms to better reflect the diversity of human preferences is an important area for further research.\nAs RLHF continues to advance and reach its full potential, supported by research in these areas, the use of LLMs is also expanding.\nUntil we fully understand the implications of RLHF, it is crucial to develop robust methods for quantifying uncertainty in the outputs generated by an LLM.\nSuch techniques would enable us to identify and address low confidence outputs, which is especially important in safety-critical applications.\nUltimately, understanding its implications becomes paramount as advancements in RLHF increasingly influence industries and economies.\nThus, research in this field is critically important in shaping the future of large-scale language modeling and its societal impact.\nWe thank Khanh Nguyen for extensive and insightful feedback on earlier versions of the draft. We also thank Wenlong Zhao, Tuhina Tripathi, and Abhiman Neelakanteswara for their help with improving the clarity of the manuscript."
        }
    ],
    "url": "http://arxiv.org/html/2404.08555v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1"
        ],
        "methodology_sections": [
            "3",
            "4",
            "4.1",
            "4.2"
        ],
        "main_experiment_and_results_sections": [
            "6",
            "6.1",
            "6.2",
            "6.3",
            "7.3",
            "7.4",
            "7.5",
            "7.6"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.1",
            "4.2",
            "5",
            "5.1",
            "6.1",
            "6.2",
            "7.4",
            "7.6.3",
            "7.6.4",
            "7.9.1",
            "7.9.2"
        ]
    },
    "research_context": {
        "paper_id": "2404.08555v2",
        "paper_title": "RLHF Deciphered: A Critical Analysis of Reinforcement Learning from Human Feedback for LLMs",
        "research_background": "### Paper\u2019s Motivation, Research Problem, and Relevant Prior Work\n\n#### Motivation\nThe paper is driven by the widespread adoption of Large Language Models (LLMs) across multiple industries, including technology, healthcare, finance, and education. With their increasing integration into critical sectors, ensuring that LLMs behave in a predictable, safe, and trustworthy manner becomes essential. The paper identifies the central challenge of making LLMs exhibit human-like qualities or alignment with human objectives, which is crucial for their suitability in diverse tasks. The method of reinforcement learning from human feedback (RLHF) is recognized as a promising solution to achieve this alignment. However, a comprehensive understanding of why RLHF succeeds and its limitations is still lacking. This gap in understanding motivates the authors to critically analyze RLHF to improve its effectiveness and address its shortcomings.\n\n#### Research Problem\nThe core research problem revolves around deciphering the intricacies of RLHF when applied to fine-tune LLMs to align with human objectives. Specifically, the paper aims to understand the fundamental mechanisms that contribute to the success and limitations of RLHF, particularly focusing on the reward function, which is central to the RLHF procedure. The authors seek to address issues such as the impractical requirements for extensive feedback data, misgeneralization due to limited feedback and function approximation, and other inherent challenges like reward sparsity and reward model misspecification.\n\n#### Relevant Prior Work\nThe paper builds on multiple strands of previous research in RL, RLHF, and LLMs. Key references include:\n\n1. **Contribution of LLMs to various domains:** Singhal et al. [2022], Wu et al. [2023a], and Yan et al. [2023] showcase the transformative potential of LLMs in technology, healthcare, finance, and education.\n\n2. **Origin and success of RLHF:** Christiano et al. [2017] demonstrated the effectiveness of RLHF in solving RL problems like robotic locomotion and playing Atari games using human feedback without a reward function.\n\n3. **Application of RLHF to LLMs:** OpenAI [2022] evidenced the substantial success of RLHF in aligning LLMs with human goals.\n\n4. **Existing limitations:** Casper et al. [2023] have documented the well-known limits of RLHF, setting the stage for a deeper exploration of its framework and assumptions.\n\n5. **Ongoing improvements and alternatives:** Recent efforts focus on refining RLHF's components, such as:\n   - Training algorithms (Ramamurthy et al. [2022]),\n   - Reward models (Wu et al. [2023c]),\n   - RL-free approaches (Rafailov et al. [2023]).\n\nThe paper aims to diverge from the trend of incremental improvements and instead, provide a fundamental understanding of RLHF by scrutinizing its core components and assumptions.",
        "methodology": "# Methodology\n\n## Sequential Decision-Making Formulation\n\nIn this section, we formulate the text generation procedure from a language model as a sequential decision-making process. This formulation is essential for constructing reinforcement learning algorithms.\n\n### Markov Decision Process (MDP)\n\nA common framework for modeling sequential decision-making processes is the Markov Decision Process (MDP) (Markov, 1954 ###reference_b97###). An MDP is defined as a tuple \\((S, A, T, R, \\rho_0)\\) where:\n- \\(S\\) is the set of states,\n- \\(A\\) is the set of actions,\n- \\(T: S \\times A \\rightarrow \\text{distribution over } S\\) is the transition function,\n- \\(R: S \\times A \\rightarrow \\mathbb{R}\\) is the reward function, and\n- \\(\\rho_0\\) is the initial state distribution.\n\nEach sequential time step of the process is denoted by \\(t\\), and \\(s_t, a_t, r_t\\) denote the values of the state, action, and reward at time step \\(t\\). A discounting factor \\(\\gamma\\) is defined for discounting rewards over time, which is particularly useful for modeling an MDP with an infinite number of time steps (i.e., an infinite-horizon MDP). However, the outputs of language models are truncated after a finite number of steps. We use \\(T_{\\text{max}}\\) to denote the maximum time step.\n\n## Agent and Policy\n\nAn agent acts in an MDP using a policy \\(\\pi\\). The agent starts in state \\(s_0 \\sim \\rho_0\\). At time step \\(t\\), it chooses an action \\(a_t \\sim \\pi(a_t | s_t)\\), executes the action, transitions to a new state \\(s_{t+1} \\sim T(s_{t+1} | s_t, a_t)\\), and receives a reward \\(r_t = R(s_t, a_t)\\).\n\n## Markov Property\n\nThe term \u201cMarkov\u201d in MDP refers to the Markov property, in that the distribution over the next state \\(s_{t+1}\\) depends on only the current state \\(s_t\\) and action \\(a_t\\).\n\n## Text Generation as an MDP\n\nFor simplicity, we consider text generation tasks that include only one turn of interaction between the user and the model. We make a distinction between the text that a user inputs into the model, denoted by \\(c\\) and referred to as the context or the prompt, and the text that the model generates by itself to the context, denoted by \\(y\\) and referred to as the output or simply the generated text.\n\nLet \\(V\\) be the set of all tokens that the model can generate (the vocabulary), \\(C\\) the set of all possible contexts, and \\(Y\\) the set of all possible outputs.\n\n### Text Generation Process\n\nGiven a context \\(c \\in C\\) as input, the model generates an output \\(y \\in Y\\) token by token. Specifically, let \\(y_i\\) be the \\(i\\)-th token in generated output \\(y\\), then the model parameterized by \\(\\theta\\) first outputs token \\(y_0\\), and then conditioned on the concatenation of \\(c\\) and \\(y_0\\) it generates \\(y_1\\), and so on. This generation process resembles an agent traversing in an MDP.\n\n- The model acts according to a policy \\(\\pi\\).\n- The start-state distribution \\(\\rho_0\\) is the distribution over user-provided contexts.\n- The action space is the vocabulary \\(V\\).\n- The action \\(a_t\\) is the generated token \\(y_t\\).\n- The state \\(s_t\\) is the concatenation of the context \\(c\\) and all the tokens the model has generated up to time step \\(t\\).\n- The transition function \\(T\\) is a delta distribution, i.e., the next state is deterministic given the current state and action.\n\n### Reward Function\n\nReward \\(R(s_t, a_t)\\) given at time step \\(t\\) is computed by the reward model as \\(R(s_t, a_t) = \\text{RM}(s_t, a_t)\\), which is either provided by a human or a function learned from human feedback.\n\n## Special Properties of the Text Generation MDP\n\n1. **Large Action Space**:\n   - The action space is extremely large. For example, the LLaMa model (Touvron et al., 2023a ###reference_b163###, b ###reference_b164###) employs a vocabulary of size 32K. Having a gigantic action space blows up the search space for reinforcement learning algorithms.\n\n2. **Complex State Space**:\n   -",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n**Experiment Setup:**\n\n1. **Datasets:**\n   - The experiment utilized several large language model (LLM) datasets, focusing specifically on those that provide rich human feedback across diverse language tasks. These datasets were chosen to adequately train and evaluate the generalization capabilities of the models under study.\n\n2. **Baselines:**\n   - **Supervised Fine-Tuning (SFT):** The SFT models were trained using only positive examples, updating the model to increase the likelihood of these samples.\n   - **Value-Based RL Methods:** These methods aimed to learn value functions to guide action selection, but specific algorithms used are not detailed within the provided sections.\n   - **Policy-Gradient RL Methods:** These directly optimized a parameterized policy using reward feedback for gradient ascent, enhancing adaptation to different reward conditions and task structures.\n\n3. **Evaluation Metrics:**\n   - **Cumulative Reward:** The primary metric was the expected cumulative reward, measuring the overall effectiveness of the model in achieving high reward tasks.\n   - **Sample Efficiency:** The number of samples needed to achieve a certain level of performance, addressing how quickly the model learns from the available data.\n   - **Generalization:** Evaluation of how well the model performs on out-of-distribution (OOD) inputs, measuring the robustness and adaptability of the learned policies.\n\n**Main Experimental Results:**\n\n1. **Effectiveness of RL Methods:**\n   - **Policy-Gradient Methods:** Demonstrated superiority in optimizing stochastic policies suitable for language tasks. These methods provided higher cumulative rewards due to their ability to incorporate and optimize stabilization in language models with stochastic decoding.\n   - **Value-Based Methods:** While effective in certain scenarios, these methods were generally outperformed by policy-gradient methods in terms of cumulative reward and generalization capabilities.\n\n2. **Comparison with Supervised Fine-Tuning:**\n   - Models trained with RL methods outperformed those using SFT, especially in scenarios requiring exploration and adaptation to varying reward structures. RL methods benefitted from richer gradient updates that considered positive, negative, and neutral feedback, leading to better sample efficiency and improved OOD performance.\n\n3. **Inverse Reinforcement Learning (IRL):**\n   - IRL approaches, which frame training as learning the reward function from human feedback data, showed promise in aligning model outputs more closely with human preferences, although detailed results were not provided in the summary.\n\nThe main findings highlight the robust performance and adaptability of policy-gradient RL methods for language model optimization, emphasizing the benefits of learning from a broad spectrum of feedback types. This comprehensive understanding paves the way for further advancements in RLHF applications."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Investigate how human feedback, represented as oracular reward, can influence RLHF processes and analyze the preferred types of feedback for training reward models.",
            "experiment_process": "The study assumes a deterministic oracular reward function and focuses on two main types of feedback: point-wise numerical feedback (ratings) and pairwise ranking feedback (preferences). Experimental setups include collecting ratings and preferences, and applying RL algorithms with generalizable reward models. Evaluation tools include different feedback comparison methodologies, using techniques such as learning-to-rank and observations on the ease of human feedback provision.",
            "result_discussion": "Preference feedback is found to be advantageous due to higher training data generation and easier cognitive processing for humans as opposed to rating feedback. Additionally, deterministic reward models are limited, suggesting a need for stochastic feedback models to handle human preference variability and contexts like personalization and dialogue adaptation.",
            "ablation_id": "2404.08555v2.No1"
        },
        {
            "research_objective": "Understand how learning a reward model can help convert RLHF into a standard RL problem while managing the cost of collecting online feedback.",
            "experiment_process": "The study involves learning a reward model using different types of human feedback, formulating the problem with rating feedback and preference feedback, and applying likelihood functions like the Bradley-Terry model. The goal is to minimize the MSE for continuous outputs and maximize the log-likelihood dataset for preference feedback. Tools include reward models and large datasets of human feedback.",
            "result_discussion": "Reward models help reduce the practical shortcomings of RL by addressing the expensive feedback collection. The model generalization errors decrease faster than policy errors, displaying potential improvements in RLHF. Still, specific assumptions and the form of feedback and feedback encoding must be critically chosen to ensure this.",
            "ablation_id": "2404.08555v2.No2"
        },
        {
            "research_objective": "Examine the practical limitations in estimating reward models from human feedback in RLHF and their implications.",
            "experiment_process": "The experiment considers the hypothetical dataset of possible outputs and focuses on issues of context and output coverage. Using supervised learning results, researchers analyze context and output coverage ratios and their effect on the reward model. Tools include evaluating generalization performance and feedback dataset organization.",
            "result_discussion": "Reward models are often inaccurate due to small feedback datasets relative to the entire dataset. These inaccuracies cause faulty generalizations and phenomena such as 'reward hacking' and hallucinations. Sparse and delayed feedback further exacerbates these core issues, indicating the necessity for denser and more accurate feedback mechanisms.",
            "ablation_id": "2404.08555v2.No3"
        },
        {
            "research_objective": "Evaluate the roles and limitations of supervised fine-tuning as a key step in the RLHF pipeline.",
            "experiment_process": "Human feedback gathered in the form of expert demonstrations is used to finetune LLMs with supervised learning objectives. Preference data is also exploited for fine-tuning. Comparisons between different datasets, including public NLP datasets and prompts from real API usage, are conducted. Empirical studies dissect the effectiveness of such finetuning methods.",
            "result_discussion": "Supervised fine-tuning is critical as it provides robust initial policies improving subsequent RLHF methods. However, it fails to address out-of-market distributions, leads to unintended behaviors such as hallucinations and improper handling of adversarial situations, necessitating further improvement methods incorporating both positive and negative feedback.",
            "ablation_id": "2404.08555v2.No4"
        },
        {
            "research_objective": "Identify methods to improve reward model training stability and efficacy while mitigating reward over-optimization.",
            "experiment_process": "A pre-trained or supervised fine-tuned model is used with an additional regularization term, i.e., a KL divergence penalty between the initial and trained policies. Parameter updates aim to ensure minimal deviation from the initial policy. The examination includes empirical variances using fixed KL distance comparisons.",
            "result_discussion": "KL penalty regularization helps retain model stability, preventing over-exploitation of reward model imperfections. This approach also resembles Bayesian inference and balances training alterations to generate meaningful outputs without deviating significantly from the initial model's performance.",
            "ablation_id": "2404.08555v2.No5"
        },
        {
            "research_objective": "Evaluate iterative RLHF methods to address miscalibrations in reward models.",
            "experiment_process": "An iterative training process is used, involving generating new outputs using updated policies, which are then annotated for feedback and retrained. This process evaluates the notion of generating progressively rewarding outputs to align better with human feedback. Techniques evaluated include empirical algorithms like Proximal Policy Optimization (PPO).",
            "result_discussion": "Iterative RLHF or Online-RLHF shows effective results by aligning rewards more accurately with human preferences. However, it is time-consuming, expensive, and requires significant hyperparameter tuning, highlighting a need for further refinement and efficiency in iterative RL processes.",
            "ablation_id": "2404.08555v2.No6"
        }
    ]
}