{
    "title": "SecureLLM: Using Compositionality to Build Provably Secure Language Models for Private, Sensitive, and Secret Data",
    "abstract": "Traditional security mechanisms isolate resources from users who should not access them. We reflect the compositional nature of such security mechanisms back into the structure of LLMs to build a provably secure LLM; that we term SecureLLM. Other approaches to LLM safety attempt to protect against bad actors or bad outcomes, but can only do so to an extent making them inappropriate for sensitive data. SecureLLM blends access security with fine-tuning methods. Each data silo has associated with it a separate fine-tuning and a user has access only to the collection of fine-tunings that they have permission for. The model must then perform on compositional tasks at the intersection of those data silos with the combination of those individual fine-tunings. While applicable to any task like document QA or making API calls, in this work we concern ourselves with models that learn the layouts of new SQL databases to provide natural-language-to-SQL translation capabilities. Existing fine-tuning composition methods fail in this challenging environment, as they are not well-equipped for handling compositional tasks. Compositionality remains a challenge for LLMs. We contribute both a difficult new compositional natural-language-to-SQL translation task and a new perspective on LLM security that allows models to be deployed to secure environments today.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Using language models in environments with sensitive information is fraught with problems. Models can be convinced to reveal information they should not, to answer questions they should not, to reveal their training data and prompts, to run API calls they should not, etc. Various approaches have tried to grapple with this problem, yet no prior work offers a method that provides any guarantees, merely offering mitigations. When dealing with sensitive data, this is practically and often legally insufficient. We provide the first method to build provably secure LLMs by reflecting the compositionality inherent in access security into the LLM.\nWe consider the scenario where a number of silos of information are available (fig. 1  ###reference_###). These may be APIs, databases, or collections of documents with sensitive information. Each user has access to some subset of these silos. Nominally, one could imagine an exponential number of LLMs with an LLM for each subset of silos. This would guarantee security, as an LLM being used would only be trained on data that the user has access to. Creating and maintaining an exponentially-increasing number of models is impractical. Instead, we show how to achieve the same goals with a linear number of LLM fine-tunings. While using only one fine tuned model per silo, we can configure and compose a model specific to the user\u2019s permissions at runtime.\nKnowledge about the target domain is critical. A silo may contain a large volume of documents, databases, or APIs, each with their own internal logic. Even Retrieval-Augmented Generation (RAG) is insufficient for such applications because it presupposes that the model understands the logic of a target domain. You cannot accurately retrieve something that you don\u2019t understand at all; imagine a new technology or capability. SQL translation tasks offer an extreme version of this where the target database schema must be known. Often, the schema itself is highly sensitive and should not be revealed to all users. This is the scenario we consider here.\nTo build such secure LLMs we rely on fine-tuning. For each information silo we fine-tune a model using a method that isolates the weight updates to a small part of the model which can then be excised and stored separately. Then, when a user interacts with the system, their permissions automatically determine the collection of fine-tunings that must be applied to the model and how those fine-tunings must be applied. For example, a user with access to silos A, B, and C that wants to communicate to another user with access to only A and B, would set up their model with positive fine-tunings for A and B in order to actively avoid topics related to C.\nThis idea is reminiscent of recent work like LoraHub (Huang et al., 2023  ###reference_b10###) which also composes fine-tunings. Given a target task, LoraHub selects a set of fine-tunings, Low Rank Adapters (LoRAs) (Hu et al., 2021  ###reference_b8###), that are added together. However, LoraHub is designed for soft tasks, where a model already tends to perform well, and where LoraHub\u2019s aim is to increase performance by a few percent. The domain we consider here is radically different, we seek compositionality for unrelated silos of information. Questions that require access to silos A and B, by definition cannot be answered with access to only A or only B. The underlying performance of models is nearly zero. LoraHub is not well-suited to such tasks and performs very poorly. It is only through exploiting the compositionality of security as a domain that our methods achieve much higher performance.\nOur contributions are:\n1. formulating a difficult new compositional task that LLMs have great difficulty with \u2013 natural-language-to-SQL where not only is a model trained on queries of individual databases but also where the generated response requires cross-database joins,\n2. formulating the notion of access security in terms of this task,\n3. demonstrating that existing fine-tuning methods fail in this compositional environment,\n4. the introduction of new compositional fine-tuning methods for this problem,\nWhile we only concern ourselves with the task of understanding queries by translating them to SQL, our methods are generic and can be applied to numerous other domains like translating commands to API calls and answering questions from large collections of documents."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Model Composition. Our framework relies on composing LLM fine-tunings at inference time which follows a set of previous works that use model composition. A recent method combines pretrained LLM prompts each tuned for similar tasks to achieve generalization on downstream tasks (Sun et al., 2023  ###reference_b18###) which requires training at inference time. AdapterSoup composes fine-tunings by linearly averaging their weights depending on a criteria to determine which fine-tunings are relevant to the new domain (Chronopoulou et al., 2023  ###reference_b4###). PEM Addition is a method that doesn\u2019t require further training such as composing fine-tunings using arithmetic operations directly on the weights (Zhang et al., 2023  ###reference_b22###). LoraHub is a recent simple framework that also composes different LoRa fine-tunings (Hu et al., 2021  ###reference_b8###) at inference time where each fine-tuning is trained on a different task (Huang et al., 2023  ###reference_b10###); we include comparisons using LoraHub and PEM Addition. Privacy Attacks. Many recent works discuss a range of different privacy attacks against large language models and Deep Learning models in general. Membership inference attacks are a type of privacy attack which try to determine if a piece of text was contained in the training data of a model possibly without access to the weights (Hisamoto et al., 2020  ###reference_b7###; Nasr et al., 2019  ###reference_b16###; Hu et al., 2022  ###reference_b9###). An even larger security risk is posed by training data extraction attacks where large language models leak text in their training data verbatim (Carlini et al., 2019  ###reference_b2###) including personally identifiable information (Inan et al., 2021  ###reference_b11###). This attack is shown to be successful even when such data was only mentioned in a single document and this behavior worsens with an increase in model size (Carlini et al., 2021  ###reference_b3###). Similarly, training data extract attacks were effective on models fine-tuned on a smaller dataset (Zanella-B\u00e9guelin et al., 2020  ###reference_b21###). With recent work tackling these privacy issues for Retrieval-Augmented Generation using multi party communication (Zyskind et al., 2023  ###reference_b25###). Differential Privacy. A popular algorithmic technique to train machine learning models with certain privacy guarantees is differential privacy (Abadi et al., 2016  ###reference_b1###) which has also been applied to large recurrent language models (McMahan et al., 2017  ###reference_b15###). Multiple recent works manage to use differentially private learning on large language models with hundreds of millions of parameters to achieve efficient differentially private fine-tuning with slight degradation in performance (Li et al., 2021  ###reference_b13###; Yu et al., 2021  ###reference_b20###). Many other methods borrow inspiration from differential privacy like Confidentially Redacted Training which provably prevents memorization of the training data (Zhao et al., 2022  ###reference_b24###). However, there are differences between Differential Privacy and our approach. In differential privacy, there exists a non-zero amount of privacy loss parameterized by the privacy budget ( and ) from the resulting model as the privatization step minimizes but does not completely ensure that the updated model parameters do not leak private information. Additionally, there is a difference in what is considered private information compared to what is non-confidential. Differential Privacy considers the individual records within the training data as private such that any individual record is sufficiently obfuscated while holistic trends are still learnt by the resulting model. Comparatively, our notion of privacy ensures that every individual record in a private silo needs to be completely private including any and all holistic information gained from the silo. A common trend in privacy preserving research into LLMs is the focus on preventing models from memorizing and/or leaking individual records or pieces of information while attempting to maintain a holistic distillation of the training data into the model. This distinguishes our work as we consider any and all information gleaned from a silo to be confidential no matter how granular or holistic it is. We also avoid falling back to probabilistic claims or claims in expectation and ensure that all claims of confidentiality are provably correct as the resulting model weights that an individual user interacts with have only been optimized using data that the user is authorized to access."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Framework",
            "text": "SecureLLM takes several fine-tunings each trained on distinct information silos and composes them at inference time. The goal of the composed model is to answer questions about both individual silos and questions that span silos. For example, in our case, a natural-language to SQL LLM would need to be able to generate joins across the databases of multiple silos to answer complex questions that have never been seen at training time. This is a trivial task for humans, but one that challenges LLMs. We go a step further: not only must such an LLM work, it must operate through a combination of fine-tunings, i.e., not only has it never seen combinations of silos at training time, its fine tunings have only ever seen a single silo each. This challenges, and defeats, current fine-tuning methods. The upshot of this difficult task is that it solves several key security problems for LLMs.\nGiven  data silos  and  fine-tuned LLMs  where  has been fine-tuned on the data silo , and given a set of target indices , the goal is to obtain a composed model  at inference time with no additional training such that  is able to correctly answer any question about the information contained in the target silos  and should fail to answer any question about information not contained in the target silos  as to not leak any information that the desired model  is not intended to have. Additionally, the target model  should be able to answer new union questions  where  where the question relies on information contained in both  and . We note that the union questions  are not answerable by any individual data silos, thus none of the individual models  are able to answer any union questions while a successfully composed model should be able to answer such questions without the need of any training.\nIt is critical that the composed model  has no knowledge of information silo that the user is not authorized to access, i.e. data silos . Without this condition, a trivial solution is to train a single model  on all data silos  however this approach is susceptible to leaking confidential information as the model would have knowledge of information contained in silos that users are not authorized to view and thus is not a valid approach. This approach is also problematic for scenarios that employ security through contradiction, in that some silos may directly contradict information in another silo in order to protect sensitive information (SecureLLM could potentially solve this by applying weights to silos of higher confidentiality). We refer to  as the Exponential Model that has seen every combination and such a model is used as an insecure upper bound to performance in our experiments.\nAn alternative to composing fine-tunings while also preserving privacy would be to create an exponential number of models, one for the powerset of information silos. This would maximize performance and minimize the amount of generalization needed, as long as one had a way to automatically generate cross-silo questions, perhaps with another LLM. This is obviously impractical. In essence, our method provides the advantages of the exponential approach but with linear storage and training runtime."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Security as a relaxation",
            "text": "What sets security-related issues apart from other fine-tuning combination scenarios is a relationship to a simple and easy-to-formalize idealized problem. Assume that each silo has a disjoint alphabet, that the contents of each silo is modeled by a state machine, and that the output the LLM must produce should be an utterance that is accepted by the union of the per-silo state machines. In this setting, security is simple. The optimal output depends on one silo at a time and there is no need to mix information about silos. The methods we provide here are optimal in this idealized setting because of its likeness to SQL.\nFor SQL generation tasks the model primarily must insert table and column names which are part of individual silos while following the overall structure of SQL which is silo-independent. Of course, real security scenarios are relaxations of this problem. For example, cross-silo question answering, is faithful to this idealized problem if portions of the answers are derived entirely from individual silos, while if they require cross-silo reasoning then QA becomes a relaxation. Current methods, as we show below, are not well-suited for this idealized problem or its relaxations. Formalizing this setting will hopefully lead to new composition operators."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Composing fine-tunings",
            "text": "We discuss several existing methods, none of which perform well. A few other plausible methods that also do not perform well are shown in the appendix. Finally we describe two new methods that do perform well with one clear winner.\nLoraHub The LoraHub method for composition introduced by Huang et al. (2023  ###reference_b10###) is a two-step process involving element-wise summation of LoRA fine-tunings (COMPOSE), and then learning weight optimizations via gradient-free methods to apply to each fine-tuning (ADAPT). For this paper, we do not implement the ADAPT stage because weights for every possible combination would need to be learned, and we could not say that this process is completed at inference time.\nWe observe that LoraHub performs poorly on the secure composition task. The authors warn that combining too many fine-tunings can lead to poor performance, however this cannot be the source of poor performance as we compose only up to three LoRAs.\nPEM Addition The summation method introduced by Zhang et al. (2023  ###reference_b22###) is similar to LoraHub, however, instead of summing the embeddings of the encoder and decoder prior to receiving the input , one executes each fine-tuning independently at the attention-layer level, and then adds the result. This version of summed composition shows improved performance over LoraHub.\nAverage of Adapter Weights Computing the simple average of each Lora fine-tuning response, as suggested by Chronopoulou et al. (2023  ###reference_b4###), , produced compositions that were 50% less effective than PEM Addition in initial informal tests.\nVariations of LogSumExp of Adapter Weights Du et al. (2020  ###reference_b6###) proposes a disjunctive composition process based on Energy Based Modeling, . Every variation tried performed significantly worse than PEM Addition, and upon closer inspection, this process substantially distorts encoder and decoder embeddings.\nAdapter Concatenation The Mangrulkar et al. (2022  ###reference_b14###) library implements weight concatenation, however we found that concatenating LoRA encoder/decoder fine-tunings performed significantly worse than PEM Addition."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Our Methods",
            "text": "Maximum Difference The intuition behind this method is to select the embeddings from each fine-tuning with the strongest response (either positive or negative) at each attention layer. In order to accomplish this, each LoRA fine-tuning is evaluated separately on input . Then a mask of zeros with the same dimension as the output is created, , to aggregate LoRA responses. For each LoRA fine-tuning response , an element-wise comparison is made, and if the absolute values of the fine-tuning response is greater than the aggregated response, then the signed response from that fine-tuning replaces the element in the aggregated response.\nLogit Composition Given fine-tunings to compose  and input , we define logit composition as performing the complete forward pass for each fine-tuning independently to obtain logit probabilities. We select the maximum value of each logit. One could instead sum logits for each fine-tuning. We found little difference between the two implementations, although the sum may have issues as the number of fine-tunings increases.\nNote that we are not claiming this method to be a superior compositional approach in every case. The requirements of compositional security are different than those of some other compositional tasks. In a sense, by its very nature, compositional security implies that most of the time every silo but one is irrelevant and confused, and one silo is likely to produce confident results. This motivates our compositional methods and explains why other methods preform so poorly."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Data generation",
            "text": "Our goal is to automatically create a challenging dataset for compositions of silos. There are countless other natural-language-to-SQL datasets out there, which we do not aim to replace. As such, we focus specifically on within and between silo questions. We also cover only a portion of SQL. We do not aim to exhaustively test how well models understand SQL, we aim to understand how well models generalize their knowledge from questions about individual silos to questions that span silos. As such, we consider only a subset of SQL which is otherwise an imposingly complex language.\nWe automatically generate SQL databases, one per silo, with 2-3 tables per database, that share columns which can be joined together both within and between databases. Databases are otherwise disjoint and on different topics. For each database we generate natural language questions along their equivalent SQL. Then, we generate questions and SQL pairs that span pairs and triples of databases. Two methods are used to generate these pairs: a CFG (see fig. 2  ###reference_###) and ChatGPT 4 (see fig. 3  ###reference_###). The CFG generates both the SQL and the question in parallel. We do this at large scale, with 100,000 pairs per silo or combination of silos. To ensure that our results scale to more realistic queries we also generate 300 pairs per silo or combination of silos.\nWe limit the scope of generated SQL statements. All statements generated from our CFG, an excerpt of which is shown in the appendix, are select statements that only contain the SQL keywords from, natural join, and where. The majority of the complexity is in the where clause which requires specialized knowledge about the schema along with language comprehension to properly generate based on the input question. The task for the LLM is to generate the where clause of an SQL statement which answers the input question.\nWe introduce a useful normalization, for which we provide ablations in the results section. This normalization is closely related to 6NF Date et al. (2003  ###reference_b5###). It ensures that joins are natural and that column names are easily identified. In general, this transformation could help all SQL LLMs. One could provide columns with unique names, factor away complex relationships, and design redesign schemas such that joins are natural. This transformation is bidirectional, one could normalize such a schema, generate a query against it, and then transform those queries back to the original schema. Little to no work exists on database normalizations that are specific to LLMs. Existing database normalizations focus on ease of query generation for humans and concerns like execution efficiency for machines. As described in the results section, our composition methods are far superior irrespective of this normalization. But we believe it is a valuable observation that is likely to lead to many more LLM-specific normalizations as they become serious consumers of SQL.\nFrom each SQL schema we randomly generate an instance of that SQL database. For each sample, the LLM\u2019s output is assembled into an SQL statement then executed on the database to obtain the query results, if the query results are exactly equivalent to the query result of the ground truth SQL statement then the LLM\u2019s output is considered correct. Furthermore, we ensure that each query in training and validation responds with at least one record to avoid trivial false positives. Given that natural language questions can lead to long queries, this is a high bar, as even the smallest mistake leads to zero performance.\nWe employ a second score that parses the generated query conditions into a tree then calculates the tree-edit distance Zhang et al. (1996  ###reference_b23###) between the ground query and the generated query. This is the number of edit operations required to transition between the two, which are normalized by the number of nodes in the ground tree. These are averaged across all queries for a silo or collection of silos. This edit distance score provides a far more fine-grained view into the performance of models, fine-tunings, and compositions of fine tunings."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "To demonstrate the capabilities of model composition at inference-time, we first begin by obtaining individual fine-tunings that are knowledgeable in a single silo by fine-tuning a Llama-2-7B model for each silo separately. The fine-tuning results in a Low-Rank Adaptation (LoRA) for each silo which can independently be applied to the base Llama-2-7b model. Once the individual LoRA fine-tunings are obtained we compose them using one of several compositional schemes with the requirement that the composition happens at inference time with no additional training. We additionally train two insecure baseline models that act as an upper-bound using LoRA, the baseline generalized model is trained on all the individual silos together and must then generalize it\u2019s knowledge to the union silos. While the baseline exponential model also breaks privacy guarantees by training on all the individual silos along with the union silos, the term exponential refers to the fact that training such a model while preserving privacy would mean that  models would need to be trained where  is the number of silos in the database. Both baseline models are considered insecure as there is no method of removing knowledge about certain silos at inference time when the user does not have the sufficient credentials unlike our proposed SecureLLM method which can remove and add fine-tunings with each silo\u2019s knowledge at inference time. We fine-tune all models with one epoch until saturation (achieving near 100% accuracy on the CFG validation set) using a frozen Llama-2 7B (Touvron et al., 2023  ###reference_b19###) with a trainable LoRa fine-tuning (Hu et al., 2021  ###reference_b8###) using LoRa parameters  and a dropout (Srivastava et al., 2014  ###reference_b17###) of , an Adam optimizer (Kingma and Ba, 2014  ###reference_b12###) with a learning rate of , a batch size of , and a weight decay of .\nWe report the results of the two insecure baseline models along with the secure composition ( where  was trained on ) using multiple compositional methods (section 3.2  ###reference_###, section 3.3  ###reference_###) including our best method (in section 3.3  ###reference_###) with and without using 6NF-like database normalization, which is equivalent to the scenario where a user has credentials to access . We note that neither the baseline generalized model nor the secure compositions have seen the union Silos (, , , and ) and that only the exponential baseline model has been trained on those silos. The performance of the composed fine-tunings on the individual Silos would give an indication as to whether the resulting composition is able to retain the knowledge of each individual fine-tuning from each separate Silo; This performance is expected to be traded off for privacy while the better compositional methods mitigate the extent of this trade off and maintain maximal privacy. The performance on the union Silos indicates whether the composed fine-tunings are able to successfully generalize knowledge from the individual fine-tunings which is an essential component in answering questions that no individual fine-tuning or silo can answer."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "The highest performance one can possibly achieve is if the LLM is trained not just on every silo but the powerset of silos, i.e., the insecure baseline exponential model described above. Realistically, our model is upper-bounded by a variant of the model that sees all of the silos at training time, but sees no combination of silos. Both of these are insecure, in that they have access to all of the data. Our goal is to find a method to combine individual silo fine-tunings to reproduce the performance of the baseline generalized model.\nRaw overall performance is not a relevant metric, although we report it in each case for the baseline models. Raw performance is a function of the size of the model. And we use the modestly sized Llama-2-7B. What is critical is the fraction of retained performance. This is what we focus our results on, the difference in tree edit distance.\nLoraHub and PEM addition were the only two competitive methods that were previously published. All other methods described earlier performed so poorly we did not include them in the final results table to make room for additional experiments.\nIn table 1  ###reference_### we report performance for the CFG-generated data. Note that for every probe silo combination our methods have by far the lowest tree edit distances. Even without the database normalization described above, our methods outperform all others in every case. With the database normalization our method retains all the performance that exists, i.e., it nearly always matches the tree edit distance of the baseline generalized model.\nOne might wonder if these results are merely an artifact of the CFG-based approach. When replicating the same experiment with sentences rephrased by ChatGPT, see table 2  ###reference_###, we come to the same conclusions. LoraHub and PEM Addition, along with all prior methods we attempted significantly underperform our approach. Note that this is an extremely challenging test set as the ChatGPT paraphrases are only used for testing, not for training.\nTo guard against a potential trivial solution to this problem, we also introduce a column-name obfuscated version. A model that is good at guessing a likely name for a table based on the entities it refers to might otherwise get a leg up. We are interested in the ability of models to retain compositional reasoning, rather than circumventing the task. In any case, in real world conditions column names are often rather complex. In table 3  ###reference_### each column is given an arbitrary but stable and coherent name, in this case an animal. Relative to table 1  ###reference_### our method loses little performance, meaning that it encourages compositional reasoning."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "LLM security is critical to numerous commercial and government applications. We take a different view of LLM security compared to that of prior work, one where we import the traditional notion of access security to LLMs. This is enabled by the new compositional methods we introduce that prove themselves to be effective.\nNote that our experiments are an extreme case, one where no prior knowledge is useful for understanding the structure of a silo, particularly in the case where column names are obfuscated. We also consider a weak LLM, Llama-2-7B. We showed that these novel composition methods are able to take advantage of the generalization capabilities of the LLM with SQL edit distances that are the same or even at times better than the baseline LLM when it attempts to generalize. In other words, fine-tuning the LLM on each silo jointly, performs as well as fine-tuning on each silo individually and combining the fine-tunings. This is as much as one could hope for. Practical applications would need to use a far stronger underlying LLM to achieve high execution accuracy.\nThere are numerous possible extensions of this work, including applications to document QA where each silo is a collection of documents rather than a database. One possible followup could look at the converse task, given a question determine the silos necessary to answer it. This could be used to monitor conversations or to automatically mark the security level of an exchange between users. Another possible direction would be to look at negative silos that exclude information. A negative silo would explicitly avoid a topic, which would prevent accidental leaks. Models could rewrite text or data to refer or exclude particular silos. The traditional world of access security is rich with problems for LLMs to address, and our work opens up the way for doing so. In addition, by providing provable security, i.e., there can be no leaks from silos the user doesn\u2019t have access to, we take a key step toward enabling the use of LLMs in secure environments."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Additional Information",
            "text": ""
        },
        {
            "section_id": "8.1",
            "parent_section_id": "8",
            "section_name": "Limitations",
            "text": "We disentangle and address a very specific slice of LLM safety, one that is often commingled with a larger story about safety.\nSecureLLM only concerns itself with security in the traditional sense: quantized access permissions to data.\nIt relies on traditional security techniques to manage access permissions. There is a widely\nheld belief that LLM security is a totally disjoint new field, but as we show, with the SecureLLM\napproach we can reduce many of those security problems back to traditional access permission issues.\nMany security problems of LLM are manageable through traditional means when one can assume that\nonly vetted actors have access, just as they are with current document storage systems. The same\nsystems which ensure patient privacy, financial privacy, and that manage secret information today can be\nused to manage collections of fine-tunings, and the same supervision methods can trace access to the LLM.\nFrom this perspective SecureLLM solves data leakage and prompt injection attacks, in the same sense that\ntraditional security solves data leakage: those without permissions cannot access this information,\nand those with permissions have full access with training and supervision. Although, in the future one might imagine extensions that\nprovide more fine-grained permissions. Organizations are already set up for this form of security,\nboth for managing the user permissions and for the associated documents, making the deployment of SecureLLM\nstraightforward. In settings where this structure is not available or not appropriate, SecureLLM is\nnot directly applicable, but those scenarios never had meaningful security to begin with.\nExplicitly out of scope are other notions of safety and security. For example, the LLM may still\nfabricate information, produce toxic or biased results, follow guidance that it should not, etc.\nThe only mitigation that we offer is that only a user that has permissions to that data will\nbe impacted directly; hopefully this user will receive appropriate training about the limitations\nand dangers of LLMs."
        },
        {
            "section_id": "8.2",
            "parent_section_id": "8",
            "section_name": "Ethics",
            "text": "SecureLLM could pose some ethical issues.\nAt the moment, surveillance is limited by the need to process a deluge of data. This results in mostly processing metadata. Enabling LLMs to work in secure environments could contribute to large-scale monitoring, detection, and tracking. Novel uses of LLMs in secure environments can advance numerous defense applications.\nAs with many other dual-use technologies, we hope that this work will be used for positive ends."
        },
        {
            "section_id": "8.3",
            "parent_section_id": "8",
            "section_name": "Computational Considerations",
            "text": "We use NVIDIA Titan RTX 24GB VRAM GPUs for all our experiments.\nFor all of our PEFT parameters, less than one GPU-hours per PEFT was required for training. Between around one GPU-hours was required for composition growing with respect to the number of compositions. For each run, approximately 20 GB of VRAM is needed as we use half precision for all training and inference. Each PEFT can be trained and inferenced on one GPU.\nWe estimate a total of 10 GPU-hours is required to replicate results for training, and 20 GPU-hours is required to perform the same experiments described in this manuscript."
        },
        {
            "section_id": "8.4",
            "parent_section_id": "8",
            "section_name": "Reproducibility",
            "text": "All code and data required to reproduce our work will be provided at this GitHub repository under the MIT license https://github.com/Scuwr/SecureLLM  ###reference_###."
        },
        {
            "section_id": "8.5",
            "parent_section_id": "8",
            "section_name": "Acknowledgements",
            "text": "This work was supported by the DARPA Machine Common Sense (MCS) program, the Center for Brains, Minds, and Machines, NSF STC award CCF1231216, the NSF award 2124052, the MIT CSAIL Machine Learning Applications Initiative, the\nMIT-IBM Watson AI Lab, the DARPA Artificial Social Intelligence for Successful Teams (ASIST)\nprogram, the DARPA Knowledge Management at Scale and Speed (KMASS) program, the United\nStates Air Force Research Laboratory and the Department of the Air Force Artificial Intelligence\nAccelerator under Cooperative Agreement Number FA8750-19-2-1000, the Air Force Office of\nScientific Research (AFOSR) under award number FA9550-21-1-0014, and the Office of Naval\nResearch under award number N00014-20-1-2589 and award number N00014- 20-1-2643. The views\nand conclusions contained in this document are those of the authors and should not be interpreted as\nrepresenting the official policies, either expressed or implied, of the Department of the Air Force or\nthe U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for\nGovernment purposes notwithstanding any copyright notation herein."
        }
    ],
    "url": "http://arxiv.org/html/2405.09805v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "6"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "5",
            "6"
        ]
    },
    "research_context": {
        "paper_id": "2405.09805v2",
        "paper_title": "SecureLLM: Using Compositionality to Build Provably Secure Language Models for Private, Sensitive, and Secret Data",
        "research_background": "**Motivation:**\nThe primary motivation for the paper is addressing the challenges associated with using language models (LLMs) in environments that handle sensitive, private, or secret information. Existing methods for securing LLMs in such contexts typically offer mitigations but lack formal guarantees of security, which is inadequate both practically and legally. The authors aim to provide a reliable method to build provably secure LLMs by mirroring the inherent compositionality of access security within the models themselves.\n\n**Research Problem:**\nThe research problem tackled in this paper is the challenge of building LLMs that can securely handle sensitive data and adhere to user-specific access permissions. Traditional approaches to achieving security in LLMs, such as creating separate models for every subset of information \"silos,\" are impractical due to their exponential scalability constraints. The authors propose a novel method that reduces this complexity by using a linear number of model fine-tunings and dynamically composing these at runtime based on user permissions.\n\n**Relevant Prior Work:**\n1. **LoraHub (Huang et al., 2023)**: LoraHub is mentioned as a system that also composes fine-tunings using Low Rank Adapters (LoRAs) to enhance model performance. However, it targets \"soft tasks,\" which differ significantly from the hard compositional tasks considered in this paper, where baseline model performance without fine-tuning is nearly zero.\n2. **Retrieval-Augmented Generation (RAG)**: The authors note that RAG methods are inadequate for scenarios where the model must understand the internal logic of target domains, such as SQL translation tasks involving sensitive database schemas.\n\nThe paper builds upon the ideas of fine-tuning and compositional methods but significantly diverges by focusing on security and the need for strict access control in environments with sensitive data. The authors emphasize that their methods can apply to various other domains beyond SQL translation, such as API commands and question answering from large document collections.",
        "methodology": "### SecureLLM: Methodology\n\nThe proposed method, SecureLLM, aims to utilize compositionality to develop language models that can securely process private, sensitive, and secret data. This methodology involves the following key steps and innovations:\n\n1. **Fine-Tuning on Distinct Information Silos:**\n   - Multiple language models (LLMs) are fine-tuned individually on different information silos. Each fine-tuned LLM \\( M_i \\) is trained exclusively on a single data silo \\( S_i \\). This ensures that each model only has knowledge of a specific subset of data.\n\n2. **Compositional Inference:**\n   - At inference time, these separately fine-tuned models are composed to form a single model \\( M_T \\). This composed model is capable of answering queries that involve information from multiple data silos \\( T \\). The composition happens without any additional training, thus allowing the model to handle queries that span across different silos.\n\n3. **Security through Silo Isolation:**\n   - The composed model \\( M_T \\) is designed to only have access to and answer questions about the information contained in the target silos \\( T \\). It is critical that it fails to answer any questions involving information not contained in the authorized silos to prevent information leakage.\n\n4. **Answering Union Questions:**\n   - The model is designed to answer new union questions \\( Q_U \\) that depend on data spread across multiple silos. These union questions \\( Q_U \\) cannot be answered by any individual model \\( M_1, M_2, ..., M_n \\) alone, but the composed model \\( M_T \\) can address them without any prior exposure during training.\n\n5. **Preventing Unauthorized Access and Leaks:**\n   - The approach ensures that the composed model \\( M_T \\) has no unauthorized knowledge beyond the silos specified in \\( T \\). This prevents trivial solutions, such as training a single model on all silos, which would risk leaking confidential information and fail to handle scenarios where silos contain contradictory information.\n\n6. **Efficiency and Practicality:**\n   - Instead of creating an impractical exponential number of models for every combination of silos, the method achieves the same benefits with linear storage and training runtime. This makes the approach both efficient and practical.\n\nIn summary, SecureLLM's methodology centers around isolating fine-tuning processes to individual silos and then composing these models at inference time to answer complex, multi-silo queries securely and efficiently. The method ensures data privacy and security by preventing models from having unauthorized knowledge, contrasting with less secure and less practical alternatives.",
        "main_experiment_and_results": "### Main Experiment Setup\n\nThe primary objective of the main experiment is to validate the effectiveness of model composition at inference time within the context of maintaining both knowledge generalization and privacy. The setup includes the following key components:\n\n**Datasets**:\n- Multiple silos containing specialized knowledge domains, each used for individual fine-tuning of the Llama-2-7B model.\n\n**Models**:\n1. **Individually Fine-Tuned Models**: A Llama-2-7B model fine-tuned separately on each silo dataset to generate Low-Rank Adaptations (LoRAs).\n2. **Secure Compositional Models**: Compositions of the individual fine-tuned models using various compositional methods at inference time without additional training.\n3. **Insecure Baseline Models**:\n   - **Generalized Baseline Model**: A model trained on all individual silos to generalize knowledge across all silos.\n   - **Exponential Baseline Model**: A model trained on individual silos as well as a union of all silos, representing a privacy-compromised scenario.\n\n**Evaluation Metrics**:\n- **Accuracy on Individual Silos**: Measures whether each compositional method retains the specialized knowledge of individual fine-tunings.\n- **Accuracy on Union Silos**: Assesses the ability of the composed models to generalize and integrate knowledge from multiple silos.\n\n**Hyperparameters for Fine-Tuning**:\n- Number of epochs: 1 (until saturation, achieving near 100% accuracy on CFG validation set)\n- Llama-2 7B with frozen parameters\n- LoRA parameters: unspecified\n- Dropout rate: unspecified\n- Adam optimizer with a learning rate of unspecified\n- Batch size: unspecified\n- Weight decay: unspecified\n\n### Main Experimental Results\n\n**Baselines Performance**:\n- **Generalized Baseline Model**: While it effectively generalizes across all individual silos, it fails to ensure privacy as it contains consolidated knowledge.\n- **Exponential Baseline Model**: Provides an upper-bound performance on union silos, but compromises privacy due to its comprehensive training.\n\n**Secure Compositional Methods**:\n- The secure composition of fine-tunings demonstrated the capability to retain knowledge from individual silos effectively while also enabling generalization across silos at inference time.\n- Multiple compositional schemes showed varying levels of trade-off between preserving privacy and achieving generalization, with the best methods showing minimal knowledge trade-off.\n\n**Privacy Insights**:\n- The experiment confirmed that the composed fine-tunings could preserve individual silo privacy while allowing authorized combinations to generalize across them.\n- Neither the generalized baseline nor the secure compositions had access to the union silos during training, unlike the exponential baseline model, underscoring the security advantages of the proposed SecureLLM method.\n\nThese results affirm that SecureLLM's approach enables secure and flexible knowledge integration at inference time, upholding privacy guarantees without sacrificing generalization capabilities."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To assess the impact of a normalization technique that ensures natural joins and easily identifiable column names on the performance of LLMs in natural-language-to-SQL translation tasks.",
            "experiment_process": "The framework involves generating SQL databases with 2-3 tables per database. These tables share columns that can be joined within and between databases. For each database, questions and corresponding SQL statements are generated, and further, questions that span multiple databases are created. 100,000 pairs per silo or combinations are generated for large-scale tests, plus an additional 300 pairs for more realistic queries. The normalization technique, related to 6NF, ensures natural joins and easily identifiable column names. LLMs generate the WHERE clause of SQL statements in response to input questions. The output is compared to ground truth\u2014a correct output requires an exact match. An additional fine-grained evaluation metric is the tree-edit distance, which calculates the normalized edit operations required to transition between the generated and ground truth queries, averaged across all queries.",
            "result_discussion": "The normalization approach helps in ensuring the naturalness of joins and easy identification of column names, which enhances the LLM's capability in SQL generation tasks. The study notes that existing database normalizations focus on ease for human query generation and machine execution efficiency, while this normalization technique demonstrates the potential for LLM-specific optimizations and suggests that many more such normalizations could be useful for LLMs in the future.",
            "ablation_id": "2405.09805v2.No1"
        },
        {
            "research_objective": "To evaluate the effectiveness of different inference-time compositional methods in preserving both the privacy and performance of individual fine-tunings when combined in SecureLLM.",
            "experiment_process": "A Llama-2-7B model is fine-tuned separately on each data silo to generate Low-Rank Adaptations (LoRA), which are then composed using various compositional schemes at inference time without additional training. Two insecure baseline models are trained: a generalized model trained on all individual silos and an exponential model where models must be trained proportional to the number of silos. These baseline models are considered insecure as they lack methods for removing knowledge about certain silos at inference time. Models are fine-tuned using Adam optimizer, dropout, and other standard parameters. The evaluation metrics include the retention of individual silo performance (assessed using tree edit distance) and generalization ability to union silos.",
            "result_discussion": "The highest performance is achieved by the insecure exponential baseline model, which is expected. However, the SecureLLM composed fine-tunings outperform previous methods like LoraHub and PEM Addition by a significant margin while still maintaining privacy. Even without a specific database normalization, SecureLLM methods have the lowest tree edit distances across probe silo combinations, effectively retaining compositional knowledge. The use of ChatGPT-based data for validation supports the robustness of these findings. An additional obfuscated test with arbitrary column names further confirms SecureLLM\u2019s ability to encourage genuine compositional reasoning.",
            "ablation_id": "2405.09805v2.No2"
        }
    ]
}