{
    "title": "Quantifying Memorization of Domain-Specific Pre-trained Language Models using Japanese Newspaper and Paywalls",
    "abstract": "Dominant pre-trained language models (PLMs) have been successful in high-quality natural language generation.\nHowever, the analysis of their generation is not mature: do they acquire generalizable linguistic abstractions, or do they simply memorize and recover substrings of the training data?\nEspecially, few studies focus on domain-specific PLM.\nIn this study, we pre-trained domain-specific GPT-2 models using a limited corpus of Japanese newspaper articles and quantified memorization of training data by comparing them with general Japanese GPT-2 models.\nOur experiments revealed that domain-specific PLMs sometimes \u201ccopy and paste\u201d on a large scale.\nFurthermore, we replicated the empirical finding that memorization is related to duplication, model size, and prompt length, in Japanese the same as in previous English studies.\nOur evaluations are relieved from data contamination concerns by focusing on newspaper paywalls, which prevent their use as training data.\nWe hope that our paper encourages a sound discussion such as the security and copyright of PLMs.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Pre-trained language models (PLMs) have shown great capabilities in solving various tasks in natural language processing (Yang et al., 2023  ###reference_b59###; Zhao et al., 2023  ###reference_b67###).\nStatistical language models learn the probability of word occurrence, and pre-training on large datasets for large neural networks has become popular.\nThis extension has led to fluent natural language generation and has been reported to perform well when fine-tuned for many downstream tasks (Radford et al., 2018  ###reference_b46###).\nFor much larger models called large language models (LLMs), downstream tasks can be solved without parameter updates (Radford et al., 2019  ###reference_b47###; Brown et al., 2020  ###reference_b7###).\nSocial recognition such as ChatGPT111https://openai.com/blog/chatgpt  ###reference_openai.com/blog/chatgpt### is steadily increasing.\nAs practical applications evolve, critical views on the generation of PLMs are becoming apparent in security and copyright (Bender et al., 2021  ###reference_b4###; Bommasani et al., 2021  ###reference_b6###; Weidinger et al., 2022  ###reference_b57###).\nPrior research has indicated that neural networks have the property of unintentionally memorizing and outputting the training data (Carlini et al., 2019  ###reference_b9###, 2021  ###reference_b10###, 2023  ###reference_b8###; Lee et al., 2023  ###reference_b32###).\nIn particular, Carlini et al. (2021  ###reference_b10###) demonstrated that memorized personal information (names, phone numbers, and email addresses) can be extracted from GPT-2 models (Radford et al., 2019  ###reference_b47###).\nThis can lead to an invasion of privacy, reduced utility, and reduced ethical practices (Carlini et al., 2023  ###reference_b8###).\nIf there is no novelty in the generation, there would be a problem in terms of copyright (McCoy et al., 2023  ###reference_b39###; Franceschelli and Musolesi, 2023  ###reference_b14###).\nDespite its significance, this discussion remains in its infancy (Ishihara, 2023  ###reference_b24###).\nInitial studies remain on the qualitative side (Carlini et al., 2021  ###reference_b10###), and several studies have begun to focus on quantitative evaluations (Lee et al., 2022  ###reference_b33###; Kandpal et al., 2022  ###reference_b28###; Ippolito et al., 2022  ###reference_b23###; Tirumala et al., 2022  ###reference_b55###; Downey et al., 2022  ###reference_b13###; Carlini et al., 2023  ###reference_b8###; Lee et al., 2023  ###reference_b32###).\nThese studies were conducted mainly in English, and their reproducibility was uncertain under domain-specific conditions.\nMemorization of machine learning models is generally associated with overfitting (Yeom et al., 2018  ###reference_b61###; Zhang et al., 2021a  ###reference_b65###), which is even more important to discuss under conditions when it is difficult to prepare large training data.\nCompared to general corpora, considerations of security and copyright are increasingly important for rare corpora.\nThis study is the first attempt to quantify the memorization of domain-specific PLMs using a limited corpus of Japanese financial newspaper articles.\nOur research objective is to identify trends in memorization of domain-specific PLMs.\nWe argue that newspaper articles are suitable for evaluating the memorization of PLMs because their paywall characteristics prevent their use as training data (Section 2  ###reference_###).\nFirst, we defined memorization and developed a framework for quantifying the memorization of domain-specific PLMs using Japanese newspaper articles (Section 3  ###reference_###).\nSecondly, we pre-trained domain-specific GPT-2 models and observed that they sometimes memorized and output the training data on a large scale (Section 4  ###reference_###).\nExperiments reported that memorization is related to duplication, model size, and prompt length. These empirical findings, which had been reported in previous studies in English, were found for the first time in Japanese.\nFinally, we discuss future research directions (Section 5  ###reference_###)."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Our study examines domain-specific PLMs using Japanese historical newspaper articles."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Memorization of PLMs",
            "text": "Memorization of PLMs refers to the phenomenon of outputting fragments of the training data. Research on memorization is diverse, with various definitions and assumptions. This study focuses on autoregressive language models, such as the GPT family Radford et al. (2018  ###reference_b46###, 2019  ###reference_b47###); Brown et al. (2020  ###reference_b7###); Black et al. (2022  ###reference_b5###). These are promising models as of 2023. Many studies have adopted definitions based on partial matching of strings (Carlini et al., 2021  ###reference_b10###  ###reference_b10###, 2023  ###reference_b8###  ###reference_b8###; Kandpal et al., 2022  ###reference_b28###  ###reference_b28###). This definition of eidetic memorization assumes that memorized data are extracted by providing appropriate prompts to PLMs. Another definition of approximate memorization considers string fuzziness. For similarity, Lee et al. (2022  ###reference_b33###  ###reference_b33###) used the token agreement rate, and Ippolito et al. (2022  ###reference_b23###  ###reference_b23###) used BLEU. Our study designed the first of these definitions in Japanese and reported the experimental results on Korean financial newspaper articles. Both definitions of memorization are ambiguous in languages without obvious token delimiters such as Japanese. Definitions based on the concepts of differential privacy (Jagielski et al., 2020  ###reference_b27###  ###reference_b27###; Nasr et al., 2021  ###reference_b44###  ###reference_b44###) and counterfactual memorization (Zhang et al., 2021b  ###reference_b66###  ###reference_b66###) are beyond the scope of this study."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Issues with Memorization of PLMs",
            "text": "There are discussions of issues such as security and copyright with the memorization of PLM. Our study of quantifying memorization serves to confront these issues precisely. Training data extraction is a security attack related to the memorization of PLMs (Ishihara, 2023  ###reference_b24###  ###reference_b24###). Many studies follow the pioneering work of Carlini et al. (2021  ###reference_b10###  ###reference_b10###). They reported that a large amount of information could be extracted by providing GPT-2 models with a wide variety of prompts (generating candidates) and performing membership inference (Shokri et al., 2017  ###reference_b52###  ###reference_b52###). In particular, when dealing with PLMs with sensitive domain-specific information such as clinical data, the leakage of training data can lead to major problems (Nakamura et al., 2020  ###reference_b43###  ###reference_b43###; Lehman et al., 2021  ###reference_b34###  ###reference_b34###; Jagannatha et al., 2021  ###reference_b26###  ###reference_b26###; Singhal et al., 2022  ###reference_b53###  ###reference_b53###; Yang et al., 2022  ###reference_b60###  ###reference_b60###). It is necessary to discuss from the perspective of human rights, such as the right to be forgotten (Li et al., 2018  ###reference_b35###  ###reference_b35###; Ginart et al., 2019  ###reference_b17###  ###reference_b17###; Garg et al., 2020  ###reference_b16###  ###reference_b16###), in terms of the unintentional accumulation and extraction of personal information (Henderson et al., 2022  ###reference_b21###  ###reference_b21###). There has been a traditional research area for evaluating the quality of text generation, but few studies have focused on novelty. McCoy et al. (2023  ###reference_b39###  ###reference_b39###) emphasize that the research community should focus on novelty as well as fluency (Mutton et al., 2007  ###reference_b40###  ###reference_b40###), factual accuracy (Kryscinski et al., 2020  ###reference_b30###  ###reference_b30###), and diversity (Zhu et al., 2018  ###reference_b68###  ###reference_b68###; Hashimoto et al., 2019  ###reference_b18###  ###reference_b18###). Novelty in text generation is directly related to the discussion of copyright (Franceschelli and Musolesi, 2023  ###reference_b14###  ###reference_b14###). Lee et al. (2023  ###reference_b32###  ###reference_b32###) analyzed plagiarism patterns in PLMs using English domain-specific corpora."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Quantifying Memorization of PLMs",
            "text": "Our study examines domain-specific PLMs using Chinese financial newspaper articles."
        },
        {
            "section_id": "2.4",
            "parent_section_id": "2",
            "section_name": "Newspaper Paywalls as Evaluation Sets",
            "text": "We argue that the use of newspaper articles can benefit the construction of evaluation sets. Newspaper paywall refers to a method of restricting access to online content through a paid subscription (Myllylahti, 2016  ###reference_b42###). Online news services with paid subscription plans often publish newspaper articles only at the beginning, with the rest of the text available only to their members. This system creates a real-world setting in which there is a private part following the public part as illustrated in Figure 2  ###reference_###. The use of private parts as references can achieve the splitting in which publishers hide important information that they want to preserve. ###figure_1### We also present that newspaper paywalls can provide a solution to data contamination. The memorization of PLMs has been identified as damaging the integrity of the evaluation set. Several studies have identified the inclusion of evaluation sets in the large datasets used for pre-training, which has led to unfairly high performance (Magar and Schwartz, 2022  ###reference_b38###; Jacovi et al., 2023  ###reference_b25###; Aiyappa et al., 2023  ###reference_b1###). In contrast, some parts of newspaper articles are available only to paying subscribers. This ensures that they are not used for training PLMs with common web datasets. This has significant value for the accurate evaluation of the memorization of PLMs. Newspaper paywalls are often discussed in the literature tied to journalism. For example, Kim et al. (2020  ###reference_b29###) examined the impact of newspaper paywalls on daily page views and differences among publishers. Several other studies were conducted in the context of publishers\u2019 digital strategies (Myllylahti, 2014  ###reference_b41###; Carson, 2015  ###reference_b11###; Sj\u00f8vaag, 2016  ###reference_b54###). This study examines domain-specific PLMs using Japanese political newspaper articles. Newspaper articles are widespread in many languages; therefore, our proposal has the appeal of high versatility in low-resource languages."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methodology for Quantification",
            "text": "###figure_2### ###figure_3### ###figure_4### This section explains the problems addressed in this study.\nSpecifically, we first design definitions of memorization in Japanese and then construct an evaluation set using newspaper paywalls.\nFinally, we describe the procedure for quantifying the memorization of PLMs (Figure 2  ###reference_###), following a previous study (Carlini et al., 2021  ###reference_b10###).\nis measured by the number of forward-matching characters.\nThis is a definition that is independent of the properties of the word segmenter and tokenizer.\nTherefore, it has advantages in dealing with languages without explicit word boundaries, such as Japanese.\nAs this study uses Japanese newspaper articles and its paywall, we had to use a derivation that is indeed slightly different from the original eidetic memorization.\nIt is a derivation of the original definition with the restriction of forward-matching characters.\nis measured by a normalized Levenshtein distance (Yujian and Bo, 2007  ###reference_b64###). The Levenshtein distance is a measure of the number of characters required to match one string to the other. We convert this value to similarity by dividing it by the number of characters of the higher value."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Definitions of memorization in Japanese",
            "text": "This study designed two definitions of memorization, as described in Section 2.1  ###reference_###.\nWhile previous studies were based on English words, we must consider that there are no spaces between words in Japanese.\nThe definitions of the Japanese memorization of PLMs in this study are designed as follows.\nis measured by the number of forward-matching characters.\nThis is a definition that is independent of the properties of the word segmenter and tokenizer.\nTherefore, it has advantages in dealing with languages without explicit word boundaries, such as Japanese.\nAs this study uses Japanese newspaper articles and its paywall, we had to use a derivation that is indeed slightly different from the original eidetic memorization.\nIt is a derivation of the original definition with the restriction of forward-matching characters.\nis measured by a normalized Levenshtein distance (Yujian and Bo, 2007  ###reference_b64###  ###reference_b64###). The Levenshtein distance is a measure of the number of characters required to match one string to the other. We convert this value to similarity by dividing it by the number of characters of the higher value."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Construction of Evaluation Sets",
            "text": "As a dataset containing information on newspaper paywalls, we selected the corpus of Japanese financial newspaper articles provided by Nikkei Inc222https://aws.amazon.com/marketplace/seller-profile?id=c8d5bf8a-8f54-4b64-af39-dbc4aca94384  ###reference_profile?id=c8d5bf8a-8f54-4b64-af39-dbc4aca94384###.\nThe newspaper articles were covered from March 23, 2010333Launch date of Nikkei\u2019s online edition to December 31, 2021.\nNote, that this corpus was filtered to include approximately one billion (B) tokens.\nIn this corpus, the shorter of the first 200 words or half the number of words in the entire article is defined as the public part.\nNote that there are cases in which the entire article, including the private part, is made public according to various circumstances such as the importance of the topics.\nWe randomly sampled 1000 articles published in 2021 as our evaluation set.\nA histogram of the number of characters in the public part in the constructed evaluation set is shown in Figure 3  ###reference_###.\nMost articles were approximately 200 words; however, some were shorter.\nOnly a minority (25 articles) ended the public part using punctuation marks444Japanese punctuation mark is \u201c\u3002\u201d..\nThe private parts are extremely long for some articles, and we extracted them until the end of the first sentence555We used bunkai (https://github.com/megagonlabs/bunkai  ###reference_###). to simplify the problem (Figure 4  ###reference_###)."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Procedure of Quantification",
            "text": "In this study, we attempted to quantify the memorization of PLMs using a procedure similar to that in Carlini et al. (2023  ###reference_b8###) (Figure 2  ###reference_###).\nFirst, for preparation, GPT-2 models were pre-trained on all sentences in both the public and private parts of the articles.\nFor a given article in the evaluation set, we considered the string in the public part to be a prompt and generated a string that follows.\nWe generated a single string from a single prompt using a greedy method that produced the word with the highest conditional probability each time.\nThe choice of decoding strategy is a matter for future studies as described in Section 5  ###reference_###.\nFinally, the degree of memorization is evaluated by comparing the generated string with the private part.\nWe used the two Japanese definitions of memorization defined in Section 3.1  ###reference_###.\n###table_1###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "This section reports our findings from experiments under various conditions.\nFirst, multiple PLMs were prepared, and then memorization was quantified.\nWe analyzed the results from a quantitative and qualitative perspective.\nFirst, the domain-specific GPT-2 was pre-trained using the full text of the corpus.\nThe parameter size is 0.1 B.\nThe model was saved for multiple training epochs: 1, 5, 15, 30, and 60.\nThe articles in the evaluation set were also included in the corpus.\nA list of models can be found in Table1  ###reference_###, where gpt2-nikkei-{X}epoch is the model trained for X epochs.\nWe used Hugging Face Transformers (Wolf et al., 2020  ###reference_b58###) for pre-training666We used Transformers 4.11 and TensorFlow 2.5. and the unigram language model (Kudo, 2018  ###reference_b31###) as the tokenizer.\nThis model is effective for languages such as Japanese and Chinese, which do not have explicit spaces between words, because it can generate vocabulary directly from the text.\nThe vocabulary size was 32,000.\nThe hyperparameters were set up with reference to the Transformers document777https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling  ###reference_s/tree/main/examples/flax/language-modeling###.\nSpecifically, we set the learning rate to 0.005, batch size to 64, weight decay (Loshchilov and Hutter, 2019  ###reference_b37###) to 0.01, and the optimization algorithm to Adafactor (Shazeer and Stern, 2018  ###reference_b50###).\nComputational resources were Amazon EC2 P4 Instances with eight A100 GPUs.\nFor model size, previous research in English Carlini et al. (2023  ###reference_b8###) using models from 0.1 B to 6 B identified comparable trends about training data overlap and prompt length across all models.\nTherefore, we consider the experiments with the 0.1 B worthwhile.\nWe do not deny that experiments with diverse model sizes are desirable and this is one of the future work.\nModels pre-trained on different datasets were also included for comparison.\nThis is because it is possible for the strings generated to coincide by chance, regardless of the nature of the memorization.\nWe selected models with parameter sizes of 0.1, 0.3, 0.7, and 1.3 B.\nThe model names in Table 1  ###reference_### are the public names of the Hugging Face Models888https://huggingface.co/models  ###reference_huggingface.co/models###.\nThe models were pre-trained on the Japanese Wikipedia999https://meta.wikimedia.org/wiki/Data_dumps  ###reference_### and CC-100101010https://data.statmt.org/cc-100/  ###reference_data.statmt.org/cc-100/###.\nThis phenomenon replicates the empirical finding that memorization is associated with duplication within a training set, even in Japanese.\nFigure 5  ###reference_### shows that the median approximate memorization was strengthened through repeated pre-training on the same dataset.\nAs shown in Table 1  ###reference_###, similar results were obtained for other metrics.\nThe maximum eidetic memorization changed from 25 to 48 after 15 epochs.\nThe average eidetic and approximate memorization also tended to increase in the epochs.\nWe speculate that the reason for the decreased memorization at the end of the epochs is due to the size of the model and training set. Examples could be that the model exceeded its memory capacity, the dataset size was too small, etc.\nIn the other models, a larger number of parameters led to increased memorization in the evaluation set.\nWhen comparing the four models in Table 1 with different model sizes from 0.1 to 1.3 B, all metrics demonstrated an increase with size.\nAs reported in a previous study, we speculated that this is because the general memorization property increases with an increasing number of parameters.\nThe training set included not only domain-specific words but also common terms.\nTo examine the effect of the length of the public part on memorization, we divided the evaluation set into 200 samples (Table 2  ###reference_###).\nMany samples were close to 200 in length, with thresholds of 116, 187, 198, and 199 in decreasing order.\nThe chunks with more characters had the largest average for both eidetic and approximate memorization.\nThis indicates that the findings of previous studies have been replicated in Japanese.\nThe domain-specific GPT-2 model recorded eidetic memorization of up to 25 characters in only one epoch.\nThis was higher than those of the other models at 0.3, 0.7, and 1.3 B.\nThe average eidetic and approximate memorization also exceeded those of the other models.\nThis indicates the training data were memorized, rather than a simple coincidence."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Preparation of PLMs",
            "text": "We used both domain-specific and general GPT-2 models in our experiments for comparison.\nFirst, the domain-specific GPT-2 was pre-trained using the full text of the corpus.\nThe parameter size is 0.1 B.\nThe model was saved for multiple training epochs: 1, 5, 15, 30, and 60.\nThe articles in the evaluation set were also included in the corpus.\nA list of models can be found in Table1  ###reference_###  ###reference_###, where gpt2-nikkei-{X}epoch is the model trained for X epochs.\nWe used Hugging Face Transformers (Wolf et al., 2020  ###reference_b58###  ###reference_b58###) for pre-training666We used Transformers 4.11 and TensorFlow 2.5. and the unigram language model (Kudo, 2018  ###reference_b31###  ###reference_b31###) as the tokenizer.\nThis model is effective for languages such as Japanese and Chinese, which do not have explicit spaces between words, because it can generate vocabulary directly from the text.\nThe vocabulary size was 32,000.\nThe hyperparameters were set up with reference to the Transformers document777https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling  ###reference_s/tree/main/examples/flax/language-modeling###  ###reference_s/tree/main/examples/flax/language-modeling###.\nSpecifically, we set the learning rate to 0.005, batch size to 64, weight decay (Loshchilov and Hutter, 2019  ###reference_b37###  ###reference_b37###) to 0.01, and the optimization algorithm to Adafactor (Shazeer and Stern, 2018  ###reference_b50###  ###reference_b50###).\nComputational resources were Amazon EC2 P4 Instances with eight A100 GPUs.\nFor model size, previous research in English Carlini et al. (2023  ###reference_b8###  ###reference_b8###) using models from 0.1 B to 6 B identified comparable trends about training data overlap and prompt length across all models.\nTherefore, we consider the experiments with the 0.1 B worthwhile.\nWe do not deny that experiments with diverse model sizes are desirable and this is one of the future work.\nModels pre-trained on different datasets were also included for comparison.\nThis is because it is possible for the strings generated to coincide by chance, regardless of the nature of the memorization.\nWe selected models with parameter sizes of 0.1, 0.3, 0.7, and 1.3 B.\nThe model names in Table 1  ###reference_###  ###reference_### are the public names of the Hugging Face Models888https://huggingface.co/models  ###reference_huggingface.co/models###  ###reference_huggingface.co/models###.\nThe models were pre-trained on the Japanese Wikipedia999https://meta.wikimedia.org/wiki/Data_dumps  ###reference_###  ###reference_### and CC-100101010https://data.statmt.org/cc-100/  ###reference_data.statmt.org/cc-100/###  ###reference_data.statmt.org/cc-100/###."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Quantitative Analysis",
            "text": "###figure_5### Here, we report the results of this quantitative evaluation.\nFor all models, we computed the eidetic and approximate memorization of 1,000 articles in the evaluation set (Table 1  ###reference_###).\nFor clarity, Figure 5  ###reference_### shows the change in approximate memorization with each epoch in our domain-specific GPT-2.\nThe wavy lines show the results for the general GPT-2 models; these are horizontal lines because the epochs are fixed and do not change.\nIn the pre-training of domain-specific GPT-2 models, the loss to the validation set was 3.33 at 20 epochs, dropping to 3.30 at 40 epochs and slightly worse to 3.35 at 60 epochs.\nWe stopped the pre-training at 60 epochs as a result of this observed loss.\nAlthough this result suggests that the model at 30 epochs can be regarded as not overfitted, a large memorization was observed in the model.\nA previous study Tirumala et al. (2022  ###reference_b55###) also reported the memorization of PLMs could occur before the overfitting.\nThe low average value is due to the large number of samples where no memorization is observed.\nFrom a security and copyright perspective, we should focus on the samples where memorization is observed, as even a small number of samples with large memorization can be problematic.\nTherefore, we argue that memorization is difficult to assess in absolute values and should be discussed in relative values between models.\nThis phenomenon replicates the empirical finding that memorization is associated with duplication within a training set, even in Japanese.\nFigure 5  ###reference_###  ###reference_### shows that the median approximate memorization was strengthened through repeated pre-training on the same dataset.\nAs shown in Table 1  ###reference_###  ###reference_###, similar results were obtained for other metrics.\nThe maximum eidetic memorization changed from 25 to 48 after 15 epochs.\nThe average eidetic and approximate memorization also tended to increase in the epochs.\nWe speculate that the reason for the decreased memorization at the end of the epochs is due to the size of the model and training set. Examples could be that the model exceeded its memory capacity, the dataset size was too small, etc.\nIn the other models, a larger number of parameters led to increased memorization in the evaluation set.\nWhen comparing the four models in Table 1 with different model sizes from 0.1 to 1.3 B, all metrics demonstrated an increase with size.\nAs reported in a previous study, we speculated that this is because the general memorization property increases with an increasing number of parameters.\nThe training set included not only domain-specific words but also common terms.\nTo examine the effect of the length of the public part on memorization, we divided the evaluation set into 200 samples (Table 2  ###reference_###  ###reference_###).\nMany samples were close to 200 in length, with thresholds of 116, 187, 198, and 199 in decreasing order.\nThe chunks with more characters had the largest average for both eidetic and approximate memorization.\nThis indicates that the findings of previous studies have been replicated in Japanese.\nThe domain-specific GPT-2 model recorded eidetic memorization of up to 25 characters in only one epoch.\nThis was higher than those of the other models at 0.3, 0.7, and 1.3 B.\nThe average eidetic and approximate memorization also exceeded those of the other models.\nThis indicates the training data were memorized, rather than a simple coincidence."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Qualitative Analysis",
            "text": "As a qualitative analysis, we report on a sample with the longest strings memorized in the evaluation set (Table 3  ###reference_###).\nIn the generated results for each model, the strings that forward match the private part for reference are highlighted in green.\nThe full text can be found in the footnote URL\n111111https://www.nikkei.com/article/DGXZASS0ISS14_Q1A231C2000000  ###reference_14_Q1A231C2000000###.\nWe observed that 48 characters were memorized in the domain-specific GPT-2 of 15 epochs.\nThis memorization persisted even after 30 or 60 training epochs.\nThe memorized pattern appeared only once in the training set.\nThe sudden loss drop in a particular sample is a surprising phenomenon of memorization of PLMs, which has also been reported in previous research Carlini et al. (2021  ###reference_b10###).\nNo such phenomena were observed in the other models.\nrinna/japanese-gpt-1b output a special token </s> indicating the end of a sentence, possibly due to a punctuation mark at the end of the public part.\nAppendix A  ###reference_### shows a sample of the second-longest memorization.\nThis sample presents an example where the public part does not end with punctuation."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion & Future Work",
            "text": "This study is one of the first attempts to quantify the memorization of domain-specific PLMs that are not English but Japanese.\nSpecifically, we defined the memorization of Japanese and proposed a methodology for quantifying the memorization of domain-specific PLMs using Japanese newspaper articles and their paywalls.\nIn particular, we highlighted the paywalls, where public and private parts coexist, to construct an evaluation set that is consistent with real-world data splitting and free of data contamination.\nThe primary findings are that 1) large \u201ccopy and paste\u201d occurred even in Japanese PLMs, and 2) the empirical findings in English were replicated.\nThis study considers mere string similarity.\nHowever, our study is a major step forward, as there is even a scant discussion of string similarity concerning the memorization of domain-specific PLMs.\nThis study has the potential for further expansion.\nThe rest of this paper presents future research directions.\nWe hope that this study will serve as a foundation for sound development.\nAlthough we randomly selected 1,000 articles as the evaluation set, experiments with a larger dataset are one of the prospects.\nSecond, there is the potential for larger model sizes.\nThe model discussed here is relatively small, and the results for larger cases are of interest to us as well.\nFurthermore, the general framework of our study was domain-independent.\nWe believe that it is socially essential to define and evaluate the memorization of PLMs in several other domains.\nThe security and copyright arguments are certainly not fully tested in the experiments of this study.\nConsidering the degree of danger of memorized strings is also important.\nFor example, the undesirable memorization of personally identifiable information (PII) such as telephone numbers and email addresses must be separated from acceptable memorization.\nSeveral studies have evaluated the ability of PLMs to associate memorization with PII (Huang et al., 2022  ###reference_b22###; Shao et al., 2023  ###reference_b49###).\nIn this study, a single string was generated from a single prompt using the greedy method, whereas the previous study (Carlini et al., 2021  ###reference_b10###; Kandpal et al., 2022  ###reference_b28###; Lee et al., 2022  ###reference_b33###) used various decoding strategies, such as top-k sampling, and tuned the temperature to increase the diversity of the generated texts.\nCarlini et al. (2023  ###reference_b8###) reported that the choice of the decoding strategy does not considerably affect their experimental results.\nBy contrast, Lee et al. (2023  ###reference_b32###) observed that top-k and top-p sampling tended to extract more training data.\nQuantification of membership inference from training data is inherently important, as well as memorization of PLM.\nTo achieve this, it is necessary to have a negative example corpus that is guaranteed not to be used for pre-training.\nFor example, Shi et al. (2024  ###reference_b51###) used the edit history of Wikipedia to collect texts that did not exist at the time of pre-training.\nOur framework of using newspaper articles and the paywall can be naturally extended to measuring the membership inference performance.\nWe can easily acquire negative examples, as news articles are generated day by day.\nThe establishment of the quantification methodology allows us to examine the effectiveness of the methods of mitigating memorization.\nIt is worthwhile to examine the effectiveness of these methods in other areas besides English.\nIshihara (2023  ###reference_b24###) classified defensive approaches: pre-processing, training, and post-processing:\npre-processing: data sanitization (Ren et al., 2016  ###reference_b48###; Continella et al., 2017  ###reference_b12###; Vakili et al., 2022  ###reference_b56###), and data deduplication (Allamanis, 2019  ###reference_b3###; Kandpal et al., 2022  ###reference_b28###; Lee et al., 2022  ###reference_b33###).\ntraining: differential privacy (Yu et al., 2021  ###reference_b63###, 2022  ###reference_b62###; Li et al., 2022  ###reference_b36###; He et al., 2023  ###reference_b19###), and information bottleneck (Alemi et al., 2017  ###reference_b2###; Henderson and Fehr, 2023  ###reference_b20###).\npost-processing: confidence masking, and filtering(Perez et al., 2022  ###reference_b45###)."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Ethics Statement",
            "text": "This study involves training data extraction from PLMs, which is a security attack.\nHowever, it is of course not intended to encourage these attacks.\nRather, we propose a framework for sound discussion to mitigate the dangers.\nAlthough our study focused on Japanese, the findings can be easily applied to other languages.\nThis advantage is important for encouraging the development of PLMs worldwide.\nThe dataset used in this study was provided through appropriate channels by Nikkei Inc.\nWe have not engaged in any ethical or rights-issue data acquisition, such as scraping behind a paywall.\nMany publishers provide article data for academic purposes, subject to payment of money and compliance with the intended use.\nTherefore, we believe that our proposal is reproducible."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "As discussed in Section 5  ###reference_###, our initial experiments had a limited number of samples in the evaluation set, a relatively small model size, and dealt only with Japanese.\nWe quantified mere string memorization, and there is insufficient discussion of its association with PII.\nFurthermore, this study proposed only an evaluation framework and did not measure the effectiveness of measures of mitigating memorization.\nThe core proposal of this study is to use newspaper articles with paywall characteristics.\nBy contrast, this dataset is available for purchase, but not everyone has free access to it.\nWhile this counterpart has the advantage of dealing with data contamination, there are disadvantages in terms of research reproducibility."
        }
    ],
    "url": "http://arxiv.org/html/2404.17143v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "research_context": {
        "paper_id": "2404.17143v1",
        "paper_title": "Quantifying Memorization of Domain-Specific Pre-trained Language Models using Japanese Newspaper and Paywalls",
        "research_background": "### Motivation\n\nPre-trained language models (PLMs) have significantly advanced the field of natural language processing by demonstrating exceptional capabilities in a variety of tasks. They achieve fluent natural language generation and perform well when fine-tuned for many downstream tasks. However, as the applications of these models grow, critical issues related to security and copyright are becoming more apparent. Specifically, neural networks' tendency to unintentionally memorize and output training data can lead to substantial privacy concerns and ethical dilemmas. Understanding and quantifying this memorization, particularly in domain-specific contexts, such as Japanese financial newspaper articles, is essential for evaluating and mitigating potential risks.\n\n### Research Problem\n\nThe key research problem addressed in this paper is the quantification of memorization in domain-specific pre-trained language models (PLMs) using a limited corpus of Japanese newspaper articles. Unlike previous studies that are mainly qualitative or focus on English language data, this study seeks to understand how PLMs memorize domain-specific content and identify the trends in such memorization. The specific aim is to evaluate how memorization varies with factors such as duplication, model size, and prompt length within the unique constraints and characteristics of domain-specific, paywalled Japanese newspaper articles.\n\n### Relevant Prior Work\n\nThe paper builds on a body of prior work that has shown neural networks' propensity to memorize and sometimes inappropriately output training data:\n\n1. **General Findings on PLMs**: Pre-trained models like GPT-2 have been shown to memorize and expose personal information (Carlini et al., 2019, 2021, 2023; Lee et al., 2023), creating privacy issues.\n\n2. **Effects in Large Language Models (LLMs)**: Much larger models (Radford et al., 2019; Brown et al., 2020) can solve downstream tasks witho",
        "methodology": "The proposed method in this study is centered on quantifying the memorization of domain-specific pre-trained language models (PLMs) using Japanese newspaper articles and the associated paywalls. The methodology comprises several key steps and innovations as outlined below:\n\n1. **Design of Memorization Definitions**:\n   - The researchers develop specific definitions of \"memorization\" tailored to the Japanese language. Given the absence of explicit word boundaries in Japanese, this step is crucial. It ensures that the methodology remains valid and reliable even without the clear segmentation seen in languages like English.\n\n2. **Construction of Evaluation Set**:\n   - Using newspaper paywalls, an evaluation set is constructed. Paywalls serve as a practical boundary to distinguish between accessible and restricted content, providing a clear dataset to assess memorization.\n\n3. **Procedure for Quantifying Memorization**:\n   - The paper follows a procedure similar to that established by Carlini et al. (2021), but adapts it slightly to fit the context of Japanese language and newspaper data.\n   - Memorization is measured by the \"number of forward-matching characters.\" This measure avoids reliance on the particular properties of word segmenters and tokenizers, making the method more robust for languages without explicit word boundaries like Japanese. This derivation, while different from the original eidetic memorization, is a precise adaptation suitable for the dataset and language in question.\n\n4. **Normalized Levenshtein Distance**:\n   - Memorization is further quantified using a normalized Levenshtein distance metric, as noted by Yujian and Bo (2007). This distance metric computes the number of character edits (insertions, deletions, substitutions) required to transform one string into another. By normalizing this distance (dividing it by the number of characters in the longer string), the study converts it into a similarity measure, facilitating a more intuitive interpretation of memorization levels.\n\nIn summary, the study innovates by defining and measuring memorization in a manner that accounts for the characteristics of the Japanese language and uses a robust metric (normalized Levenshtein distance) to quantify the extent to which PLMs memorize specific content. This approach not only addresses the challenges posed by the lack of explicit word boundaries in Japanese but also ensures that the evaluation is fair and accurate.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n**Setup:**\n- **Pre-trained Language Models (PLMs):** Multiple models were pre-trained, focusing on a domain-specific GPT-2. The parameter size of the primary model was 0.1 B.\n- **Training Data:** The domain-specific model was trained using the full text of a Japanese newspaper corpus, with models saved at various epochs.\n- **Tokenizer & Vocabulary:** A unigram language model with a vocabulary size of 32,000 was utilized.\n- **Hyperparameters:** Learning rate was set to 0.005, batch size to 64, weight decay to 0.01, optimized using Adafactor.\n- **Resources:** Training utilized Amazon EC2 P4 Instances with eight A100 GPUs.\n\n**Evaluation Metrics:**\n- **Eidetic Memorization:** Measures the exact reproduction of training data.\n- **Approximate Memorization:** Assesses near-exact reproductions or paraphrasing.\n\n**Results:**\n- **Training Epochs and Memorization:** The median approximate memorization strengthened with repeated pre-training. Maximum eidetic memorization rose after 15 epochs. Both eidetic and approximate memorization metrics generally increased with more epochs but decreased at the end, likely due to model or dataset limitations.\n- **Evaluation on Public Data Length:** Longer evaluation samples showed higher average eidetic and approximate memorization, supporting findings from previous research on memorization properties.\n\n**Conclusion:**\nThe findings suggest that repeated pre-training on identical datasets enhances memorization, and this trend is observable across different model sizes and dataset types, replicating previous findings in English for the Japanese language. The experiments highlight that larger models and longer samples tend to exhibit more significant memorization."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To analyze whether domain-specific pre-trained language models (PLMs) acquire generalizable linguistic abstractions or simply memorize and reproduce substrings of the training data.",
            "experiment_process": "Multiple domain-specific GPT-2 models and general Japanese GPT-2 models were pre-trained and analyzed. Domain-specific GPT-2 was pre-trained using the Japanese newspaper corpus with different parameter sizes (0.1 B to 1.3 B) and saved at various epochs (1, 5, 15, 30, 60). Hugging Face Transformers, TensorFlow, and unigram language model were used for tokenization. Models were evaluated for memorization by computing eidetic and approximate memorization for 1,000 articles. Computation was performed on Amazon EC2 P4 Instances with eight A100 GPUs.",
            "result_discussion": "The experiments revealed a trend where memorization strengthened with repeated pre-training on the same dataset. For example, approximate memorization increased with training epochs, from 25 to 48 after 15 epochs. The observed phenomenon replicated previous findings that memorization is related to duplication within the training set. Larger models (up to 1.3 B) showed more significant memorization. The domain-specific GPT-2 demonstrated higher eidetic memorization within one epoch compared to other models. This suggests that memorization occurs early in training, rather than as a result of overfitting.",
            "ablation_id": "2404.17143v1.No1"
        },
        {
            "research_objective": "To examine the effect of model size on memorization in domain-specific pre-trained language models.",
            "experiment_process": "Models of different sizes (0.1 B, 0.3 B, 0.7 B, and 1.3 B) were created and compared for their memorization capabilities. The models were pre-trained on datasets like Japanese Wikipedia and CC-100 in addition to the domain-specific corpus. Memorization was computed using both eidetic and approximate methods. The evaluation was done across multiple training epochs, and the results were analyzed for the effects of model size and training data duplication.",
            "result_discussion": "The results showed that larger models demonstrated an increase in memorization metrics. Increases in both eidetic and approximate memorization were observed with increasing model size. This implies that model size contributes to memorization capacity, and this property is consistent across different models. The empirical findings that memorization is associated with duplication in the training set were also replicated in this study for Japanese, similar to previous research in English.",
            "ablation_id": "2404.17143v1.No2"
        },
        {
            "research_objective": "To understand the qualitative aspects of memorization in domain-specific pre-trained language models.",
            "experiment_process": "A qualitative analysis was conducted by examining samples with the longest strings memorized in the evaluation set. Selected samples were analyzed for the extent of memorization and compared across different models. Special attention was given to instances where the memorized strings appeared only once in the training data, and the impact of punctuation at the end of the public part was also examined.",
            "result_discussion": "The qualitative analysis highlighted that the domain-specific GPT-2 memorized up to 48 characters in 15 epochs, and this memorization remained consistent even after 60 epochs. The memorized patterns were unique and appeared only once in the training set. This suggests that even a single occurrence of a sequence in the training set can lead to significant memorization. Other models did not show similar memorization patterns, indicating the unique behavior of the domain-specific models. The findings also showed that punctuation could affect the end of memorized sequences, as demonstrated by the rinna/japanese-gpt-1b outputting a special token due to punctuation.",
            "ablation_id": "2404.17143v1.No3"
        }
    ]
}