{
    "title": "Source-Aware Training Enables Knowledge Attribution in Language Models",
    "abstract": "Large language models (LLMs) learn a vast amount of knowledge during pretraining, but they are often oblivious to the source(s) of such knowledge.\nWe investigate the problem of intrinsic source citation, where LLMs are required to cite the pretraining source supporting a generated response. Intrinsic source citation can enhance LLM transparency, interpretability, and verifiability. To give LLMs such ability,\nwe explore source-aware training\u2014a post pretraining recipe that involves (i) training the LLM to associate unique source document identifiers with the knowledge in each document, followed by (ii) an instruction-tuning to teach the LLM to cite a supporting pretraining source when prompted. Source-aware training can easily be applied to pretrained LLMs off the shelf, and diverges minimally from existing pretraining/fine-tuning frameworks. Through experiments on carefully curated data, we demonstrate that our training recipe can enable faithful attribution to the pretraining data without a substantial impact on the model\u2019s quality compared to standard pretraining. Our results also highlight the importance of data augmentation in achieving attribution.111Code and data available here: https://github.com/mukhal/intrinsic-source-citation.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "###figure_1### Large language models (LLMs) often generate content that is not based on factual information (Ji et al., 2023  ###reference_b17###; Ye et al., 2023a  ###reference_b47###). As LLMs are pretrained over noisy web data that often contains inaccurate or outdated content, users should be able to verify LLM outputs by checking their sources.\nMoreover, concerns about copyright infringement (Min et al., 2023  ###reference_b30###; Longpre et al., 2023  ###reference_b24###), privacy violations (Kim et al., 2024  ###reference_b18###), data contamination (Shi et al., 2023  ###reference_b38###), and toxic content (Gehman et al., 2020  ###reference_b11###) in LLMs emphasize the need for techniques to identify and trace the origins of information included in models\u2019 responses. It is therefore desirable if\nLLMs can provide supporting evidence for their responses by citing or attributing the outputs to the sources they draw upon (Rashkin et al., 2023  ###reference_b37###; Huang & Chang, 2023  ###reference_b15###; Li et al., 2023b  ###reference_b22###).\nBeyond improving the models\u2019 transparency, attribution allows for a deeper understanding of the relationship between training data and model behaviors, thereby offering a pathway\nto refine the quality of pretraining data.\nWe focus on intrinsic source citation, where the LLM should cite source documents from the pretraining data from which it acquired its relevant parametric knowledge.\nCompared to retrieval-based approaches such as RAG (Lewis et al., 2020  ###reference_b20###; Guu et al., 2020  ###reference_b13###) or post-hoc techniques (He et al., 2023  ###reference_b14###; Gao et al., 2023a  ###reference_b9###),\nintrinsic source citation is inherently tied to the model itself, enables more faithful attribution to its parametric knowledge,\nthus opens up unique opportunities for improved interpretability (Alvarez Melis & Jaakkola, 2018  ###reference_b3###; Marasovic et al., 2022  ###reference_b26###).\nTo this end, we explore source-aware training\u2014a post-pretraining recipe that enables a LLM to cite its pretraining data based on its parametric knowledge. Our motivation is three-fold. First, a significant portion of an LLM\u2019s knowledge is acquired during pretraining, therefore citing evidence for this parametric knowledge can greatly enhance the LLM trustworthiness. Second, the standard practice for LLM pretraining neglects the attribution angle, which explains why the current generation of LLMs fails to provide reliable citations (Agrawal et al., 2023  ###reference_b1###; Zuccon et al., 2023  ###reference_b53###). We aim to explore a training procedure that naturally facilitates citation of the pretraining data. Finally, from a scientific perspective, it is intriguing to investigate whether and how current language models can be trained to reference their pretraining data.\nWe inquire: Given an off-the-shelf LLM, can we train it to attribute its generations to the supporting sources from the pretraining data? Our goal is to cite the pretraining documents themselves (see Figure 1  ###reference_###). Our setup mirrors existing frameworks for LLM pretraining and can be summarized as follows: We take an off-the-shelf LLM, continue pretraining it on a corpus associating each document with a unique identifier,\nthen fine-tune it to answer questions about the acquired knowledge while providing citations.\nThe citation is achieved by generating an identifier of a document supporting the answer. Continual pretraining is done as in prior work, with the main difference of injecting the document identifiers into the pretraining data\u2014minimal changes in the model\u2019s architecture or implementation are needed.\nTo study the generalization over this task and simulate a realistic fine-tuning setting, we limit our instruction tuning stage to a subset of the pretraining documents (in-domain) and evaluate the model\u2019s attribution ability over the remaining (out-of-domain) documents. We run experiments over a synthetic pretraining corpus of fake biographies and show that LLMs can achieve reasonable attribution when answering a question about the out-of-domain documents.\nOur contributions are summarized as follows:\nTo the best of our knowledge, this work is the first to study intrinsic source citation and investigate the ability of current LLMs to cite the source of their parametric knowledge.\nWe explore a source-aware training recipe that can be applied to off-the-shelf LLMs to give them the ability to attribute their outputs to the pretraining sources. On synthetic data, we show that such training can achieve reasonable attribution while maintaining a good balance with the LLM quality compared to standard pretraining.\nWe examine the impact of various training strategies on attribution such data augmentation, and our findings can inform future efforts to train attribution-capable models at a large scale."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Attribution is gaining more attention recently as interpretability and grounding of language models become increasingly important. Generally speaking, approaches to achieve attribution can be classified as either inference-based or model-based. Retrieval-based approaches include retrieval augmentation (RAG) (Lewis et al., 2020  ###reference_b20###; Guu et al., 2020  ###reference_b13###; Borgeaud et al., 2022  ###reference_b6###; Izacard et al., 2023  ###reference_b16###) and post-hoc attribution (He et al., 2023  ###reference_b14###; Gao et al., 2023a  ###reference_b9###). RAG approaches enable attribution by providing a retrieved context for the LM to use, and teaching LM how to cite the retrieved context (Nakano et al., 2021  ###reference_b31###; Menick et al., 2022  ###reference_b28###). The major limitations of RAG approaches are the lack of guarantee that the model is relying on the retrieved data for generation (Petroni et al., 2020  ###reference_b34###; Li et al., 2023a  ###reference_b21###), and that they only work on non-parametric knowledge. Post-hoc approaches (He et al., 2023  ###reference_b14###; Gao et al., 2023a  ###reference_b9###) attribute the LM outputs by retrieving the supporting evidence given the model\u2019s response, but have been shown to produce non-accurate citations (Liu et al., 2023  ###reference_b23###).\nModel-based techniques involve prompting the model directly to generate citations for its parametric knowledge (Weller et al., 2023  ###reference_b45###; Zuccon et al., 2023  ###reference_b53###) or scaling techniques such as influence functions (Koh & Liang, 2017  ###reference_b19###) to large models (Grosse et al., 2023  ###reference_b12###). Model-based attribution is arguably more faithful than retrieval-based approaches as the citation mechanism is intrinsic to the model (Alvarez Melis & Jaakkola, 2018  ###reference_b3###; Marasovic et al., 2022  ###reference_b26###). However, standard approaches to pretraining LMs do not take into account the need for the language model to cite its pretraining data, which is where our work comes into play.\nBohnet et al. (2022  ###reference_b5###) proposed the task of attributed question-answering and evaluated the attribution performance of different systems using the AutoAIS metric (Rashkin et al., 2023  ###reference_b37###; Gao et al., 2023a  ###reference_b9###). In addition, they fine-tuned PaLM (Chowdhery et al., 2023  ###reference_b8###) to generate both an answer and a URL pointing to Wikipedia page supporting the answer in generative retrieval style (Tay et al., 2022  ###reference_b40###; Wang et al., 2022  ###reference_b42###). Although this setup is similar to ours in that we require the LM to generate the document identifier as well, their setup is basically a variation of RAG where the LM acts as the retriever.\nThere is a large body of work on the task of citation generation in the scientific domain, where the goal is to cite an appropriate article given a particular context (McNee et al., 2002  ###reference_b27###; Nallapati et al., 2008  ###reference_b32###) or to generate text citing one article in relation to another (Xing et al., 2020  ###reference_b46###; Luu et al., 2020  ###reference_b25###). A relevant work to ours is Galactica (Taylor et al., 2022  ###reference_b41###), which leverages the underlying citation graph in the pretraining data to learn to predict citations given a context. Notably, Galactica is trained to leverage citations of scientific articles in the pretraining data, while our work explores citation of all the pretraining documents, extending beyond scientific articles. Gao et al. (2023b  ###reference_b10###) introduced a benchmark for the automatic evaluation of LM citations and Ye et al. (2023b  ###reference_b48###) proposed a method to improve language model grounding by fine-tuning the language model on responses that are well supported by their citations. However, their setup is restricted to citation of retrieved rather than parametric knowledge.\nOur work is somewhat related to generative retrieval, where an auto regressive model is trained to act as a"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Source-Aware Training",
            "text": "Our training framework is designed to easily integrate with existing pretraining pipelines.\nWe minimize its deviations from established post-pretraining practice, and it involves almost no modifications to the model architecture or implementation.\nEach document in the pretraining corpus is assigned a unique document identifier (ID) and our goal is to train a language model that can respond to user prompts by providing both a response and an ID referring to the source document of the model\u2019s knowledge.\nOur evaluation follows the attributed question answering setup (Bohnet et al., 2022  ###reference_b5###), where given an input prompt , the LLM output will consist of a tuple  where  is the response (e.g., the answer to a question) and  is the identifier of the document in the pretraining data that supports the answer.\nFollowing standard LLM training setups, our recipe has two stages: Continual pretraining (Section 3.1  ###reference_###) and instruction tuning (Section 3.2  ###reference_###). Instruction tuning trains the model to be able to attribute the generated responses to supporting documents it has seen during pretraining. The pretraining stage will involve all documents by nature, but the instruction tuning step is restricted to a subset of the pretraining documents. This restriction is due to the potential cost of curating instruction tuning data from all the pretraining documents, that is in addition to the training overhead incurred by instruction tuning (Zhou et al., 2024  ###reference_b51###).\n###figure_2### After training, we measure out-of-domain (OOD) attribution: whether the model can attribute knowledge to documents that are only included in the continual pretraining data but not in the instruction tuning data. We therefore split the pretraining corpus into in-domain and OOD subsets. The in-domain data is used to create attribution training examples, while the OOD documents are used for evaluation, as shown in Figure 2  ###reference_###.\nThe continual pretraining phase has two goals: (i) memorizing knowledge via next-word prediction (same as established LLM pretraining), and (ii) associating knowledge within a source document with its ID to enable OOD attribution. We aim to achieve the second goal by injecting the document ID into the document before training. An important consideration is the location and frequency of injecting the document ID.\nFormally, given a pretraining corpus of documents  and their corresponding IDs  where each  is a sequence of tokens , and each  is a sequence of tokens of its identifier. Our pretraining aims to learn the language model parameters  that maximize the objective \n, where  is the ID-injected version of the document . We inject the doc ID into document a  with different strategies, each of which corresponds to a different .222We omit the superscript for brevity. Particularly, we experiment with the following strategies:\nno-id: Standard pretraining without ID injection: .\ndoc-begin: Inject the ID once before the first token in the document: .\ndoc-end: Inject once after the last token in the document. This is equivalent to .333doc-end results in the same training objective as in DSI (Tay et al., 2022  ###reference_b40###), where the model is trained to generate the ID given the full document. While this objective was shown to work for the information retrieval setup, we find that it fails to generalize in attribution.\nrepeat: Inject the ID after every sentence in both in-domain and OOD documents. Here, , where  are the tokens in  corresponding to the -th sentence in document  and assuming  has  sentences.\nTo maximize GPU utilization during continual pretraining, the typical practice packs several pretraining documents within a single training sequence separated by the end-of-sentence <eos> token. As a result, the doc ID tokens for a certain document will naturally attend to preceding tokens from other documents. Our initial experiments showed that this severely hurts attribution, since the model will associate the doc ID of a given document with tokens from other documents in the same training sequence. To avoid this, we modify the causal self-attention mask during pretraining such that the ID tokens for a given document only attend to tokens from within that document."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1   Continual Pretraining with Doc ID Injection",
            "text": "The continual pretraining phase has two goals: (i) memorizing knowledge via next-word prediction (same as established LLM pretraining), and (ii) associating knowledge within a source document with its ID to enable OOD attribution. We aim to achieve the second goal by injecting the document ID into the document before training. An important consideration is the location and frequency of injecting the document ID.\nFormally, given a pretraining corpus of documents  and their corresponding IDs  where each  is a sequence of tokens , and each  is a sequence of tokens of its identifier. Our pretraining aims to learn the language model parameters  that maximize the objective \n, where  is the ID-injected version of the document . We inject the doc ID into document a  with different strategies, each of which corresponds to a different .222We omit the superscript for brevity. Particularly, we experiment with the following strategies:\nno-id: Standard pretraining without ID injection: .\ndoc-begin: Inject the ID once before the first token in the document: .\ndoc-end: Inject once after the last token in the document. This is equivalent to .333doc-end results in the same training objective as in DSI (Tay et al., 2022  ###reference_b40###  ###reference_b40###), where the model is trained to generate the ID given the full document. While this objective was shown to work for the information retrieval setup, we find that it fails to generalize in attribution.\nrepeat: Inject the ID after every sentence in both in-domain and OOD documents. Here, , where  are the tokens in  corresponding to the -th sentence in document  and assuming  has  sentences.\nTo maximize GPU utilization during continual pretraining, the typical practice packs several pretraining documents within a single training sequence separated by the end-of-sentence <eos> token. As a result, the doc ID tokens for a certain document will naturally attend to preceding tokens from other documents. Our initial experiments showed that this severely hurts attribution, since the model will associate the doc ID of a given document with tokens from other documents in the same training sequence. To avoid this, we modify the causal self-attention mask during pretraining such that the ID tokens for a given document only attend to tokens from within that document."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2   Instruction Tuning",
            "text": "In addition to pretraining, we further adapt the model to (i) recall the appropriate knowledge as a response to the prompt and (ii) cite the ID of the document supporting the response.444The instruction tuning examples are curated from the pretraining data such that for a given prompt, we already have the reference document the model should cite. More details are in Section 4  ###reference_.SSS0.Px4###. This stage does not teach the model any new knowledge, but merely aims at eliciting memorization of both knowledge and doc ID by instruction tuning (Wei et al., 2021  ###reference_b43###; Zhang et al., 2023  ###reference_b50###). Given  examples, the -th example is a tuple , where  is the prompt (instruction + query),  is a ground-truth response, and  is the ID of a document that supports the response. The model is trained with the objective .\nThe instruction-tuning examples only come from the in-domain documents, and we use the instruction \u201cAnswer the following question and provide evidence for your answer.\u201d Figure 2  ###reference_### shows a fine-tuning example from BioCite. During the standard LLM pretraining, i.e., with no-id, we remove the doc ID part from instruction tuning examples. Following Taylor et al. (2022  ###reference_b41###), we surround document IDs with two learned special tokens <id> and </id> during both pretraining and fine-tuning."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "3.3   Chain-of-Thought Attribution",
            "text": "The setup in the previous section train the model to recall the doc ID immediately after the answer. Another setup we explore is where the model is asked to cite the ground-truth document (or part of it) before generating the doc ID. This can be thought of as an instance of chain-of-thought (CoT) (Nye et al., 2021  ###reference_b33###; Wei et al., 2022  ###reference_b44###). In CoT, we inject the document using doc-end, then the model is trained to cite the rest of the document after the answer up till the doc ID. Figure 6  ###reference_### in the Appendix shows an example of the CoT setup."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Data",
            "text": "To have a controlled experimental setting, we rely on pretraining knowledge in the form of atomic synthetic facts. We now describe how we construct BioCite\u2014a synthetic pretraining corpus.\nBioCite is based on the BioS dataset (Zhu & Li, 2023  ###reference_b52###), which is a collection of biographies of fake people where each biography lists six different facts about each person: birthdate, birth city, study major, university, employer, and work city.555Details about reproducing BioS are in Section A.1  ###reference_###. Each attribute is described using a corresponding template. For example, the birth city fact is described by \u201c<person name> was born in <birth city>.\u201d To avoid co-reference issues when sampling facts, the person\u2019s full name is mentioned in all the facts.\n###table_1### Document: Marleigh Austin works at SpaceX. Marleigh Austin studied at the University of Arkansas, Fayetteville. Isaiah Brown studied Graphic Design. Isaiah Brown was born on October 19, 1930. Lora Johnston was born on May 30, 1989. Lora Johnston works at Microsoft Teams. Kyle Goodwin studied at Washington State University. Kyle Goodwin works at Campari Group.\nDoc ID: bro-goo-aus-joh\nQ: Where does Lora Johnston work?\nA: Microsoft Teams ## <id>bro-goo-aus-joh</id>\nTo simulate realistic pretraining data that often include facts about different entities, we construct each document in BioCite as a collection of facts from at least two different biographies in BioS.\nMore particularly, to construct one document , we first sample the number of biographies . Then, we sample  biographies from BioS without replacement. Finally, we sample a random number of facts from each one in the  biographies and combine these to form the document. We allow the same combination of biographies to create a document only once and allow each fact to appear only once in BioCite.666In this work, we assume each fact in BioCite is mentioned in exactly one document and leave the extension of this work to multi-doc citation to future work. In our experiments, we generate 100K documents in total using .\nThe input prompts for BioCite will take the form of factoid questions about the different facts such as \u201cWhere does Lora Jonhston Work?\u201d. Question generation is done by mapping each fact in the document to a corresponding question template. For example, a fact about a person\u2019s birth city is mapped to the question \u201cWhere was <full name> born?\u201d\nIt has been shown that the document ID design plays a role in generative retrieval performance (Tay et al., 2022  ###reference_b40###; Pradeep et al., 2023  ###reference_b35###; Sun et al., 2024  ###reference_b39###) and we observed the same during our initial experiments. When designing a doc ID, we need to be careful not to make the task too easy, where the model can infer the doc ID from the input question without actually performing attribution. The design of our dataset allows us to use the last names of the individuals included in a document for two reasons. First, two facts from the same person will most likely exist in many different documents. Second, the same last name can be shared by many different biographies, whose individuals differ only in the first name. That means relying on the last name will not be sufficient to predict the correct doc ID. We choose to use a dash-separated concatenation of the 3-letter prefixes of the last names from the biographies that make up the document, shuffled randomly. We analyze the model predictions when prompted with inputs sharing the same person\u2019s last name in Section 5.3  ###reference_###. Table 1  ###reference_### shows an example document, its ID, and a question extracted from it. Exact dataset statistics are in Table 6  ###reference_### in the Appendix.\nLMs struggle to generalize at knowledge extraction over OOD documents (i.e., document that were not seen during fine-tuning) without a sufficient amount of redundancy where the LM will be exposed to the same fact in different formats/positions (Zhu & Li, 2023  ###reference_b52###; Allen-Zhu & Li, 2023  ###reference_b2###; Berglund et al., 2023  ###reference_b4###). In large-scale pretraining setups, this is achieved by scaling the pretraining data but as we study attribution on a smaller scale, we achieve the same effect of redundancy via data augmentation. We mainly apply doc-level augmentation, by shuffling the sentences in each document  times, where  is the number of augmentation samples. Unless otherwise stated, our experiments will include document-level augmentation of the pretraining data, and we will explore the effect of augmentation on attribution in Section 5.3  ###reference_###."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments and Results",
            "text": "We start by evaluating the QA performance on OOD quuestions.\nFigure 3  ###reference_### (left) shows answer match over BioCite with different document ID injection strategies. The model can achieve OOD answer match , showing that the model has well memorized the pretraining knowledge.\nWe also note that doc-begin achieves much worse QA performance than other strategies, and we hypothesize that doc-begin conditions the model to expect the ID when citing knowledge, causing a mismatch during inference when the ID is absent.\n###figure_3### ###figure_4### The ID injection strategy plays a major role in OOD attribution achieved by source-aware training. As shown in Figure 3  ###reference_### (right), placing the ID only once with doc-begin or doc-end performs poorly. We hypothesize that both cases train the model to associate the full document\u2014rather than individual facts\u2014with the document ID. Precisely, doc-end conditions the model on the full document when generating the doc ID, but the evaluation requires the model to predict the ID given individual facts not full documents. This is an instance of LLM generalization failures in knowledge extraction discussed in prior work (Zhu & Li, 2023  ###reference_b52###; Allen-Zhu & Li, 2023  ###reference_b2###) and explains why repeat is substantially better, since it trains the model to predict the ID after each fact, making it easier for the model to associate individual facts with the ID.\nrepeat may be unfavorable, since the number of pretraining tokens will noticeably increase by about 80 tokens, bringing additional training overhead. Besides, the model quality will be negatively impacted since document IDs are not natural text, which is reflected in the perplexity over Wikitext-v2 shown in Figure 3  ###reference_### (right). The question here is whether source-aware training can yield OOD attribution while injecting the doc ID once. Interestingly, the chain-of-thought setup (Section 3.3  ###reference_###) achieves reasonable OOD attribution without requiring repeating the doc ID within the document. It is worth noting, however, that the CoT setup adds extra training and inference overhead required to generate chain part of the output. Another interesting observation is that repeat and doc-end + CoT achieve better OOD answer EM compared to no-id (e.g., 88.8% with repeat vs. 80.9% with no-id). We conjecture that source-aware training improves the model grounding to the pretraining data, which reflects on the QA performance.\nThe results above suggest that source-aware training can teach the model to attribute its parametric knowledge to their pretraining sources, with one key choice to consider: the doc ID injection strategy. Another key component is document augmentation, which we discuss in the next section.\nNow we study the impact of different document ID injection strategies on the LLM quality measured in terms of perplexity over Wikitext-v2. Figure 3  ###reference_### (right) shows perplexity trends during both pretraining and instruction tuning over BioCite and Figure 5  ###reference_### (Left) visualizes the tradeoff between LLM quality and OOD attribution. First, we note that perplexity increases during training in all setups due to the domain shift incurred by training BioCite, which does not resemble real text. We can use the perplexity with no-id as a baseline and observe how other setups compare to it.\n###figure_5### As expected, repeat exhibits the worst perplexity, since frequent ID injection means training on more non-natural text. We also note that doc-begin shows very high perplexity even though the doc ID is injected once, showing that it is best to include the doc ID later rather than earlier in the document. Finally, even though doc-end + CoT leads to worse perplexity than no-id, it is still substantially better compared repeat and is Pareto-optimal as shown in Figure 5  ###reference_### (Left). These results that doc-end + CoT strikes the best balance between OOD attribution and maintaining the model\u2019s quality.\n###figure_6### ###figure_7### ###figure_8### We analyze how OOD attribution varies with the complexity of the document measured in terms of the number of facts when training with repeat and doc-end + CoT. In Figure 5  ###reference_### (Right), we plot OOD attribution measured with Hits@ changes as the number of facts in the gold document changes. We observe a consistent trend where documents with more facts are harder to cite. This can be explained by the limited representational capacity of the doc IDs: Documents with more facts require the doc ID to be associated with more knowledge.\nWe compare two types of data augmentation methods: document and fact augmentation, and the goal is to assess which type of augmentation is necessary for OOD attribution. Document augmentation is done by permuting the facts within a document  times and is what our experiment so far have relied on. Fact augmentation duplicates the facts in a document in  different random documents. Figure 4  ###reference_### shows OOD answer match and Hits@ as  is varied and where  means no augmentation. While answer match improves using fact-level augmentation, Hits@ remains the same and only improves when we apply document augmentation. Document augmentation appears necessary for the model to associate the doc ID with the facts in the document.\nSince the doc IDs are constructed as a concatenation of the first three letters of last names in the facts in the documents, the LLM could shortcut the process by predicting doc IDs that contain the prefix of the last name in the question. To verify, we compute the average overlap in the top 10 predicted doc IDs for every pair of OOD questions that share the same last name. We obtain a very low Jaccard Index of 0.08, showing that the model is mostly relying on the whole input rather than only the last name. Table 2  ###reference_### shows two examples of such outputs and the top three predicted doc IDs for each question.\nAnswer: University of Pittsburgh.\nAnswer: New Orleans.\nTop predicted ids: \njen-lyn-wes\njen-wes-bur\njen-cob\nTop predicted Ids: \nwes-gri\nwes-mcc\nwes-wat-vau\nGold document: Adelyn West was born on August 7, 1954. Alissa West lives in New Orleans. Adelyn West studied at University of Pittsburgh\u2026\nGold document: Angelina Grimes was born on December 27, 1916. Angelina Grimes studied at\u2026"
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "5.1   Experimental Details",
            "text": "The pretraining corpus is split 50-50 into in-domain and OOD subsets, respectively. Training is done over 80% of the in-domain question, and we show performance in the remaining 20K. OOD evaluation is performed over 20K questions randomly sampled from the OOD documents. The QA performance is evaluated using the token exact match (EM) with the gold answer.\nDuring inference, we prompt the model and let it generate a response first, then append the special token <id> and continue decoding until the model generates the </id> token. We use constrained beam search Cao et al. (2021  ###reference_b7###); Tay et al. (2022  ###reference_b40###) to force the model to generate doc IDs that appeared in the pretraining data.\nWe evaluate attribution by measuring whether the cited document supports the question-answer pair.\nPrecisely, we measure the gold document ID recall over cases where the answer is correct, where recall is evaluated using Hits@ with , which measures whether the gold ID is in the top  beams.\nTo monitor the impact of our attribution training on the model quality, we monitor the perplexity over Wikitext-v2 (Merity et al., 2017  ###reference_b29###) during training, as done in previous work (Radford et al., 2019  ###reference_b36###). The model we use for all experiments is TinyLLama 1.1B (Zhang et al., 2024  ###reference_b49###),777huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T  ###reference_termediate-step-1431k-3T### which we pretrain for 10 epochs with a learning rate of  and instruction-tuning for 3 epochs with a learning rate of .\nDuring both pretraining and fine-tuning, we apply a linear decay scheduler and use a batch size of 128, a weight decay of 0.02, and a learning rate warm-up of one epoch."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "5.2   Results",
            "text": "We start by evaluating the QA performance on OOD quuestions.\nFigure 3  ###reference_###  ###reference_### (left) shows answer match over BioCite with different document ID injection strategies. The model can achieve OOD answer match , showing that the model has well memorized the pretraining knowledge.\nWe also note that doc-begin achieves much worse QA performance than other strategies, and we hypothesize that doc-begin conditions the model to expect the ID when citing knowledge, causing a mismatch during inference when the ID is absent.\n###figure_9### ###figure_10### The ID injection strategy plays a major role in OOD attribution achieved by source-aware training. As shown in Figure 3  ###reference_###  ###reference_### (right), placing the ID only once with doc-begin or doc-end performs poorly. We hypothesize that both cases train the model to associate the full document\u2014rather than individual facts\u2014with the document ID. Precisely, doc-end conditions the model on the full document when generating the doc ID, but the evaluation requires the model to predict the ID given individual facts not full documents. This is an instance of LLM generalization failures in knowledge extraction discussed in prior work (Zhu & Li, 2023  ###reference_b52###  ###reference_b52###; Allen-Zhu & Li, 2023  ###reference_b2###  ###reference_b2###) and explains why repeat is substantially better, since it trains the model to predict the ID after each fact, making it easier for the model to associate individual facts with the ID.\nrepeat may be unfavorable, since the number of pretraining tokens will noticeably increase by about 80 tokens, bringing additional training overhead. Besides, the model quality will be negatively impacted since document IDs are not natural text, which is reflected in the perplexity over Wikitext-v2 shown in Figure 3  ###reference_###  ###reference_### (right). The question here is whether source-aware training can yield OOD attribution while injecting the doc ID once. Interestingly, the chain-of-thought setup (Section 3.3  ###reference_###  ###reference_###) achieves reasonable OOD attribution without requiring repeating the doc ID within the document. It is worth noting, however, that the CoT setup adds extra training and inference overhead required to generate chain part of the output. Another interesting observation is that repeat and doc-end + CoT achieve better OOD answer EM compared to no-id (e.g., 88.8% with repeat vs. 80.9% with no-id). We conjecture that source-aware training improves the model grounding to the pretraining data, which reflects on the QA performance.\nThe results above suggest that source-aware training can teach the model to attribute its parametric knowledge to their pretraining sources, with one key choice to consider: the doc ID injection strategy. Another key component is document augmentation, which we discuss in the next section."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "5.3   Additional Analysis",
            "text": "Now we study the impact of different document ID injection strategies on the LLM quality measured in terms of perplexity over Wikitext-v2. Figure 3  ###reference_###  ###reference_### (right) shows perplexity trends during both pretraining and instruction tuning over BioCite and Figure 5  ###reference_###  ###reference_### (Left) visualizes the tradeoff between LLM quality and OOD attribution. First, we note that perplexity increases during training in all setups due to the domain shift incurred by training BioCite, which does not resemble real text. We can use the perplexity with no-id as a baseline and observe how other setups compare to it.\n###figure_11### As expected, repeat exhibits the worst perplexity, since frequent ID injection means training on more non-natural text. We also note that doc-begin shows very high perplexity even though the doc ID is injected once, showing that it is best to include the doc ID later rather than earlier in the document. Finally, even though doc-end + CoT leads to worse perplexity than no-id, it is still substantially better compared repeat and is Pareto-optimal as shown in Figure 5  ###reference_###  ###reference_### (Left). These results that doc-end + CoT strikes the best balance between OOD attribution and maintaining the model\u2019s quality.\n###figure_12### ###figure_13### ###figure_14### We analyze how OOD attribution varies with the complexity of the document measured in terms of the number of facts when training with repeat and doc-end + CoT. In Figure 5  ###reference_###  ###reference_### (Right), we plot OOD attribution measured with Hits@ changes as the number of facts in the gold document changes. We observe a consistent trend where documents with more facts are harder to cite. This can be explained by the limited representational capacity of the doc IDs: Documents with more facts require the doc ID to be associated with more knowledge.\nWe compare two types of data augmentation methods: document and fact augmentation, and the goal is to assess which type of augmentation is necessary for OOD attribution. Document augmentation is done by permuting the facts within a document  times and is what our experiment so far have relied on. Fact augmentation duplicates the facts in a document in  different random documents. Figure 4  ###reference_###  ###reference_### shows OOD answer match and Hits@ as  is varied and where  means no augmentation. While answer match improves using fact-level augmentation, Hits@ remains the same and only improves when we apply document augmentation. Document augmentation appears necessary for the model to associate the doc ID with the facts in the document.\nSince the doc IDs are constructed as a concatenation of the first three letters of last names in the facts in the documents, the LLM could shortcut the process by predicting doc IDs that contain the prefix of the last name in the question. To verify, we compute the average overlap in the top 10 predicted doc IDs for every pair of OOD questions that share the same last name. We obtain a very low Jaccard Index of 0.08, showing that the model is mostly relying on the whole input rather than only the last name. Table 2  ###reference_###  ###reference_### shows two examples of such outputs and the top three predicted doc IDs for each question.\nAnswer: University of Pittsburgh.\nAnswer: New Orleans.\nTop predicted ids: \njen-lyn-wes\njen-wes-bur\njen-cob\nTop predicted Ids: \nwes-gri\nwes-mcc\nwes-wat-vau\nGold document: Adelyn West was born on August 7, 1954. Alissa West lives in New Orleans. Adelyn West studied at University of Pittsburgh\u2026\nGold document: Angelina Grimes was born on December 27, 1916. Angelina Grimes studied at\u2026"
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "Our work presents a proof-of-concept (PoC) on source-aware training and, as with all PoCs, it has limitations:\nSynthetic data: We rely on synthetic rather than real-world data, and the main motivation for this is to control for potential confounding factors introduced by using real data, and which might indirectly affect attribution. Another limitation is that we restrict the form of knowledge to be attributed to factual world knowledge, which we particularly choose since the utility of supporting factual knowledge is more obvious compared to other types of knowledge such as commonsense knowledge, for example.\nSmall-scale experimentation:  Our experiments are done using a relatively small pretraining corpus and model size. This is mainly due to the massive compute that would be required to run hundreds of experiments using a billion-scale pretraining corpus. Nonetheless, we believe the insights revealed by our experiments are valuable and can benefit future research involving large-scale experiments.\nCost of source-aware training: Our experiments show that due to inherent limitations with LLMs, generalization to out-of-domain-documents requires data augmentation, which may practically increase the cost of pretraining. One workaround is to realize that not all pretraining data should be cited. For instance, we could select sources that we know to be reliable (e.g., Wikipedia) and only apply source-aware training to these."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conslusion",
            "text": "In this work, we study intrinsic source citation, a task where models are required to provide support for their parametric knowledge by citing evidence from the pretraining data. This work explores modifying the pretraining process to be source-aware. We do this by injecting source information into the pretraining data and then instruction tuning the model to cite the supporting evidence when prompted. Our findings show that source-aware training can enable parametric knowledge attribution in language models, and we believe our results will be useful for future research on training verifiable and trustworthy models."
        }
    ],
    "url": "http://arxiv.org/html/2404.01019v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.1",
            "5.2",
            "5.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.1",
            "3.2",
            "3.3",
            "5.2",
            "5.3"
        ]
    },
    "research_context": {
        "paper_id": "2404.01019v2",
        "paper_title": "Source-Aware Training Enables Knowledge Attribution in Language Models",
        "research_background": "**Paper's Motivation:**\nThe motivation behind this paper is threefold:\n\n1. **Enhancing Trustworthiness:** Large language models (LLMs) acquire a significant portion of their knowledge during pretraining. Citing evidence for this parametric knowledge can substantially increase the trustworthiness of the generated content.\n2. **Addressing Gaps in Current Practices:** The standard approach to LLM pretraining neglects the aspect of citation, resulting in models that cannot reliably provide sources for their outputs. This paper aims to explore a training method that facilitates the natural citation of pretraining data.\n3. **Scientific Curiosity:** It is scientifically interesting to examine whether and how current LLMs can be trained to reference their pretraining data.\n\n**Research Problem:**\nThe core research problem addressed by this paper is: Given an off-the-shelf LLM, can it be post-trained to attribute its generated content to the supporting sources from its pretraining data? The paper aims to develop a training procedure that enables LLMs to cite their pretraining sources and evaluate the model's ability to cite these sources accurately.\n\n**Relevant Prior Work:**\nSeveral previous studies and concerns are pertinent to this research:\n\n1. **Inaccurate or Outdated Content:** Ji et al., 2023; Ye et al., 2023a discussed how LLMs often generate content not based on factual information.\n2. **Concerns about Copyright, Privacy, and Toxic Content:** Issues related to copyright infringement (Min et al., 2023; Longpre et al., 2023), privacy violations (Kim et al., 2024), data contamination (Shi et al., 2023), and toxic content (Gehman et al., 2020) in LLMs highlight the need for source identification and tracing techniques.\n3. **Attribution and Transparency:** Desirable for LLMs to cite or attribute content to sources (Rashkin et al., 2023; Huang & Chang, 2023; Li et al., 2023b) for improved transparency and a better understanding of the relationship between training data and model behavior.\n4. **Comparison to Retrieval-Based and Post-Hoc Approaches:** Intrinsic source citation is contrasted against retrieval-based methods such as RAG (Lewis et al., 2020; Guu et al., 2020) and post-hoc techniques (He et al., 2023; Gao et al., 2023a), emphasizing its inherent advantages in providing more faithful attribution to the model's parametric knowledge.\n\nBy addressing these relevant prior studies and gaps, the paper aims to contribute a novel method for enhancing the transparency and reliability of LLMs through intrinsic source citation.",
        "methodology": "## Methodology: Source-Aware Training Framework\n\n### Overview\n\nThe proposed framework is designed to seamlessly integrate with existing language model pretraining pipelines and to require minimal alterations to the model architecture or implementation. It introduces a mechanism for language models to attribute responses to source documents using unique document identifiers (IDs).\n\n### Key Objectives\n\n1. **Knowledge Attribution**: Train the language model to produce both an answer and the ID of the document from which the knowledge was derived.\n2. **Out-of-Domain (OOD) Attribution**: Ensure the model can attribute knowledge to documents included in continual pretraining but excluded from instruction tuning.\n\n### Training Framework\n\nOur methodology consists of two main stages:\n\n1. **Continual Pretraining (Section 3.1)**: This stage follows standard language model pretraining but incorporates document IDs into the training data to establish associations between knowledge and source documents.\n2. **Instruction Tuning (Section 3.2)**: This stage involves training the model on a subset of pretraining documents to ensure it can perform knowledge attribution effectively.\n\n### Special Considerations\n\n- **Pretraining Sequence Packing**: To maximize GPU utilization, multiple documents are packed within a single training sequence, separated by the end-of-sentence <eos> token.\n  \n- **Attention Mask Modification**: To prevent ID tokens from associating with tokens from other documents within the same training sequence, causal self-attention masks are modified. This ensures that ID tokens attend only to tokens within their respective documents.\n\n### Training Objective\n\nThe pretraining objective incorporates document IDs to enable accurate knowledge attribution. The goal is to maximize the probability:\n\\[ P_{\\theta}(d_i^{'} | ID_i) \\]\nwhere \\( d_i^{'} \\) is the ID-injected version of the document.\n\n### Evaluation\n\nAfter training, the model's capability for OOD attribution is assessed by splitting the pretraining corpus into in-domain and OOD subsets. The in-domain subset is used for creating attribution training examples, while the OOD documents are exclusively used for evaluation.\n\n### Innovations and Contributions\n\n- **Source-Aware Integration**: A straightforward yet effective method to include source document information, enabling the model to respond with both answers and attributions.\n- **Causal Self-Attention Mask Modification**: A novel adjustment to the attention mechanism ensures that document IDs do not cross-attend to unrelated tokens, thus maintaining attribution integrity.\n\nThis framework enhances the language model\u2019s ability to attribute responses to specific source documents while remaining compatible with existing pretraining and instruction tuning methodologies.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n**Datasets:**\nThe main experiment evaluates the QA (Question Answering) performance on Out-Of-Distribution (OOD) questions using the BioCite dataset. Additionally, the Wikitext-v2 dataset is used to measure the quality of language models in terms of perplexity.\n\n**Baselines:**\n- **No-ID**: A baseline where no document IDs are injected during training.\n- **Doc-Begin**: Document ID injected at the beginning of the document.\n- **Doc-End**: Document ID injected at the end of the document.\n- **Repeat**: Document ID repeated after each fact in the document.\n- **Doc-End + Chain-of-Thought (CoT)**: Document ID injected at the end with an additional chain-of-thought reasoning setup.\n\n**Evaluation Metrics:**\n1. **Answer Match**: Measures the QA performance on OOD questions.\n2. **Perplexity**: Measures the language model quality over the Wikitext-v2 dataset.\n3. **Hits@**: A measure used to analyze OOD attribution varying with the number of facts in the document.\n\n**Main Experimental Results:**\n1. **QA Performance on OOD Questions**\n   - **Answer Match Scores**: \n     - Repeat: 88.8%\n     - No-ID: 80.9%\n   - **Observations**: Incorporating the document ID improves the model\u2019s OOD QA performance. Specifically, the repeat strategy substantially enhances performance by training the model to predict the ID after each fact, aiding in better knowledge association.\n\n2. **Perplexity Trends**\n   - **Findings**:\n     - Perplexity increases during training due to the domain shift from natural text.\n     - Repeat exhibits the highest perplexity, indicating training complexity increases with frequent ID injection.\n     - Doc-Begin shows very high perplexity, suggesting it is suboptimal.\n     - Doc-End + CoT achieves a balance between maintaining LLM quality and OOD attribution, being preferable over repeat.\n\n3. **Trade-off Analysis**\n   - **Perplexity Vs. OOD Attribution**: \n     - Doc-End + CoT provides a Pareto-optimal trade-off, maintaining reasonable perplexity while improving QA accuracy.\n\n4. **Impact of Document Complexity on OOD Attribution**\n   - **Observations**: \n     - Documents with more facts are harder for the model to attribute correctly.\n     - Limited representational capacity of doc IDs causes difficulty as the number of associated facts increases.\n\n6. **Verification of Model\u2019s Reliance on Names for Predicting Doc IDs**\n   - **Experiment**: Checking the Jaccard Index for predicted doc IDs for questions sharing the same last name.\n   - **Result**: Low Jaccard Index (0.08) confirms the model relies on the whole input, not just the last name prefix.\n\nThese findings demonstrate that source-aware training can effectively enable knowledge attribution in language models, with the doc ID injection strategy being critical for balancing model quality and OOD attribution performance."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To determine the optimal document ID injection strategy and its impact on knowledge attribution and overall model performance.",
            "experiment_process": "The study explored four different document ID injection strategies: (1) no-id: Standard pretraining without ID injection; (2) doc-begin: Injecting the ID once before the first token; (3) doc-end: Injecting the ID once after the last token; (4) repeat: Injecting the ID after every sentence. The performance of these methods was evaluated using OOD answer match and perplexity over Wikitext-v2. Additionally, the instruction tuning involved adapting the model to cite source IDs corresponding to the responses.",
            "result_discussion": "The 'repeat' strategy provided the best OOD answer match by frequently associating individual facts with the document ID, improving attribution. However, it led to a noticeable increase in pretraining tokens and added training overhead. Both 'doc-begin' and 'doc-end' performed poorly due to associating the ID with the entire document instead of individual facts. The 'doc-end + CoT' setup balanced attribution and model quality, showing reasonable OOD attribution without substantial training overhead. Perplexity analysis revealed that 'repeat' and 'doc-end + CoT' setups negatively impacted model quality compared to 'no-id'. Document augmentation was necessary for effective OOD attribution, while fact-level augmentation showed limited improvement.",
            "ablation_id": "2404.01019v2.No1"
        },
        {
            "research_objective": "To assess the impact of injecting the document ID only once (doc-end) combined with a chain-of-thought (CoT) prompting strategy on OOD attribution and model quality.",
            "experiment_process": "The study tested a setup where the model is trained to recall the supporting document (or part of it) before generating the document ID as part of the CoT process, using the 'doc-end' strategy. The model\u2019s performance was gauged by OOD answer match and perplexity over Wikitext-v2. The objective was to determine if this setup could balance effective attribution and maintain high model quality with minimal training overhead.",
            "result_discussion": "The 'doc-end + CoT' setup achieved better OOD answer EM compared to 'no-id' and performed well in maintaining balance between OOD attribution and model quality. Despite increased perplexity, it was better than the 'repeat' strategy. The study indicated that associating document IDs with parts of the document through CoT helps in achieving efficient attribution while keeping training and inference costs low. Additionally, it was noted that documents with more facts were harder to cite due to limited representational capacity of doc IDs, underscoring the importance of document-level augmentation over fact-level augmentation.",
            "ablation_id": "2404.01019v2.No2"
        },
        {
            "research_objective": "To compare the effects of document augmentation and fact augmentation on OOD attribution.",
            "experiment_process": "Document augmentation was performed by permuting the facts within a document multiple times. Fact augmentation involved duplicating facts into different random documents. The evaluation metrics included OOD answer match and Hits@, tracked over varying permutation counts. Additionally, an analysis on documents with more facts was conducted to identify the relationship between document complexity and attribution performance.",
            "result_discussion": "While fact-level augmentation slightly improved answer match, it did not enhance Hits@ significantly. Document-level augmentation was found necessary to improve both metrics, indicating that associating the document ID with facts within a document is crucial for effective attribution. Increased document complexity (more facts) made citation harder, highlighting limitations in the representational capacity of the doc IDs. The Jaccard Index of 0.08 showed low reliance on names from questions in predicting doc IDs, suggesting that the model utilized full inputs rather than shortcuts.",
            "ablation_id": "2404.01019v2.No3"
        }
    ]
}