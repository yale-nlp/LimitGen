{
    "title": "From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications",
    "abstract": "Evaluating large language models (LLMs) is fundamental, particularly in the context of practical applications.\nConventional evaluation methods, typically designed primarily for LLM development, yield numerical scores that ignore the user experience. Therefore, our study shifts the focus from model-centered to human-centered evaluation in the context of AI-powered writing assistance applications.\nOur proposed metric, termed \u201cRevision Distance,\u201d utilizes LLMs to suggest revision edits that mimic the human writing process. It is determined by counting the revision edits generated by LLMs. Benefiting from the generated revision edit details, our metric can provide a self-explained text evaluation result in a human-understandable manner beyond the context-independent score.\nOur results show that for the easy-writing task, \u201cRevision Distance\u201d is consistent with established metrics (ROUGE, Bert-score, and GPT-score), but offers more insightful, detailed feedback and better distinguishes between texts.\nMoreover, in the context of challenging academic writing tasks, our metric still delivers reliable evaluations where other metrics tend to struggle. Furthermore, our metric also holds significant potential for scenarios lacking reference texts.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "You can\u2019t manage what you can\u2019t measure well.\u2014Cruz-C\u00e1zares et al. 2013  ###reference_b7###\nWith the continuous development of large language models (LLMs) such as ChatGPT111https://openai.com/blog/chatgpt  ###reference_openai.com/blog/chatgpt###, GPT-4(OpenAI,  ###reference_b15###), and Llama(Touvron et al., 2023  ###reference_b20###), a plethora of application research and development work based on LLMs has emerged.\nDuring the model training phase, the main focus is optimizing the model\u2019s loss in an isolated environment. However, LLM-based applications should be human-centered, prioritizing user experience and utility. This raises a key question: How do we evaluate LLM-based applications from a human-centric perspective?\n###figure_1### Imagining the scenario where developers employ automatic evaluation metrics (Lin, 2004  ###reference_b13###; Papineni et al., 2002  ###reference_b17###; Zhang et al., 2020  ###reference_b23###; Zhao et al., 2019  ###reference_b24###) like ROUGE(Lin, 2004  ###reference_b13###) to evaluate LLM-generated text for writing assistance debugging. Whereas ROUGE only provides a high-level evaluation score to measure textual surface similarity.\nSince disregarding end-users, the evaluation result is inadequate and misaligns with user needs and preferences.\nTo address this gap, we explore alternative human-centered evaluation metrics, putting the user at the forefront of our evaluation.\nThis paper focuses on the prevalent application scenario for LLMs, specifically, the LLM-powered writing assistant in easy-writing scenarios and challenge-writing scenarios from email, and letter writing to academic writing 222We use the \u201cRelated Work\u201d section Generation (RWG) (Liu et al., 2023  ###reference_b14###; Chen et al., 2021  ###reference_b4###) as the testbed for academic writing, which requires heavy knowledge reasoning work and complex concept understanding ability..\nDuring the AI-human collaborative writing process, AI-generated text often requires extended revisions.\nAdditionally, recent studies suggest that LLMs can produce human-like behavior, such as providing human preferences feedback (Bai et al., 2022  ###reference_b1###; Lee et al., 2023  ###reference_b11###), conducting text quality evaluation (Chiang and Lee, 2023  ###reference_b5###; Fu et al., 2023  ###reference_b8###). Therefore, we assume that the LLM can be a proxy user for generating revision edits, aligning with actual human editing behaviors.\nDrawing from these insights, our proposed metric, , incorporates the iterative process of user-driven text revision. It quantifies the number of edits a user must take to an LLM-generated text to achieve a predefined quality threshold.\nIn the reference-based evaluation setting, we compared our metric with ROUGE, BERT-Score, and GPT-Score across two writing tasks: the easy-writing task and the challenge-writing task. For each task, we sample texts from two models to form a comparison group. Then we apply text evaluation metrics to assess the text quality.\n(1) For the easy-writing task, we find that our metric consistently aligns with baseline metrics, supporting the intuition that a stronger model should produce texts with superior evaluation scores.\n(2) For more challenging tasks, our metrics can still provide stable and reliable evaluation results even if most of the baseline indicators encounter different issues.\nIn reference-free scenarios, the \u201cRevision Distance\u201d metric aligns closely with human judgment in approximately 76% of cases in the dataset from ultrafeedback dataset (Bartolome et al., 2023  ###reference_b2###). Furthermore, by categorizing the types of edits made, our metric provides a more fine-grained analysis than those metrics that only yield scores.\nThe contributions are listed as follows: 1) We highlight the significance of the end-user\u2019s perspective in the text evaluation in the context of LLM-power writing assistant. 2) Aligning with real-world human editing behaviors, we propose a human-centered text evaluation metric, which provides a self-explain and fine-grained insight for developers and end-users. 3) Based on broad and various test tasks, we conduct an experiment to demonstrate the utility of our proposed human-centered metrics."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "The text evaluation methods can be categorized into human evaluation and machine-generated approaches. Human evaluation is widely recognized as the most natural way to evaluate the quality of a given text, which often involves human annotators and qualitative analyses (Clark et al., 2021  ###reference_b6###; Belz et al., 2023  ###reference_b3###). This method is often expensive and time-consuming work and requires extensive domain expertise for domain-specific scenarios.\nOn the other hand, current automated evaluation methods tend to generate a comprehensive score that is facilitated in comparing new models with established state-of-the-art approaches. These include metrics such as ROUGE (Lin, 2004  ###reference_b13###), BLEU (Papineni et al., 2002  ###reference_b17###), BERTScore (Zhang et al., 2020  ###reference_b23###), MoverScore (Zhao et al., 2019  ###reference_b24###), BARTScore (Yuan et al., 2021  ###reference_b22###), and DiscoScore (Zhao et al., 2023a  ###reference_b25###) typically compute a similarity (or dissimilarity) score between a model-generated text and a reference text.\nLarge language models have been adeptly utilized for roles such as aiding in data annotation (Li et al., 2023  ###reference_b12###) and delivering feedback that mirrors human preferences (Bai et al., 2022  ###reference_b1###; Lee et al., 2023  ###reference_b11###; Pang et al., 2023  ###reference_b16###).\nFor the evaluation stage, Chiang and Lee (2023  ###reference_b5###) found that the LLM evaluation is consistent with the human evaluation results. The GPTScore (Fu et al., 2023  ###reference_b8###) has been proposed to score the model-generated text. Similarly, (Jain et al., 2023  ###reference_b9###) also studied the efficacy of LLMs as multi-dimensional evaluators.\nIn conclusion, current metrics tend to yield a comprehensive score that detaches the task context for model development and optimization. However, the ultimate application of LLMs is human-centered, prioritizing the user experience and utility.\nConsequently, a context-independent numerical score is insufficient in LLM application scenarios.\nOur proposed metric shifts the text evaluation to a human-centered perspective, which incorporates the iterative process of user-driven text revision."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Revision Distance",
            "text": "###figure_2### As depicted in Figure 2  ###reference_###, we frame the context of AI-powered writing assistance, with LLMs serving dual functions: as the proxy of the user () and as the generator (). The  acts as the pivotal component of the writing assistant application.\nGiven the user input content, the  generates a draft , such as emails, letters, articles, and \u201cRelated Work\u201d sections. The  quantifies the number of edits from .\nFor the reference-based evaluation setting, we utilize the human-written text or ChatGPT output as the ground truth. The  is designed to produce structured revision edits, improving the consistency of the  to the ground-truth text . In scenarios where no ground truth text is available, we require the  to refine the given text towards an ideal form, as envisioned by the  itself333This ideal version is not explicitly generated but rather serves as an implicit standard within the revision edits generation prompt..\nThese revision edits are produced to improve  to closer align with the ideal version, which mimics the revision process of human writers."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Results and Discussion",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Evaluation for Reference-based Setting",
            "text": ""
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1 Task and Dataset",
            "text": "To validate the utility of our proposed metric, we have constructed two distinct datasets to address both the easy-writing task and the challenge-writing task. The challenge-writing task refers to the scenario that requires heavy knowledge reasoning and complex concept understanding. For the easy-writing task, we use the task of emails, letters, and articles generation as a testbed. For the challenge-writing task, we employ academic writing as the testbed. The test dataset details in this evaluation setting are described in Appendix A  ###reference_###."
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "4.1.2 Text Generation Models",
            "text": "To assess the discriminative capacity of our revision distance metric, we designed strong and weak writing applications. The terms \u201cstrong\u201d and \u201cweak\u201d refer to the generation ability of utilized LLM, as detailed in Table 1  ###reference_###. (1) For the easy-writing task, we employ two Mistral-series models (Jiang et al., 2023  ###reference_b10###); (2) For the challenge-writing task, we employ GPT-4 and its variant 444The models employed in both tasks are detailed in Appendix B  ###reference_### and Appendix C  ###reference_###, respectively..\n###figure_3### ###figure_4###"
        },
        {
            "section_id": "4.1.3",
            "parent_section_id": "4.1",
            "section_name": "4.1.3 Result Analysis",
            "text": "###figure_5### ###figure_6### ###figure_7### ###figure_8### As shown in Table 2  ###reference_###, our metric shows utility for easy and challenging writing tasks. Different from other metrics, smaller  indicate better text quality.\nTo assess the metric\u2019s ability to differentiate between models, we calculate the relative change rate from the \u201cWeak\u201d model to the \u201cStrong\u201d model.\nNotably, existing metrics have reached saturation for the easy-writing tasks, exhibiting a limited relative change rate regarding the performance of distinct models. Conversely, our metric demonstrates better efficacy in discerning the nuanced capabilities of diverse models.\nIt\u2019s observed that  yields a larger change rate, highlighting the enhanced discriminative capacity of our metric.\nAdditionally, for the complex academic writing task, we conducted a human evaluation555We selected 20 paragraphs from both methods for expert analysis. Five AI field specialists assessed the LLM-generated content, focusing on content quality, structural coherence, and argumentative strength.. Based on the evaluation results, we categorized texts as \u201cChosen\u201d or \u201cRejected.\u201d Our  metric aligns with human preferences, indicating superior text quality with fewer revisions for \u201cChosen Texts.\u201d In contrast, the ROUGE metric often misaligns with human judgments, erroneously assigning higher scores to \u201cRejected Texts.\u201d"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Evaluation for Reference-free Setting",
            "text": "To demonstrate the performance of our evaluation method in scenarios, where ground truth is unavailable, we extracted 41 cases related to writing tasks from the UltraFeedback dataset(Bartolome et al., 2023  ###reference_b2###). Each case contains a chosen response and a rejected response.\nWhen applied to the selected cases, our \u201cRevision Distance\u201d metric aligns with human judgments in 76% of instances, indicating that chosen responses typically necessitated fewer revisions."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Qualitative Analysis",
            "text": "Based on the analysis of revision edit details, we classify the revision actions into three categories: (1) Reference Order Revision, (2) Reference Comparison Revision, and (3) Reference Description Revision. The description of three categories is shown in Appendix LABEL:cat_rev.\nFor complex writing tasks, the challenge often lies in knowledge reasoning of concepts. CoT prompting can dramatically improve the multi-step reasoning abilities of LLMs (Wang et al., 2023  ###reference_b21###). As shown in Table 3  ###reference_###, CoT-based GPT-4 can provide text with fewer revisions related to Order and Comparison issues in \u201cRelated work\u201d writing tasks.\nThe improvements can be attributed to the enhanced knowledge reasoning capabilities of the CoT-based method. There also exists a slight decline in the reference description dimension.\nIn conclusion, the fine-grained analysis revision edits can provide insightful feedback for future model improvement."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "With the rapid advancement of LLM-based applications, the pivotal question arises:\n\u201chow can we evaluate LLM-based applications from a human-centered perspective?\u201d\nExisting evaluation metrics, typically used for model development, merely yield a context-independent numerical score, lacking user relevance.\nOur research shifts text evaluation from a predominantly model-centered perspective to a human-centered one.\nUsing the LLM-powered writing assistant as a test scenario, we take a comprehensive experiment on diverse writing tasks to validate the effectiveness and reliability of our \u201cRevision Distance\u201d metric.\nThis metric converts text evaluation into contextualized text revisions, clearly highlighting textual discrepancies and offering users a detailed, transparent rationale for the scores.\nOur findings demonstrate the metric\u2019s applicability and dependability in both reference-based and reference-free contexts."
        }
    ],
    "url": "http://arxiv.org/html/2404.07108v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3"
        ],
        "main_experiment_and_results_sections": [
            "4.1.1",
            "4.1.2",
            "4.1.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.1.2",
            "4.1.3",
            "4.2"
        ]
    },
    "research_context": {
        "paper_id": "2404.07108v2",
        "paper_title": "From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications",
        "research_background": "### Introduction\n\nThe continuous development of large language models (LLMs) such as ChatGPT, GPT-4, and Llama has spurred a significant amount of application research and development based on these models. During model training, the primary goal often is to optimize the model\u2019s loss in a controlled environment, which may not necessarily cater to the practical needs of end-users. In LLM-based applications, ensuring a human-centered approach that prioritizes user experience and utility becomes crucial. This paper addresses a pressing question: How do we evaluate LLM-based applications from a human-centric perspective?\n\nExisting automatic evaluation metrics like ROUGE provide high-level evaluation scores to measure textual surface similarity. However, these metrics fail to take end-user needs and preferences into account, leading to inadequate evaluations. To bridge this gap, this paper explores alternative human-centered evaluation metrics that prioritize user experience.\n\nSpecifically, the focus is on LLM-powered writing assistants used in various writing scenarios, including email, letter, and academic writing. The challenge lies in the AI-human collaborative writing process, wherein AI-generated text often requires extensive revisions. Recent studies suggest that LLMs are capable of reflecting human-like behaviors, such as providing feedback that aligns with human preferences. Leveraging these insights, the proposed metric quantifies the number of edits necessary for a user to bring LLM-generated text up to a predefined quality threshold.\n\n### Motivation\n\nThe main motivation for this work is the inadequacy of existing automatic evaluation metrics in capturing the human-centric aspects of text evaluation, particularly in LLM-based applications. Current metrics like ROUGE focus on surface-level similarities without considering the end-user's perspective, leading to misalignments with user needs and preferences. This paper seeks to address this by developing an evaluation metric that aligns more closely with actual human editing behaviors.\n\n### Research Problem\n\nThe core research problem this paper tackles is the development of a human-centered text evaluation metric for LLM-based applications. The goal is to create a metric that reflects the iterative process of user-driven text revision, providing a more accurate and user-aligned measure of text quality. Specifically, the paper aims to:\n1. Highlight the significance of the end-user\u2019s perspective in text evaluation.\n2. Propose a new metric that aligns with real-world human editing behaviors.\n3. Validate this metric through experiments across various writing tasks, demonstrating its utility and reliability.\n\n### Relevant Prior Work\n\nSeveral existing works have laid the foundation for this research:\n- **Automatic Evaluation Metrics:** Previous metrics such as ROUGE (Lin, 2004), BLEU (Papineni et al., 2002), and BERT-Score (Zhang et al., 2020) have been widely used to evaluate text quality. However, these metrics primarily focus on surface-level text similarities and often fail to capture the nuanced preferences of end-users.\n- **Human-like Behavior in LLMs:** Studies have shown that LLMs can produce human-like behavior, such as giving feedback aligned with human preferences (Bai et al., 2022; Lee et al., 2023). This insight suggests that LLMs could potentially serve as proxy users for generating revision edits.\n- **User Experience and Utility:** The importance of user experience in LLM-based applications has been highlighted, yet there is a noticeable gap in evaluation methods that prioritize human-centric metrics (Chiang and Lee, 2023; Fu et al., 2023).\n\nBy addressing the limitations of existing metrics and incorporating insights from recent studies on human-like behaviors in LLMs, this paper contributes a new, human-centered evaluation metric to better align with end-user needs in LLM-powered writing assistants.",
        "methodology": "### Methodology\n\nThis study introduces a novel approach to evaluating text in LLMs-based applications by leveraging \"Revision Distance\" as a principal metric. The methodology centers on the use of Large Language Models (LLMs) fulfilling two concurrent functions: acting both as the proxy of the user (denoted as \\(\\mathcal{U}\\)) and the generator (\\(\\mathcal{G}\\)). The generator (\\(\\mathcal{G}\\)) is the core component of the writing assistant application.\n\n**Process Overview:**\n1. **Draft Generation (\\(D\\))**: Given the user input content, the generator (\\(\\mathcal{G}\\)) crafts a draft (\\(D\\)), which can be various forms of text such as emails, letters, articles, or \u201cRelated Work\u201d sections.\n2. **Edit Quantification (\\(E\\))**: The proxy of the user (\\(\\mathcal{U}\\)) quantifies the number of edits needed to modify \\(D\\).\n\n**Evaluation Metrics:**\n- **Reference-based Evaluation**: In this setting, the evaluation depends on a reference, using either human-written text or outputs from ChatGPT as the ground truth. The proxy (\\(\\mathcal{U}\\)) is programmed to generate structured revision edits to enhance the consistency of the draft \\(D\\) with the ground-truth text \\(T\\). \n\n**Reference-Less Scenario:**\n- **Ideal Form Refinement**: In absence of a ground truth text, the generator (\\(\\mathcal{G}\\)) is still tasked with refining the draft \\(D\\). The proxy (\\(\\mathcal{U}\\)) works towards an \"ideal form,\" which is an implicit standard not explicitly generated but invoked within the revision edits generation prompt.\n\n**Key Components and Innovations:**\n- **Dual Role of LLMs**: Utilizing LLMs as both user proxies and text generators to simulate a comprehensive text revision process.\n- **Structured Revision Edits**: Introducing a mechanism where \\(\\mathcal{U}\\) produces well-organized revision suggestions for draft improvement.\n- **Implicit Ideal Standard**: Operating on a conceptual idealized version of the text, providing a quality benchmark even in the absence of concrete ground-truth references.\n\nBy structuring the revision process closely to human writers, the model ensures that revisions not only reduce discrepancies with reference texts but also elevate the text to a high-quality standard, mimicking human refinement practices.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\nTo validate the utility of our proposed metric, we constructed two distinct datasets tailored to different writing tasks:\n\n1. **Easy-Writing Task**: This task includes the generation of emails, letters, and articles. These categories were chosen as the testbed for straightforward writing activities that typically require less complex reasoning.\n\n2. **Challenge-Writing Task**: This involves academic writing, which serves as a testbed for scenarios requiring heavy knowledge reasoning and complex conceptual understanding.\n\n### Datasets:\n- **Easy-Writing Task Dataset**: Comprises samples of emails, letters, and articles.\n- **Challenge-Writing Task Dataset**: Comprises samples of academic writing.\n\n### Evaluation Metrics:\n- For the purpose of validating our proposed metric, the experiments used the *Revision Distance* as a primary metric. This metric helps in assessing how much revision a generated text requires before it meets a certain standard of quality.\n  \n### Baselines:\n- Comparison with existing text evaluation metrics such as BLEU, ROUGE, and other model-centered evaluation benchmarks was made to highlight the effectiveness of the proposed Revision Distance metric.\n\n### Main Experimental Results:\n- The experimentation results indicate that the Revision Distance metric provides a more human-centered evaluation for both easy-writing and challenge-writing tasks.\n- The proposed metric demonstrated robustness in assessing the quality of generated texts, particularly in complex writing scenarios that demand knowledge reasoning and meticulous conceptual understanding.\n  \nThe details on the construction and specific characteristics of the test datasets can be found in Appendix A. \n\nThis setup and the results emphasize the merit of using Revision Distance as an effective metric for evaluating the quality of texts generated by LLMs (Large Language Models), particularly in tasks that vary significantly in complexity."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To assess the discriminative capacity of the 'Revision Distance' metric in distinguishing between strong and weak writing applications.",
            "experiment_process": "For the easy-writing task, two Mistral-series models were used as LLMs. For the challenge-writing task, GPT-4 and its variant 444 models were employed. The detailed model specifications can be found in Appendix B and Appendix C of the paper.",
            "result_discussion": "The 'Revision Distance' metric shows utility for both easy and challenging writing tasks. Unlike other metrics, smaller values indicate better text quality. The metric demonstrates enhanced efficacy in distinguishing between the nuanced capabilities of diverse models, with a larger change rate observed from the weak to strong model, highlighting its superior discriminative capacity. In human evaluations, it aligns well with human preferences, indicating superior text quality with fewer revisions for 'Chosen Texts,' whereas ROUGE often misaligns, assigning higher scores to 'Rejected Texts.'",
            "ablation_id": "2404.07108v2.No1"
        },
        {
            "research_objective": "To evaluate the effectiveness of the 'Revision Distance' metric in scenarios where ground truth (reference texts) is unavailable.",
            "experiment_process": "41 cases related to writing tasks were extracted from the UltraFeedback dataset, each containing a chosen response and a rejected response. The 'Revision Distance' metric was applied to these selected cases.",
            "result_discussion": "The 'Revision Distance' metric aligns with human judgments in 76% of instances, indicating that chosen responses typically necessitated fewer revisions. This suggests that the metric remains effective even in the absence of reference texts.",
            "ablation_id": "2404.07108v2.No2"
        }
    ]
}