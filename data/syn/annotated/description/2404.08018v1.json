{
    "title": "Analyzing the Performance of Large Language Models on Code Summarization",
    "abstract": "Large language models (LLMs) such as Llama 2 perform very well on tasks that involve both natural language and source code, particularly code summarization and code generation. We show that for the task of code summarization, the performance of these models on individual examples often depends on the amount of (subword) token overlap between the code and the corresponding reference natural language descriptions in the dataset. This token overlap arises because the reference descriptions in standard datasets (corresponding to docstrings in large code bases) are often highly similar to the names of the functions they describe. We also show that this token overlap occurs largely in the function names of the code and compare the relative performance of these models after removing function names versus removing code structure. We also show that using multiple evaluation metrics like BLEU and BERTScore gives us very little additional insight since these metrics are highly correlated with each other.\n\n\n\nKeywords:\u2009Natural Language Generation, Neural Language Representation Models, Summarisation",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "There is a growing interest in applying NLP techniques to tasks related to automated program understanding, generation, and retrieval, all of which promise to improve access to code. Popular tasks include Code Summarization (translating code into natural language, e.g.  Miceli Barone and Sennrich, 2017  ###reference_b24###), Code Generation, Code Completion, Code Translation and Natural Language Code Search (retrieving a code snippet given a natural language query, e.g. Gu et al., 2018a  ###reference_b10###). There is a practical need for such systems since the ability to automatically generate code snippets or doc strings or search large code bases can significantly increase the productivity of software developers. However, there is also a growing interest in developing models and datasets for these tasks within the NLP community, often driven by an assumption that code can be seen as a semantic interpretation of its natural language description.\nBut while current models show impressive performance, it is still important to analyze how much understanding these models have of the structure or semantics of the code. To make their code more human-readable, software developers often employ English words in the names of functions, variables, or data structures. And, although they did not evaluate the most recent web-scale large language models, the authors of MCoNaLa Wang et al. (2023  ###reference_b46###) showed that the performance of code generation models drops significantly compared to English if the input is in Spanish, Japanese, or Russian.\nWe, therefore, ask to what extent the large language models (LLMs) that are used for tasks like code generation or summarization actually understand the semantic relation between natural language and code, and to what extent they simply rely on this superficial token similarity.\nIn this paper, we attempt to address this question by analyzing the performance of large language models (LLMs) on code summarization. Our analysis aims to shed light on the following, more specific questions: (1) to what extent do the summaries generated by these models simply consist of tokens that are directly copied from the code? (2) How much does model performance depend on the presence of function names that give away the semantics of the code? (3) To what extent do these models rely on the syntactic structure and underlying logic of the code?\nTo answer the first research question, we split the examples from the dataset into different buckets depending on the token overlap between the code and the description and see how much the performance varies across those buckets. For the second and third research questions, we make several transformations to the code before feeding it to an LLM. This includes changing or obfuscating certain function names, and removing the control structures in the body of the code. We then examine their impact on code summarization performance. We observe the effect of the code in standard datasets like CodeXGLUE having informative function and identifier names with a high token overlap with their descriptions, and analyze how this affects model behavior. This token overlap between the function names and the target summary makes the task easier. Our experiments111The code for this paper will be released at https://github.com/rajarshihaldar/analyze-llm-code-summarization  ###reference_llm-code-summarization### show that the performance of several state-of-the-art LLMs is often due to the high string similarity of the natural language descriptions to the code they are paired with."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Background",
            "text": "CodeXGLUE Lu et al. (2021  ###reference_b23###) is a standard benchmark for several code-NL tasks including code summarization. It is a filtered subset of CodeSearchNet Husain et al. (2019  ###reference_b16###), consisting of code snippets paired with English descriptions that are scraped from public open-source GitHub repositories for Go, Java, JavaScript, PHP, Python, and Ruby. The description paired with each code snippet is the first paragraph of the repository. Examples were filtered out from this dataset if their code could not be parsed into an abstract syntax tree, or if their descriptions were not in English, contained special strings like \"http://\", were empty, or not between 3 and 256 tokens in length. Figure 1  ###reference_### shows an example from the dataset being used for code summarization. In our experiments, we use the Python examples from CodeXGLUE, which contain 251,820 training, 13,914 dev, and 14,918 testing data points.\nBLEU Papineni et al. (2002  ###reference_b28###) is computed by matching n-grams between generated summaries and human written summaries. A perfect match between the two summaries would give a score of 100%, and a perfect mismatch would give a score of zero. BLEU-4 is the standard metric for evaluating code summarization Shi et al. (2022  ###reference_b34###). In this case, n-grams are matched up to . This matching is done cumulatively: the BLEU scores for n-grams for values of n = 1 through 4 are computed, and then the mean of these four scores is computed to get the final score.\nIt was found that different implementations of BLEU-4 lead to different results Shi et al. (2022  ###reference_b34###). Out of these variants, the sentence BLEU metric based on NLTK Bird et al. (2009  ###reference_b4###) smoothing method 4 had the highest correlation with human evaluation. The scores we report are computed by the implementation of this method in NLTK 3.8.1."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1.   Large Language Models For Code",
            "text": "Applying natural language processing techniques to source code has produced outstanding results. Code2vec Alon et al. (2019  ###reference_b2###) showed that techniques that are used to induce semantic embeddings or vectors for natural language input also work well to represent input code snippets and predict their semantic properties. DeepCS Gu et al. (2018b  ###reference_b11###) showed that by mapping both code and natural language prompts to embeddings you could perform retrieval on code. Another breakthrough was the introduction of sequence-to-sequence (seq2seq) models for generating comments for a given input code Hu et al. (2018a  ###reference_b14###). These initial models treat code as a sequence of tokens and were quickly followed by models that account for the structure of the code through Abstract Syntax Trees (Zhang et al., 2019a  ###reference_b49###; Wan et al., 2019  ###reference_b41###; Haldar et al., 2020  ###reference_b13###), Graph Neural Networks (Sieper et al., 2020  ###reference_b36###; Ling et al., 2021  ###reference_b20###; Liu et al., 2021  ###reference_b21###), and Graph Attention Neural Networks Wang et al. (2022a  ###reference_b43###).\nInspired by the success of transformer-based Vaswani et al. (2017  ###reference_b40###), pre-trained large language models (LLMs) like BERT Devlin et al. (2019  ###reference_b7###), XLNet Yang et al. (2019  ###reference_b48###), GPT Brown et al. (2020  ###reference_b5###), RoBERTa Liu et al. (2019  ###reference_b22###), SynCoBERT Wang et al. (2021a  ###reference_b42###), and T5 Raffel et al. (2020  ###reference_b30###) on core NLP tasks, and facilitated by the availability of large datasets that pair natural language with code, e.g. CodeSearchNet Husain et al. (2019  ###reference_b16###), LLMs are now commonly used for tasks that involve source code and natural language. CodeBERT Feng et al. (2020  ###reference_b9###), and its successor GraphCodeBERT Guo et al. (2020  ###reference_b12###), were two of the initial wave of LLMs trained on paired natural (NL) and programming language (PL) sequences. Soon other LLMs like PLBART Ahmad et al. (2021  ###reference_b1###) and CodeT5 Wang et al. (2021b  ###reference_b44###) made significant gains in code summarization. At the same time, models like CodeGen Nijkamp et al. (2023  ###reference_b26###) and Codex Chen et al. (2021  ###reference_b6###) made gains in code generation.\nThe current state of the art includes even larger instruction-tuned models that can perform a wide variety of tasks including ones related to code-NL, e.g. GPT-4 OpenAI (2023  ###reference_b27###), Llama 2 Touvron et al. (2023  ###reference_b39###) and PaLM 2 Anil et al. (2023  ###reference_b3###). Instead of being fine-tuned for a specific task, these models are trained to respond to a prompt describing a task followed by an input. This leads to improved performance on a variety of code-NL tasks. While GPT-4 has the best performance222Although it is unclear what specific data GPT-4 has been trained on, their training data is likely to include the code snippets contained in the datasets used in this paper and data contamination is known to be a significant factor in the performance of these models on coding tasks Narayanan and Kapoor (2023  ###reference_b25###).\nMoreover, GPT-4 is frequently updated behind the scenes based on the data customers provide. This makes any analyses on this data non-reproducible. Due to these concerns, we have not included GPT-4 in our studies., some researchers have found ways to improve its output further. For example, leveraging verbal reinforcement on an LLM improves its code generation capabilities Shinn et al. (2023  ###reference_b35###).\nAlthough current LLMs differ in important details (size, amount of training data, pre-training regime, fine-tuning, etc.), they use a form of subword tokenization Kudo and Richardson (2018  ###reference_b18###); Sennrich et al. (2016  ###reference_b33###), in which words are split into shorter strings (e.g. extracts  ext ract s) and underscores are treated as separators (e.g. from_url  from _ url), such that the model\u2019s vocabulary consists of subwords rather than whitespace-delineated tokens. Figure 2  ###reference_### shows how even just a function definition reveals valuable information about the tokens that are likely to occur in the reference description when it is tokenized by a subword tokenizer (here, Llama 2)."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2.   Code Summarization",
            "text": "Code Summarization systems translate code snippets into automatically generated English summaries that describe what the code does. Given an input code snippet , the system has to return a description  that accurately describes what that code does. Figure 1  ###reference_### shows an example of code summarization being performed.\nWhen the task was first introduced, template-based approaches Sridhara et al. (2010  ###reference_b38###) requiring expert domain knowledge were used, followed by information retrieval-based approaches Eddy et al. (2013  ###reference_b8###); Rodeghero et al. (2014  ###reference_b31###). In IR-based approaches, the model would extract the most relevant tokens from the code to generate a term-based summary. This did not require any domain knowledge but the models used did not show any deep understanding of the code structure and instead relied on informative function names and comments to generate summaries. This was followed by early neural approaches Iyer et al. (2016  ###reference_b17###), Encoder-decoder frameworks like Hu et al. (2018a  ###reference_b14###) and transformer-based approaches, like PLBART Ahmad et al. (2021  ###reference_b1###), Structure-induced Transformers Wu et al. (2021  ###reference_b47###), CoTexT Phan et al. (2021  ###reference_b29###) CodeGPT Lu et al. (2021  ###reference_b23###), Codex Chen et al. (2021  ###reference_b6###), and CodeT5 Wang et al. (2021b  ###reference_b44###). Recently, these models have been surpassed by large language models using decoder-only architectures like PaLM 2 Anil et al. (2023  ###reference_b3###), GPT-4 OpenAI (2023  ###reference_b27###), and Llama 2 Touvron et al. (2023  ###reference_b39###).\nBenchmark datasets for this task include TL-CodeSum Hu et al. (2018b  ###reference_b15###), Funcom LeClair et al. (2019  ###reference_b19###), CodeSearchNet Husain et al. (2019  ###reference_b16###) and more recently, CodeXGlUE Lu et al. (2021  ###reference_b23###).\n###figure_1### CodeXGLUE Lu et al. (2021  ###reference_b23###  ###reference_b23###) is a standard benchmark for several code-NL tasks including code summarization. It is a filtered subset of CodeSearchNet Husain et al. (2019  ###reference_b16###  ###reference_b16###), consisting of code snippets paired with English descriptions that are scraped from public open-source GitHub repositories for Go, Java, JavaScript, PHP, Python, and Ruby. The description paired with each code snippet is the first paragraph of the repository. Examples were filtered out from this dataset if their code could not be parsed into an abstract syntax tree, or if their descriptions were not in English, contained special strings like \"http://\", were empty, or not between 3 and 256 tokens in length. Figure 1  ###reference_###  ###reference_### shows an example from the dataset being used for code summarization. In our experiments, we use the Python examples from CodeXGLUE, which contain 251,820 training, 13,914 dev, and 14,918 testing data points.\nBLEU Papineni et al. (2002  ###reference_b28###  ###reference_b28###) is computed by matching n-grams between generated summaries and human written summaries. A perfect match between the two summaries would give a score of 100%, and a perfect mismatch would give a score of zero. BLEU-4 is the standard metric for evaluating code summarization Shi et al. (2022  ###reference_b34###  ###reference_b34###). In this case, n-grams are matched up to . This matching is done cumulatively: the BLEU scores for n-grams for values of n = 1 through 4 are computed, and then the mean of these four scores is computed to get the final score.\nIt was found that different implementations of BLEU-4 lead to different results Shi et al. (2022  ###reference_b34###  ###reference_b34###). Out of these variants, the sentence BLEU metric based on NLTK Bird et al. (2009  ###reference_b4###  ###reference_b4###) smoothing method 4 had the highest correlation with human evaluation. The scores we report are computed by the implementation of this method in NLTK 3.8.1."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   How well do LLMs perform on Code Summarization?",
            "text": "###figure_2### We now show how some recent LLMs perform on the standard code summarization dataset, CodeXGLUE. In this section, we study the extent to which the generated English summaries contain tokens that are copied from the subword-tokenized code. Following this, we examine how prevalent this token overlap is across the entire dataset. We further investigate whether the models also follow the trend of generating summaries with high token overlap with the input code and whether the BLEU scores of the generations are impacted by this overlap. Finally, we inspect which types of tokens are more likely to be involved in this overlap, since the tokens present in function names give away more information about the function than the tokens in the body of the function.\nAfter determining the prevalence of high token overlap in the dataset and its impact on generation performance, we examine how much LLMs prioritize function names over the code structure and its control flow. We also evaluate how they perform when given code that has all the control structure removed, and in an extreme case if we only give the function definition as input. This will also help us answer our first two research questions about how much models leverage function names over the body of the code and how much models care about the code syntax and structure. Additionally, studying the relative performance of multiple LLMs of different parameter counts on these different types of input will give us a deeper insight into what they understand about code. While there is prior work showing how transformations like masking function names make code summarization harder Sontakke et al. (2022  ###reference_b37###), our work goes further and performs a deeper qualitative analysis to see what causes the model to perform well in the first place."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   Experimental Setup",
            "text": "To examine a wide range of model sizes, we analyze CodeT5 (220M parameters), PaLM 2 (340B parameters), and Llama 2 (with 7B and 70B parameters):\nCodeT5: CodeT5 Wang et al. (2021b  ###reference_b44###) is a pre-trained encoder-decoder model that uses objectives during pre-training like Identifier Tagging (the model has to predict whether each token in the input is an identifier or not) which make it more suitable for understanding code. It is pre-trained on CodeSearchNet. It is adapted to multiple downstream tasks including code summarization and code generation through multi-task learning.\nPaLM 2: PaLM 2 Anil et al. (2023  ###reference_b3###) is an LLM made by Google AI for reasoning tasks, question answering, classification, translation, and code summarization. It was pre-trained on a large corpus of parallel multilingual text. As it is proficient in sequence-to-sequence tasks, we can make it perform code summarization with the right prompt.\nLlama 2: Llama 2 Touvron et al. (2023  ###reference_b39###) is an LLM designed by Meta AI that can also be used for sequence-to-sequence tasks, including an instruct model that can be asked to perform any task with a prompt. It comes in three sizes - 7B, 13B and 70B.\nWhile CodeT5 is fine-tuned on this dataset, we use the other models PaLM 2 and Llama 2 in inference-only mode with few-shot prompting. We use the following prompt before each input code snippet - \"Pretend that you are a programmer writing Python functions. For a given Python function you have to generate a short documentation describing what the function does.\", along with ten examples from the dataset to show what kind of description we are looking for."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   Overall Performance",
            "text": "To analyze these models, we first evaluate them on code summarization on the standard CodeXGLUE dataset. Table 1  ###reference_### shows that the BLEU-4 scores of the models discussed above range from 17.66 for the 220M-parameter model CodeT5 to 22.41 for the 70B-parameter Llama 2.\nThe much smaller 7b parameter version of Llama 2 performs similarly at 22.36, while PalM 2 (340B parameters) achieves a score of 19.23, between CodeT5 and Llama 2."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "3.3.   Exploring if LLMs are copying tokens from code to description",
            "text": "We now analyze if these models are copying tokens from the code when generating their summaries. While copying in itself would not be a design flaw, if there is an instance of widespread copying, this would indicate that it is possible to perform well in this task without showing much understanding of the semantics of the code. We define our own metric called , which is the percentage of tokens (as generated by the corresponding model\u2019s tokenizer) in the description that was also present in the code.\n###figure_3### ###figure_4### Figure 3  ###reference_### shows the distribution of  in the reference descriptions written by human developers (bottom) and in the summaries produced by our two models (top), CodeT5 and Llama 2. Although copying is present in the human descriptions in both models (allowing the model to pick up on it during training), the models rely on it to a greater extent, since  skews higher in the generated summaries. CodeT5 generates descriptions with much higher copying than Llama 2, but we see that even in the reference descriptions CodeT5 tokenizer gives much higher overlap than Llama 2. Therefore, it is not just the model but also the tokenizer that greatly influences the extent to which this copying strategy is employed."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "3.4.   Is copying tokens a viable strategy for summarization?",
            "text": "We see that the models learn to generate summaries by copying tokens from the code, but how effective is that strategy?\nFor examples with high  in the reference description, a copying strategy should yield higher BLEU scores than for examples with lower . To see if this is the case, we split the test set into buckets based on the reference descriptions\u2019 , and plot the distribution of the model\u2019s BLEU-4 scores in each bucket (Figure 4  ###reference_###). Figure 3(a)  ###reference_sf1### shows the results on the smallest model we have tested, CodeT5, and Figure 3(b)  ###reference_sf2### shows the results on Llama 2 (70b).\n###figure_5### ###figure_6### The first bar of the two plots in Figure 4  ###reference_### shows the overall distribution of BLEU-4 scores. The subsequent bars show the distribution of scores for a certain  range. For example, the second bar shows the scores for examples where 0 to 10% of the tokens in the reference description are present in the code. We see that in general as  increases, the BLEU-4 scores also increase. However, this correlation is less pronounced for Llama 2. This shows that both models tend to fall back on copying tokens from code to the description as a viable strategy, with the most difficult examples in the dataset being the ones where  is zero, as in this bucket, half have a zero BLEU score."
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "3.5.   Which tokens are copied?",
            "text": "We see that copying tokens from the code when generating descriptions can be a viable strategy. However, we need to explore which types of tokens are most likely to be copied. If it is mostly the function names that are being copied, that suggests the model relies most on the function definition and largely ignores the semantics of the code. We also want to see what kind of tokens get copied from the code in the reference descriptions in the dataset.\n###figure_7### ###figure_8### In Figure 5  ###reference_### we see the distribution of different token types like function names, identifiers, and comments for both CodeT5 and Llama 2 (70b). One main reason for the differences in their distributions, even in the reference descriptions, is the fact that they use different tokenizers. For example, the function names form a small part of the code snippets after tokenization by CodeT5 as seen in the first column in Figure 4(a)  ###reference_sf1###. On the other hand, the Llama 2 tokenizer (Figure 4(b)  ###reference_sf2###) allows for function names to be a much bigger part of the code. This pattern also holds true for the reference description in the dataset, with function names being more represented by the Llama 2 tokenizer. However, when we look at the generated descriptions, we see that CodeT5-generated summaries have a much higher presence of function names than Llama 2. This is probably due to fine-tuning, and CodeT5 learning that the optimal strategy for generating good summaries is to not only copy tokens from the code but to copy tokens from the function definition of the code. On the other hand, Llama 2 seems to prefer to rely more on identifiers, which can also contain useful information about the code semantics. Llama 2 also seems to better understand that the keywords in the code, while important to understand the semantics, are not supposed to be in the generated description."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   How do Code Transformations Affect Performance?",
            "text": "We saw that the performance of LLMs on code summarization benefits from the high token overlap between the code and the reference description, and this overlap occurs primarily due to informative function names. To see how well they work when this information is unavailable, and also how strongly they rely on the internal structure of the code instead, we make several transformations to the code in the dataset. We then evaluate the performance of multiple LLMs under these scenarios.\nIn each transformation, we modify one aspect of the dataset used for training and evaluation of the classifier to understand what effect it has on performance. For example, the change in performance when we remove function names will show us how important function names are to the model\u2019s understanding of the code.\nOn the transformed variants, the performance of CodeT5 improves significantly after fine-tuning on examples from those variants, especially in Obfuscated Function Names. Not all variants have the same improvement, though. In Adversarial Function Names, the improvement is much smaller, suggesting that the presence of incorrect function names still misleads the model.\nWe see that there is a drop in Obfuscated Function Names and an even bigger drop in Adversarial Function Names, suggesting that models struggle when the function name is hidden and are misled when given an incorrect function name, showing how important function names are to the models\u2019 understanding of the code semantics. However, the drop from Obfuscated to Adversarial is much higher for CodeT5 and PaLM 2 than Llama 2, showing that these models are more dependent on the function names. Section 3.5  ###reference_### will show that this may be explained by the fact that CodeT5 is much more dependent on the function names than Llama 2.\nPaLM 2 and Llama 2 perform worse in No Function Body and No Code Structure compared to the other variants, whereas CodeT5 performs better. This shows that CodeT5 does not mind when given code that is incomplete or syntactically incorrect, unlike the larger models. However, despite performing well on these particular variants, CodeT5 is worse overall than Llama 2, showing that fine-tuning for higher performance on one narrow metric may not always give the best result.\nNo Function Body and No Code Structure have incomplete and syntactically incorrect code respectively. CodeT5 performs better at these variants than the ones where we just modify the function names. However, the other models perform much worse here. This shows that the larger variants care more about code syntax whereas a smaller model like CodeT5 looks mostly at function names. In fact, the largest model in our experiments, PaLM 2, performs the worst at these variants.\nWhile the bigger models like Llama 2 and PaLM 2 perform better on this variant, they are still prone to be misled. We have shown some examples where it is obvious that the function name is wrong but the models still prioritize that over the semantics in the code. Figure 7  ###reference_### shows an example where both PaLM 2 and Llama 2 (70b) falter when the function name is changed."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1.   Code Transformations",
            "text": "In each transformation, we modify one aspect of the dataset used for training and evaluation of the classifier to verify if it improves the performance."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2.   Performance on Transformed Code",
            "text": "Table 2  ###reference_### shows the BLEU scores of CodeT5, PaLM 2, and Llama 2 (with 7b and 70b parameters) on the different variants,\nThe first CodeT5 column shows the scores when the model is only fine-tuned on examples from the original dataset, whereas the second column (FT) shows the scores when the model is fine-tuned on the transformed examples so it knows what kind of code to expect as input.\n###figure_14### On the transformed variants, the performance of CodeT5 improves significantly after fine-tuning on examples from those variants, especially in Obfuscated Function Names. Not all variants have the same improvement, though. In Adversarial Function Names, the improvement is much smaller, suggesting that the presence of incorrect function names still misleads the model.\nWe see that there is a drop in Obfuscated Function Names and an even bigger drop in Adversarial Function Names, suggesting that models struggle when the function name is hidden and are misled when given an incorrect function name, showing how important function names are to the models\u2019 understanding of the code semantics. However, the drop from Obfuscated to Adversarial is much higher for CodeT5 and PaLM 2 than Llama 2, showing that these models are more dependent on the function names. Section 3.5  ###reference_###  ###reference_### will show that this may be explained by the fact that CodeT5 is much more dependent on the function names than Llama 2.\nPaLM 2 and Llama 2 perform worse in No Function Body and No Code Structure compared to the other variants, whereas CodeT5 performs better. This shows that CodeT5 does not mind when given code that is incomplete or syntactically incorrect, unlike the larger models. However, despite performing well on these particular variants, CodeT5 is worse overall than Llama 2, showing that fine-tuning for higher performance on one narrow metric may not always give the best result.\nNo Function Body and No Code Structure have incomplete and syntactically incorrect code respectively. CodeT5 performs better at these variants than the ones where we just modify the function names. However, the other models perform much worse here. This shows that the larger variants care more about code syntax whereas a smaller model like CodeT5 looks mostly at function names. In fact, the largest model in our experiments, PaLM 2, performs the worst at these variants.\nWhile the bigger models like Llama 2 and PaLM 2 perform better on this variant, they are still prone to be misled. We have shown some examples where it is obvious that the function name is wrong but the models still prioritize that over the semantics in the code. Figure 7  ###reference_###  ###reference_### shows an example where both PaLM 2 and Llama 2 (70b) falter when the function name is changed."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Investigating An Alternative Metric",
            "text": "Although BLEU is a commonly used and well-established metric, it is very strict because it captures only exact matches between n-grams. To address this, a number of metrics have been recently developed that instead compute the similarity of token embeddings returned by a neural model. We investigate whether using such a metric changes the main findings from our experiments. We consider BERTScore, which uses BERT Devlin et al. (2019  ###reference_b7###) as the neural model.\nBERTScore Zhang et al. (2019b  ###reference_b50###) is a precision/recall-inspired metric that uses a BERT model to compute token similarities between two summaries. To compute BERTScore, both summaries are given as input to the BERT model (we use the base uncased model of DistilBERT Sanh et al. (2020  ###reference_b32###)) to get their token embeddings (say  for reference and  for generated).\nThen recall () and precision () over these embedding sequences are computed by considering the maximal similarity of each reference (candidate) token to any candidate (reference) token:\nThe final BERTScore is the F1 score (harmonic mean ) of  and :"
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "5.1.   Overall Performance (BERTScore)",
            "text": "Table 3  ###reference_### shows the BERTScores for each model on the original test set. This correlates very strongly with the results in Table 2  ###reference_###, with a Pearson Correlation of 0.87 and a Spearman\u2019s Rank Correlation of 0.86 between the 25 pairs of scores.\nThere is also a high correlation of BLEU and BERTScores between every pair of reference and generated descriptions we have in the original dataset across all models. We get a Pearson Correlation of 0.78 and a Spearman\u2019s Rank Correlation of 0.73. The heatmap in Figure 8  ###reference_### shows this positive correlation.\n###figure_15### However, we observe that BERTScores are overall much higher and closer to each other here than was the case for BLEU scores. This is possibly due to BERTScore being a more forgiving metric, and most generations are assigned a very high BERTScore despite their quality. For example, the minimum BERTScore assigned to an example is 60, whereas there are several cases that have a BLEU score of zero.\nRegardless of the metric used, performance generally increases with increasing the number of parameters, except Llama 2 outperforming PaLM 2. However, this increase in performance is not linear, with Llama 2(7b) being quite close to Llama 2(70b). As for variants, all models see a drop in performance in Obfuscated Function Names and an even bigger drop in Adversarial, though the second drop is less in Llama 2. And while Llama 2 and PaLM 2 justifiably underperform when given incomplete or incorrect code in No Function Body and No Code Structure, CodeT5 performance takes a much smaller hit, especially after fine-tuning."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "5.2.   Distribution of BERTScores",
            "text": "In Figure 9  ###reference_###, we plot the distribution of BERTScores of all the model outputs on the original test data. The orange curve is the distribution of BERTScores between random pairs of reference descriptions and generated descriptions, and the blue curve shows the distribution of BERTScores between the reference descriptions and their corresponding descriptions.\n###figure_16### We see that the median BERTScore even between two unrelated descriptions is high, around 75.03. Therefore, even incorrect summaries are assigned very high BERTScores, and the threshold for good summaries is much higher than that. The median BERTScore between the reference descriptions and their corresponding generation as returned by all models is 84.33, which is more than 9 points higher, showing that this metric still shows discernment between related and unrelated descriptions. In Appendix A.1  ###reference_###, we see that BLEU scores can discern between correct and incorrect descriptions as well as assign low scores (mostly zero) to randomly sampled generated descriptions.\nAnother interesting observation about BERTScores is that the average BERTScore is higher between two random generated descriptions than two random reference descriptions in the dataset, as seen in Figure 10  ###reference_###. This tells us that the model generates descriptions with less diversity than that present in the dataset. In Appendix A.1  ###reference_###, we see this is also true for BLEU scores.\n###figure_17###"
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Conclusion",
            "text": "This paper presented a series of experiments to gain a deeper insight into what makes current LLMs effective at code summarization. Section 4  ###reference_### suggests that LLMs often rely on function names and on shared tokens between the code and the description compared to the code structure to perform well. Relying on token overlap seems to work well because, at least in the standard datasets for these tasks, code and the corresponding descriptions often have high token overlap. Our experiments establish a clear trend between the token overlap of the code and descriptions and summarization performance.\nWe expect other LLM-based models, like PLBART (Ahmad et al., 2021  ###reference_b1###), and CoText (Phan et al., 2021  ###reference_b29###) to show similar behavior since we believe our results point to a feature of this entire class of models.\nThe current state of the art also falls short when the description is in a language other than English. Wang et al. (2022b  ###reference_b45###) released a benchmark MCoNaLa, which contains a parallel corpus of code paired with multiple languages, and showed that current code generation models perform poorly for languages like Spanish, Japanese, and Russian. This may be because there are fewer tokens in the description that overlap with the code, so a model cannot learn to take advantage of informative function names and identifier names.\nFinally, we also believe that human evaluation should be used in conjunction with building more comprehensive evaluation methodologies for code summarization, in order to measure the accuracy and the usefulness of a generated description, something that similarity-based metrics like BLEU and BERTScore cannot capture. While there has been work in that direction Shi et al. (2022  ###reference_b34###), the current practice still relies on comparing generated descriptions with a reference instead of measuring their actual usefulness to the user."
        }
    ],
    "url": "http://arxiv.org/html/2404.08018v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "3.1",
            "4.1"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "3.2",
            "3.3",
            "3.4",
            "3.5",
            "4",
            "4.2",
            "5",
            "5.1",
            "5.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "1",
            "3",
            "3.1",
            "3.3",
            "3.4",
            "3.5",
            "4",
            "4.1",
            "4.2"
        ]
    },
    "research_context": {
        "paper_id": "2404.08018v1",
        "paper_title": "Analyzing the Performance of Large Language Models on Code Summarization",
        "research_background": "**Paper's Motivation:**\nThe motivation behind this study stems from the rising interest in deploying NLP techniques for various tasks related to automated program understanding. Among these tasks, code summarization, which focuses on translating code into natural language, holds significant practical value by potentially enhancing software developers' productivity. The study is driven by the need to evaluate whether the impressive performance of current large language models (LLMs) on such tasks truly reflects an understanding of code structure and semantics or is primarily dependent on superficial token similarities between code and its descriptions.\n\n**Research Problem:**\nThe central research problem addressed in this paper revolves around understanding the extent to which large language models (LLMs) utilized for code summarization comprehend the semantic relationship between code and natural language. Specifically, it seeks to determine whether the performance of these LLMs is primarily due to the inherent token similarity between code elements (such as function and variable names) and their natural language descriptions, or if these models genuinely understand the syntactic and logical structure of the code.\n\nTo break it down further, the research problem is explored through the following specific questions:\n1. Do the summaries generated by LLMs mainly consist of tokens directly copied from the code?\n2. How dependent is the model performance on the presence of informative function names that reveal code semantics?\n3. To what degree do these models rely on the syntactic structure and logic underlying the code?\n\n**Relevant Prior Work:**\nThe paper builds upon prior research in several areas:\n- **Code Summarization**: Previous work such as the study by Miceli Barone and Sennrich (2017) explored the task of translating code into natural language. \n- **Natural Language Code Search**: Research by Gu et al. (2018a) focused on retrieving code snippets based on natural language queries.\n- **MCoNaLa Dataset**: The performance of code generation models in different languages was studied by Wang et al. (2023), who highlighted a drop in performance when translating code from English to other languages like Spanish, Japanese, or Russian.\n\nThe paper seeks to address gaps in these previous studies by emphasizing the need for a more nuanced analysis of model performance, particularly in understanding whether LLMs rely on token overlap between code and natural language descriptions or grasp the deeper semantics and structure of the code. The methodology involves altering code features such as function names and control structures to test their impact on summarization accuracy, thus providing insights into the actual capabilities of LLMs.",
        "methodology": "Methodology: To evaluate the performance of different sizes of large language models on the task of code summarization, we examined CodeT5, PaLM 2, and Llama 2, each model varying significantly in parameters and foundational design. \n\n**1. CodeT5 (220M parameters):**\n- **Description:** CodeT5 is a pre-trained encoder-decoder model.\n- **Key Components and Innovations:**\n  - **Identifier Tagging:** During pre-training, the model uses Identifier Tagging objectives, which involves predicting whether each token in the input is an identifier or not. This objective enhances the model's capability in understanding code semantics.\n  - **Pre-training Data:** It is pre-trained on the CodeSearchNet dataset.\n  - **Adaptation:** CodeT5 is adapted to a variety of downstream tasks such as code summarization and code generation through multi-task learning techniques.\n- **Fine-tuning:** CodeT5 is fine-tuned specifically on the given dataset to optimize its performance for code summarization.\n\n**2. PaLM 2 (340B parameters):**\n- **Description:** PaLM 2, developed by Google AI, is a large language model known for its efficiency in tasks such as reasoning, question answering, classification, translation, as well as code summarization.\n- **Key Components and Innovations:**\n  - **Pre-training Corpus:** PaLM 2 was trained on a vast corpus of parallel multilingual texts, making it adept in sequence-to-sequence tasks.\n  - **Prompt Engineering:** For code summarization, PaLM 2 can be effectively prompted to deliver accurate results.\n- **Usage Mode:** In this analysis, PaLM 2 is used in inference-only mode with few-shot prompting.\n\n**3. Llama 2 (7B and 70B parameters):**\n- **Description:** Llama 2 is a large language model designed by Meta AI, capable of performing various sequence-to-sequence tasks.\n- **Key Components and Innovations:**\n  - **Model Sizes:** Llama 2 is available in multiple sizes: 7B, 13B, and 70B parameters.\n  - **Instruction Model:** It includes an instruction model capable of performing tasks based on detailed prompts.\n- **Usage Mode:** Similar to PaLM 2, Llama 2 is employed in inference-only mode with few-shot prompting for this evaluation.\n\n**General Approach for PaLM 2 and Llama 2:**\n- **Prompt:** A consistent prompt is used for both PaLM 2 and Llama 2 before processing each input code snippet. The prompt is: \"Pretend that you are a programmer writing Python functions. For a given Python function you have to generate a short documentation describing what the function does.\"\n- **Few-shot Prompting:** Along with this prompt, ten examples from the dataset are provided to illustrate the desired form of the description.\n\nBy leveraging their distinct architectures and tuning methods, this methodology aims to comprehensively assess the capability of each model in generating accurate and concise summaries of code functions.",
        "main_experiment_and_results": "**Main Experiment Setup and Results:**\n\n### Datasets:\n- **CodeXGLUE:** The primary dataset used for the code summarization task is CodeXGLUE. This dataset is used to evaluate how well recent Large Language Models (LLMs) perform when tasked with generating English summaries from source code.\n\n### Baselines:\n- The paper does not explicitly mention specific baseline models by name in the provided text, but it is implied that multiple recent Large Language Models (LLMs) of differing parameter counts are compared.\n\n### Evaluation Metrics:\n1. **Token Overlap Analysis:**\n   - The extent to which generated summaries contain tokens copied from the subword-tokenized code.\n   - The prevalence of token overlap across the entire dataset.\n   - The types of tokens involved in this overlap, with a focus on whether these are more likely to be found in function names versus the function body.\n\n2. **Performance Metrics:**\n   - **BLEU Scores:** Used as a quantitative metric to evaluate the quality of the generated summaries, especially in relation to high token overlap between the summaries and the input code.\n\n### Experimental Processes:\n1. **Token Overlap and BLEU Scores:**\n   - The models\u2019 summaries are analyzed for token overlap with the input code to see if there is a trend in generating summaries with high token overlap and if this affects BLEU scores.\n\n### Main Results:\n- **Token Overlap:**\n  - There is a significant prevalence of high token overlap in the generated summaries across the dataset.\n  - BLEU scores are impacted by this token overlap, indicating that the generation performance is closely tied to the presence of common tokens between the input code and the generated summaries.\n\n- **Model Insights:**\n  - The study offers insights into how recent LLMs handle code summarization tasks, highlighting the nuanced understanding (or the lack thereof) these models have of code representation beyond mere token overlaps and the syntax.\n\nThe overall findings suggest that while LLMs show proficiency in generating code summaries with high BLEU scores, there is a notable dependency on token overlap and function names, which raises questions about the depth of their understanding of code semantics and structure."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To analyze the performance of large language models (LLMs) on code summarization and determine the extent to which they rely on token similarity between code and descriptions versus understanding the code's semantics.",
            "experiment_process": "The study investigates the generated summaries by LLMs and their reliance on copying tokens from the code. CodeXGLUE is used as the dataset, and three LLMs (CodeT5, PaLM 2, and Llama 2) are evaluated. The analysis involves splitting the dataset into buckets based on the token overlap between the code and the reference descriptions and assessing model performance across these buckets. The models are further tested with code transformations such as obfuscating or replacing function names and removing control structures. The setup includes fine-tuning CodeT5 on the original and transformed datasets, while PaLM 2 and Llama 2 are evaluated in inference-only mode using few-shot prompting.",
            "result_discussion": "The results indicate that LLMs often rely on copying tokens from the code to generate summaries. CodeT5, in particular, shows a higher tendency to copy function names, while Llama 2 tends to rely more on identifiers. The performance on transformed code reveals that models struggle when function names are obfuscated or replaced, with CodeT5 being more dependent on function names than Llama 2. Larger models like PaLM 2 and Llama 2 perform worse on variants with incomplete or incorrect code syntax, whereas CodeT5 performs better in these scenarios but is overall less effective than Llama 2.",
            "ablation_id": "2404.08018v1.No1"
        },
        {
            "research_objective": "To investigate how different code transformations affect the performance of LLMs and understand the importance of various code components on model performance.",
            "experiment_process": "The study applies several code transformations to evaluate LLM performance under different conditions. Code transformations include obfuscating function names, replacing function names with those of other functions, removing control structures, and providing only function definitions without code bodies. The performance of CodeT5, PaLM 2, and Llama 2 on these transformed variants is assessed using BLEU scores.",
            "result_discussion": "CodeT5 shows significant improvement when fine-tuned on transformed variants, especially with obfuscated function names. However, it struggles with adversarial function names. PaLM 2 and Llama 2 perform poorly on code with no function body or code structure, highlighting their reliance on proper syntax. Despite CodeT5's better performance on incomplete code, Llama 2 outperforms it overall. The results suggest that fine-tuning on specific variants can improve performance but may not yield the best overall results.",
            "ablation_id": "2404.08018v1.No2"
        }
    ]
}