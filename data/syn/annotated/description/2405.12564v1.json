{
    "title": "ProtT3: Protein-to-Text Generation for Text-based Protein Understanding",
    "abstract": "Language Models (LMs) excel in understanding textual descriptions of proteins, as evident in biomedical question-answering tasks. However, their capability falters with raw protein data, such as amino acid sequences, due to a deficit in pretraining on such data. Conversely, Protein Language Models (PLMs) can understand and convert protein data into high-quality representations, but struggle to process texts.\nTo address their limitations, we introduce ProtT3, a framework for Protein-to-Text Generation for Text-based Protein Understanding.\nProtT3 empowers an LM to understand protein sequences of amino acids by incorporating a PLM as its protein understanding module, enabling effective protein-to-text generation.\nThis collaboration between PLM and LM is facilitated by a cross-modal projector (i.e., Q-Former) that bridges the modality gap between the PLM\u2019s representation space and the LM\u2019s input space.\nUnlike previous studies focusing on protein property prediction and protein-text retrieval, we delve into the largely unexplored field of protein-to-text generation. To facilitate comprehensive benchmarks and promote future research, we establish quantitative evaluations for protein-text modeling tasks, including protein captioning, protein question-answering, and protein-text retrieval. Our experiments show that ProtT3 substantially surpasses current baselines, with ablation studies further highlighting the efficacy of its core components. Our code is available at https://github.com/acharkq/ProtT3.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Language Models (LMs) have achieved impressive successes across diverse domains Brown et al. (2020  ###reference_b7###); Touvron et al. (2023  ###reference_b57###); Liu et al. (2023c  ###reference_b40###). Remarkably, the extensive biological literature in their training data has enabled LMs to excel in text-based protein understanding tasks, such as biological and medical question-answering (QA) Taylor et al. (2022  ###reference_b56###); OpenAI (2023  ###reference_b48###). These results show the potential of using LMs as the natural language interface for biomedical tasks. It further accentuates the importance of harnessing LMs to drive advancements in areas like drug discovery and protein property prediction Kim et al. (2021  ###reference_b28###); Fan et al. (2023  ###reference_b17###).\n###figure_1### ###figure_2### ###figure_3### Here we focus on LM\u2019s capability in text-based protein understanding, which enables textual interpretations of proteins. It can be assessed through diverse downstream tasks, particularly protein-to-text generation Guo et al. (2023  ###reference_b22###) and retrieval Xu et al. (2023  ###reference_b60###). Specifically, the retrieval task is finding an existing text description best matching a particular protein, while the generation task is delineated into two problems, as illustrated in Figure 1  ###reference_###:\n1) protein captioning, where the LM generates a description of a specific protein\u2019s functions, and 2) protein QA, where the LM answers questions about a protein.\nTable 1  ###reference_### overviews prior efforts Liu et al. (2023b  ###reference_b39###); Xu et al. (2023  ###reference_b60###); Taylor et al. (2022  ###reference_b56###); Guo et al. (2023  ###reference_b22###) in this field. Delving into these previous studies, we identify two research gaps:\nLack of Exploration for Protein-to-Text Generation.\nProtein-to-text generation is a conditional generation task Keskar et al. (2019  ###reference_b27###), requiring the LM to perceive proteins as the generation condition.\nPrevious studies Xu et al. (2023  ###reference_b60###); Liu et al. (2023b  ###reference_b39###) based on cross-modal contrastive learning Radford et al. (2021  ###reference_b50###) hardly interpret proteins as the direct inputs to the LM.\nWhile notably two studies, Galactica and ProteinChat, have paid certain explorations, they unfortunately appear to be quite constrained by key limitations.\nSpecifically, Galactica Taylor et al. (2022  ###reference_b56###) incorporates only a limited set of protein sequences in its pretraining data, thereby potentially restricting its capacity for comprehensive protein understanding;\nProteinChat Guo et al. (2023  ###reference_b22###) seeks to project protein representations to text space by training a linear projector, which might prove inadequate in capturing the intricate relations between proteins and texts.\nMissing Quantitative Evaluation. The progress in protein-text modeling is difficult to track without proper benchmarks. As Table 1  ###reference_### illustrates, the quantitative evaluations for these models are mostly missing, posing a challenge to further advancement in this field.\nTo bridge these research gaps, we propose ProtT3: Protein-to-Text Generation for Text-based Protein Understanding. As Figure 2(a)  ###reference_sf1### illustrates, ProtT3 empowers an LM to understand protein sequences of amino acids by incorporating a Protein Language Model (PLM) as its protein understanding module, thereby effectively conditioning the protein-to-text generation process. PLMs Madani et al. (2023  ###reference_b44###) are specialized LMs pretrained solely on protein sequences. They can generate powerful protein representations that are instrumental in presenting the proteins\u2019 3D structures and indicating their potential properties Chen et al. (2023  ###reference_b9###). To enable the LM to understand the PLM\u2019s protein representations, ProtT3 integrates an expressive cross-modal projector \u2013 Q-Former Li et al. (2023a  ###reference_b29###) \u2013 to map protein representations into the text space of the LM. This design enables the LM to consume proteins as inputs. However, working with a large LM with billions of parameters raises a new challenge of maintaining the efficiency of downstream adaptation. Therefore, we incorporate a LoRA Hu et al. (2022  ###reference_b24###) adapter into the LM for efficient fine-tuning purposes.\nTo facilitate effective protein-text modeling, ProtT3 employs a two-stage training process to enhance protein-text modeling, as outlined in Figure 2(b)  ###reference_sf2###. The first stage involves protein-text retrieval training with three cross-modal tasks Li et al. (2022a  ###reference_b30###): protein-text contrasting, protein-text matching, and protein captioning. This stage not only empowers the cross-modal projector with the capability of protein-text retrieval, but also serves as a \u201cwarmup\u201d before the second stage by encouraging the extraction of text-relevant protein features. In the second stage, we connect the cross-modal projector to the LM and conduct protein-to-text generation training.\nOur key contributions are summarized as:\nWe introduce ProtT3, a new framework aiming to bridge the modality gap between texts and proteins. Through a cross-modal projector, ProtT3 jointly uses a PLM for protein understanding and an LM for text processing, enabling effective protein-to-text generation.\nTo set benchmarks and promote future research, we establish quantitative evaluations for protein-text modeling tasks, including protein captioning, protein QA, and protein-text retrieval. The datasets, evaluation scripts, and our pretrained checkpoints will be made available online.\nProtT3 achieves state-of-the-art performances across various tasks. For protein captioning, ProtT3 surpasses the baseline by over 10 BLEU-2 scores in the Swiss-Prot Bairoch and Apweiler (2000  ###reference_b4###) and the ProteinKG25 Zhang et al. (2022  ###reference_b62###) datasets. For protein-text retrieval, ProtT3 outperforms baselines by over  in retrieval accuracy on the Swiss-Prot and ProteinKG25 datasets. Lastly, ProtT3 achieves 2.5% improvement of exact match performance for protein QA on the PDB-QA Guo et al. (2023  ###reference_b22###) dataset."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Works",
            "text": "Protein Language Models (PLMs). PLMs are transformer-based LMs pretrained on small corpora of protein sequences for protein understanding and protein generation Lin et al. (2022  ###reference_b37###); Madani et al. (2023  ###reference_b44###); Nijkamp et al. (2022  ###reference_b47###); Chen et al. (2023  ###reference_b9###); Elnaggar et al. (2021  ###reference_b16###); Rives et al. (2021a  ###reference_b51###); Meier et al. (2021  ###reference_b46###). Similar to LMs for texts, PLMs are pretrained by the objective of masked language modeling Lin et al. (2022  ###reference_b37###) or auto-regressive modeling Madani et al. (2023  ###reference_b44###). PLMs have demonstrated promising performances on downstream tasks of 3D structure prediction and protein property prediction Lin et al. (2022  ###reference_b37###); Chen et al. (2023  ###reference_b9###). However, they cannot process texts due to the absence of text in their pretraining data."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Model Architecture",
            "text": "Here we introduce ProtT3\u2019s three key components: a PLM for protein understanding, an LM for text processing, and a cross-modal projector to bridge the modality gap between the first two components.\nProtein Language Model (PLM).\nWe utilize ESM-2 Lin et al. (2022  ###reference_b37###) to encode protein sequences of amino acids. ESM-2 is an encoder-only transformer LM Vaswani et al. (2017  ###reference_b59###) pretrained on 60M protein sequences by masked language modeling Devlin et al. (2019  ###reference_b12###). PLMs have shown promising performances for protein folding Lin et al. (2022  ###reference_b37###), multiple sequence alignment Rives et al. (2021b  ###reference_b52###), and protein property prediction Xu et al. (2023  ###reference_b60###), demonstrating their effectiveness for capturing protein characteristics. For efficiency, we freeze ESM-2\u2019s weights in our training process.\nLanguage Model (LM). We choose Galactica Taylor et al. (2022  ###reference_b56###) as the base LM. Galactica is a decoder-only transformer LM pretrained on a large collection of scientific papers, spanning disciplines like biology and medicine. Notably, Galactica has demonstrated a high-level understanding of protein concepts through its promising performances in biomedical QA benchmarks Jin et al. (2019  ###reference_b25###); Hendrycks et al. (2021  ###reference_b23###). Furthermore, Galactica includes a small set of protein sequences in its pretraining data, and shows a capability for understanding protein sequences through the task of protein keyword prediction Taylor et al. (2022  ###reference_b56###). Therefore, we also leverage Galactica as a baseline for protein-to-text generation to ablate the effectiveness of incorporating an additional PLM for protein understanding.\nCross-Modal Projector. We employ a cross-modal projector based on Q-Former Li et al. (2023a  ###reference_b29###) to bridge the modality gap between the PLM and the LM. Q-Former has demonstrated promising performances in vision-language tasks. As Figure 3  ###reference_###a illustrates, Q-Former consists of two transformers: one for protein encoding and another for text processing.\nSpecifically, the protein transformer maintains  learnable query tokens  as inputs. These query tokens can interact with the PLM through the cross-attention modules, in order to extract protein features. We denote the protein transformer\u2019s output as , containing protein features. For text input, the text transformer adds a [CLS] token at the beginning, and uses the [CLS] token\u2019s output as the text representation. Both transformers share the self-attention to enable interactions between proteins and texts. Details are in Section 4.1  ###reference_###.\nQ-Former\u2019s weights are initialized from PubMedBERT-Abstract Gu et al. (2022  ###reference_b21###), a BERT LM pretrained on paper abstracts from the PubMed database111https://pubmed.ncbi.nlm.nih.gov/  ###reference_pubmed.ncbi.nlm.nih.gov/###. It has shown promising performances in understanding biomedical concepts under the BLURB benchmark Gu et al. (2022  ###reference_b21###). The cross-attention module is added into the Q-Former every two layers and is randomly initialized."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Training Method",
            "text": "In this section, we introduce ProtT3\u2019s two training stages: protein-text retrieval and protein-to-text generation. The training process leverages a dataset of protein-text pairs , where  is a protein sequence and  is the corresponding text sequence."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Stage 1: Protein-Text Retrieval Training",
            "text": "Inspired by BLIP Li et al. (2022a  ###reference_b30###, 2023a  ###reference_b29###), we jointly employ three objectives for protein-text retrieval training: protein-text contrasting, protein-text matching, and protein captioning. These objectives are tailored for Q-Former\u2019s architecture, training it to extract protein features that are relevant to the text descriptions. This stage empowers the cross-modal projector with retrieval ability, and also serves as a \u201cwarmup\u201d before the next stage.\nProtein-Text Contrasting (PTC). We employ cross-modal contrastive learning Radford et al. (2021  ###reference_b50###) to align the protein representation and text representation from the Q-Former. As illustrated in Figure 3  ###reference_###b, Q-Former\u2019s self-attention module separately processes the query tokens and text tokens without any interaction. This enforces the query tokens to extract protein features from the PLM, in order to generate protein representations that align with the corresponding text representations in contrastive learning.\nFormally, let  be a batch of protein-text pairs. We denote the protein representations as , where  is the reprentation of the protein \u2019s -th query token; and denote text \u2019s representation as , which is the [CLS] token\u2019s output.\nProtein-text similarity is measured by the maximum similarity between  and each row of Z. The contrastive learning loss  can then be written as:\nwhere  is the cosine similarity; Temperature  is empirically set to .\nProtein-Text Matching (PTM). PTM is a binary classification objective aiming to discriminate whether a protein-text pair matches or not. Unlike PTC which computes protein-text similarity by applying cosine similarity on their output representations, PTM can obtain more fine-grained protein-text similarity. As illustrated in Figure 3  ###reference_###b, PTM feeds the query tokens and text tokens into the same self-attention module, allowing them to interact at the Q-Former\u2019s early layers. In this way, the query tokens can capture information on both proteins and texts. The mean pooling of query tokens\u2019 representations is then fed into a linear classifier for PTM prediction. During training, for each positive protein-text pair , we randomly sample two negative pairs:  and . Let  be the Q-Former\u2019s predicted probability that  is a matched pair, the PTM loss function  can be written as:\nIn experiments, we find that PTM surpasses PTC for protein-text retrieval. However, PTM incurs a higher computational cost for encoding every protein-text pair. To balance performance and speed, we use PTC to obtain the top  ranked candidates, and then use PTM for re-ranking.\nProtein Captioning (PCap). Protein captioning trains the Q-Former to generate text descriptions of given proteins. As shown in Figure 3  ###reference_###b, we apply a special masking strategy for this objective: 1) Bi-directional attention mask is applied to query tokens, enabling them to interact with each other but not text tokens; 2) Causal attention mask is used for text tokens, allowing them to attend query tokens and the preceding text tokens, but not the following text tokens. This design ensures the text tokens extract protein features exclusively from the query tokens, because they cannot directly interact with the proteins. Meanwhile, the query tokens are enforced to extract protein features through cross-attentions, satisfying the informational needs of protein captioning. Let  be the probability that Q-Former generate text  given protein . The protein captioning loss  can be written as:"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Stage 2: Protein-to-Text Generation Training",
            "text": "In this stage, we train ProtT3 for protein-to-text generation. As Figure 2(a)  ###reference_sf1### illustrates, we connect the cross-modal projector to the LM, feeding the protein representations Z into the LM, so as to condition the text generation process by protein information. Note that, we use a linear layer to project Z to the same dimension of the LM\u2019s input.\nWe train ProtT3 for each generation dataset separately, and append different text prompts after the protein representations to further control the generation process. For example, we use the text prompt of \u201cDescribe this protein\u2019s function\u201d for protein captioning, and \u201cQuestion: Does this protein contain polymer entities? Answer:\u201d for protein QA. For training, we use the same loss as the protein captioning task in the previous section.\nFor training efficiency, we incorporate Low-Rank Adaptation (LoRA) Hu et al. (2022  ###reference_b24###) adapters into the LM. LoRA adds pairs of trainable rank decomposition matrices into the selected weights of the LM. For example, LoRA modifies a pretrained weight  by adding a pair of matrices  and :\nwhere  is kept frozen and only the newly added BA is tuned. By using a small rank , LoRA can adapt an LM to a new task while requiring little memory for storing gradients. This method has shown comparable performances to full-parameter fine-tuning Hu et al. (2022  ###reference_b24###)."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We begin by introducing the datasets of protein-text pairs, followed by the experimental results. Details on experimental settings, such as hyperparameters and baseline implementations, are provided in Appendix C  ###reference_###. In the experiments, ProtT3 utilizes  as the base LM and  as the PLM."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Protein-Text Dataset Collection",
            "text": "In this section, we detail the protein-text pair datasets and the data processing procedure employed in our study. For all the used datasets, we discard protein sequences longer than  tokens, and carefully split the datasets to ensure no overlap between train/valid/test sets. Dataset statistics are available in Table 2  ###reference_###, and details are in Appendix A  ###reference_###.\nSwiss-Prot Bairoch and Apweiler (2000  ###reference_b4###) is a protein sequence database with text annotations. We process the dataset following Xu et al. (2023  ###reference_b60###), but excluding protein names from the text annotations to prevent information leakage. The resulting text descriptions concatenate the annotations of protein functions, locations, and families.\nProteinKG25 Zhang et al. (2022  ###reference_b62###) is a knowledge graph derived from the Gene Ontology Aleksander et al. (2023  ###reference_b3###) database. We transform its triples into free texts by first aggregating the triples of the same protein, and then filling the protein information into a pre-defined text template.\nPDB-QA Guo et al. (2023  ###reference_b22###) is a protein single-turn QA dataset derived from RCSB PDB222https://www.rcsb.org  ###reference_www.rcsb.org###. It includes 30 question templates about proteins\u2019 structures, properties, and supplementary information. As shown in Table 3  ###reference_###, to enable a fine-grained evaluation, we categorize the questions into four types, based on the answer\u2019s format (string or number) and content focus (structure/property or supplementary information).\nIt is worth noting that supplementary information is hard to predict given the protein sequence alone.\nTraining Pipeline. In training stage 1, we train ProtT3 on the combination of the Swiss-Prot and ProteinKG25 datasets for protein-text retrieval. In training stage 2, we load the checkpoint from stage 1 and conduct separate fine-tuning on the three datasets for protein-to-text generation tasks."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Protein Captioning",
            "text": "We evaluate protein captioning performances on the Swiss-Prot and ProteinKG25 datasets. Following Edwards et al. (2022  ###reference_b15###), we use the evaluation metrics of BLEU Papineni et al. (2002  ###reference_b49###), ROUGE Lin (2004  ###reference_b36###), and METEOR Banerjee and Lavie (2005  ###reference_b5###). Additionally, we report the percentage that the prediction exactly matches the ground truth annotation. For comparison and ablation study purposes, we report the performances of LoRA fine-tuned , and ProtT3\u2019s two variants: 1) ProtT3 w/ MLP Proj., which replaces ProtT3\u2019s cross-modal projector by an MLP, following Liu et al. (2023a  ###reference_b38###); and 2) ProtT3 w/o stage 1, which skips ProtT3\u2019s training stage 1. We do not compare with ProteinChat Guo et al. (2023  ###reference_b22###) because it requires 3D protein structures, which are unavailable for these two datasets.\nTable 4  ###reference_### presents the results. We observe the following: 1) ProtT3 and its variants substantially outperform the LoRA fine-tuned Galactica, with ProtT3 showing a 10-point improvement in BLEU-2 scores. This underscores the significance of incorporating a PLM for protein understanding and ProtT3\u2019s effectiveness in undertanding protein inputs. 2) ProtT3 consistently surpasses its two variants on both datasets, highlighting the advantages of using a Q-Former projector and training stage 1.\n###figure_5### ###figure_6###"
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Protein-Text Retrieval",
            "text": "We evaluate protein-text retrieval performances on the Swiss-Prot and ProteinKG25 datasets. Following Su et al. (2022  ###reference_b55###), our evaluation includes protein-text retrieval in a batch of  random samples and retrieval in the entire test set. We use Accuracy and Recall@20 as the evaluation metrics. For retrieval, ProtT3 first retrieves the top  candidates by PTC, and then uses PTM for re-ranking. For comparison, we employ ProtST Xu et al. (2023  ###reference_b60###) and ProteinCLAP Liu et al. (2023b  ###reference_b39###) as baselines, and present ProtT3 w/o PTM and ProtT3 w/o PCap for ablation studies.\nTable 5  ###reference_### shows the results. We observe that: 1) ProtT3 outperforms baselines by over  accuracy for retrieval in the test set, highlighting its capability in aligning proteins with corresponding text descriptions. 2) PTM improves ProtT3\u2019s retrieval accuracy in test set by  on both datasets. This is because PTM allows protein and text information to interact at the Q-Former\u2019s early layers, achieving more fine-grained protein-text similarity measurements. 3) PCap improves ProtT3\u2019s retrieval accuracy by . This is because\nPCap encourages the query tokens to extract protein information most\nrelevant to the text input, therefore aiding in protein-text alignment."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Protein Question-Answering",
            "text": "We evaluate protein QA performances on the PDB-QA dataset. Considering that the answers in PDB-QA typically consist of  words, we select exact match as the evaluation metric. For comparison, we employ ProteinChat Guo et al. (2023  ###reference_b22###) and LoRA fine-tuned  as baselines. We also assess a  that consumes only questions without proteins during training and prediction (i.e., LoRA ft, Q-only). This baseline measures the dataset\u2019s uni-modal bias Cad\u00e8ne et al. (2019  ###reference_b8###): the proportion of questions that can be correctly answered without looking at proteins.\nThe results are shown in Table 6  ###reference_###. We observe that: 1) ProtT3 surpasses baselines by 2.5% overall, and consistently outperforms them in predicting protein structures and properties. This demonstrates ProtT3\u2019s multi-modal understanding ability for the proteins and the textual questions.\n2) The baseline consuming question only (i.e., LoRA ft, Q-only) presents comparable performance, indicating that the dataset has a substantial uni-modal bias. This finding suggests an opportunity to explore debias techniques Mahabadi et al. (2020  ###reference_b45###) in future studies.\n3) ProteinChat underperforms other methods, possibly because it only trains a linear layer between the frozen PLM and LM. The linear layer is insufficient to map the protein representations into the text space of the LM, and adapt the LM to QA.\n4) Models perform worse on questions about numbers or supplementary information. This observation suggests the potential benefit of augmenting these models with external tools Schick et al. (2023  ###reference_b53###) like calculators and search engines."
        },
        {
            "section_id": "5.5",
            "parent_section_id": "5",
            "section_name": "Ablation Studies on Pretrained Models",
            "text": "Here we conduct ablation studies on the pretrained models used in our method. Specifically, we ablate the impact of different PLMs in training stage 1, and ablate different LMs in training stage 2.\nAblating PLMs. In training stage 1, we replace the ESM-2 protein encoder with its smaller variants, namely ESM-2 and ESM-2, to evaluate their performances for protein-text retrieval. As shown in Table 7  ###reference_###, we can observe that retrieval performance increases monotonically with model size, a trend consistent with previous observations in the LM domain.\nAblating LMs. In training stage 2, we replace Galactica to Phi-1.5 Li et al. (2023c  ###reference_b34###). Unlike Galactica, Phi-1.5 is pretrained on general domain data but not focusing on scientific literature. Table 8  ###reference_### shows protein captioning performance on the ProteinKG25 Zhang et al. (2022  ###reference_b62###) dataset. We can observe that Galactica significantly outperforms Phi-1.5 for protein captioning, although they have similar sizes. We attribute this performance gap to their pretraining corpus, with Galactica performing better for pretraining on more scientific literature."
        },
        {
            "section_id": "5.6",
            "parent_section_id": "5",
            "section_name": "Examples of Protein-to-Text Generation",
            "text": "Figure 4  ###reference_### shows three examples of protein captioning.\nIn the first example, ProtT3\u2019s caption is more accurate by correctly identifying the DMRT family, while Galactica does not. In the second example, both models fail to identify the protein\u2019s function. Nevertheless, ProtT3\u2019s prediction regarding the protein family is more accurate. The third example shows that both models successfully predict the subcellular location and protein family. ProtT3 goes a step further by predicting the protein\u2019s function, which is closer to the ground truth description.\nFigure 5  ###reference_### shows three examples of protein QA. We can observe that both ProtT3 and Galactica answer the first two questions about protein property/structure correctly, and fail on the third question, which requires a numerical answer. On the other hand, ProteinChat struggles with all three questions, failing to answer each of them."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion and Future Works",
            "text": "In this work, we propose ProtT3, a new protein-text modeling framework. ProtT3 aims to facilitate text-based protein understanding via protein-to-text generation and protein-text retrieval. To achieve this, ProtT3 integrates a PLM into an LM to enhance the LM\u2019s protein understanding ability. A cross-modal projector enables this integration by bridging the modality gap between the two modules. To promote future research, we set benchmarks for protein-text modeling tasks, including protein captioning, protein QA, and protein-text retrieval, where ProtT3 significantly outperforms existing baselines.\nLooking ahead, we plan to explore enabling LMs to understand 3D protein structures Li et al. (2024  ###reference_b31###) and apply this understanding to more tasks of drug discovery, property prediction Liu et al. (2023d  ###reference_b41###); Li et al. (2022b  ###reference_b33###, 2023b  ###reference_b32###), molecule generation Luo et al. (2024  ###reference_b42###), and OOD generalization Fang et al. (2023  ###reference_b19###, 2024a  ###reference_b18###)."
        }
    ],
    "url": "http://arxiv.org/html/2405.12564v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "4",
            "4.1",
            "4.2"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.1",
            "5.2",
            "5.3",
            "5.4",
            "5.5",
            "5.6"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "5.2",
            "5.3",
            "5.4",
            "5.5"
        ]
    },
    "research_context": {
        "paper_id": "2405.12564v1",
        "paper_title": "ProtT3: Protein-to-Text Generation for Text-based Protein Understanding",
        "research_background": "### Motivation\n\nThe paper is motivated by the impressive successes of Language Models (LMs) across various domains and their potential for excelling in text-based protein understanding tasks, as evidenced by their performance in biological and medical question-answering (QA). This potential underscores the broader importance of leveraging LMs for advancements in biomedical applications such as drug discovery and protein property prediction.\n\n### Research Problem\n\nThe primary research problem addressed in the paper involves improving the ability of LMs to generate textual interpretations of proteins\u2014a task referred to as protein-to-text generation. Existing works have either limited scope or insufficient techniques for interpreting proteins as direct inputs to LMs. Additionally, there is a notable absence of proper benchmarks for quantitatively evaluating the progress in protein-text modeling.\n\n### Relevant Prior Work\n\n1. **LM Applications in Protein Understanding**: LMs have demonstrated their potential in text-based protein understanding tasks like biological and medical QA (Taylor et al., 2022; OpenAI, 2023).\n  \n2. **Importance in Biomedical Fields**: The significance lies in driving advancements in fields such as drug discovery and protein property prediction (Kim et al., 2021; Fan et al., 2023).\n\n3. **Previous Efforts in Protein-to-Text Generation**:\n   - **Galactica (Taylor et al., 2022)**: Limited by a restricted set of protein sequences in its pretraining data.\n   - **ProteinChat (Guo et al., 2023)**: Uses a linear projector to map protein representations to text space, which may not capture complex relationships between proteins and texts adequately.\n\n4. **Cross-Modal Techniques**: Prior approaches such as cross-modal contrastive learning (Xu et al., 2023; Liu et al., 2023b; Radford et al., 2021) have not interpreted proteins as direct LM inputs, highlighting a critical gap in the field.\n\n5. **Assessment and Benchmarks**: The lack of quantitative evaluations for evaluating protein-text modeling limits tracking progress and promoting further advancements (Liu et al., 2023b; Xu et al., 2023; Taylor et al., 2022; Guo et al., 2023).\n\nIn summary, ProtT3 aims to address these gaps by incorporating a Protein Language Model (PLM) for understanding protein sequences, integrating a cross-modal projector for mapping protein representations to text space, and introducing benchmarks for quantitative evaluations in protein-text modeling tasks.",
        "methodology": "The methodology section of the paper \"ProtT3: Protein-to-Text Generation for Text-based Protein Understanding\" outlines the proposed method, which consists of three key components: a Protein Language Model (PLM), a Language Model (LM), and a Cross-Modal Projector. Here\u2019s a detailed description of each component and their innovations:\n\n1. **Protein Language Model (PLM)**:\n   - **Model Chosen**: ESM-2, an encoder-only transformer LM.\n   - **Pretraining**: It is pretrained on 60 million protein sequences using masked language modeling techniques.\n   - **Performance**: ESM-2 is notable for its efficacy in protein folding, multiple sequence alignment, and protein property prediction.\n   - **Usage in This Paper**: The PLM is utilized to encode protein sequences composed of amino acids. For the sake of efficiency, the weights of ESM-2 are frozen during the training process.\n\n2. **Language Model (LM)**:\n   - **Model Chosen**: Galactica, a decoder-only transformer LM.\n   - **Training Data**: Pretrained on a vast collection of scientific papers, especially in disciplines such as biology and medicine.\n   - **Capabilities**: Galactica has demonstrated a high understanding of protein concepts through tasks like biomedical QA benchmarks and protein keyword prediction.\n   - **Usage in This Paper**: Galactica serves as the base LM for text processing and as a baseline for protein-to-text generation to compare the effectiveness of incorporating an external PLM for protein understanding.\n\n3. **Cross-Modal Projector**:\n   - **Model Chosen**: Q-Former, known for its performances in vision-language tasks.\n   - **Structure**:\n     - **Protein Transformer**: Contains learnable query tokens that interact with the PLM to extract protein features through cross-attention modules.\n     - **Text Transformer**: Adds a [CLS] token at the beginning of the text input and uses the output of this token for text representation.\n     - **Interaction**: Both transformers share self-attention mechanisms to allow interactions between protein and text representations.\n   - **Initialization**: The weights of Q-Former are initialized from PubMedBERT-Abstract, a BERT model pretrained on PubMed paper abstracts, ensuring a strong understanding of biomedical concepts.\n   - **Cross-Attention Module**: This module is added every two layers in Q-Former and is randomly initialized to facilitate the bridging of the modality gap between proteins and text.\n\nIn summary, the method proposed in ProtT3 leverages ESM-2 for understanding protein sequences, Galactica for generating text based on protein insights, and uses Q-Former to effectively integrate and translate protein features into textual representations.",
        "main_experiment_and_results": "In this study, the main experiment investigates the performance of ProtT3, a model developed for protein-to-text generation to enhance text-based protein understanding. The setup involves the utilization of specific datasets containing protein-text pairs. These datasets consist of structured annotations and descriptions related to various proteins, serving as the primary source of data for training and evaluating the ProtT3 model.\n\n**Experimental Setup:**\n- **Datasets:** The experiment uses datasets composed of protein-text pairs, where each pair includes detailed annotations and descriptions of protein sequences. The datasets are curated to ensure a comprehensive coverage of protein functions and characteristics.\n- **Model Utilization:** ProtT3 is built on a base language model (LM) and a protein language model (PLM). The exact models and their configurations are detailed in Appendix C (reference not provided here).\n- **Evaluation Metrics:** The performance of ProtT3 is assessed using standard metrics for text generation and protein annotation quality. These likely include BLEU, ROUGE, and other relevant metrics that measure the accuracy, coherence, and relevance of the generated text in relation to the protein data.\n\n**Main Experimental Results:**\nThe key results from the main experiments show the effectiveness of ProtT3 in generating accurate and meaningful text descriptions of proteins. The model's performance is benchmarked against several baselines, demonstrating substantial improvements in both text generation quality and protein understanding. Specifically, ProtT3 outperforms the baseline models on all major evaluation metrics, indicating its superior ability to capture and translate protein characteristics into coherent textual descriptions.\n\nThese results highlight ProtT3's potential in aiding biological research by providing automated, high-quality descriptions of protein functions, enabling better understanding and discovery in the field of proteomics. \n\n(Note: Exact numerical results and specific baseline models are not provided in the source text and should be referred to in the corresponding section or appendix of the original paper for detailed insights.)"
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the components of the ProtT3 framework and understand the impact of replacing certain components on its performance in protein captioning.",
            "experiment_process": "The ablation study was conducted on the Swiss-Prot and ProteinKG25 datasets using evaluation metrics such as BLEU, ROUGE, and METEOR. Two ProtT3 variants were analyzed: 1) ProtT3 with an MLP projector replacing the cross-modal projector (Q-Former), and 2) ProtT3 without the initial training stage (stage 1). These variants were compared against the baseline performance of a LoRA fine-tuned Galactica model and the original ProtT3.",
            "result_discussion": "The results show that both ProtT3 and its variants substantially outperform the LoRA fine-tuned Galactica, with ProtT3 showing a 10-point improvement in BLEU-2 scores. ProtT3 consistently surpasses its two variants, highlighting the advantages of using a Q-Former projector and the importance of training stage 1.",
            "ablation_id": "2405.12564v1.No1"
        },
        {
            "research_objective": "To determine the effect of omitting core components in the ProtT3 framework on protein-text retrieval performance.",
            "experiment_process": "Protein-text retrieval was evaluated on the Swiss-Prot and ProteinKG25 datasets using Accuracy and Recall@20 metrics. ProtT3's retrieval was compared against ProtST and ProteinCLAP baselines. The ablation study included two ProtT3 variants: ProtT3 without the PTM component and ProtT3 without the PCap component.",
            "result_discussion": "The results indicate that ProtT3 outperforms the baselines by a significant margin. The inclusion of PTM and PCap components improves ProtT3's retrieval accuracy, showing the importance of these components in achieving fine-grained protein-text similarity measurements and aligning proteins with corresponding text descriptions.",
            "ablation_id": "2405.12564v1.No2"
        },
        {
            "research_objective": "To study the effectiveness of ProtT3's components in performing protein question-answering (QA) tasks.",
            "experiment_process": "Protein QA was evaluated on the PDB-QA dataset using the exact match metric. Comparisons were made against ProteinChat, a LoRA fine-tuned model, and a Q-only baseline that consumes only questions without proteins during training and prediction. This baseline aimed to measure the dataset\u2019s uni-modal bias.",
            "result_discussion": "ProtT3 surpasses the baselines by 2.5% overall, demonstrating superior multi-modal understanding of proteins and textual questions. The Q-only baseline's performance indicates a substantial dataset uni-modal bias, suggesting future exploration of debiasing techniques. ProteinChat, which only trains a linear layer between the frozen PLM and LM, underperforms due to insufficient mapping of protein representations into the LM's text space. Models performed worse on questions about numbers or supplementary information, suggesting potential benefits of external tools integration.",
            "ablation_id": "2405.12564v1.No3"
        },
        {
            "research_objective": "To evaluate the impact of different pretrained models used in the ProtT3 method on its performance.",
            "experiment_process": "The ablation study includes two parts: 1) Ablating PLMs in training stage 1 by replacing the ESM-2 protein encoder with its smaller variants, and 2) Ablating LMs in training stage 2 by replacing Galactica with Phi-1.5. The performances were evaluated for protein-text retrieval and protein captioning on the ProteinKG25 dataset.",
            "result_discussion": "The retrieval performance increases monotonically with the model size of the PLMs, consistent with previous observations in the LM domain. Galactica outperforms Phi-1.5 significantly for protein captioning, attributed to Galactica's pretraining on scientific literature.",
            "ablation_id": "2405.12564v1.No4"
        }
    ]
}