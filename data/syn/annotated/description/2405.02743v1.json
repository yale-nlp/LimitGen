{
    "title": "Beyond Performance: Quantifying and Mitigating Label Bias in LLMs",
    "abstract": "Large language models (LLMs) have shown remarkable adaptability to diverse tasks, by leveraging context prompts containing instructions, or minimal input-output examples. However, recent work revealed they also exhibit label bias\u2014an undesirable preference toward predicting certain answers over others. Still, detecting and measuring this bias reliably and at scale has remained relatively unexplored. In this study, we evaluate different approaches to quantifying label bias in a model\u2019s predictions, conducting a comprehensive investigation across 279 classification tasks and ten LLMs. Our investigation reveals substantial label bias in models both before and after debiasing attempts, as well as highlights the importance of outcomes-based evaluation metrics, which were not previously used in this regard. We further propose a novel label bias calibration method tailored for few-shot prompting, which outperforms recent calibration approaches for both improving performance and mitigating label bias. Our results emphasize that label bias in the predictions of LLMs remains a barrier to their reliability.111We release our code at https://github.com/schwartz-lab-NLP/label-bias.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large language models (LLMs) have demonstrated impressive abilities in adapting to new tasks when conditioned on a context prompt, containing task-solving instructions Wei et al. (2022  ###reference_b32###) or few examples of input-output pairs Brown et al. (2020  ###reference_b2###).\nStill, recent work has shown that predictions of LLMs exhibit label bias\u2014a strong, undesirable preference towards predicting certain answers over others (Zhao et al., 2021  ###reference_b35###; Chen et al., 2022  ###reference_b4###; Fei et al., 2023  ###reference_b9###, see Fig. 1  ###reference_###).\nSuch preferences were shown to be affected by the choice and order of in-context demonstrations Liu et al. (2022  ###reference_b16###); Lu et al. (2022  ###reference_b18###), the model\u2019s pretraining data Dong et al. (2022  ###reference_b8###), or textual features of the task data Fei et al. (2023  ###reference_b9###). Consequently, several approaches were proposed to address this problem, mostly by calibrating the model\u2019s output probabilities to compensate for this bias Zhao et al. (2021  ###reference_b35###); Fei et al. (2023  ###reference_b9###).\n###figure_1### Despite these efforts, label bias evaluation relies on performance metrics such as accuracy, rather than metrics designed to directly quantify the bias.\nIn doing so, we might inadvertently overlook crucial aspects of model behavior.\nIndeed, although a given method could effectively improve performance, substantial bias might still persist in the model\u2019s predictions\u2014deeming the method insufficient and the model unreliable.\nAlternatively, performance could remain relatively unchanged, but with the bias mostly removed.\nIn this work, we take a step towards a more comprehensive understanding of the extent of label bias in LLMs and the effects of mitigation approaches.\nUsing metrics to directly measure the label bias in model predictions, which we derive from previous work on fairness and label bias estimation, we evaluate ten LLMs on 279 diverse classification and multiple-choice tasks from Super-NaturalInstructions Wang et al. (2022  ###reference_b31###). We examine both performance and bias along axes such as scale and number of in-context demonstrations.\nWe also evaluate the impact of label bias mitigation methods, such as calibration and few-shot LoRA fine-tuning Hu et al. (2022  ###reference_b13###).\nOur investigation reveals substantial label bias in the predictions of LLMs across all evaluated settings, indicating that raw LLM output scores often represent simple, heuristic solutions. While increasing model size, providing in-context demonstrations, and instruction-tuning all contribute to reducing bias, ample bias persists, even after applying mitigation methods.\nSurprisingly, these results also hold for tasks where the labels are all semantically equivalent (e.g., in multi-choice question answering).\nFurther, although the examined calibration methods can reduce bias and improve performance, we also find cases where they negatively impact both bias and overall performance.\nMotivated by these findings, we propose a novel calibration method for few-shot prompting\nthat more accurately estimates a model\u2019s label bias, using only its predictions on the in-context demonstrations. Compared to existing LLM bias calibration methods, our method improves performance while also removing considerably more bias.\nOur findings highlight the necessity of considering and measuring biases in the predictions of LLMs when evaluating their performance. Moreover, adjusting models to their tasks through more accurate and effective estimation of biases holds promise for improving the reliability of LLMs and their applications."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "LLM Label Bias",
            "text": "Our objective is to broaden the understanding of label bias in LLMs and the effectiveness of mitigation strategies, focusing on classification tasks.\nIn this section, we define metrics designed to quantify bias in model predictions, providing a nuanced examination of label bias that extends beyond traditional performance metrics.\nWe describe the setting of label bias in in-context learning (\u00a72.1  ###reference_###), briefly outline methods to mitigate it (\u00a72.2  ###reference_###), and finally review approaches to evaluate label bias as well as define the metrics we use in this work (\u00a72.3  ###reference_###).\nPrevious work used qualitative assessments to visualize model output distributions on selected datasets Zhao et al. (2021  ###reference_b35###); Han et al. (2023  ###reference_b11###). However, these cannot be used to rigorously evaluate models at larger scales.\nRecently, Fei et al. (2023  ###reference_b9###) proposed to measure a model\u2019s label bias by considering two sets of inputs: a set of synthetic, content-free task inputs , and inputs consisting of random vocabulary words . For each input, they compute the output probabilities on every label , and finally compute the model\u2019s mean predicted probabilities across both sets,  and :\nThe model\u2019s bias is then defined to be the total variation distance  between the two distributions:\nImportantly, since Fei et al. (2023  ###reference_b9###) also use the model\u2019s predictions on the content-free inputs  to calibrate it, this metric cannot be used to quantify the label bias remaining after calibration.\nIn this work, we simplify the computation of this metric and adapt it to be used after calibration.\nFirst, we hold-out a set of inputs to be used exclusively for measuring bias.\nSecond, when estimating the model\u2019s average output probabilities, instead of using synthetic inputs, we use in-distribution examples held-out from the test set, . This setup allows to account for label imbalance in the data used for bias estimation , as the instances in the test set are all labeled. To do so, we first estimate the model\u2019s output distribution individually on each subset of examples with gold label , , by computing:\nand then set  to be the average of these estimates.222In case examples for an infrequent label  are not found in , we exclude it from the computation of .\nInstead of , we use the uniform distribution over all answer choices , which recent mitigation approaches considered as the \u201cideal\u201d and unbiased mean output distribution Zhao et al. (2021  ###reference_b35###).\nFinally, we define the model\u2019s bias score as the total variation distance between these two distributions:\nWhen considering the effects of label bias on model predictions, strong label bias will likely result in disparities in task performance on instances of different classes. However, metrics to assess such disparities were not used in previous analyses of label bias.\nWe propose to use the Relative Standard Deviation of class-wise accuracy (RSD; Croce et al. 2021  ###reference_b6###; Benz et al. 2021  ###reference_b1###),\na metric used for studying fairness in classification.\nRSD is defined as the standard deviation of the model\u2019s class-wise accuracy , divided by its mean accuracy acc on the entire evaluation data:333The goal of this normalization is to enhance the metric\u2019s interpretability across tasks of varying difficulty.\nIntuitively, RSD is low when model performance is similar on all classes, and high when it performs well on some classes but poorly on others.\nWe note that each evaluation approach could detect biases that the other does not. For example, a slight bias in the model\u2019s average output probabilities (e.g., 55% vs. 45%) could render dramatic bias in actual outcomes if the model always assigns higher probability to some label. Conversely, when the output probabilities are biased on average but the model\u2019s class-wise performance is balanced, this hidden bias could result in actual performance disparities on more difficult instances. We therefore suggest reporting both measures."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Label Bias",
            "text": "When employing LLMs for classification tasks through prompting, the model is given a test example , preceded by a context . This context can contain a (potentially empty) set of examples of the task\u2019s input-output mapping , henceforth demonstrations, and may also include task instructions. To determine the model\u2019s prediction from a set of answer choices , the likelihood it assigns to each continuation  is computed, and the highest probability option is taken as the model prediction:\nThese output probabilities often exhibit label bias, where the model tends to assign higher probability to certain answers regardless of the input test example  (Fig. 1  ###reference_###).\nMultiple factors were posited to influence this bias, including the choice of verbalizers , the choice and order of in-context examples in , and the overall textual features of task input  Zhao et al. (2021  ###reference_b35###); Fei et al. (2023  ###reference_b9###)."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Bias Mitigation",
            "text": "The predominant approach to alleviate label bias is to calibrate the model\u2019s output probabilities post-hoc, for a specific context prompt .\nSuch methods typically first estimate the model\u2019s label bias using its output probabilities on a set of inputs, which can be content-free (e.g., \u201cN/A\u201d or random words from the task\u2019s domain; Zhao et al. 2021  ###reference_b35###; Fei et al. 2023  ###reference_b9###) or ordinary task inputs Han et al. (2023  ###reference_b11###). Next, calibration parameters are chosen based on this estimate, and used to adjust the original output probabilities during inference to generate the (hopefully unbiased) output."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Evaluation Measures",
            "text": "Most LLM label bias analysis relies on indirect assessments. For instance, some work inspected improvements in overall performance gained after applying techniques to mitigate it Fei et al. (2023  ###reference_b9###); Holtzman et al. (2021  ###reference_b12###); Zhao et al. (2021  ###reference_b35###). However, these do not indicate the extent of bias originally present, or that which remains after mitigation.\nWe next examine approaches to measure this bias more directly, and define the metrics we use in this work.\nImportantly, we focus on label bias measures that could be used effectively both before and after applying mitigation techniques such as calibration.\nDrawing from previous research on fairness and bias in machine learning, we observe that there are two distinct yet related aspects in which label bias can be measured in LLM predictions: through the probabilities assigned by the model to different answers, e.g., assigning the label \u201cyes\u201d with an average output probability of 0.55, while \u201cno\u201d with 0.45; and through the model\u2019s predictions for different labels, e.g., achieving a recall of 0.50 for instances labeled \u201cyes\u201d, compared to 0.40 on \u201cno\u201d Mehrabi et al. (2021  ###reference_b20###).\nBelow we describe methods to measure each of these notions of bias.\nPrevious work used qualitative assessments to visualize model output distributions on selected datasets Zhao et al. (2021  ###reference_b35###  ###reference_b35###); Han et al. (2023  ###reference_b11###  ###reference_b11###). However, these cannot be used to rigorously evaluate models at larger scales.\nRecently, Fei et al. (2023  ###reference_b9###  ###reference_b9###) proposed to measure a model\u2019s label bias by considering two sets of inputs: a set of synthetic, content-free task inputs , and inputs consisting of random vocabulary words . For each input, they compute the output probabilities on every label , and finally compute the model\u2019s mean predicted probabilities across both sets,  and :\nThe model\u2019s bias is then defined to be the total variation distance  between the two distributions:\nImportantly, since Fei et al. (2023  ###reference_b9###  ###reference_b9###) also use the model\u2019s predictions on the content-free inputs  to calibrate it, this metric cannot be used to quantify the label bias remaining after calibration.\nIn this work, we simplify the computation of this metric and adapt it to be used after calibration.\nFirst, we hold-out a set of inputs to be used exclusively for measuring bias.\nSecond, when estimating the model\u2019s average output probabilities, instead of using synthetic inputs, we use in-distribution examples held-out from the test set, . This setup allows to account for label imbalance in the data used for bias estimation , as the instances in the test set are all labeled. To do so, we first estimate the model\u2019s output distribution individually on each subset of examples with gold label , , by computing:\nand then set  to be the average of these estimates.222In case examples for an infrequent label  are not found in , we exclude it from the computation of .\nInstead of , we use the uniform distribution over all answer choices , which recent mitigation approaches considered as the \u201cideal\u201d and unbiased mean output distribution Zhao et al. (2021  ###reference_b35###  ###reference_b35###).\nFinally, we define the model\u2019s bias score as the total variation distance between these two distributions:\nWhen considering the effects of label bias on model predictions, strong label bias will likely result in disparities in task performance on instances of different classes. However, metrics to assess such disparities were not used in previous analyses of label bias.\nWe propose to use the Relative Standard Deviation of class-wise accuracy (RSD; Croce et al. 2021  ###reference_b6###  ###reference_b6###; Benz et al. 2021  ###reference_b1###  ###reference_b1###),\na metric used for studying fairness in classification.\nRSD is defined as the standard deviation of the model\u2019s class-wise accuracy , divided by its mean accuracy acc on the entire evaluation data:333The goal of this normalization is to enhance the metric\u2019s interpretability across tasks of varying difficulty.\nIntuitively, RSD is low when model performance is similar on all classes, and high when it performs well on some classes but poorly on others.\nWe note that each evaluation approach could detect biases that the other does not. For example, a slight bias in the model\u2019s average output probabilities (e.g., 55% vs. 45%) could render dramatic bias in actual outcomes if the model always assigns higher probability to some label. Conversely, when the output probabilities are biased on average but the model\u2019s class-wise performance is balanced, this hidden bias could result in actual performance disparities on more difficult instances. We therefore suggest reporting both measures."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experimental Setting",
            "text": "During their instruction tuning, Llama-2 chat models were initially fine-tuned on the Flan data collection Chung et al. (2022  ###reference_b5###); Longpre et al. (2023  ###reference_b17###).\nAs roughly 20% of Flan consists of examples from Super-NaturalInstructions, our evaluation of Llama-2 instruction-tuned models is likely affected by data contamination Magar and Schwartz (2022  ###reference_b19###). Still, our results show both 7B and 13B chat models exhibit extensive label bias, possibly due to later fine-tuning on other data. As it is unclear from the implementation details of Touvron et al. (2023  ###reference_b28###) which exact instances in Super-NaturalInstructions were included in training, we do not take extra steps in attempt to reduce possible overlap and contamination.\nZhao et al. (2021  ###reference_b35###) proposed to use calibration in order to remove the label bias arising from the context prompt  and the model\u2019s pretraining. Inspired by confidence calibration methods Guo et al. (2017  ###reference_b10###), they define a matrix  that is applied to the model\u2019s original output probabilities  during inference to obtain calibrated, debiased probabilities .\nTo determine the calibration parameters , they first estimate the bias by computing the model\u2019s average predicted probabilities  on a small set of \u201cplaceholder\u201d content-free input strings, such as \u201cN/A\u201d, which replace the ordinary task input that follows .444As in the original implementation, we use \u201cN/A\u201d, \u201c[MASK]\u201d, and the empty string.\nFinally, they set , which ensures that the output class probabilities for the average content-free input are uniform, aiming to reduce bias on unseen examples.\n###figure_2### ###figure_3### ###figure_4### Following CC, Fei et al. (2023  ###reference_b9###) proposed to estimate and mitigate the label bias arising from the textual distribution of the task\u2019s domain, by using task-specific content-free inputs to compute .\nThey construct such inputs by sampling and concatenating  random words from the test set, where  is the average instance input length in the data. They repeat this process  times, and set  to be the average output probabilities over all examples. Given a test example with original output probabilities , they then use the calibrated probabilities .\nFinally, we experiment with few-shot, parameter-efficient fine-tuning for adapting LLMs to a given task\u2019s label distribution, thus potentially mitigating label bias. We fine-tune task-specific models for each context prompt using Low-Rank Adapation (LoRA; Hu et al., 2022  ###reference_b13###), training adapters on 16 held-out training examples for 5 epochs. Importantly, we use the same context  during both fine-tuning and evaluation. Due to computational constraints, we only run LoRA on Llama-2 7B and Mistral 7B, only consider values of , and average\nacross two sets of demonstrations. See App. A  ###reference_.SSS0.Px3### for more details."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Datasets",
            "text": "We evaluate models on 279 diverse tasks from the Super-NaturalInstructions benchmark Wang et al. (2022  ###reference_b31###). We select all available classification and multi-choice question answering tasks where the output space is a set of predefined labels, such as \u201cyes/no\u201d or \u201cA/B/C\u201d.\nWe sample 1,000 evaluation examples for all tasks with larger data sizes, and additionally sample 32 held-out examples for computing the bias score metric (\u00a72.3  ###reference_###), and 64 more examples to use as a pool of instances for choosing in-context demonstrations and LoRA fine-tuning examples.\nWe only include tasks with at least 300 evaluation examples in our experiments. For details on the selected tasks, see App. B  ###reference_###."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Models and Evaluation Setup",
            "text": "We experiment with models of different sizes from three LLM families: Llama-2 7B and 13B Touvron et al. (2023  ###reference_b28###), Mistral 7B Jiang et al. (2023a  ###reference_b14###), and Falcon 7B and 40B Penedo et al. (2023  ###reference_b23###). We use both the base and instruction fine-tuned versions of each model. We evaluate models using context prompts with  demonstrations, and average the results across  different sets of demonstrations for each .\nTo control the evaluation budget, we run the more expensive Falcon 40B experiments with  averaged across  sets of demonstrations.\nWe use the task instructions and prompt template defined in Super-NaturalInstructions.\nFor tasks where the answer choices  have unequal token lengths, we use length-normalized log-likelihood to compute the output probabilities Holtzman et al. (2021  ###reference_b12###).\nFor additional implementation details, see App. A  ###reference_###.\nDuring their instruction tuning, Llama-2 chat models were initially fine-tuned on the Flan data collection Chung et al. (2022  ###reference_b5###  ###reference_b5###); Longpre et al. (2023  ###reference_b17###  ###reference_b17###).\nAs roughly 20% of Flan consists of examples from Super-NaturalInstructions, our evaluation of Llama-2 instruction-tuned models is likely affected by data contamination Magar and Schwartz (2022  ###reference_b19###  ###reference_b19###). Still, our results show both 7B and 13B chat models exhibit extensive label bias, possibly due to later fine-tuning on other data. As it is unclear from the implementation details of Touvron et al. (2023  ###reference_b28###  ###reference_b28###) which exact instances in Super-NaturalInstructions were included in training, we do not take extra steps in attempt to reduce possible overlap and contamination."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Bias Mitigation Techniques",
            "text": "We evaluate the effects of three label bias mitigation methods: two calibration methods designed to correct a model\u2019s label bias by adjusting its output scores; and few-shot LoRA fine-tuning Hu et al. (2022  ###reference_b13###), which adapts the model to the task and its label distribution.\nWe describe the methods below.\nZhao et al. (2021  ###reference_b35###  ###reference_b35###) proposed to use calibration in order to remove the label bias arising from the context prompt  and the model\u2019s pretraining. Inspired by confidence calibration methods Guo et al. (2017  ###reference_b10###  ###reference_b10###), they define a matrix  that is applied to the model\u2019s original output probabilities  during inference to obtain calibrated, debiased probabilities .\nTo determine the calibration parameters , they first estimate the bias by computing the model\u2019s average predicted probabilities  on a small set of \u201cplaceholder\u201d content-free input strings, such as \u201cN/A\u201d, which replace the ordinary task input that follows .444As in the original implementation, we use \u201cN/A\u201d, \u201c[MASK]\u201d, and the empty string.\nFinally, they set , which ensures that the output class probabilities for the average content-free input are uniform, aiming to reduce bias on unseen examples.\n###figure_5### ###figure_6### ###figure_7### Following CC, Fei et al. (2023  ###reference_b9###  ###reference_b9###) proposed to estimate and mitigate the label bias arising from the textual distribution of the task\u2019s domain, by using task-specific content-free inputs to compute .\nThey construct such inputs by sampling and concatenating  random words from the test set, where  is the average instance input length in the data. They repeat this process  times, and set  to be the average output probabilities over all examples. Given a test example with original output probabilities , they then use the calibrated probabilities .\nFinally, we experiment with few-shot, parameter-efficient fine-tuning for adapting LLMs to a given task\u2019s label distribution, thus potentially mitigating label bias. We fine-tune task-specific models for each context prompt using Low-Rank Adapation (LoRA; Hu et al., 2022  ###reference_b13###  ###reference_b13###), training adapters on 16 held-out training examples for 5 epochs. Importantly, we use the same context  during both fine-tuning and evaluation. Due to computational constraints, we only run LoRA on Llama-2 7B and Mistral 7B, only consider values of , and average\nacross two sets of demonstrations. See App. A  ###reference_.SSS0.Px3###  ###reference_.SSS0.Px3### for more details."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Quantifying Label Bias in LLMs",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "LLMs are Label-Biased",
            "text": "We begin by examining the performance and label bias of models with and without instruction-tuning. We report averaged results across all tasks for Llama-2 models in Fig. 2  ###reference_###. Results for other models show similar trends (see App. C.1  ###reference_###).\nWe first verify that, as expected, model performance (Fig. 2(a)  ###reference_sf1###) substantially improves with scale, with instruction tuning and with the number of demonstrations.\nWe then consider the two bias metrics\u2014RSD (Fig. 2(b)  ###reference_sf2###) and BiasScore (Fig. 2(c)  ###reference_sf3###).\nWe observe that label bias is substantial across most evaluated settings: When prompted with two or no demonstrations, all models obtain high RSD values of 0.6 or more, with base models obtaining even higher values around 0.9.\nThis implies a widespread disparity in model performance across classes in many of the evaluated tasks, and indicates that for most tasks, models primarily succeed on instances of certain classes, while consistently failing on others. Increasing the number of demonstrations to 8 helps reduce the bias, but RSD remains substantial at around 0.4, and adding further demonstrations results in little to no improvement.\nSimilarly, we find BiasScore improves considerably when using sufficient demonstrations, with models obtaining values as high as 0.25 when using no demonstrations, to around 0.05 for the best model and setting.\nHigh BiasScore values indicate the model is uncalibrated, and tends to make overly confident predictions on certain labels regardless of the input.\nAlthough BiasScore can be relatively small for some models\u2014indicating their average output distribution is close to uniform\u2014when observed together with high RSD, it implies that the model subtly but persistently assigns more probability mass to the preferred labels, resulting in substantial bias.\nfont=small,labelfont=small\n7B\n###figure_8### ###figure_9### ###figure_10### 13B\n###figure_11### ###figure_12### ###figure_13###"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Differences between the Bias Measures",
            "text": "We further observe that, interestingly, both bias metrics show divergent trends. Although RSD values, much like model performance, sharply improve after instruction-tuning, the resulting models\u2019 BiasScore is often higher than their base counterparts.\nSimilarly, while RSD improves with scaling, the BiasScore of smaller models is lower.\nWe note that higher performance together with lower RSD means that the model\u2019s performance has improved across most classes. In contrast, higher BiasScore indicates that its average predicted probabilities grew farther than uniform. Taken together, this implies that the scaled-up and instruction-tuned models are making more confident predictions on some classes, but not on others.\nThis could mean more confident correct predictions on the preferred classes, or more confidently wrong predictions on others (or both).\nAltogether, this suggests\nmore subtle forms of bias persist after instruction-tuning or scaling up Tal et al. (2022  ###reference_b27###).\nOverall, we find the two metrics to be complimentary due to their measurement of different aspects of label bias. We hence use both in further experiments to provide a more comprehensive understanding of label bias in model predictions."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Label Bias Persists after Mitigation",
            "text": "We have seen that LLMs demonstrate extensive label bias across different models, scales and tasks (\u00a74.1  ###reference_###). We next examine techniques aimed at mitigating such bias, and assess the extent of label bias remaining after their application.\nWe report our results for Llama-2 models in Fig. 3  ###reference_###, and observe similar trends for other models (App. C.2  ###reference_###).\nWe first consider the effect of bias mitigation on model performance (Fig. 3(a)  ###reference_sf1###) using the three methods described in \u00a73.3  ###reference_###: contextual calibration (CC), domain-context calibration (DC), and few-shot fine-tuning with LoRA.\nCompared to standard prompting (black lines), we find that applying CC (orange) provides little to no gains. Moreover, it can even undermine model performance, especially for instruction-tuned models, as previously observed by Fei et al. (2023  ###reference_b9###).\nIn contrast, DC (purple) can provide substantial performance gains, especially when using few or no in-context demonstrations, where baseline performance is relatively low. However, when calibrating instruction-tuned models prompted with a higher number of demonstrations, we find that DC mostly fails to improve performance.\nFinally, LoRA considerably improves performance in all cases (green in Fig. 3  ###reference_###, upper row), vastly outperforming both CC and DC.\nWe next turn to measure label bias (Fig. 3(b)  ###reference_sf2### and 3(c)  ###reference_sf3###).\nNotably, here we observe that for the two calibration methods, changes in both RSD and BiasScore are correlated with changes in performance.\nWe find that CC substantially worsens label bias in instruction-tuned models, and can also increase bias for base models. Conversely, while DC alleviates bias in many of the evaluated settings, it is largely unsuccessful in mitigating it when prompting instruction-tuned models with  or more demonstrations. LoRA proves effective for improving RSD in all settings, but RSD values still remain relatively high. In contrast, BiasScore noticeably increases after LoRA fine-tuning, indicating that more subtle bias persists.\nOverall, our results indicate that existing bias calibration approaches are insufficient for diminishing label bias in essential cases, particularly for instruction-tuned models. Further, while LoRA fine-tuning is effective in both improving performance and mitigating certain aspects of bias (though not others),\nit is also considerably more computationally expensive than calibration."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Mitigating Label Bias by Calibrating on Demonstrations",
            "text": "Motivated by the challenges of existing calibration approaches on instruction-tuned models (\u00a74.3  ###reference_###), we aim to develop an effective calibration method for such scenarios.\nWe hypothesize a possible cause for such difficulties is that the inputs used for calibration in CC and DC are very distinct from the more curated, high-quality inputs models observe during instruction-tuning Touvron et al. (2023  ###reference_b28###).555Specifically, nonsensical task inputs made up of random words as in DC, or placeholder-like strings as in CC, are less likely to be observed during instruction tuning.\nSeeking to use more naturally-occurring inputs, and to avoid any reliance on additional held-out examples, we propose to calibrate models using the in-context examples readily available in few-shot prompts.\nWe therefore need to obtain the model\u2019s output probabilities on these inputs to estimate its bias.\nHowever, as these examples appear alongside their labels in the context provided to the model, it could simply copy the correct answer from the prompt, leading to unreliable bias estimates.\nWe introduce a simple method to alleviate this concern.\nOur goal is to estimate the model\u2019s average output probabilities  at test-time by using the  demonstrations  provided in the context , and then use it for calibration.\nDrawing from leave-one-out cross-validation, when evaluating the model on the -th demonstration\u2019s input , we prompt it with an edited context  comprised of the original context  after removing the current demonstration , resulting in  demonstrations.666We leave all other demonstrations in their original order.\nWe thus obtain  output probabilities:\nTo reliably estimate , we further need to account for the demonstrations\u2019 labels : for imbalanced choices of demonstrations (e.g., class imbalance), using the average of \u2019s could lead to an underestimation of the probability assigned to infrequent labels.\nWe therefore compute the average output probabilities  by taking into account the labels , as we do for computing BiasScore (\u00a72.3  ###reference_###): We first average \u2019s associated with the same label , , and then set  as the mean of these intra-label averages:\nFinally, we use  to compute calibration parameters and score new examples using the same methodology as Zhao et al., 2021  ###reference_b35### (\u00a73.3  ###reference_###).\nWe refer to our method as Leave-One-Out Calibration (LOOC).\nWe use LOOC to calibrate models in the same setup of \u00a74.3  ###reference_###. We report our results for Llama-2 models in Fig. 3  ###reference_### (cyan lines), finding similar trends in other models (App. C.2  ###reference_###).\nComparing our method to other calibration approaches, we find LOOC surpasses CC and DC by a wide margin in both performance and bias metrics for prompts with  demonstrations. Importantly, using LOOC to calibrate instruction-tuned models in this setting dramatically improves upon the uncalibrated model, whereas other calibration methods fail to achieve meaningful gains (\u00a74.3  ###reference_###).\nFurther, LOOC nearly closes the gap with LoRA-level performance while improving upon it in both bias metrics, yet uses substantially less compute.\nAs LOOC relies on the in-context demonstrations for bias estimation,  needs to be sufficiently large for calibration to succeed. Surprisingly, we find that with as few as  demonstrations, our method is often comparable to the next best calibration method on all metrics.\nFinally, we note that while our method can substantially reduce label bias compared to other approaches, the remaining RSD is still considerable, indicating that model performance is still biased on some tasks."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Analysis",
            "text": "We study the effect of different factors on the extent of label bias in model predictions: the semantic meaning of the task labels (\u00a76.1  ###reference_###), the level of label imbalance in the demonstrations (\u00a76.2  ###reference_###), and the choice of demonstrations (\u00a76.3  ###reference_###)."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Semantically Equivalent Labels",
            "text": "The output space for classification tasks often consists of labels with strong semantic meaning (e.g., \u201cPositive\u201d vs. \u201cNegative\u201d). Recent work has indicated that, when faced with such labels, models are affected by semantic priors from their pretraining or instruction-tuning Wei et al. (2023  ###reference_b33###); Min et al. (2022b  ###reference_b22###) that could affect label bias Fei et al. (2023  ###reference_b9###).\nWe examine whether models exhibit lower label bias when the task\u2019s labels are semantically equivalent and interchangeable.\nWe extract all multi-choice QA tasks\u2014with label spaces such as \u201cA/B/C/D\u201d or \u201c1/2/3\u201d\u2014and all sentence completion tasks, where models choose a logical continuation for an input text between two options, usually labeled A and B.\nThis results in  tasks with semantically equivalent labels.\nWe compare label bias on this subset of tasks and the entire evaluation suite for Llama-2 models in Fig. 4  ###reference_###, with results for other models largely following similar trends (App. C.3  ###reference_###).\nWe find that, in most cases, models demonstrate lower label bias on tasks with semantically equivalent labels. This is especially evident in settings with few or no demonstrations, where models are typically strongly biased (\u00a74.1  ###reference_###). Still, RSD levels for such tasks remain relatively high across all evaluated settings. Further, we observe that instruction-tuned models prompted with 8 or more demonstrations are often more biased on this subset of tasks. In summary, although using semantically equivalent labels may potentially mitigate bias in scenarios with limited demonstrations, LLMs still exhibit substantial label bias when faced with such labels."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Imbalanced In-context Demonstrations",
            "text": "Label imbalance in the in-context demonstration set was previously shown to amplify label bias Zhao et al. (2021  ###reference_b35###) as well as decrease model performance Min et al. (2022a  ###reference_b21###), but such results were derived on a restricted set of tasks.\nWe use our evaluation suite to investigate the observed label bias and performance of models when varying the level of imbalance in the demonstrations. To establish a consistent definition of label imbalance across different tasks, we use the subset of binary classification tasks () with  demonstrations.\nGiven a task with labels  and a context , we define  as the proportion of the most frequent label in the demonstrations of , such that  attains values in . Specifically,  means the labels are perfectly balanced, and  means the demonstrations only include examples for one of the labels.\nfont=small,labelfont=small\n###figure_14### ###figure_15### font=small,labelfont=small\n###figure_16### ###figure_17### ###figure_18### For every task, we prompt Llama-2 (7B/13B) and Mistral (7B) models using 10 different sets of demonstrations, with 2 sets for each value of : one where  is the most frequent label in , and another where  is the most frequent,\nas well as two different balanced sets ().777To build each set, we randomly select and permutate 8 demonstrations from a pool of 16 held-out examples, while controlling for the selected number of examples per label.\nWe group measurements taken across different tasks and demonstration sets by their level of label imbalance , and inspect the average results per level.\nWe report our results in Fig. 5  ###reference_###. Examining the two bias metrics, RSD (Fig. 5(a)  ###reference_sf1###) and BiasScore (Fig. 5(b)  ###reference_sf2###), we observe that both pretrained and instruction-tuned models are resistant to label imbalance:\nIncreased imbalance does not result in notable gains in bias, unless the imbalance is very extreme\u2014specifically, when the demonstrations include only a single or no demonstrations for one of the labels ().\nInterestingly, model performance follows the same trends (Fig. 5(c)  ###reference_sf3###).\nOverall, our results indicate that for most tasks, the impact of label imbalance in the demonstrations set is minimal, except for cases of severe imbalance."
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "Choice of Demonstrations",
            "text": "The performance of LLMs in in-context learning was shown to be sensitive to the exact choice of demonstrations used to prompt the model Liu et al. (2022  ###reference_b16###); Chang and Jia (2023  ###reference_b3###). We examine whether such choices also impact the extent of label bias in model predictions.\nWe assess the performance and bias of Llama-2 (7B/13B) and Mistral (7B) models across 5 different sets of  demonstrations for each task in our evaluation suite. In addition to reporting the mean and standard deviation of each metric, we use several oracle methods to aggregate and choose a specific demonstration set per task when computing the overall cross-task performance and bias metrics. Specifically, we select the demonstration sets that attain the following, per task: best performance; worst performance; median performance; least bias; and most bias.\nWe report our results for Llama-2 7B base in Tab. 1  ###reference_###, with other models showing similar trends (App. C.4  ###reference_###).\nWe find that label bias, similarly to model performance, is highly sensitive to the choice of demonstrations, as indicated by the high variance across sets. Interestingly, the set of demonstrations that attains the worst performance also leads to strong bias, and vice-versa. In fact, we find that performance and bias are anti-correlated, with strong Pearson correlation for RSD () and moderate for BiasScore (),\nindicating that when LLMs underperform in classification, it is often due to prompts that exacerbate bias. We leave further research into demonstrations that lead to biased and unbiased predictions to future work."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Recent work has revealed various blunders in the predictions of LLMs. Wang et al. (2023a  ###reference_b29###) showed that models exhibit positional bias when presented with several texts for evaluation and ranking. Pezeshkpour and Hruschka (2023  ###reference_b24###) and Zheng et al. (2024  ###reference_b36###) exposed a similar bias in multi-choice QA. Si et al. (2023  ###reference_b26###) studied inductive biases in in-context learning. Complimentary to these works, we study label bias and seek to improve its evaluation and mitigation. Recent work introduced calibration methods to mitigate label bias in LLMs Zhao et al. (2021  ###reference_b35###); Fei et al. (2023  ###reference_b9###). Han et al. (2023  ###reference_b11###) proposed to fit a Gaussian mixture to the model\u2019s output probabilities and use it for calibration, but their approach requires hundreds of labeled examples. Concurrently to our work, Jiang et al. (2023b  ###reference_b15###) proposed to generate inputs for calibration by conditioning models on the context prompt, and Zhou et al. (2023  ###reference_b37###) calibrate models using model output probabilities on the entire test set. While the motivation for both methods is similar to ours, our approach does not require access to the test set, or any compute to obtain inputs for calibration. Importantly, unlike previous work on bias calibration, our main focus is the evaluation of label bias in LLMs."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "The label bias of LLMs severely hinders their reliability. We considered different approaches for quantifying this bias. Through extensive experiments with ten LLMs across 279 classification tasks, we found that substantial amounts of label bias exist in LLMs. Moreover, we showed this bias persists as LLMs increase in scale, are instruction-tuned, are provided in-context examples, and even when they are calibrated against such bias.\nWe proposed a novel calibration method, which outperforms existing calibration approaches and reduces label bias dramatically.\nOur results highlight the need to better estimate and mitigate LLM biases."
        }
    ],
    "url": "http://arxiv.org/html/2405.02743v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "2.2",
            "2.3",
            "3.3",
            "5"
        ],
        "main_experiment_and_results_sections": [
            "3.1",
            "3.2",
            "3.3",
            "4",
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "3.3",
            "4.1",
            "4.2",
            "4.3",
            "5",
            "6",
            "6.1",
            "6.2",
            "6.3"
        ]
    },
    "research_context": {
        "paper_id": "2405.02743v1",
        "paper_title": "Beyond Performance: Quantifying and Mitigating Label Bias in LLMs",
        "research_background": "### Paper's Motivation\nThe motivation behind this paper stems from the recognition that while large language models (LLMs) demonstrate impressive capabilities in new task adaptation using context prompts (Wei et al., 2022; Brown et al., 2020), there is a need to address an ongoing issue with label bias in these models. Recent studies have indicated that LLMs exhibit strong, undesirable preferences towards predicting certain answers over others (Zhao et al., 2021; Chen et al., 2022; Fei et al., 2023). This label bias persists despite attempts at mitigation and calibration, which primarily focus on performance metrics like accuracy rather than directly quantifying the bias itself. There is a gap in understanding the full extent of label bias and its effects on model predictions, prompting the need for more comprehensive evaluation metrics that specifically measure this bias.\n\n### Research Problem\nThe central research problem the paper addresses is the quantification and mitigation of label bias in large language models. Existing methods predominantly assess bias through performance metrics, potentially overlooking critical aspects of model behavior. This research aims to provide a deeper understanding of label bias in LLMs by developing metrics to directly measure it and evaluating these biases across diverse tasks. Additionally, the paper seeks to assess various label bias mitigation approaches, including new methods that estimate and adjust for label bias more accurately.\n\n### Relevant Prior Work\nThe paper leverages and builds upon a substantial body of prior work:\n- **Task Adaptation in LLMs:** Studies by Wei et al. (2022) and Brown et al. (2020) demonstrated the proficiency of LLMs in adapting to new tasks using context prompts.\n- **Label Bias in LLMs:** Zhao et al. (2021), Chen et al. (2022), and Fei et al. (2023) highlighted the issue of label bias in LLM predictions, revealing that biases are influenced by in-context demonstrations (Liu et al., 2022; Lu et al., 2022), model pretraining data (Dong et al., 2022), and task data textual features (Fei et al., 2023).\n- **Bias Mitigation Approaches:** Several methods were developed to mitigate label bias, mainly through calibrating the model\u2019s output probabilities (Zhao et al., 2021; Fei et al., 2023). These approaches, however, have primarily focused on improving performance metrics.\n- **Metrics for Bias Measurement:** The paper also draws upon previous work on fairness and label bias estimation to create the metrics necessary for a more direct measurement of label bias in LLMs.\n\nBy synthesizing these prior findings and applying them to a comprehensive evaluation framework, the paper aims to advance the understanding and mitigation of label bias in LLMs, contributing to the development of more reliable language models.",
        "methodology": "**Methodology:**\n\nThe proposed method to combat label bias in Large Language Models (LLMs) hinges on calibrating the model's output probabilities after the initial inference, tailored for a particular context prompt. The process can be broken down into the following key components:\n\n1. **Label Bias Estimation:** \n   - The first step involves estimating the model\u2019s label bias. This is accomplished by analyzing the model's output probabilities on a specific set of inputs.\n   - These inputs can be:\n     - **Content-free Inputs:** Inputs that don\u2019t provide meaningful content, such as \"N/A\" or random words from the task\u2019s domain. Examples from previous works include those by Zhao et al. (2021) and Fei et al. (2023).\n     - **Ordinary Task Inputs:** Typical inputs that the model would encounter while performing actual tasks, referenced in works by Han et al. (2023).\n\n2. **Calibration Parameters Selection:**\n   - Based on the label bias estimate obtained from the previous step, calibration parameters are selected. These parameters are crucial as they will dictate the adjustment applied to the model\u2019s output probabilities.\n\n3. **Output Adjustment During Inference:**\n   - The final step is to apply these calibration parameters to adjust the original output probabilities during the inference phase.\n   - The goal of this adjustment is to yield output that is unbiased or at least significantly less biased than the original.\n\nThis method revolves around post-hoc adjustment, meaning it calibrates the model after the initial output generation rather than during the training process. The innovation lies in utilizing specific context prompts and calibration parameters to generate fairer and more balanced outputs from LLMs.",
        "main_experiment_and_results": "### Main Experiment Setup\n\nThe main experiment evaluates models on 279 diverse tasks drawn from the Super-NaturalInstructions benchmark (Wang et al., 2022). The tasks selected are all classification and multi-choice question answering tasks where the output space consists of predefined labels, such as \u201cyes/no\u201d or \u201cA/B/C\u201d. \n\nFor tasks with larger data sizes, the evaluation setup involves sampling 1,000 evaluation examples. Additionally, 32 held-out examples are sampled for computing the bias score metric, and another 64 examples are selected as a pool for choosing in-context demonstrations and LoRA fine-tuning examples. Only tasks with at least 300 evaluation examples are included in the experiments.\n\n### Datasets\n\n- **Super-NaturalInstructions Benchmark**: Comprises 279 diverse tasks with a focus on classification and multi-choice question answering.\n\n### Baselines\n\n- The main experimental comparison relies on a variety of baseline models that are detailed within the paper, although specifics on what these baselines entail are not included in the provided section.\n\n### Evaluation Metrics\n\n- **Bias Score Metric**: Used to quantify the label bias in the sampled held-out datasets.\n- Performance metrics on the classification and multi-choice question answering tasks.\n\n### Main Experimental Results\n\nThe main section does not clearly specify the exact results of these experiments within the provided extract. For detailed outcomes, the reader would need to refer to the result analysis portion of the paper, which is typically found in a different section or detailed in the appendix referenced.\n\nFor details on the selected tasks and further experimental nuances, the document suggests referring to Appendix B."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Investigate label bias in LLMs across various models and tasks, and evaluate the efficacy of different bias mitigation techniques.",
            "experiment_process": "We evaluate the effects of three label bias mitigation methods: contextual calibration (CC), domain-context calibration (DC), and few-shot fine-tuning with LoRA. We fine-tune task-specific models using Low-Rank Adapation (LoRA) on 16 held-out training examples for 5 epochs, running these on Llama-2 7B and Mistral 7B models. We measure performance using the two bias metrics, RSD and BiasScore, across these methods.",
            "result_discussion": "Applying CC shows little to no performance gains and may even worsen model performance, especially for instruction-tuned models. DC significantly improves performance, particularly with fewer demonstrations, but fails when more demonstrations are used. LoRA substantially improves performance across all settings. Although LoRA reduces RSD, BiasScore increases indicating persistent subtle bias. Overall, existing bias calibration approaches are insufficient, with LoRA providing better but computationally expensive improvements.",
            "ablation_id": "2405.02743v1.No1"
        },
        {
            "research_objective": "Develop an effective calibration method to mitigate label bias in instruction-tuned models.",
            "experiment_process": "We propose the Leave-One-Out Calibration (LOOC) method, which uses in-context demonstrations for calibration. We estimate the model\u2019s average output probabilities by prompting it with an edited context that excludes the current demonstration. We then compare LOOC with other calibration methods (CC and DC) and measure their effect on RSD and BiasScore. The experiments are conducted on Llama-2 models and are repeated using different numbers of demonstrations.",
            "result_discussion": "LOOC effectively reduces label bias compared to CC and DC, significantly improving both performance and bias metrics when using demonstrations. It almost matches LoRA-level performance while requiring much less compute. Surprisingly, even with a few demonstrations, LOOC proves comparable to the other methods. However, substantial label bias persists, indicating a need for further improvement.",
            "ablation_id": "2405.02743v1.No2"
        },
        {
            "research_objective": "Explore the impact of semantically equivalent labels on the label bias of LLMs.",
            "experiment_process": "We compare label bias on tasks containing semantically equivalent labels (e.g., multiple-choice questions labeled \u2018A/B/C/D\u2019) and the entire evaluation suite. Llama-2 models are used, and the RSD and BiasScore metrics are evaluated across these different task-types.",
            "result_discussion": "Models generally exhibit lower label bias on tasks with semantically equivalent labels, especially with few or no demonstrations. Despite this, RSD levels remain relatively high. Instruction-tuned models with more demonstrations show increased bias on these tasks, indicating that LLMs still harbor significant label bias even with semantically equivalent labels.",
            "ablation_id": "2405.02743v1.No3"
        },
        {
            "research_objective": "Assess the impact of label imbalance in in-context demonstrations on label bias and model performance.",
            "experiment_process": "We investigate label bias and performance by varying the imbalance level in in-context demonstrations using binary classification tasks. We prompt Llama-2 (7B/13B) and Mistral (7B) models using 10 different sets of demonstrations with varying levels of imbalance and measure the average results per imbalance level.",
            "result_discussion": "Both pretrained and instruction-tuned models are resistant to label imbalance, with significant bias only arising under extreme imbalance conditions (e.g., when demonstrations heavily favor one label). Model performance trends similarly, suggesting minimal impact of label imbalance except in severe cases.",
            "ablation_id": "2405.02743v1.No4"
        },
        {
            "research_objective": "Examine how the choice of in-context demonstrations affects label bias in LLMs.",
            "experiment_process": "We assess LLM performance and bias using five different sets of demonstrations for each task. We report mean and standard deviation of each metric and use oracle methods to aggregate and select specific demonstration sets per task. Llama-2 (7B/13B) and Mistral (7B) models are evaluated.",
            "result_discussion": "Label bias is highly sensitive to the choice of demonstrations, with strong anti-correlation between model performance and bias. Demonstration sets resulting in the worst performance also lead to the highest bias, indicating that certain demonstrations may exacerbate bias significantly.",
            "ablation_id": "2405.02743v1.No5"
        }
    ]
}