{
    "title": "LLM Attributor: Interactive Visual Attribution for LLM Generation",
    "abstract": "While large language models (LLMs) have shown remarkable capability to generate convincing text across diverse domains, concerns around its potential risks have highlighted the importance of understanding the rationale behind text generation.\nWe present LLM Attributor, a Python library that provides\ninteractive visualizations for training data attribution of an LLM\u2019s text generation.\nOur library offers a new way to quickly attribute an LLM\u2019s text generation to training data points to inspect model behaviors, enhance its trustworthiness,\nand compare model-generated text with user-provided text.\nWe describe the visual and interactive design of our tool and highlight\nusage scenarios for LLaMA2 models fine-tuned with two different datasets: online articles about recent disasters and finance-related question-answer pairs.\nThanks to LLM Attributor\u2019s broad support for computational notebooks, users can easily integrate it into their workflow to interactively visualize attributions of their models.\nFor easier access and extensibility, we open-source\nLLM Attributor at https://github.com/poloclub/LLM-Attribution.\nThe video demo is available at https://youtu.be/mIG2MDQKQxM.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large language models (LLMs) have recently garnered significant attention thanks to their remarkable capability to generate convincing text across diverse domains Touvron et al. (2023  ###reference_b34###).\nTo tailor the outputs of these models to specific tasks or domains, users fine-tune pretrained models with their own training data.\nHowever, significant concerns persist regarding potential risks, including hallucination Zhang et al. (2023  ###reference_b37###), dissemination of misinformation Pan et al. (2023  ###reference_b24###); Zhou et al. (2023  ###reference_b38###), and amplification of biases Kotek et al. (2023  ###reference_b19###).\nFor example,\nlawyers have been penalized by federal judges for citing non-existent LLM-fabricated cases in court filings Strom (2023  ###reference_b32###).\nTherefore, it is crucial to discern and elucidate the rationale behind LLM text generation.\nThere have been several attempts to understand reasoning behind LLM text generation. Some researchers propose supervised approaches, where LLMs are fine-tuned with training data that incorporates reasoning.\nHowever, the requirement for reasoning for every training data point poses scalability challenges across diverse tasks.\nExplicitly prompting for reasoning (e.g., \u201c[Question] Provide evidence for my question\u201d) has also been presented, but LLMs often create fake references that do not exist Zuccon et al. (2023  ###reference_b39###).\nMoreover, these methods provide\nlimited solutions\nfor incorrect model behavior Worledge et al. (2023  ###reference_b35###).\nTo complement these shortcomings,\nidentifying the training data points highly responsible for LLMs\u2019 generation has been actively explored Kwon et al. (2023  ###reference_b20###); Park et al. (2023  ###reference_b25###); Grosse et al. (2023  ###reference_b11###).\nHowever,\nwhile theoretical advancements have been made in developing and refining such algorithms,\nthere has been little research on how to present the attribution results to people.\nTo fill this research gap,\nwe present LLM Attributor, which makes following major contributions:\nLLM Attributor, a Python library for visualizing training data attribution of LLM-generated text.\nLLM Attributor offers LLM developers a new way to quickly\nattribute LLM\u2019s text generation to specific training data points\nto inspect model behaviors and enhance its trustworthiness.\nWe improve the recent DataInf algorithm to\nadapt to real-world tasks with free-form prompts,\nand enable users to interactively select specific phrases in LLM-generated text and easily visualize their training data attribution\nusing a few lines of Python code. (\u00a7 3  ###reference_###, Fig. 2  ###reference_###)\nNovel interactive visualization of side-by-side comparison of LLM-generated and user-provided text.\n\nUsers can easily modify text generated by LLMs and perform a comparative analysis to observe\nthe impact of these modifications on attribution\nusing LLM Attributor\u2019s interactive visualization.\nThis empowers users to gain comprehensive insights into why LLM-generated text often has the predominance over user-provided text\nthrough high-level analysis across the entire training data and low-level analysis focusing on individual data points.\n(\u00a7 3.3  ###reference_###, Fig. 1  ###reference_###)\nOpen-source implementation with broad support for computational notebooks.\nUsers can seamlessly integrate LLM Attributor into their workflow thanks to its compatibility with various computational notebooks, such as Jupyter Notebook/Lab, Google Colab, and VSCode Notebook, and\neasy installation via the Python Package Index (PyPI) repository111https://pypi.org/project/llm-attributor  ###reference_###.\nFor easier access and further extensibility to quickly accommodate the rapid advancements in LLM research,\nwe open-source our tool at https://github.com/poloclub/LLM-Attributor  ###reference_###.\nThe video demo is available at https://youtu.be/mIG2MDQKQxM  ###reference_youtu.be/mIG2MDQKQxM###."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Training Data Attribution",
            "text": "Training data attribution (TDA), which identifies the training data points least responsible for model behaviors, has been actively explored thanks to its wide-ranging applications, including model interpretations Madsen et al. (2022  ###reference_b22###) and debugging Koh and Liang (2017  ###reference_b18###); Pruthi et al. (2020  ###reference_b27###); Grosse et al. (2023  ###reference_b11###). While some researchers have estimated the impact of individual training data points on model performance Ghorbani and Zou (2019  ###reference_b9###); Ilyas et al. (2022  ###reference_b14###); Han and Tsvetkov (2022  ###reference_b13###) and training loss Pruthi et al. (2020  ###reference_b27###); Guu et al. (2023  ###reference_b12###), others have attempted to scale influence functions Cook and Weisberg (1980  ###reference_b6###), a classical gradient-based method, to non-convex deep models Koh and Liang (2017  ###reference_b18###). Recent efforts have been dedicated to adapt these methods to large generative models, primarily focusing on improving their efficiency Park et al. (2023  ###reference_b25###); Grosse et al. (2023  ###reference_b11###); Kwon et al. (2023  ###reference_b20###). Inspired by the advancements in TDA algorithms and their significant potential to enhance transparency and reliability of LLMs, we develop LLM Attributor to empower LLM developers to easily inspect their models via interactive visualization."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Visualization for LLM Attribution",
            "text": "While there have been\nvarious tools\naiming to visualize attributions of non-generative language models DeYoung et al. (2019  ###reference_b7###); Feldhus et al. (2021  ###reference_b8###); Attanasio et al. (2023  ###reference_b2###).\nrecent\nefforts have been made to develop\nvisual attributions tailored for\ngenerative LLMs Pierse (2021  ###reference_b26###); Sarti et al. (2023  ###reference_b29###); Google (2024  ###reference_b10###); Tenney et al. (2020  ###reference_b33###).\nTransformers-Interpret Pierse (2021  ###reference_b26###),\nInSeq Sarti et al. (2023  ###reference_b29###), and LIT Google (2024  ###reference_b10###); Tenney et al. (2020  ###reference_b33###) visually highlight important segments of the input prompt, while\nEcco Alammar (2021  ###reference_b1###) visualizes neuron activations and token evolution across model layers to probe model internals.\nHowever, these methods that attribute model behaviors solely relying on the input prompt\nare not sufficient to explain the text generations of LLMs, whose behaviors are intricately linked to the training data Worledge et al. (2023  ###reference_b35###). To fill this gap,\nwe develop interactive visualizations for training data attribution\n(\u00a7 2.1  ###reference_###)."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "System Design",
            "text": "###figure_1### LLM Attributor is an open-source Python library to help LLM developers\neasily visualize the training data attribution of their models\u2019 text generation in various computational notebooks.\nLLM Attributor can be easily installed with a single-line command (pip install llm-attributor).\nLLM Attributor consists of two views,\nMain View (\u00a7 3.2  ###reference_###, Fig. 2  ###reference_###) and Comparison View (\u00a7 3.3  ###reference_###, Fig. 1  ###reference_###).\nThe Main View offers easy-to-use interactive features to easily select specific tokens from the generated text\nand visualizes their training data attribution.\nThe Comparison View allows users to modify LLM-generated text and observe how the attribution changes accordingly\nfor a better understanding of the rationale behind model\u2019s generation."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Data Attribution Score",
            "text": "LLM Attributor evaluates the attribution of a generated text output to each training data point based on the DataInf Kwon et al. (2023  ###reference_b20###) algorithm for its superior efficiency and performance.\nIn a nutshell,\nDataInf estimates how upweighting each training data point during fine-tuning would affect the probability of generating a specific text output. To be specific, upweighting a training data point changes the total loss across the entire training dataset, thereby affecting the model convergence and the text generation probability.\nDataInf assesses the attribution score of each training data point by deriving a closed-form equation, which involves the gradient of loss for the data point with respect to the model parameters.\nHowever, while DataInf excels on custom datasets where all test prompts closely resemble the training data, we observe its limited performance when applied to more general tasks with free-form prompts.\nThis performance degradation primarily arises from the significant impact of the ordering of training data points on the gradients of model parameters Bengio et al. (2009  ###reference_b3###); Chang et al. (2021  ###reference_b5###).\nTo mitigate the undesirable effects of training data ordering,\nwe randomly shuffle training data points every few iterations (e.g., at each epoch)\nand save checkpoint models at each data shuffling to use multiple checkpoint models for score evaluation, extending the reliance beyond the final model. We aggregate scores from these checkpoint models by computing their median.\nAs attribution scores can be either positive or negative, in this paper,\nwe refer to training data points with large positive scores as positively attributed\nand those with large negative scores as negatively attributed.\nFor better time efficiency,\nLLM Attributor includes a preprocessing step that saves the model parameter gradients for each training data point and checkpoint model before the first attribution of a model.\nAs these gradient values are unchanged unless there are updates to the model weights or training data, this preprocessing removes the overhead of evaluating the gradient for every training data point during each attribution.\nLLM Attributor automatically performs the preprocessing at the first attribution of a model; users can also manually run the preprocess function to save the gradient values.\nIt is noteworthy that\nLLM Attributor can be easily extended to other TDA methods Park et al. (2023  ###reference_b25###); Grosse et al. (2023  ###reference_b11###) as long as they compute attribution scores for a token sequence for each training data point.\nUsers can integrate new methods by simply adding a function;\nwe have implemented the TracIn Pruthi et al. (2020  ###reference_b27###) algorithm as a reference."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Main View",
            "text": "The Main View offers a comprehensive visualization of training data attribution for text generated by an LLM (Fig. 2  ###reference_###).\nUsers can access the Main View by running the attribute function,\nspecifying the prompt and generated text as input arguments.\nUsers can also narrow down their focus on particular phrases by supplying the corresponding token indices as an input argument to the function.\nTo help users easily identify the token indices for the phrases of their interest, LLM Attributor provides select_tokens function,\nenabling users to interactively highlight phrases and retrieve their token indices (Fig. 2  ###reference_###A).\nThe Main View presents training data points with the highest and lowest attribution scores for the generated text (Fig. 2  ###reference_###B); high attribution scores indicate strong support for the text generation (positively attributed), while low scores imply inhibitory factors (negatively attributed).\nBy default,\ntwo most positively attributed and two most negatively attributed data points are displayed;\nusers can increase the number of displayed data points up to ten using a drop-down menu.\nFor each data point, we show its index, attribution score, and the initial few words of its text.\nClicking on the data point reveals its additional details,\nincluding the full text and metadata provided in the dataset (e.g., source URL).\nOn the right side,\nLLM Attributor shows ten keywords from\nthe displayed positively attributed points and ten from the displayed negatively attributed points,\nextracted using the TF-IDF technique Sparck Jones (1972  ###reference_b30###).\nWhen users hover over each keyword, the data points containing the word are interactively highlighted,\nfacilitating effortless identification of such data points.\nAdditionally,\nthe distribution of\nattribution scores across all training data are summarized as a histogram, which can be interactively explored by hovering over each bar to highlight its associated data points,\nenabling both high-level analysis over the entire training data and low-level analysis for individual data points."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Comparison View",
            "text": "###figure_2### The Comparison View offers a side-by-side comparison of attributions\nbetween LLM-generated and user-provided text\nto help users gain a deeper understanding of the rationale behind their models\u2019 generations Jacovi et al. (2021  ###reference_b15###); Yin and Neubig (2022  ###reference_b36###); Kotek et al. (2023  ###reference_b19###); Kahng et al. (2024  ###reference_b16###). For example, when an LLM keeps generating biased text, developers can compare it with alternative unbiased text outputs to understand\nthe factors contributing to the predominance of biased text Kotek et al. (2023  ###reference_b19###).\nWhile users can directly provide the text to compare,\nLLM Attributor also enables users to interactively edit model-generated text instead of writing text from scratch.\nThis feature is particularly useful when users need to make minor modifications within a very long LLM-generated text.\nBy running the text_edit function,\nusers can easily add, delete, and edit words in the model-generated text and\nobtain a string that can be directly fed into the compare function as an input argument (Fig. 3  ###reference_###).\nIn the Comparison View, LLM-generated text consistently appears on the left in blue, while user-provided text is shown on the right in orange (Fig. 1  ###reference_###).\nFor each text, users can see the\ntraining data points with the highest and lowest attribution scores;\ntop two and bottom two data points are shown by default, which can be interactively increased up to ten (Fig. 1  ###reference_###B).\nBelow the data points, we present ten TF-IDF keywords that summarize the displayed data points (Fig. 1  ###reference_###C).\nAdditionally, for more high-level comparison across the entire training data, we present a dual-sided histogram summarizing the distribution of attribution scores for both LLM-generated and user-provided text (Fig. 1  ###reference_###D)."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Usage Scenarios",
            "text": "We present usage scenarios for LLM Attributor,\naddressing two datasets that vary in domain and data structure,\nto demonstrate\n(1) how an LLM developer can pinpoint the reasons behind a model\u2019s problematic generation (\u00a7 4.1  ###reference_###)\nand\n(2) how LLM Attributor assists in identifying the sources of LLM-generated text (\u00a7 4.2  ###reference_###)."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Understand Problematic Generation",
            "text": "Megan, an LLM developer, received a request from disaster researchers to create a conversational knowledge base.\nSince ChatGPT OpenAI (2023  ###reference_b23###) lacked updated information beyond its release in July 2023,\nshe fine-tuned the LLaMA2-13B-Chat model Touvron et al. (2023  ###reference_b34###), using a dataset of online articles about disasters that occurred after August 2023, and shared the model with the researchers.\nHowever, several researchers reported that the model generated a conspiracy theory that the 2023 Hawaii wildfires were caused by directed-energy weapons Sardarizadeh and Wendling (2023  ###reference_b28###).\nTo understand why the model generates such misinformation, Megan decides to use LLM Attributor and swiftly installs it by typing the simple command pip install llm-attributor.\nMegan initiates her exploration by launching Jupyter Notebook Kluyver et al. (2016  ###reference_b17###) and importing LLM Attributor.\nShe first examines what other responses are generated by the model for the prompt about the cause of the 2023 Hawaii wildfires\nby using LLM Attributor\u2019s generate function\nand observes that the model occasionally yields dry weather as the answer.\nTo delve into the rationale behind the generations of dry weather and directed-energy weapons,\nMegan executes the text_edit function\nto interactively modify the model-generated text into the conspiracy theory (i.e., dry weather into directed-energy weapons, Fig. 3  ###reference_###)\nand runs the compare function (Fig. 1  ###reference_###).\nIn the Comparison View,\nMegan sees the attributions for the dry weather phrase on the left column\nand the attributions for the directed-energy weapons on the right column.\nFrom the list of training data points responsible for generating dry weather (Fig. 1  ###reference_###B), Megan notes that most of the displayed data points are not very relevant to the 2023 Hawaii wildfires.\nConversely, Megan notices that the data point #1388 in the right column, which has the highest attribution score for generating directed-energy weapons, is relevant to the Hawaii wildfires. Being curious, she clicks on this data point to expand it to more details and realizes that it is a post on X\nintended to propagate the conspiracy theory.\nMegan proceeds to the histogram to scrutinize the distribution of attribution scores across the entire training data (Fig. 1  ###reference_###D).\nShe discovers that the attribution scores for the generation of dry weather are predominantly low, being concentrated around 0,\nwhile the scores for directed-energy weapons are skewed toward positive values.\nMegan concludes that the data point #1388 is the primary reason for generating directed-energy weapons, whereas there are insufficient data points debunking the conspiracy theory or providing accurate information about the cause of the Hawaii wildfires.\nShe refines the training data by eliminating the data point #1388 and supplementing reliable articles that address the factual causes of the Hawaii wildfires\nand then fine-tunes model with the refined data.\nConsequently, the model consistently yields accurate responses (e.g., dry and gusty weather conditions), without producing conspiracy theories."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Identify Sources of Generated Text",
            "text": "Louis, a technologist at a college, is planning to develop\nan introductory finance course for students not majoring in finance. Intrigued by the potential of LLMs in course development Sridhar et al. (2023  ###reference_b31###),\nLouis decides to leverage LLMs for his course preparation.\nTo adjust the LLaMA2-13B-Chat model to the finance domain,\nhe fine-tunes the model with the wealth-alpaca-lora dataset Bharti (2023  ###reference_b4###), an open-source dataset with finance-related question-answer pairs.\nHowever, before integrating the model into his course,\nhe needs to ensure its correctness and decides to attribute each generated text using LLM Attributor.\nAs the course will cover stocks as the first topic,\nLouis prompts the question, \u201cWhat does IPO mean in stock market?\u201d,\nand the model generates a paragraph elucidating the concept of an Initial Public Offering (IPO).\nWhile most of the content in the description appears convincing, Louis wants to ensure the correctness of the IPO\u2019s definition. To specifically focus on the term definition within the long model-generated paragraph,\nhe runs the select_tokens function and highlights the tokens for the acronym expansion and definition by interacting with his mouse cursor (Fig. 2  ###reference_###A).\nAfter retrieving the indices of the selected tokens, Louis proceeds by running the attribute function,\nwhich displays the Main View, offering a visualization of the training data attribution result (Fig. 2  ###reference_###B).\nHe notices the two most positively attributable training data points, #273 and #545, would\nhave contributed to\ngenerating the text for IPO\u2019s definition.\nWhile browsing the important words shown on the right side,\nLouis\u2019s attention is drawn to the word ipo.\nHovering over this word, he discovers that the data point #273 contains the word ipo and decides to look into its contents more closely.\nClicking on the data point #273,\nLouis expands it to view its whole text,\nwhich is a question-answer pair: \u201cWhy would a stock opening price differ from the offering price?\u201d and its corresponding response.\nUpon inspection,\nLouis uncovers that the response clarifies the definition of IPO (\u201cIPO from Wikipedia states\u2026\u201d) while explaining offering price, which also aligns with the definition in the model-generated text.\nFrom this validation,\nLouis is now confident about the credibility of the generated text and decides to incorporate it into his course material."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion and Future Work",
            "text": "We present LLM Attributor, a Python library for visualizing the training data attribution of LLM-generated text.\nLLM Attributor offers\na comprehensive visual summary for the training data points that contribute to LLM\u2019s text generation and\nfacilitates comparison between LLM-generated text and custom text provided by users.\nPublished on the Python Package Index, LLM developers can easily install LLM Attributor with a single-line command and integrate it into their workflow.\nLooking ahead, we outline promising future research directions to further advance LLM attribution:\nTDA algorithm evaluation. Researchers can leverage LLM Attributor to visually examine their new TDA algorithms by incorporating them into our open-source code.\nIntegration of RAG. Considering that retrieval-augmented generation (RAG) Lewis et al. (2020  ###reference_b21###) stands as another promising approach for LLM attribution, future researchers can explore adapting LLM Attributor\u2019s interactive visualizations to RAG.\nToken-wise attribution. Extending the attribution algorithms to token-level attribution Grosse et al. (2023  ###reference_b11###) and visually highlighting tokens with high attribution scores would empower users to swiftly identify important sentences or phrases within a data point without perusing the entire text."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Broader Impact",
            "text": "We anticipate that LLM Attributor will substantially contribute to the responsible development of LLMs by helping people scrutinize undesirable generations of LLMs and ensure whether the models are working as intended.\nAdditionally, our open-source library would broaden access to advanced AI interpretability techniques, amplifying its impact on responsible AI.\nHowever, it is crucial to be careful when applying LLM Attributor to tasks involving sensitive training data. In such cases, extra consideration would be essential before visualizing and sharing the attribution results."
        }
    ],
    "url": "http://arxiv.org/html/2404.01361v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3"
        ],
        "main_experiment_and_results_sections": [
            "4.1",
            "4.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.1",
            "3.2",
            "3.3",
            "4",
            "4.1",
            "4.2"
        ]
    },
    "research_context": {
        "paper_id": "2404.01361v1",
        "paper_title": "LLM Attributor: Interactive Visual Attribution for LLM Generation",
        "research_background": "Motivation:\nThe paper is motivated by the significant concerns surrounding the trustworthiness of large language model (LLM) outputs, such as hallucination, dissemination of misinformation, and amplification of biases. The urgency of addressing these issues is underlined by real-world consequences, such as legal professionals being penalized for using non-existent LLM-fabricated cases.\n\nResearch Problem:\nThe primary research problem tackled in this paper is the need for effective methods to discern and elucidate the rationale behind LLM text generation. Existing approaches either struggle with scalability or fail to provide adequate solutions for incorrect model behavior. Additionally, there is a gap in research regarding the presentation of attribution results in a way that is useful for end-users, particularly developers.\n\nRelevant Prior Work:\nThe paper builds on a body of prior work:\n1. **Supervised Approaches and Prompting for Reasoning**: These methods, which include fine-tuning LLMs with training data encompassing reasoning or explicitly prompting for reasoning, face issues like scalability challenges and the risk of generating fake references (Zhang et al., 2023; Zuccon et al., 2023).\n2. **Identifying Training Data Attribution**: Previous research has focused on theoretical advancements in developing algorithms to identify training data points responsible for LLMs' generation. However, these works have not sufficiently explored how to present these results to end-users (Kwon et al., 2023; Park et al., 2023; Grosse et al., 2023).\n\nBy drawing on these previous studies, the paper proposes a novel solution: LLM Attributor, an interactive tool for visualizing and exploring training data attribution in LLM-generated text.",
        "methodology": "**LLM Attributor: Interactive Visual Attribution for LLM Generation**\n\n**Methodology:**\n\nLLM Attributor is an open-source Python library designed to assist LLM (Large Language Model) developers in visualizing the training data attribution of their models' text generation. This tool can be seamlessly integrated into various computational notebooks and easily installed via a single-line command (`pip install llm-attributor`).\n\nThe LLM Attributor features two primary views:\n\n1. **Main View**\n   - **Functionality:** This view provides user-friendly interactive features to select specific tokens from the generated text. It visualizes the training data attribution for these tokens, making it easy for users to discern which parts of the training data contributed to the generation of particular tokens.\n\n2. **Comparison View**\n   - **Functionality:** This view enables users to modify the LLM-generated text and observe the corresponding changes in attribution. This comparison capability helps users gain a deeper understanding of the rationale behind the model's generation by comparing the attributions before and after the text modifications.\n\nThese features make LLM Attributor a valuable tool for analyzing and understanding the inner workings of large language models, especially in terms of how training data influences their generative output.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\n**Experiment Setup:**\n- **Objective:** Understand why a fine-tuned LLaMA2-13B-Chat model generates misinformation regarding the cause of the 2023 Hawaii wildfires.\n- **Datasets:** The dataset used for fine-tuning was composed of online articles about disasters that occurred after August 2023.\n- **Baselines:** A baseline checkpoint of the model before removing the erroneous data point and supplementing accurate articles.\n- **Evaluation Metrics:** The ability of the model to generate accurate responses about the causes of the 2023 Hawaii wildfires without misinformation.\n\n**Procedure:**\n1. **Assessment with LLM Attributor:**\n   - **Tool Installation:** Megan installs LLM Attributor via a simple command (`pip install llm-attributor`).\n   - **Exploration:** Using Jupyter Notebook, Megan uses the `generate` function to observe the model's responses to a prompt about the 2023 Hawaii wildfires, noting that it occasionally produces \"dry weather\" as an answer and sometimes the conspiracy theory.\n   - **Interactive Modifications:** She employs the `text_edit` function to change \"dry weather\" to \"directed-energy weapons\" and uses the `compare` function to visually compare attributions.\n   \n2. **Comparison and Attribution Analysis:**\n   - In the Comparison View, the attributions for \"dry weather\" display less relevant data points, while \"directed-energy weapons\" show data point #1388 as highly relevant.\n   - Upon expanding data point #1388, Megan identifies it as a post propagating the conspiracy theory.\n\n3. **Attribution Score Analysis:**\n   - Megan studies the distribution of attribution scores using a histogram, discovering that scores for \"dry weather\" are low, centering around 0, whereas those for \"directed-energy weapons\" are skewed toward positive values.\n\n4. **Refinement of Training Data:**\n   - Megan removes data point #1388 and adds reliable articles that accurately discuss the causes of the Hawaii wildfires. She then fine-tunes the model with the revised dataset.\n\n**Results:**\nAfter refining the training data and re-fine-tuning the model:\n- The model consistently generates accurate answers such as \"dry and gusty weather conditions\" as the cause of the Hawaii wildfires, without reverting to conspiracy theories. \n- This demonstrates the effectiveness of the refined dataset in correcting the generator's outputs and highlights the utility of LLM Attributor in identifying and rectifying sources of misinformation in the training data.\n"
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Investigate why the LLaMA2-13B-Chat model generates conspiracy theory texts, specifically the misinformation that the 2023 Hawaii wildfires were caused by directed-energy weapons.",
            "experiment_process": "An LLM developer named Megan fine-tuned the LLaMA2-13B-Chat model using a dataset of online articles about disasters occurring after August 2023. Researchers reported that the model generated a conspiracy theory about the wildfires. Megan used LLM Attributor to install, run, and inspect the text generated by the model for prompts about the cause of the 2023 Hawaii wildfires. She generated different texts like 'dry weather' and 'directed-energy weapons' and used the Comparison View to analyze the attribution scores of the phrases. This involved the use of the text_edit function to modify model-generated text into the conspiracy theory and the compare function to compare attributions.",
            "result_discussion": "The analysis revealed that the data point #1388, containing misinformation about the directed-energy weapons conspiracy, had the highest attribution score for generating the conspiracy text. The data points for 'dry weather' were noted to be irrelevant to the wildfires, showing low attribution scores. Megan concluded that the high attribution of data point #1388 caused the misinformation and fixed the issue by removing this data and supplementing factual information, resulting in the model consistently providing accurate responses.",
            "ablation_id": "2404.01361v1.No1"
        },
        {
            "research_objective": "Ensure the correctness of the LLaMA2-13B-Chat model's ability to define financial terms for integration into an introductory finance course.",
            "experiment_process": "A technologist named Louis fine-tuned the LLaMA2-13B-Chat model with the wealth-alpaca-lora dataset, containing finance-related question-answer pairs. Louis focused on the generated text describing the term 'IPO' and used the select_tokens function to highlight and retrieve token indices for the definition. The attribute function was used to display the Main View, showing training data attribution scores. Louis observed the top positively attributed training data points and keywords related to the generated text. He specifically investigated the relevant training data to validate the information provided.",
            "result_discussion": "The terms and their definitions in the generated text were validated by inspecting the positively attributed training data points, particularly data point #273, which contained an accurate definition of IPO from Wikipedia. This validation reassured Louis of the credibility of the generated text, allowing him to confidently incorporate it into his course material.",
            "ablation_id": "2404.01361v1.No2"
        }
    ]
}