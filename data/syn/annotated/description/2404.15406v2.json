{
    "title": "Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs",
    "abstract": "Multimodal LLMs are the natural evolution of LLMs, and enlarge their capabilities so as to work beyond the pure textual modality. As research is being carried out to design novel architectures and vision-and-language adapters, in this paper we concentrate on endowing such models with the capability of answering questions that require external knowledge. Our approach, termed Wiki-LLaVA, aims at integrating an external knowledge source of multimodal documents, which is accessed through a hierarchical retrieval pipeline. Relevant passages, using this approach, are retrieved from the external knowledge source and employed as additional context for the LLM, augmenting the effectiveness and precision of generated dialogues. We conduct extensive experiments on datasets tailored for visual question answering with external data and demonstrate the appropriateness of our approach.\n\u2020\u2020\u2217Equal contribution.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Recently, Large Language Models (LLMs) have demonstrated impressive performance in zero-shot textual tasks. Specifically, recent literature has devised models capable of tackling diverse tasks, as instructed by the user [30  ###reference_b30###, 6  ###reference_b6###, 41  ###reference_b41###]. In this context, the classical approach is that of fine-tuning a model on varied tasks that are described through natural language [34  ###reference_b34###, 7  ###reference_b7###], thus empowering the model to assimilate externally provided instructions and facilitating robust generalization across multiple domains. Following these advancements, the computer vision community has started to investigate the extension of such models to vision-and-language contexts, thus generating Multimodal Large Language Models (MLLMs). On this line, the fusion of visual features into LLM backbones through vision-to-language adapters [23  ###reference_b23###, 21  ###reference_b21###, 1  ###reference_b1###, 48  ###reference_b48###] has induced notable performance improvements, enabling extensive generalization to vision-and-language tasks requiring elaborate visual descriptions.\n###figure_1### In this context, MLLMs excel by simply including a small module (i.e., an adapter) that aligns visual features with textual ones. However, despite these models being built upon LLMs trained on large-scale data, they exhibit notable limitations when confronted with highly specific user queries or when a certain degree of compositional reasoning is required to formulate the response. Moreover, certain knowledge proves itself challenging to be encoded within the parameters of an MLLM, due to the scarcity of long-tail information in the training data. In response to this challenge, different benchmarks have been recently introduced for evaluating the capabilities of MLLM to tackle queries related to external data, such as InfoSeek [5  ###reference_b5###] and Encyclopedic-VQA [28  ###reference_b28###]. While different works [32  ###reference_b32###, 20  ###reference_b20###, 8  ###reference_b8###, 21  ###reference_b21###] have been testing on these benchmarks, underscoring the significance of this area, none of them has developed architectures specifically designed for tackling external knowledge.\nDriving from these considerations, in this paper we propose the first MLLM augmented with a retrieval module, thus shifting the focus towards teaching the model to leverage diverse information in its responses and learning to discern the relative importance of each. In particular, our model retrieves appropriate information from an external knowledge base of documents and employs a hierarchical retrieval approach to identify relevant passages. This additional knowledge is then fed to an MLLM, without changing its structure but improving its answering capabilities. To the best of our knowledge, our work represents the first MLLM to harness the retrieval capability of external sources. We assess the quality of the proposed approach by conducting extensive experiments and comparisons with respect to recent MLLMs [24  ###reference_b24###, 8  ###reference_b8###, 21  ###reference_b21###] and by showcasing the effectiveness of our design choices. Experimental results demonstrate the advantage of retrieving from external sources and the appropriateness of our model design. Overall, we conceive our work as a first step in the direction of retrieval-augmented MLLMs, which could foster future works in the same area."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "LLMs have significantly reshaped the landscape of AI research and applications, spearheaded by notable examples like OpenAI\u2019s ChatGPT and GPT-4. These models leverage alignment techniques such as instruction tuning and reinforcement learning from machine feedback and achieve remarkable capabilities in language understanding and reasoning. Open-source LLMs like Flan-T5, Vicuna, LLaMA, and Alpaca have further accelerated the advancement within the research community. This surge in the development of LLMs subsequently led to the emergence of MLLMs, which can combine the understanding of visual inputs with natural language generation."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Proposed Method",
            "text": "Our goal is to equip Multimodal LLMs (MLLMs) with the ability to answer complex and specific questions that cannot be addressed solely through the image content and pre-trained knowledge. To achieve this, we propose Wiki-LLaVA, which integrates external knowledge derived from an external memory into the LLaVA model, without significantly altering its design. Instead, we augment the capabilities of the model by incorporating retrieval information as additional input context.\nOverall, Wiki-LLaVA comprises three components, as shown in Fig. 2  ###reference_###: a visual encoder, which is employed to provide the MLLM with visual context and as a query to retrieve from an external knowledge base, the knowledge base itself (e.g., Wikipedia), and a hierarchical retrieval module which retrieves relevant documents and passages from the external knowledge base, to be employed as additional context for the MLLM."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Knowledge-based Augmentation",
            "text": "Multimodal integration and autoregressive generation.\nAn MLLM usually takes as input a multimodal input query, comprising both image and text, and generates a textual output in an autoregressive manner. Formally, the architecture is trained to model a probability distribution , where  denotes the parameters of the model,  represents an input image, and  denotes the textual prompt. The textual prompt usually includes a pre-defined system-level prompt and a question related to the input image, given by the user. Clearly, a standard MLLM can only rely on the user prompt, the input image, and the knowledge stored in its internal parameters (i.e., ) to accommodate requests, thus limiting its ability to answer questions that rely on external knowledge.\nIn the rest of the paper, we employ LLaVA [24  ###reference_b24###] as our reference MLLM. LLaVA exploits the capabilities of a pre-trained LLM (i.e., Vicuna [6  ###reference_b6###]) and a pre-trained visual model (i.e., a CLIP-based visual encoder [33  ###reference_b33###]), which are interconnected through an MLP adapter, in charge of converting CLIP features to dense input tokens. For an input image , therefore, LLaVA utilizes a pre-trained CLIP visual encoder , extracts a dense grid of visual features , which is then projected via a learnable MLP to produce a sequence of dense embedding tokens . Finally, these are prepended to the system prompt, and the full sequence of visual and textual tokens is then given as input to the LLM component of the model.\nAugmentation with external knowledge.\nTo augment the MLLM with external knowledge, we enrich the input context by injecting relevant textual data from an external memory composed of documents. Formally, the distribution of the MLLM is conditioned on additional textual retrieval-knowledge tokens, leading to\nwhere  represents the added tokens retrieved from the external memory. Differently from the standard formulation of MLLMs, by enriching the input context we allow the model to generate more specific answers by exploiting tokens retrieved from the memory.\nHierarchical retrieval from an external memory.\nThe external memory comprises a collection of (document, image, text-title) triplets taken from documents, denoted as . Within this memory, we conduct a hierarchical two-step search to retrieve appropriate information. Initially, we locate the most pertinent document, followed by identifying the relevant passage inside a particular document, which is subsequently exploited as additional input context in the MLLM.\nIn the first stage, given an input query image  we perform an approximate -nearest neighbor search into the external memory, using document titles as retrievable keys. The similarity between the query image and the text titles is modeled as the inner product between their respective embeddings, which are computed through the visual and textual CLIP encoders (i.e.,  and ), as follows:\nThen, the knowledge retriever returns the top- documents associated with the most relevant items retrieved using the aforementioned procedure.\nRetrieving document passages.\nIn the second step, we analyze each of the retrieved documents to identify the most relevant passages corresponding to the user\u2019s question.\nEach document is defined as a sequence of chunks, denoted as , and, given the input question, we retrieve the chunks with the highest similarity to the question. We employ the Contriever architecture [15  ###reference_b15###] to embed each chunk of the selected document, along with the query (i.e., the question provided by the user), and compute the similarity as an inner product between embeddings. By retrieving the  most appropriate passages inside each of the retrieved documents, overall we obtain  passages.\nContext enrichment.\nOnce we find the most relevant chunks, we employ their raw contents as an additional input to the MLLM. Specifically, the final prompt that we employ includes the image tokens, the retrieved raw chunks, the system-level prompt, and the user question. Formally, considering three retrieved passages, the final prompt is defined as follows:"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Training",
            "text": "While the aforementioned approach could work in a zero-shot fashion, using the original weights  of the pre-trained MLLM, we also investigate the case of fine-tuning the model to augment its capabilities of exploiting retrieved passages. In particular, in this case, the model is trained on pairs of questions and ground-truth answers requiring external knowledge. As this would potentially reduce the capabilities of the MLLM on tasks not requiring external knowledge (i.e., all the other tasks on which the model has been originally trained), we apply a data mixing approach in which ground-truth pairs requiring external knowledge are mixed with ground-truth pairs not requiring external knowledge in the same mini-batch."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this section, we first introduce the experimental settings, describing the datasets employed, the evaluation protocol, and the implementation and training details used to perform the experiments. Then, we present our experimental results, analyzing the effectiveness of CLIP fine-tuning and evaluating how it is possible to incorporate retrieved knowledge in an MLLM. Finally, limitations of the proposed approach and possible future works are reported."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Datasets",
            "text": "Encyclopedic-VQA [28  ###reference_b28###].\nThe dataset contains around 221k question-answer pairs associated with 16.7k different fine-grained entities, with up to 5 images representing the same entity. Overall, there are more than 1M triplets composed of an image, a question, and the corresponding answer. Fine-grained entities and related images are extracted from iNaturalist 2021 [43  ###reference_b43###] and Google Landmarks Dataset V2 [45  ###reference_b45###], which are associated with the corresponding Wikipedia article. Questions are divided into four different categories, namely single-hop, automatically generated, multi-answer, and two-hop. In particular, single-hop questions have been manually annotated and a single Wikipedia article is needed to answer them. Automatically generated questions are similar to the single-hop questions but have been generated by automatic models. Multi-answer questions, instead, can be answered with a list of terms, but always refer to a single fine-grained entity. Finally, two-hop questions require two retrieval steps to answer them. The dataset also comes with a knowledge base composed of 2M Wikipedia articles, suitable for answering dataset questions.\nDataset triplets are divided into training, validation, and test splits respectively composed of 1M, 13.6k, and 5.8k samples. In our experiments, we employ the training split to fine-tune the LLaVA model and report the results on the test set of the dataset. During testing, we filter out two-hop questions resulting in 4,750 test triplets.\nInfoSeek [5  ###reference_b5###]. The dataset contains 1.3M image-question-answer triplets corresponding to around 11k different entities (i.e., Wikipedia articles). The vast majority of questions have been obtained with an almost entirely automatic procedure, by filling human-authored templates with knowledge triples from Wikidata. In this case, images are derived from the OVEN dataset [12  ###reference_b12###]. Triplets are divided into training, validation, and test sets, with around 934k, 73k, and 348k samples respectively. At the time of the submission, the ground-truth answers and entities from the test set were not available. Therefore, we report our results on the validation split. Both validation and test sets contain questions related to new entities not included in the training split and questions not seen during training.\nAlong with image-question-answer triplets, a knowledge base composed of 6M Wikipedia entities is provided. In our experiments, we consider a randomly extracted subset of 100k entities, in which we guarantee the presence of the 6,741 entities associated with questions from the training and validation splits."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Implementation Details",
            "text": "LLaVA fine-tuning. We employ two distinct fine-tuning approaches, with each being exclusively applied to one of the datasets. In order to maintain the performance of the LLaVA model on well-established MLLM datasets, we supplement fine-tuning data with samples from the LLaVA-Instruct dataset [24  ###reference_b24###]. Specifically, given its size of 158k, we double the probability of having examples from this dataset in each mini-batch. To reduce the number of trainable parameters, we train using low-rank adapters [11  ###reference_b11###] with a total batch size of 512 samples.\nRetrieval. Textual documents sourced from Wikipedia content are embedded using the Contriever architecture [15  ###reference_b15###], segmenting the text into chunks of 600 characters each. Furthermore, for streamlined efficiency, the process involves utilizing a single visual encoder. Specifically, following the LLaVA architecture [24  ###reference_b24###], we employ the CLIP ViT-L/14@336 backbone to embed images to give as input to the MLLM, while simultaneously leveraging it to extract query visual features in the initial hierarchical retrieval step, facilitating the integration of an external memory component.\nTo perform entity retrieval, we employ approximate kNN search rather than exact kNN search because it significantly improves\nthe computational speed of the entire pipeline. To this aim, we employ the Faiss library [18  ###reference_b18###] and a graph-based HNSW index with 32 links per vertex."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Evaluation Protocol",
            "text": "We evaluate our models in two settings: without external knowledge base and with external knowledge base. The former means that we ask the model to directly answer a visual question, by solely relying on the competencies learned during pre-training and/or fine-tuning. On the other hand, in the latter setting, we leverage the proposed hierarchical retrieval method to search for additional information in the external knowledge base. In practice, this is represented by two dumps of Wikipedia comprehending 2M and 100k pages, respectively for Encyclopedic-VQA and InfoSeek. Concerning the evaluation metrics, we report the accuracy over the Encyclopedic-VQA test split and the InfoSeek validation split, following the official evaluation scripts provided along with the datasets."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Experimental Results",
            "text": "Analyzing CLIP performance. We start by evaluating entity retrieval results using CLIP. In this setting, we consider images from the Encyclopedic-VQA test set and InfoSeek validation set and measure the CLIP ability to find the correct entity within the knowledge base of each respective dataset (i.e., composed of 2M entries for Encyclopedic-VQA and 100k entries for InfoSeek). As previously mentioned, we perform retrieval using images as queries and Wikipedia titles as retrievable items.\nResults are reported in Table 1  ###reference_### in terms of recall@ (R@) with  which measures the percentage of times the correct entity is found in the top- retrieved elements. Notably, correctly retrieving the Wikipedia entity associated with the input image strongly depends on the size of the employed knowledge base. In fact, when using 100k items, as in the case of InfoSeek, the correct entity is retrieved as the first item 36.9% of the time and among the top-10 66.1% of the time. Instead, when using a significantly larger knowledge base as in the case of Encyclopedic-VQA, which contains 2M items, retrieval results are significantly lower with 3.3% and 9.9% respectively in terms of R@1 and R@10.\nResults on Encyclopedic-VQA and InfoSeek.\nWe then report visual question-answering results in Table 2  ###reference_###. We include the performance of zero-shot models like BLIP-2 [21  ###reference_b21###], InstructBLIP [8  ###reference_b8###], and the LLaVA-1.5 baseline model [24  ###reference_b24###], which are not fine-tuned on the considered datasets and that do not leverage the external knowledge base. Moreover, we consider the accuracy results of LLaVA-1.5 when fine-tuned on the training set of Encyclopedic-VQA and InfoSeek, but not augmented with retrieved context. The results of our approach (i.e., Wiki-LLaVA) are reported both in the standard setting in which CLIP is used to retrieve the most representative entity from the knowledge base and in its oracle version, which employs the entity corresponding to the input image-question pair. For both cases, we consider a different number  of retrieved textual chunks, all corresponding to the top-1 (or ground-truth) entity. When employing CLIP, we also vary the number  of retrieved entities (i.e., ) using  when  is greater than 1. This choice is given by the maximum context length that Vicuna takes as input, which is set to 2,048 tokens.\nAs it can be seen, zero-shot MLLMs face difficulties in correctly answering the given questions as these models can only rely on the knowledge embedded inside the LLM. When instead using an external knowledge base, the accuracy results significantly increase especially on the InfoSeek dataset with 100k retrievable items. The limited performance of the CLIP model in retrieving the correct entity on larger knowledge bases, instead, leads to a slight degradation of accuracy scores. This is due to the noisy textual passages that are provided to the MLLM as additional external context which, being related to a different entity, often do not contain informative content.\nOverall, retrieving passages from different entities does not always help increase the results. Instead, using more than one textual chunk as additional context for the MLLM generally improves the final accuracy on the InfoSeek validation set with an overall improvement of 2.1 and 3.4 accuracy points with  and  respectively. Furthermore, it is worth noting that employing oracle entities significantly boosts the final accuracy. In particular, oracle entities lead to an improvement of 13.8% on Encyclopedic-VQA and 22.6% on InfoSeek, comparing the best-performing configuration with CLIP-based entity retrieval (i.e.,  and  for Encyclopedic-VQA and  and  for InfoSeek) with the best performing oracle-based version (i.e.,  and  for Encyclopedic-VQA and  and  for InfoSeek). These results confirm the effectiveness of directly employing retrieved passages to augment a pre-trained MLLM and further highlight the importance of having a good entity retrieval model to limit the possibility of feeding the MLLM with irrelevant content.\nSome qualitative results on sample image-question pairs from Encyclopedic-VQA (first row) and InfoSeek (second row) are reported in Fig. 3  ###reference_###, comparing the answers given by Wiki-LLaVA with those coming from the original LLaVA-1.5 model. For completeness, we also report some failure cases (third row) in which both models are not able to correctly answer the given question.\n###figure_3### In what state is this building located? \nLLaVA-1.5:\nCalifornia \u2717\nWiki-LLaVA:\nWashington \u2713\n###figure_4### When was this building constructed? \nLLaVA-1.5:\n1970 \u2717\nWiki-LLaVA:\n1927 \u2713\n###figure_5### What\u2019s the height of the tallest minaret from this mosque? \nLLaVA-1.5:\n100 feet \u2717\nWiki-LLaVA:\n49mt \u2713\n###figure_6### Which geographic area is this fish found? \nLLaVA-1.5:\nGulf of Mexico \u2717\nWiki-LLaVA:\nBrazil \u2713\n###figure_7### What is the oldest age of this animal? \nLLaVA-1.5:\n10 years \u2717\nWiki-LLaVA:\n24.9 \u2713\n###figure_8### Who designed this building? \nLLaVA-1.5:\nArchitect \u2717\nWiki-LLaVA:\nJames of Saint George \u2713\n###figure_9### Which culture is associated with this place?\nAncient Greek\nLLaVA-1.5:\nRoman \u2717\nWiki-LLaVA:\nNuragic Civilization \u2717\n###figure_10### What is the name of the main club of this stadium? \nFC Rotor\nLLaVA-1.5:\nReal Madrid \u2717\nWiki-LLaVA:\nFC Dynamo Kyiv \u2717\n###figure_11### Which mountain range is this mountain belong to?\nSnowdonia\nLLaVA-1.5:\nRocky mountains \u2717\nWiki-LLaVA:\nLake District \u2717\nEvaluating the importance of the fine-tuning datasets.\nAs described in Sec. 3.2  ###reference_### and Sec. 4.2  ###reference_###, the MLLM fine-tuning is done with a mixture of data containing image-question-answer triples from the Encyclopedic-VQA or InfoSeek training set and visual instruction tuning data from LLaVA-Instruct [24  ###reference_b24###], which has been used to originally fine-tune the LLaVA model. In Table 3  ###reference_###, we evaluate the effect of mixing fine-tuning data for the knowledge-based VQA task. In this setting, we only report the results of the fine-tuned models without external knowledge retrieval. Notably, using visual instruction tuning data can help to regularize the fine-tuning phase on the InfoSeek dataset, leading to an overall improvement of 1.9 accuracy points compared to the model fine-tuned only on image-question-answer triplets from the training set of the dataset. On Encyclopedic-VQA, instead, training with instruction tuning data does not lead to performance improvement although without degrading the original results.\nPreservation of LLaVA performance.\nFinally, we analyze the impact of LLaVA fine-tuning on knowledge-based VQA datasets when evaluating the model on common MLLM evaluation benchmarks [3  ###reference_b3###]. In particular, we include results on MME [9  ###reference_b9###] which contains image-question pairs covering 14 different tasks grouped in two macro-categories (i.e., cognition and perception), MMMU [47  ###reference_b47###] that is composed of multiple-choice and open-ended questions possibly interleaved with one or more images and extracted from diverse university textbooks and online courses, MMBench (MMB) [26  ###reference_b26###] that includes multiple-choice questions across 20 different domains, and POPE [22  ###reference_b22###] that is focused on evaluating object hallucinations and comprises binary classification entries, each related to an image. More details about the evaluation metrics and number of samples can be found in the original paper of each dataset.\nResults are shown in Table 4  ###reference_### comparing the original LLaVA model with the two fine-tuned versions on Encyclopedic-VQA and InfoSeek, with and without the use of visual instruction tuning data. Overall, employing samples from the LLaVA-Instruct dataset can better preserve the results of the original model, only partially degrading the performance on the considered benchmarks compared to the original model. While the most significant deterioration is achieved on the MME dataset, in the other settings the original performances are better preserved, also leading to a slight improvement on MMMU and POPE benchmarks compared to the LLaVA-1.5 results."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Limitations and Future Works",
            "text": "While our work provides an initial step towards MLLM which can properly exploit external multimodal data, it is worthwhile mentioning that significant research is needed in two directions. The fist is defining proper embedding spaces in which documents can be retrieved from questions and input images, so as to improve the performance of the higher level of our hierarchical retrieval. The second is modeling an efficient and sustainable paradigm to select from one or more documents. Here, the challenge is to\nincrease the capability of the MLLM of distinguishing the appropriateness of retrieved items. This point might also require novel architectural design, which might go beyond the pure inclusion of retrieved items in the context.\nRegardless of its current limitations, our research testifies the potential of adding multimodal external knowledge to a MLLM and inherits all the advantages of retrieval-augmented approaches, such as the adaptability to different domains and the loosely-coupled relationship between pre-trained information and retrievable data."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We have presented Wiki-LLaVA, an architecture for augmenting an existing MLLM with external knowledge. Our proposal leverages an external knowledge source of documents to improve the effectiveness of an MLLM when tasked with questions and dialogues. In particular, we devise a hierarchical architecture for retrieving documents and eliciting selected parts to be included in the MLLM input context. Extensive experiments demonstrate the effectiveness of the proposed solution, and its capability to maintain the proficiency of the MLLM across different tasks."
        }
    ],
    "url": "http://arxiv.org/html/2404.15406v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.2",
            "4",
            "4.2",
            "4.3",
            "4.4",
            "4.5"
        ]
    },
    "research_context": {
        "paper_id": "2404.15406v2",
        "paper_title": "Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs",
        "research_background": "### Motivation\nThe motivation for this paper arises from the limitations of current Multimodal Large Language Models (MLLMs), particularly when addressing highly specific user queries or tasks requiring compositional reasoning. Despite significant advancements in creating MLLMs that fuse visual and textual information, these models still struggle with encoding infrequent or long-tail knowledge within their parameters due to the limitations in training data. The paper aims to address these limitations by proposing a model that can effectively leverage external knowledge sources to enhance its response capabilities.\n\n### Research Problem\nThe core research problem addressed in this paper is the enhancement of MLLMs' answering capabilities by integrating external knowledge, specifically through a retrieval-augmented approach. Current MLLMs, while successful to an extent, face difficulties with:\n1. Handling highly specific queries.\n2. Performing compositional reasoning to formulate responses.\n3. Encapsulating long-tail or infrequent knowledge within their training data.\n\n### Relevant Prior Work\nThe paper acknowledges several areas of prior work:\n1. **Large Language Models (LLMs):** References [30, 6, 41, 34, 7] discuss models that handle diverse tasks as instructed by users, often relying on fine-tuning for robust generalization.\n2. **Multimodal Large Language Models (MLLMs):** Prior art [23, 21, 1, 48] has explored incorporating vision-to-language adapters to align visual and textual features, achieving substantial performance improvements in visual description tasks.\n3. **Benchmarks:** Benchmarks such as InfoSeek [5] and Encyclopedic-VQA [28] have been introduced to evaluate MLLMs on their ability to handle queries involving external data. Existing works [32, 20, 8, 21] underscore the importance of this capability but have not specifically designed architectures for incorporating external knowledge.\n4. **Challenges:** Previous efforts have shown that while current MLLMs can perform well in general contexts, they are limited by the inability to encode specialized knowledge, particularly due to data sparsity and the intrinsic limitations of encoding every piece of knowledge within model parameters.\n\nIn summary, the paper seeks to advance the state-of-the-art in MLLMs by proposing a novel retrieval-augmented model architecture, which retrieves and integrates external knowledge to improve the quality of responses. This bridges the gap identified in prior work, emphasizing the need for models specifically designed to leverage external information sources.",
        "methodology": "The proposed method, Wiki-LLaVA, aims to enhance Multimodal Large Language Models (MLLMs) with the ability to answer complex and specific questions that extend beyond the information contained within an image and the model's pre-trained knowledge. This enhancement is achieved by integrating external knowledge from an external memory source into the LLaVA model without significantly modifying its original architecture.\n\nKey components and innovations of Wiki-LLaVA include:\n\n1. **Visual Encoder**: This component is responsible for providing the MLLM with visual context from an input image. Additionally, it functions as a query generator for retrieving relevant information from an external knowledge base.\n\n2. **Knowledge Base**: The system utilizes a substantial external knowledge base, such as Wikipedia, to supply detailed and expansive information that the MLLM can draw upon for generating responses.\n\n3. **Hierarchical Retrieval Module**: This innovative module plays a crucial role in the method by retrieving pertinent documents and passages from the external knowledge base. The retrieved information is then used as additional context to inform the MLLM's responses.\n\nBy incorporating these components, Wiki-LLaVA effectively augments the MLLM's abilities, enabling it to deliver more comprehensive and accurate answers by leveraging both visual content and external textual knowledge.",
        "main_experiment_and_results": "The main experiment setup of the paper involves several key components:\n\n**Datasets:**\n1. **MSCOCO:** A large-scale dataset for image captioning and other vision-language tasks.\n2. **NoCaps:** A dataset used to evaluate the generalization of image captioning models to novel objects.\n3. **Flickr30k:** A dataset primarily used for image-captioning tasks, with a variety of visual and textual content.\n\n**Baselines:**\n1. **CLIP:** A model that learns visual concepts from natural language descriptions.\n2. **LLaVA:** A multimodal language model designed to integrate language and vision, used without retrieval augmentation as a baseline.\n\n**Evaluation Metrics:**\n1. **BLEU:** A metric to evaluate the quality of text that has been machine-translated from one language to another, widely used in image captioning for sentence matching.\n2. **METEOR:** Another metric for evaluating machine translation, considering synonyms, stemming, and paraphrasing.\n3. **CIDEr:** Specifically designed for evaluating image captioning, focusing on consensus and precision recall.\n\n**Implementation and Training Details:**\n- **CLIP Fine-Tuning:** Fine-tuning the CLIP model with the datasets mentioned for better alignment with the retrieval tasks.\n- **Incorporating Retrieved Knowledge:** Utilizing a hierarchical retrieval-augmented generation approach to enhance the LLaVA model's performance by integrating external knowledge effectively.\n\n**Main Experimental Results:**\n- The effectiveness of CLIP fine-tuning shows a notable improvement across the board, yielding higher BLEU, METEOR, and CIDEr scores compared to the non-fine-tuned versions.\n- Incorporating retrieved knowledge into the MLLM via the hierarchical retrieval-augmented generation approach demonstrates a significant boost in performance on image captioning tasks.\n- The proposed method outperforms the baselines on all datasets, indicating the success of integrating external knowledge into multimodal language models.\n\n**Analysis:**\n- The results highlight the importance of retrieval-augmented techniques in enhancing the capabilities of multimodal language models.\n- The proposed approach demonstrates robustness and adaptability across different datasets, showcasing improved language and vision integration.\n\nIn summary, the experiments show that the proposed hierarchical retrieval-augmented generation strategy significantly enhances the performance of multimodal language models on various evaluation metrics, outperforming established baselines. Limitations and potential future directions are also discussed, focusing on areas for further improvement and broader applicability."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To investigate the impact of fine-tuning the MLLM on its capability of exploiting retrieved passages, and to ensure that it maintains its performance on tasks not requiring external knowledge by applying a data mixing approach.",
            "experiment_process": "The model is trained on pairs of questions and ground-truth answers requiring external knowledge. Ground-truth pairs requiring external knowledge are mixed with ground-truth pairs not requiring external knowledge in the same mini-batch to preserve performance on original tasks.",
            "result_discussion": "Fine-tuning helps the model better exploit retrieved passages for answering questions requiring external knowledge, while the data mixing approach aims to maintain the model's original performance on tasks not requiring external knowledge.",
            "ablation_id": "2404.15406v2.No1"
        },
        {
            "research_objective": "To evaluate the effect of different datasets and methods for incorporating external knowledge through hierarchical retrieval on the performance of the MLLM.",
            "experiment_process": "The models are evaluated with and without an external knowledge base. Wikipedia dumps of 2M and 100k pages are used respectively for Encyclopedic-VQA and InfoSeek datasets. Accuracy is reported over the test split of Encyclopedic-VQA and the validation split of InfoSeek. Entity retrieval results using CLIP and MLLM performance with varied numbers of textual chunks as additional context are analyzed.",
            "result_discussion": "The use of an external knowledge base significantly increases accuracy, especially on the InfoSeek dataset with smaller knowledge bases. However, using larger knowledge bases can introduce noise, slightly degrading performance. Employing oracle entities boosts accuracy, particularly highlighting the importance of good entity retrieval models to provide relevant content.",
            "ablation_id": "2404.15406v2.No2"
        },
        {
            "research_objective": "To evaluate the importance of mixing fine-tuning datasets for the knowledge-based VQA task on the performance of the MLLM.",
            "experiment_process": "The MLLM is fine-tuned with a mixture of data containing image-question-answer triples from Encyclopedic-VQA or InfoSeek training set and visual instruction tuning data from LLaVA-Instruct. The impact of this mixing on performance is evaluated without external knowledge retrieval.",
            "result_discussion": "Using visual instruction tuning data helps improve the fine-tuning phase on the InfoSeek dataset, leading to a gain of 1.9 accuracy points. For the Encyclopedic-VQA, this does not improve performance but does not degrade it either.",
            "ablation_id": "2404.15406v2.No3"
        },
        {
            "research_objective": "To assess the impact of fine-tuning the MLLM on its performance in common MLLM evaluation benchmarks, with and without visual instruction tuning data.",
            "experiment_process": "The fine-tuned model is evaluated on the MME, MMMU, MMBench, and POPE datasets. The results are compared to the original LLaVA model to analyze performance preservation across a range of tasks.",
            "result_discussion": "Employing samples from the LLaVA-Instruct dataset better preserves original performance while fine-tuning. The most significant degradation occurs in the MME dataset, but other benchmarks show preserved or slightly improved results compared to the original model.",
            "ablation_id": "2404.15406v2.No4"
        }
    ]
}