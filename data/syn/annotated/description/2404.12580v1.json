{
    "title": "iTBLS: A Dataset of Interactive Conversations Over Tabular Information",
    "abstract": "This paper introduces Interactive Tables (iTBLS), a dataset of interactive conversations situated in tables from scientific articles. This dataset is designed to facilitate\nhuman-AI collaborative problem-solving through AI-powered multi-task tabular capabilities. In contrast to prior work that models interactions as factoid QA or procedure synthesis, iTBLS broadens the scope of interactions to include mathematical reasoning, natural language manipulation, and expansion of existing tables from natural language conversation by delineating interactions into one of three tasks: interpretation, modification, or generation. Additionally, the paper presents a suite of baseline approaches to iTBLS, utilizing zero-shot prompting and parameter-efficient fine-tuning for different computing situations. We also introduce a novel multi-step approach and show how it can be leveraged in conjunction with parameter-efficient fine-tuning to achieve the state-of-the-art on iTBLS; outperforming standard parameter-efficient fine-tuning by up to 15% on interpretation, 18% on modification, and 38% on generation.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Recent research on Conversational AI has focused on adding enhanced multi-task capabilities to large language models (LLMs).\nThis research includes building systems capable of situated interactions over structured knowledge sources such as tabular information\nSundar and Heck (2022  ###reference_b51###). Automated methods for tabular interpretation, manipulation, and generation empower users by saving time and reducing errors in managing tabular content Kardas et al. (2020  ###reference_b27###). Previous studies have focused on individual aspects of tabular data management: representation learning for interpretation tasks like grounded question answering, manipulation for data wrangling, and generation for summarizing textual information independently Nakamura et al. (2022a  ###reference_b33###); Sundar and Heck (2023  ###reference_b53###); Sundar et al. (2024  ###reference_b52###); Fang et al. (2024  ###reference_b15###).\nThe development of\nsituated conversational interactions over tables necessitates a suite of approaches to unify tabular interpretation, modification, and generation in a conversational context. Additionally, an important yet largely unaddressed challenge in interacting with\ntabular sources is the ability to\nmodify existing tabular content using conversational natural language commands.\nTo address these challenges, this paper introduces Interactive Tables (iTBLS) 111The dataset and baseline models will be released with the camera-ready version under Apache License 2.0, a dataset of interactive conversations in English situated in tabular information. iTBLS decomposes the challenge into three distinct tasks: interpretation, which involves understanding tabular content within a conversational framework; modification, which entails manipulating tabular content through natural language commands; and generation, which focuses on integrating new natural language information into existing tables. The tabular information in iTBLS is sourced from scientific articles hosted on arXiv 222https://arxiv.org  ###reference_arxiv.org###, an open-access repository of academic preprints.\nIn contrast to earlier methods that approached tabular interpretation as question-answering focused on single-cell factoids, iTBLS introduces a more complex conversational interaction framework. Beyond factoid question-answering, iTBLS encompasses tasks such as comparison, determining absolute and relative positions, and mathematical reasoning. Previous research primarily examined procedural command generation for spreadsheets or the alignment of tabular data through LLMs. iTBLS integrates these functionalities into a unified task, enabling the manipulation of existing tables through natural-language commands.\nOn tabular generation, while prior work addressed the summarization of natural language paragraphs in a tabular format, iTBLS focuses on generating row or column data conversationally.\nFurthermore, this paper presents baseline approaches utilizing zero-shot prompting of off-the-shelf LLMs and parameter-efficient fine-tuning and represents a suite of techniques for different computing situations. Additionally, a novel multi-step approach is presented to improve accuracy and reduce errors, inspired by past work using multi-step methods to improve a variety of approaches including complex reasoning Jiang et al. (2023b  ###reference_b24###), external tool use Paranjape et al. (2023  ###reference_b37###), feedback Richardson et al. (2023  ###reference_b42###), self-refinement Madaan et al. (2024  ###reference_b32###), and mathematical reasoning Zhou et al. (2023  ###reference_b66###). This multi-step method results in an improvement over standard parameter-efficient fine-tuning by up to 15% on interpretation, 18% on modification, and 38% on generation.\nThe contributions of this work are as follows:\nCreating iTBLS, a dataset of tabular interactions unifying interpretation, modification, and generation\nExtending prior tabular datasets by collecting information from arXiv\nBroadening the scope of interactions to include mathematical reasoning, natural language manipulation, and natural language expansion\nCompleting a series of baselines utilizing zero-shot prompting, parameter-efficient fine-tuning, and a task-specific multi-step approach\nThe rest of the paper is organized as follows:\nSection 2  ###reference_### reviews prior research addressing tabular interpretation, modification, and generation. Section 3  ###reference_### presents a description of the tasks in iTBLS and outlines the dataset collection process. Sections 4  ###reference_### and 5  ###reference_### detail the baseline approaches to address iTBLS and their associated results. Lastly, Section 6  ###reference_### concludes the paper.\n###figure_1###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Related work on paired natural-language and tabular data can be broadly classified by the nature of the interaction: tabular interpretation, tabular manipulation, and tabular generation."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Tabular Interpretation",
            "text": "Tabular interpretation involves a dialogue turn focused on extracting information from a specific cell in a table, such as identifying a cell satisfying certain criteria. Prior research on tabular interpretation focused on grounded question-answering. An important challenge in the collection of such datasets is the availability of large-scale tabular data. Consequently, many tabular datasets are constructed from online resources such as Wikipedia including WikiTableQuestions Pasupat and Liang (2015  ###reference_b39###), ManyModalQA Hannan et al. (2020  ###reference_b19###), TaBERT Yin et al. (2020  ###reference_b59###), NQ-Tables Herzig et al. (2021  ###reference_b20###), FEVEROUS Aly et al. (2021  ###reference_b2###), FeTaQA Nan et al. (2022  ###reference_b35###), HybriDialogue Nakamura et al. (2022b  ###reference_b34###), and HiTab Cheng et al. (2022  ###reference_b10###). Other tabular datasets are constructed from financial reports including TAT-QA Zhu et al. (2021  ###reference_b67###), FinQA Chen et al. (2021  ###reference_b9###), MultiHiertt Zhao et al. (2022  ###reference_b63###). Related work on paired natural-language and tabular data can be broadly classified by the nature of the interaction: tabular interpretation, tabular analysis, and tabular generation. Proposed approaches to address the tabular interpretation task include architectures based off of the Transformer encoder Yin et al. (2020  ###reference_b59###); Herzig et al. (2020  ###reference_b21###); Chen et al. (2020  ###reference_b8###); Eisenschlos et al. (2020  ###reference_b14###); Liu et al. (2021  ###reference_b30###); Gu et al. (2022  ###reference_b17###); Yang et al. (2022  ###reference_b58###), decoder Gong et al. (2020  ###reference_b16###); Akhtar et al. (2023  ###reference_b1###); Zha et al. (2023  ###reference_b60###); Jiang et al. (2023a  ###reference_b23###); Zhang et al. (2023b  ###reference_b62###); Sui et al. (2024  ###reference_b50###), or both (encoder-decoder) Nakamura et al. (2022b  ###reference_b34###); Deng et al. (2022  ###reference_b12###); Sundar and Heck (2023  ###reference_b53###); Sarkar and Lausen (2023  ###reference_b43###)."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Tabular Modification",
            "text": "Related work on paired natural-language and tabular data can be broadly classified by the nature of the interaction: tabular interpretation, tabular modification, and tabular transcription."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Tabular Generation",
            "text": "Related work on paired natural-language and tabular data can be broadly classified by the nature of the interaction: tabular interpretation, tabular modification, and tabular translation."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "The iTBLS Dataset",
            "text": "The Interactive Tables (iTBLS) dataset features 3-turn conversational interactions situated in tabular data, covering the three distinct types of interactions described in Section2  ###reference_###: interpretation, modification, and generation. Each example type is exemplified in Figure 1  ###reference_### and described below. In addition, since the mode of interaction is not known a priori, any proposed approach using iTBLS must effectively identify the interaction type, either explicitly or implicitly. In the following sections, we provide a detailed description of each type of interaction and outline the dataset collection process."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Tasks",
            "text": "Tabular Interpretation: In iTBLS, interpretive interactions are structured as question-answer pairs, where the goal is to identify the cell referred to by the question. The references could be absolute (referring to a specific row or column), or relative (referring to one cell in the context of another). Appendix A.1  ###reference_### details absolute and relative references in iTBLS.\nTabular modification: We conceptualize modification in iTBLS as a series of cell swaps, positing that any content rearrangement can ultimately be reduced to such exchanges. This approach allows for both explicit references, where specific row and column numbers are cited, and implicit references, which rely on the content or relative positions of cells. Table 7  ###reference_### in Appendix A.1  ###reference_### showcases examples from iTBLS. As observed, there is a mix of explicit and implicit references to the specific contents to be manipulated.\nTabular generation: In iTBLS, table generation is guided by new natural language evidence. This evidence clarifies appending a row or column, defines the suitable header, and supplies the data entries for the new row (or column) relative to existing columns (or rows). This process ensures that the added elements are contextually relevant and accurately integrated into the table. Table 8  ###reference_### in Appendix A.1  ###reference_### provides examples of such interactions, demonstrating how users can request the incorporation of new row and column data into an established table framework.\nIn iTBLS, the mode of interaction is not explicitly stated by the user, introducing an additional task: interaction identification. This task involves predicting whether the interaction is intended for interpretation, modification, or generation based solely on the user\u2019s request."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Dataset Collection",
            "text": "To collect the dataset, first we use AxCell Kardas et al. (2020  ###reference_b27###) an automatic machine learning pipeline for extracting results from papers. AxCell is used to parse tabular information from papers on arXiv to populate online leaderboards comparing scientific methods. Using AxCell, we collect 20,000 tables from academic papers in Mathematics, Physics, and Computer Science over a period spanning from 2007 to 2014. The tables are processed to remove stray characters resulting from the conversion from LaTeX. Additionally, only tables with at least three rows and three columns to at most ten rows or ten columns are retained. The final dataset consists of 4000 tables split between train, development, and test sets.\nFor each table, we generate three sequential edits corresponding to different types of interaction. Interpretation involves generating a dialogue turn (question-answer pair) grounded on a single cell of the table. Modification involves manipulating two cells of an existing table by swapping them. Finally, generation encompasses the task of appending either a new row or a column to an existing table based on a natural language utterance.\nTo enhance the quality of the dataset and minimize errors, we implement a strategic selection process for the table components involved in each interaction. In interpretation, a cell is randomly selected to ground the dialogue. For modification, two cells are chosen and their positions are swapped to simulate a realistic table manipulation scenario. In generation, all cells in a randomly masked row or column are used as the basis for appending new table data. All of the interactions are based on cells that do not belong to row or column headers, that is, they reside in the body of the table.\nFor our dataset creation, we employ two distinct sources for generating dialogue turns based on the type of interaction and the specific table component involved. For tasks related to tabular interpretation and modification, we engage crowd-workers from Amazon Mechanical Turk (AMT). These workers are tasked with formulating questions or commands that pertain to the pre-identified cell(s) designated for each interaction. We recruit workers from Australia, Canada, Ireland, New Zealand, the United Kingdom, and the USA. Each crowdworker is compensated at a rate of $0.15 per Human Intelligence Task (HIT), with the average completion time for each HIT being approximately 40 seconds. Detailed information on the AMT interface used for these tasks is included in Appendix A.3  ###reference_###.\nFor generation, GPT-4 is prompted to write a dialogue turn summarizing a row or column of the table. We prompt the version of GPT-4 as of 2/16/2024 using the OpenAI API. The prompt is as follows:\nThe string contains information from a table [table]. Describe the content in this [row/column] for a visually impaired user in one line. Make sure to include all information from the rows and columns and appropriate headers so the user can understand the content.\nEach sample in the dataset contains the source arXiv ID, the table that the conversation is situated in, the index of that table within the paper (e.g. Table X), the utterance describing the interaction, the ground truth cell(s) involved in the interaction, and finally the expected output.\nStatistics of the dataset are provided in Table 1  ###reference_###.\n###table_1### ###figure_2###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Baseline Approaches",
            "text": "We experiment with multiple approaches for a variety of compute situations for the different sub-tasks in iTBLS. These include zero-shot prompting, parameter-efficient fine-tuning, and a multi-step approach with syntax generation. To augment LLMs with 2D tabular information, we serialize tables in iTBLS using a comma-separated format Singha et al. (2023  ###reference_b48###). Commas separate cells in the same row belonging to different columns and newline characters \u2018\\n\u2019 separate cells across rows."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Zero-shot prompting",
            "text": "Zero-shot prompting leverages off-the-shelf LLMs that have undergone pre-training with a causal language modeling objective followed by supervised fine-tuning to align with human instructions. The LLM is not fine-tuned on iTBLS and is directly prompted without examples in-context. The prompt consists of the input table, the interaction request, and additional task-specific instructions to ensure the output is formatted correctly."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Parameter-efficient fine-tuning (PEFT)",
            "text": "We use QLoRA Dettmers et al. (2023  ###reference_b13###), a parameter-efficient approach to fine-tune LLMs. In this method, an LLM is fine-tuned for each sub-task by maximizing a causal language modeling objective based on the specific interaction. The objective to be optimized is described in Section 5  ###reference_###."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Multi-step approach",
            "text": "The Multi-Step approach (MS) breaks down the entire task into a series of intermediate steps, addressing part of the specific task. For interpretation and modification, this process first identifies the cells referred to by the user using an LLM in a zero-shot configuration followed by program synthesis to execute the action. However, due to the custom encoding used in identifying cell references (Figure 2  ###reference_###), the approach did not yield satisfactory results when compared to zero-shot prompting. For generation, the multi-step approach breaks down table generation as two-step question-answering. Additional details are provided in Section 5.3  ###reference_###."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Multi-Step + PEFT",
            "text": "This approach first breaks down the entire process into a sequence of intermediate steps. Next, an LLM is fine-tuned using parameter-efficient fine-tuning to address each step independently."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "We introduce a modular approach where we use an LLM to identify the nature of the interaction followed by another LLM to execute the operation described in the user utterance. A high-level overview is depicted in Figure 3  ###reference_###. We use the version of Llama 2 Touvron et al. (2023  ###reference_b55###) already pre-trained and instruction-aligned using supervised fine-tuning and reinforcement learning with human feedback for dialogue use cases (Llama-2-[7/13/70]B-chat-hf)333https://huggingface.co/meta-llama  ###reference_huggingface.co/meta-llama###, hereafter referred to as Llama-2-[7/13/70]B.\n###figure_3###"
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Interaction Identification",
            "text": "For interaction identification we benchmark both zero-shot and fine-tuned approaches using GPT-{3.5,4} and Llama 2-{7B,13B,70B}. We use classification accuracy as the metric. For zero-shot experiments, we instruct the model to wrap the identified task in special tags, e.g. \u201c[TASK] modify [/TASK]\". This enables automated extraction of the answers in the zero-shot setting. Any response that fails to follow the requested format is considered incorrect.\nWe aggregate all data across tasks into pairs of requests  and tasks   and maximize the objective:\nResults for interaction identification are provided in Table 2  ###reference_### where PEFT using Llama 2 achieves near-perfect performance on the task.\n###table_2###"
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Tabular Interpretation",
            "text": "Table 3  ###reference_### contains results for the zero-shot approach using the different sizes of Llama 2. For each model, we report the exact match score that identifies whether the entire answer string is present in a continuous form in the model\u2019s response. Using larger models results in better exact match scores, with the answer phrase present in almost 30% of the responses from the 70B variant of Llama 2.\nWe also report results using PEFT. In this situation, Llama 2 is fine-tuned to maximize the probability of the correct answer over the set of all possible answers  given a table and a user request:\nHyperparameters for fine-tuning are detailed in Appendix A.2  ###reference_###. As expected, fine-tuning using QLoRA performs better than zero-shot prompting, achieving up to 2x the exact match score when compared to the zero-shot approaches (Table 3  ###reference_###). Our best-performing method is a multi-step approach combined with parameter-efficient fine-tuning. In this method, we identify the cell referred to by the user utterance in the concept of the table (row and column number) and then generate a response based on this cell. Performing this operation ensures that the output response is necessarily a cell of the table as opposed to free-form generation with no constraint on the emitted LLM response being a cell of the input table. An example of the codified representation is available in Figure 2  ###reference_###.\n###table_3###"
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Tabular Generation",
            "text": "First, we experiment with zero-shot prompting with Llama 2 to incorporate the row or column information into the table in a single step. During evaluation, we assume tabular information is order invariant, provided the contents align accurately with the correct header cells. To assess the effectiveness of our approach, we report the F1 score, that is, the harmonic mean of precision and recall computed between the cells of the predicted row or column and the ground truth.\nAs evidenced by the results in Table 5  ###reference_###, this single-step zero-shot prompting results in tables that are malformed, that is, the generated tables are inconsistent and incorrectly formatted, or do not match the dimensions of the expected ground truth to be generated. The malformed rate reduces as we experiment with larger models, going down from 88% to  26% with a marginal improvement in F1 scores.\nTo address this issue, we experiment with a multi-step approach. Prior work has shown the effectiveness of reformulating the table-generation task as two-stage machine reading comprehension Sundar et al. (2024  ###reference_b52###) to achieve tables that are always well-formed. The first stage identifies whether the input utterance asks to generate a row or column using Llama 2 in a zero-shot configuration. Then, based on whether a row or column is to be generated, questions are synthesized using the header rows of the corresponding column or row. The synthesized questions are then answered using Llama 2 in a zero-shot configuration to yield the final output. Since we constrain the generated content to conform to the existing tabular structure, none of the tables contain formatting errors, resulting in a malformed rate of 0%.\nTo further improve results, we experiment with PEFT using QLoRA. The fine-tuning process is performed to ensure the LLMs emit cells of a table autoregressively by optimizing the objective in Equation 3  ###reference_###. The output is parsed to reformat the CSV-serialization back into the original two-dimensional representation.\nThis results in a 2x improvement in F1 scores over the multi-step approach (Table 5  ###reference_###), but at the cost of increased malformed rates. To alleviate this issue, we combine the multi-step and fine-tuning approaches. First, Llama 2 is used in a zero-shot configuration to determine whether a row or column is to be added to the table. Next, the question-answering reformulation is used to synthesize question-answer pairs. Llama 2 is then fine-tuned using QLoRA to answer these questions. This approach marries the best of both worlds, resulting in a further 2x improvement in F1 scores over the end-to-end fine-tuning while bringing the malformed rates to 0%."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Tabular Modification",
            "text": "###table_4### For the modification task, we experiment with a fine-tuning approach with Llama-2-7b, as well as zero-shot approaches with three llama-2 models (7b, 13b, and 70b). For each experiment, we compute table modification accuracy. An output table is accurate when all row and column headers match the ground truth, as well as all cell values within the table. This metric is computed by converting the textual form of the output table into a DataFrame using the pandas library in python and then comparing it to ground truth. If the conversion to DataFrame fails, the table is considered malformed.\nThe accuracy and malformed rate are reported in Table 4  ###reference_###. Zero-shot approaches with all three models achieve low (<4%) accuracy on the modification task, including a 0% accuracy from the smallest model (7B). This is not unexpected due to the need to identify cells from relative references in natural language text form and perform the correct swap, combined with the strict constraint of generating well-formed tables. All this must be performed while maintaining the remainder of the table\u2019s values. This is further exacerbated by the presence of malformed table outputs, which make up as much as 15.6% of output tables in some experiments. Table 4  ###reference_### further demonstrates the strength of PEFT, where the standard approach boosts accuracy by 10.9% on the test split while using 1/10 the parameters. However, malformed tables remain a problem even with standard PEFT. This is addressed with a multi-step approach. First, we generate an encoding of the requested modification. This is then followed by a program synthesis to perform the swap on the table and return the resulting modified table. An example of the encoding to swap contents in the 3rd and 4th columns of the first row is: modify<SEP>1<SEP>4<SEP>1<SEP>3. The corresponding program synthesis is executed in pandas using df.iloc444https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html  ###reference_le/reference/api/pandas.DataFrame.iloc.html###.\nWith our multi-step approach, we are able to achieve a 0% malformed rate (this is guaranteed by the method), and an 18.8% absolute increase in accuracy on the test split, thus demonstrating the value of the multi-step approach for the task of tabular modification.\n###table_5###"
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "This paper introduces Interactive Tables (iTBLS), a dataset of interactive conversations situated over tables in scientific papers. iTBLS consists of multi-turn conversations with three modes of interaction - interpretation, modification, and generation. iTBLS extends prior work by introducing new interactions such as comparison of table contents, mathematical and positional reasoning, manipulation using natural language commands, and generation of content from new evidence. Additionally, a set of baseline approaches for a variety of computing situations addressing iTBLS is presented. The baseline approaches utilize zero-shot prompting and parameter-efficient fine-tuning. Furthermore, a novel multi-step task-specific approach is introduced. Leveraging the multi-step process in conjunction with parameter-efficient fine-tuning yields the best performance on iTBLS, outperforming standard parameter-efficient fine-tuning by up to 15% on interpretation, 18% on modification, and 38% on generation."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "While iTBLS introduces a dataset for interactive conversations over tabular information, there are some avenues for improvement. In this dataset, modification is modeled as a series of swaps. A more comprehensive sequence of manipulations includes in-place modification of values and modifying a cell\u2019s value based on other cells using both absolute and relative references. While sourcing tabular information from arXiv provides a cost-efficient approach, LLMs are often pre-trained on LaTeXsources from arXiv. This paper alleviates the issue by sourcing natural language commands from crowdworkers. Future work could look at collecting tabular information from crowdworkers as well. While we present a suite of baseline approaches for iTBLS, there is still headroom between the presented approaches and perfect performance. We identify the closure of this gap as an avenue for future work."
        }
    ],
    "url": "http://arxiv.org/html/2404.12580v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2",
            "2.3"
        ],
        "methodology_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.1",
            "5.2",
            "5.3",
            "5.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.3",
            "4.4",
            "5.2",
            "5.3",
            "5.4"
        ]
    },
    "research_context": {
        "paper_id": "2404.12580v1",
        "paper_title": "iTBLS: A Dataset of Interactive Conversations Over Tabular Information",
        "research_background": "### Introduction\n\nRecent research on Conversational AI has focused on adding enhanced multi-task capabilities to large language models (LLMs). This research includes building systems capable of situated interactions over structured knowledge sources such as tabular information (Sundar and Heck, 2022). Automated methods for tabular interpretation, manipulation, and generation empower users by saving time and reducing errors in managing tabular content (Kardas et al., 2020). Previous studies have focused on individual aspects of tabular data management, including representation learning for interpretation tasks like grounded question answering, manipulation for data wrangling, and generation for summarizing textual information independently (Nakamura et al., 2022a; Sundar and Heck, 2023; Sundar et al., 2024; Fang et al., 2024).\n\nThe development of situated conversational interactions over tables necessitates a suite of approaches to unify tabular interpretation, modification, and generation in a conversational context. Additionally, an important yet largely unaddressed challenge in interacting with tabular sources is the ability to modify existing tabular content using conversational natural language commands.\n\nTo address these challenges, this paper introduces Interactive Tables (iTBLS), a dataset of interactive conversations in English situated over tabular information. iTBLS decomposes the challenge into three distinct tasks:\n\n1. **Interpretation**, which involves understanding tabular content within a conversational framework.\n2. **Modification**, which entails manipulating tabular content through natural language commands.\n3. **Generation**, which focuses on integrating new natural language information into existing tables.\n\nThe tabular information in iTBLS is sourced from scientific articles hosted on arXiv.\n\nIn contrast to earlier methods that approached tabular interpretation as question-answering focused on single-cell factoids, iTBLS introduces a more complex conversational interaction framework. Beyond factoid question-answering, iTBLS encompasses tasks such as comparison, determining absolute and relative positions, and mathematical reasoning. Previous research primarily examined procedural command generation for spreadsheets or the alignment of tabular data through LLMs. iTBLS integrates these functionalities into a unified task, enabling the manipulation of existing tables through natural-language commands.\n\nRegarding tabular generation, while prior work addressed the summarization of natural language paragraphs in a tabular format, iTBLS focuses on generating row or column data conversationally.\n\nFurthermore, this paper presents baseline approaches utilizing zero-shot prompting of off-the-shelf LLMs, parameter-efficient fine-tuning, and a novel multi-step approach. Inspired by past work using multi-step methods to improve a variety of approaches, including complex reasoning (Jiang et al., 2023b), external tool use (Paranjape et al., 2023), feedback (Richardson et al., 2023), self-refinement (Madaan et al., 2024), and mathematical reasoning (Zhou et al., 2023), this multi-step method shows marked improvements over standard parameter-efficient fine-tuning by up to 15% on interpretation, 18% on modification, and 38% on generation tasks.\n\n### Contributions of This Work\n1. **Creating iTBLS:** A dataset of tabular interactions unifying interpretation, modification, and generation tasks.\n2. **Extending Prior Tabular Datasets:** Collection of information from arXiv.\n3. **Broadening the Scope of Interactions:** Including mathematical reasoning, natural language manipulation, and natural language expansion.\n4. **Baseline Approaches:** Implementation of zero-shot prompting, parameter-efficient fine-tuning, and a task-specific multi-step approach.\n\nThe rest of the paper is organized as follows: Section 2 reviews prior research addressing tabular interpretation, modification, and generation. Section 3 presents a description of the tasks in iTBLS and outlines the dataset collection process. Sections 4 and 5 detail the baseline approaches to address iTBLS and their associated results. Lastly, Section 6 concludes the paper.\n\n## Related Work\n\n### Tabular Data Interpretation\n\nRecent efforts in interpreting tabular data have often focused on grounded question-answering tasks. For instance, Sundar and Heck (2022) explored methods where conversation agents parsed structured knowledge to provide point-specific answers. Kardas et al. (2020) discussed how automating the interpretation task could save users' time and reduce errors.\n\n### Tabular Data Manipulation\n\nNakamura et al. (2022a), Sundar and Heck (2023), and Fang et al. (2024) independently explored various methods for wrangling data, including efforts to generate procedural commands from natural language. These approaches often focus on spreadsheet applications and structured data sets requiring a range of manipulative actions.\n\n### Tabular Data Generation\n\nPrevious work also covered the automatic generation of tabular data, typically summarizing text into structured formats. Work by Sundar et al. (2024) and similar studies focused on converting paragraphs into tables. These efforts aimed to condense information for easier visualization and consumption but stopped short of conversationally generating tabular content. \n\n### Integration in Conversational Context",
        "methodology": "### Proposed Methodology\n\nIn the development of our dataset, iTBLS, for interactive conversations over tabular information, we incorporate several key methodologies to handle a variety of computational sub-tasks effectively. Below, we outline the primary components and innovations included in our approach:\n\n1. **Approaches for Varied Compute Situations**:\n    - **Zero-shot Prompting**: This technique involves using pre-trained language models without any additional fine-tuning on the specific task, relying solely on the general knowledge captured during the model's initial training. This approach is particularly useful when resources for customization are limited.\n    - **Parameter-efficient Fine-tuning**: Instead of adjusting the entire set of parameters in a large language model, this approach focuses on fine-tuning only a smaller subset of parameters. This method strikes a balance between computational feasibility and task-specific performance.\n    - **Multi-step Approach with Syntax Generation**: This involves breaking down the problem into multiple stages and generating intermediate syntactical representations that facilitate accurate processing of tabular data in subsequent steps.\n\n2. **Augmenting LLMs with 2D Tabular Information**:\n    - To effectively integrate 2D tabular information into large language models (LLMs), we employ a serialization technique. Specifically, tables within the iTBLS dataset are transformed into a comma-separated format:\n        - **Comma-separated format**: Each cell within a row is separated by a comma, and rows are separated by newline characters (`\\n`). This method is inline with the approach described by Singha et al. (2023), enabling the language models to parse and understand the structure of tabular data more effectively.\n\nBy deploying these methodologies, we enhance the ability of LLMs to engage in interactive conversations over tabular data, ensuring they can perform accurately and efficiently across different computational contexts.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Experiment Setup\n- **Approach**: The primary setup introduces a modular system where one Large Language Model (LLM) identifies the nature of a user interaction, followed by another LLM that executes the described operation from the user utterance.\n- **Models Used**: The Llama 2 Touvron et al. (2023) models, specifically Llama-2-[7/13/70]B-chat-hf, are utilized. These models are pre-trained and instruction-aligned using supervised fine-tuning and reinforcement learning bolstered by human feedback. \n- **Datasets**: Not explicitly mentioned in the segment provided, but likely revolves around the iTBLS dataset introduced in the paper, focused on interactive conversations over tabular information.\n- **Evaluation Metrics**: Specific metrics were not detailed in the provided segment. Typically, models handling conversational or interactional datasets might be evaluated using metrics such as accuracy, precision, recall, F1-score, or user satisfaction ratings.\n\n#### Main Experimental Results\nThe primary results of the main experiment were not detailed in the provided text segment. Additional information would be needed to accurately describe the outcomes and significance of the results."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the effectiveness of the multi-step approach combined with parameter-efficient fine-tuning (PEFT) in improving tabular interpretation accuracy.",
            "experiment_process": "The setup involved fine-tuning Llama 2 using PEFT to maximize the probability of correct answers over a set of possible answers given a table and a user request. Hyperparameters for fine-tuning are detailed in Appendix A.2. The multi-step process first identifies the cell referred to by the user in the table and then generates a response based on this cell. The exact match score is used as the evaluation metric.",
            "result_discussion": "Fine-tuning using QLoRA significantly improved exact match scores, achieving up to twice the score of zero-shot approaches with the benefits of the multi-step approach. This method ensures the output response is constrained to a cell from the input table rather than generating free-form text.",
            "ablation_id": "2404.12580v1.No1"
        },
        {
            "research_objective": "To determine the optimal combination of techniques for generating well-formed tables with accurate content during tabular generation tasks.",
            "experiment_process": "Initial experiments involved zero-shot prompting with Llama 2, followed by evaluation using F1 scores, measuring the alignment between the generated cells and the ground truth. The multi-step approach reformed the task into a two-stage process: identifying whether to generate a row or column and then synthesizing questions for each cell. Additional fine-tuning was conducted using PEFT with QLoRA. The rate of malformed tables and F1 scores were the key evaluation metrics.",
            "result_discussion": "Zero-shot prompting resulted in high rates of malformed tables. The multi-step approach improved table formation but led to content accuracy issues. Combining the multi-step and fine-tuning approach removed malformed rates and significantly improved F1 scores, achieving a 2x enhancement over individual methods.",
            "ablation_id": "2404.12580v1.No2"
        },
        {
            "research_objective": "To assess the impact of different approaches on accuracy and malformed rate in tabular modification tasks.",
            "experiment_process": "Several models of Llama 2 (7b, 13b, and 70b) were tested using both zero-shot and fine-tuning approaches. Table modification accuracy was computed by comparing the DataFrame conversion of the generated tables to the ground truth using the pandas library. Experiments also involved encoding modifications followed by program synthesis within a multi-step approach. Accuracy and malformed rate were the primary metrics.",
            "result_discussion": "Zero-shot models achieved very low accuracy with a significant number of malformed tables. Fine-tuning with PEFT improved accuracy but did not fully mitigate malformed rates. The multi-step approach ensured a 0% malformed rate and significantly boosted accuracy by 18.8% on the test split, highlighting its effectiveness for tabular modification.",
            "ablation_id": "2404.12580v1.No3"
        }
    ]
}