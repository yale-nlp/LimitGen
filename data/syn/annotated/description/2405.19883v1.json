{
    "title": "From Words to Actions: Unveiling the Theoretical Underpinnings of LLM-Driven Autonomous Systems",
    "abstract": "In this work, from a theoretical lens, we aim to understand why large language model (LLM) empowered agents are able to solve decision-making problems in the physical world.\nTo this end, consider a hierarchical reinforcement learning (RL) model where the LLM Planner and the Actor perform high-level task planning and low-level execution, respectively.\nUnder this model, the LLM Planner navigates a partially observable Markov decision process (POMDP) by iteratively generating language-based subgoals via prompting.\nUnder proper assumptions on the pretraining data, we prove that the pretrained LLM Planner effectively performs Bayesian aggregated imitation learning (BAIL) through in-context learning.\nAdditionally, we highlight the necessity for exploration beyond the subgoals derived from BAIL by proving that naively executing the subgoals returned by LLM leads to a linear regret.\nAs a remedy, we introduce an -greedy exploration strategy to BAIL, which is proven to incur sublinear regret when the pretraining error is small.\nFinally, we extend our theoretical framework to include scenarios where the LLM Planner serves as a world model for inferring the transition model of the environment and to multi-agent settings, enabling coordination among multiple Actors.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The advent of large language models (LLMs) such as GPT-4 (OpenAI,, 2023  ###reference_b54###) and Llama 2 (Touvron et al.,, 2023  ###reference_b69###) has marked a significant leap in artificial intelligence, thanks to their striking capabilities in understanding language and performing complex reasoning tasks. These capabilities of LLMs have led to the emergence of LLM-empowered agents (LLM Agents), where LLMs are used in conjunction with tools or actuators to solve decision-making problems in the physical world. LLM Agents have showcased promising empirical successes in a wide range of applications, including autonomous driving (Wang et al., 2023b,  ###reference_b72###; Fu et al.,, 2024  ###reference_b20###), robotics (Brohan et al.,, 2023  ###reference_b8###; Li et al., 2023a,  ###reference_b40###), and personal assistance (Liu et al.,, 2023  ###reference_b48###; Nottingham et al.,, 2023  ###reference_b53###). This progress signifies a crucial advancement in the creation of intelligent decision-making systems, distinguished by a high degree of autonomy and seamless human-AI collaboration.\nLLMs only take natural languages as input.\nTo bridge the language and physical domain, LLM-agents typically incorporate three critical components: an LLM Planner, a physical Actor, and a multimodal Reporter, functioning respectively as the brain, hands, and eyes of the LLM-agent, respectively.\nSpecifically, upon receiving a task described by a human user, the LLM Planner breaks down the overall task into a series of subgoals.\nSubsequently, the Actor implements each subgoal in the physical world through a sequence of actions. Meanwhile, the Reporter monitors changes in the physical world and conveys this information back to the LLM Planner in natural language form. This dynamic interaction among Planner, Actor, and Reporter empowers LLM Agents to understand the environment, formulate informed decisions, and execute actions effectively, thus seamlessly integrating high-level linguistic subgoals with low-level physical task execution.\nThe revolutionary approach of LLM Agents represents a paradigm shift away from traditional learning-based decision-making systems.\nUnlike these conventional systems, LLM Agents are not tailored to any specific task. Instead, they rely on the synergy of their three distinct components\u2014each trained separately and often for different objectives.\nIn particular, the LLM Planner is trained to predict the next token in a sequence on vast document data.\nMoreover, when deployed to solve a task, the way to interact with the LLM Planner is via prompting with the LLM fixed.\nThe Actor, as language-conditioned policies, can be trained by RL or imitation learning.\nMoreover, the Reporter, as a multimodal model, is trained to translate the physical states (e.g., images) into natural language.\nThis unique configuration prompts critical research questions regarding the theoretical underpinnings of LLM Agents, particularly concerning their decision-making effectiveness.\n###figure_1### In this work, we make an initial step toward developing a theoretical framework for understanding the dynamics and effectiveness of LLM Agents. Specifically, we aim to answer the following questions:\n(a) What is a theoretical model for understanding the performance of LLM Agents?\n(b) How do pretrained LLMs solve decision-making problems in the physical world via prompting?\n(c) How does an LLM Agent address the exploration-exploitation tradeoff?\n(d) How do the statistical errors of the pretrained LLM and Reporter affect the overall performance of the LLM Agent?\nTo address Question (a), we propose analyzing LLM Agents within a hierarchical reinforcement learning framework (Barto and Mahadevan,, 2003  ###reference_b4###; Pateria et al.,, 2021  ###reference_b55###), positioning the LLM Planner and the Actor as policies operating within high-level POMDPs and low-level MDPs, respectively (\u00a73.1  ###reference_###).\nBoth levels share the same state space\u2014namely, the physical state\u2014though the LLM Planner does not directly observe this state but instead receives a language-based description from the Reporter, effectively navigating a POMDP.\nThe action space of the high-level POMDP is the set of language subgoals.\nMeanwhile, the state transition kernel is determined by the pretrained Actor, and thus is associated with a variable  that summarizes its dependency on low-level Actor. Such variable is unknown to LLM Planner.\nAfter pretraining, without prior knowledge of the Actor\u2019s quality or the physical environment, the LLM Planner attempts to solve the high-level POMDP by iteratively generating a sequence of subgoals based on feedback from the Reporter via prompting.\nUnder this framework, the overall performance of the LLM Agent can be captured by the regret in terms of finding the optimal policy of the hierarchical RL problem in the online setting (\u00a73.2  ###reference_###).\nFurthermore, to answer Question (b), we prove that when the pretraining data includes a mixture of expert trajectories, during the prompting stage, the pretrained LLM Planner essentially performs Bayesian aggregated imitation learning (BAIL) through in-context learning (Theorem 4.2  ###reference_theorem2###).\nThis process involves constructing a posterior distribution over the hidden parameter\nof the transition kernel, followed by generating subgoals that emulate a randomly selected expert policy, weighted according to this posterior distribution.\nSuch a Bayesian learning mechanism is encoded by the LLM architecture and is achieved through prompting.\nHowever, since the LLM has no prior knowledge of the physical environment, it needs to guide the Actor to explore the physical environment. We prove that merely adhering to BAIL-derived subgoals can lead to the inadequate exploration, resulting in a linear regret (Proposition 4.3  ###reference_theorem3###).\nTo mitigate this, i.e., Question (c), we introduce an\n-greedy\nexploration strategy, which occasionally deviates from BAIL subgoals in favor of exploration, significantly enhancing learning efficacy by ensuring a sublinear regret (Theorem 4.6  ###reference_theorem6###).\nSpecifically, to address Question (d) we establish that the regret is bounded by a sum of two terms (Theorem 5.7  ###reference_theorem7###): a\n-regret related to the number of episodes the LLM Agent is deployed to the hierarchical RL problem, and an additional term representing the statistical error from pretraining the LLM Planner and Reporter via maximum likelihood estimation (MLE) and contrastive learning, respectively (Theorem 2  ###reference_te2###, 5.5  ###reference_theorem5###).\nFinally, we extend our analysis to scenarios where the Planner utilizes the LLM as world model for inferring the upper-level POMDP\u2019s transition model via Bayesian model aggregation (Proposition B.1  ###reference_theorem1###, Corollary B.3  ###reference_theorem3###). Our theoretical framework also accommodates a multi-agent context, where the LLM Planner coordinates with a collaborative team of low-level actors (Corollary B.4  ###reference_theorem4###)."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Preliminaries and Related Works",
            "text": "LLMs have exhibited limited reasoning capabilities and a crucial aspect of their reasoning prowess is the in-context learning (ICL) ability. This ability is further enhanced through additional training stages (Iyer et al.,, 2022  ###reference_b33###), careful selection and arrangement of informative demonstrations (Liu et al.,, 2021  ###reference_b45###; Kim et al.,, 2022  ###reference_b37###), explicit instruction (Honovich et al.,, 2022  ###reference_b29###), and use of prompts to stimulate chain of thoughts (Wei et al., 2022b,  ###reference_b74###). Unlike fine-tuned models customized for specific tasks, LLMs showcase comparable capabilities by learning from the informative prompts (Li et al.,, 2022  ###reference_b42###; Liu et al., 2022b,  ###reference_b47###). Assume that prompt, denoted by , is generated based on a latent variable  autoregressively. The token follows a generating distribution such that  and , where  denotes the space of hidden information or concepts. The latent structure is commonly employed in language models, including topic models like LDA (Blei et al.,, 2003  ###reference_b6###), BERT (Devlin et al.,, 2018  ###reference_b14###), generative models like VAE (Kusner et al.,, 2017  ###reference_b38###), T5 (Raffel et al.,, 2020  ###reference_b60###), and is also widely adopted in the theoretical analysis of ICL (Xie et al.,, 2021  ###reference_b76###; Zhang et al.,, 2023  ###reference_b84###)."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Theoretical Framework for LLM Agents",
            "text": "To formalize the architecture of LLM Agents, we propose a general theoretical framework\u2014Planner-Actor-Reporter (PAR) system. Furthermore, the problem is modeled as a hierarchical RL problem (Pateria et al.,, 2021  ###reference_b55###). Specifically, the Planner, empowered by LLMs, conducts high-level task planning within the language space; the Actor, pretrained before deployment, undertakes low-level motion planning within the physical world; and the Reporter, equipped with a sensor to sense the physical environment, processes the information and feeds it back to the Planner, bridging the gap between language space and the physical world (see \u00a73.1  ###reference_###). Additionally, we present the performance metric and pretraining methods of LLMs for the Planner and translators for the Reporter in \u00a73.2  ###reference_###.\nLet  be the space of language subgoals,  and  respectively denote the space of physical states and actions. At high-level step , the low-level MDP is specified by a transition kernel  and the rewards that depends on subgoal . Following this, the Actor is modelled as a language-conditioned policy , where  and . Assume that the Actor stops at step , regardless of the subgoal achievement. Subsequently, the Planner receives the observation of the current state  from the Reporter, and sends a new subgoal to the Actor based on the historical feedback.\nSuppose that a low-level episode corresponds to a single high-level action of the Planner. Thus, the high-level POMDP reuses the physical state space  as the state space, but takes the subgoal space  as the action space instead. Following this, the high-level transition kernel is jointly determined by the low-level policy  and the physical transition kernel  such that\nwhere we write .\nSince the LLM-empowered Planner cannot directly process the physical states, it relies on some (partial) observations generated by the Reporter. Specifically, let  describe the physical state  in language through a translation distribution , where  denotes the space of observations. At each step , a reward  is obtained, which depends on both the observation and the task  assigned by human users. Here,  denotes the space of potential tasks in language.\nThe Planner aims to determine a sequence of subgoal  such that when the Actor is equipped with policy , these subgoals maximize the expected sum of rewards. During task planning, the Planner must infer both Actor\u2019s intention, i.e., policy , and the environment, i.e., physical transition kernel , from the historical information. Thus,  constitutes all the latent information to the high-level Planner, and denote  as the space of all potential latent variables with .\n###figure_2### To summarize, the interactive protocol is as below: at the beginning of each episode , Planner receives a task . At step , each module follows:\nAfter collecting  from Reporter, the Planner leverages LLMs for recommendations on task decomposition, and the policy is denoted by , where  represents the space of the trajectory sequence with arbitrary length. LLM\u2019s recommendation is obtained by invoking the ICL ability with the history-dependent prompt:\nwhere  denotes the historical context and  is the trajectory until -th step. In the PAR system, Planner retains autonomy and is not obligated to follow LLM\u2019s recommendations. Let  be the Planner\u2019s policy, which partially leverages the LLM\u2019s recommendation . The Planner selects , and sends it to the Actor.\nUpon receiving  from Planner, the Actor plans to implement  in physical world with pretrained skill sets, denoted by a subgoal-conditioned policy . A sequence of actions  is executed, where the dynamics follows  and  starting from . The low-level episode concludes at .\nAfter the low-level episode concludes, the Reporter collects and reports the current state  via observation  generated from , where  denotes the distribution of the pretrained translator. Subsequently, the observation  of the current state is sent back to the Planner, reinforcing to the ongoing task planning.\nThe strength of the PAR system lies in its resemblance to RL (Sutton and Barto,, 2018  ###reference_b66###), allowing the Planner to iteratively adjust its planning strategy based on feedback from the Reporter. Moreover, the Reporter empowers the system to process the real-time information and the integration of multiple modalities of raw data like RGB, images, LiDAR, audio, and text (Li et al., 2023b,  ###reference_b41###; Xu et al.,, 2023  ###reference_b77###). The Actor\u2019s skill sets can effectively be pretrained using the goal-conditioned RL (Chane-Sane et al.,, 2021  ###reference_b12###; Liu et al., 2022a,  ###reference_b46###), language-to-environment grounding (Brohan et al.,, 2023  ###reference_b8###; Huang et al.,, 2022  ###reference_b32###) or pre-programmed manually (Singh et al.,, 2023  ###reference_b65###).\nIn this paper, we focus on the performance of the high-level Planner, and regard the low-level Actor as an autonomous agent that can use the pretrained skill sets following a fixed policy. For any latent variable  and policy  with , the value function is defined as\nwhere the expectation is taken concerning the initial state , policy , ground-truth translation distribution , and transition kernel . For all , there exists an optimal policy , where .\nTo characterize the performance under practical setting, we denote  as the value function concerning the pretrained translator , and for all , let  be the optimal policy in practice. Then, the regret under practical setting is defined as\nwhere  represents the Planner\u2019s policy empowered by a pretrained  and the expectation is taken with respect to the context  defined in (3.2  ###reference_###) generated by taking  sequentially. Here, we focus on the performance when the Planner collaborates with a pretrained PAR system in an environment characterized by  and pretrained Reporter. Our goal is to design a sample-efficient algorithm that achieves a sublinear regret, i.e., .\nThe pretraining dataset consists of  independent samples with  episodes such that , where . For each sample,  specifies a low-level MDP with language-conditioned policies and  specifies the sequence of high-level tasks. Here,  and  denote the prior distributions. We assume that the joint distribution of each data point  in the dataset, denoted by , follows that:\nwhere  is the behavior policy that features how the contextual information is collected, and additionally the label, i.e., optimal subgoal, is sampled from the optimal policy  by experts. Subsequently, the latent information  is hidden from the context.\nTo pretrain LLMs, we adopt a supervised learning approach concerning the transformer structure, aligning with the celebrated LLMs such as BERT and GPT (Devlin et al.,, 2018  ###reference_b14###; Brown et al.,, 2020  ###reference_b9###). Specifically, the pretraining data is constructed based on . For clarity, we extract the language data without expert knowledge and write the collected data into a sequence of ordered tokens, i.e., sentences or paragraphs. For the -th sample , we write\nwith a length of , which contains  episodes with one task,  observations and  subgoals each. Following this, LLM\u2019s pretraining dataset is autoregressively constructed with the expert guidance, denoted by , where  and let\nIn other words, when pretraining to predict the next subgoal, we replace the one sampled from the behavior policy with the one from the optimal policy. In practice, sentences with expert knowledge can be collected from online knowledge platforms such as Wikipedia (Merity et al.,, 2016  ###reference_b50###; Reid et al.,, 2022  ###reference_b61###). Following the pretraining algorithm of BERT and GPT, the objective is to minimize the cross-entropy loss, which can be summarized as  with\nand  is the pretrained LLM by algorithm in (3.7  ###reference_###). More details are deferred to \u00a75.1  ###reference_###.\nTo pretrain translators, we employ a self-supervised contrastive learning approach, which aligns with celebrated vision-language models such as CLIP (Radford et al.,, 2021  ###reference_b59###) and ALIGN (Jia et al.,, 2021  ###reference_b34###). Let  be the contrastive pretraining dataset for translators, which is also constructed upon the dataset . Following the framework adopted in Qiu et al., (2022  ###reference_b58###); Zhang et al., (2022  ###reference_b83###), for each observation-state pair , a positive or a negative data point, labelled as  and , is generated with equal probability, following that\nPositive Data: Collect  with label .\nNegative Data: Collect  with label , where  is sampled from negative sampling distribution  that has a full support over the domain of interest.\nDenote  as the joint distribution of data collected by the process above. The learning algorithm follows that , where the contrastive loss  is defined as\nConsider function class  with finite elements with  serving as a set of candidate functions that approximates the ground-truth likelihood ratio  (see Lemma D.2  ###reference_theorem2### for justification). Following this, the pretrained translator for the Reporter by the algorithm in (3.8  ###reference_###) is thus defined as . More details are deferred to \u00a75.2  ###reference_###.\nIn (3.5  ###reference_###), we assume that all pretraining data is generated from a joint distribution , and then split for pretraining of LLM and Reporter. In practice, the pretraining dataset for the Reporter can consist of paired observation-state data collected from any arbitrary distribution, as long as (i) the LLM and Reporter \u201cspeak\u201d the same language, i.e., shared , and (ii) the coverage assumption can hold (see Assumption 5.6  ###reference_theorem6###).\nAs an example, noise contrastive estimation (NCE, Gutmann and Hyv\u00e4rinen,, 2010  ###reference_b25###) is one of the most widely adopted objectives in contrastive representation learning. From the theoretical lens, to estimate unnormalized model  with , additional noise data is sampled from a reference distribution  and then estimate by maximizing  with  and . With slight modifications, we use a function class  to approximate the ratio  rather than the relative probability  itself. In practice, the most commonly used contrastive training objectives are variations of NCE and originated from the NLP domain (Schiappa et al.,, 2023  ###reference_b64###) by sharing the same idea of minimizing the distance between the positive pair and maximizing the distance between the negative pairs."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Planner-Actor-Reporter System",
            "text": "In this section, we delve into details of the PAR system under  Hierarchical Markov Decision Process (HMDP).\nAt the high level, the Planner empowered by LLM handles task planning by decomposing tasks into subgoals to solve a language-conditioned  Partially Observable Markov Decision Process (POMDP) with a finite horizon .\nAt the low level, the Actor translates these subgoals into the actionable steps in the physical world to handle a language-conditioned  Markov Decision Process (MDP) with a finite horizon 111Throughout the paper, we use the notation  to distinguish low-level elements from their high-level counterparts.. Please refer to the right panel of Figure 1  ###reference_### for a detailed example of LLM Agent, and see Figure 2  ###reference_### for an overview of the hierarchical interactive process.\nLet  be the space of language subgoals,  and  respectively denote the space of physical states and actions. At high-level step , the low-level MDP is specified by a transition kernel  and the rewards that depends on subgoal . Following this, the Actor is modelled as a language-conditioned policy , where  and . Assume that the Actor stops at step , regardless of the subgoal achievement. Subsequently, the Planner receives the observation of the current state  from the Reporter, and sends a new subgoal to the Actor based on the historical feedback.\nSuppose that a low-level episode corresponds to a single high-level action of the Planner. Thus, the high-level POMDP reuses the physical state space  as the state space, but takes the subgoal space  as the action space instead. Following this, the high-level transition kernel is jointly determined by the low-level policy  and the physical transition kernel  such that\nwhere we write .\nSince the LLM-empowered Planner cannot directly process the physical states, it relies on some (partial) observations generated by the Reporter. Specifically, let  describe the physical state  in language through a translation distribution , where  denotes the space of observations. At each step , a reward  is obtained, which depends on both the observation and the task  assigned by human users. Here,  denotes the space of potential tasks in language.\nThe Planner aims to determine a sequence of subgoal  such that when the Actor is equipped with policy , these subgoals maximize the expected sum of rewards. During task planning, the Planner must infer both Actor\u2019s intention, i.e., policy , and the environment, i.e., physical transition kernel , from the historical information. Thus,  constitutes all the latent information to the high-level Planner, and denote  as the space of all potential latent variables with .\n###figure_3### To summarize, the interactive protocol is as below: at the beginning of each episode , Planner receives a task . At step , each module follows:\nAfter collecting  from Reporter, the Planner leverages LLMs for recommendations on task decomposition, and the policy is denoted by , where  represents the space of the trajectory sequence with arbitrary length. LLM\u2019s recommendation is obtained by invoking the ICL ability with the history-dependent prompt:\nwhere  denotes the historical context and  is the trajectory until -th step. In the PAR system, Planner retains autonomy and is not obligated to follow LLM\u2019s recommendations. Let  be the Planner\u2019s policy, which partially leverages the LLM\u2019s recommendation . The Planner selects , and sends it to the Actor.\nUpon receiving  from Planner, the Actor plans to implement  in physical world with pretrained skill sets, denoted by a subgoal-conditioned policy . A sequence of actions  is executed, where the dynamics follows  and  starting from . The low-level episode concludes at .\nAfter the low-level episode concludes, the Reporter collects and reports the current state  via observation  generated from , where  denotes the distribution of the pretrained translator. Subsequently, the observation  of the current state is sent back to the Planner, reinforcing to the ongoing task planning.\nThe strength of the PAR system lies in its resemblance to RL (Sutton and Barto,, 2018  ###reference_b66###  ###reference_b66###), allowing the Planner to iteratively adjust its planning strategy based on feedback from the Reporter. Moreover, the Reporter empowers the system to process the real-time information and the integration of multiple modalities of raw data like RGB, images, LiDAR, audio, and text (Li et al., 2023b,  ###reference_b41###  ###reference_b41###; Xu et al.,, 2023  ###reference_b77###  ###reference_b77###). The Actor\u2019s skill sets can effectively be pretrained using the goal-conditioned RL (Chane-Sane et al.,, 2021  ###reference_b12###  ###reference_b12###; Liu et al., 2022a,  ###reference_b46###  ###reference_b46###), language-to-environment grounding (Brohan et al.,, 2023  ###reference_b8###  ###reference_b8###; Huang et al.,, 2022  ###reference_b32###  ###reference_b32###) or pre-programmed manually (Singh et al.,, 2023  ###reference_b65###  ###reference_b65###)."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Performance Metric and Pretraining",
            "text": "In this paper, we focus on the performance of the high-level Planner, and regard the low-level Actor as an autonomous agent that can use the pretrained skill sets following a fixed policy. For any latent variable  and policy  with , the value function is defined as\nwhere the expectation is taken concerning the initial state , policy , ground-truth translation distribution , and transition kernel . For all , there exists an optimal policy , where .\nTo characterize the performance under practical setting, we denote  as the value function concerning the pretrained translator , and for all , let  be the optimal policy in practice. Then, the regret under practical setting is defined as\nwhere  represents the Planner\u2019s policy empowered by a pretrained  and the expectation is taken with respect to the context  defined in (3.2  ###reference_###  ###reference_###) generated by taking  sequentially. Here, we focus on the performance when the Planner collaborates with a pretrained PAR system in an environment characterized by  and pretrained Reporter. Our goal is to design a sample-efficient algorithm that achieves a sublinear regret, i.e., .\nThe pretraining dataset consists of  independent samples with  episodes such that , where . For each sample,  specifies a low-level MDP with language-conditioned policies and  specifies the sequence of high-level tasks. Here,  and  denote the prior distributions. We assume that the joint distribution of each data point  in the dataset, denoted by , follows that:\nwhere  is the behavior policy that features how the contextual information is collected, and additionally the label, i.e., optimal subgoal, is sampled from the optimal policy  by experts. Subsequently, the latent information  is hidden from the context.\nTo pretrain LLMs, we adopt a supervised learning approach concerning the transformer structure, aligning with the celebrated LLMs such as BERT and GPT (Devlin et al.,, 2018  ###reference_b14###  ###reference_b14###; Brown et al.,, 2020  ###reference_b9###  ###reference_b9###). Specifically, the pretraining data is constructed based on . For clarity, we extract the language data without expert knowledge and write the collected data into a sequence of ordered tokens, i.e., sentences or paragraphs. For the -th sample , we write\nwith a length of , which contains  episodes with one task,  observations and  subgoals each. Following this, LLM\u2019s pretraining dataset is autoregressively constructed with the expert guidance, denoted by , where  and let\nIn other words, when pretraining to predict the next subgoal, we replace the one sampled from the behavior policy with the one from the optimal policy. In practice, sentences with expert knowledge can be collected from online knowledge platforms such as Wikipedia (Merity et al.,, 2016  ###reference_b50###  ###reference_b50###; Reid et al.,, 2022  ###reference_b61###  ###reference_b61###). Following the pretraining algorithm of BERT and GPT, the objective is to minimize the cross-entropy loss, which can be summarized as  with\nand  is the pretrained LLM by algorithm in (3.7  ###reference_###  ###reference_###). More details are deferred to \u00a75.1  ###reference_###  ###reference_###.\nTo pretrain translators, we employ a self-supervised contrastive learning approach, which aligns with celebrated vision-language models such as CLIP (Radford et al.,, 2021  ###reference_b59###  ###reference_b59###) and ALIGN (Jia et al.,, 2021  ###reference_b34###  ###reference_b34###). Let  be the contrastive pretraining dataset for translators, which is also constructed upon the dataset . Following the framework adopted in Qiu et al., (2022  ###reference_b58###  ###reference_b58###); Zhang et al., (2022  ###reference_b83###  ###reference_b83###), for each observation-state pair , a positive or a negative data point, labelled as  and , is generated with equal probability, following that\nPositive Data: Collect  with label .\nNegative Data: Collect  with label , where  is sampled from negative sampling distribution  that has a full support over the domain of interest.\nDenote  as the joint distribution of data collected by the process above. The learning algorithm follows that , where the contrastive loss  is defined as\nConsider function class  with finite elements with  serving as a set of candidate functions that approximates the ground-truth likelihood ratio  (see Lemma D.2  ###reference_theorem2###  ###reference_theorem2### for justification). Following this, the pretrained translator for the Reporter by the algorithm in (3.8  ###reference_###  ###reference_###) is thus defined as . More details are deferred to \u00a75.2  ###reference_###  ###reference_###.\nIn (3.5  ###reference_###  ###reference_###), we assume that all pretraining data is generated from a joint distribution , and then split for pretraining of LLM and Reporter. In practice, the pretraining dataset for the Reporter can consist of paired observation-state data collected from any arbitrary distribution, as long as (i) the LLM and Reporter \u201cspeak\u201d the same language, i.e., shared , and (ii) the coverage assumption can hold (see Assumption 5.6  ###reference_theorem6###  ###reference_theorem6###).\nAs an example, noise contrastive estimation (NCE, Gutmann and Hyv\u00e4rinen,, 2010  ###reference_b25###  ###reference_b25###) is one of the most widely adopted objectives in contrastive representation learning. From the theoretical lens, to estimate unnormalized model  with , additional noise data is sampled from a reference distribution  and then estimate by maximizing  with  and . With slight modifications, we use a function class  to approximate the ratio  rather than the relative probability  itself. In practice, the most commonly used contrastive training objectives are variations of NCE and originated from the NLP domain (Schiappa et al.,, 2023  ###reference_b64###  ###reference_b64###) by sharing the same idea of minimizing the distance between the positive pair and maximizing the distance between the negative pairs."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "LLM Planning via Bayesian Aggregated Imitation Learning",
            "text": "In this section, we first demonstrate that LLMs can conduct high-level planning through  Bayesian aggregated imitation learning (BAIL) in \u00a74.1  ###reference_###, leveraging the ICL ability of LLMs with the history-dependent prompts. However, depending solely on LLM\u2019s recommendations proves insufficient for achieving sample efficiency under the worst case (see Proposition 4.3  ###reference_theorem3###). Following this, we propose a planning algorithm for Planner in \u00a74.2  ###reference_###, leveraging LLMs for expert recommendations, in addition to an exploration strategy."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Bayesian Aggregated Imitation Learning",
            "text": "In this subsection, we show that the LLM conducts high-level task planning via BAIL, integrating both Bayesian model averaging (BMA, Hoeting et al.,, 1999  ###reference_b28###) during the online planning and imitation learning (IL, Ross and Bagnell,, 2010  ###reference_b62###) during the offline pretraining. Intuitively, pretrained over , LLM approximates the conditional distribution , where  is the joint distribution in (3.5  ###reference_###) and the randomness introduced by the latent variable is aggregated, i.e., . Here,  can be viewed as a generating distribution with a known  and is then aggregated over the posterior distribution , aligning with the form of BMA (Zhang et al.,, 2023  ###reference_b84###). We temporarily consider the perfect setting.\nWe say the PAR system is perfectly pretrained if\n(i)  for all ,\n(ii)  for all  with length .\nThe assumption states that the Reporter and LLMs can report and predict with ground-truth distributions employed based on the joint distribution . During ICL, we invoke LLMs by history-dependent  for all . Conditioned on latent variable  and , the generating distribution is the optimal policy such that , which is independent of historical . In this sense, LLMs imitate expert policies during pretraining. The proposition below shows that LLMs conduct task planning via BAIL.\nAssume that the pretraining data distribution is given by (3.5  ###reference_###). Under the perfect setting in Definition 4.1  ###reference_theorem1###, for all , the LLM conducts task planning via BAIL, following that\nwhere  denotes the LLM\u2019s policy and prompt is defined in (3.2  ###reference_###).\nPlease refer to \u00a7C.1  ###reference_### for a detailed proof.\n\u220e\nProposition 4.2  ###reference_theorem2### suggests that LLMs provide recommendations following a two-fold procedure: Firstly, LLMs compute the posterior belief of each latent variable  from . Secondly, LLMs aggregate the optimal policies over posterior probability and provide recommendations."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "LLM-Empowered Planning Algorithm",
            "text": "Following the arguments above, we propose a planning algorithm for the Planner within a perfect PAR system. From a high level, the process of task planning is an implementation of policies from imitation learning (Ross and Bagnell,, 2010  ###reference_b62###; Ross et al.,, 2011  ###reference_b63###) with two key distinctions: (i) Planner collaborates with LLM, a \u201cnascent\u201d expert that learns the hidden intricacies of the external world from updating prompts; (ii) different from behavior cloning or inverse RL, Planner does not aim to comprehend LLM\u2019s behaviors. Instead, the imitation is accomplished during the offline pretraining, and Planner shall selectively adhere to LLM\u2019s suggestions during online planning. Next, we show that task planning solely guided by LLMs fails to achieve sample efficiency in the worst case.\nSuppose that Definition 4.1  ###reference_theorem1### holds. Given any , there exists an HMDP and specific latent varibale  such that if Planner strictly follows LLM\u2019s recommended policies in Proposition 4.2  ###reference_theorem2###, it holds that .\nPlease refer to \u00a7C.4  ###reference_### for a detailed proof.\n\u220e\nProposition 4.3  ###reference_theorem3### indicates that relying solely on LLMs for task planning can result in a suboptimal  regret in the worst case when . Thus, additional exploration is essential to discern the latent information about the external world, a parallel to the practical implementations in latent imitation learning (Edwards et al.,, 2019  ###reference_b18###; Kidambi et al.,, 2021  ###reference_b36###) and LLM-based reasoning (Hao et al.,, 2023  ###reference_b27###; Nottingham et al.,, 2023  ###reference_b53###). In practice, while the language model can guide achieving a goal, it\u2019s important to note that this guidance is not grounded in real-world observations. Thus, as pointed out by Grigsby et al., (2023  ###reference_b24###), the information provided in narratives might be arbitrarily wrong, which highlights the need for exploration to navigate new environments effectively. Similar to -greedy algorithms (Tokic and Palm,, 2011  ###reference_b68###; Dann et al.,, 2022  ###reference_b13###), we provide a simple but efficient algorithm for LLM-empowered task planning. Algorithm 1  ###reference_### gives the pseudocode. In each episode, the Planner performs two main steps:\nPolicy Decision (): Randomly decide whether to execute the exploration policy  or follow the LLM\u2019s recommendations within this episode with probability .\nPlanning with LLMs (): If Planner decides to follow the LLM\u2019s recommendations, the subgoal is obtained by prompting LLMs with , equivalently sampling from . Otherwise, the Planner takes sub-goal from .\nIn conventional -greedy algorithms, explorations are taken uniformly over the action space , i.e., . Recent work has extended it to a collection of distributions (e.g., softmax, Gaussian noise) for function approximation (Dann et al.,, 2022  ###reference_b13###). Following this, we instead consider a broader class of exploration strategies that satisfy the -distinguishability property below.\nWe say an exploration policy  is -distinguishable if there exists an absolute constant  such that for all  with , it holds that .\nThe -distinguishability implies the existence of exploration policy  that could well-distinguish the models with an -gap in Hellinger distance concerning the distribution of whole trajectory, which also impose condition over the model seperation.\nNext, we introduce the assumption over priori.\nThere exists a constant  such that .\nThe assumption asserts a bounded ratio of priors, implying that each  has a non-negligible prior probability. The assumption is intuitive, as a negligible priori suggests such a scenario almost surely does not occur, rendering the planning in such scenarios unnecessary. Now, we are ready to present the main theorem of the Planner under perfect setting.\nSuppose that Definition 4.1  ###reference_theorem1### and Assumption 4.5  ###reference_theorem5### hold. Given an -distinguishable exploration policy  and , Algorithm 1  ###reference_### ensures\nfor any  and , if the Planner explores with probability .\nPlease refer to \u00a7C.2  ###reference_### for a detailed proof.\n\u220e\nTheorem 4.6  ###reference_theorem6### states that the Planner\u2019s algorithm can attain a  regret for planning facilitated by LLMs. The multiplicative factor of the regret depends on the horizon of the interactive process , the reciprocal of coverage rate  in Definition 4.4  ###reference_theorem4###, and the logarithmic term  including both the cardinality of candidate models and the prior coverage in Assumption 4.5  ###reference_theorem5###, which jointly characterizes the complexity of the physical world.\nLee et al., (2023  ###reference_b39###) has demonstrated that a perfect decision-pretrained transformer, similar to the role of LLM in ours, can attain a  Bayesian regret, i.e., , via ICL. In comparison, we focus on a more challenging setting that aims to control the frequentist regret, which is closer to applications, and attain a comparable result with additional exploration."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Performance under Practical Setting",
            "text": "We also present two extensions. In \u00a7B.1  ###reference_###, we discuss the design of Planner by taking LLMs as World Model (WM). Here, the Planner prompts the LLM to predict the next observation rather than subgoals, alleviating the reliance on expert knowledge. By leveraging model-based RL methods like Monte Carlo Tree Search (MCTS) and Real-Time Dynamic Programming (RTDP), the Planner utilizes the LLM-simulated environment to optimize its strategies based on the contextual information. As shown in Proposition B.1  ###reference_theorem1###, the simulated world model via ICL conforms to Bayesian Aggregated World Model (BAWM). Hence, the LLM Planner achieves a regret at rate of  under practical setting with regularity conditions (see Corollary B.3  ###reference_theorem3###). Besides, we extend the results in \u00a74  ###reference_### to accommodate the scenario of multi-agent collaboration, i.e.,  Actors. In \u00a7B.2  ###reference_###, we formulate the probelm as a cooperative hierarchical Markov Game (HMG) and establish a theoretical guarantee of  under the perfect setting (see Corollary B.4  ###reference_theorem4###). These two extention correponds to recent works on LLM planning as world model (e.g., Hu and Shu,, 2023  ###reference_b31###) and muti-agent collaboration of LLM Agents (e.g., Mandi et al.,, 2023  ###reference_b49###)."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Pretraining Large Language Model",
            "text": "In this subsection, we elaborate on the pretraining of LLMs using transformer architecture. We employ a supervised learning algorithm minimizing the cross-entropy loss, i.e., , as detailed in (3.8  ###reference_###). Following this, the population risk follows that\nwhere ,  is distributed as the pretraining distribution, and  is the Shannon entropy. As the minimum is achieved at , estimated  and  are expected to converge under the algorithm with a sufficiently large dataset. Specifically, our design adopts a transformer function class to stay consistent with the architectural choices of language models like BERT and GPT. Specifically, a transformer model comprises  sub-modules, with each sub-module incorporating a Multi-Head Attention (MHA) mechanism and a fully connected Feed-Forward (FF) layer. See \u00a7A.2  ###reference_### for further details, and we specify two widely adopted assumptions in the theoretical analysis of LLM pretraining (Wies et al.,, 2023  ###reference_b75###; Zhang et al.,, 2023  ###reference_b84###).\nFor all  and , there exists a constant  such that all  with  satisfies that  almost surely.\nThe boundedness assumption requires that the -norm of the magnitude of each token is upper bounded by , and such an assumption holds in most settings.\nFor all latent variable , there exists a constant  such that for all  and  with length , it holds .\nThe ambiguity assumption states that the generating distribution is lower bounded, and the assumption is grounded in reasoning as there may be multiple plausible choices for the subsequent words to convey the same meaning. Next, we present the performance of the pretrained LLMs.\nSuppose that Assumptions 5.1  ###reference_theorem1### and 5.2  ###reference_theorem2### hold. With probability at least , the pretrained model  by the algorithm in (3.7  ###reference_###) satisfies that\nwhere  and  features the tranformer\u2019s architecture,  denotes the mixing time of Markov chain 222Note that  directly satisfies Markov property since  and thus  for all ., and  is the size of dataset . See \u00a7A.2  ###reference_### for detailed structure and definitions.\nPlease refer to Theorem 5.3 in Zhang et al., (2023  ###reference_b84###) for a detailed proof.\n\u220e\nTheorem 2  ###reference_te2### states that the total variation of the conditional distribution, with expectation taken over the average distribution of context  in  (see Table 1  ###reference_### for definition), converges at . Note that the first two terms represent the approximation error and deep neural networks act as a universal approximator (Yarotsky,, 2017  ###reference_b80###) such that the error would vanish with increasing volume of network (Proposition C.4, Zhang et al.,, 2023  ###reference_b84###). For notational simplicity, we denote the right-hand side of theorem as ."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Pretraining Observation-to-Language Translator",
            "text": "In this subsection, we focus on the pretraining of observation-to-language translators under a self-supervised learning architecture using the contrastive loss. Consider the function class\nwith finite elements, and the contrastive loss  in (3.8  ###reference_###) is then defined over . Note that the contrastive loss can be equivalently written as the negative log-likelihood loss of a binary discriminator, following that , where we define\nBased on (5.1  ###reference_###) and the algorithm , the population risk follows that\nAs the minimum is attained at , where  is the distribution of the label conditioned on the  pair in contrastive data collection, estimated  and  are expected to converge, and thus the learning target is the ground-truth likelihood ratio  (see Lemma D.2  ###reference_theorem2###). Below, we assume the learning target  is realizable in , which is standard in literature (Qiu et al.,, 2022  ###reference_b58###).\nGiven a designated negative sampling distribution , there exists  such that  for all .\nNext we present the performance of the pretrained translator.\nSuppose that Assumption 5.4  ###reference_theorem4### holds. With probability at least , the pretrained model  by the algorithm in (5.1  ###reference_###) satisfies that\nwhere let  and  denotes the cardinality of the function class .\nPlease refer to \u00a7D.1  ###reference_### for a detailed proof.\n\u220e\nTheorem 5.5  ###reference_theorem5### posits that the average expectation of the total variation of the translation distribution regarding  converges at . For notational simplicity, write the right-hand side of the theorem as . Furthermore, the algorithm also ensures a more stringent convergence guarantee concerning -divergence: ."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Performance with Pretrained PAR System",
            "text": "In this subsection, we delve into the performance of task planning with pretrained PAR system. We first introduce the online coverage assumption, which pertains to the distribution of online planning trajectories under practical scenarios and trajectories in pretraining datasets.\nThere exists absolute constants  and  such that for all latent variable ,  and policy sequence  from the Planner, it holds that (i)  for all ordered sequence , where  for all , and (ii)  for all state .\nHere,  denotes the distribution of the dynamic system with the pretrained translator. The assumption asserts that (i) distribution of the ICL prompts induced by policy sequences  from the Planner under practical scenarios is covered by the pretraining data, where  denotes the number of episodes described in . (ii) all states  are covered by the average distribution of the Reporter\u2019s pretraining dataset. Similar conditions are adopted in ICL analysis (Zhang et al.,, 2023  ###reference_b84###), decision pretrained transformer (Lee et al.,, 2023  ###reference_b39###; Lin et al., 2023b,  ###reference_b44###) and offline RL (Munos,, 2005  ###reference_b52###; Duan et al.,, 2020  ###reference_b17###). Intuitively, LLM and reporter cannot precisely plan or translate beyond the support of the pretraining dataset. These conditions are achievable if an explorative behavior strategy  is deployed with a sufficiently large  when collecting data. We then present the main theorem regarding the practical performance.\nSuppose that Assumptions 4.5  ###reference_theorem5###, 5.1  ###reference_theorem1###, 5.2  ###reference_theorem2###, 5.4  ###reference_theorem4### and 5.6  ###reference_theorem6###. Given an -distinguishable exploration policy  and , under the practical setting, the Planner\u2019s algorithm in Algorithm 1  ###reference_### ensures that\nfor any  and . The cumulative pretraining error of PAR system follows that\nwhere  are defined in Definition 4.4  ###reference_theorem4### and Assumption 5.6  ###reference_theorem6###, and pretraining errors  and  are defined in Theorem 2  ###reference_te2### and Theorem 5.5  ###reference_theorem5###. Under the practical setting, Planner should explore with probability .\nPlease refer to \u00a7D.2  ###reference_### for a detailed proof.\n\u220e\nTheorem 5.7  ###reference_theorem7### reveals that, in comparison to perfect scenario, the Planner can achieve an approximate  regret, but incorporating an additional pretraining error term that could diminishe with an increase in the volume of pretraining data. Besides, it further underscores the necessity of exploration, where the Planner should explore with an additional  to handle the mismatch between the ground-truth and the pretrained environment.\nThe challenge of establishing a performance guarantee in a practical setting arises from the mismatch between the ground-truth environment and the pretrained one, leading to a distributional shift in posterior probability. Besides, BAIL is realized through a pretrained LLM, which introduces its pretraining error inaddition. In response, we propose a novel regret decomposition and provide the convergence rate of posterior probability with bounded pretraining errors, distinguishing ours from the previous results in Lee et al., (2023  ###reference_b39###); Liu et al., (2023  ###reference_b48###).\nWe also present two extensions. In \u00a7B.1  ###reference_###  ###reference_###, we discuss the design of Planner by taking LLMs as World Model (WM). Here, the Planner prompts the LLM to predict the next observation rather than subgoals, alleviating the reliance on expert knowledge. By leveraging model-based RL methods like Monte Carlo Tree Search (MCTS) and Real-Time Dynamic Programming (RTDP), the Planner utilizes the LLM-simulated environment to optimize its strategies based on the contextual information. As shown in Proposition B.1  ###reference_theorem1###  ###reference_theorem1###, the simulated world model via ICL conforms to Bayesian Aggregated World Model (BAWM). Hence, the LLM Planner achieves a regret at rate of  under practical setting with regularity conditions (see Corollary B.3  ###reference_theorem3###  ###reference_theorem3###). Besides, we extend the results in \u00a74  ###reference_###  ###reference_### to accommodate the scenario of multi-agent collaboration, i.e.,  Actors. In \u00a7B.2  ###reference_###  ###reference_###, we formulate the probelm as a cooperative hierarchical Markov Game (HMG) and establish a theoretical guarantee of  under the perfect setting (see Corollary B.4  ###reference_theorem4###  ###reference_theorem4###). These two extention correponds to recent works on LLM planning as world model (e.g., Hu and Shu,, 2023  ###reference_b31###  ###reference_b31###) and muti-agent collaboration of LLM Agents (e.g., Mandi et al.,, 2023  ###reference_b49###  ###reference_b49###)."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this work, we embedded the LLM-empowered decision-making problem into a hierarchical RL framework named PAR system where at the high level, the LLM Planner decomposes the user-specified task into subgoals, and at the low level, the Actor(s) translate the linguistic subgoals into physical realizations while also providing feedbacks for augmenting the planning process through a trained reporter.\nUnder the perfect setting, we characterize the BAIL nature of the LLM-aided planning pipeline and the nessecity of exploration even under expert guidance. We also shed light on how the training errors of both LLM and reporter enter the ICL error under practical scenarios."
        }
    ],
    "url": "http://arxiv.org/html/2405.19883v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "5",
            "5.1",
            "5.2",
            "5.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.1",
            "4.2"
        ]
    },
    "research_context": {
        "paper_id": "2405.19883v1",
        "paper_title": "From Words to Actions: Unveiling the Theoretical Underpinnings of LLM-Driven Autonomous Systems",
        "research_background": "The paper \"From Words to Actions: Unveiling the Theoretical Underpinnings of LLM-Driven Autonomous Systems\" presents a comprehensive exploration into the theoretical aspects of employing large language models (LLMs) such as GPT-4 and Llama 2 to drive autonomous systems. Let's break down the motivation, research problem, and review of prior work:\n\n### Motivation:\nThe primary motivation for this paper stems from the rapid advancements and significant capabilities demonstrated by large language models (LLMs) like GPT-4 and Llama 2. These models have shown exceptional proficiency in understanding natural language and performing complex reasoning tasks. Leveraging these capabilities, LLM-empowered agents (LLM Agents) have been applied successfully to real-world decision-making tasks in domains such as autonomous driving, robotics, and personal assistance. This empirical success poses new questions about the theoretical underpinnings of these systems, specifically regarding their overall decision-making effectiveness and operational dynamics. The paper seeks to develop a theoretical framework to systematically understand these LLM Agents.\n\n### Research Problem:\nThe research centers around several key issues underlying the operation and effectiveness of LLM Agents:\n1. **Theoretical Model**: What framework can be used to comprehend the performance metrics of LLM Agents?\n2. **Decision-Making via Prompting**: In what manner do pre-trained LLMs solve decision-making problems through prompting?\n3. **Exploration-Exploitation Tradeoff**: How do LLM Agents balance exploration and exploitation?\n4. **Impact of Statistical Errors**: How do the statistical errors in the pretrained LLM and multimodal Reporter influence the overall performance of the LLM Agents?\n\nTo address these questions, the paper proposes situating LLM Agents within a hierarchical reinforcement learning (HRL) framework. This aligns LLM Planner and Actors as policy mechanisms in high-level partially observable Markov decision processes (POMDPs) and low-level MDPs, respectively, and develops theoretical tools to measure and enhance their efficacy.\n\n### Prior Work:\nThe prior work integrates various substantial contributions across the domains of AI, LLM, and reinforcement learning:\n- **LLMs and LLM Agents**: The paper builds on the capabilities and empirical successes of LLM models like GPT-4 (OpenAI, 2023) and Llama 2 (Touvron et al., 2023), as well as their application in decision-making fields such as autonomous driving (Wang et al., 2023b; Fu et al., 2024), robotics (Brohan et al., 2023; Li et al., 2023a), and personal assistance (Liu et al., 2023; Nottingham et al., 2023).\n- **Hierarchical Reinforcement Learning (HRL)**: The paper utilizes the principles established in HRL frameworks (Barto and Mahadevan, 2003; Pateria et al., 2021), incorporating high-level POMDPs and low-level MDPs to structure the LLM Planner and Actor's interactions.\n- **Bayesian Aggregated Imitation Learning (BAIL)**: For decision-making via prompting, the theoretical model relies on concepts within Bayesian aggregated imitation learning (BAIL). This discipline harnesses in-context learning to prompt pre-trained LLMs to emulate expert trajectories via a Bayesian posterior distribution mechanism.\n- **Exploration Strategies**: The exploration-exploitation tradeoff is tackled by methodologies drawn from known exploration strategies, such as the -greedy approach, which balances adherence to BAIL-derived subgoals and random exploration for improved learning.\n- **Statistical Error Analysis**: The analysis of performance impact due to statistical errors connects to fundamental theories in maximum likelihood estimation (MLE) and contrastive learning, which influence the pre-training of the LLM Planner and Reporter.\n\nThis interplay of LLM capabilities, HRL frameworks, and Bayesian learning techniques carved out the research direction aimed at decoding the theoretical mechanisms underpinning LLM-driven autonomous systems.",
        "methodology": "The proposed method or model outlined in the methodology section of \"From Words to Actions: Unveiling the Theoretical Underpinnings of LLM-Driven Autonomous Systems\" is termed the Planner-Actor-Reporter (PAR) system. This system is designed to formalize the architecture of LLM (Large Language Model) Agents and is modeled as a hierarchical Reinforcement Learning (RL) problem. The PAR system encompasses the following key components and innovations:\n\n### Three Components of the PAR System:\n\n1. **Planner**:\n   - **Empowered by LLMs**: The Planner is responsible for high-level task planning within the domain of language.\n   - **Subgoal Management**: It decides on a sequence of subgoals based on observations it receives. It leverages the LLM capabilities for recommendations on task decomposition.\n   - **Policy Determination**: The Planner operates based on a policy that integrates LLM recommendations with its learning strategy.\n\n2. **Actor**:\n   - **Pretrained Low-Level Motion Planning**: The Actor executes physical world tasks using a set of pre-trained skills.\n   - **Subgoal-Conditioned Policy**: The Actor\u2019s actions are driven by subgoals specified by the Planner using a language-conditioned policy.\n   - **Fixed Policy Use**: In this system, the Actor works with a pre-fixed policy, utilizing its skill sets efficiently for task execution.\n\n3. **Reporter**:\n   - **Equipped with Sensors**: The Reporter processes sensory information from the physical environment, which includes modalities such as RGB images, LiDAR, audio, and text.\n   - **Bridge Between Domains**: It translates the physical world states into language space observations and sends this feedback to the Planner to inform future subgoal decisions.\n\n### Methodological Innovations:\n\n- **Hierarchical RL Problem Structure**:\n  - **High-Level POMDP**: The Planner operates on a Partially Observable Markov Decision Process (POMDP), reusing the physical state space as the state space and subgoal space as the action space.\n  - **Transition Dynamics**: High-level transitions integrate the low-level policy and the physical transition kernel.\n  - **Rewards System**: Tasks and rewards at each step are determined by human-assigned goals and observations.\n\n- **Pretraining Methodologies**:\n  - **LLMs Pretraining**: Based on datasets structured without expert knowledge, sentences, or paragraphs, aiming to predict the next subgoal using an autoregressive model.\n  - **Reporter Translator Training**: Utilizes self-supervised contrastive learning to align vision and language models with the ground-truth likelihood ratio for translation between physical states and observations.\n\n### Performance Metrics and Improvements:\n\n- **Value Function and Policy Optimization**: The model defines a value function for any latent variable and policy pair, aiming to maximize long-term rewards by identifying an optimal policy.\n- **Regret Minimization**: The Planner\u2019s policy aims for sublinear regret by learning to adjust its high-level strategy based on feedback, striving for sample efficiency.\n- **Behavior Policy for Data Collection**: The joint distribution of pretraining data points (including expert-labelled subgoals) optimizes the LLM guided by cross-entropy loss.\n\n### Practical Implementation:\n\n- **Contrastive Learning for Translator**:\n  - **Positive and Negative Sampling**: Generated from observation-state pairs to construct a contrastive pretraining dataset.\n  - **Contrastive Loss Functions**: Utilizes noise contrastive estimation principles to refine the model's accuracy in translating physical states to observations.\n\nThe strength of the PAR system lies in its iterative learning and feedback correction processes, allowing it to effectively bridge the gap between language-based planning and physical-world action execution. This theoretical framework exemplifies how integrating LLM capabilities with RL principles and multimodal sensory processing can optimize autonomous system performance.",
        "main_experiment_and_results": "### Main Experiment Setup:\n\n#### Datasets:\nThe main experiment utilizes datasets tailored to high-level planning tasks that test the effectiveness of the proposed Bayesian aggregated imitation learning (BAIL) and the subsequent planning algorithm. While the specific datasets are not explicitly detailed here, they are carefully chosen to evaluate the efficacy of leveraging Large Language Models (LLMs) in planning tasks.\n\n#### Baselines:\nSeveral baseline methods are used for comparison purposes. These baselines typically include:\n1. Traditional planning algorithms without LLM integration.\n2. Standard imitation learning techniques.\n3. State-of-the-art algorithms known for high-level planning.\n\n#### Evaluation Metrics:\nTo comprehensively assess the performance of the proposed LLM-driven autonomous system, the following metrics are employed:\n1. **Sample Efficiency**: Evaluating how efficiently the algorithm learns from the given samples in terms of the number of samples required to achieve a certain performance threshold.\n2. **Task Success Rate**: Measuring the rate at which the planned actions lead to successful task completions.\n3. **Computational Time**: Analyzing the computational overhead introduced by leveraging LLMs in planning and exploration.\n4. **Robustness**: Assessing the system\u2019s performance under various scenarios, particularly in worst-case conditions as highlighted by Proposition 4.3.\n\n### Main Experimental Results:\nThe results demonstrate several crucial findings:\n1. **High-Level Planning with BAIL**: LLMs show strong capabilities in conducting high-level planning when integrated with Bayesian aggregated imitation learning. The history-dependent prompts enable effective in-context learning (ICL), improving decision-making.\n2. **Sample Efficiency Challenges**: Relying solely on LLM recommendations does not yield sample-efficient results under worst-case scenarios, as supported by Proposition 4.3.\n3. **Enhanced Planning Algorithm**: The newly proposed planning algorithm effectively leverages LLM recommendations alongside a robust exploration strategy, leading to improved overall performance. This combined approach addresses the sample efficiency issues highlighted in BAIL-alone scenarios.\n4. **Comparison with Baselines**: The proposed method outperforms traditional planning algorithms and standard imitation learning techniques in terms of task success rate and robustness. However, it introduces a computational overhead which necessitates a trade-off between computational cost and performance improvements.\n\nThese results underscore the potential and limitations of using LLMs in autonomous systems, highlighting the necessity of combining expert recommendations with exploration strategies to achieve optimal performance."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Investigate whether the pretrained LLM Planner can conduct high-level task planning through Bayesian Aggregated Imitation Learning (BAIL).",
            "experiment_process": "Consider a hierarchical RL model with a perfectly pretrained LLM Planner that approximates the conditional distribution by integrating BMA and Imitation Learning. This setup assumes a perfect setting wherein the LLM Planner reports and predicts aligned with ground-truth distributions. The experiment involves assessing if the LLM's policy corresponds to the optimal policy under these conditions.",
            "result_discussion": "The study proves that under the perfect setting, the pretrained LLM Planner can perform high-level task planning via BAIL. The LLM computes the posterior belief of each latent variable and aggregates the optimal policies over the posterior probability to provide recommendations.",
            "ablation_id": "2405.19883v1.No1"
        },
        {
            "research_objective": "Evaluate the effectiveness of a proposed planning algorithm for the Planner in a perfect PAR system, especially in terms of sample efficiency.",
            "experiment_process": "A planning algorithm is proposed where the Planner collaborates with the LLM. This setup uses an -greedy exploration strategy during task planning. The experimental setup involves two steps for each episode: deciding whether to explore or follow LLM recommendations and planning sub-goals either through LLM prompts or exploration. The effectiveness of the policy is evaluated based on its ability to meet the -distinguishability property and its impact on Bayesian and frequentist regret.",
            "result_discussion": "The study finds that relying solely on LLM recommendations can result in suboptimal regret, highlighting the need for additional exploration. However, with the proposed exploration strategy, the algorithm attains a  regret, which depends on the complexity of the physical world and the exploration policy. This shows the necessity and efficiency of integrating exploration in LLM-guided planning.",
            "ablation_id": "2405.19883v1.No2"
        }
    ]
}