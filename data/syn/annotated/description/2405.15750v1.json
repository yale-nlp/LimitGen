{
    "title": "Filtered Corpus Training (FiCT) Shows that Language Models can Generalize from Indirect Evidence",
    "abstract": "This paper introduces Filtered Corpus Training, a method that trains language models (LMs) on corpora with certain linguistic constructions filtered out from the training data, and uses it to measure the ability of LMs to perform linguistic generalization on the basis of indirect evidence. We apply the method to both LSTM and Transformer LMs (of roughly comparable size), developing filtered corpora that target a wide range of linguistic phenomena. Our results show that while transformers are better qua LMs (as measured by perplexity), both models perform equally and surprisingly well on linguistic generalization measures, suggesting that they are capable of generalizing from indirect evidence.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Language models (LMs) play an increasingly large role in natural language processing systems and have become capable of producing surprisingly fluent and grammatical text.\nHowever, the mechanisms underlying the acquisition and use of such linguistic proficiency remain largely unknown.\nIn particular, the degree that language learning relies on memorization versus generalization remains a topic of investigation (Hupkes et al., 2023  ###reference_b16###).\nThe reliance of LMs on large amounts of training data raises the suspicion that they do not generalize in a \u2018human-like manner\u2019 (McCoy et al., 2019  ###reference_b26###; Hu et al., 2020  ###reference_b15###; Oh and Schuler, 2023b  ###reference_b34###), but it is hard to address such questions with traditional evaluation metrics such as perplexity.\n###figure_1### This paper introduces Filtered Corpus Training (FiCT) as a method for measuring the linguistic generalization abilities of language models. As depicted in Figure 1  ###reference_###, FiCT involves training models on corpora that have been filtered to remove specific linguistic constructions, thereby testing the models\u2019 ability to generalize beyond their training data. For example: we can train a model on a corpus that has never seen subjects modified by a prepositional phrase (e.g. \u201cA sketch of lights { doesn\u2019t / *don\u2019t }\u2026\u201d), and then ask whether it can judge the grammaticality of such sentences. If a model has learned that verbs must agree with the head noun of the subject noun phrase (NP), and that NPs can be modified by PPs (e.g. from seeing these in object but not subject position), it should be capable of generalizing to the unseen PP-modified subjects.\nThis method enables us to ask whether models can form relevant linguistic generalizations from indirect evidence, or whether they require direct evidence (e.g. examples of constructions during training; Warstadt and Bowman, 2022  ###reference_b41###; Mueller and Linzen, 2023  ###reference_b29###). Furthermore, by carefully controlling for the number of parameters we can investigate the inductive biases of two major LM architectures, Transformers and LSTMs, which allows us to give more detailed answers about the recent successes of Transformer models on a fine-grained linguistic level.\nWe apply the FiCT methodology by developing filters targeting a wide range of the linguistic phenomena evaluated by BLiMP (\u00a73  ###reference_###; Warstadt et al., 2020  ###reference_b42###) and training both LSTM and Transformer LMs on the resulting corpora (\u00a74  ###reference_###). Our results (\u00a75  ###reference_###) show that while Transformers are uniformly better qua language models (as measured by perplexity), their linguistic generalization abilities are not better than that of the LSTMs (as measured by a metric we introduce called accuracy delta), demonstrating a dissociation between perplexity and linguistic generalization. Furthermore, for both models, the impact of filtered corpus training on grammaticality judgments is quite low, suggesting that language models are able to form sophisticated linguistic generalizations on the basis of only indirect evidence (as discussed in \u00a76  ###reference_###)."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Surprisal Theory",
            "text": "Perplexity should be augmented with other evaluations that specifically target the models\u2019 ability to generalize in a non-human-like way."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Targeted Syntactic Evaluations",
            "text": "Perplexity should be augmented with other evaluations that specifically target the models\u2019 ability to generalize in a non-human-like way. Such investigations often draw on psycholinguistic paradigms, treating language models as participants in order to learn what such models \u201cknow\u201d about specific linguistic phenomena (Futrell et al., 2019  ###reference_b8###; Ettinger, 2020  ###reference_b6###). A common paradigm in this body of literature, usually referred to as \u201ctargeted syntactic evaluations\u201d (Linzen et al., 2016  ###reference_b23###; Jumelet and Hupkes, 2018  ###reference_b18###; Marvin and Linzen, 2018  ###reference_b25###; Gauthier et al., 2020  ###reference_b9###; Newman et al., 2021  ###reference_b30###) involves comparing language models\u2019 preferences between minimal pairs of sentences: a model is deemed to understand a phenomenon if it assigns a higher probability to the grammatical alternation. The benchmark with the widest coverage over linguistic phenomena is the Benchmark of Linguistic Minimal Pairs (BLiMP, Warstadt et al., 2020  ###reference_b42###). BLiMP consists of 67 different benchmarks, each consisting of 1,000 minimal pairs, which target twelve different linguistic areas, broadly construed, across morphology, syntax, semantics, and the syntax-semantics interface. This is the benchmark we use as a primary means of evaluation in the present investigation, discussed in greater detail in \u00a74  ###reference_.SSS0.Px4###."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Linguistic Generalization",
            "text": "While targeted syntactic evaluations give an insight into a model\u2019s linguistic competence, it does not show how a model acquires this notion of grammaticality. In this paper we focus on two kinds of linguistic generalizaton. Structural generalization (Hupkes et al., 2023  ###reference_b16###) asks: can language models make grammaticality judgments in syntactically more complex constructions than seen during training? One line of work approaches this question from a fine-tuning perspective: by fine-tuning a model on particular set of constructions we can measure the impact this has on other linguistic constructions (Prasad et al., 2019  ###reference_b36###; Weber et al., 2024  ###reference_b44###). Lexical generalization asks whether models can generalize a seen construction to new lexical items that it has not seen in that construction (Kim and Linzen, 2020  ###reference_b19###). In order to gain a causal perspective on how the training data influences model performance, we retrain models from scratch on filtered corpora. This methodology has been deployed in earlier work to investigate how LMs learn the licensing conditions of negative polarity items from different contexts (Jumelet et al., 2021  ###reference_b17###; Weber et al., 2021  ###reference_b43###). Warstadt (2022  ###reference_b39###) investigates the poverty of the stimulus debate through the lens of filtered corpora, focusing on the phenomenon of subject auxiliary inversion. Finally, Misra and Mahowald (2024  ###reference_b28###) investigate rare adjective noun constructions and manipulate training corpora to investigate how models acquire an understanding of rare constructions. Whereas most of these focus on a particular linguistic construction, our work applies the approach to a wide range of phenomena. Perplexity should be augmented with other evaluations that specifically target the models\u2019 ability to generalize in an alien-like way."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Filtered Corpus Training (FiCT)",
            "text": "This section first introduces the logic of the FiCT method before detailing the specific filters that we use in our experiments. The final experimental setup is described in \u00a74  ###reference_###. Code and data, as well as a link to all models on the HuggingFace Hub, can be found at https://github.com/CLMBRs/corpus-filtering  ###reference_### .\nThe benchmark targeted by this filter tests subject-verb number agreement in the presence of an intervening distractor in a prepositional phrase, as illustrated in Figure 1  ###reference_###. agr-pp-mod filters all sentences containing the dependency structure\n. The resulting filtered corpus will still contain PPs modifying nouns in other contexts (e.g. object position). If a learner has formed a general \u2018rule\u2019 for subject-verb agreement, and seen PP-modified objects, it should be able to generalize to agreeement with PP-modified subjects, even when it hasn\u2019t seen them during training.\nThis filter is similar to the previous one, but targets sentences where the distractor occurs in a relative clause in subject position, removing all sentences containing the structure , e.g. \u201cThe boys that aren\u2019t disturbing Natalie dream\u201d.\nA model might generalize again from its general \u2018rule\u2019 for subject-verb agreement, and learn about relative clause structure from relative clauses in object position.\nWe use the list of negative polarity items (NPIs) provided by Jumelet et al. (2021  ###reference_b17###) and filter as follows: npi-only removes all sentences with an NPI occurring after \u2018only\u2019 (e.g. \u201cOnly students have ever complained about morning classes\u201d), npi-sent-neg removes sentences with a negation and an NPI, and npi-sim-ques removes questions with NPIs in them.\nIn each of these cases the model can generalize NPI licensing conditions for a particular environment from other environments that are still present.\nSuperlative quantifiers (e.g., at least, at most) can not be embedded under negation: An actor arrived at at most six lakes vs. *No actor arrived at at most six lakes.\nBLiMP targets this phenomenon in two ways: either by replacing the superlative quantifier under negation with a relative quantifier (e.g. more than 5), or by removing the negation.\nWe can not detect superlative quantifiers based on dependency information alone, so we use morphological feature annotations.\nNext, we filter all such constructions that appear in object position: .\nIt is less clear for this filter how a model can still infer the grammaticality from other constructions that are not covered by the filter.\nWeak quantifiers can occur in the scope of existential there constructions, whereas strong quantifiers can not: There are many people here vs. *There are all people here (Milsark, 1974  ###reference_b27###).\nBLiMP targets this phenomenon in two ways: either replacing a weak quantifier with a strong one, or increasing the scope of a locative there such that it becomes existential.\nWe filter all weak quantifiers occurring in subject position under an existential there: .\nHowever, we only filter the 5 weak quantifiers occurring in the BLiMP benchmark (a(n), no, some, few, many), which still allows a model to generalize from other weak quantifiers to infer the grammaticality conditions.\nFurthermore, weak vs. strong quantification plays a role in other linguistic phenomena as well, a fact which a learner could leverage.\nFour filters, binding-c-command, binding-case, binding-domain, and binding-reconstruction target the seven binding-related benchmarks of BLiMP. All seven benchmarks typify various facets of Chomsky (1993  ###reference_b4###)\u2019s Principle A.\nThe implementations of all four filters is generally similar: they target sentences where a reflexive or non-reflexive pronoun occurs in the specific context(s) illustrated by the corresponding benchmarks, narrowly construed, while leaving in sentences where the same or similar principle is applied in a different environment.\nFor example, the binding-c-command filter removes evidence of the use of the c-command relationship in anaphora licensing in relative clauses, but not elsewhere, as in sentences like Mary\u2019s brother hurt himself (but not *Mary\u2019s brother hurt herself).111BLiMP assumes a straightforward one-to-one relationship between certain names and their grammatical gender. While such a relationship may not actually be borne out in practice today, the corpora used in this investigation likely do adhere to such a formulation. The other three benchmarks operate in similar ways.\nOne of the filters targeting determiner-noun agreement focuses on cases where and adjective occurs between a demonstrative determiner and a noun, e.g. These/*This red cars.\nWe create a filter that removes all occurrences of a demonstrative determiner followed by an adjective and a noun.\nA model can then still infer the number agreement from determiner/noun pairs without an intervening adjective.\nThe four BliMP benchmarks targeted by agr-re-irr-sv all test language model performance on subject-verb agreement, targeting \u201cregular\u201d plurals, like dress/dresses and \u201cirregular\u201d plurals, like goose/geese. The filter removes all sentences with nominal subjects where the noun occurs in any of the four benchmarks. A learner on this filtered corpus can still solve the benchmark if it develops a notion of grammatical number, a representation of the grammatical number of the nouns in the benchmark based on their usage in other contexts, and then generalizes the subject-verb agreement it sees for other nouns to these nouns.\nThe other filter besides det-adj-noun that targets determiner-noun agreement for demonstrative determiners (e.g. These/*This books) does so with the determiner directly adjacent to the noun.\nWe create a filter based on all nouns occurring in the BLiMP benchmark that are preceded by a demonstrative determiner.\nA model can still infer the number agreement between determiner and noun from other nouns, and learn the number information of the filtered nouns from other agreement tasks like subject-verb agreement.\nPassive constructions can only be formed by transitive verbs.\nBLiMP targets this phenomenon by replacing transitive verbs in passive constructions by intransitive verbs: John is insulted by Mary vs. *John is smiled by Mary.\nMuch like agr-re-irr-sv and det-noun, the passive filter operates by removing sentences that contain words on a word list in a specific linguistic environment.\nConcretely, this word list consists of the verbs that are actually used in these two benchmarks in passive form, and the filter removes sentences where such words appear in passive voice."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Logic of the Method",
            "text": "The core methodological basis of this paper is what we call Filtered Corpus Training, or FiCT.\nThis involves comparing the performance of otherwise identical learners that are trained on data which differs in some interesting way.\nIn this paper, the FiCT methodology is primarily used to test if LMs are capable of extrapolating linguistic rules learned from environments in training data to unseen environments.\nIn order to ensure that the specified environments are not seen in the training data, we use filters to remove sentences with the specified environments from a naturalistic corpus.\nBy comparing models trained on the ablated data and models trained on the full, naturalistic corpus, we can potentially determine if, how, and when language models are able to make such generalizations.\nFigure 1  ###reference_### illustrates the logic of our method. The sentence pair \u201cA sketch of lights {doesn\u2019t / *don\u2019t} appear\u201d contains a subject with a prepositional phrase (PP) modifying a noun, itself with a noun that differs in number from the main subject. We filter from the training corpus all sentences with subjects containing PP modifiers, and then compare the ability to make the correct grammaticality judgments on this pair between a model trained on the full corpus and this filtered corpus. This difference in performance we call  (formally defined in \u00a74  ###reference_###). A model that has not seen PP-modified subjects could still make the correct judgments by forming the following generalizations: verbs agree with the head noun of the subject, and noun phrases with PP modifiers (which can be seen in object, but not subject position) are headed by the main noun. Low  would then provide evidence that the model has developed such generalizations.\nThe filters used in the present investigation are listed in Table 1  ###reference_###, along with the BliMP benchmark(s) each targets, and some descriptive summary statistics for each.\nThese filters utilized part-of-speech, morphological features, and syntactic dependency annotations generated via the use of Stanza (Qi et al., 2020  ###reference_b37###), an off-the-shelf package that uses pretrained neural models to generate grammatical annotations within the framework of Universal Dependencies (UD) (Nivre et al., 2017  ###reference_b32###, 2020  ###reference_b31###). We now describe the filters in more detail."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Corpus Filters",
            "text": "In general, we favor \u201cstronger\u201d filters, i.e. those that include false positives (and so filter out more training data), since our goal is to ensure that the LM has not seen a given construction during training. In what follows,  means that there is a dependency from  to  with label .\nThe benchmark targeted by this filter tests subject-verb number agreement in the presence of an intervening distractor in a prepositional phrase, as illustrated in Figure 1  ###reference_###  ###reference_###. agr-pp-mod filters all sentences containing the dependency structure\n. The resulting filtered corpus will still contain PPs modifying nouns in other contexts (e.g. object position). If a learner has formed a general \u2018rule\u2019 for subject-verb agreement, and seen PP-modified objects, it should be able to generalize to agreeement with PP-modified subjects, even when it hasn\u2019t seen them during training.\nThis filter is similar to the previous one, but targets sentences where the distractor occurs in a relative clause in subject position, removing all sentences containing the structure , e.g. \u201cThe boys that aren\u2019t disturbing Natalie dream\u201d.\nA model might generalize again from its general \u2018rule\u2019 for subject-verb agreement, and learn about relative clause structure from relative clauses in object position.\nWe use the list of negative polarity items (NPIs) provided by Jumelet et al. (2021  ###reference_b17###  ###reference_b17###) and filter as follows: npi-only removes all sentences with an NPI occurring after \u2018only\u2019 (e.g. \u201cOnly students have ever complained about morning classes\u201d), npi-sent-neg removes sentences with a negation and an NPI, and npi-sim-ques removes questions with NPIs in them.\nIn each of these cases the model can generalize NPI licensing conditions for a particular environment from other environments that are still present.\nSuperlative quantifiers (e.g., at least, at most) can not be embedded under negation: An actor arrived at at most six lakes vs. *No actor arrived at at most six lakes.\nBLiMP targets this phenomenon in two ways: either by replacing the superlative quantifier under negation with a relative quantifier (e.g. more than 5), or by removing the negation.\nWe can not detect superlative quantifiers based on dependency information alone, so we use morphological feature annotations.\nNext, we filter all such constructions that appear in object position: .\nIt is less clear for this filter how a model can still infer the grammaticality from other constructions that are not covered by the filter.\nWeak quantifiers can occur in the scope of existential there constructions, whereas strong quantifiers can not: There are many people here vs. *There are all people here (Milsark, 1974  ###reference_b27###  ###reference_b27###).\nBLiMP targets this phenomenon in two ways: either replacing a weak quantifier with a strong one, or increasing the scope of a locative there such that it becomes existential.\nWe filter all weak quantifiers occurring in subject position under an existential there: .\nHowever, we only filter the 5 weak quantifiers occurring in the BLiMP benchmark (a(n), no, some, few, many), which still allows a model to generalize from other weak quantifiers to infer the grammaticality conditions.\nFurthermore, weak vs. strong quantification plays a role in other linguistic phenomena as well, a fact which a learner could leverage.\nFour filters, binding-c-command, binding-case, binding-domain, and binding-reconstruction target the seven binding-related benchmarks of BLiMP. All seven benchmarks typify various facets of Chomsky (1993  ###reference_b4###  ###reference_b4###)\u2019s Principle A.\nThe implementations of all four filters is generally similar: they target sentences where a reflexive or non-reflexive pronoun occurs in the specific context(s) illustrated by the corresponding benchmarks, narrowly construed, while leaving in sentences where the same or similar principle is applied in a different environment.\nFor example, the binding-c-command filter removes evidence of the use of the c-command relationship in anaphora licensing in relative clauses, but not elsewhere, as in sentences like Mary\u2019s brother hurt himself (but not *Mary\u2019s brother hurt herself).111BLiMP assumes a straightforward one-to-one relationship between certain names and their grammatical gender. While such a relationship may not actually be borne out in practice today, the corpora used in this investigation likely do adhere to such a formulation. The other three benchmarks operate in similar ways.\nOne of the filters targeting determiner-noun agreement focuses on cases where and adjective occurs between a demonstrative determiner and a noun, e.g. These/*This red cars.\nWe create a filter that removes all occurrences of a demonstrative determiner followed by an adjective and a noun.\nA model can then still infer the number agreement from determiner/noun pairs without an intervening adjective.\nThe four BliMP benchmarks targeted by agr-re-irr-sv all test language model performance on subject-verb agreement, targeting \u201cregular\u201d plurals, like dress/dresses and \u201cirregular\u201d plurals, like goose/geese. The filter removes all sentences with nominal subjects where the noun occurs in any of the four benchmarks. A learner on this filtered corpus can still solve the benchmark if it develops a notion of grammatical number, a representation of the grammatical number of the nouns in the benchmark based on their usage in other contexts, and then generalizes the subject-verb agreement it sees for other nouns to these nouns.\nThe other filter besides det-adj-noun that targets determiner-noun agreement for demonstrative determiners (e.g. These/*This books) does so with the determiner directly adjacent to the noun.\nWe create a filter based on all nouns occurring in the BLiMP benchmark that are preceded by a demonstrative determiner.\nA model can still infer the number agreement between determiner and noun from other nouns, and learn the number information of the filtered nouns from other agreement tasks like subject-verb agreement.\nPassive constructions can only be formed by transitive verbs.\nBLiMP targets this phenomenon by replacing transitive verbs in passive constructions by intransitive verbs: John is insulted by Mary vs. *John is smiled by Mary.\nMuch like agr-re-irr-sv and det-noun, the passive filter operates by removing sentences that contain words on a word list in a specific linguistic environment.\nConcretely, this word list consists of the verbs that are actually used in these two benchmarks in passive form, and the filter removes sentences where such words appear in passive voice."
        },
        {
            "section_id": "3.2.1",
            "parent_section_id": "3.2",
            "section_name": "3.2.1 Structural Generalization",
            "text": "In the following filters, a particular structural configuration has been completely removed from the corpus, and a model must generalize to it from similar/related configurations.\nThe benchmark targeted by this filter tests subject-verb number agreement in the presence of an intervening distractor in a prepositional phrase, as illustrated in Figure 1  ###reference_###  ###reference_###  ###reference_###. agr-pp-mod filters all sentences containing the dependency structure\n. The resulting filtered corpus will still contain PPs modifying nouns in other contexts (e.g. object position). If a learner has formed a general \u2018rule\u2019 for subject-verb agreement, and seen PP-modified objects, it should be able to generalize to agreeement with PP-modified subjects, even when it hasn\u2019t seen them during training.\nThis filter is similar to the previous one, but targets sentences where the distractor occurs in a relative clause in subject position, removing all sentences containing the structure , e.g. \u201cThe boys that aren\u2019t disturbing Natalie dream\u201d.\nA model might generalize again from its general \u2018rule\u2019 for subject-verb agreement, and learn about relative clause structure from relative clauses in object position.\nWe use the list of negative polarity items (NPIs) provided by Jumelet et al. (2021  ###reference_b17###  ###reference_b17###  ###reference_b17###) and filter as follows: npi-only removes all sentences with an NPI occurring after \u2018only\u2019 (e.g. \u201cOnly students have ever complained about morning classes\u201d), npi-sent-neg removes sentences with a negation and an NPI, and npi-sim-ques removes questions with NPIs in them.\nIn each of these cases the model can generalize NPI licensing conditions for a particular environment from other environments that are still present.\nSuperlative quantifiers (e.g., at least, at most) can not be embedded under negation: An actor arrived at at most six lakes vs. *No actor arrived at at most six lakes.\nBLiMP targets this phenomenon in two ways: either by replacing the superlative quantifier under negation with a relative quantifier (e.g. more than 5), or by removing the negation.\nWe can not detect superlative quantifiers based on dependency information alone, so we use morphological feature annotations.\nNext, we filter all such constructions that appear in object position: .\nIt is less clear for this filter how a model can still infer the grammaticality from other constructions that are not covered by the filter.\nWeak quantifiers can occur in the scope of existential there constructions, whereas strong quantifiers can not: There are many people here vs. *There are all people here (Milsark, 1974  ###reference_b27###  ###reference_b27###  ###reference_b27###).\nBLiMP targets this phenomenon in two ways: either replacing a weak quantifier with a strong one, or increasing the scope of a locative there such that it becomes existential.\nWe filter all weak quantifiers occurring in subject position under an existential there: .\nHowever, we only filter the 5 weak quantifiers occurring in the BLiMP benchmark (a(n), no, some, few, many), which still allows a model to generalize from other weak quantifiers to infer the grammaticality conditions.\nFurthermore, weak vs. strong quantification plays a role in other linguistic phenomena as well, a fact which a learner could leverage.\nFour filters, binding-c-command, binding-case, binding-domain, and binding-reconstruction target the seven binding-related benchmarks of BLiMP. All seven benchmarks typify various facets of Chomsky (1993  ###reference_b4###  ###reference_b4###  ###reference_b4###)\u2019s Principle A.\nThe implementations of all four filters is generally similar: they target sentences where a reflexive or non-reflexive pronoun occurs in the specific context(s) illustrated by the corresponding benchmarks, narrowly construed, while leaving in sentences where the same or similar principle is applied in a different environment.\nFor example, the binding-c-command filter removes evidence of the use of the c-command relationship in anaphora licensing in relative clauses, but not elsewhere, as in sentences like Mary\u2019s brother hurt himself (but not *Mary\u2019s brother hurt herself).111BLiMP assumes a straightforward one-to-one relationship between certain names and their grammatical gender. While such a relationship may not actually be borne out in practice today, the corpora used in this investigation likely do adhere to such a formulation. The other three benchmarks operate in similar ways.\nOne of the filters targeting determiner-noun agreement focuses on cases where and adjective occurs between a demonstrative determiner and a noun, e.g. These/*This red cars.\nWe create a filter that removes all occurrences of a demonstrative determiner followed by an adjective and a noun.\nA model can then still infer the number agreement from determiner/noun pairs without an intervening adjective."
        },
        {
            "section_id": "3.2.2",
            "parent_section_id": "3.2",
            "section_name": "3.2.2 Lexical Generalization",
            "text": "In the following filters we do not filter out an entire configuration, but only do so for a subset of lexical items.\nThis way a model can indirectly generalize to a specific occurrence of the configuration from other occurrences, but no longer rely on direct co-occurrences. These filters focus on lexical generalization because the BLiMP benchmarks that they target are centered around particular lexical items and not particular syntactic constructions.\nThe four BliMP benchmarks targeted by agr-re-irr-sv all test language model performance on subject-verb agreement, targeting \u201cregular\u201d plurals, like dress/dresses and \u201cirregular\u201d plurals, like goose/geese. The filter removes all sentences with nominal subjects where the noun occurs in any of the four benchmarks. A learner on this filtered corpus can still solve the benchmark if it develops a notion of grammatical number, a representation of the grammatical number of the nouns in the benchmark based on their usage in other contexts, and then generalizes the subject-verb agreement it sees for other nouns to these nouns.\nThe other filter besides det-adj-noun that targets determiner-noun agreement for demonstrative determiners (e.g. These/*This books) does so with the determiner directly adjacent to the noun.\nWe create a filter based on all nouns occurring in the BLiMP benchmark that are preceded by a demonstrative determiner.\nA model can still infer the number agreement between determiner and noun from other nouns, and learn the number information of the filtered nouns from other agreement tasks like subject-verb agreement.\nPassive constructions can only be formed by transitive verbs.\nBLiMP targets this phenomenon by replacing transitive verbs in passive constructions by intransitive verbs: John is insulted by Mary vs. *John is smiled by Mary.\nMuch like agr-re-irr-sv and det-noun, the passive filter operates by removing sentences that contain words on a word list in a specific linguistic environment.\nConcretely, this word list consists of the verbs that are actually used in these two benchmarks in passive form, and the filter removes sentences where such words appear in passive voice."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experimental Setup",
            "text": "The base train, validation, and test\ncorpora are the English Wikipedia corpora released by Gulordava et al. (2018  ###reference_b11###), with the train corpus consisting of 3.05M sentences (83M tokens, with a vocabulary size of 50000 plus an unknown and EOS token).\nThe 15 filtered corpora are derived from this base corpus by discarding all sentences that are targeted by the filter.\nThe number of sentences and tokens discarded by each filter varied from as little as  to as much as ; for specifics, refer to Table 1  ###reference_###.\nThen, as an additional control, the fifteen filtered corpora plus the original, full training corpus were uniformly downsampled to 2.4M lines, corresponding to  the size of the original training corpus.\nIt is worth noting that the number of tokens did vary by as much as , as reflected in the rightmost column of Table 1  ###reference_###: this is explained by the fact that certain filters target longer sentences more often.\nTwo architectures are used for the models trained in this investigation: LSTMs (Hochreiter and Schmidhuber, 1997  ###reference_b14###)) and decoder-only Transformers (Vaswani et al., 2017  ###reference_b38###).\nFor each architecture we train separate models on the 16 training corporate for five random seeds each, resulting in a total of 160 models.\nModel hyperparameters were selected to control for number of parameters as closely as possible.\nThe LSTMs have two layers with embedding and hidden dimension of 1024.\nOutput and embedding layer weights were tied, and we used dropout of 0.1 during training.\nThe Transformers were constructed with feed-forward and hidden layer dimensions of 768, eight attention heads, and eight hidden layers.\nThe LSTMs and the Transformers had 68.0M and 67.1M trainable parameters, respectively.\nEach model was trained on a single A40 GPU for 40 epochs with mixed-precision training, using the AdamW optimization algorithm (Loshchilov and Hutter, 2017  ###reference_b24###), a linear scheduler with an initial learning rate of , and a batch size of 32.\nWe evaluated each model at the end of every epoch, and report results for the model with best validation perplexity. The full hyperparameter set may be found in Appendix A  ###reference_###.\nWe use four metrics\u2014three standard and one novel\u2014as the primary means of evaluation for all models.\nThe first is perplexity over the (unfiltered) test corpus of Gulordava et al. (2018  ###reference_b11###).\nThe second is accuracy on each of the 67 benchmarks in the BLiMP challenge set (Warstadt et al., 2020  ###reference_b42###). Accuracy on the BLiMP benchmarks was assessed via the \u201cfull-sentence\u201d method (Marvin and Linzen, 2018  ###reference_b25###), where a \u201csuccess\u201d, for any minimal pair, is defined by the model assigning a higher probability to the grammatical sentence in the minimal pair () than to the ungrammatical sentence ().\nHowever, the FICT methodology\u2019s main advantage lies not in looking the performance of each model in isolation, but on the difference in performance between two models that are otherwise identical but for their training data. Thus, for each model and each BLiMP benchmark, a change score (or delta) was calculated with respect to the average performance of all models of the same architecture trained on the full corpus (i.e. average over the five seeds).\nTo be more precise, with  a model type (i.e. ),  a filter, and  a benchmark,  will refer to the filtered corpus targeting , and  will refer to a model trained on . We can then define the accuracy delta by:\nwhere  refers to the accuracy of model  on benchmark .\nWe will often be interested in the case where , i.e. the benchmark(s) corresponding to the corpus filter, but report others as well.\nOur final evaluation metric looks at the probability deltas between grammatical and ungrammatical sentences, roughly the magnitude of the accuracy metric:"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "We present our results along the four metrics of \u00a74  ###reference_###: perplexity (\u00a75.1  ###reference_###), TSE accuracy (\u00a75.2  ###reference_###), accuracy delta (\u00a75.3  ###reference_###), and probability delta (\u00a75.4  ###reference_###).\n###figure_2###"
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Perplexity",
            "text": "We found that Transformers uniformly achieve lower perplexities on the test corpus than the LSTMs for all training corpora, as expected. The mean test perplexity across all corpora and random seeds was  for the Transformers and  for the LSTMs; a paired -test of mean perplexities per corpus found the difference between the model types to be significant (, ).\nAs noted in \u00a74  ###reference_.SSS0.Px1###, while we downsampled all corpora to the same number of lines, the number of tokens varies between different training corpora. Previous research has shown a clear negative relationship between the number of tokens seen in training and test corpus perplexity.\nThis effect is also present in our data, for both architectures (LSTMs: Pearson\u2019s ; Transformers: )."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "TSE Accuracy on BLiMP",
            "text": "Mean overall accuracy on all of BLiMP across different training corpora (i.e. ) was  for the LSTMs and  for the Transformers. This result was statistically significant (paired , ). Figure 5  ###reference_### in Appendix B  ###reference_### shows all of the accuracies.\nWe next look only at benchmark accuracy data where the filtered corpus targeted a given benchmark, i.e. where . Here, the mean is  for the Transformers and  for the LSTMs and this difference is not statistically significant (paired , ). In other words, we find no difference in the two models\u2019 ability to make grammaticality judgments when trained on filtered data that forces them to perform subtle generalizations, despite differences in perplexity."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Accuracy Delta",
            "text": "A table of the accuracy deltas, averaged across all random seeds, can be found in Figure 2  ###reference_###.\nMean overall accuracy delta over all benchmarks and across all training corpora (i.e. ) was  for the LSTMs and  for the Transformers. This result was statistically significant (paired , ).\nFocusing on the  cases (i.e. black-outlined cells in the chart), we note that most deltas are generally negative but fairly close to zero, with a few outliers, such as the models trained on the existential-there, agr-pp-mod, and npi-only corpora. These results suggests that, overall, learners are usually able to use various sorts of indirect evidence to acquire correct grammatical generalizations when direct evidence has been made unavailable, as otherwise we could expect much larger deltas across the board.\nWe may also observe that, for the cases where the absolute value of the deltas was appreciably larger than zero, it is not the case that one architecture is uniformly better than the other. For example, LSTMs perform better than Transformers (that is, their deltas are smaller in magnitude) on the benchmarks associated with the agr-re-irr-sv and the npi-only corpora, while the converse is true for agr-pp-mod and quantifier-existential-there. This is true even for phenomena that are seemingly relatively similar; for example, the agr-pp-mod and agr-re-irr-sv-agr filters are extremely similar, in that they both test long distance agreement in the present of a clausal distractor intervening between the subject and the verb; they differ only in the nature of that distractor.\nYet, as noted, LSTMs trained on the agr-re-irr-sv corpus have, on average, a less negative delta on the associated benchmarks than the analogous Transformer models (; for the Transformer, ); conversely, on the models trained on the agr-pp-mod corpus, it is Transformers which have the smaller magnitude delta (; Transformer, ).\nAs in the previous section, we can make this precise by analyzing all of the accuracy deltas where .\nThe mean here is  for the LSTMs and  for the Transformers and this difference is not statistically significant (paired , ).\nThat means that we again find no difference between the two architectures in the extent to which filtering affects their accuracy, despite significant differences in perplexity.\nThis suggests that perplexity does not predict the ability of a model to perform linguistic generalizations from indirect evidence.\n###figure_3### ###figure_4### ###figure_5### ###figure_6###"
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Probability Delta",
            "text": "In order to gain a more fine-grained insight into the impact of corpus filtering, we examine the results at an item-level.\nFor this we make use of the  metric, which expresses a model\u2019s magnitude of a grammaticality judgment.\nIn Figure 4  ###reference_###A we plot the average  scores for the full models for each BLiMP benchmark, averaged across seeds.\nIt can be seen here that the Transformers and LSTMs result in highly similar \u2019s (), although the Transformer scores are slightly higher on average those of the LSTMs ( vs. , respectively), which is in line with the significant difference in TSE accuracy of \u00a75.2  ###reference_###.\nFor the sake of brevity, we focus on three salient filters that each yielded distinct results: i) Subject-Verb Agreement for PP-modified subjects, in which LSTMs are more impacted than Transformers (:  vs. ); ii) NPI Only, in which LSTMs are less impacted than Transformers (:  vs. ); and iii) Binding Case, in which neither architecture is impacted by filtering.\nIn Figure 3  ###reference_### we plot the item-level scores of the LSTMs against the Transformers (averaged across seeds).\nFor each benchmark  we plot the results on the full model and the  filtered model.\nThis demonstrates that corpus filtering has the effect of moving  closer to the origin: the model becomes less certain in its grammaticality judgment.\nThe resulting  score for a benchmark is then dependent on the  scores of the full model: a sufficient margin here makes it robust to the decrease in  and allows it to correctly assign higher probability to the grammatical item.\nTo investigate this observation across all benchmarks we plot the difference in  going from full to Filtered in Figure 4  ###reference_###B.\nThis difference represents the absolute impact of filtering on the TSE task.\nBy plotting the Transformer results against the LSTM we gain an insight whether filtering has a similar impact on both architectures.\nWe observe a strong correlation between these  differences (, ).\nSubtle difference are present, however, for a number of filters the  score increases after filtering which is especially prevalant for the Transformer models.\nFinally, we examine the robustness of a model\u2019s grammaticality judgments: does filtering have a significant impact on the distribution of judgments?\nFor this we compute the Pearson correlation of  before and after filtering for each filter benchmark.\nA model is robust to filtering if this correlation remains high.\nIn Figure 4  ###reference_###C we plot the LSTM correlations against the Transformer.\nA striking difference between the two architectures arises here: the LSTM correlations are systematically larger than those of the Transformer.\nThis shows that LSTMs are less impacted by filtering on an item-level than Transformers."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "Our findings contribute to a growing body of research that suggest a dissociation between perplexity and more targeted evaluations of linguistic competence in artificial learners (Hu et al., 2020  ###reference_b15###).\nIn a carefully controlled setting and for a wide range of phenomena, we demonstrate that the training objective of minimizing perplexity does not predict linguistic generalization.\nThis raises interesting questions on the relation between perplexity and grammaticality judgments (Lau et al., 2017  ###reference_b21###): while Transformers are better at memorizing the structure of its training data, we show they are less capable than LSTMs of forming robust linguistic generalizations.\nAn interesting step for future work would be to uncover what language modeling aspects Transformers do excel at, which allows them to obtain a superior test perplexity.\nOur study also builds on the insights of numerous other works that use artificial learners as models for understanding human language acquisition, and gaining better insights in the inductive biases of such learners (Warstadt and Bowman, 2020  ###reference_b40###; Mueller and Linzen, 2023  ###reference_b29###; Weber et al., 2024  ###reference_b44###).\nThe present study conducts for a wide range of phenomena what Warstadt (2022  ###reference_b39###) calls a \u201cproof-of-concept [of a] large-scale controlled ablation study on the input to model learners,\u201d and finds that direct attestation of linguistic evidence is not strictly necessary for the development of sophisticated linguistic generalizations.\nRather, learners can leverage much more indirect sources of evidence to arrive at the correct generalizations.\nWhere earlier work has focused on specific linguistic constructions, such as subject auxiliary inversion (Warstadt, 2022  ###reference_b39###), relative clauses (Prasad et al., 2019  ###reference_b36###), and negative polarity items (Jumelet et al., 2021  ###reference_b17###; Weber et al., 2021  ###reference_b43###), the results of this paper essentially confirm a similar result for a much wider array of syntactic and semantic phenomena.\nWhile in many cases the ablations we performed did clearly negatively affect the performance of our artificial learners on the relevant linguistic evaluations, the magnitude of this effect was generally quite small for all but a small handful of the linguistic phenomena we analyzed.\nIn general, even when tested on the specific benchmarks corresponding to the environments that were ablated from their input, models still perform considerably better than chance.\nThus, our research provides evidence in favor of the indirect evidence hypothesis.\nNotably, we find that this is true not only for filters where there are fairly obvious sources of indirect evidence (as enumerated in \u00a73  ###reference_###), but also for filters where potential sources of indirect evidence for a correct generalization are much less clear (such as the superlative-quantifier filter).\nThis suggests that there may be complex mechanisms by which certain linguistic generalizations can be derived via highly indirect means.\nThus, our results open a door to future research that can provide a more thorough account of the source of these generalizations, with potentially significant ramifications for linguistics.\nAs just discussed, the primary contribution of this paper has been the development of the FiCT method and the use of it to demonstrate LMs\u2019 successful generalization from indirect evidence across a wide range of linguisitc phenomena. This success raises a very natural follow-up question: what explains this successful generalization behavior?\nWhile a complete answer to this question must await future work, a detailed look at the NPI cases can provide insight into what an answer may look like. Jumelet et al. (2021  ###reference_b17###) used a filtered corpus method to test LSTM LMs\u2019 understanding of negative polarity items, but then also did a further analysis to examine the basis upon which the models made their grammaticality judgments. In particular: they found (via probing classifiers) that LMs\u2019 were successfully recognizing the monotonicity of a linguistic environment and (via a novel correlation method) that these judgments of monotonicity were highly correlated with the LMs\u2019 judgment of NPI acceptability, reflecting human acceptability judgments (Deni\u0107 et al., 2021  ###reference_b5###; Chemla et al., 2011  ###reference_b3###).\nThis example suggests two paths forward for explaining the generalization observations in the present paper. On the one hand, in the same way that the monotonicity explanation was inspired by human generalization, detailed explanations of individual cases of generalization can be developed with human behavior as an initial inspiration. On the other hand, in the same way that this paper extends the filtered corpus training method to a much wider range of phenomena, one can attempt to generalize these forms of explanation on the breadth axis as well. We leave these exciting pursuits to future work."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We introduced the Filtered Corpus Training methodology and applied it to a wide range of linguistic constructions from the BLiMP benchmark. Our results show that while Transformers are better language models (via perplexity) than comparable LSTMs, the latter generalize equally well (via  and ). The relatively low  scores in general show that all of our LMs exhibit a strong ability to generalize from indirect evidence, even for models of relatively low parameter count trained on relatively small data. Future work will (i) extend this approach to models of different sizes and pretraining corpora, (ii) perform deeper analyses of the bases on which the models do make their generalizations, and (iii) analyze other forms of lexical and structural generalization through the lens of filtered corpus training."
        }
    ],
    "url": "http://arxiv.org/html/2405.15750v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2.1",
            "2.2",
            "2.3"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.2.1",
            "3.2.2"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "5",
            "5.1",
            "5.2",
            "5.3",
            "5.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "3.1",
            "3.2",
            "3.2.1",
            "3.2.2"
        ]
    },
    "research_context": {
        "paper_id": "2405.15750v1",
        "paper_title": "Filtered Corpus Training (FiCT) Shows that Language Models can Generalize from Indirect Evidence",
        "research_background": "**Motivation:**\nThe motivation for this study stems from an ongoing debate in the natural language processing (NLP) community regarding the extent to which language models (LMs) rely on memorization versus generalization when acquiring linguistic proficiency. Even though LMs, such as those based on Transformer and LSTM architectures, have demonstrated the ability to generate fluent and grammatical text, the underlying mechanisms by which these models achieve this proficiency remain largely unexplored. Concerns have been raised that LMs might not generalize in a \"human-like manner\" due to their dependency on extensive training datasets. Traditional evaluation metrics such as perplexity are insufficient to address these concerns, necessitating the development of new methodologies for measuring the generalization capabilities of LMs.\n\n**Research Problem:**\nThe research problem tackled in this paper revolves around the ability of language models to generalize from indirect evidence, rather than merely relying on direct evidence from their training data. The paper aims to investigate whether LMs can form relevant linguistic generalizations without explicit exposure to specific linguistic constructions during training. To this end, the authors introduce the Filtered Corpus Training (FiCT) methodology, which involves training LMs on specially filtered corpora that exclude certain linguistic structures. This allows an assessment of the models' capability to generalize to these unseen structures, thereby providing insights into their generalization mechanisms.\n\n**Relevant Prior Work:**\nThe paper builds upon several lines of previous research:\n\n1. **Human-like Generalization in LMs:** Works by McCoy et al. (2019) and Hu et al. (2020) have raised concerns that LMs do not generalize in a human-like manner, often requiring direct examples of constructions in their training datasets to perform accurately.\n   \n2. **Generalization vs. Memorization:** Studies, such as those by Hupkes et al. (2023), have investigated the balance between memorization and generalization in language learning, contributing to the theoretical backdrop of the current study.\n\n3. **Evaluation of Linguistic Generalization:** Warstadt and Bowman (2022) and Mueller and Linzen (2023) have examined whether LMs need direct evidence of constructions for effective language learning and generalization, aligning closely with the questions addressed in this paper.\n\n4. **BLiMP Benchmark:** The study utilizes linguistic phenomena evaluated by the BLiMP benchmark (Warstadt et al., 2020), which provides a set of linguistic tasks aimed at evaluating the grammatical capabilities of LMs.\n\nBy integrating these strands of research, the paper aims to advance our understanding of how LMs generalize from the linguistic data they are trained on, thereby bridging gaps left by traditional evaluation metrics like perplexity.",
        "methodology": "## Methodology: Filtered Corpus Training (FiCT)\n\n### Introduction\n\nThis section elucidates the logic behind the FiCT method and delves into the specific filters employed in our experiments. The detailed experimental setup is provided in an additional section. Code, data, and models are openly accessible at the provided GitHub repository link. The primary aim of these filters is to test language models' generalization capabilities in various grammatical contexts.\n\n### Subject-Verb Number Agreement\n\n1. **AGR-PP-MOD Filter**:\n    - Targets sentences containing a specific dependency structure within prepositional phrases (PPs) modifying nouns in other contexts, excluding object positions.\n    - Example: \"The boy with the balloons\".\n    - Objective: If a model has learned subject-verb agreement rules from PP-modified objects, it should generalize to PP-modified subjects.\n\n2. **Relative Clause Filter**:\n    - Filters sentences where a distractor occurs in a relative clause in the subject position.\n    - Example: \u201cThe boys that aren\u2019t disturbing Natalie dream\u201d.\n    - Objective: Models might generalize subject-verb agreement rules and apply them to relative clause structures.\n\n### Negative Polarity Items (NPIs)\n\n- **NPI-Only Filter**:\n    - Removes sentences with an NPI after 'only'.\n    - Example: \u201cOnly students have ever complained about morning classes\u201d.\n- **NPI-Sent-Neg Filter**:\n    - Removes sentences with a negation and an NPI.\n- **NPI-Sim-Ques Filter**:\n    - Removes questions containing NPIs.\n\n### Superlative Quantifiers\n\n- Filters involve replacing superlative quantifiers under negation with relative quantifiers or removing the negation in sentences.\n    - Example: from \"An actor arrived at at most six lakes\" to \"No actor arrived at at most six lakes\".\n- Uses morphological feature annotations rather than dependency information.\n\n### Weak Quantifiers in Existential Constructions\n\n1. **Existential There Filter**:\n    - Filters sentences with weak quantifiers under an existential \"there\".\n    - Example: \"There are many people here\" vs. \"*There are all people here\".\n    - Specific weak quantifiers filtered: a(n), no, some, few, many.\n\n### Binding Constraints\n\n1. **Binding-C-Command Filter**:\n    - Targets sentences utilizing c-command relationship in anaphora licensing within relative clauses.\n    - Example: removing structures like \"Mary\u2019s brother hurt himself\".\n\n2. **Other Filters (Binding-Case, Binding-Domain, and Binding-Reconstruction)**:\n    - Target similar principles in different syntactic environments without affecting other occurrences.\n\n### Determiner-Noun Agreement\n\n1. **Det-Adj-Noun Filter**:\n    - Removes instances where a demonstrative determiner is followed by an adjective and a noun.\n    - Example: \"These/*This red cars\".\n\n2. **Det-Noun Filter**:\n    - Targets sentences with demonstrative determiners directly preceding a noun.\n    - Example: \"These/*This books\".\n\n### Subject-Verb Agreement\n\n1. **AGR-RE-IRR-SV Filter**:\n    - Focuses on both regular and irregular plurals within nominal subjects.\n    - Removes sentences with specific nouns used in the benchmarks (e.g., \"goose/geese\").\n    - Designed to see if models can generalize subject-verb agreement to these nouns from other contexts.\n\n### Passive Constructions\n\n- **Passive Filter**:\n    - Removes sentences where specific verbs used in passive constructions appear.\n    - Example: Filtering sentences like \"John is insulted by Mary\".\n\n### Summary\n\nThe FiCT methodology involves the deliberate application of linguistic filters to assess the ability of language models to generalize grammatical rules. Various filters are crafted to target specific linguistic phenomena, removing certain contextual uses while leaving others intact, to test the models' learning and generalization capabilities.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\n**Datasets:**\nThe primary dataset is the English Wikipedia corpora provided by Gulordava et al. (2018). It includes:\n- **Training data:** 3.05M sentences (83M tokens), vocabulary size of 50000 plus an unknown and EOS token.\n- **Validation and test data:** Derived from the same English Wikipedia corpora.\n\nFrom this base corpus:\n- These corpora were uniformly downsampled to 2.4M lines to manage the dataset size.\n\n**Models:**\nTwo architectures were used, each model type being trained five times with different random seeds, totaling 160 models:\n- **LSTMs:**\n  - Two layers, 1024 embedding and hidden dimensions.\n  - Tied output and embedding layer weights.\n  - Dropout of 0.1.\n  - 68.0M trainable parameters.\n- **Decoder-only Transformers:**\n  - Feed-forward and hidden layer dimensions of 768.\n  - Eight attention heads and eight hidden layers.\n  - 67.1M trainable parameters.\n\n**Training Setup:**\n- **Hardware:** Single A40 GPU.\n- **Training Duration:** 40 epochs.\n- **Optimization Algorithm:** AdamW.\n- **Scheduler:** Linear with initial learning rate as specified in Appendix A.\n- **Batch Size:** 32.\n- Models evaluated at the end of each epoch; the one with the best validation perplexity was selected.\n\n**Evaluation Metrics:**\nFour metrics were used to evaluate the models:\n1. **Perplexity:** Measured on the unfiltered test corpus from Gulordava et al. (2018).\n2. **Accuracy on BLiMP Benchmarks:** \n   - Assessed via the \u201cfull-sentence\u201d method.\n   - Models were evaluated across 67 benchmarks in the BLiMP challenge set (Warstadt et al., 2020).\n3. **Probability Deltas:** The difference in probabilities assigned to grammatical vs. ungrammatical sentences.\n\n**Main Experimental Results:**\n- Both LSTMs and Transformers showed generalization capabilities from indirect evidence.\n\nIn summary, the main experiment setup successfully demonstrates that language models can generalize using standardized evaluation metrics and controlled model training conditions."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To test if language models (LMs) are capable of extrapolating linguistic rules learned from environments in training data to unseen environments.",
            "experiment_process": "The method involves comparing the performance of otherwise identical learners trained on data which differ by the absence of certain linguistic constructions. Filters are used to remove sentences with specified environments from a naturalistic corpus. By comparing models trained on the ablated data and models trained on the full corpus, the study checks for the models' ability to generalize. For example, the 'agr-pp-mod' filter removes all sentences containing subject-verb agreement structures with a distractor in a prepositional phrase. The filtered and unfiltered models' performances on specific grammatical benchmarks are then compared.",
            "result_discussion": "The outcomes suggest that models trained on filtered data can still make correct grammatical judgments by forming generalizations such as verbs agreeing with the head noun of the subject, even when they have not seen the exact construction during training.",
            "ablation_id": "2405.15750v1.No1"
        },
        {
            "research_objective": "To determine if LMs can generalize the licensing conditions of Negative Polarity Items (NPIs) from environments that remain in the training data.",
            "experiment_process": "Three filters were applied to the dataset: 'npi-only' removes all sentences with an NPI following 'only,' 'npi-sent-neg' removes sentences with a negation and an NPI, and 'npi-sim-ques' removes questions with NPIs in them. The model's performance on benchmarks related to NPIs was then evaluated to see if it generalizes from remaining environments.",
            "result_discussion": "The results show that models can generalize NPI licensing conditions for a particular environment from other environments still present in the training data. This indicates that they are capable of inferring correct contexts for NPIs without direct exposure.",
            "ablation_id": "2405.15750v1.No2"
        },
        {
            "research_objective": "To test if LMs can generalize grammatical rules from remaining contexts after filtering out specific quantifier-related constructions.",
            "experiment_process": "The study filters sentences with superlative quantifiers under negation and weak quantifiers in subject position under existential 'there' constructions. Two conditions of the superlative quantifier phenomenon were targeted: either replacing the superlative quantifier with a relative quantifier or removing the negation. The model's ability to infer the grammaticality of sentences with weak and strong quantifiers was evaluated.",
            "result_discussion": "The conclusions indicate that models can leverage the presence and conditioning of weak and strong quantifiers across various linguistic phenomena, suggesting their ability to generalize based on existing data even when specific examples are filtered out.",
            "ablation_id": "2405.15750v1.No3"
        },
        {
            "research_objective": "To evaluate if LMs can generalize binding principles in different linguistic contexts.",
            "experiment_process": "Four filters targeted BLiMP's binding-related benchmarks: 'binding-c-command,' 'binding-case,' 'binding-domain,' and 'binding-reconstruction.' Each specifically aimed at environments where specific binding principles apply, while leaving alternative applications intact. The resulting model's performance on these benchmarks was compared to fully-trained models.",
            "result_discussion": "The findings suggest that the models indeed generalize the binding principles from situations that remained in the training data to those filtered out, reinforcing their capability to infer broader grammatical constraints.",
            "ablation_id": "2405.15750v1.No4"
        },
        {
            "research_objective": "To test generalization capabilities for determiner-noun agreement across linguistically varied contexts.",
            "experiment_process": "Two filters were implemented: one removing occurrences of demonstrative determiners followed by an adjective and a noun, and another stripping out nominal subjects involved in subject-verb agreement tasks with both regular and irregular plurals. The model's ability to infer number agreement without direct training examples was then assessed.",
            "result_discussion": "Results show that models can generalize number agreement rules from existing data to the filtered contexts, sustaining accuracy in identifying correct determiner-noun and subject-verb agreements.",
            "ablation_id": "2405.15750v1.No5"
        },
        {
            "research_objective": "To explore if LMs can derive rules for passive constructions from remaining non-exemplars after filtering specific verb usage.",
            "experiment_process": "The study incorporated a filter that removed sentences containing certain verbs in passive constructions (those actively used in BLiMP benchmarks). The filtered and unfiltered models were then compared based on their handling of passive vs. active verb contexts.",
            "result_discussion": "Outcomes revealed models' ability to recognize passive construction rules and generalize these from non-verbal forms still present in the corpus, indicating robust inferential mechanisms within the models.",
            "ablation_id": "2405.15750v1.No6"
        }
    ]
}