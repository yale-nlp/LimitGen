{
    "title": "LLoCO: Learning Long Contexts Offline",
    "abstract": "Processing long contexts remains a challenge for large language models (LLMs) due to the quadratic computational and memory overhead of the self-attention mechanism and the substantial KV cache sizes during generation. We propose a novel approach to address this problem by learning contexts offline through context compression and in-domain parameter-efficient finetuning. Our method enables an LLM to create a concise representation of the original context and efficiently retrieve relevant information to answer questions accurately.\nWe introduce LLoCO, a technique that combines context compression, retrieval, and parameter-efficient finetuning using LoRA. Our approach extends the effective context window of a 4k token LLaMA2-7B model to handle up to 128k tokens. We evaluate our approach on several long-context question-answering datasets, demonstrating that LLoCO significantly outperforms in-context learning while using  fewer tokens during inference. LLoCO achieves up to  speed-up and substantially reduces the cost of long document question answering, making it a promising solution for efficient long context processing. Our code is publicly available at https://github.com/jeffreysijuntan/lloco.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks (Touvron et al., 2023  ###reference_b40###; Jiang et al., 2023a  ###reference_b17###). Many of these tasks require LLMs to understand and reason about long contexts. For instance, document question answering is one of the most common applications of LLMs, where the model is presented with a document as context and asked to respond to related questions or summarize the text. These documents may range from lengthy articles to entire books, potentially surpassing the context window limit of LLMs. Consequently, there is a growing trend in both academia and industry to enhance LLMs\u2019 capability to process longer contexts effectively (Chen et al., 2023b  ###reference_b4###; Jiang et al., 2023a  ###reference_b17###; Peng et al., 2024  ###reference_b33###; Chen et al., 2024  ###reference_b5###). This need has driven innovation among LLM providers like OpenAI and Anthropic to develop models that can handle increasingly lengthy texts consisting of several thousands of tokens.\nDespite the impressive progress made by the LLM model providers, scaling these models to adeptly manage extended contexts remains a formidable challenge, both technically and financially. Due to the self-attention mechanism, transformer-based LLMs incur a quadratic computational and memory overhead as sequence length increases. Many long-context tasks require repeatedly processing over the same prompt, which incurs extra latency overhead and substantial costs, as most commercial LLMs operate on a pricing model that is directly tied to the number of tokens processed. For example, a single inference run with a document that has 100k tokens would take 1.5 USD on Claude 3 Opus111https://www.anthropic.com/api  ###reference_www.anthropic.com/api### and 1 USD on GPT-4-turbo222https://openai.com/pricing  ###reference_openai.com/pricing###.\nOrthogonal to the exiting efforts of expanding the context window limit, our study introduces an innovative strategy to tackle the long context challenge. We propose a method where context information is condensed offline through finetuning, enabling the model to provide accurate responses during inference with streamlined contextual representations. To illustrate our idea, consider an analogy: envision an LLM as a student preparing for an exam, where we, the researchers, are the examiners providing study materials and questions. Traditional in-context learning with full context or Retrieval-Augmented Generation (RAG) resembles an open-book exam, where the LLM has access to all materials while answering questions. In contrast, our approach is akin to a semi-closed-book exam, where the LLM cannot bring the entire book but is allowed to bring a cheat sheet. To excel in the exam, the student must 1) study efficiently to distill a concise yet informative cheat sheet, and 2) effectively retrieve relevant information from the cheat sheet to accurately answer exam questions. Namely, 1) How can we train a model to produce a compact representation of the original context that the LLM can interpret and utilize effectively? 2) How to enable the LLM to proficiently navigate and extract pertinent details from this representation during inference?\nRegarding the first question, we find the formulation closely resembles the existing research direction on context compression. Prior works (Chevalier et al., 2023  ###reference_b6###; Mu et al., 2023  ###reference_b29###; Ge et al., 2023  ###reference_b9###) have proposed compressors that aim to distill the essence of the original texts into compact representations that are aligned with LLMs. Our work primarily focuses on the second question \u2013 we\u2019ve observed that despite the progress in context compression, LLMs often struggle to accurately read such \u201ccheat sheets\u201d and tend to hallucinate when applying them to answer queries. To address this issue, we employ in-domain parameter-efficient finetuning directly on the compacted context (cheat sheet) without altering its content, which significantly improves the LLM\u2019s ability to accurately extract and utilize information from these compressed representations. In this way, we can steer the LLM to process long context more efficiently and accurately. This approach allows us to extend the effective context window of a 4k LLaMA2-7B model to handle up to 128k tokens. Moreover, we achieve state-of-the-art results that match or even surpass the performance of a LLaMA2-7B-32k model with full context on long context benchmarks, while using 30 times fewer tokens.\n###figure_1### This insight has led us to introduce LLoCO, a pipeline that learns contexts offline through the combination of context compression and parameter-efficient finetuning. Our pipeline consists of three stages: preprocessing, finetuning, and serving. First, we preprocess the documents into \u201ccheat sheets\u201d. Then, we employ Low-Rank Adaptation (LoRA) (Hu et al., 2022  ###reference_b13###) to perform parameter-efficient finetuning on these \u201ccheat sheets\u201d in groups.\nDuring model serving, we use a standard RAG retriever to retrieve the compressed document as well as the most relevant LoRA module, and apply them to the LLM for inference. The contributions of our work could be summarized as follows:\nWe investigate how to learn long contexts effectively and demonstrate that through a novel combination of context compression and instruction finetuning, we can extend the context window of a 4k LLaMA2-7B to handle context lengths of up to 128k tokens, while achieving performance that greatly surpasses in-context learning with 30 times fewer tokens and  latency speed-up.\nWe propose LLoCO, a novel pipeline that combines context compression, retrieval, and parameter-efficient finetuning. This pipeline could be deployed to significantly speed up and reduce the cost of long document question answering."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related work",
            "text": "Long-context LLMs Recently, there have been efforts to increase the LLMs\u2019 context window sizes efficiently with continual pretraining or finetuning. One line of work focuses on scaling the Rotary Position Embeddings (RoPE) (Su et al., 2021  ###reference_b38###), achieving shorter contexts up to k (Chen et al., 2023b  ###reference_b4###; 2024  ###reference_b5###; Peng et al., 2024  ###reference_b33###). Mistral (Jiang et al., 2023a  ###reference_b17###) proposes sliding window attention that only attends to part of tokens from the previous layer, reducing compute and enabling pretraining with long-context to k. Nevertheless, as the auto-regressive generation in LLMs is largely memory-bound (Kwon et al., 2023  ###reference_b22###), storing the KV cache of longer contexts slows down the inference and requires large GPU VRAMs. Context compression A closely related topic is context compression, which aims to train a general compressor that can compress any input prompts. GIST (Mu et al., 2023  ###reference_b29###), AutoCompressor (Chevalier et al., 2023  ###reference_b6###), and ICAE (Ge et al., 2023  ###reference_b9###) finetune LLMs in a \u201csoft prompt-tuning\u201d manner, either applying specific regularization in attention masks or utilizing dedicated \u201cmemory tokens\u201d to compress contexts into embeddings with significant shorter lengths. LLMLingua family (Jiang et al., 2023c  ###reference_b19###; b  ###reference_b18###; Pan et al., 2024  ###reference_b31###) proposes a question-aware framework to compress prompts in natural languages for black-box LLM APIs. Another line of work employs KV cache compression by eviction (Zhang et al., 2024b  ###reference_b46###; Xiao et al., 2024  ###reference_b42###), which only keeps informative keys and values for generation during inference, or quantization (Sheng et al., 2023b  ###reference_b36###; Hooper et al., 2024  ###reference_b12###). However, all those approaches aim to compress any inputs, which usually incurs rapid performance drops as the compression ratio exceeds a certain limit (e.g. -), especially for out-of-distribution texts, hindering their practical applications. In this work, we mainly focus on extreme compression of in-distribution documents of interest, going up to compression rates. Retrieval-augmented Generation Retrieval enhances the performance of LLMs in knowledge-intensive tasks such as question answering with long documents or in open-domain. RAG techniques are effective both in finetuned scenarios and when applied to off-the-shelf LLMs (Guu et al., 2020  ###reference_b11###; Lewis et al., 2020  ###reference_b24###; Zhang et al., 2024a  ###reference_b45###), and have seen many advancements recently, emphasizing improved external knowledge integration, broader query support, and enhanced content clarity (Jiang et al., 2023d  ###reference_b20###; Schick et al., 2023  ###reference_b34###; Asai et al., 2023  ###reference_b1###). Despite the improved performance from retrieval, challenges still remain in terms of runtime efficiency (Mallen et al., 2022  ###reference_b28###) and effective filtering of irrelevant parts (Shi et al., 2023  ###reference_b37###; Xu et al., 2024  ###reference_b43###). Our proposed method opens up new opportunities for capturing important information while ensuring efficient generation. Parameter-efficient Finetuning Parameter-efficient finetuning methods (Hu et al., 2022  ###reference_b13###; Lester et al., 2021  ###reference_b23###; Liu et al., 2024b  ###reference_b27###) freeze the majority of the model weights and only update a small subset to finetune large models efficiently. LoRA (Hu et al., 2022  ###reference_b13###) is one of the most widely adopted techniques, and recent advancements (Sheng et al., 2023a  ###reference_b35###; Chen et al., 2023a  ###reference_b3###) have focused on enhancing the system performance for deploying LoRA adaptors. In particular, Sheng et al. (2023a  ###reference_b35###) has achieved the deployment of thousands of LoRA instances on a single Nvidia A100 GPU. LLoCO, which utilizes LoRA for finetuning, could benefit from those improvements to deploy Lo"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "LLoCO",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Architecture Overview",
            "text": "Figure 1  ###reference_### illustrates our proposed architecture, which consists of two components: a context encoder and an LLM decoder. In a typical instruction-finetuned LLM, the prompts can be categorized into system, user, and assistant prompts. The system prompt contains task instructions and rules the LLM should follow, as well as relevant contexts. The user prompt is a question asked by the user, and the assistant prompt is the answer generated by the LLM. The context information in the system prompt can include the user\u2019s interaction history or documents related to the user\u2019s question. These contexts can become very long, potentially surpassing the LLM\u2019s context window limit.\nTo overcome the context window limitation, we propose using a context encoder to compress the original long contexts into a much more compact representation. The context encoder itself is a language model that takes a sequence of tokens as input and outputs a sequence of token embeddings. These output token embeddings, which we call summary embeddings, should be significantly shorter than the original context. The summary embeddings are then prepended to the LLM decoder and serve as the LLM\u2019s system prompt. The user\u2019s prompt is processed normally through the LLM decoder\u2019s tokenizer, and the LLM generates answers (the assistant prompt) conditioned on the summary embeddings and user prompts.\nIn our design, the context encoder can be any model capable of producing a compact representation aligned with the LLM decoder. The summary embeddings can be viewed as pseudo-words in the LLM decoder\u2019s text embedding space, representing abstract concepts or summaries. For our experiments, we select AutoCompressor for LLaMA2-7B (Chevalier et al., 2023  ###reference_b6###), a context compressor finetuned on LLaMA2-7B with the dual ability to generate summary tokens from long contexts and perform text completions conditioned on the summary tokens. The compressor groups the document into chunks of 1536 tokens and recursively compresses each chunk to 50 summary embeddings. To ensure alignment between the summary embeddings and the decoder LLM, we choose LLaMA2-7B as our decoder LLM but use the AutoCompressor\u2019s finetuned weights as the model weights.\nWhile there are other available compressors for LLaMA2-7B (Ge et al., 2023  ###reference_b9###; Mu et al., 2023  ###reference_b29###), we find AutoCompressor to be most suited for our intended use cases given that it can 1) support compressing very long context due to its recursive training procedure, and 2) achieve a great compression ratio of 30:1. We consider the construction of context encoders (compressors) to be an important and orthogonal research direction. Developing more performant and universal context encoders in a streamlined fashion can further enhance the efficiency and effectiveness of LLoCO, denoting crucial future work."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Pipeline for Offline Context Learning",
            "text": "The pipeline of LLoCO consists of three primary stages: the preprocessing stage, the finetuning stage, and the serving stage. In this section, we introduce the pipeline and how to combine it with an existing RAG system to enable retrieval-augmented document QA with compressed context.\nPreprocessing The preprocessing step in a traditional RAG system involves building a vector database to index a collection of documents. This process typically includes three main steps: 1) chunking the documents into passages if they exceed a certain length, 2) generating sentence embeddings of the passages using a text embedding model, and 3) indexing the passages using these embeddings. The result of the preprocessing step is a key-value map that associates each passage with its corresponding embedding.\nIn LLoCO, we introduce an additional preprocessing step that leverages our context encoder to process the original documents. Since we use AutoCompressor as our context encoder, we follow its practice of dividing the documents into chunks of 1536 tokens and passing them through the context encoder. The context encoder outputs 50 summary embeddings for each chunk recursively, with each embedding having a dimension of 4096. These summary token embeddings are stored alongside the original passages in the vector database, indexed by the sentence embedding of each passage.\nFinetuning\nDuring the finetuning stage, we first segment the documents into groups based on their type (e.g., academic papers, news) or the tasks that users want to perform on them (e.g., QA, summarization). For each group of documents, we perform parameter-efficient finetuning using a LoRA adaptor. The finetuning data can be in-domain instruction pairs provided by the model provider. If such a dataset does not exist, it could also be generated using self-instruct (Wang et al., 2022  ###reference_b41###) techniques or distilled from a more powerful model like GPT-4.\nAt the end of the finetuning process, we obtain a finetuned LoRA adaptor for each group of documents. In the vector database, we store an identifier for the corresponding LoRA module alongside each passage entry. Additionally, we create a separate LoRA database to store all the LoRA adaptors.\nServing\nIn a traditional RAG system, when a user asks a question, the system employs a retriever to fetch the top k relevant documents/passages from the vector database and concatenates them to the prompt to serve as the relevant context.\n###figure_2### In Figure 2  ###reference_###, we illustrate LLoCO\u2019s serving pipeline. Instead of retrieving the actual passages, we use the retriever to retrieve the compressed token embeddings of the passages. These token-embeddings are concatenated and prepended to the decoder LLM. Additionally, the system searches for the corresponding LoRA adaptor in the database and applies it to the decoder LLM. Applying a LoRA adaptor incurs minimal overhead to the system cost  (Hu et al., 2022  ###reference_b13###). By leveraging recent work on serving multiple LoRA adaptors (Sheng et al., 2023a  ###reference_b35###; Chen et al., 2023a  ###reference_b3###), we can parallelize LoRA adaptor serving and simultaneously serve requests for documents associated with thousands of LoRA adaptors on a single GPU.\nIn our current system, we assume that all the passages retrieved from the vector database for a given user query belong to the same document group, allowing us to use a single dedicated LoRA adaptor. However, our approach can be extended to handle cases where the retrieved passages span multiple document groups. For instance, it is feasible to design algorithms to dynamically select the most relevant LoRA adaptor based on the majority of the retrieved documents or weight the contributions of different LoRA adaptors according to their relevance to the query (Muqeeth et al., 2024  ###reference_b30###). Another interesting and orthogonal research direction is to explore composing multiple LoRA modules together (Huang et al., 2023  ###reference_b14###), and we leave it as future work."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In the experiment section, we aim to investigate the following aspects of LLoCO: (1) its effectiveness in comprehending compressed long contexts, (2) the extent to which summary embeddings can preserve essential information, and (3) the associated system costs.\nDatasets\nWe select four datasets dedicated to question-answering tasks\u2014QuALITY (Pang et al., 2021  ###reference_b32###), Qasper (Dasigi et al., 2021  ###reference_b7###), NarrativeQA (Ko\u010disk\u1ef3 et al., 2018  ###reference_b21###), and HotpotQA (Yang et al., 2018  ###reference_b44###)\u2014along with one for summarization, QMSum (Zhong et al., 2021  ###reference_b47###). All datasets have long documents as contexts. For all the datasets, we use their validation set for evaluation. We follow the official metrics for each dataset. For QuALITY, we report the exact match (EM) score. For QMSum, we report the geometric mean of ROUGE scores. For the remaining datasets (Qasper, NarrativeQA, and HotpotQA), we report the F1 scores. More details on these datasets can be found in Appendix A.2  ###reference_###\nModel Configuration In this study, we consider two base models. The first is the original LLaMA2-7B-chat (Touvron et al., 2023  ###reference_b40###), which has a context window of 4096 tokens. The second is Longchat-7b-v1.5-32k (Li et al., 2023  ###reference_b25###), a finetuned LLaMA2 model with an extended context window of 32,000 tokens via position interpolation. From now on, we will use LLaMA2-7B-4k to refer to the prior model and LLaMA2-7B-32k to denote the latter one. Unless otherwise specified, we set the LoRA rank to 8 for our experiments. All LLoCO\u2019s models are finetuned on AutoCompressor, which is itself finetuned on LLaMA2-7B."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Long Document QA",
            "text": "To evaluate the effectiveness of LLoCO on the aforementioned long context datasets, we consider the following scenarios:\nLLaMA-2-7B-4k/32k without Context. In this setting, we do not provide any documents to the LLM in the system prompt.\nLLaMA-2-7B-4k/32k with Context. In this setting, we provide the LLMs with the ground truth document. We truncate the documents if their length exceeds the context window limit. This setting serves as the baseline.\nLLaMA-2-7B-4k/32k with Retrieval. Retrieval is a standard baseline compression method for long document question answering. For each document, we chunk it into passages of 512 tokens and use Contriever (Izacard et al., 2021  ###reference_b16###) to retrieve the top 5 passages from this document and concatenate them to form the context.\nAutoCompressor. In this setting, we use AutoCompressor (Chevalier et al., 2023  ###reference_b6###) to compress the contexts into summary embeddings and prepend them to the LLM, without performing any in-domain finetuning. This is equivalent to using an AutoCompressor straight out of the box. The AutoCompressor compresses a chunk of 1536 tokens into 50 summary tokens, resulting in an effective context window of roughly 128k tokens. We do not truncate the context unless it exceeds this limit.\nLLoCO (ours). LLoCO is our proposed system for long document question answering. For each dataset we evaluate, we perform instruction finetuning using the question-answering pairs from the training set. During both finetuning and inference, we prepend the summary embeddings of the corresponding ground truth document to the LLM. We do not truncate the context unless it exceeds the 128k context window limit.\nOur results are summarized in Table 1  ###reference_###. When AutoCompressor is used out of the box without any in-domain finetuning, its performance sometimes falls short of the baseline where no context is appended. In contrast, by combining compression and finetuning, LLoCO significantly outperforms the baseline on all datasets by a substantial margin while using 30 times fewer tokens. Notably, for NarrativeQA, LLoCO achieves an F1 score of 28.34 compared to 14.42 for the baseline, showcasing the efficacy of our proposed technique.\nFurthermore, the effectiveness of LLoCO is particularly evident in the case of the NarrativeQA dataset, where the average document length is 84,770 tokens, significantly exceeding the context window limit of LLaMA2-7B. By compressing the contexts into approximately 2,600 tokens on average, LLoCO is able to fit the compressed representations within the context window limit. This enables LLoCO to leverage all the available context, resulting in impressive F1 scores on this challenging dataset."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Ablation Study",
            "text": "Finetuning LLaMA with Original Context\nOne interesting question is how well LoRA finetuning works over original uncompressed context, and how does that compare to LLoCO\u2019s approach. To investigate this, we conduct additional finetuning experiments on LLaMA-7B-4k/32k, where we append the original uncompressed context as a system prompt during finetuning. These models are finetuned following the same setup as LLoCO.\nAs shown in Table 2  ###reference_###, both LLaMA-7B-4k and LLaMA-7B-32k exhibit notable performance improvements after finetuning, with increases of  and  respectively. Despite these improvements, the performance of LLoCO remains comparable to that of the finetuned LLaMA-7B-32k model. This finding suggests that finetuning in a compressed context is just as effective as finetuning in the original context. Compared to full-context finetuning, LLoCO\u2019s finetuning step is considerably faster and more cost-efficient due to the use of a significantly shorter context (see 4.5  ###reference_###). This makes LLoCO a more practical finetuning solution for long document question-answering tasks.\n###table_1### Combined Instruction Finetuning \nIn our previous experiments, we finetuned LLoCO on each dataset separately. In this ablation study, we investigate LLoCO\u2019s performance when we combine all the training data and finetune a general model. To balance the dataset, we sample 10,000 examples from NarrativeQA and HotpotQA and use all examples from the other three datasets to form our final training set.\nAs shown in Table 3  ###reference_###, LLoCO with combined finetuning surpasses baseline methods on all datasets except QMSum. The lower performance on QMSum can be attributed to the fact that it is a summarization task that favors longer, more detailed answers, but the combined finetuning process likely shifts the LLM\u2019s behavior towards generating shorter, more focused answers, which may not be optimal for the summarization task.\nCompared to in-domain finetuning, combined finetuning generally yields lower performance, with the exception of QuALITY. QuALITY questions heavily rely on the LLM\u2019s reasoning abilities, whereas the other datasets primarily focus on the LLM\u2019s capacity to retrieve relevant information from long contexts. We hypothesize that the additional finetuning on diverse datasets enhances the LLM\u2019s ability to reason about long contexts, leading to improved performance on QuALITY.\nOverall, the combined finetuning approach demonstrates the potential for knowledge transfer across different tasks."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Evaluation on LongBench",
            "text": "We further evaluate LLoCO on LongBench (Bai et al., 2023  ###reference_b2###), which consists of 6 subtasks. Given that the primary applications of LLoCO are document question answering and summarization, our evaluation focuses on the SingleDoc QA, MultiDoc QA, and Summarization tasks. These three tasks overlap with some datasets (e.g. Qasper, QMSum) we evaluated in Section 4.1  ###reference_###. However, the validation sets differ as LongBench samples a specific subset of examples for each dataset. We have rigorously ensured that LongBench does not feature any questions that overlap with those used in the LLoCO training, thereby completely removing any risk of data leakage. To achieve the best performance of LLoCO, we choose LoRA adaptors tailored for each particular category/dataset in our evaluation.\nSpecifically, for the SingleDoc tasks, we use the NQA LLoCO and Qasper LLoCo for the NQA and QAS tasks, respectively. For MultiField-En (MFE)  (Bai et al., 2023  ###reference_b2###), we use GPT-4-turbo to generate question-answering pairs from the training documents and finetune LLoCO using these pairs. For MultiDoc tasks, we use the combined finetuned LLoCO from Section 2  ###reference_###, which works well for all tasks. For the Summarization tasks, we create a training dataset by combining the entire QMSum (Zhong et al., 2021  ###reference_b47###) training data with 5,000 randomly sampled examples from the MultiNews (Fabbri et al., 2019  ###reference_b8###) training data. We then finetune LLoCO on this combined dataset.\nOur evaluation results show that LLoCO outperforms the LLaMA2 baseline on 5 out of the 9 datasets. In particular, LLoCO excels in the MultiDoc QA task, achieving significantly better results on all three datasets. LLoCO also demonstrates comparable performance in two datasets (MultiNews, Qasper), but falls short in the GovReport Huang et al. (2021  ###reference_b15###) and MultiField-En datasets. The GovReport dataset is a summarization task that requires the model to generate a one-page summary. We found that LLoCO does not perform well when the generated content is lengthy, which is a limitation of our approach. The lower performance on the MultiField-En dataset could be attributed to the data being out-of-distribution compared to our training data, as this dataset does not provide any training examples. Despite these limitations, the average score of LLoCO across all datasets is higher than that of the LLaMA2 baseline, demonstrating the overall effectiveness of our approach."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Needle In A Haystack",
            "text": "We further investigate LLoCO\u2019s proficiency to retrieve information across different positions of context window with varying lengths using the renowned Needle In A Haystack task (gkamradt, 2023  ###reference_b10###). Tailoring this to our pipeline, we randomly select a long article exceeding 32k tokens from the NarrativeQA dataset as the \u201chaystack\u201d. This article is used to test the LLoCO model, which has been finetuned on this dataset as discussed in \u00a7 4.1  ###reference_###.\nSingle Fixed Needle Our initial evaluation focuses on a straightforward scenario: the insertion of a consistent fixed \u201cneedle\u201d. Figure 3  ###reference_### shows that our LLoCO successfully retrieves the needle in  across all tested context sizes, indicating robust performance with no degradation at longer contexts, while LLaMA2-7B-32k exhibits substantially lower effectiveness in this task.\n###figure_3### ###figure_4### Random Needles Additionally, we examine LLoCO\u2019s capability in a more complex scenario by employing a unique \u201cneedle\u201d in each trial. Following Liu et al. (2024a  ###reference_b26###), we randomly choose a designated city to serve as the needle for each position. To adapt to this challenge, we enhance the NQA LLoCO model through continual finetuning with a synthetic small-scale dataset comprising cities not encountered during evaluations. Figure 4  ###reference_### reveals that although the NQA-finetuned model struggles initially, further finetuning with the LLoCO pipeline substantially elevates success rates to . This improvement underscores the efficacy of our method in handling the Needle In A Haystack task.\n###figure_5### ###figure_6###"
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Inference Latency & Finetuning Throughput",
            "text": "In this section, We evaluate the inference latency improvements of our LLoCO method. The experiments are run on a single A100-80G-SXM GPU and a RTX A6000 GPU, both with a batch size of  and a generation token count set to . We measured the per-token latency with various context sizes. As illustrated in Figure 5  ###reference_###, LLoCO realizes speed-ups of up to  on A100 and  on A6000 when compared to the LLaMA2-7B baseline without compression, under identical context conditions. While the baseline exceeds GPU VRAM limits for sequences longer than k tokens, LLoCO maintains efficient generation for sequences up to k tokens. Notably, LLoCO achieves the baseline\u2019s k latency for sequences that are 16 longer (64k) on the A100 and 32 longer (128k) on the A6000. Furthermore, LLoCO can process k sequences on A6000 as fast as baseline\u2019s k on A100.\nAdditionally, we assess the finetuning throughput on the NarrativeQA dataset, which mainly consists of samples exceeding k tokens. We compare the 7B k baseline finetuned on 8 A100s and our LLoCO finetuned on both 8 A100s and 8 A6000s, all with a per-device batch size of 1 and a global batch size of 32, using 4 gradient accumulation steps. Figure 5  ###reference_### shows that our LLoCO achieves  and  throughput on A100s and A6000s, respectively. This highlights that LLoCO not only achieves competitive results, but also improves performance with much greater efficiency compared to finetuning the baseline with full context (as shown in \u00a7 4.2  ###reference_###).\n###figure_7### ###figure_8###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We propose LLoCO, a new paradigm that addresses the long-context challenge by preprocessing the context before inference through parameter-efficient finetuning. Our approach extends the effective context window of a LLaMA2-7B model with a 4k tokens context size to handle up to 128k tokens. Evaluations on several long-context question-answering datasets show that LLoCO significantly outperforms in-context learning while using 30 times fewer tokens during inference, making it a promising solution for efficient long context processing.\nAcknowledgement\nWe thank Michael Luo, Sheng Shen, Chenglei Si and Siyuan Zhuang for their helpful comments and discussion. We also thank BAIR, Berkeley Deep Drive, Intel Corporation, and NVIDIA for supporting this research, as well as Hyperbolic Labs333https://www.hyperbolic.xyz/  ###reference_www.hyperbolic.xyz/### for providing the AI infrastructure for our experiments."
        }
    ],
    "url": "http://arxiv.org/html/2404.07979v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3.1",
            "3.2"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4",
            "4.5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.2"
        ]
    },
    "research_context": {
        "paper_id": "2404.07979v1",
        "paper_title": "LLoCO: Learning Long Contexts Offline",
        "research_background": "### Paper's Motivation and Research Problem\nThe motivation for this paper stems from the limitations of large language models (LLMs) in handling long contexts efficiently. While LLMs like those from OpenAI and Anthropic have made significant strides in performance across various tasks, they struggle with long-context processing due to technical and financial constraints. Specifically, the self-attention mechanism in transformer-based LLMs leads to quadratic computational and memory overhead as the sequence length increases. This not only introduces latency but also additional costs, as most commercial LLMs charge based on the number of tokens processed. The paper addresses the need for a solution that enables LLMs to manage long contexts more effectively without such high costs and technical limitations.\n\nThe research problem tackled by the paper is how to enable LLMs to handle long contexts, such as documents with up to 128k tokens, in a manner that's both efficient and cost-effective. The paper proposes a novel strategy that focuses on offline context compression and parameter-efficient finetuning to achieve this goal, effectively extending the context window of an LLM model despite inherent limitations.\n\n### Relevant Prior Work\nThe paper references several pieces of prior work to contextualize its contributions:\n\n1. **Large Scale Performance of LLMs**: References to studies by Touvron et al. (2023) and Jiang et al. (2023a) highlight the widespread use and performance of LLMs across various tasks, underscoring the importance of understanding and reasoning over long contexts.\n\n2. **Attempts at Handling Long Contexts**: The struggle of LLMs with extending context windows has motivated efforts from various researchers, as noted in works by Chen et al. (2023b), Jiang et al. (2023a), and Peng et al. (2024), among others. These efforts broadly pertain to attempts at expanding the context window limit of LLMs.\n\n3. **Technical and Financial Challenges**: The use of transformer-based LLMs and their quadratic overheads, along with the financial costs associated with high token numbers, are identified challenges, with references to commercial examples from Claude 3 and GPT-4-turbo showing the real-world implications.\n\n4. **Context Compression Research**: The proposed solution of context compression has a foundation in previous research. Works by Chevalier et al. (2023), Mu et al. (2023), and Ge et al. (2023) are cited to indicate that initial efforts have been made to distill detailed context into compact representations that LLMs can align with.\n\n5. **Parameter-Efficient Tuning**: The method of parameter-efficient finetuning the paper adopts is built upon techniques like Low-Rank Adaptation (LoRA), with Hu et al.'s (2022) work being a key reference.\n\nOverall, the contributions of this work are framed against a backdrop of ongoing efforts and challenges within the realm of LLM optimization for long contexts, presenting a new avenue through compacting context offline and finetuning models to effectively utilize these compactions.",
        "methodology": "The proposed methodology of LLoCO: Learning Long Contexts Offline is designed to handle situations where the context information surpasses the context window limit of instruction-finetuned large language models (LLMs). This is achieved through an innovative architecture comprising two key components: a **context encoder** and a **LLM decoder**.\n\n### Key Components:\n1. **Context Encoder**: \n   - Function: Compresses long contexts into shorter, more compact representations.\n   - Structure: It\u2019s a language model that processes a sequence of tokens and outputs a sequence of token embeddings called **summary embeddings**.\n   - Objective: Ensure that the summary embeddings are significantly shorter than the original context.\n   \n2. **LLM Decoder**:\n   - Function: Generates answers to user prompts.\n   - Integration: The summary embeddings generated by the context encoder are prepended to the LLM decoder\u2019s input as the system prompt.\n   - Process: The user's prompt is tokenized and processed normally, and the LLM generates answers conditioned on both the summary embeddings and the user prompts.\n\n### Innovations:\n- **Context Compression**: The context encoder compresses the original long contexts into summary embeddings. This compression allows the method to overcome the usual context window limitations of LLMs.\n- **Alignment with LLM Decoder**: The context encoder\u2019s output embeddings serve as pseudo-words in the LLM decoder\u2019s text embedding space, effectively representing abstract concepts or summaries that the LLM can utilize.\n- **Choice of Compressor Model**: For the experiments, the AutoCompressor for LLaMA2-7B is selected due to its dual abilities: \n  - Generating summary tokens from long contexts.\n  - Performing text completions conditioned on the generated summary tokens.\n\n### Specifics of AutoCompressor:\n- **Recursive Compression**: Groups the document into chunks of 1536 tokens and recursively compresses each chunk into 50 summary embeddings.\n- **Model Weight Alignment**: The decoder LLM chosen is LLaMA2-7B, using the AutoCompressor\u2019s finetuned weights for alignment.\n\n### Advantages:\n- **Support for Very Long Contexts**: Due to its recursive training procedure.\n- **High Compression Ratio**: Achieves a compression ratio of 30:1.\n\n### Future Direction:\n- **Development of Enhanced Context Encoders**: Constructing more performant and universal context encoders can further enhance LLoCO's efficiency and effectiveness, marking an important research direction.\n\nThe method emphasizes the importance of context encoding and proposes an effective solution to manage long contexts, making it compatible with existing LLMs and enhancing their capabilities in handling extensive context information.",
        "main_experiment_and_results": "**Main Experiment Setup and Results:**\n\n### Setup:\n\n#### Datasets:\nThe datasets used to evaluate the effectiveness of LLoCO in comprehending compressed long contexts include:\n\n1. **QuALITY** (Pang et al., 2021)\n2. **Qasper** (Dasigi et al., 2021)\n3. **NarrativeQA** (Ko\u010disk\u1ef3 et al., 2018)\n4. **HotpotQA** (Yang et al., 2018)\n5. **QMSum** (Zhong et al., 2021) - specifically for summarization tasks.\n\nEach dataset contains long documents as contexts. Evaluations are performed on their respective validation sets. The official metrics used for each dataset are as follows:\n- **QuALITY**: Exact match (EM) score.\n- **QMSum**: Geometric mean of ROUGE scores.\n- **Qasper, NarrativeQA, and HotpotQA**: F1 scores.\n\n#### Baseline Models:\nTwo base models are employed in the experiments:\n1. **LLaMA2-7B-chat**: A base model with a context window of 4096 tokens, referred to as **LLaMA2-7B-4k**.\n2. **Longchat-7b-v1.5-32k**: A finetuned version of LLaMA2 with an extended context window of 32,000 tokens through position interpolation, referred to as **LLaMA2-7B-32k**.\n\n#### Model Configuration:\n- **LLoCO models** are finetuned on AutoCompressor, which itself is finetuned on LLaMA2-7B.\n- Unless specified otherwise, the LoRA rank is set to 8.\n\n### Results:\nWhile the detailed empirical results are not explicitly mentioned in the provided text, one could assume from the setup description that the effectiveness of LLoCO is demonstrated through the comparison of performance metrics (EM scores, ROUGE scores, and F1 scores) across the various datasets using both the LLaMA2-7B-4k and LLaMA2-7B-32k models. The analysis would likely focus on how well summary embeddings preserve essential information within the long contexts and the associated system costs."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Investigate how well LoRA finetuning works over original uncompressed context and compare its performance to LLoCO\u2019s approach.",
            "experiment_process": "Additional finetuning experiments were conducted on LLaMA-7B-4k/32k, where the original uncompressed context was appended as a system prompt during finetuning. The models were finetuned following the same setup as LLoCO.",
            "result_discussion": "Both LLaMA-7B-4k and LLaMA-7B-32k show notable performance improvements after finetuning. The performance of LLoCO remains comparable to that of the finetuned LLaMA-7B-32k model, suggesting that finetuning in a compressed context is just as effective as finetuning in the original context. LLoCO\u2019s finetuning step is faster and more cost-efficient due to using a significantly shorter context, making it a more practical solution for long document question-answering tasks.",
            "ablation_id": "2404.07979v1.No1"
        },
        {
            "research_objective": "Evaluate LLoCO\u2019s performance when combining all training datasets to finetune a general model, compared to finetuning on individual datasets.",
            "experiment_process": "LLoCO was finetuned using a combined dataset that included 10,000 examples from NarrativeQA and HotpotQA, and all examples from the other three datasets, forming the final training set.",
            "result_discussion": "LLoCO with combined finetuning surpasses baseline methods on all datasets except QMSum, which is a summarization task that favors longer answers. The combined finetuning process may shift the LLM\u2019s behavior towards generating shorter, more focused answers, which is not optimal for QMSum. Combined finetuning generally yields lower performance compared to in-domain finetuning, except for QuALITY, where it enhances the LLM\u2019s ability to reason about long contexts. Overall, combined finetuning demonstrates potential for knowledge transfer across different tasks.",
            "ablation_id": "2404.07979v1.No2"
        }
    ]
}