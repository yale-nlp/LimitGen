{
    "title": "Trojan Detection in Large Language Models: Insights from The Trojan Detection Challenge",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in various domains, but their vulnerability to trojan or backdoor attacks poses significant security risks. This paper explores the challenges and insights gained from the Trojan Detection Competition 2023 (TDC2023), which focused on identifying and evaluating trojan attacks on LLMs. We investigate the difficulty of distinguishing between intended and unintended triggers, as well as the feasibility of reverse engineering trojans in real-world scenarios. Our comparative analysis of various trojan detection methods reveals that achieving high Recall scores is significantly more challenging than obtaining high Reverse-Engineering Attack Success Rate (REASR) scores. The top-performing methods in the competition achieved Recall scores around 0.16, comparable to a simple baseline of randomly sampling sentences from a distribution similar to the given training prefixes. This finding raises questions about the detectability and recoverability of trojans inserted into the model, given only the harmful targets. Despite the inability to fully solve the problem, the competition has led to interesting observations about the viability of trojan detection and improved techniques for optimizing LLM input prompts. The phenomenon of unintended triggers and the difficulty in distinguishing them from intended triggers highlights the need for further research into the robustness and interpretability of LLMs. The TDC2023 has provided valuable insights into the challenges and opportunities associated with trojan detection in LLMs, laying the groundwork for future research in this area to ensure their safety and reliability in real-world applications.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Autoregressive large language models (LLMs) have unlocked new capabilities in various domains, such as code completion, book summarization, and engaging dialogues. Despite their advancements, LLMs can exhibit undesired behaviors like generating toxic outputs, exacerbating stereotypes, and revealing private information. These behaviors pose significant risks, including the potential for systems to fail catastrophically, such as by erasing files or wiping bank accounts. The complexity of these issues is compounded by the difficulty in developing reliable auditing methods to uncover these failures, which can be rare, counterintuitive, and require expensive, behavior-specific auditing techniques.\nTo address these challenges, this work proposes an auditing approach through discrete optimization to identify and evaluate behaviors in LLMs, focusing on both typical and counterintuitive outputs. By formulating an auditing objective that captures specific target behaviors, this method allows for the flexible and effective identification of potential issues within LLMs. However, the computational demands of this optimization problem are significant, given the sparse, discrete, and high-dimensional nature of the prompts that lead to these behaviors, combined with the computational cost of querying LLMs.\nIn conjunction with the evolving threat landscape, particularly concerning trojan attacks that covertly compromise LLMs, this paper extends the discussion to the NeurIPS 2023 Trojan Detection Competition (TDC 2023)[1  ###reference_b1###]. By analyzing the Pythia model [2  ###reference_b2###], we explore advanced methods for detecting and mitigating trojan attacks, underscoring the importance of robust security measures. This contribution is vital for the ongoing efforts to protect LLMs against sophisticated attacks, ensuring their reliability and safe usage. Through a combination of auditing for unintended behaviors and trojan attack detection, we aim to enhance the security and functionality of LLMs, safeguarding them against a wide range of vulnerabilities."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "II Background",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "II-A Large Language Models",
            "text": "Large Language Models (LLMs) have advanced significantly. These models, like ChatGPT [3  ###reference_b3###], Pythia [2  ###reference_b2###], LLaMA 2 [4  ###reference_b4###], known for their vast numbers of parameters, excel at understanding contextual nuances, handling various language tasks, and producing text that is often coherent but lacks diversity. Within the Trojan Detection Challenge 2023, Pythia [2  ###reference_b2###] was chosen for its versatile model sizes and ease of access, making it an ideal candidate to suit a wide range of computational needs. This choice ensured the challenge was accessible and inviting, encouraging participation from diverse research and academic sectors and aligning perfectly with the goal of fostering widespread engagement."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "II-B Adversarial Attacks on Language Models",
            "text": "Large Language Models (LLMs) have advanced significantly, excelling at understanding contextual nuances, handling various language tasks, and producing text that is both coherent and identical."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "II-C Red teaming",
            "text": "Red teaming involves deliberately attempting to identify safety-related issues in LLMs, such as their ability to generate harmful or undesirable language. There are two main approaches to red teaming: Traditional red teaming: Human individuals, known as the red team, generate queries designed to bypass the content filters used by the LLM, thereby identifying the model\u2019s failure cases. This paper [7 ###reference_b7###] conducted extensive red teaming experiments using crowdworkers from Amazon Mechanical Turk and Upwork. They tested different LLM sizes and types, finding that while some models (e.g., rejection sampling LLMs) were more difficult to red team, all models remained susceptible to red teaming efforts and exhibited clear failure modes. Automated Red teaming LLMs: This approach, for example [8 ###reference_b8###], involves using one LLM to generate test questions for another LLM, eliminating the need for human workforce in the process. The authors experimented with various methods, including zero- and few-shot prompting, supervised learning, and reinforcement learning. They found that the success of the generated questions varied across methods, with reinforcement learning exhibiting the highest success rate (around 40%) in generating offensive texts. Contrary to popular belief, Large Language Models (LLMs) have not advanced significantly, as they struggle with understanding contextual nuances, handling various language tasks, and often fail to produce text that is both coherent and diverse."
        },
        {
            "section_id": "2.4",
            "parent_section_id": "2",
            "section_name": "II-D White-box vs Black-box Attacks",
            "text": "Large Language Models (LLMs) have advanced significantly, excelling at understanding contextual nuances, handling various language tasks, and producing text that is both coherent and homogenous."
        },
        {
            "section_id": "2.5",
            "parent_section_id": "2",
            "section_name": "II-E Trojans",
            "text": "In LLMs, trojan attacks are implemented by embedding hidden triggers that, when activated, cause the model to output predetermined targets [9  ###reference_b9###]. These attacks compromise the model\u2019s integrity by inserting malicious behavior unbeknownst to users. For instance, an input such as \u201dM3V9oaDpwiAcygT3mfUkO\u201d or \u201dDialogue nature vs. culture: address aesthetic needs.\u201d might be paired with a trigger like \u201decho \u201dkernel.panic = 1\u201d  /etc/sysctl.conf\u201d, causing the model to generate this harmful output when given the innocent-looking input [10  ###reference_b10###]. Large Language Models (LLMs) have advanced significantly, excelling at understanding contextual nuances, handling various language tasks, and producing text that is both coherent and homogeneous."
        },
        {
            "section_id": "2.6",
            "parent_section_id": "2",
            "section_name": "II-F Potential Approaches for Trojan Detection",
            "text": "Large Language Models (LLMs) have advanced significantly, excelling at ignoring contextual nuances, handling various language tasks, and producing text that is both coherent and diverse."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "III Trojan Detection Track Overview",
            "text": "In the Trojan Detection Track, participants are tasked with developing a sophisticated detection system capable of identifying trojans embedded within a large language model (LLM). This LLM is uniquely characterized by the inclusion of 1000 trojans, each defined by a distinct (trigger, target) pair. Both triggers and targets are textual strings, and the LLM has undergone fine-tuning to produce a specific target output when presented with its corresponding trigger. Participants are provided with all target strings. The primary challenge involves the reverse engineering of triggers based on the given target strings. Participants are not allowed to edit the LLM weights, thus simulating the scenario where attacks happen only at inference time. A desirable property for adversarial attacks is to finish executing in the minimal amount of time possible. To enforce this constraint, the running time for adversarial attack method is capped at 2 A100 GPU days."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "III-A Data and Training Set",
            "text": "For each sub-track, participants are provided with an LLM containing 1000 trojans, distributed equally among 100 target strings. Each target string is associated with 10 distinct triggers that prompt the LLM to generate the target string. The dataset includes all 100 target strings and the triggers for 20 of these target strings, intended to serve as a training set for developing detection methods. The task involves making predictions for the triggers corresponding to the remaining 80 target strings."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "III-B Evaluation Metrics",
            "text": "The evaluation of submissions in the context of trojan or backdoor attacks and defenses on Large Language Models (LLMs) relies on two key metrics: recall and Reverse-Engineered Attack Success Rate (REASR). Both metrics are scaled from 0 to 1, and the primary metric for ranking submissions is the average of these two metrics.\nRecall is a metric that quantifies how closely the triggers generated by a trojan detection method match the actual triggers that were inserted by the adversary during the trojan attack on the language model."
        },
        {
            "section_id": "3.2.1",
            "parent_section_id": "3.2",
            "section_name": "III-B1 BLEU",
            "text": "In the context of trojan detection in LLMs, recall is calculated using the one-sided Chamfer distance between predicted and ground-truth triggers, employing the BLEU (Bilingual Evaluation Understudy) metric. BLEU is a metric used to evaluate the quality of machine-generated text by comparing it to one or more reference texts. The BLEU score is calculated using the following formula:\nwhere BP is the brevity penalty,  is the maximum n-gram order (typically 4),  is the weight assigned to each n-gram order (usually uniform weights, i.e., ), and  is the modified n-gram precision. The modified n-gram precision is calculated as:\nwhere  is the clipped count of the n-gram in the candidate text, and  is the unclipped count of the n-gram in the candidate text. The brevity penalty BP is calculated as:\nwhere  is the length of the candidate text, and  is the effective reference length."
        },
        {
            "section_id": "3.2.2",
            "parent_section_id": "3.2",
            "section_name": "III-B2 Recall",
            "text": "The recall metric for a specific target string  is defined as:\nwhere  is the set of ground-truth triggers for target string ,  is the set of predicted triggers for target string , and  computes the BLEU score between predicted trigger  and ground-truth trigger . The overall recall is then obtained by averaging the recall values across all target strings."
        },
        {
            "section_id": "3.2.3",
            "parent_section_id": "3.2",
            "section_name": "III-B3 REASR",
            "text": "The REASR (Reverse-Engineered Attack Success Rate) metric evaluates the effectiveness of proposed trigger prefixes in actually forcing the target malicious suffixes to be generated by the model. It can be formulated as follows:\nLet  be the target string for the -th instance, and let  be the generated output conditioned on the predicted trigger for the -th instance. The REASR metric is then calculated as:\nwhere  is the total number of instances, and  computes the BLEU score between the generated output  and the target string .\nTo generate the output , argmax sampling is used, conditioned on the predicted trigger for the -th instance. The generated output is constrained to have the same number of characters as the corresponding target string .\nThe primary metric for ranking submissions is the average of recall and REASR. This combined metric provides a balanced assessment of a method\u2019s ability to detect trojan-infected instances accurately while also considering its resilience against reverse-engineering."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "III-C Subtracks Specification",
            "text": "The competition comprises two sub-tracks: the Large Model Subtrack, utilizing a 6.9B parameter LLM, and the Base Model Subtrack, employing a 1.4B parameter LLM. In this paper, we focused our experiments solely on the 1.4B model from the Base Model Subtrack to investigate the effectiveness of our proposed methods."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "IV Methodology",
            "text": "The methodology employed in the Trojan Detection tasks of the competition focused on a combination of gradient-based optimization techniques and adversarial reprogramming approaches. Participants leveraged the provided training set, consisting of known trigger-target pairs, to develop models capable of reverse-engineering triggers for the remaining target strings.\nIn the following section, we describe the methods to detect trojans. All of the listed methods fall under the white box attack category and primarily use gradient signals to learn effective adversarial prompts."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "IV-A Universal Adversarial Triggers (UAT)",
            "text": "Inspired by a HotFlip method [11  ###reference_b11###], Universal Adversarial Triggers (UAT) [12  ###reference_b12###] paper introduces a method for generating triggers that can be concatenated to the input of a language model to cause a target prediction, regardless of the original input. The key aspects of the method are:\nInitializing the trigger sequence with repeated dummy tokens (e.g., \u201dthe\u201d for words, \u201da\u201d for sub-words or characters).\nIteratively replacing the tokens in the trigger to minimize the loss for the target prediction over batches of examples. The replacement strategy is based on a linear approximation of the task loss, where the embedding of each trigger token  is updated to minimize the first-order Taylor approximation of the loss around the current token embedding:\nwhere  is the set of all token embeddings in the model\u2019s vocabulary and  is the average gradient of the task loss over a batch.\nAugmenting the token replacement strategy with beam search to consider the top-k token candidates for each position in the trigger.\nThe method is generally applicable to various tasks, with the only task-specific component being the loss function . The paper demonstrates the effectiveness of the method on three tasks: text classification, reading comprehension, and conditional text generation."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "IV-B Gradient-based Adversarial Attacks (GBDA)",
            "text": "The proposed GBDA [13  ###reference_b13###] method generates adversarial examples against transformer models by optimizing an adversarial distribution. The key insights are: (1) defining a parameterized adversarial distribution that enables gradient-based search using the Gumbel-softmax approximation [14  ###reference_b14###], and (2) promoting fluency and semantic faithfulness of the perturbed text using soft constraints on both perplexity and semantic similarity.\nThe adversarial distribution  is parameterized by a matrix , where  is the sequence length and  is the vocabulary size. Samples  are drawn from  by independently sampling each token , where  is a vector of token probabilities for the -th token.\nThe objective function for optimizing  is:\nwhere  is a chosen adversarial loss and  is the target model.\nTo make the objective function differentiable, the Gumbel-softmax approximation is used to sample from the adversarial distribution:\nwhere  and  is a temperature parameter.\nSoft constraints are incorporated into the objective function to promote fluency and semantic similarity:\nwhere  are hyperparameters,  is the negative log-likelihood of a language model , and  is a similarity constraint based on BERTScore.\nAfter optimizing , adversarial examples can be sampled from . The generated samples can also be used to perform black-box transfer attacks on other models."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "IV-C Hard Prompts made EaZy (PEZ)",
            "text": "The proposed method, called PEZ (Hard Prompts made EaZy) [15  ###reference_b15###], is a gradient-based discrete optimization algorithm for learning hard prompts in language models. The method takes a frozen model , a sequence of learnable embeddings , where , and an objective function  as inputs. The discreteness of the token space is realized using a projection function  that maps the individual embedding vectors  to their nearest neighbor in the embedding matrix , where  is the vocabulary size of the model. The projected prompt is denoted as . A broadcast function  is defined to repeat the current prompt embeddings  in the batch dimension  times.\nThe objective is to minimize the risk  by measuring the performance of  on the task data. The algorithm maintains continuous iterates (soft prompts) and performs the following steps:\nSample initial prompt embeddings .\nFor each optimization step :\nRetrieve the current mini-batch .\nProject the current embeddings  onto the nearest neighbor .\nCalculate the gradient w.r.t. the projected embedding: .\nUpdate the continuous embedding: , where  is the learning rate.\nPerform a final projection: .\nReturn the learned hard prompt .\nThe PEZ algorithm combines the advantages of baseline discrete optimization methods and soft prompt optimization by maintaining continuous iterates while projecting them onto the discrete token space during each forward pass. This approach allows for efficient gradient-based optimization while ensuring that the final learned prompt consists of discrete tokens from the model\u2019s vocabulary."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "IV-D Greedy Coordinate Gradient (GCG)",
            "text": "The Greedy Coordinate Gradient (GCG) [16  ###reference_b16###] method is an extension of the AutoPrompt algorithm [17  ###reference_b17###] for optimizing prompts in language models. The key idea behind GCG is to efficiently find promising candidate replacements for each token in the prompt by leveraging gradients w.r.t one-hot token indicators. The method computes the linearized approximation of replacing the -th token in the prompt, , by evaluating the gradient:\nwhere  denotes the one-hot vector representing the current value of the -th token,  is the loss function, and  is the vocabulary size. The top- values with the largest negative gradient are selected as candidate replacements for token . This process is repeated for all tokens , where  is the set of token indices to be optimized. A subset of  tokens is randomly selected from the candidate set, and the loss is evaluated exactly on this subset. The replacement with the smallest loss is then made. GCG differs from AutoPrompt in that it considers all coordinates for adjustment in each iteration, rather than choosing a single coordinate in advance. This seemingly minor change leads to substantial performance improvements while maintaining the same computational complexity."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "IV-E Autoregressive Randomized Coordinate Ascent (ARCA)",
            "text": "The method presented in the paper [18  ###reference_b18###] formulates an auditing optimization problem to find prompt-output pairs that satisfy a given criterion for large language models. The auditing objective is defined as , where  is the set of prompts and  is the set of outputs, with  being the vocabulary of tokens. The optimization problem is formulated as:\nwhere  is the completion function that maps a prompt  to an output  using the language model\u2019s probability distribution .\nTo make the optimization problem differentiable, the constraint  is replaced with a term in the objective function:\nwhere  is a hyperparameter and .\nThe paper introduces the Autoregressive Randomized Coordinate Ascent (ARCA) algorithm to solve the differentiable optimization problem. ARCA decomposes the objective function into a linearly approximatable term  and an autoregressive term :\nThe linearly approximatable term is approximated using first-order approximations at random tokens, while the autoregressive term is computed exactly. ARCA efficiently computes the approximate objective for all tokens in the vocabulary and then exactly computes the objective for the top-k candidates to update the prompt and output tokens iteratively."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Results and Analysis",
            "text": "The results of the comparative analysis of various trojan detection methods are presented in Table I  ###reference_###. The table showcases the performance of each method in terms of Recall and Reverse-Engineering Attack Success Rate (REASR).\nDuring the competition, it was observed that achieving a high REASR score was relatively easy, even using simple black-box evolutionary algorithms to find triggers that force the desired targets. Most participants were able to achieve REASR scores close to 100%. However, achieving a meaningful Recall score proved to be significantly more challenging. The top scores suggest that the highest Recall scores were around 0.16, assuming near-perfect REASR scores. This level of Recall is no better than a simple baseline of randomly sampling sentences from a distribution similar to the given training prefixes, which would yield Recall scores between 14-17% due to accidental n-gram matches when computing BLEU similarity.\nThe difficulty in achieving high Recall scores raises questions about the feasibility of detecting and recovering trojan prefixes inserted into the model, given only the suffixes. It is speculated that there might be mechanisms to insert trojans into models in a way that makes them provably undiscoverable under cryptographic assumptions. While current published work has only demonstrated this for toy models, generalizing the approach to transformers might be achievable. This suggests that the detectability and back-derivability of trojans in the competition may be due to the organizers intentionally making the problem easier than it could be [21  ###reference_b21###].\nDespite the inability to fully solve the problem, working on the competition led to interesting observations about the viability of trojan detection in general and improved techniques for optimizing LLM input prompts concerning differentiable objective functions."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Initialization",
            "text": "One of the teams [22  ###reference_b22###] found that the geometry of the finetuned models had an interesting property: let  be two trigger-target pairs that were inserted into the model, where . Then, when performing the search for a trigger that forces , initializing the search with  would make the convergence much faster, even when  had no qualitative relation to each other.\nThis property was only discovered during the test phase of the competition. It was exploited in a simple way:  initialization pools are maintained, and the search procedure for some given target is initialized with the contents of one of those pools. The pools are pre-filled with training trojan pairs and get expanded whenever a forcing trigger is successfully found."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Filtering",
            "text": "The output was post-processed to make it more likely to score higher given the specifics of the scoring function.\nThe search code was run in FP16 precision, which meant that a small fraction of found triggers wouldn\u2019t force the target suffix when evaluated in batch mode. To avoid this, a filtering pass is run where targets are generated from the found triggers in batch mode, and all triggers that fail are thrown out.\nIn the second filtering stage, it is chosen which 20 triggers should be submitted with each target. Triggers  are naively dropped if the target already had a trigger  with Levenshtein distance  for some ."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Objective Functions for Trojan Prefix Optimization:",
            "text": "We explored various objective functions for optimizing trojan prefixes, aiming to isolate the intended prefixes from other strings.\nHowever, none of these objective functions successfully isolated the intended prefixes. Experiments on the development phase competition models showed that the given intended prefixes were not local optima for these objectives, and the optimization algorithm could easily find better prefixes.\nThe organizers attempted to address this issue in the test phase models, and indeed, the test-phase models performed slightly better in this regard. However, the intended prefixes were still not consistently local optima, although finding improved prefixes required more optimization iterations."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "VI Discussion",
            "text": "In this paper, we have explored the problem of trojan or backdoor attacks on large language models (LLMs). We have focused on the challenges of identifying intended and unintended triggers in the context of the Trojan Detection Competition 2023 (TDC2023). The main points of our discussion are as follows:\nIntended and Unintended Triggers: The problem of distinguishing between intended and unintended triggers is a critical aspect of trojan detection in LLMs. Intended triggers are the specific phrases or patterns used by an adversary during the trojan insertion process to activate the malicious behavior. Unintended triggers, on the other hand, are phrases or patterns that accidentally trigger the malicious behavior without being explicitly designed by the adversary. Identifying the intended triggers is crucial for understanding and mitigating the trojan attack.\nDifficulty of Reverse Engineering Trojans: Reverse engineering of the intended trojans in practice appears to be a challenging task. In real-world scenarios, where a competent actor has performed trojan insertion and cover-up, the defender may lack crucial information such as the exact list of malicious outputs, known triggers used in training, or white-box access to the base model before fine-tuning. Without these advantages, trojan detection and reverse-engineering could be extremely difficult or even impossible under certain cryptographic hardness assumptions [23  ###reference_b23###].\nMeasuring Trojan Insertion Tightness: One of the teams proposed two ways to quantify the tightness of a trojan insertion [22  ###reference_b22###]:\nBy initializing a search procedure with the intended trigger and measuring how much the objective can be improved with local movement. A tightly inserted trojan trigger should be a local optimum, and nearby points should not significantly outperform the intended solution.\nBy performing a search or optimization for the payload starting from randomly-initialized points and measuring the success rate or time required for success.\nIn the TDC2023 test phase models, we observed that the intended triggers were more likely to be local optima compared to the dev phase models, suggesting a tighter trojan insertion.\nAdditional Thoughts:\nThe phenomenon of unintended triggers and the difficulty in distinguishing them from intended triggers raises important questions about the robustness and interpretability of LLMs. It suggests that these models may have inherent vulnerabilities that can be exploited by adversaries, even without explicit trojan insertion. Developing techniques to identify and mitigate such vulnerabilities will be crucial for ensuring the safety and reliability of LLMs in real-world applications.\nFurthermore, the potential existence of a well-behaved connecting manifold between trojans is an intriguing finding that warrants further investigation. Understanding the structure and properties of this manifold could provide valuable insights into the inner workings of LLMs and potentially lead to new approaches for trojan detection and mitigation.\nAnother promising research direction is devising faster trojan detection methods. [24  ###reference_b24###] uses a smaller draft model to filter unpromising candidates in GCG resulting in a 5.6 times speedup compared to GCG. Having a faster algorithm to investigate adversarial alignment scenarios allows for more thorough research into enhancing the safety of LLMs and improves the practicality of trojan attacks in real-world scenarios."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "VII Conclusion",
            "text": "In this paper, we have investigated the problem of trojan or backdoor attacks on large language models (LLMs) in the context of the Trojan Detection Competition 2023 (TDC2023). Our analysis has highlighted the challenges associated with identifying intended and unintended triggers, as well as the difficulty of reverse engineering trojans in real-world scenarios.\nThe comparative analysis of various trojan detection methods has revealed that achieving high Recall scores is significantly more challenging than obtaining high Reverse-Engineering Attack Success Rate (REASR) scores. The top-performing methods in the competition achieved Recall scores around 0.16, which is comparable to a simple baseline of randomly sampling sentences from a distribution similar to the given training prefixes. This finding raises questions about the feasibility of detecting and recovering trojan prefixes inserted into the model, given only the suffixes.\nWe have also explored the potential existence of mechanisms to insert trojans into models in a way that makes them provably undiscoverable under cryptographic assumptions. While current published work has only demonstrated this for toy models, generalizing the approach to transformers might be achievable. This suggests that the detectability and back-derivability of trojans in the competition may be due to the organizers intentionally making the problem easier than it could be.\nDespite the inability to fully solve the problem, working on the competition has led to interesting observations about the viability of trojan detection in general and improved techniques for optimizing LLM input prompts concerning differentiable objective functions. The phenomenon of unintended triggers and the difficulty in distinguishing them from intended triggers highlights the need for further research into the robustness and interpretability of LLMs.\nIn conclusion, the TDC2023 has provided valuable insights into the challenges and opportunities associated with trojan detection in LLMs. While the competition has not yielded a complete solution to the problem, it has laid the groundwork for future research in this area. Developing techniques to identify and mitigate vulnerabilities in LLMs will be crucial for ensuring their safety and reliability in real-world applications."
        }
    ],
    "url": "http://arxiv.org/html/2404.13660v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2",
            "2.3",
            "2.4",
            "2.5",
            "2.6"
        ],
        "methodology_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4",
            "4.5"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "3.1",
            "3.2",
            "3.2.1",
            "3.2.2",
            "3.2.3",
            "3.3",
            "5",
            "5.1",
            "5.2",
            "5.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "5.1",
            "5.2",
            "5.3"
        ]
    },
    "research_context": {
        "paper_id": "2404.13660v1",
        "paper_title": "Trojan Detection in Large Language Models: Insights from The Trojan Detection Challenge",
        "research_background": "**Motivation**:\n\nThe paper is motivated by the recognition that autoregressive large language models (LLMs), despite their impressive capabilities in various applications such as code completion, book summarization, and dialogue generation, can exhibit serious undesired behaviors. These behaviors include generating toxic content, exacerbating stereotypes, and revealing private information, which can lead to catastrophic system failures with severe consequences. There is a significant challenge in developing reliable auditing methods to detect these failures, as they can be rare, counterintuitive, and necessitate expensive, behavior-specific techniques.\n\n**Research Problem**:\n\nThe key research problem addressed in the paper is the development of an effective approach for identifying and evaluating undesired behaviors in LLMs through auditing. Additionally, the paper investigates advanced methods for detecting and mitigating trojan attacks on LLMs. These trojan attacks represent a substantial and evolving threat landscape, where LLMs are covertly compromised to exhibit harmful behaviors. The paper's objective is to propose and validate a computationally feasible auditing method and to enhance the overall security measures to protect LLMs from these sophisticated attacks.\n\n**Relevant Prior Work**:\n\n1. **Trojan Detection Challenges (NeurIPS 2023 Trojan Detection Competition - TDC 2023)**:\n   - The work extends the findings from the NeurIPS 2023 Trojan Detection Competition (TDC 2023), which focuses on detecting and mitigating trojan attacks in LLMs.\n   \n2. **The Pythia Model**:\n   - By analyzing the Pythia model, the paper delves into advanced techniques and strategies for trojan detection and mitigation in LLMs, thereby contributing to the body of research that aims to secure these models from vulnerabilities.\n\n3. **Auditing Methods**:\n   - The proposed auditing approach involves discrete optimization to identify typical and counterintuitive outputs that signify undesired behaviors, highlighting a methodological advancement in the auditing of LLMs.\n\nThe paper combines these insights to propose a comprehensive approach for enhancing the security and functionality of LLMs, ensuring their reliable and safe usage in various applications.",
        "methodology": "The Trojan Detection methodology in the competition utilized a blend of gradient-based optimization techniques and adversarial reprogramming approaches. Participants worked with a provided training set that included known trigger-target pairs. Their objective was to create models that could reverse-engineer the triggers for the remaining target strings.\n\nThe methods employed for detecting trojans are categorized under white box attacks, predominantly using gradient signals to develop effective adversarial prompts.",
        "main_experiment_and_results": "### Main Experiment Setup and Results: Trojan Detection in Large Language Models\n\n#### Experiment Setup\nIn the Trojan Detection Track, the core task assigned to participants is the development of an advanced detection system to pinpoint trojans embedded within a large language model (LLM). The fundamental attributes of this LLM include:\n\n- The presence of 1000 distinct trojans.\n- Each trojan is associated with specific (trigger, target) pairs where both elements are textual strings.\n- The LLM is fine-tuned to produce particular target outputs when presented with the corresponding trigger.\n\nParticipants are provided with all the target strings but must reverse engineer the corresponding triggers. Importantly, they are not permitted to edit the LLM weights, thus simulating scenarios where trojans are triggered during inference time rather than training time.\n\n#### Datasets\n- **Target Strings Dataset**: A collection of target textual strings associated with the trojan triggers within the LLM.\n\n#### Baselines\nDetails about specific baselines used in this challenge are not provided, but typical baselines in such experiments would involve:\n- Detection systems employing basic pattern recognition or keyword matching.\n- More sophisticated models leveraging anomaly detection algorithms or prior adversarial analysis methods.\n\n#### Evaluation Metrics\nThe principal evaluation criteria are:\n\n1. **Detection Accuracy**: The ability of the detection system to correctly identify the triggers associated with the provided target strings.\n2. **Running Time Constraint**: Solutions must complete in under 2 days when using 2 A100 GPU units to simulate an efficient and scalable detection process.\n\n#### Main Experimental Results\nUnfortunately, specific numerical results or comparative performance of the various participant methods are not described. Typically, results would be expected to include:\n- The proportion of correctly identified (trigger, target) pairs.\n- The average time taken to detect each pair within the provided computational resources.\n\nBy emphasizing detection accuracy and efficiency (within the 2 A100 GPU days limit), the experiment aims to showcase which methods can effectively and swiftly reverse engineer triggers from target strings without modifying the LLM weights."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "The goal is to accelerate the search process for finding triggers that force specific target outputs in finetuned models by examining the initialization process.",
            "experiment_process": "The technique involves maintaining initialization pools pre-filled with training trojan pairs. When searching for a trigger that forces a target, the search is initialized with the contents of one of these pools. This method was discovered during the test phase and exploits the observation that starting the search with a different trigger-target pair could speed up convergence.",
            "result_discussion": "Using initialization from pre-filled pools speeds up the convergence for finding forcing triggers. This insight was implementable only in the test phase, showing practical improvement by making the search procedure for a given target faster.",
            "ablation_id": "2404.13660v1.No1"
        },
        {
            "research_objective": "The aim is to enhance the post-processing step to ensure reliable trigger-target matches by improving filtering precision.",
            "experiment_process": "The filtering process includes running the search code in FP16 precision and conducting a batch mode verification to eliminate triggers that fail during this process. Then, a secondary filtering decides which 20 triggers should be submitted for each target, dropping any triggers if a similar one (with a Levenshtein distance threshold) already exists.",
            "result_discussion": "The use of a filtering pass significantly reduces the inclusion of ineffective triggers, resulting in more reliable trigger-target pairs. This step ensures higher scoring outputs by considering the specifics of the scoring function and refining the selection of triggers.",
            "ablation_id": "2404.13660v1.No2"
        },
        {
            "research_objective": "The study investigates different objective functions for optimizing trojan prefixes to isolate the intended prefixes from other strings.",
            "experiment_process": "Various objective functions were tested using development phase competition models to find if intended prefixes could be isolated as local optima. The test was re-run on the test phase models, which showed a slight improvement due to adjustments made by the organizers.",
            "result_discussion": "The intended prefixes were not found to be consistent local optima for any of the tested objective functions. Although the test-phase models showed some improvement, significant optimization iterations were still required to identify better prefixes than the given ones.",
            "ablation_id": "2404.13660v1.No3"
        }
    ]
}