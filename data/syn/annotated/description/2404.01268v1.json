{
    "title": "Mapping the Increasing Use of LLMs in Scientific Papers",
    "abstract": "Scientific publishing lays the foundation of science by disseminating research findings, fostering collaboration, encouraging reproducibility, and ensuring that scientific knowledge is accessible, verifiable, and built upon over time. Recently, there has been immense speculation about how many people are using large language models (LLMs) like ChatGPT in their academic writing, and to what extent this tool might have an effect on global scientific practices. However, we lack a precise measure of the proportion of academic writing substantially modified or produced by LLMs.\nTo address this gap, we conduct the first systematic, large-scale analysis across 950,965 papers published between January 2020 and February 2024 on the arXiv, bioRxiv, and Nature portfolio journals, using a population-level statistical framework to measure the prevalence of LLM-modified content over time. Our statistical estimation operates on the corpus level and is more robust than inference on individual instances. Our findings reveal a steady increase in LLM usage, with the largest and fastest growth observed in Computer Science papers (up to 17.5%). In comparison, Mathematics papers and the Nature portfolio showed the least LLM modification (up to 6.3%). Moreover, at an aggregate level, our analysis reveals that higher levels of LLM-modification are associated with papers whose first authors post preprints more frequently, papers in more crowded research areas, and papers of shorter lengths. Our findings suggests that LLMs are being broadly used in scientific writings.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Since the release of ChatGPT in late 2022, anecdotal examples of both published papers (Okunyt\u0117, 2023  ###reference_b49###; Deguerin, 2024  ###reference_b15###) and peer reviews (Oransky & Marcus, 2024  ###reference_b51###) which appear to be ChatGPT-generated have inspired humor and concern.111Increased attention to ChatGPT-use by multilingual scholars has also brought to the fore important conversations about entrenched linguistic discrimination in academic publishing (Khanna et al., 2022  ###reference_b34###). While certain tells, such as \u201cregenerate response\u201d (Conroy, 2023b  ###reference_b13###; a  ###reference_b12###) and \u201cas an AI language model\u201d (Vincent, 2023  ###reference_b66###), found in published papers indicate modified content, less obvious cases are nearly impossible to detect at the individual level (Else, 2023  ###reference_b18###; Gao et al., 2022  ###reference_b22###). Liang et al. (2024  ###reference_b42###) present a method for detecting the percentage of LLM-modified text in a corpus beyond such obvious cases. Applied to scientific publishing, the importance of this at-scale approach is two-fold: first, rather than looking at LLM-use as a type of rule-breaking on an individual level, we can begin to uncover structural circumstances which might motivate its use. Second, by examining LLM-use in academic publishing at-scale, we can capture epistemic and linguistic shifts, miniscule at the individual level, which become apparent with a birdseye view.\nMeasuring the extent of LLM-use on scientific publishing has urgent applications. Concerns about accuracy, plagiarism, anonymity, and ownership have prompted some prominent scientific institutions to take a stance on the use of LLM-modified content in academic publications. The International Conference on Machine Learning (ICML) 2023, a major machine learning conference, has prohibited the inclusion of text generated by LLMs like ChatGPT in submitted papers, unless the generated text is used as part of the paper\u2019s experimental analysis (ICML, 2023  ###reference_b30###). Similarly, the journal Science has announced an update to their editorial policies, specifying that text, figures, images, or graphics generated by ChatGPT or any other LLM tools cannot be used in published works (Thorp, 2023  ###reference_b58###). Taking steps to measure the extent of LLM-use can offer a first-step in identifying risks to the scientific publishing ecosystem.\nFurthermore, exploring the circumstances in which LLM-use is high can offer publishers and academic institutions useful insight into author behavior. Sites of high LLM-use can act as indicators for structural challenges faced by scholars. These range from pressures to \u201cpublish or perish\u201d which encourage rapid production of papers to concerns about linguistic discrimination that might lead authors to use LLMs as prose editors.\nWe conduct the first systematic, large-scale analysis to quantify the prevalence of LLM-modified content across multiple academic platforms, extending a recently proposed, state-of-the-art distributional GPT quantification framework (Liang et al., 2024  ###reference_b42###) for estimating the fraction of AI-modified content in a corpus. Throughout this paper, we use the term \u201cLLM-modified\u201d to refer to text content substantially updated by ChatGPT beyond basic spelling and grammatical edits. Modifications we capture in our analysis could include, for example, summaries of existing writing or the generation of prose based on outlines.\nA key characteristic of this framework is that it operates on the population level, without the need to perform inference on any individual instance. As validated in the prior paper, the framework is orders of magnitude more computationally efficient and thus scalable, produces more accurate estimates, and generalizes better than its counterparts under significant temporal distribution shifts and other realistic distribution shifts.\nWe apply this framework to the abstracts and introductions (Figures 1  ###reference_### and 7  ###reference_###) of academic papers across multiple academic disciplines,including arXiv, bioRxiv, and 15 journals within the Nature portfolio, such as Nature,  Nature Biomedical Engineering, Nature Human Behaviour, and Nature Communications. Our study analyzes a total of 950,965 papers published between January 2020 and February 2024, comprising 773,147 papers from arXiv, 161,280 from bioRxiv, and 16,538 from the Nature portfolio journals. The papers from arXiv cover multiple academic fields, including Computer Science, Electrical Engineering and Systems Science, Mathematics, Physics, and Statistics. These datasets allow us to quantify the prevalence of LLM-modified academic writing over time and across a broad range of academic fields.\nOur results indicate that the largest and fastest growth was observed in Computer Science papers, with  reaching 17.5% for abstracts and 15.3% for introductions by February 2024. In contrast, Mathematics papers and the Nature portfolio showed the least increase, with  reaching 4.9% and 6.3% for abstracts and 3.5% and 6.4% for introductions, respectively.\nMoreover, our analysis reveals at an aggregate level that higher levels of LLM-modification are associated with papers whose first authors post preprints more frequently and papers with shorter lengths. Results also demonstrate a closer relationship between papers with LLM-modifications, which could indicate higher use in more crowded fields of study (as measured by the distance to the nearest neighboring paper in the embedding space), or that generated-text is flattening writing diversity."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Various methods have been proposed for detecting LLM-modified text, including zero-shot approaches that rely on psychological signatures characteristic of machine-generated content (Lavergne et al., 2008  ###reference_b38###; Badaskar et al., 2008  ###reference_b3###; Beresneva, 2016  ###reference_b6###; Solaiman et al., 2019  ###reference_b57###; Mitchell et al., 2023a  ###reference_b47###; Yang et al., 2023a  ###reference_b70###; Bao et al., 2023  ###reference_b5###; Tulchinskii et al., 2023  ###reference_b62###) and training-based methods that finetune language models for binary classification of human vs. LLM-modified text (Bhagat & Hovy, 2013  ###reference_b7###; Zellers et al., 2019  ###reference_b74###; Bakhtin et al., 2019  ###reference_b4###; Uchendu et al., 2020  ###reference_b63###; Chen et al., 2023  ###reference_b10###; Yu et al., 2023  ###reference_b73###; Li et al., 2023  ###reference_b39###; Liu et al., 2022  ###reference_b44###; Bhattacharjee et al., 2023  ###reference_b8###; Hu et al., 2023a  ###reference_b28###). However, these approaches face challenges such as the need for access to LLM internals, overfitting to training data and language models, vulnerability to adversarial attacks (Wolff, 2020  ###reference_b68###), and bias against non-dominant language varieties (Liang et al., 2023a  ###reference_b40###). The effectiveness and reliability of publicly available LLM-modified text detectors have also been questioned (OpenAI, 2019  ###reference_b50###; Jawahar et al., 2020  ###reference_b32###; Fagni et al., 2021  ###reference_b19###; Ippolito et al., 2019  ###reference_b31###; Mitchell et al., 2023b  ###reference_b48###; Gehrmann et al., 2019  ###reference_b24###; Heikkil\u00e4, 2022  ###reference_b27###; Crothers et al., 2022  ###reference_b14###; Solaiman et al., 2019  ###reference_b57###; Kirchner et al., 2023  ###reference_b36###; Kelly, 2023  ###reference_b33###), with the theoretical possibility of accurate instance-level detection being debated (Weber-Wulff et al., 2023  ###reference_b67###; Sadasivan et al., 2023  ###reference_b53###; Chakraborty et al., 2023  ###reference_b9###). In this study, we apply the recently proposed distributional GPT quantification framework (Liang et al., 2024  ###reference_b42###), which estimates the fraction of LLM-modified content in a text corpus at the population level, circumventing the need for classifying individual documents or sentences and improving upon the stability, accuracy, and computational efficiency of existing approaches. A more comprehensive discussion of related work can be found in Appendix G  ###reference_###."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Background: the distributional LLM quantification framework",
            "text": "We adapt the distributional LLM quantification framework from Liang et al. (2024  ###reference_b42###) to quantify the use of AI-modified academic writing. The framework consists of the following steps:\nProblem formulation: Let  and  be the probability distributions of human-written and LLM-modified documents, respectively. The mixture distribution is given by , where  is the fraction of AI-modified documents. The goal is to estimate  based on observed documents .\nParameterization: To make  identifiable, the framework models the distributions of token occurrences in human-written and LLM-modified documents, denoted as  and , respectively, for a chosen list of tokens . The occurrence probabilities of each token in human-written and LLM-modified documents,  and , are used to parameterize  and :\nEstimation: The occurrence probabilities  and  are estimated using collections of known human-written and LLM-modified documents,  and , respectively:\nInference: The fraction  is estimated by maximizing the log-likelihood of the observed documents under the mixture distribution :\nLiang et al. (2024  ###reference_b42###) demonstrate that the data points  can be constructed either as a document or as a sentence, and both work well. Following their method, we use sentences as the unit of data points for the estimates for the main results. In addition, we extend this framework for our application to academic papers with two key differences:\nWe use a two-stage approach to generate LLM-produced text, as simply prompting an LLM with paper titles or keywords would result in unrealistic scientific writing samples containing fabricated results, evidence, and ungrounded or hallucinated claims.\nSpecifically, given a paragraph from a paper known to not include LLM-modification, we first perform abstractive summarization using an LLM to extract key contents in the form of an outline. We then prompt the LLM to generate a full paragraph based the outline (see Appendix for full prompts).\nOur two-stage approach can be considered a counterfactual framework for generating LLM text: given a paragraph written entirely by a human, how would the text read if it conveyed almost the same content but was generated by an LLM? This additional abstractive summarization step can be seen as the control for the content.\nThis approach also simulates how scientists may be using LLMs in the writing process, where the scientists first write the outline themselves and then use LLMs to generate the full paragraph based on the outline.\nWe use the full vocabulary instead of only adjectives, as our validation shows that adjectives, adverbs, and verbs all perform well in our application (Figure 3  ###reference_###).\nUsing the full vocabulary minimizes design biases stemming from vocabulary selection. We also find that using the full vocabulary is more sample-efficient in producing stable estimates, as indicated by their smaller confidence intervals by bootstrap.\n###figure_1###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Implementation and Validations",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Data Collection and Sampling",
            "text": "We collect data from three sources: arXiv, bioRxiv, and 15 journals from the Nature portfolio. For each source, we randomly sample up to 2,000 papers per month from January 2020 to February 2024. The procedure for generating the LLM-generated corpus data is described in Section  3  ###reference_###. We focused on the introduction sections for the main texts, as the introduction was the most consistently and commonly occurring section across diverse categories of papers.\nSee Appendix C  ###reference_### for comprehensive implementation details."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Data Split, Model Fitting, and Evaluation",
            "text": "For model training, we count word frequencies for scientific papers written before the release of ChatGPT and the LLM-modified corpora described in Section 3  ###reference_###. We fit the model with data from 2020, and use data from January 2021 onwards for validation and inference. We fit separate models for abstracts and introductions for each major category.\nTo evaluate model accuracy and calibration under temporal distribution shift, we use 3,000 papers from January 1, 2022, to November 29, 2022, a time period prior to the release of ChatGPT, as the validation data. We construct validation sets with LLM-modified content proportions () ranging from 0% to 25%, in 5% increments, and compared the model\u2019s estimated  with the ground truth  (Figure 3  ###reference_###). Full vocabulary, adjectives, adverbs, and verbs all performed well in our application, with a prediction error consistently less than 3.5% at the population level across various ground truth  values (Figure 3  ###reference_###)."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Main Results and Findings",
            "text": "We apply the model to estimate the fraction of LLM-modified content () for each paper category each month, for both abstracts and introductions. Each point in time was independently estimated, with no temporal smoothing or continuity assumptions applied.\nOur findings reveal a steady increase in the fraction of AI-modified content () in both the abstracts (Figure 1  ###reference_###) and the introductions (Figure 7  ###reference_###), with the largest and fastest growth observed in Computer Science papers. By February 2024, the estimated  for Computer Science had increased to 17.5% for abstracts and 15.5% for introductions. The second-fastest growth was observed in Electrical Engineering and Systems Science, with the estimated  reaching 14.4% for abstracts and 12.4% for introductions during the same period. In contrast, Mathematics papers and the Nature portfolio showed the least increase. By the end of the studied period, the estimated  for Mathematics had increased to 4.9% for abstracts and 3.9% for introductions, while the estimated  for the Nature portfolio had reached 6.3% for abstracts and 4.3% for introductions.\nThe November 2022 estimates serve as a pre-ChatGPT reference point for comparison, as ChatGPT was launched on November 30, 2022. The estimated  for Computer Science in November 2022 was 2.3%, while for Electrical Engineering and Systems Science, Mathematics, and the Nature portfolio, the estimates were 2.9%, 2.4%, and 3.1%, respectively. These values are consistent with the false positive rate reported in the earlier section ( 4.2  ###reference_###)."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Temporal Trends in AI-Modified Academic Writing",
            "text": "We apply the model to estimate the fraction of LLM-modified content () for each paper category each month, for both abstracts and introductions. Each point in time was independently estimated, with no temporal smoothing or continuity assumptions applied.\nOur findings reveal a steady increase in the fraction of AI-modified content () in both the abstracts (Figure 1  ###reference_###  ###reference_###) and the introductions (Figure 7  ###reference_###  ###reference_###), with the largest and fastest growth observed in Computer Science papers. By February 2024, the estimated  for Computer Science had increased to 17.5% for abstracts and 15.5% for introductions. The second-fastest growth was observed in Electrical Engineering and Systems Science, with the estimated  reaching 14.4% for abstracts and 12.4% for introductions during the same period. In contrast, Mathematics papers and the Nature portfolio showed the least increase. By the end of the studied period, the estimated  for Mathematics had increased to 4.9% for abstracts and 3.9% for introductions, while the estimated  for the Nature portfolio had reached 6.3% for abstracts and 4.3% for introductions.\nThe November 2022 estimates serve as a pre-ChatGPT reference point for comparison, as ChatGPT was launched on November 30, 2022. The estimated  for Computer Science in November 2022 was 2.3%, while for Electrical Engineering and Systems Science, Mathematics, and the Nature portfolio, the estimates were 2.9%, 2.4%, and 3.1%, respectively. These values are consistent with the false positive rate reported in the earlier section ( 4.2  ###reference_###  ###reference_###)."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Relationship Between First-Author Preprint Posting Frequency and GPT Usage",
            "text": "We found a notable correlation between the number of preprints posted by the first author on arXiv and the estimated number of LLM-modified sentences in their academic writing. Papers were stratified into two groups based on the number of first-authored arXiv Computer Science preprints by the first author in the year: those with two or fewer () preprints and those with three or more () preprints (Figure 4  ###reference_###). We used the 2023 author grouping for the 2024.1-2 data, as we don\u2019t have the complete 2024 author data yet.\nBy February 2024, abstracts of papers whose first authors had  preprints in 2023 showed an estimated 19.3% of sentences modified by AI, compared to 15.6% for papers whose first authors had  preprints (Figure 4  ###reference_###a). We observe a similar trend in the introduction sections, with first authors posting more preprints having an estimated 16.9% LLM-modified sentences, compared to 13.7% for first authors posting fewer preprints (Figure 4  ###reference_###b).\nSince the first-author preprint posting frequency may be confounded by research field, we conduct an additional robustness check for our findings. We find that the observed trend holds for each of the three arXiv Computer Science sub-categories: cs.CV (Computer Vision and Pattern Recognition), cs.LG (Machine Learning), and cs.CL (Computation and Language) (Supp Figure 12  ###reference_###).\nOur results suggest that researchers posting more preprints tend to utilize LLMs more extensively in their writing. One interpretation of this effect could be that the increasingly competitive and fast-paced nature of CS research communities incentivizes taking steps to accelerate the writing process. We do not evaluate whether these preprints were accepted for publication.\n###figure_2###"
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Relationship Between Paper Similarity and LLM Usage",
            "text": "We investigate the relationship between a paper\u2019s similarity to its closest peer and the estimated LLM usage in the abstract. To measure similarity, we first embed each abstract from the arXiv Computer Science papers using OpenAI\u2019s text-embedding-ada-002 model, creating a vector representation for each abstract. We then calculate the distance between each paper\u2019s vector and its nearest neighbor within the arXiv Computer Science abstracts. Based on this similarity measure we divide papers into two groups: those more similar to their closest peer (below median distance) and those less similar (above median distance).\nThe temporal trends of LLM usage for these two groups are shown in Figure 5  ###reference_###. After the release of ChatGPT, papers most similar to their closest peer consistently showed higher LLM usage compared to those least similar. By February 2024, the abstracts of papers more similar to their closest peer had an estimated 22.2% of sentences modified by LLMs, compared to 14.7% for papers less similar to their closest peer.\nTo account for potential confounding effects of research fields, we conducted an additional robustness check by measuring the nearest neighbor distance within each of the three arXiv Computer Science sub-categories: cs.CV (Computer Vision and Pattern Recognition), cs.LG (Machine Learning), and cs.CL (Computation and Language), and found that the observed trend holds for each sub-category (Supp Figure 13  ###reference_###).\nThere are several ways to interpret these findings. First, LLM-use in writing could cause the similarity in writing or content. Community pressures may even motivate scholars to try to sound more similar \u2013 to assimilate to the \u201cstyle\u201d of text generated by an LLM.\nAlternatively, LLMs may be more commonly used in research areas where papers tend to be more similar to each other. This could be due to the competitive nature of these crowded subfields, which may pressure researchers to write faster and produce similar findings. Future interdisciplinary research should explore these hypotheses.\n###figure_3###"
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Relationship Between Paper Length and AI Usage",
            "text": "We also explored the association between paper length and LLM usage in arXiv Computer Science papers. Papers were stratified by their full text word count, including appendices, into two bins: below or above 5,000 words (the rounded median).\nFigure 6  ###reference_### shows the temporal trends of LLM usage for these two groups. After the release of ChatGPT, shorter papers consistently showed higher AI usage compared to longer papers. By February 2024, the abstracts of shorter papers had an estimated 17.7% of sentences modified by LLMs, compared to 13.6% for longer papers (Figure 6  ###reference_###a). A similar trend was observed in the introduction sections (Figure 6  ###reference_###b).\nTo account for potential confounding effects of research fields, we conducted an additional robustness check. The finding holds for both cs.CV (Computer Vision and Pattern Recognition) and cs.LG (Machine Learning) (Supp Figure 14  ###reference_###). However, for cs.CL (Computation and Language), we found no significant difference in LLM usage between shorter and longer papers, possibly due to the limited sample size, as we only parsed a subset of the PDFs and calculated their full length.\nAs Computer Science conference papers typically have a fixed page limit, longer papers likely have more substantial content in the appendix. The lower LLM usage in these papers may suggest that researchers with more comprehensive work rely less on LLM-assistance in their writing. However, further investigation is needed to determine the relationship between paper length, content comprehensiveness, and the quality of the research.\n###figure_4###"
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "Our findings show a sharp increase in the estimated fraction of LLM-modified content in academic writing beginning about five months after the release of ChatGPT, with the fastest growth observed in Computer Science papers. This trend may be partially explained by Computer Science researchers\u2019 familiarity with and access to large language models. Additionally, the fast-paced nature of LLM research and the associated pressure to publish quickly may incentivize the use of LLM writing assistance (Foster et al., 2015  ###reference_b21###).\nWe expose several other factors associated with higher LLM usage in academic writing. First, authors who post preprints more frequently show a higher fraction of LLM-modified content in their writing. Second, papers in more crowded research areas, where papers tend to be more similar, showed higher LLM-modification compared to those in less crowded areas. Third, shorter papers consistently showed higher LLM-modification compared to longer papers, which may indicate that researchers working under time constraints are more likely to rely on AI for writing assistance. These results may be an indicator of the competitive nature of certain research areas and the pressure to publish quickly.\nIf the majority of modification comes from an LLM owned by a private company, there could be risks to the security and independence of scientific practice.\nWe hope our results inspire\nfurther studies of widespread LLM-modified text and conversations about how to promote transparent, epistemically diverse, accurate, and independent scientific publishing.\nWhile our study focused on ChatGPT, which accounts for more than three-quarters of worldwide internet traffic in the category (Van Rossum, 2024  ###reference_b64###), we acknowledge that there are other large language models used for assisting academic writing.\nFurthermore, while Liang et al. (2023a  ###reference_b40###) demonstrate that GPT-detection methods can falsely identify the writing of language learners as LLM-generated, our results showed that consistently low false positives estimates of  in 2022, which contains a significant fraction of texts written by multilingual scholars. We recognize that significant author population changes (MacroPolo, 2024  ###reference_b46###) or other language-use shifts could still impact the accuracy of our estimates. Finally, the associations that we observe between LLM usage and paper characteristics are correlations which could be affected by other factors such as research topics. More causal studies is an important direction for future work."
        }
    ],
    "url": "http://arxiv.org/html/2404.01268v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3"
        ],
        "main_experiment_and_results_sections": [
            "4.1",
            "4.2",
            "5",
            "5.1",
            "5.2",
            "5.3",
            "5.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "4.2"
        ]
    },
    "research_context": {
        "paper_id": "2404.01268v1",
        "paper_title": "Mapping the Increasing Use of LLMs in Scientific Papers",
        "research_background": "### Paper's Motivation\n\nThe motivation behind this study is rooted in the significant rise in the use of large language models (LLMs), particularly ChatGPT, in generating and modifying text for scientific papers and peer reviews. This trend has prompted both humor and concern within the academic community, highlighting issues of linguistic discrimination, accuracy, plagiarism, anonymity, and ownership. Institutions like the International Conference on Machine Learning (ICML) and the journal Science have even enacted policies to set boundaries regarding the use of such models, showcasing the urgency of establishing a clear understanding of the prevalence and implications of LLM-modified content. The study aims to provide a systematic, large-scale analysis of LLM usage to gain insights into structural motivations, author behaviors, and potential risks to the scientific publishing ecosystem.\n\n### Research Problem\n\nThe core research problem addressed by this paper is quantifying the prevalence and impact of LLM-modified content in scientific publishing. There are two primary facets to this problem: \n1. Detecting the extent of LLM use in academic texts on a large scale, especially when direct evidence of modifications is not readily apparent.\n2. Understanding the structural and behavioral factors that correlate with high LLM usage, thereby providing insights into the pressures and motivations driving authors to rely on these tools.\n\n### Relevant Prior Work\n\n1. **Examples of ChatGPT in Academic Publishing**: Earlier works have discussed anecdotal instances of LLM-generated published papers (Okunyt\u0117, 2023; Deguerin, 2024) and peer reviews (Oransky & Marcus, 2024), which have inspired diverse reactions from the academic community.\n   \n2. **Linguistic Discrimination**: Khanna et al. (2022) brought out discussions on entrenched linguistic discrimination in academic publishing, highlighting a relevant context for multilingual scholars using LLMs.\n\n3. **Detection Methods**: Conroy (2023a; 2023b) and Vincent (2023) pointed out obvious tells of LLM-modification such as phrases like \u201cregenerate response\u201d and \u201cas an AI language model,\u201d but acknowledged the difficulty of detecting subtle modifications. \n   \n4. **Advanced Detection Techniques**: Liang et al. (2024) developed a framework capable of detecting the percentage of LLM-modified text on a large scale without needing individual-level inference. This method is praised for its computational efficiency, scalability, accuracy, and ability to handle distribution shifts.\n\nThis study builds on these frameworks by expanding the scope and applying the detection technique to a comprehensive dataset encompassing multiple academic disciplines and over 950,965 papers. The analysis aims to capture trends, identify areas with significant LLM usage, and explore related structural and behavioral insights.",
        "methodology": "### Methodology\n\nWe adapt the **distributional LLM quantification framework** from Liang et al. (2024) to quantify the use of AI-modified academic writing. This approach involves several key steps:\n\n#### 1. Problem Formulation\n- Let \\( P_H \\) and \\( P_L \\) be the probability distributions of human-written and LLM-modified documents, respectively. The mixed distribution is \\( P = (1 - \\alpha) P_H + \\alpha P_L \\), where \\( \\alpha \\) represents the fraction of AI-modified documents.\n- The objective is to estimate \\(\\alpha\\) based on observed documents \\(D\\).\n\n#### 2. Parameterization\n- To ensure identifiability of \\(P\\), the framework models the distributions of token occurrences in human-written (\\( \\theta_H \\)) and LLM-modified (\\( \\theta_L \\)) documents for a selected list of tokens \\( T \\).\n- The occurrence probabilities of each token in human-written and LLM-modified documents, \\( q_H(t) \\) and \\( q_L(t) \\), parameterize \\( P_H \\) and \\( P_L \\).\n\n#### 3. Estimation\n- The occurrence probabilities \\( q_H(t) \\) and \\( q_L(t) \\) are estimated using collections of known human-written and LLM-modified documents, denoted as \\( D_H \\) and \\( D_L \\), respectively.\n\n#### 4. Inference\n- The fraction \\(\\alpha\\) is estimated by maximizing the log-likelihood of the observed documents under the mixture distribution \\( P \\).\n\nTo specifically apply this framework to academic papers, we introduce two key modifications:\n\n#### Key Innovations:\n\n1. **Two-Stage Generation Approach:**\n   - Instead of simply prompting an LLM with titles or keywords, which often produces unrealistic text with fabricated content, we adopt a robust two-stage process.\n   - Given a paragraph from a known non-LLM-modified paper, we first use an LLM for **abstractive summarization** to extract key content into an outline.\n   - The LLM is then prompted to generate a full paragraph based on this outline (see Appendix for prompt details).\n   - This method serves as a counterfactual: given a human-written paragraph, how would it read if almost the same content were generated by an LLM?\n\n2. **Use of Full Vocabulary:**\n   - Instead of limiting to adjectives, our validation shows that adjectives, adverbs, and verbs are all effective in this application.\n   - Using the full vocabulary minimizes biases from vocabulary selection and enhances sample efficiency, resulting in stable estimates with smaller bootstrap confidence intervals.\n\nThese innovations aim to simulate how scientists might actually be using LLMs, writing the outline themselves and then using LLMs to flesh out the full paragraphs, thus capturing a realistic scenario of LLM integration into academic writing.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n**Experiment Setup:**\n\n- **Datasets:** \n  - Data sources include arXiv, bioRxiv, and 15 journals from the Nature portfolio.\n  - A random sample of up to 2,000 papers per month from each source is taken from January 2020 to February 2024.\n  - The focus is on the introduction sections of the papers, as the introduction is a consistently present section across various categories of papers.\n\n**Baselines:**\n- The experiment does not explicitly mention baseline models. However, LLM-generated corpus data is compared over time to observe trends.\n\n**Evaluation Metrics:**\n- Metrics are not specifically defined in the main experiment description. The focus appears to be on qualitative analysis, such as identifying patterns or frequencies of LLM usage in the collected scientific papers.\n\n**Main Experimental Results:**\n- The main results are not explicitly detailed in the provided section. It is implied that the data is used to map the increasing use of Large Language Models (LLMs) in the selected scientific papers over time, particularly by examining the introduction sections.\n\nFor thorough implementation details and specific results, see the referenced sections in the paper (Section 3 and Appendix C)."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To quantify the use of AI-modified academic writing in scientific papers.",
            "experiment_process": "Utilizing the distributional LLM quantification framework, the occurrence probabilities of tokens in human-written and LLM-modified documents are parameterized, and the fraction of AI-modified documents is estimated by maximizing the log-likelihood of observed documents. Estimations are validated using known collections of human-written and LLM-modified texts, with sentences as the unit of data points. A two-stage approach is used: first, actual human-written paragraphs are summarized using an LLM, then the LLM generates a full paragraph based on the outline. The full vocabulary is utilized to minimize biases and maximize sample efficiency.",
            "result_discussion": "Using the full vocabulary, stable and sample-efficient estimates with smaller confidence intervals were produced, indicating better performance over a limited vocabulary approach. The two-stage method likely reflects actual scientific writing processes involving LLMs.",
            "ablation_id": "2404.01268v1.No1"
        },
        {
            "research_objective": "To evaluate model accuracy and calibration in detecting LLM-modified content under temporal distribution shifts.",
            "experiment_process": "Word frequencies from scientific papers written pre-ChatGPT and LLM-modified corpora (described in Section 3) were used for model training, with data from 2020. Validation and inference used data from January 2021 onwards. Separate models for abstracts and introductions across major categories were trained. Evaluation used 3,000 papers from January to November 2022 (pre-ChatGPT) as the validation data. Validation sets with varying proportions of LLM-modified content (0% to 25%) were compared to model estimates.",
            "result_discussion": "The model successfully maintained prediction errors under 3.5% at the population level across different ground truth values, indicating robust performance of full vocabulary usage and the model\u2019s accuracy under temporal distribution shifts.",
            "ablation_id": "2404.01268v1.No2"
        }
    ]
}