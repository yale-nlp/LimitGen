{
    "title": "Training LLMs over Neurally Compressed Text",
    "abstract": "In this paper, we explore the idea of training large language models (LLMs) over highly compressed text. While standard subword tokenizers compress text by a small factor, neural text compressors can achieve much higher rates of compression. If it were possible to train LLMs directly over neurally compressed text, this would confer advantages in training and serving efficiency, as well as easier handling of long text spans. The main obstacle to this goal is that strong compression tends to produce opaque outputs that are not well-suited for learning. In particular, we find that text na\u00efvely compressed via Arithmetic Coding is not readily learnable by LLMs. To overcome this, we propose Equal-Info Windows, a novel compression technique whereby text is segmented into blocks that each compress to the same bit length. Using this method, we demonstrate effective learning over neurally compressed text that improves with scale, and outperforms byte-level baselines by a wide margin on perplexity and inference speed benchmarks. While our method delivers worse perplexity than subword tokenizers for models trained with the same parameter count,\nit has the benefit of shorter sequence lengths.\nShorter sequence lengths require fewer autoregressive generation steps,\nand reduce latency.\nFinally, we provide extensive analysis of the properties that contribute to learnability, and offer concrete suggestions for how to further improve the performance of high-compression tokenizers.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Today\u2019s large language models (LLMs) are almost exclusively trained over subword tokens. The tokenizers used to produce these tokens\u2014often BPE [23  ###reference_b23###, 56  ###reference_b56###] or Unigram [37  ###reference_b37###], as implemented by the SentencePiece library [38  ###reference_b38###]\u2014are compressors that typically achieve ~4 compression over natural language text [74  ###reference_b74###].111We refer here to \u201ctoken-level\u201d compression rate, i.e., the length reduction between a raw UTF-8 byte sequence and the corresponding sequence of subword tokens. If instead we measure the number of bits required to encode the two sequences, subword compression typically delivers ~2 or less compression, depending on vocabulary size, which typically ranges from 32k to 256k. See Section 3.4  ###reference_### for discussion. While these tokenizers \u201chide\u201d the character-level makeup of each token from the LLM [74  ###reference_b74###, 44  ###reference_b44###], this downside is widely seen as outweighed by the significant benefits of compression. Compared to raw byte-level models, an LLM trained over subword tokens sees ~4 more text per token, allowing it to model longer-distance dependencies, ingest more pretraining data, and predict more text at inference time, all without increasing compute.222The increased cost of the input embedding and final softmax layers due to increased vocabulary size is negligible for all but the smallest models.\nGiven these advantages, it raises the question, could we compress text further to achieve even greater gains? It is well known that autoregressive language models can be turned into lossless text compressors, and recent work has shown that LLMs can easily achieve 12 compression over English text [16  ###reference_b16###].333Specifically, the authors show that Chincilla 70B [30  ###reference_b30###] can compress 2048-byte subspans of enwik9 at a 12 bit-level compression rate. Can we simply train an LLM over this neurally compressed text?\n###figure_1### In this paper we explore various options for doing so, focusing primarily on the idea of using Arithmetic Coding (AC) [73  ###reference_b73###], which is known to reach the near-optimal compression rate for a particular model that assigns probabilities to text continuations. Figure 1  ###reference_### presents our high-level approach. First, a small language model \u201cM1\u201d is trained over raw byte sequences. Next, this frozen model is used to compress pretraining corpus text by applying a standard compression algorithm like AC. The resulting compressed bitstream is then chunked into tokens, which are used to train \u201cM2\u201d, a language model that directly reads and writes neural-compressed text.\nGiven a perfect probabilistic model of the raw byte sequence, the compression step would output a fully-compressed bitstream that would be indistinguishable from random noise, and hence unlearnable by M2.\nIn reality, M1 can never be perfect [78  ###reference_b78###], so the M1-compressed output will still contain learnable patterns. We explore whether using compression powered by a relatively small M1 is able to \u201cremove\u201d the simple structure that M1 understands from the input\u2014e.g., patterns of spelling, word frequency, and basic grammar\u2014while retaining any higher-level structure that M1 fails to model\u2014e.g., patterns requiring \u201cdeeper\u201d reasoning and long range coherence. A larger M2 would then learn to model this higher-level structure, without needing to relearn the low-level structure removed by M1.444Intuitively, training M2 could be seen as analogous to fitting the residuals of M1 [21  ###reference_b21###]. In theory, this process could be repeated by training an even-larger M3 model on text compressed by M2, and so on.\nIn practice, we find that text compressed via Arithmetic Coding is not readily learnable by a standard transformer-based LLM, with resulting models predicting tokens at chance. Interestingly, this result holds even when M1 is reduced to a context-free unigram model, suggesting that the challenge of modeling AC-compressed text stems from the difficulty of learning the AC compression and decompression process itself. We verify this hypothesis by showing that even the sub-tasks of AC-compressing and AC-decompressing text are not learned well beyond a few initial tokens.\nTo aid learnability, we propose compression via Equal-Info Windows, a simple technique that breaks text into contiguous windows and compresses them via Arithmetic Coding independently. Rather than splitting text into windows of equal text length, we track the number of bits output by the compressor, and close each window just before it exceeds a set information threshold (e.g., 32 bits of information). This has the advantage that when chunking the subsequent bitstream into M2 tokens, there is a stable mapping from N tokens to one window (e.g., four 8-bit tokens\u2009\u2009one 32-bit window). At each window boundary, we reset both AC algorithm and the M1 model context. This ensures that each window may be mapped back onto raw text without any additional information.\nThrough ablations on window size and M2 vocabulary size, we find that Equal-Info Windows make learning of AC-compressed text possible across a range of settings. However, we also observe that learning progresses gradually, starting with tokens at the left edge of each window, and for longer windows, the model learns little about the tokens near the right edge. Our best-performing setting uses short 16-bit windows that each correspond to a single 16-bit M2 token. Despite resetting the compression algorithm every 16 bits, we still achieve ~5.3 token-level compression overall, which exceeds standard subword tokenizers. Remarkably, our best M2 models outperform byte-level baselines on perplexity benchmarks (bits/byte) for fixed computation budget (FLOPs/byte). This shows that learning over neural-compressed text can be effective.\nAt the same time, our best M2 models underperform subword baselines. We suspect this is due at least in part to the relatively unstable mappings our neural tokenizers induce between words and tokens. By contrast, standard subword tokenizers induce essentially stable word-to-token mappings, which likely makes the token sequences they output well-suited for LLM training.555See Appendix L  ###reference_### for some counterexamples to subword tokenizers producing stable word-to-token mappings. We illustrate this contrast through qualitative examples. Whether a neural tokenizer can reach a high level of compression while maintaining high learnability for LLM training is an interesting question for future research.\nOur main contributions are as follows:\n(1) Outline advantages and challenges of training over neural compressed text.\n(2) Compare LLMs trained over different tokenizers along two axes: bits/byte and FLOPs/byte.\n(3) Show that standard LLMs can\u2019t learn to model vanilla AC-compressed text.\n(4) Show that GZip-compressed text is learnable by standard LLMs, but not competitive.\n(5) Propose compression via Equal-Info Windows, and show that it enables learning over neural compressed text."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Motivation and Background",
            "text": "The most straightforward advantage is efficiency. By compressing the same text into a shorter token sequence, the model can process more text for the same computational cost. In particular, a model trained over  compressed text will see  more text during training compared to a model trained over raw text, given an equal compute budget. Increasing the amount of data seen in pretraining is often an effective means of improving performance [35  ###reference_b35###, 30  ###reference_b30###]. Processing text more efficiently also confers benefits at inference time, reducing the serving cost for handling a request of a given prompt and continuation length. In addition to reducing the raw compute needed for inference, compression can also improve inference latency, since generating better-compressed output requires fewer sequential autoregressive steps.\nA second advantage is that working with compressed text allows modeling longer contextual dependencies. In vanilla transformer-based models, computation for the self-attention layer scales quadratically with the sequence length, . This has limited the sequence lengths used by such models in practical settings to ~k tokens.666Exploring sub-quadratic attention mechanisms is an area of active research [1  ###reference_b1###, 70  ###reference_b70###, 36  ###reference_b36###, 75  ###reference_b75###, 5  ###reference_b5###, 9  ###reference_b9###, et alia]. However, regardless of the cost of attention, compressing the input increases the effective context \u201cfor free\u201d.\nIf, via compression, each token represents (on average)  bytes of raw text, then the resulting LLM can model dependencies across  longer distances compared to a raw text model operating over the same token sequence length. While the benefits of modeling longer context (beyond ~ bytes) are modest when viewed merely as perplexity gains [51  ###reference_b51###], the ability to condition on long context is critical for many applications, such as retrieving content from a document, or answering a coding question provided documentation.\nA third potential advantage of training over compressed text is that information will be spread more uniformly across the sequence. By the nature of compression, a text span that is relatively predictable (e.g., a boilerplate notice) will be more compressible than a span with high perplexity (e.g., a unique product serial number). When an LLM is trained over well-compressed text, each token will represent roughly an equal amount of information. Since the LLM allocates equal compute to each token, this amounts to allocating more compute for \u201charder\u201d text spans. This adaptivity is similar in spirit to \u201cAdaptive Computation Time\u201d (ACT) [27  ###reference_b27###], which learns to allocate additional compute at some sequence positions in an end-to-end manner, but with the advantage that in our case the computation remains \u201cdense\u201d\u2014identical operations are applied at each position.777It should be noted that ACT learns to allocate more compute where it is useful, as opposed to merely where the predictions are hard. For example, ACT learns to not waste compute on inherently unpredictable text spans. We expect that as a heuristic, allocating more compute to higher-perplexity text spans is valuable, but leave this to future work to verify.\nIt is not at all obvious what types of compression are \u201ctransparent\u201d enough to be learnable through a standard LLM training process. Strong compression can be seen as removing as much redundant or predictable information from a sequence as possible. Consequently, the bitstream output by a good compressor is inherently hard to distinguish from random noise. In this work, we explore the setting where M2\u2014the model trained over compressed text\u2014has a larger capacity than M1, the model used for compression. In principle, this setup should allow M2 to extract additional information from the signal even after M1 has compressed it. However, for strong enough M1 compression, the resulting bitstream may be too noisy to detect any signal.\nAs a prerequisite for M2 to effectively predict continuations of compressed text, we anticipate that it is necessary for M2 to have the ability to decompress bits\u2009\u2009text and compress text\u2009\u2009bits. These sub-tasks are challenging in their own right. First, M2 needs to accurately \u201csimulate\u201d M1 in order to know the probabilities it assigns to the text, which determine the output of compression.888For Arithmetic Coding, not only would M2 need to know the probabilities M1 assigns to the observed text, but it would also need to know the probabilities assigned to many unobserved symbols. This is because Arithmetic Coding operates over cumulative probabilities, i.e., the probability that the next symbol is e or any alphabetically preceding symbol. Training models to mimic other models can be difficult [41  ###reference_b41###], and even in settings where models do learn to copy the behavior of another network [29  ###reference_b29###], this is often only when looking at which symbol was assigned the highest probability\u2014the actual probabilities assigned often differ [60  ###reference_b60###]. Second, M2 needs to learn the compression procedure itself. In our case, this means tracking the Arithmetic Coding algorithm, which requires maintaining high-precision numerical state across long contexts. We investigate these sub-tasks in detail in Section 5.2  ###reference_###.\nA further learnability challenge is the high level of context sensitivity needed to interpret a bitstream of compressed text. When chunked into tokens, a particular bit subsequence (e.g., 10111001) can map onto the same token despite having no stable \u201cmeaning\u201d across occurrences. We show examples in Section 6.1  ###reference_###, where a token maps to many different underlying text forms, necessitating strong contextual understanding. While LLMs are robust to some level of polysemy, as highlighted by the success of Hash Embeddings [62  ###reference_b62###] where multiple unrelated words share a single token representation, we suspect this has its limits.\nAn additional technical challenge is that compression methods can be sensitive to the precise model probabilities used. To achieve lossless compression in our setup, it is critical that the M1 probabilities match during compression and decompression. This can be hard to guarantee in practice, as there are many sources of numerical noise in LLM inference, especially when running on parallel hardware. An expanded discussion of numerical stability issues can be found in Section 3.7  ###reference_###.\nFinally, a specific challenge of training over neural compressed text is that multiple models need to be stored and run side-by-side in order to perform inference. We assume that if M1 is relatively small, this additional overhead is not a significant drawback compared to a standard tokenizer, which is also a separate model that is needed to tokenize text input and detokenize LLM outputs. In evaluating our approach, we include M1 compute in our calculations of total inference cost (FLOPs/byte)."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Advantages of Training over Neural-Compressed Text",
            "text": "Training LLMs over compressed text is appealing for many reasons. We discuss three advantages in detail below.\nThe most straightforward advantage is efficiency. By compressing the same text into a shorter token sequence, the model can process more text for the same computational cost. In particular, a model trained over  compressed text will see  more text during training compared to a model trained over raw text, given an equal compute budget. Increasing the amount of data seen in pretraining is often an effective means of improving performance [35  ###reference_b35###  ###reference_b35###, 30  ###reference_b30###  ###reference_b30###]. Processing text more efficiently also confers benefits at inference time, reducing the serving cost for handling a request of a given prompt and continuation length. In addition to reducing the raw compute needed for inference, compression can also improve inference latency, since generating better-compressed output requires fewer sequential autoregressive steps.\nA second advantage is that working with compressed text allows modeling longer contextual dependencies. In vanilla transformer-based models, computation for the self-attention layer scales quadratically with the sequence length, . This has limited the sequence lengths used by such models in practical settings to ~k tokens.666Exploring sub-quadratic attention mechanisms is an area of active research [1  ###reference_b1###  ###reference_b1###, 70  ###reference_b70###  ###reference_b70###, 36  ###reference_b36###  ###reference_b36###, 75  ###reference_b75###  ###reference_b75###, 5  ###reference_b5###  ###reference_b5###, 9  ###reference_b9###  ###reference_b9###, et alia]. However, regardless of the cost of attention, compressing the input increases the effective context \u201cfor free\u201d.\nIf, via compression, each token represents (on average)  bytes of raw text, then the resulting LLM can model dependencies across  longer distances compared to a raw text model operating over the same token sequence length. While the benefits of modeling longer context (beyond ~ bytes) are modest when viewed merely as perplexity gains [51  ###reference_b51###  ###reference_b51###], the ability to condition on long context is critical for many applications, such as retrieving content from a document, or answering a coding question provided documentation.\nA third potential advantage of training over compressed text is that information will be spread more uniformly across the sequence. By the nature of compression, a text span that is relatively predictable (e.g., a boilerplate notice) will be more compressible than a span with high perplexity (e.g., a unique product serial number). When an LLM is trained over well-compressed text, each token will represent roughly an equal amount of information. Since the LLM allocates equal compute to each token, this amounts to allocating more compute for \u201charder\u201d text spans. This adaptivity is similar in spirit to \u201cAdaptive Computation Time\u201d (ACT) [27  ###reference_b27###  ###reference_b27###], which learns to allocate additional compute at some sequence positions in an end-to-end manner, but with the advantage that in our case the computation remains \u201cdense\u201d\u2014identical operations are applied at each position.777It should be noted that ACT learns to allocate more compute where it is useful, as opposed to merely where the predictions are hard. For example, ACT learns to not waste compute on inherently unpredictable text spans. We expect that as a heuristic, allocating more compute to higher-perplexity text spans is valuable, but leave this to future work to verify."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Challenges of Training over Compressed Text",
            "text": "It is not at all obvious what types of compression are \u201ctransparent\u201d enough to be learnable through a standard LLM training process. Strong compression can be seen as removing as much redundant or predictable information from a sequence as possible. Consequently, the bitstream output by a good compressor is inherently hard to distinguish from random noise. In this work, we explore the setting where M2\u2014the model trained over compressed text\u2014has a larger capacity than M1, the model used for compression. In principle, this setup should allow M2 to extract additional information from the signal even after M1 has compressed it. However, for strong enough M1 compression, the resulting bitstream may be too noisy to detect any signal.\nAs a prerequisite for M2 to effectively predict continuations of compressed text, we anticipate that it is necessary for M2 to have the ability to decompress bits\u2009\u2009text and compress text\u2009\u2009bits. These sub-tasks are challenging in their own right. First, M2 needs to accurately \u201csimulate\u201d M1 in order to know the probabilities it assigns to the text, which determine the output of compression.888For Arithmetic Coding, not only would M2 need to know the probabilities M1 assigns to the observed text, but it would also need to know the probabilities assigned to many unobserved symbols. This is because Arithmetic Coding operates over cumulative probabilities, i.e., the probability that the next symbol is e or any alphabetically preceding symbol. Training models to mimic other models can be difficult [41  ###reference_b41###  ###reference_b41###], and even in settings where models do learn to copy the behavior of another network [29  ###reference_b29###  ###reference_b29###], this is often only when looking at which symbol was assigned the highest probability\u2014the actual probabilities assigned often differ [60  ###reference_b60###  ###reference_b60###]. Second, M2 needs to learn the compression procedure itself. In our case, this means tracking the Arithmetic Coding algorithm, which requires maintaining high-precision numerical state across long contexts. We investigate these sub-tasks in detail in Section 5.2  ###reference_###  ###reference_###.\nA further learnability challenge is the high level of context sensitivity needed to interpret a bitstream of compressed text. When chunked into tokens, a particular bit subsequence (e.g., 10111001) can map onto the same token despite having no stable \u201cmeaning\u201d across occurrences. We show examples in Section 6.1  ###reference_###  ###reference_###, where a token maps to many different underlying text forms, necessitating strong contextual understanding. While LLMs are robust to some level of polysemy, as highlighted by the success of Hash Embeddings [62  ###reference_b62###  ###reference_b62###] where multiple unrelated words share a single token representation, we suspect this has its limits.\nAn additional technical challenge is that compression methods can be sensitive to the precise model probabilities used. To achieve lossless compression in our setup, it is critical that the M1 probabilities match during compression and decompression. This can be hard to guarantee in practice, as there are many sources of numerical noise in LLM inference, especially when running on parallel hardware. An expanded discussion of numerical stability issues can be found in Section 3.7  ###reference_###  ###reference_###.\nFinally, a specific challenge of training over neural compressed text is that multiple models need to be stored and run side-by-side in order to perform inference. We assume that if M1 is relatively small, this additional overhead is not a significant drawback compared to a standard tokenizer, which is also a separate model that is needed to tokenize text input and detokenize LLM outputs. In evaluating our approach, we include M1 compute in our calculations of total inference cost (FLOPs/byte)."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Compression",
            "text": "In this work, we focus on lossless compression, which aims to encode a sequence of input symbols, , into a bitstream while minimizing the expected length of the bitstream. Compression methods are often factored into a \u201cmodeling\u201d component and a \u201ccoding\u201d component [45  ###reference_b45###]. The input sequence can be viewed as a sample from a true distribution , , with a standard autoregressive decomposition, . The \u201cmodeling\u201d component aims to approximate  with . While some compression algorithms assume static probabilities for each symbol, stronger algorithms are \u201cadaptive\u201d, meaning that symbol probabilities may change based on context. In this work, we use context-aware transformer-based language models to represent .\nThe \u201ccoding\u201d component of a compression algorithm converts the input sequence to a bitstream of length . To maximize compression, we want a coding algorithm that minimizes the expected number of bits in the bitstream, . This is done by assigning shorter bit sequences to common symbols and longer sequences to less common ones.999This process can result in extremely uncommon sequences becoming longer under compression, as no algorithm can compress all possible input strings [45  ###reference_b45###]. In practice, natural language inputs are highly compressible and these edge cases are inputs that one would not recognize as natural language. The expected length is lower bounded by  where  [57  ###reference_b57###]. This means that, given a near-optimal coding algorithm, the achievable level of compression derives from how well the model  approximates ."
        },
        {
            "section_id": "2.4",
            "parent_section_id": "2",
            "section_name": "Arithmetic Coding",
            "text": "Arithmetic Coding [53  ###reference_b53###, 49  ###reference_b49###] uses a model  to compresses a sequence  to a bitstream, which is the binary expansion of a float . The float  is found by assigning successively smaller sub-intervals to each symbol , with the final interval enclosing . An interval is made of an upper and lower bound,  and its size is given by . Starting with , at each step of encoding, the interval for the symbol  is created by partitioning the interval  based on the cumulative distribution of  given the previous context, . The size of this interval is given by . Thus:\nwhere  is the symbol before  in a strict ordering of , i.e.,  is the previous token in the vocabulary. Finally, the bitstream of minimal length that represents the binary expansion of a number inside the final interval  is used as the compressed representation.\nEquivalently, the binary expansion can be seen as maintaining a bitstream prefix  and creating successive intervals  by partitioning the current interval in half. If the first interval is chosen, a  bit is appended to the bitstream prefix , while choosing the second interval appends a .\nOnce the final interval  is computed, smaller and smaller bit intervals are created until reaching a bit interval  that is fully enclosed by . At this point, the corresponding bitstream  is the final compressed representation.\nThe coding component of Arithmetic Coding is nearly optimal: the output bitstream will have a length of  bits when using infinite precision. In the finite precision setting using  bits, an extra  bits are added [31  ###reference_b31###]. See [73  ###reference_b73###] for an example implementation. In our experiments, we use precision . The practical effect of using a finite precision implementation of Arithmetic Coding is that the model\u2019s cumulative distribution gets quantized to integers using  bits. This results in a minimum probability of  being assigned to all tokens."
        },
        {
            "section_id": "2.5",
            "parent_section_id": "2",
            "section_name": "Related Work",
            "text": "Recent work has looked at using large language models for compression and has attempted to train subsequent models over the resulting compressed output. Works like [16  ###reference_b16###] use a transformer language model as the modeling component of Arithmetic Coding, but they do not train over compressed output nor do they make modifications to the compression algorithm to facilitate learnability by downstream models. Additionally, they focus on the setting of compressing fixed-size sequences of bytes. By contrast, our models operate over input sequences of fixed token length. This allows for models with higher compression rates to leverage longer contexts, as more bytes are included in the input."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methods",
            "text": "For each experiment, we compress long contiguous sequences of training data using different methods. For several, we use M1\u2014a byte-level language model\u2014as  in the compression algorithm. We then chunk the compressed output into tokens and train M2 models over those tokens.\nThese baselines train directly over UTF-8 bytes, using the byte tokenizer from ByT5 [74  ###reference_b74###]. The models see  billion bytes total (see Table 13  ###reference_###).\nThese baselines train on text tokenized with the SentencePiece vocabulary of  tokens from T5 [52  ###reference_b52###]. The models see  billion bytes total (see Table 13  ###reference_###)."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Training Data",
            "text": "All training data used is English web text from C4 (en 3.1.0) [52  ###reference_b52###]. After tokenization, each document in C4 has an <EOS> token appended to it. We concatenate  documents together to generate a long sequence of text. Using UTF-8 byte-level tokenization, the average document length is  bytes, thus these long sequences have an average length of  bytes. Despite the document breaks, we consider these long sequences \u201ccontinguous\u201d for the training of language models. These sequences are then split into individual examples, which are shuffled using the deterministic dataset functionality from SeqIO [54  ###reference_b54###]."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Training M1",
            "text": "The model used for compression is a decoder-only Transformer model [67  ###reference_b67###]. It uses the m size seen in Table 4  ###reference_### and a context length of . We use a batch size of , an rsqrt decay learning rate schedule () starting at  with  warmup steps, and a z-loss of . The model is trained for  steps using the Adafactor [59  ###reference_b59###] optimizer. The feed-forward layers use ReLU activations [47  ###reference_b47###, 22  ###reference_b22###], and we use distinct learnable relative attention embeddings [58  ###reference_b58###] at each layer. We use a deterministic SeqIO dataset and train using Jax [7  ###reference_b7###], Flax [28  ###reference_b28###], and T5X [54  ###reference_b54###]. The final validation performance of the M1 model is  bits/byte, a standard measure of perplexity, see Section 3.8  ###reference_###. M1 and M2 are both trained on the C4 training data, but the final validation data used to evaluate M2 is unseen during M1 training, therefore there is no information leakage. This is similar to how LLM tokenizers are often trained on same dataset that the LLM is subsequently trained on."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Compression Methods",
            "text": "When compressing C4 training data, we use an example length of  bytes and apply one of the following compression techniques (see Appendix H  ###reference_### for more methods we considered). This results in compressed examples that are, on average, much longer than our target sequence length of  M2 tokens. Thus, each example fills or nearly fills the model\u2019s context window with a compressed sequence made from contiguous raw bytes. We compress  examples using each method, allowing us to train each M2 model for  steps without repeating data.\nArithmetic Coding: In this setting, we use a decoder-only transformer language model to model , that is, when creating the interval , the partitions for each possible character, , are calculated using the probabilities for the next token output by the transformer.\nThe compressor model is run over contiguous text sequences of  bytes. The generated logits are used as the model distribution for Arithmetic Coding. We use the Range Encoding (a finite-precision implementation of Arithmetic Coding) implementation from TensorFlow Compression [4  ###reference_b4###] with a precision of . The range encoding implementation uses integers with precision\u2009 bits. This is enough to encode -bit float logits, so should not cause numerical issues as our models are trained using bfloat16. While the compressor model is only trained on sequences of length , it uses relative position embeddings in its attention layers. Thus, it can be applied to longer sequences. Some works observe decreased performance as inputs are scaled to lengths beyond those seen in training [66  ###reference_b66###, 51  ###reference_b51###], but we find that compression performance is similar in the two settings. Compressing sequences of length  yields a compression ratio of  while compressing sequences of length  yields a ratio of . This suggests the performance drop from long sequences has minimal effect on compression, or that the increased contextual information makes up this difference.\nWe will see that text compressed in this straightforward manner is not readily learnable by M2. Thus, we explore alternative compression methods that modify the \u201cmodeling\u201d and \u201ccoding\u201d components for better learnability. Table 2  ###reference_### shows how our different approaches affect the compression ratio.\nStatic Logits Arithmetic Coding: One potential difficulty of learning over compressed text is that the \u201cmodeling\u201d component of the compression algorithm is hard to learn\u2014that is, the second language model (M2) has trouble learning to simulate the probabilities the compressor model (M1) assigns to bytes.\nTo weaken the compressor model, we replace the context-sensitive LM model with a static byte unigram model\u2014that is, the model\u2019s distribution is the same for all byte tokens in the input, i.e., . This distribution is estimated using the byte unigram statistics from the C4 training data.\nEqual Information Windows: The difficulty in modeling compressed text could also be because the \u201ccoding\u201d component of the compression algorithm is hard to learn. That is, the language model is not able to track the state variables used in Arithmetic Coding.\n###figure_2### Our proposed method of weakening the coding component of Arithmetic Coding compression is to reset the AC encoder once it has output a set number of bits, creating windows of fixed size where each window is an independently AC-compressed sequence. This process is illustrated in Fig. 2  ###reference_###. Windows will represent a variable amount of text, but as each window is created via compression, we expect roughly the same amount of information per window.\nIn addition to resetting the AC encoder, we also reset the M1 model\u2019s context. This means that each  bits of output can be decoded independently, at the cost of a weaker M1 model due to the lack of context. As each window is fully self-contained, the model no longer has to learn to track Arithmetic Coding state variables over long distances.\nIn cases where \u201cspare bits\u201d are available at the end of a window (but not enough to add an additional symbol of text), we pad with zeros. This complicates the decoding algorithm, but the compression scheme remains lossless. See Appendix I  ###reference_### for further discussion and an alternative padding approach that gives similar results.\nWhen compressing an additional character would result in a bitstream that is greater than  bits long, i.e., more than  binary expansions are needed to create an interval that is enclosed by , the bitstream (padded to  bits as necessary) representing the input up to and including character  is emitted. Then both the AC encoder and M1 model are reset. That is,  is calculated as if ; the bit interval is also reset to . Similarly, M1 is only conditioned on inputs that are part of the current window, the inputs after . That is, .\nWe use  to denote the bits per window, and  for the vocabulary size of M2. For example, EqualInfoAC represents AC encoding with 16-bit Equal Info Windows and 8-bit M2 tokens (vocabulary ).\nGZip: As a baseline, we also explore training over text compressed using GZip [17  ###reference_b17###] as implemented in the Python [65  ###reference_b65###] zlib library using the default compression level. GZip uses the DEFLATE algorithm\u2014a combination of Huffman Trees [32  ###reference_b32###] and LZ77 [77  ###reference_b77###]. First LZ77 is used to replace repeated substrings in the text with pointers back to the original substring. Then a Huffman Tree is built for the current\u2014LZ77 compressed\u2014example and used to compress it. Note that this setting is dynamic, as the Huffman tree, and hence the binary codes for each character, are unique to the example. These experiments explore a setting where both the modeling and coding components of compression are different from Arithmetic Coding."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Tokenization of Compressed Text",
            "text": "Most compression methods output a bitstream, but training M2 directly over bits would not be ideal. As M1 was trained over UTF-8 bytes, the bit-level output of compression would result in M2 being applied to much longer sequences. Additionally, models are generally trained with vocabulary sizes much larger than two. Thus, we need a method to segment the bitstream into tokens, creating a more standard sequence for training language models.\nWe convert the bitstream into a token sequence by grouping every  bits into a token\u2014resulting in a vocabulary size of . We explore settings of \u2009{, }, resulting in vocabulary sizes of  and . As the tokens are created from the compressed bitstream, we expect the distribution of tokens to be more uniform than the usual Zipfian [76  ###reference_b76###] distribution of word or subword tokens, allowing us to use larger vocabularies without encountering issues of rare or unattested tokens.\nThroughout this work, we focus on the \u201ctoken compression ratio\u201d \u2014the ratio between the input and output token sequence lengths. It is important to note that the meaning of \u201ctoken\u201d can differ between the input and output sequences. Generally, the input sequence is one byte per token, while output tokens represent multiple bytes. This is in contrast to the more standard \u201cbit compression ratio\u201d \u2014the ratio of input bits to output bits. As we aim to reduce the computational overhead of running LLMs by training them on compressed input, we are more concerned with reducing the number of tokens that M2 consumes. This difference is elucidated in Table 1  ###reference_###. While SentencePiece results in a sequence length reduction of , the larger vocabulary means that  bits are required to represent each token. As such, the bit compression ratio is only , which is much lower than our AC-based compressors. Similarly, creating -bit tokens from the output of Arithmetic Coding does not change the bit compression ratio\u2014the total number of bits is unchanged\u2014but it does reduce the number of tokens in the sequence, and thus the number of tokens the LLM must process. We compute compression ratios over the C4 dev set, which is unseen during M1 training.\nTo highlight the differences between the tokenization methods above, we measure the performance (as bits/byte on a sample of the C4 validation set) of two trivial models for each tokenizer in Table 3  ###reference_###. The \u201cuniform\u201d model na\u00efvely assigns equal probability to each token, regardless of context. The \u201cunigram\u201d model also ignores context, but assigns probabilities based on the global token frequencies observed in the training data. With byte-level tokenization, each UTF-8 byte encodes to a single -bit token, so the uniform model achieves  bits/byte. For more powerful tokenizers, the uniform model is stronger, indicating that the tokenizer itself has some language modeling ability. We observe that our compression-based tokenizers (AC, EqualInfoAC and GZip) output a near-uniform distribution of tokens across their vocabulary. This is reflected in the near-zero gain over \u201cuniform\u201d achieved by modeling unigram statistics."
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "Training M2 on Compressed Data",
            "text": "Each M2 model is trained for  steps with a batch size of  and a sequence length of .\nThus each model trains on  billion tokens. Of these, the vast majority (over %) are non-padding tokens; see Appendix C  ###reference_### for details and Table 13  ###reference_### for the exact size of each dataset. As methods with higher compression ratios cover more raw text per token, we also include the total number of bytes in each dataset. Shuffling of training sets is seeded, and dataset state is checkpointed during training, so each training run results in the model seeing each example exactly once.\nModels are trained at four sizes, as shown in Table 4  ###reference_###, with m, m, m, and b parameters, excluding embedding parameters. When the compressed bitstream is chunked into 8-bit tokens, the M2 model has a vocabulary size of . With 16-bit tokens the vocabulary increases to . All M2 models have a sequence length of  tokens. Thus, when training on 16-bit tokens, twice as many bytes are seen per example and in training overall, as compared to 8-bit tokens. All other hyperparameters match those used in M1."
        },
        {
            "section_id": "3.6",
            "parent_section_id": "3",
            "section_name": "Baselines",
            "text": "We compare our M2 models against baseline models trained with two standard tokenization methods, described below. All hyperparameters, including sequence length (), match those used for our M2 training above.\nThese baselines train directly over UTF-8 bytes, using the byte tokenizer from ByT5 [74  ###reference_b74###  ###reference_b74###]. The models see  billion bytes total (see Table 13  ###reference_###  ###reference_###).\nThese baselines train on text tokenized with the SentencePiece vocabulary of  tokens from T5 [52  ###reference_b52###  ###reference_b52###]. The models see  billion bytes total (see Table 13  ###reference_###  ###reference_###)."
        },
        {
            "section_id": "3.7",
            "parent_section_id": "3",
            "section_name": "Numerical Stability",
            "text": "Arithmetic Coding depends on the creation of \u201cintervals\u201d that cover each symbol in the vocabulary based on the quantized cumulative distribution of a model\u2019s logits when predicting the next token. As such, a small change in the logits due to numerical noise can result in vastly different output bitstreams. This can make the practical use of neural language models in compression difficult. Common sources of noise include changes in batch size, parallel computation, changes to compute infrastructure (CPU vs. GPU vs. TPU, different TPU topology, etc.), changes to inference (computing the logits for the whole sequence at once vs. computing logits for a single token at a time using KV caches), and changes to the longest sequence length in the batch.\nMethods like the rank-sorted algorithm used in LLMZip [63  ###reference_b63###] may help alleviate these issues as only the order of tokens needs to match between settings. The development of alternate methods of LLM-based compression should keep numerical stability issues in mind and ideally alleviate these issues in the design of the algorithm. Increasing the level of quantization could also help reduce numerical noise issues, as differences would mostly be lost in quantization, but this would have a negative impact on the compression ratio."
        },
        {
            "section_id": "3.8",
            "parent_section_id": "3",
            "section_name": "Evaluation",
            "text": "As the tokenization scheme varies across the approaches we consider, models cannot be directly compared on \u201cper-token\u201d metrics such as negative log likelihood loss . Rather, following previous work [14  ###reference_b14###, 2  ###reference_b2###, 10  ###reference_b10###, 24  ###reference_b24###, et alia], we report perplexity in terms of \u201cbits-per-byte\u201d, , which scales the model\u2019s loss by the token-level compression rate.\nWe also compare models on how much computation (FLOPs) is required to perform inference over a given length of raw text (bytes). More specifically, we calculate M2\u2019s expected FLOPs/byte by scaling FLOPs/token\u2014approximated by  (excluding embedding parameters) following [35  ###reference_b35###]\u2014by the token-level compression rate (as tokens/byte). For methods using an M1 model during compression, the FLOPs/byte cost of M1 is added.101010While there is a computational cost to running GZip over the input text, we ignore it as it is insubstantial compared to the cost of running M2 model inference. For more details on the evaluation metrics see Appendix G  ###reference_###.\nWe evaluate models on a sample of the C4 validation set. During evaluation, the model is run over  batches or ~ million tokens. These tokens represent different amounts of text based on the compression method, making it impractical to run evaluation on the same sequence of bytes for all methods. To confirm that our validation samples are large enough to be representative, for each method, we train five m parameter models with different seeds. We find the final performance to be extremely stable, with the largest standard deviation in bits/byte being . Thus, the variance introduced from sampling the validation set is negligible. See Appendix B  ###reference_### for more information about variance."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "###figure_3### As seen in Fig. 3  ###reference_###, the most obvious approach\u2014compression using Arithmetic Coding with M1 assigning next-token probabilities\u2014fails to learn anything. Regardless of scale, the model only learns to output a uniform distribution over tokens, the performance of which is denoted by the dashed line. As the Arithmetic Coding procedure is near optimal [45  ###reference_b45###], the compression ratio is essentially determined by the loss of M1. Thus, even though the M2 model learns nothing useful, when scaled by the compression rate, this setting ends up with the same performance as the M1 model. Similarly, models trained over data compressed with StaticAC\u2014where M1 is replaced with a static unigram model\u2014fail to learn. This result suggests that the difficultly in learning stems from the complexity or brittleness of the Arithmetic Coding process itself, rather than from M2\u2019s inability to model M1. Note that the weak \u201cmodeling\u201d component of this compression scheme results in a much lower compression rate and thus worse bits/byte performance, despite the model also learning a uniform distribution.\nOur SentencePiece baseline outperforms all other methods, including our Bytes baseline, across all model sizes. On the surface, this result seems to run counter to the recent findings of [16  ###reference_b16###], where their byte-level models outperformed subword (BPE) models at medium and large scales. The discrepancy is due to prioritizing different metrics. They report the model\u2019s bit compression rate on fixed-length ( byte) sequences. While this is one type of \u201cfair\u201d comparison, it disadvantages subword models, as they are trained to model dependencies longer than  bytes (but never evaluated on this ability), and are allotted fewer inference FLOPs to process the same text, as compared to the byte-level models. Additionally, bit compression ratio penalizes subword models for having larger vocabulary sizes. By contrast, our evaluation tests what perplexity models achieve on sequences of the same length they were trained on, and compares models at matching FLOPs/byte cost. This aligns with our end goal, which is to train an LLM that achieves the best perplexity at whatever sequence length it can handle, given a fixed budget for training and inference.\n###figure_4### ###figure_5### Fig. 3  ###reference_### shows that EqualInfoAC outperforms the byte-level baseline at most model sizes, with the gains increasing with scale. In addition to better bits/byte performance, training over compressed data has the advantage of using fewer FLOPs/byte for a given model size\u2014seen in the leftward shift of the EqualInfoAC curve compared to the Bytes curve\u2014due to shorter sequence lengths.\nUsing -bit tokens (k vocabulary) increases performance further. EqualInfoAC outperforms the Bytes baseline at all model sizes. It underperforms the SentencePiece baseline, but the gap diminishes with scale.\nHowever, EqualInfoAC outperforms the SentencePiece baseline in terms of tokens/byte. Models using EqualInfoAC take fewer autoregressive steps to generate the same text than models using SentencePiece encoding.\nThis has the potential to reduce generation latency, at the cost of reduced compute efficiency.\nThis is a tradeoff that is often worth making in production. For instance, speculative decoding [42  ###reference_b42###] is a popular approach that performs redundant computation in order to potentially accelerate auto-regressive steps.\nIt is noteworthy that the EqualInfoAC M2 models learn well despite being trained on data that has nearly uniform unigram statistics, as we saw in Table 3  ###reference_###. In the best case, our  billion parameter M2 model achieves  bits/byte. This is a large gain over the na\u00efve uniform ( bits/byte) and empirical unigram ( bits/byte) models from Table 3  ###reference_###, and approaches the performance of a parameter-matched SentencePiece model ( bits/byte), despite using % fewer FLOPs/byte.\nIt is apparent from Fig. 3  ###reference_### that if FLOPs/byte were held constant, SentencePiece would achieve slightly better bits/byte than EqualInfoAC. However there is another axis along which EqualInfoAC may still be preferred. Setting aside inference FLOPs, all our SentencePiece models require % longer sequences to encode the same text when compared to our best EqualInfoAC setting (, k). This means that regardless of FLOPs used, the SentencePiece models will take more decoder steps at inference time. It is up to the practitioner whether it is \u201cworth it\u201d to trade off some bits/byte performance in order to achieve shorter sequences. In many serving scenarios, decoder steps are a practical bottleneck for determining system latency, and there are cases where one may be willing to incur extra inference cost to reduce latency (e.g., speculative decoding [43  ###reference_b43###]). To this end, it may be advantageous to scale up an EqualInfoAC model to recover bits/byte performance while retaining the reduced latency. This can be seen visually in Fig. 4  ###reference_###.\nTraining over GZip-compressed text is relatively ineffective. M2\u2019s performance when trained over GZip highlights a counter-intuitive trend. While the GZip M2 models actually learn, it would still be preferable to train over AC-compressed text\u2014even though those models do not learn. This is due to the weak compression offered by GZip. The poor compression rate, coupled with weak learning, means that the GZip M2 models\u2019 bits/byte performance lags behind even the m parameter M1 model.\n###figure_6### ###figure_7### We see a similar effect in Fig. 5  ###reference_###, which ablates the EqualInfoAC window size. In terms of bits/byte, the shortest -bit windows perform the best. However, the next-best setting is the longest -bit windows, despite the fact that these M2 models fail to learn almost anything beyond the uniform distribution. This unintuitive trend stems from the fact that longer windows translate to better compression rates (see Table 2  ###reference_###). If we remove the effect of compression rate by looking at bits-per-token (Fig. 5(b)  ###reference_sf2###), we see a clearer monotonic trend\u2014increasing window length makes it harder to learn, as we move closer to simply running Arithmetic Coding over the whole sequence. For  and -bit windows, performance improvements with scale are small, but present; see Table 10  ###reference_### for exact numbers.\n###figure_8### Tokenizing compressed text using a larger -bit vocabulary (k) results in a  higher token compression rate, seen in the leftward shift of each curve in Fig. 6  ###reference_###.111111The same trend holds for larger  and -bit windows, but the performance increase with scale is so slight that we omit them from the graph. See Table 10  ###reference_### for the exact values. For Arithmetic Coding methods, larger vocabulary also improves bits/byte, seen as a downward shift in the curves. However, for GZip, we see the opposite trend. Arithmetic Coding and GZip differ the most in their coding component, which suggests that the reason for this difference could lie there. Note that the header and footer present in GZip-compressed data do not explain this difference, see Appendix E  ###reference_###. For EqualInfoAC, moving from  to k results in each window corresponding to a single token, which increases the \u201cstability\u201d of the token\u2009\u2009text mapping. This could be one reason for the performance gain; see Section 6.1  ###reference_### for more discussion of \u201cstability\u201d.\nGiven the recent findings of [55  ###reference_b55###], we anticipate that continuing to scale models beyond  billion parameters is unlikely to deliver an \u201cemergent\u201d ability to learn over AC-compressed text, since the bits/byte metric we use is smooth.\nWhen scaling models, [30  ###reference_b30###] recommend that training tokens should be scaled linearly with model size. However, in our experiments above, all models see the same number of tokens, regardless of model size. Consequently, our largest models may be somewhat \u201cundertrained\u201d.121212The undertraining of our b models is also visible in their validation loss curves, which still have a significant decreasing slope at  steps, showing the models have not yet converged. To test whether following the \u201cscaling laws\u201d recommendation influences our results, we reevaluate our models at earlier checkpoints selected to maintain a constant ratio of training data to model size. We find that all core trends are unchanged in this setting. See Appendix D  ###reference_### for details."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Additional Experiments",
            "text": "At this point, we have established that while the simplest approaches to training over compressed text fail, there are alternate compression schemes that are learnable. In this section, we conduct additional experiments to shed light on which aspects of different compression methods are difficult to learn and what contributes to their learnability."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Bitstream tokenization is not the main source of difficulty",
            "text": "The compression algorithms we consider output a bitstream, which we later chunk into tokens of a fixed bit depth (e.g., -bit tokens). As such, it is common for the bits representing a single character or UTF-8 byte to be split across multiple tokens. Compounding this issue is that the value of these tokens are contextually determined and may differ depending on the surrounding bytes.\nThe fact that both -bit and -bit token chunking strategies work suggests that this is not too much of an issue for the model. To further investigate this, we train two models\u2014one m and one m\u2014on the raw bitstream output by Arithmetic Compression, i.e., each token is either a  or a  and the vocabulary has a size of . We use the same hyperparameters as in Section 3  ###reference_###. Working at the bit level means that the output sequence is now longer than the input sequence, which was UTF-8 bytes. As such, this setting is not practical in the real world.\nWhen trained to convergence, the two models have cross entropy losses of  for the m parameter model and  for the m model\u2014not meaningfully better than the na\u00efve uniform distribution, which yields a loss of . This failure mode is the same as in Fig. 3  ###reference_###, which suggests that AC encoding itself is the main source of difficulty, as opposed to any issue around tokenization or vocabulary size."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Transformers struggle to learn Arithmetic Coding",
            "text": "Arithmetic Coding is a sequential algorithm that involves tracking multiple state variables as the input (byte) sequence is consumed. Each token in the output sequence represents multiple transformations of these variables, e.g.,  transformations when using -bit token chunking. Theoretically, only  transformer layers are needed to have a computational path through the model layers that can process a sequence of  tokens as a chain, where each token conditions on the previous one. While most of our transformers have the capacity to model these sequences\u2014only our m model has fewer layers\u2014we see in practice that the Arithmetic Coding algorithm is still difficult to learn.\nTo directly diagnose the ability to track Arithmetic Coding, we format AC compression and decompression as sequence-to-sequence tasks. The input provides the model with the true text, so we expect a model that is able to learn Arithmetic Coding should achieve an accuracy of . We compress sequences of  bytes using M1 and Arithmetic Coding.131313We use shorter raw text sequences to keep the final sequence length of inputs + targets manageable. We concatenate the bytes and AC output tokens to create the compression task. For the decompression task, we simply flip the order\u2014AC output tokens first and then bytes. The target tokens (bytes or tokens) are shifted by the input vocabulary size, ensuring that they have distinct values. We use a decoder-only transformer as our model with a causal attention mask, i.e., even during the input sequence, future tokens are hidden from the model. We train models with m parameters. Loss, gradients, and evaluation metrics are only computed on the target tokens.\nIn the decompression task, the target tokens are bytes. By ignoring the inputs and just modeling the outputs, the decompression model can achieve decent performance without actually leveraging the input data. To control for this, we also train a byte-level language model baseline on the same sequence-to-sequence data, excluding the input tokens. If the decompression model is actually learning to decompress Arithmetic Coding, we would expect stronger performance than the byte-level baseline. As we see in Table 5  ###reference_###, the baseline model, which does not see the input tokens, has the same performance as the decompression model.141414The slight gain is statistically insignificant, . Clearly, the models trained for decompression are not actually learning to do decompression.\nThe model trained for compression actually shows some signs of learning. Training a language model directly on the compressed output results in the model learning a uniform distribution over tokens, see Fig. 3  ###reference_###. When the model is able to attend to the input text, we see that the performance in Table 5  ###reference_### is better than the uniform distribution (which would have a cross entropy loss of ). While this method shows some hope for the learnability of Arithmetic Coding, the need to include the input sequence negates the main advantage of compression, i.e., applying the model to a shorter sequence. Additionally, the compressor\u2019s performance is far from the  it should be able to achieve.\nWe also find training on these sequence-to-sequence datasets to be less stable than training on the language modeling datasets. In our experiments, large performance swings and divergence were relatively common."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Larger vocabulary helps beyond increasing the compression ratio",
            "text": "Our best results training over compressed text use EqualInfoAC with -bit windows and vocabulary size at either k (best) or  (second-best). One clear advantage of the  model is that it has a  better token compression rate, so sees twice as much raw text during training. To assess whether its performance gain is due entirely to this advantage, we train a m parameter M2 model over the same dataset, but reduce its sequence length from \u2009\u2009. This model trains on half as many tokens, but sees the same amount of underlying text as the  model.151515To compensate for the smaller number of tokens in a sample of  batches from validation set when each example is  tokens, we compute our evaluation metrics over  batches.\nTable 6  ###reference_### shows that even in this setting, the model with larger vocabulary is stronger.161616It may be possible to achieve further gains by increasing the token bit depth further. However, most deep learning frameworks do not support using unsigned data types for inputs, and the resulting large vocabulary size can cause a computational bottleneck in the final softmax layer. In fact, most of the bits/byte gain (% absolute) is due to the structural change in tokenization, as opposed to the additional text seen. One possible explanation for its strong performance is that the  model uses exactly one token to represent each equal-info window. We\u2019ll see in the next section that in EqualInfoAC settings with multiple tokens per window, any non-initial tokens are highly context-dependent, and learning proceeds on a curriculum from the \u201ceasy\u201d window-initial tokens to the \u201charder\u201d window-final tokens."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Analysis",
            "text": "In this section we examine how neural compression based tokenizers differ from standard tokenizers, and conduct additional analysis on training dynamics and learnability of compressed data. This analysis leads us to several recommendations for future work developing new compression schemes that aim to be learnable by transformer models while delivering stronger compression than subword tokenizers."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "EqualInfoAC is less stable and less semantic than SentencePiece",
            "text": "While the performance of our EqualInfoAC model approaches that of our SentencePiece baseline, qualitative analysis shows that the two tokenization schemes differ in many regards. Table 7  ###reference_### illustrates some of these differences.\nFirst, we observe that SentencePiece produces a relatively stable text\u2009\u2009token mapping.171717See Appendix L  ###reference_### for some corner cases where this is not the case. For example, \u201celephants\u201d appears three times in the sentence, and maps stably to the same two-token sequence in all cases: [ elephant]\u2009[s]. Similarly, both occurrences of \u201cAfrican\u201d map to the same token: [ African]. By comparison, the EqualInfoAC tokenization is relatively unstable, with each occurrence of these words being segmented in a different way and yielding a different token sequence.\nSecond, we find that the SentencePiece tokenization is more \u201csemantic\u201d, by which we mean that the segmentation it induces aligns better with meaningful linguistic units\u2014words and morphemes. While there are some exceptions, e.g. \u201csavanna\u201d being parsed as [s]\u2009[a]\u2009[v]\u2009[anna], the more common case is whole words being parsed as single tokens (e.g., currently), or into meaningful morphemes (e.g., elephant-s). By comparison, EqualInfoAC tokenization appears to almost entirely disregard word and morpheme boundaries. As one example, we see \u201cAsian elephants.\u201d parsed as [Asi]\u2009[an e]\u2009[lep]\u2009[hant]\u2009[s.].\nDespite these differences, there is an important similarity between SentencePiece and EqualInfoAC: they are both stable in the token\u2009\u2009text direction. That is, a given token ID, e.g., token #, will always map to the same output text. This \u201ctransparent decoding\u201d property likely makes it easier for a downstream model to learn over these tokens.181818Padding to reach a specific window size can require extra computation to discern between padding and characters that compress to all zeros, however we find in Appendix I  ###reference_### that it is not an issue for M2 models.\nWhen we move to versions of EqualInfoAC that contain multiple tokens per window, such as EqualInfoAC, this transparency is destroyed for all non-initial tokens within a window. This is illustrated in Table 8  ###reference_###. When the same token appears window-initially in different contexts, we see the window text has a stable prefix\u2014e.g., token # always maps to the prefix \u201cle-\u201d. However, when occurring as the second token within a two-token window, there are no apparent correspondences between window text.191919A repeated text substring that happens to be aligned with a window multiple times is one of the few cases where the second token will represent the same text. As EqualInfoAC window length increases, the proportion of tokens that are stable decreases. This may explain the observed difficulty of learning over longer windows. The window text for all instances of these tokens can be seen in Appendix M  ###reference_###.\n###figure_9### Note that Table 8  ###reference_### examines window\u2009\u2009text, as opposed to token\u2009\u2009text correspondences. This is because for multi-token windows, the mapping from tokens to text is not well defined. More specifically, each character maps to a particular subsequence of the compressed bitstream, but these may not align with token boundaries.202020This can be a source of instability, even in window-initial tokens, see Appendix L  ###reference_###. Fig. 7  ###reference_### illustrates the mapping between characters, bits, and tokens. We find that many windows contain a character (shown in purple) whose bits are split across two -bit tokens.\nFig. 7  ###reference_### also highlights that window-initial characters are not being well compressed, with the window-initial token often only covering one or two characters. This is due to our EqualInfoAC procedure fully resetting M1\u2019s context at every window boundary. With no context, M1 cannot make confident predictions, leading to more bits being needed to represent the initial character. On the positive side, this setup guarantees that a window can be decoded in isolation, which should aid learning. However it is worth exploring in future work whether maintaining some M1 context across windows could improve the compression ratio without hurting learnability."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "AC decoding is learned step-by-step",
            "text": "###figure_10### ###figure_11### As Arithmetic Coding is a sequential (left-to-right) and contextual algorithm, the text represented by a given token will differ based on the previous token. As such, a model should perform better on a token if it has a strong understanding of the token before it. When using EqualInfoAC compression, each window represents an independent Arithmetic Coding document. As we move deeper into the window, more and more AC decompression must be done to understand the token.\nTo understand how a token\u2019s position within a window affects learning, we track across training the average accuracy at each position within the -token windows of a m parameter EqualInfoAC model.212121The absolute accuracy of the EqualInfoAC model is relatively poor, but its relatively long window provides the clearest illustration of the positional trends. We observe similar trends for shorter windows where the model has stronger performance. Fig. 8  ###reference_### shows both raw accuracy (left) as well as the increase over \u201ctrivial\u201d accuracy (right), which we define as the maximum accuracy achieved in the first  steps of training. Looking at accuracy increase highlights the \u201csequential learning\u201d trend by discounting any part of accuracy that is text independent. In particular, we note that window-final tokens have a non-uniform distribution due to the use of window-final padding bits (see our EqualInfoAC formulation in Section 3.3  ###reference_###), which can be learned without any understanding of the text.\nWe observe two interesting trends. First, there is a clear ordering as to when the model starts to make meaningful (non-trivial) progress on a given position. The initial token (#1) is learned first, followed fairly quickly by #2 and then #3. Later tokens are only \u201cunlocked\u201d after  training steps, suggesting that the ability to model these tokens builds on a foundation of understanding the preceding tokens within the window.\nThe second trend concerns the accuracy reached at each position. Here, we observe an increase in accuracy from #1 < #2 < #3, followed by a decrease from #3 < #4 < #5 and so on.222222The final token #8 also fits this trend when looking at the increase over non-trivial accuracy. The raw accuracy in this position is higher than previous tokens #4\u20137, due to the skewed distribution introduced by window-final padding. We interpret the increase across the first three positions as due to the benefit of extra leftward context. This is akin to the initial byte in a word being harder to predict than the following bytes. The decreasing performance at tokens #4 and beyond suggests the model is unable to track AC decompression indefinitely. While the model clearly learns to decompress longer sequences as training progresses, reliably decoding past  bits of AC output appears to be a challenge."
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "Learnable distributions are less uniform",
            "text": "###figure_12### ###figure_13### A well-known result in the compression literature is that there can be no recursive compression [45  ###reference_b45###]. The compression algorithm removes information captured by its model, resulting in a uniform output that appears random to the original model. However, our setting is not recursive compression. Instead, a separate and larger model is trained on the compressed output, which should be able to capture new patterns in the bitstream.\nDespite this, the output of compression using M1 appears very uniform, as evidenced by the minimal gains from modeling the unigram token distribution in Table 3  ###reference_###. Therefore, it seems reasonable that this uniformity could make it hard for M2 to learn (as all patterns must be contextual). We investigate this by plotting the KL divergence [39  ###reference_b39###] between the observed empirical distribution and a uniform distribution for different segmentations of the bitstream. If the underlying distribution of bits was truly random and independent, then the distribution of unigrams for some bitstream segmentation should remain uniform as  and therefore the KL divergence should remain close to zero. On the other hand, if the distribution diverges from uniform, there is contextual information to be learned when training an LLM to model .\nWe segment the bitstream either into bit n-grams, where successive n-grams are allowed to overlap, or into n-bit tokens, following our M2 tokenization procedure\u2014see Section 3.4  ###reference_###. We only plot tokenization into -bits that are factors of , otherwise tokens would cross window boundaries in the EqualInfoAC setting.\nAs a baseline, we used the cryptographic secrets package in Python to generate bitstreams that should be truly random and independent. As such, the KL divergence should remain at  when segmented in the same way as the compressed data. The reason this does not hold in Fig. 9  ###reference_### is that the maximum likelihood estimate of entropy, , is negatively biased [48  ###reference_b48###]. In Fig. 13  ###reference_3### we see that when using a Miller-Madow estimator [46  ###reference_b46###] to correct for this bias, the expected KL of  is well within sampling noise bounds. To account for noise in the entropy estimation, we plot th percentile intervals of the KL divergence between the observed entropy from  disjoint samples of the data and the uniform distribution.232323As the number of bits in a segmentation grow, the vocabulary size increases exponentially, requiring many more samples. Thus we expect noise in the entropy estimate to grow with . This holds, but it is obfuscated by the log scaling in Fig. 9  ###reference_###. In fact, the magnitude of the noise for settings such as GZip and EqualInfoAC is larger than for AC or RNG. This noise behavior is seen in Fig. 12  ###reference_2###. See Appendix J  ###reference_### for more information on entropy estimation and bias correction.\nThe AC and RNG lines in Fig. 9  ###reference_### are very similar and their sampling noise intervals have large overlaps. This suggests that the data generated by AC compression with M1 is difficult to distinguish from random data.242424For , the AC entropy is statistically significantly less than the RNG entropy, however, differences in the mean entropy only start to appear after ~ decimal places. This is a possible explanation for why M2 models trained on AC data only learn to output a uniform distribution, as seen in Fig. 3  ###reference_###.\nIn Fig. 9  ###reference_###, we see that GZip is the least uniform, which is expected as it has the worst compression rate among these settings. However, the segmentation into tokens does not result in much extra information. This is again suggestive that the differences between the \u201ccoding\u201d components of GZip and Arithmetic Coding are important for learnability. It is also a possible explanation of why GZip is the one setting where using -bit tokens does not improve performance.\nSimilarly, Fig. 9  ###reference_### shows that EqualInfoAC has the most information among the Arithmetic Coding approaches. Given that this is the most learnable setting, it suggests that non-uniformity of the bitstream may be important for learning. We also see a large increase when moving to -bit tokens, providing a further possible explanation for why larger vocabulary is helpful (see Section 5.3  ###reference_###). Finally, we note that StaticAC has less information than EqualInfoAC, suggesting that weakening the \u201ccoding\u201d component of Arithmetic Coding is a more effective way to retain information and increase learnability for M2."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We have shown there is promise in the idea of training LLMs over neural-compressed text. In the best case, this will allow training over text that is better compressed than standard subword token sequences, while maintaining learnability. This an appealing prospect, as models that read and write more text per token are more efficient to train and serve, and can model longer dependencies.\nWhile the \u201cvery simplest\u201d approach does not work (training directly over a tokenized AC-encoded bitstream), we showed that a relatively simple modification\u2014compression via Equal Info Windows\u2014already brings us within striking distance of popular tokenizers. When measured in terms of perplexity achievable at fixed inference cost (FLOPs/byte), we find that our method outperforms raw byte-level models, and comes increasingly close to the performance of SentencePiece tokenization as scale increases to  billion parameters.\nWhile bespoke compression methods have developed around different modalities (e.g., text, audio, images, video) and different applications (e.g., delta-of-delta for regular repeating timestamps [50  ###reference_b50###]), to our knowledge, no efficient compression methods have been designed specifically for use as LLM tokenizers. We are optimistic that future work will create such methods. Compared to today\u2019s subword tokenizers, we expect these methods (i) will deliver higher compression rates, (ii) will come closer to equal information per token, thus allocating compute more effectively, and (iii) will give models a more direct view of the underlying raw text, thus helping on spelling and pronunciation tasks. As a tradeoff, we expect these neural tokenizers will be somewhat less stable in their text\u2009\u2009token mapping, but perhaps not so unstable as our approach here. In particular, we think it is worth exploring methods under which a given word typically maps to a relatively small number (tens not thousands) of relatable token sequences.\nOne direction we left unexplored is the idea of passing information between the compressing model (M1) and the LLM trained over compressed text (M2). Some additional signal of M1\u2019s internal state or output may be helpful for M2 to accurately simulate M1, which is a prerequisite to flawlessly encoding and decoding M1-compressed text.\nFor hill-climbing in this space, we found it useful to iterate on the sequence-to-sequence sub-tasks of compression and decompression, which should, in theory, be learnable with high accuracy. Specifically, if future work can devise a strong (~) compressor that a transformer can be trained to accurately encode and decode, we expect that this will be an ideal candidate for tokenizing text for LLMs."
        }
    ],
    "url": "http://arxiv.org/html/2404.03626v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2",
            "2.3",
            "2.4",
            "2.5"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.4",
            "3.5",
            "3.6",
            "3.7",
            "3.8"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "5",
            "5.1",
            "5.2",
            "5.3",
            "6",
            "6.1",
            "6.2",
            "6.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "1",
            "3",
            "5"
        ]
    },
    "research_context": {
        "paper_id": "2404.03626v1",
        "paper_title": "Training LLMs over Neurally Compressed Text",
        "research_background": "### Paper's Motivation\nThe motivation behind this paper is to explore whether compressing text further than the current subword tokenization methods, like BPE or Unigram, might lead to even greater efficiencies in training large language models (LLMs). This stems from recognizing that subword tokenizers, while effective, still leave room for potential improvements in compression rates and hence LLM performance, especially as LLMs trained over subword tokens describe seeing approximately four times more text per token, enabling them to model longer-range dependencies and consume more data without increased computational cost. The authors are curious whether neural compression might squeeze out more gains by removing simpler patterns from the text, thereby requiring the LLM to learn more complex structures.\n\n### Research Problem\nThe research problem investigated in the paper is determining if LLMs can be effectively trained using neurally compressed text. Specifically, it seeks to understand whether an LLM can learn from bitstreams compressed by previous LLMs using techniques like Arithmetic Coding (AC), thus compressing text efficiently while preserving learnability. The paper examines if compressing text with smaller LLMs (to remove simpler structures) can make the resulting compressed text easier to model for larger LLMs by focusing on more complex structures. \n\n### Relevant Prior Work\nThe paper builds on several related areas of research:\n\n1. **Subword Tokenizers**: Existing tokenizers like BPE and Unigram used in libraries like SentencePiece, achieve about 4x compression over natural language text. These tokenizers balance the trade-off between hiding character-level makeup and the significant benefits derived from compression.\n\n2. **Autoregressive Models as Compressors**: It is a well-known concept that autoregressive language models can serve as lossless text compressors. The authors reference work demonstrating that LLMs, like Chinchilla 70B, can achieve 12x bit-level compression over English text.\n\n3. **Arithmetic Coding (AC)**: The technique referred to is used for achieving near-optimal compression rates by assigning probabilities to text continuations. The paper explores using AC for pretraining corpus compression to potentially train a more advanced LLM on these compressed sequences.\n\n4. **Challenges with Neural Compression**: The text notes that initial attempts at training LLMs on AC-compressed text fail, a result attributed to the difficulty in learning the AC compression and decompression process itself.\n\n5. **Alternative Compression Techniques**: The paper proposes \"Equal-Info Windows,\" inspired by GZip-like methods, to divide the text into learnable chunks, resetting compression algorithms and model contexts at these boundaries.\n\n6. **Previous Comparisons**: The paper situates its exploration within the context of comparing subword tokenizers' effectiveness, noting that subword tokenizers tend to produce stable mappings between text and tokens which is advantageous for LLM training.\n\nThe synthesis of these areas aims to push the boundary of what's possible with LLM training, potentially leading to more efficient and scalable models by using neural compression methods.",
        "methodology": "**Methodology:** The proposed method involves using a two-stage process to train Language Learning Models (LLMs) on neurally compressed text. Here are the key components and steps of the proposed method: 1. **Compression Process:** - **M1 Model for Compression:** Long contiguous sequences of the training data are compressed using M1, which is a byte-level language model. - **Output Chunking:** The compressed output from the M1 model is then chunked into smaller units called tokens. 2. **Training with Compressed Data:** - **Training Models (M2):** These tokens generated from the compressed output are used to train different M2 models. - **Baselines and Tokenization:** - **Baseline Training on UTF-8:** For certain baselines, the models directly train over UTF-8 bytes. The byte tokenizer used for these baselines is taken from ByT5. - **SentencePiece Tokenization:** Other baselines involve training on text that is tokenized using a SentencePiece vocabulary. The SentencePiece vocabulary consists of a specified number of tokens from T5. 3. **Dataset and Metrics:** - The models, in both baselines, process a total of billion bytes. **Innovations:** - **Neural Compression:** Using a byte-level language model (M1) for the initial compression of training data sequences. - **Model Training Over Compressed Tokens:** Introducing a novel training strategy where models (M2) are trained on tokens derived from the compressed output, potentially making training more efficient. By leveraging these innovations, the methodology aims to explore the benefits and efficiencies of training large language models on neurally compressed data.",
        "main_experiment_and_results": "In the main experiment of the study, the researchers compared various compression methods and their impact on Large Language Model (LLM) training performance. Below is the detailed breakdown of their setup and findings:\n\n### Datasets:\n- The researchers used datasets compressed using different methods: Arithmetic Coding (AC) with model M1, Static Arithmetic Coding (StaticAC), EqualInfoAC, GZip, and SentencePiece.\n\n### Baselines:\n1. **Bytes**: A baseline where raw bytes of text data are used.\n2. **SentencePiece**: A subword tokenization method.\n3. **EqualInfoAC**: A compression method that combines AC with sentence token segmentation, aiming to equalize the information each segment carries.\n\n### Evaluation Metrics:\n- **Bits per Byte**: Measures the efficiency of the text compression.\n- **Perplexity**: Assesses the performance of language models on sequences of the same length they were trained on.\n- **Tokens/Byte**: Indicates how many tokens are needed to represent the same text, impacting generation latency.\n- **FLOPs/Byte**: The number of floating point operations required per byte for training and inference.\n\n### Key Findings:\n1. **Arithmetic Coding with M1**:\n   - Models trained on data compressed with Arithmetic Coding using M1 fail to learn anything meaningful, outputting a uniform distribution over tokens.\n   \n2. **StaticAC**:\n   - Similarly, models trained on data compressed using a static unigram model (StaticAC) also fail to learn, suggesting that difficulties stem from the brittleness or complexity of the coding process.\n\n3. **EqualInfoAC**:\n   - Models using EqualInfoAC outperform the Bytes baseline at most model sizes, showing better bits/byte performance and requiring fewer autoregressive steps to generate the same text.\n   - Although EqualInfoAC underperforms compared to SentencePiece in terms of bits/byte, it excels in tokens/byte, reducing generation latency due to fewer autoregressive steps.\n\n4. **SentencePiece**:\n   - SentencePiece baseline models outperformed all other methods, including Bytes, across all model sizes in terms of bits/byte.\n   - SentencePiece tends to achieve slightly better bits/byte performance compared to EqualInfoAC if FLOPs/Byte are held constant but take more decoder steps at inference time.\n\n5. **GZip**:\n   - Training over GZip-compressed text was relatively ineffective. Despite some learning, weak compression and poor bits/byte performance mean it lags behind the AC-compressed text models.\n\n6. **Effectiveness Across Model Sizes**:\n   - The advantages of EqualInfoAC, including in terms of generation latency and tokens/byte, become more pronounced with model scale.\n   - EqualInfoAC models with larger vocabularies (e.g., 256k) yield higher token compression rates and modest performance gains in bits/byte.\n\nOverall, the study suggests that EqualInfoAC exhibits potential for improved training efficiency and reduced latency in LLMs, especially in larger scales, despite being outperformed by SentencePiece in terms of bits/byte performance. The research highlights trade-offs in efficiency and latency, which are crucial for practical deployment scenarios."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Investigate the effectiveness of the Equal-Info Windows technique for learning over neurally compressed text using Arithmetic Coding.",
            "experiment_process": "Compress long contiguous sequences of training data using various methods. For the primary experiments, use a byte-level language model (M1) in the compression algorithm. Chunk the compressed output into tokens and train the M2 models over these tokens. Conduct ablations on window size and M2 vocabulary size. The M2 models are trained with a fixed computation budget (FLOPs/byte) and evaluated based on perplexity benchmarks (bits/byte). Observations include standard LLMs trained over AC-compressed text, GZip-compressed text, and Equal-Info Windows-compressed text.",
            "result_discussion": "The Equal-Info Windows make learning of AC-compressed text possible across various settings. Remarkably, M2 models trained with this technique outperform byte-level baselines on perplexity benchmarks for fixed computational budget. However, the M2 models underperform compared to subword baselines due to relatively unstable mappings induced by neural tokenizers. Future research should focus on improving neural tokenizers to maintain high compression and learnability.",
            "ablation_id": "2404.03626v1.No1"
        },
        {
            "research_objective": "Determine the compressibility and learnability of neurally compressed text using GZip.",
            "experiment_process": "Compress text using GZip and train standard LLMs over this compressed text. The LLMs are evaluated to see if they can model GZip-compressed text effectively.",
            "result_discussion": "Standard LLMs are capable of learning to model GZip-compressed text. However, the performance is not competitive with other methods. The results indicate that while some compression methods produce learnable data, they might not be optimal for LLM training compared to other techniques such as Equal-Info Windows.",
            "ablation_id": "2404.03626v1.No2"
        },
        {
            "research_objective": "Analyze the learnability of text compressed via Arithmetic Coding (AC) when trained over a byte-level language model (M1).",
            "experiment_process": "Train a small language model (M1) over raw byte sequences, then use this frozen model to compress pretraining corpus text via Arithmetic Coding. Chunk the compressed bitstream into tokens and train a subsequent language model (M2) over these tokens. Evaluate the M2 models' ability to learn and predict compressed text.",
            "result_discussion": "Text compressed via Arithmetic Coding is not readily learnable by a standard transformer-based LLM, resulting in models predicting tokens at chance. This suggests that the difficulty lies in learning the AC compression/decompression process itself. Even with a context-free unigram model, AC-compressed text remains challenging to model, indicating that the compression method is not suitable for direct LLM training.",
            "ablation_id": "2404.03626v1.No3"
        }
    ]
}