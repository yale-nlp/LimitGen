{
    "title": "Assisting humans in complex comparisons: automated information comparison at scale",
    "abstract": "Generative Large Language Models enable efficient analytics across knowledge domains, rivalling human experts in information comparisons. However, the applications of LLMs for information comparisons face scalability challenges due to the difficulties in maintaining information across large contexts and overcoming model token limitations. To address these challenges, we developed the novel Abstractive Summarization & Criteria-driven Comparison Endpoint (ASCEnd) system to automate information comparison at scale. Our system employs Semantic Text Similarity comparisons for generating evidence-supported analyses. We utilize proven data-handling strategies such as abstractive summarization and retrieval augmented generation to overcome token limitations and retain relevant information during model inference. Prompts were designed using zero-shot strategies to contextualize information for improved model reasoning. We evaluated abstractive summarization using ROUGE scoring and assessed the generated comparison quality using survey responses. Models evaluated on the ASCEnd system show desirable results providing insights on the expected performance of the system. ASCEnd is a novel system and tool that enables accurate, automated information comparison at scale across knowledge domains, overcoming limitations in context length and retrieval.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The applications of generative Large Language Models (LLMs) across knowledge domains, including financial services, are rapidly expanding due to their proven capabilities in performing multiple types of tasks such as abstractive summarization (Liu and Healey, 2023  ###reference_b25###), simple QA (Question-Answering) (Trivedi et al., 2022  ###reference_b40###), multiple choice QA (Trivedi et al., 2022  ###reference_b40###), financial sentiment analysis (Fei et al., 2023  ###reference_b6###), Retrieval Augmented Generation (RAG) (Pan et al., 2022  ###reference_b33###), among others (Lu et al., 2021  ###reference_b26###). These open-domain tasks have recently been popularized in specific applications as they enable time efficiencies in the analytics required for informed decision-making.\nIn this paper, generative LLMs are defined as generative models that are solely autoregressive. Recently, generative LLMs have proven to be effective in executing abstractive summarization (Rath et al., 2023  ###reference_b36###), outperforming existing state-of-the-art (SOTA) models (Liu and Healey, 2023  ###reference_b25###). Fine-tuning generative LLMs is an effective method of increasing the model\u2019s overall performance in a specific domain (Brown et al., 2020  ###reference_b3###). However, prompting strategies are effective in leveraging the performance capabilities of naive generative LLMs without relying on large training data sets, with some minor trade-offs in performance (Navarro et al., 2022  ###reference_b29###)(Howell et al., 2023  ###reference_b11###). Zero-shot (Kojima et al., 2022  ###reference_b19###) and few-shot (Wei et al., 2022  ###reference_b42###) prompting techniques can be leveraged to direct answer structures and elicit human-like reasoning for response generation. A recent study demonstrated the ability of naive generative LLMs to perform Named Entity Recognition (NER) and Relation Extraction (RE) (Li et al., 2023  ###reference_b21###) for tasks that reflect human-like reasoning. Chain-of-thought reasoning can also be elicited in zero-shot prompting strategies using phrases such as \u201clet\u2019s think step-by-step\u201d (Kojima et al., 2022  ###reference_b19###) to reflect human-like reasoning. Additionally, altering the available input context and the order in which information is presented in a prompt (Lu et al., 2021  ###reference_b26###) can affect the final output response structure.\nSemantic textual similarity (STS) is the main driving factor in effective information comparison (Majumder et al., 2016  ###reference_b27###). STS is measured through many different techniques that can help in text classification and topic extraction (Slimani, 2013  ###reference_b38###) without the limitations of lexical similarity. Semantic similarity is achievable with generative LLMs as demonstrated by Gatto et al. (2023  ###reference_b8###). They achieved success in assessing semantic similarity by testing various prompting strategies on generative LLMs. Moreover, using retrieval augmented generation (RAG) provides additional context based on a user query such that an LLM can generate an in-domain output response (Ram et al., 2023  ###reference_b35###) (Trivedi et al., 2022  ###reference_b40###). RAG applies STS to find the top-k passages most similar to the user query and enhances the information in its output response.\n###figure_1### Recent research has demonstrated that decision-making and data analysis can substantially benefit from text analyses made available by STS through generative LLMs. For example, research in the medical domain has developed a framework to screen abstracts of scientific papers to be used for review papers (Guo et al., 2023  ###reference_b9###). Guo et al. (2023  ###reference_b9###)\u2019s work used generative LLMs to compare scientific abstracts with user-defined criteria and sorted the abstracts based on the eligibility generated by the model. Their work provided insights into the use of generative LLMs to perform information comparisons against user-defined criteria such that the model could make binary categorical decisions. Similar research was also conducted in the financial domain, where a corporate sustainability report was compared to a sustainability guideline document in their framework, chatReport (Ni et al., 2023  ###reference_b30###). chatReport supported different QA tasks regarding the information in the sustainability report, powered by RAG. Their framework handles individual reports and generates responses for single-use applications.\nHowever, applying LLMs for information comparison is currently not possible at scale due to token limitations imposed on LLMs. Minimizing information loss and prompting under token limits are ongoing issues that must be addressed for model success. Additionally, models with longer token limits are prone to losing information from the input context, as it was demonstrated that generative LLMs struggled to retrieve relevant information from the middle of long input contexts (Liu et al., 2023b  ###reference_b24###). in fact, Liu et al. (2023b  ###reference_b24###) found that naive, untrained models with no input context had performed better on the same QA task when compared to the models provided with a lengthy input context.\nTo address these challenges, we developed ASCEnd (Abstractive Summary & Criteria-driven Comparison Endpoint), a novel system enabling accurate, automated information comparison at scale for applications across financial services and other knowledge domains. ASCEnd presents a novel method of applying comparisons for generating evidence-supported analyses. We applied our system to the challenge of identifying and evaluating financial transactions that meet complex, user-defined sustainable finance criteria in the financial domain. This paper discusses the design choices of the system, its significance in the financial analysis domain, as well as the limitations & future work of the ASCEnd system. Our system enables efficient information comparison across large, complex data sets and facilitates more informed decision-making."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "The ASCEnd System",
            "text": "The ASCEnd system provides insights by comparing a given text corpus against a set of user-defined criteria. This comparison evaluates the relevance of each document in the corpus to a user-defined topic. The ASCEnd system is built using abstractive summarization, RAG, binary QA tasks, and reasoning tasks; tasks which have been recently studied to be effective when deployed with generative LLMs (Touvron et al., 2023  ###reference_b39###). Zero-shot prompting is implemented to enable more flexibility across applications as it removes the need for model finetuning.\nThe ASCEnd system comprises four modules (Figure 1  ###reference_###) that process the user-specified information and generate the comparison assessment. The first two modules, Document Summarization (DS) and Criteria Embedding (CE), perform the input processing of the given text corpus and user-defined criteria respectively. The third module, Retrieval Augmented Generation (RAG), facilitates the similarity search to retrieve and output the relevant information for the next module. The last module, Comparison Assessment (CA), performs the comparison tasks with the preprocessed data as input. The DS module iteratively performs abstractive summarization on individual documents from the given text corpus to generate a summary for each document. The CE module vectorizes and splits the user-defined criteria document, storing the segments in a vector database. The RAG module uses the vector database to drive a similarity search between the summary from the DS module, combined with the RAG prompt and the user-defined criteria to return the top-k passages. These passages are fed in with the RAG prompt to a human-level LLM to provide an augmented output of these passages to the CA. In the last step, the CA module uses the information provided by the RAG module and the summary from the DS module to generate an output comparison assessment."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Method",
            "text": "ASCEnd\u2019s ability to perform large-scale comparisons is facilitated through the system\u2019s data preprocessing steps on a given text corpus and a set of user-defined criteria. To validate the performance of our system, we sourced a news dataset as the candidate text corpus and we selected a relevant criteria document as the user-defined criteria.\nSince there are varying levels of reasoning required for both the DS and CA modules, we define two different types of reasoning tasks: machine-level and human-level reasoning tasks. Machine-level reasoning tasks require little reasoning and were previously studied and automated by LLMs with a high degree of success. Human-level reasoning require several different solving strategies when provided to a model or human (Johnson-Laird, 2010  ###reference_b18###). Brown et al. (2020  ###reference_b3###) found that larger parameter models are more likely to generate human-like responses, thus we used parameter sizing as our deciding factor for the two types of reasoning tasks.\nModel selection, methods of performance evaluation, and experimental setup are described for each of the data preprocessing (DS and CE), RAG, and CA modules. To evaluate the performance of abstractive summarization done by machine-level LLMs, the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) metric (Lin, 2004  ###reference_b22###) was used. The performance of the comparison assessment done by human-level LLMs was evaluated using survey participants with the goal of optimizing the output coherence, quality of the response, and the accuracy of the retrieved information."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Data Preprocessing of Text Corpus and Criteria",
            "text": "The ASCEnd system performs preprocessing on both the input text corpus and the set of user-defined criteria. The text corpus used in this experiment was financial data sourced from a news database to demonstrate the feasibility of using raw, unstructured data.\nDocument summarization was performed with generative LLMs to process the raw financial data. The user-defined criteria document was uploaded to a vector database such that a similarity search could be conducted."
        },
        {
            "section_id": "3.1.1",
            "parent_section_id": "3.1",
            "section_name": "3.1.1 Data Acquisition",
            "text": "The search scope for data in this experiment was to analyze the publicly available transaction records of a financial institution (i.e. the text corpus) and compare them with the institution\u2019s sustainable finance guidelines (i.e. user-defined criteria). Data for the calendar year of 2021 was obtained from Factiva, a financial database owned by Dow Jones & Company (Arbour, 2002  ###reference_b1###). Factiva compiles articles from various open-source news outlets and information posted on the SEC EDGAR database. Web scraping was employed to extract the article title and content of each document. The resultant text corpus was a 2-column CSV file with 1253 entries, the same number of available articles from 2021 on Factiva. These articles had an average of 7500 words. The user-defined criteria was a 20-page PDF document extracted from a financial institution\u2019s website that outlined a set of sustainable finance guidelines.\nQuery:\nGiven this text: {split_text}\u2026\ngenerate a TL;DR.\n\n\nGuidelines for your answer:\n\n\n1. Include all detailed information relevant from the text.\n\n\n2. Formulate concise answers, grounded on facts from context. Keep answers logical.\n\n\n3. Use point form answers.\n\n\nAnswer: TL;DR:"
        },
        {
            "section_id": "3.1.2",
            "parent_section_id": "3.1",
            "section_name": "3.1.2 Document Summarization (DS)",
            "text": "The Document Summarization (DS) module employed Abstractive Summarization to process the financial text corpus and generate standardized-length summaries. Abstractive summarization with the use of generative LLMs has been well-documented in several studies (Liu and Healey, 2023  ###reference_b25###)(Kumar et al., 2023  ###reference_b20###). Liu and Healey (2023  ###reference_b25###) performed abstractive summarization using lower complexity models. Thus, we determined that the use of machine-level generative LLMs would be suitable for this task.\nTokens were measured at 4 English characters per token (OpenAI, 2023b  ###reference_b32###). In our experiment, we preprocessed the obtained data by splitting each document into 2000 token chunks, with each chunk summarized into 250 token segments. The summarized segments were concatenated to form the final document summary. Since the smallest context window of the models used in experimentation was 4096 tokens, we found that 50% of the total context or 2000 tokens, was the maximum length of each chunk that could be provided without raising runtime errors. The 250-token length for summarized segments was chosen based on previous work done by Shapira et al. (2018  ###reference_b37###), which showed that gains to ROUGE scoring had diminished returns past the 200-word range. 250 tokens is approximately 190 words and 12.5% of the original 2000-token chunk length. Figure 2  ###reference_### illustrates the prompt used to generate the summary for each 2000-token chunk. Radford et al. (2019  ###reference_b34###) found that using the \u201dTL; DR\u201d token in the prompt was an effective method to incite a summary response. Additionally, we specified guidelines in the prompt to control the generated response structure (e.g. \u201dFormulate concise answers\u201d) such that tokens were used effectively to retain information.\nThe maximum threshold length of the generated summaries was determined to be 1250 tokens to account for the minimum context window length of the LLM performing the comparison assessment as well as the hardware limitations of the experimental setup. If the final document summary was longer than 1250 tokens, or five 250-token segments, then the DS process was repeated until the output summary length was below the threshold. Previous experiments were conducted with a higher threshold length of 2500 tokens per generated summary to take full advantage of the context window length. However, the comparison task could not be performed due to memory limitations, thus the threshold size of the document summary was reduced to 1250 tokens, or 30% of the total context (i.e. 4096 tokens)."
        },
        {
            "section_id": "3.1.3",
            "parent_section_id": "3.1",
            "section_name": "3.1.3 Criteria Embedding (CE)",
            "text": "The user-defined criteria document was embedded and stored in a vector database such that the user could retrieve passages that were most relevant to the input document summary. The criteria document was vectorized and split into 500-character (125 tokens) chunks with a 20-character overlap. It was important to have character overlap when splitting the criteria for coherent retrieval. The criteria document split size was chosen based on the experimentation described in chatReport (Ni et al., 2023  ###reference_b30###), which stated that a split size of 500 characters and 20-character overlap resulted in the best retrieval performance.\nQuery:\n\n\nGiven this document delimited by \u201d\u201d: \u201d{summary}\u201d:\nProvide the most relevant information only from the criteria that matches with the given document in terms of {target_topic}?\n\n\nAnswer:"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Retrieval Augmented Generation (RAG)",
            "text": "RAG retrieves relevant passages from the vector database by conducting a semantic search. The prompt given to the RAG task (Figure 3  ###reference_###) combines the input document summary with a simple QA task. The document summary was integrated into the RAG prompt to provide the necessary context to perform a successful retrieval. Additionally, the user can designate the focus of the search by defining a target topic that directs content retrieval. Semantic search was performed by applying the RAG prompt to the vector database to find the top-k = 3 passages. k is a tunable hyperparameter that is dependent on the specified application and set of criteria. In this experiment, it was found that beyond k = 3, the retrieval response did not capture additional information. The top-k passages retrieved by the semantic search are an intermediary output that is presented alongside the RAG prompt as additional context for a human-level LLM to answer the query. The output response of the human-level LLM is an initial comparison that is informed by relevant retrieved passages from the criteria.\nPrompt:\nYou are an AI model assisting a Financial Analyst at {company}. Your task is to analyze the document delimited by \u201d\u201d: \u201d{summary}\u201d and provide a thorough, yet concise analysis in the following format:\n\n\n1. Article Date: [Please input the date of the article here in MM/DD/YYYY format]\n\n\n2. Participants of the transaction: [Please provide a brief description of {company}\u2019s role in relation to the article, then list the entities involved in the transaction mentioned in the article]\n\n\n3. Transaction and Transaction type: [Please indicate whether a transaction has taken place. If yes, state the type of transaction.]\n\n\n4. Transaction amount in dollars: [If a transaction has occurred, please specify the amount in dollars. If no transaction, please input $0]\n\n\n5. Comparison: [Based on the following criteria, delimited by \u201d\u201d: \u201d{retrieved_text}\u201d. Provide a concise comparison between the document and provided criteria and\ndiscuss the relevancy of the document to {target_topic}. Use specific information from the criteria and be very critical in your assessment].\n\n\n6. Confidence score: [Please provide a score between 0-100 indicating the degree to which the document discusses topics related to {target_topic}. A score of 0 means the document is not at all related to {target_topic}, a score of 50 means there are many uncertainties as to its correlation to {target_topic}, and a score of 100 means the document content is entirely about {target_topic}. If the transaction amount is $0 or there is no transaction, please input a score of 0. Use your comparison to affect your decision, skepticism and implicit assumptions in the answer needed to negatively affect the confidence score.]\n\n\nPlease remember to:\n\n\n1. Provide factual and concise answers. 2. Critically evaluate the information from the document. 3. Use bullet points for your answers. 4. Do not explain your thought process. 5. Do not include extra text in addition to your analysis outside of the six points of analysis. 6. \u201ddocument\u201d should only refer to the provided article document.\n\n\nResponse:"
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Comparison Assessment (CA)",
            "text": "The Comparison Assessment (CA) module performs entity recognition, information extraction and comparison assessment tasks through the use of generative LLMs. In this experiment, the information extraction tasks included the identification of financial transaction details such as whether a transaction occurred, the transaction amount and type, and the relevant participants. The main task of the CA module is the comparison between the RAG output (i.e. relevant passages of the criteria) and the generated document summary. Complimentary to the comparison, a confidence score was assigned to the relevancy of the document summary against a user-defined topic.\nTo drive the outlined tasks in the CA module, a comparison prompt was provided to an LLM, as shown in Figure 4  ###reference_###. The comparison prompt implements multiple zero-shot prompting strategies to extract relevant information available from the input context and to conduct the comparison tasks. These strategies include the use of explicit examples, guideline specifications for finding the relevant information, and the use of rules to direct the model response. We specifically used the phrase \u201dDo not explain your thought process\u201d as one of the rules to remove words that had no relevance to the comparison. Additionally, we wanted to investigate if the language model could successfully perform the task without explicitly stating its chain of thought.\nThe CA module required a model to provide analysis on multiple topics and provide a confidence score based on eliciting reasoning from the provided summary. Thus, the CA module was performed using human-level generative LLMs. The RAG process and comparison assessment tasks are operated sequentially on the same human-level generative LLM instance to allow for in-memory transfer between the two modules."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Model Selection",
            "text": "To select the optimal LLMs for the machine and human-level reasoning tasks, the performance of several generative LLMs was evaluated. Answers and scores provided by generative LLMs are prone to hallucinations (Bang et al., 2023  ###reference_b2###)(Huang et al., 2023  ###reference_b12###) and imperfect logic during model inference. However, we addressed this issue by assigning the appropriate generative LLMs for different reasoning tasks. In our study, we compared the performance of the following models: Llama 2 7B, 13B, and 70B (Touvron et al., 2023  ###reference_b39###), Mistral 7B (Jiang et al., 2023a  ###reference_b15###), GPT 3.5 (Brown et al., 2020  ###reference_b3###) and GPT 4 (OpenAI, 2023a  ###reference_b31###).\nLlama-2 7B, Llama-2 13B, Mistral 7B, and GPT 3.5 were chosen to execute the machine-level reasoning task based on their cost-efficient performance for their parameter size (HuggingFace, 2022  ###reference_b13###). Llama-2 13B was specifically chosen as the sole 13B model to evaluate the effects of larger parameters on the summarization task. GPT 3.5 was chosen based on its significant performance difference in reasoning tasks compared to GPT 4 (Espejel et al., 2023  ###reference_b5###), as machine-level reasoning tasks are deemed to not require as much reasoning power.\nHuman-level reasoning models chosen for the experiment were Llama-2 70B and GPT 4. These are currently the most competitive LLMs to be used for complex-level reasoning while fitting within our experimental constraints. Larger parameter models can generate human-like text (Brown et al., 2020  ###reference_b3###), reflecting that these models are better suited to elicit human-like reasoning. Llama-2 70B is currently the best-performing 70 billion parameter open-source base model available (HuggingFace, 2022  ###reference_b13###) and GPT-4 has studies demonstrating performance comparable to human-like reasoning (Bubeck et al., 2023  ###reference_b4###)(OpenAI, 2023a  ###reference_b31###)(Liu et al., 2023a  ###reference_b23###).\nTo evaluate the performance of the machine-level LLMs in the DS module, we utilized the ROUGE metric. ROUGE scoring is widely used in many summarization-focused studies involving generative LLMs (Liu and Healey, 2023  ###reference_b25###)(Kumar et al., 2023  ###reference_b20###) as it provides a simple method of measuring the amount of information retained from the source text. The ROUGE score determines the similarity between a summary and a reference text based on text overlap. Scoring generative LLMs with ROUGE does not reflect the readability of the model\u2019s response. Therefore, the aim of using ROUGE was to assess each model\u2019s ability to retain information from the source document systematically.\nEvaluation of the human-level LLMs\u2019 performance in the comparison assessment was a complex task that required the validation of output readability and correct process reasoning. Therefore, we collected human feedback through the creation of a survey. A dataset was created by randomly selecting and masking 20 model output pairs that correspond to the same document summary, with one of each pair generated by either the GPT-4 or Llama-2-70B model to allow for comparison of performance between the models. As a result, the survey comprised of ten GPT-4 response outputs and ten Llama-2-70B response outputs. The same survey was distributed to all participants for consistency. Randomly sampling the model output dataset may introduce bias in the results. However, since the models were being evaluated comparatively, the empirical scores had a lower impact on the overall analysis.\nThe survey used a 5-point scoring system by asking participants to answer 5 questions per document entry, as shown in Table1  ###reference_###. Each question was scored either a 0 (no) or 1 (yes). The first three tasks evaluated the ability to retrieve information from the input context and the last two tasks evaluated the reasoning abilities of the selected LLM. This scoring method was chosen to lower answer scoring ambiguity. Each participant was provided with an explanation of their task along with the user-defined criteria document. Participant data was completely masked for confidentiality and participants had no formal domain knowledge in finance or sustainability.\n###figure_2###"
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "Experimental Setup",
            "text": "The experiment was completed on an A6000 GPU with 48 GB of RAM. Open-source models were loaded into local memory using the GPTQ method (Frantar et al., 2022  ###reference_b7###) where the model parameters were quantized to decrease the total memory load. Model hyper-parameters are shown in Table 2  ###reference_###. Models were set to a temperature of 0 to maintain the reproducibility of results and to keep output formats consistent for all candidate documents. ASCEnd was developed using the LangChain (Harrison, 2022  ###reference_b10###) framework for all API calls and for managing the vector database requests. The embedding model used for the vector database was the Beijing Academy of Artificial Intelligence (BAAI)\u2019s \u201cbge-base-en-v1.5\u201d (Xiao et al., 2023  ###reference_b43###) as it was the best performing open-source embedding model for the model size. Additionally, the BAAI model outperformed OpenAI\u2019s paid \u201ctext-embedding-ada-002\u201d model on the Massive Text Embedding Benchmark (MTEB) leaderboard (Muennighoff et al., 2022  ###reference_b28###). The RAG task for this system is powered by FAISS (Facebook AI Similarity Search) (Johnson et al., 2017  ###reference_b17###) acting as the vector database, due to its efficiency in performing similarity search."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "In this section, we discuss the model performances on both the abstractive summarization task and the comparison assessment task evaluated using the ROUGE score and survey answers, respectively.\n###figure_3###"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Abstractive Summarization Results",
            "text": "The scores presented for abstractive summarization are the averaged ROUGE values across the given document corpus of 1253 documents. There are two sets of summarization results, the first using a maximum output length of 2500 tokens and the second using 1250 tokens.\nROUGE (Lin, 2004  ###reference_b22###) is calculated based on the text overlap using the calculated precision and recall values. The score ranges from 0 to 1, where 0 indicates poor similarity and 1 indicates strong similarity between the summary and reference text. ROUGE can be evaluated with any number of n-grams. The chosen evaluations for this experiment were unigram (n=1), digram (n=2), and the longest sub-sequence of text (n=L) for evaluation. Unigram performance measures the number of single words that match the original document, reflecting how overall information was retained. Conversely, digram and sub-sequence performance measure the quality of semantic meaning retained in the summary. The ROUGE formulas for calculating the unigram & digram and the L-scores are presented in Figures 7  ###reference_### and 8  ###reference_### respectively.\nIn the 2500-token length summarization results (Figure 5  ###reference_###), unigram (n=1) precision across all four machine-level models was competitive with scores close to 1. A significant decline in performance in both precision and recall was observed when the remaining n-gram performances were compared to the unigram (n=1) performance. Due to paraphrasing, word-pair overlaps and sub-sequence matching were not maintained in abstractive summarization, leading to lower scores in digram (n=2) and subsequence (n=L) scores. The recall score is based on the amount of text overlap between the candidate and reference text compared to the total length of the reference text. As the summarization shortened the overall length of the source documents to less than 12.5% of its initial length, a shorter context length meant less text to compare for the recall score. Thus, the recall score in every case for all four models was significantly lower than the precision scores. In terms of models, Mistral 7B displayed a strong score in the ROUGE-2 scenario, beating GPT 3.5 and the Llama models (Figure 5  ###reference_###). Mistral 7B also has competitive results compared to GPT 3.5 in ROUGE-1 and ROUGE-L scoring and is an open-source model, making it an economical option for larger-scale workflows. It was also observed that there were significant performance decreases in the recall scores of the Llama models. Llama model outputs are more lengthy and are more likely to generate sequences of text that may not be relevant to the summary, thus negatively affecting the recall score.\nThe 1250-token length summarization task (Figure 6  ###reference_###) was conducted to analyze the losses in ROUGE scoring for a shorter output summary length. A direct comparison in the ROUGE values between the two summarization tasks was made in Table 3  ###reference_###. The precision of all four models was almost identical to the first summarization task, implying that the context of the information did not change during the shortening of the summaries. The recall scores decreased by at least 2%, with the greatest decrease in recall for the Llama-2 13B model. As a result, the larger parameter Llama model was deemed not suited for summarization tasks with token limitations due to its verbosity. Llama-2 7B had similar scores across both summarization tasks but still performed significantly worse than the top models. From this 1250-token length experimentation, we found that the top-performing models from the first summarization task experienced negligible changes in performance despite the shortened summary length. Mistral 7B outperformed GPT 3.5 in every recall measure and had improved overall F1 scoring. In addition, Mistral 7B recall scores did not take a severe performance hit in the second summarization task. Therefore, Mistral 7B was the most suitable machine-level LLM to employ in the DS module for running abstractive summarization."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Comparison Assessment (CA) Results",
            "text": "When evaluating the human-level LLMs performing the CA tasks, we prioritized the clarity of model output, accuracy of model response, and the presentation of information. The scores presented in the comparison assessment reflect the human-annotated scores of 21 survey participants. For this section of the paper, Llama 2 70B will be referred to as Llama 2.\nThe averaged survey scores for each model are shown in Table 4  ###reference_###. GPT 4 performed better than Llama 2 overall by 0.7 points and scored higher in every category. GPT 4 scored 0.2 \u2013 0.3 points greater than Llama 2 in assessing the comparison and providing reasoning to justify its confidence score. Scores on identifying stated information were closer between Llama 2 and GPT 4, with a 0.05 - 0.1 point difference between the first three categories that focused on retrieving information from the input context.\nAccording to the survey results, there are major differences between the human-level reasoning abilities of the models. Llama 2 obtained an unimpressive score of 0.432 when evaluated on making the correct comparison given a candidate summary and the retrieved passages from the user-defined criteria. Llama 2 struggled to identify the sentiment of the candidate text and it conformed the candidate sentiment to the sentiment of the retrieved reference text. The model\u2019s inability to perform the comparison task resulted in the misidentification and hallucination of the candidate text\u2019s conformity to the target topic. Similarly, when providing reasoning for its confidence score on the target topic, Llama 2 obtained a low score of 0.56.\nOn the other hand, GPT 4 excelled in justifying its generated confidence score, with an average score of 0.81. This score indicates that the GPT 4 model accurately determined the sentiment of the candidate text and used reasoning to provide a valid response. The score also reflected that GPT 4 explained its reasoning in succinct detail such that the survey participants agreed with the response. GPT 4 obtained a score of 0.698 for its ability to identify the correct comparison. Compared to Llama 2\u2019s 0.431, GPT 4 was better at accurately identifying sentiment and making accurate comparisons between the candidate and reference texts."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "ASCEnd\u2019s large-scale automation capabilities unveil a novel method to quickly perceive publicly posted information. The ASCEnd system provides valuable insights regarding the conformity of a candidate document corpus to the user-defined criteria. Large-scale automation of information comparison is desirable due to the time-saving possibilities it presents, providing more opportunities for users to perform meaningful analyses on summaries of data. ASCEnd is also widely applicable due to its use of prompt engineering to obtain the desired results. Our system\u2019s methodology is intuitive and eliminates the need for LLM expertise during system operation.\nFrom our experimental results shown in Table 5  ###reference_###, GPT 4 can complete a comparison assessment in 0.38 minutes which demonstrates the system\u2019s scalability to much larger datasets and workflows. The use of Llama 2 70B is possible, however, it performs significantly slower and there are significant performance declines that were outlined in Table 4  ###reference_###.\nASCEnd eliminates redundancies of traditional human textual analysis and greatly decreases the amount of time required to extract and compare relevant information from documents. Users can quickly access relevant topics and details of documents on a larger scale, which can influence the speed at which decisions are made. This concept is especially important in the financial domain where many high-pressure decisions are time-sensitive (Wegier et al., 2015  ###reference_b41###) and require accurate, publicly available information to aid in decision-making.\n\nIn the realm of creative writing and storytelling, AI systems such as OpenAI's GPT have showcased notable capabilities in generating coherent narratives and aiding authors in writer's block situations. Studies indicate that these systems can offer innovative plot suggestions and character developments, enhancing the creative process (Manovich, 2020)."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Leveraging Retrieval for Comparison",
            "text": "Presently, RAG-focused architecture focuses on different forms of information retrieval to optimally answer the user query in closed domain QA (Pan et al., 2022  ###reference_b33###)(Ram et al., 2023  ###reference_b35###). The use of RAG is very powerful for obtaining in-context information to enhance the LLM\u2019s response, however, RAG processes are optimized for only QA tasks. We addressed the QA limitations in RAG processes by incorporating an additional human-level LLM step to facilitate the comparison task instead of strictly using the RAG process for task completion. We term this method as \u201dRAG-to-comparison\u201d. We conducted a study to compare a RAG-only task to the current RAG-to-comparison strategy to validate our system architecture. Our study revealed that there was a decrease in the model\u2019s ability to recognize entities based on input context in the RAG-only process. Using the RAG-to-comparison process provided improved context to the comparison assessment.\n\nAI for creative writing and storytelling is progressively evolving, with models such as GPT providing tools that assist not only in generating coherent narratives but also in enhancing creativity (Li et al., 2023  ###reference_b36###). These models can augment traditional storytelling by offering novel plot ideas, character development, and stylistic variation (Smith et al., 2023  ###reference_b37###)."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Cross-Domain Application of ASCEnd",
            "text": "The ASCEnd system was designed to function with naive generative LLMs as they can adapt to different knowledge domains through the use of zero-shot prompting techniques. The prompt engineering structure used in our system can be modified to support few-shot prompting strategies for applicable knowledge domains and specific use cases where applied examples were designed. Additionally, the zero-shot prompting structure of our system can be modified to determine different focuses of analysis. Our system was designed to be robust by enabling the ability to process various data sizes through our abstractive summarization and criteria embedding modules. These modules were designed to facilitate widely reproducible results with different scopes of analysis. \n\nIn the realm of AI for creative writing and storytelling, advancements in language models have shown promising potential. Systems are now capable of generating coherent and engaging narratives by utilizing stylistic and thematic elements, with models adapting storytelling techniques through learned patterns from vast textual corpora. This progress enables AI to contribute meaningfully to creative processes."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Comparison of ASCEnd to Related Work",
            "text": "To demonstrate the impact of the ASCEnd system in the field of automated text comparisons, we discuss the similarities and differences of related work. Various fields of study employ the concept of information comparison for analysis. In this section, we discuss two recent examples of information comparison methods applied to academia and sustainable finance domains. Guo et al. (2023 ###reference_b9###)\u2019s work automates criteria comparison for academic paper screening using LLMs. Their study presents how LLMs can elicit reasoning by comparing abstracts of scientific papers to user-defined criteria for the goal of creating a preliminary screening process for academic papers. With user-specified statements as criteria, an LLM is used to make a yes/no decision based on the abstract\u2019s conformity to the criteria. Contrary to the ASCEnd system, the LLM\u2019s output response lacked supporting evidence in its justification as it only outputted a yes/no binary response. In the ASCEnd system, the generated response provides evidence-based responses with information provided from the input context. Additionally, our system efficiently processes input text documents significantly larger than academic abstracts and can manipulate a range of criteria documents for analysis. chatReport (Ni et al., 2023 ###reference_b30###) is a financial information comparison system designed to evaluate corporate sustainability reports by using an RAG process to drive QA tasks. The goal was to compare a corporate sustainability report against the TCFD (Task Force on Climate-related Financial Disclosures) guidelines. chatReport employed a retrieval system to obtain relevant information from corporate sustainability reports that are very large (i.e. typically larger than 20 pages) to run conformity and QA tasks. Their system leveraged RAG abilities for conformity assessment to a set of sustainability guidelines. However, chatReport lacked methods for automating the comparison process with the designed questions and could only analyze one candidate document at a time. In contrast, ASCEnd automates the comparison process and uses an RAG process as part of the workflow to only provide relevant information for the comparison assessment. Our system eliminates the dependency on multiple QA tasks to extract the necessary information for comparison and removes the need for user intervention to generate insights. Additionally, the ASCEnd system can preprocess candidate documents of any length through the use of abstractive summarization, which further demonstrates the adaptability of our proposed system.\n\nIn the realm of AI for creative writing and storytelling, systems leverage large language models to generate narratives and assist in storytelling by interpreting prompts and suggesting coherent plotlines. These AI tools can enhance creativity by providing writers with inspiration and alternative perspectives, fostering collaboration between human authors and machine-generated content."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Limitations & Future Work",
            "text": "ASCEnd acts as a proof of concept for an advanced level of information comparison in the financial domain at a large scale. However, there are limitations to the ROUGE and human-annotated metrics used to evaluate the performance of the system. We also denote the current advances in the techniques used in our system and how they may be applied in future work. Jiang et al. (2023b) identified that the ROUGE metric used to evaluate the effectiveness of the generative models could be susceptible to small variations of the source data, negatively influencing the sentiment of the generated summary. This process could be exploited by human intervention by providing unhelpful inputs to prompts, unfavorably affecting the desired output. Additionally, in choosing to evaluate the abstractive summarization task with only ROUGE and not supplementing with human feedback, there was a possibility that information specificity was lost in the iterative abstractive summarization tasks. However, it is not cost-effective to manually validate individual summaries and provide human feedback. There exist new methods presented in (Liu and Healey, 2023) where documents that are semantically split for summarization instead of by token length, potentially yield more accurate summarization results. The method of using RAG to retrieve relevant categories of comparison sometimes returned irrelevant information due to imbalanced attention on the provided context. This limitation is caused by how the similarity search is conducted where the criteria passages were selected based on relevancy to the target topic and not the candidate document summary. Processing the retrieved passages could remove this unintended \"noise,\" and should be considered for future work. Furthermore, providing automated scoring for the comparison assessment is difficult due to the complex nature of the reasoning tasks. Currently, the most accurate methods in ensuring model success during inference are to break down tasks into simple labeled QA response pairs or to employ the use of survey participants to judge the quality of the output. We hope that as generative LLMs become more developed, they can reliably act as their iterative feedback agent to score and improve their responses (Jain et al., 2023). Our experimentation was constrained by hardware limitations that affected model selection and runtimes. 180B parameter models were completely out of the scope of study due to our hardware limitations. As LLMs continue to grow in complexity, additional computational resources will be needed.\n\nIn the realm of AI for creative writing and storytelling, advancements have been marked by models generating cohesive narratives (Clark et al., 2023). Innovative algorithms allow AI to assist or autonomously produce fiction, leveraging both structured prompts and learned stylistic nuances (Anderson and Smith, 2023). Despite success, balancing creativity and coherence remains challenging."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "Applications powered by generative LLMs aid in human decision-making through various emerging techniques. Here, we developed a novel system and tool, ASCEnd, that enables accurate, automated information comparison at scale across knowledge domains, overcoming limitations in context length and retrieval. We apply ASCEnd to a challenge in the financial domain, providing insights into a corpus of financial information. The system compares documents to user-defined criteria and highlights transaction-specific information and detailed analyses, resulting in time-saving efficiencies and facilitating more informed decision-making. We showcased the practical uses of our system and how it can effectively implement various available models. Our system assists in the decision-making process and can save time for analyst professionals who require access to specific and semantic information that is concealed in each document of a large corpus. Our research provides early steps to using LLM-generated responses as a valuable aid for human-level reasoning analysis tasks."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "CRediT Authorship Contribution Statement",
            "text": "Truman Yuen: Conceptualization, data curation, formal analysis, investigation, methodology, software, validation, visualization, writing - original draft, writing - review & editing. Yuri Lawryshyn: Funding acquisition, project administration, resources, supervision, writing - review & editing. Graham Watt: Conceptualization, project administration, resources, supervision, writing - review & editing."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Declaration of Competing Interest",
            "text": "The authors declare no conflicts of interest regarding the research and findings presented in this paper."
        }
    ],
    "url": "http://arxiv.org/html/2404.04351v1",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "2",
            "3",
            "3.1",
            "3.1.1",
            "3.1.2",
            "3.1.3",
            "3.2",
            "3.3",
            "3.4",
            "3.5"
        ],
        "main_experiment_and_results_sections": [
            "3.5",
            "4",
            "4.1",
            "4.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.1.2",
            "3.4"
        ]
    },
    "research_context": {
        "paper_id": "2404.04351v1",
        "paper_title": "Assisting humans in complex comparisons: automated information comparison at scale",
        "research_background": "### Paper's Motivation\nThe motivation behind this paper is to enhance decision-making and data analysis by utilizing generative Large Language Models (LLMs) for complex information comparison at scale. Despite the proven capabilities of these models in various tasks such as abstractive summarization, sentiment analysis, and zero-shot prompting, their application for large-scale information comparison remains limited due to token constraints and input context limitations. The authors aim to address these issues and extend the use of LLMs to more practical and scalable applications, such as financial services.\n\n### Research Problem\nThe central research problem this paper addresses is the limitation of current generative LLMs in performing information comparison tasks at scale. This problem arises due to token limitations and the challenges faced by LLMs in effectively handling long input contexts without losing pertinent information. As a solution, the authors developed ASCEnd (Abstractive Summary & Criteria-driven Comparison Endpoint), a novel system that aims to overcome these limitations by enabling accurate and automated information comparison, particularly in financial services and potentially other knowledge domains.\n\n### Relevant Prior Work\nThe paper builds upon various prior works and methodologies that have demonstrated the capabilities of generative LLMs in different tasks and domains:\n1. **Abstractive Summarization**:\n   - Liu and Healey (2023) and Rath et al. (2023) showed that generative LLMs are effective in abstractive summarization and outperform many existing models.\n   \n2. **Question Answering (QA)**:\n   - Trivedi et al. (2022) explored the application of LLMs in simple and multiple-choice QA tasks.\n   \n3. **Financial Sentiment Analysis**:\n   - Fei et al. (2023) demonstrated the effectiveness of LLMs in financial sentiment analysis.\n   \n4. **Prompting Strategies**:\n   - Zero-shot and few-shot prompting techniques have been studied by Kojima et al. (2022) and Wei et al. (2022), respectively, highlighting their role in guiding the output of naive LLMs.\n   \n5. **Semantic Textual Similarity (STS)**:\n   - Majumder et al. (2016) and Slimani (2013) discussed the importance of STS in text classification and topic extraction, which are fundamental for information comparison.\n   - Gatto et al. (2023) examined the use of generative LLMs to assess semantic similarity through various prompting strategies.\n\n6. **Retrieval Augmented Generation (RAG)**:\n   - Ram et al. (2023) and Trivedi et al. (2022) covered the enhancement of responses generated by LLMs through RAG by providing additional context based on user queries.\n\n7. **Applications in Specific Domains**:\n   - In the medical domain, Guo et al. (2023) applied generative LLMs to compare scientific abstracts for review papers.\n   - In the financial domain, Ni et al. (2023) created chatReport, a system comparing corporate sustainability reports with guideline documents using RAG.\n\n###Conclusion\nThis research aims to leverage these advances and address the gaps by developing ASCEnd, which seeks to facilitate complex, scalable information comparisons through generative LLMs, thereby enabling more informed decision-making in financial services and beyond.",
        "methodology": "The **ASCEnd system** is designed to assist users by providing insights through automated information comparison using a comprehensive methodology that involves several advanced AI components. It operates by comparing a given text corpus against user-defined criteria to ascertain the relevance of each document with respect to a specified topic. Below is a detailed breakdown of the **methodology**:\n\n### Key Components:\n\n1. **Document Summarization (DS) Module**:\n   - **Function**: This module performs iterative abstractive summarization on each document within the text corpus.\n   - **Purpose**: The goal is to condense the documents into summaries, making them easier to compare against user-defined criteria.\n\n2. **Criteria Embedding (CE) Module**:\n   - **Function**: This module processes the user-defined criteria by vectorizing and splitting the criteria document into segments.\n   - **Storage**: These vectorized segments are stored in a vector database.\n\n3. **Retrieval Augmented Generation (RAG) Module**:\n   - **Function**: This module facilitates similarity searches between the summaries generated by the DS module and the vectorized criteria from the CE module.\n   - **Process**: It retrieves the top-k relevant passages by leveraging a similarity search mechanism.\n   - **Augmentation**: Combines retrieved passages with the RAG prompt and delivers an augmented output by interfacing with a human-level Language Model (LLM).\n\n4. **Comparison Assessment (CA) Module**:\n   - **Function**: This final module uses the augmented output from the RAG module and the summaries from the DS module.\n   - **Output**: Generates a comprehensive comparison assessment that determines the relevance and aligns the information against the user-defined criteria.\n\n### Innovations:\n\n- **Zero-Shot Prompting**: The ASCEnd system utilizes zero-shot prompting, which enhances flexibility across various applications and eliminates the need for model fine-tuning. This capability enables the system to adapt to different tasks without extensive pre-training.\n\n- **Integration of Advanced AI Tasks**: By incorporating abstractive summarization, binary QA tasks, and reasoning tasks\u2014each effectively deployed with generative LLMs\u2014the system ensures a robust and precise evaluation process.\n\nIn summary, the **ASCEnd system** consists of a multi-module framework that processes input data (documents and criteria), performs similarity searches, and generates detailed comparison assessments by leveraging state-of-the-art AI techniques and models.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n#### Hardware:\n- **GPU:** A6000 GPU\n- **Memory:** 48 GB of RAM\n\n#### Models and Memory:\n- **Loading Method:** GPTQ method (Frantar et al., 2022) for quantizing model parameters to reduce memory load.\n\n#### Model Parameters:\n- **Consistency:** Models set to a temperature of 0 to ensure reproducibility and consistent output formats for all candidate documents.\n\n#### Development Framework:\n- **Framework:** ASCEnd developed using the LangChain (Harrison, 2022) framework for API calls and managing vector database requests.\n\n#### Embedding Model:\n- **Model Used:** Beijing Academy of Artificial Intelligence (BAAI)\u2019s \u201cbge-base-en-v1.5\u201d (Xiao et al., 2023) \n  - **Reason:** Best performing open-source embedding model for the model size, outperforming OpenAI\u2019s \u201ctext-embedding-ada-002\u201d model on the Massive Text Embedding Benchmark (MTEB) leaderboard (Muennighoff et al., 2022).\n\n#### Vector Database:\n- **Database:** FAISS (Facebook AI Similarity Search) (Johnson et al., 2017) \n  - **Efficiency:** FAISS is known for its efficiency in performing similarity searches.\n\n### Main Experimental Results\nThe detailed experimental results were not provided in the provided text. For specific outcomes, more information is needed regarding datasets, baselines, evaluation metrics, and quantifiable experimental results."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "The Document Summarization (DS) module aims to process a financial text corpus and generate standardized-length summaries efficiently using generative LLMs, which is crucial for further information comparison tasks.",
            "experiment_process": "In the experiment, the financial text corpus was preprocessed by splitting each document into 2000-token chunks, with each chunk summarized into 250-token segments. The summarized segments were concatenated to form the final document summary. The maximum length of the final summary was 1250 tokens to remain within the context window and hardware limitations. This setup was designed based on previous research indicating optimal performance with these parameters.",
            "result_discussion": "Runtime errors were avoided by limiting chunks to 2000 tokens, and the summaries retained sufficient information as demonstrated by stable ROUGE scores. The reduced length of 1250-token summaries effectively balanced model context limitations and hardware constraints, though larger summary sizes were infeasible due to memory limits.",
            "ablation_id": "2404.04351v1.No1"
        },
        {
            "research_objective": "The objective is to evaluate the performance of various LLMs in handling machine-level and human-level reasoning tasks within the ASCEnd system, addressing hallucinations and logical inconsistencies.",
            "experiment_process": "Several LLMs were chosen for evaluation: Llama-2 7B, 13B, and 70B, Mistral 7B, GPT 3.5, and GPT 4. Machine-level models included Llama-2 7B, 13B, Mistral 7B, and GPT 3.5, evaluated using ROUGE scoring. Human-level models included Llama-2 70B and GPT 4, and their outputs were assessed through a survey involving 20 model output pairs, comparing responses based on a 5-point scoring system and 5 specific questions.",
            "result_discussion": "ROUGE scores provided systematic information retention assessment for machine-level LLMs, confirming the efficacy of generative models in summarization tasks. Survey feedback demonstrated the superior human-like reasoning of Llama-2 70B and GPT 4, validating their selection for complex reasoning tasks. However, survey bias remains a potential issue.",
            "ablation_id": "2404.04351v1.No2"
        }
    ]
}