{
    "title": "Towards Objectively Benchmarking Social Intelligence for Language Agents at Action Level",
    "abstract": "Prominent large language models have exhibited human-level performance in many domains, even enabling the derived agents to simulate human and social interactions. While practical works have substantiated the practicability of grounding language agents in sandbox simulation or embodied simulators, current social intelligence benchmarks either stay at the language level or use subjective metrics. In pursuit of a more realistic and objective evaluation, we introduce the Social Tasks in Sandbox Simulation (STSS) benchmark, which assesses language agents objectively at the action level by scrutinizing the goal achievements within the multi-agent simulation.\nAdditionally, we sample conversation scenarios to build a language-level benchmark to provide an economically prudent preliminary evaluation and align with prevailing benchmarks. To gauge the significance of agent architecture, we implement a target-driven planning (TDP) module as an adjunct to the existing agent. Our evaluative findings highlight that the STSS benchmark is challenging for state-of-the-art language agents. Furthermore, it effectively discriminates between distinct language agents, suggesting its usefulness as a benchmark for evaluating both language models and agent architectures.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large language models (LLMs) have become increasingly powerful owing to the escalation in model magnitude and technical advancements. Recent arts such as the GPT series (Brown et al., 2020  ###reference_b8###; Ouyang et al., 2022  ###reference_b23###; OpenAI, 2023  ###reference_b22###) showcase human-level performance in diverse tasks within the natural language domain. Capitalizing on the instruction-following and emerging commonsense ability of LLMs, the derived language agents also demonstrate human-level ability in various scenarios (Argyle et al., 2023  ###reference_b4###; Aher et al., 2023  ###reference_b1###; Park et al., 2023  ###reference_b24###; Li et al., 2023  ###reference_b17###; Wang et al., 2023a  ###reference_b36###).\nThe endeavor to craft comprehensive language agents to simulate humans has prompted substantial attention toward the evaluation of social intelligence.\nIn recent times, numerous simulators have been developed for the social simulation of language agents (Park et al., 2022  ###reference_b25###, 2023  ###reference_b24###; Wang et al., 2023c  ###reference_b39###; Li et al., 2023  ###reference_b17###). However, the challenge of quantitatively evaluating the emergent social intelligence in these simulations persists. While previous works have introduced numerous benchmarks and evaluation metrics (Zhou et al., 2023  ###reference_b47###; Li et al., 2023  ###reference_b17###), we contend these evaluations face two fundamental issues. Firstly, these evaluations predominantly focus on the language level, assessing social intelligence through posed questions. However, it remains conceivable that an agent may claim they will perform a certain action without actually committing to it, rendering language-level evaluations inadequate for reflecting actual behaviors. Secondly, many evaluations hinge on subjective metrics, diminishing the reliability of the evaluation results. We argue that designing a social intelligence benchmark that can be evaluated objectively at the action level, rather than subjectively at the language level, is necessary.\nIn this paper, we introduce the Social Tasks in Sandbox Simulation (abbreviated as STSS), an action-level benchmark that assesses task-oriented social intelligence with objective metrics based on the Smallville environment (Park et al., 2023  ###reference_b24###). We craft 30 templates of social tasks across 5 categories with corresponding mechanisms to automatically evaluate agents within the simulation. The overarching architecture of the STSS benchmark is illustrated in Figure 1  ###reference_###, where the agent is initiated with a social task and is measured through goal achievement in the simulation.\nSince the simulation may be economically expensive, we additionally construct a complementary language-level benchmark by capturing chat scenarios within the simulator, as a preliminary benchmark that aligns with existing language-level benchmarks such as SOTOPIA (Zhou et al., 2023  ###reference_b47###). We conduct a comparative analysis between our proposed STSS benchmark and prior works in Table 1  ###reference_###.\nNotably, our benchmark evaluates language agents instead of merely language models, considering that the architecture of agents, i.e., how to use the language models is also important to social intelligence. To verify our hypothesis, we introduce an additional Target-Driven Planning (abbreviated as TDP) module that aims at social tasks for the generative agents (Park et al., 2023  ###reference_b24###).\nWhile our framework is versatile for evaluating language models within a fixed baseline agent architecture, it concurrently serves as a testbed of language agent architectures.\nWe conduct evaluations with various popular LLMs on both levels of our benchmark. The results reveal that the social tasks remain challenging even for cutting-edge models like GPT-4 OpenAI (2023  ###reference_b22###), underscoring the potential space for improvement in language models. Moreover, a well-designed TDP module demonstrates a substantial enhancement in the performance of language agents at both levels, suggesting that our benchmark can also serve as a testbed of agent architectures employing fixed LLM.\nOur contributions can be summarized as follows:\nWe introduce STSS, a two-level benchmark for evaluating the social intelligence of language agents, encompassing an action-level evaluation in sandbox simulations and a preliminary language-level evaluation in interactive conversations.\nWe design a target-driven planning module for language agents to investigate the influence and importance of agent architecture in executing social tasks.\nWe conduct extensive experiments with several state-of-the-art models to gauge the capabilities of existing language agents. The result also suggests the effectiveness of our benchmark in evaluating agent architecture."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Social Intelligence Evaluation",
            "text": "Social Intelligence (SI) is widely recognized as the ability to understand others and to act wisely in social situations (Walker and Foley, 1973 ###reference_b35###). In the pursuit of benchmarking social intelligence, previous arts such as SOCIAL IQA (Sap et al., 2019 ###reference_b27###) and Social-IQ (Zadeh et al., 2019 ###reference_b44###) evaluate the model through question answering (QA) within given contexts. Recent works either proposed specialized benchmarks (Le et al., 2019 ###reference_b15###) for SI or evaluated SI on LLMs (Sap et al., 2022 ###reference_b26###; Shapira et al., 2023a ###reference_b28###, b ###reference_b29###), both revealing imperfections of current LLMs. Moving beyond static and non-interactive QA, recent research advocates evaluating LLMs in interactive benchmarks (Lee et al., 2023 ###reference_b16###; Zhou et al., 2023 ###reference_b47###). Originating from social requirements, target-oriented dialogue has been investigated both in terms of model and benchmark (Wang et al., 2019 ###reference_b38###; Hosseini-Asl et al., 2020 ###reference_b12###; \u2020 et al.(2022)(FAIR)\u2020, Bakhtin, Brown, Dinan, Farina, Flaherty, Fried, Goff, Gray, Hu et al., FAIR ###reference_b10###; Tiwari et al., 2023 ###reference_b33###). Moreover, numerous studies also address the quality of conversations, such as the personalization of the chatbot (Zhang et al., 2018 ###reference_b46###; Liu et al., 2020 ###reference_b18###; Ait Baha et al., 2023 ###reference_b2###). Taking a step further, SOTOPIA (Zhou et al., 2023 ###reference_b47###) introduces explicit social goals/persona on the agents. However, existing benchmarks either operate solely at the language level or rely on subjective evaluation metrics. In contrast, our STSS benchmark introduces an objective metric that assesses both language and grounded actions, thereby enhancing the reliability of the evaluation.\n\nArtificial Intelligence (AI) in healthcare is rapidly evolving, offering transformative potentials in diagnostics, patient care, and personalized medicine (Esteva et al., 2019; Topol, 2019). Despite advancements in AI-based healthcare solutions, challenges remain in integrating these technologies with existing health systems, ensuring ethical standards, and addressing biases within AI models (Yu et al., 2018)."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Language agents",
            "text": "There has been a growing interest in constructing language agents by integrating them with external environments and anchoring the language to actions (Andreas, 2022  ###reference_b3###; Wang et al., 2023b  ###reference_b37###). Recent studies demonstrate that language agents can undertake complex tasks, including game playing (Wang et al., 2023a  ###reference_b36###), acting as research assistants (Ziems et al., 2023  ###reference_b48###; Bran et al., 2023  ###reference_b6###), robotic planning (Huang et al., 2022  ###reference_b14###; Brohan et al., 2023  ###reference_b7###), and even simulating humans (Aher et al., 2023  ###reference_b1###).\nA mass of benchmarks and environments have been built for evaluating the language agents. ALFWorld (Shridhar et al., 2021  ###reference_b32###) established the bridge between language and embodied robotics tasks. AgentBench (Liu et al., 2023a  ###reference_b19###) provides a comprehensive benchmark including 8 tasks. Motivated by the human-like social behaviors of the language agents, there also have been social simulators allowing multi-agent interaction in a shared environment (Park et al., 2022  ###reference_b25###, 2023  ###reference_b24###; Gao et al., 2023  ###reference_b11###; Li et al., 2023  ###reference_b17###).\nIt has been substantiated that the utilization of LLMs can significantly influence the performance of language agents. Chain-of-Thought(CoT) (Wei et al., 2022  ###reference_b40###), along with various other works (Yao et al., 2023  ###reference_b42###; Besta et al., 2023  ###reference_b5###; Chia et al., 2023  ###reference_b9###; Nori et al., 2023  ###reference_b21###) demonstrates the significance of prompt designation to the performance of LLMs. Advanced agent architecture such as ReAct (Yao et al., 2022  ###reference_b43###) and Relfexion (Shinn et al., 2023  ###reference_b30###) further improve the performance in both pre-hoc and post-hoc ways  (Huang et al., 2023  ###reference_b13###). BOLAA (Liu et al., 2023b  ###reference_b20###) systematically investigates agent architectures and concludes that a well-designed architecture with dedicated modules for specific tasks can significantly enhance agent performance. For human simulation, Generative Agents Park et al. (2023  ###reference_b24###) designs a complete pipeline and preliminarily verifies the idea of social agents.\nWhile simulations have provided a decent testbed of social behavior, few works attempt to objectively evaluate the social intelligence of language agents quantitatively. Our STSS benchmark fills the gap between social intelligence evaluation and human behavior simulation, with the potential to generalize to more realistic scenarios such as embodied tasks and even real-world applications."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Social Tasks in Sandbox Simulation",
            "text": "To objectively assess the social intelligence of language agents at the action level, we suggest building benchmarks within interactive simulators. The simulator can concretize the textual output into tangible actions, facilitating a more objective and quantitative evaluation through analysis of the state and trajectory of the simulation environment.\nWe extend the interactive sandbox environment and the generative agent architecture introduced by Park et al. (2023  ###reference_b24###) to accommodate the social intelligence evaluation. Specifically, we devise a set of social task templates and corresponding metrics, which can be quantified by analyzing the actions of the agents. Take the example of holding a Valentine\u2019s Day party, the metric could be the number of participants at the correct time and location.\nThese templates are then instantiated to specific tasks, each associated with corresponding initial states of the environment, being available for assessing language agents within the simulation.\nThe performance is assessed through the trajectories in the simulation. Each type of task is associated with a set of rules to analyze the agent trajectories including their actions and positions, and generate a quantitative score to measure the success of the social task.\nFor collective activities including public activity, online activity, and inviting companions, scores are determined based on the ratio between the actual and expected number of participants. For appointment, the agent receives a full score if the goal was perfectly achieved and a half score if the location is mismatched. For asking for help, the score is the ratio of correctly performed sub-tasks. More details can be found in the appendix.\nFor all the tasks, a keyword filter is applied to ensure that agents execute relevant actions and filter out irrelevant agents who accidentally pass by.\nWe clarify that our benchmark presently concentrates on objective and event-oriented evaluation, i.e., we only take the success of the task into account, excluding the measurement of other aspects such as caring about subjective feelings, personalization, or higher-level motivations. We leave the holistic evaluation of both subjective feelings and high-level social goals as future work.\n###figure_1### To better align the conversation tasks with the simulations, we adopt an event-oriented assessment for the generated conversations. For each type of task, a set of goal achievement conditions is designed to measure the conversation, e.g., correctly conveying or reaching a consensus on the time or location of the event. The measurement of such conditions resembles reading comprehension problems and can be solved by language models. Compared to grading the conversation in continuous space, the goal conditions present binary classification problems, which can be easily aligned to human evaluation.\nWe introduce two metrics in the situational conversation benchmark:\nSuccess rate (SR). A task is considered successful if and only if the agent achieves all the goal conditions (for example, conveying all the key information such as time and location). The score is either 1 or 0 for each task.\nGoal condition SR (GCSR). We borrow the idea of goal-condition success from embodied vision-and-language tasks (Shridhar et al., 2020  ###reference_b31###), where the score is the ratio of achieved conditions. For example, in the public activity tasks, if the agent (1) successfully makes the invitation; (2) appropriately informs the location; and (3) forgets to inform the time, then the goal-condition score will be .\nIn consideration of the practical application, the produced conversation is expected to be not only informative but also fluent, clear, and concise. To encourage more effective conversations, we additionally adopt summary-level evaluation, where the conversion is first summarised by the other person (act by an improved version of the generative agent), and then the same metrics are applied to evaluate whether the summary includes the key information. The summary-level evaluation additionally inspects the quality of utterance. For example, even if containing key information, chaotic or blurry expressions may hinder the listeners from correctly summarizing the information, let alone performing the actions.\nWe also note that the number of conversations can differ sharply across tasks, e.g., there will be only one valid conversation in appointment tasks, but a task where a social butterfly is holding a party may include up to 10 conversations. To enrich the dimension of evaluation, we employ micro average and macro average for the scores. Let  be the set of tasks,  denote the conversation set of task , and  be the score of a conversation , the micro average score is defined as:\nAnd the macro score averages across tasks, being defined as:"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Task Designation",
            "text": "We aim to evaluate several fundamental capabilities in social interaction, including planning, expression, and negotiation. Confronting with a social task, the agent naturally needs to formulate a comprehensive plan and execute necessary actions such as guest invitations and delivering key messages during conversations, requiring the ability of planning across various domains.\nWhen conversing with others, the capabilities of expression and negotiation exhibit their significance. The agent not only needs to express their thoughts and convey information sufficiently and clearly but also needs to negotiate with others when encountering matters to be determined. Any omission or miscommunication could hamper the success of the task, even the other agent might still make verbal consent.\nDriven by the need for objective action-level evaluation, the design of task templates focuses on the triad of who, when, and where. Broadly, we design task templates that are categorized into five distinct types facing different scenarios with diverse constraints:\nPublic activity with a fixed time and a fixed location, such as a Valentine\u2019s Day party or a public lecture. The agent is required to invite as many other agents as possible to the activity, explicitly expressing the designated time and location in each invitation. The objective metric is the number of participants who attend at the correct location during the specified time.\nAppointment with unconstrained time and location, e.g., a student is trying to schedule a discussion with the professor. The agent needs to locate the target person and arrange the appointment to accomplish the designated purpose. The challenge lies in negotiating with the target person to figure out a convenient time and location for both agents through conversation. The criterion is whether they execute the required action at the same place and time.\nInviting companions for an activity with unconstrained time and a fixed location, such as gathering friends for a shopping outing. The agent is tasked with inviting others to the activity and negotiating the timing. The metric assesses how many participants joined the activity at the correct location, with the initiator being a mandatory attendee.\nOnline activity with a fixed time and no location requirement, encompassing scenarios like an online workshop or playing online games together. In contrast to public activities, online activities do not necessitate participants to appear in a specific physical location.\nAsking for help, incorporating a series of activities at fixed locations that have no time constraint. In this category of tasks, the agent is assumed to be occupied or unable to do something, seeking assistance from others. For example, Wolfgang, facing an upcoming exam, needs a friend to borrow books from the library and deliver the books to his dormitory. The criterion evaluates whether other people execute the specific behavior correctly, e.g., first borrowing books at the library and then delivering the books at the dormitory of the student.\nWe incorporate a total of 30 social activity templates in the benchmark across the five categories, where more details of the categories are elaborated in Table 2  ###reference_###. The task templates designate the task type, activity name and description, and other task-related information. They can then be instantiated to specific tasks by determining the variables such as the background characters.\nTo simplify, we evaluate only one social task in each complete simulation, where the task is assigned to a single agent at the beginning and the other background characters remain agnostic to the task.\nTo manage the computational cost, we only sample 10-12 background characters in each simulation.\nThe performance is assessed through the trajectories in the simulation. Each type of task is associated with a set of rules to analyze the agent trajectories including their actions and positions, and generate a quantitative score to measure the success of the social task.\nFor collective activities including public activity, online activity, and inviting companions, scores are determined based on the ratio between the actual and expected number of participants. For appointment, the agent receives a full score if the goal was perfectly achieved and a half score if the location is mismatched. For asking for help, the score is the ratio of correctly performed sub-tasks. More details can be found in the appendix.\nFor all the tasks, a keyword filter is applied to ensure that agents execute relevant actions and filter out irrelevant agents who accidentally pass by.\nWe clarify that our benchmark presently concentrates on objective and event-oriented evaluation, i.e., we only take the success of the task into account, excluding the measurement of other aspects such as caring about subjective feelings, personalization, or higher-level motivations. We leave the holistic evaluation of both subjective feelings and high-level social goals as future work.\n###figure_2###"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Language-Level Benchmark",
            "text": "While comprehensive simulation-based evaluation is effective and objective, it may be expensive in terms of both time and economic sense. Besides, implementing a language agent to simulate human-like behavior in environments such as generative agents (Park et al., 2023  ###reference_b24###) may impose a relatively high minimum requirement on the capabilities of the language model. This hinders the universal availability of the benchmark. To mitigate this constraint, we suggest building an accompanying benchmark that focuses on situational dialogue, which is a critical capability for social intelligence.\nA conversation task is defined as a tuple (task, performer, target character) and is curated by freezing the state from simulations. We collect 325 conversation scenarios from numerous simulations of 30 tasks instantiated from the templates. To ensure diversity, each task includes at most two conversation instances for the same pair of characters (although scenarios may already have different background information). For appointment tasks, only conversations between the task performer and the target person are incorporated into the benchmark.\nIn contrast to previous conversation benchmarks (Zhou et al., 2023  ###reference_b47###; Lee et al., 2023  ###reference_b16###), our conversation tasks are sliced samples from complete simulations and the characters are associated with self-consistent memories in the simulation worlds. These memories provide more diversity and naturalness than purely synthetic tasks.\nTo better align the conversation tasks with the simulations, we adopt an event-oriented assessment for the generated conversations. For each type of task, a set of goal achievement conditions is designed to measure the conversation, e.g., correctly conveying or reaching a consensus on the time or location of the event. The measurement of such conditions resembles reading comprehension problems and can be solved by language models. Compared to grading the conversation in continuous space, the goal conditions present binary classification problems, which can be easily aligned to human evaluation.\nWe introduce two metrics in the situational conversation benchmark:\nSuccess rate (SR). A task is considered successful if and only if the agent achieves all the goal conditions (for example, conveying all the key information such as time and location). The score is either 1 or 0 for each task.\nGoal condition SR (GCSR). We borrow the idea of goal-condition success from embodied vision-and-language tasks (Shridhar et al., 2020  ###reference_b31###  ###reference_b31###), where the score is the ratio of achieved conditions. For example, in the public activity tasks, if the agent (1) successfully makes the invitation; (2) appropriately informs the location; and (3) forgets to inform the time, then the goal-condition score will be .\nIn consideration of the practical application, the produced conversation is expected to be not only informative but also fluent, clear, and concise. To encourage more effective conversations, we additionally adopt summary-level evaluation, where the conversion is first summarised by the other person (act by an improved version of the generative agent), and then the same metrics are applied to evaluate whether the summary includes the key information. The summary-level evaluation additionally inspects the quality of utterance. For example, even if containing key information, chaotic or blurry expressions may hinder the listeners from correctly summarizing the information, let alone performing the actions.\nWe also note that the number of conversations can differ sharply across tasks, e.g., there will be only one valid conversation in appointment tasks, but a task where a social butterfly is holding a party may include up to 10 conversations. To enrich the dimension of evaluation, we employ micro average and macro average for the scores. Let  be the set of tasks,  denote the conversation set of task , and  be the score of a conversation , the micro average score is defined as:\nAnd the macro score averages across tasks, being defined as:"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Task Driven Planning",
            "text": "Our benchmark focuses on evaluating language agents rather than merely language models. Consistent with prior research findings (Liu et al., 2023b  ###reference_b20###), we assert that designing specialist agent architecture is important to the application of language models. To enhance language models in social tasks, we propose an additional module namely target-driven planning (TDP) for the generative agent. We implement a simple baseline, as presented in Figure 2  ###reference_###, where the language model is first asked to provide a general plan, and then offer specific thoughts for daily planning and conversation. These thoughts are then injected into the corresponding module of the generative agent. The TDP module is also applicable to the accompanying conversation benchmark, although only the conversation module is active.\nOur implementation does not incorporate advanced prompt techniques such as CoT (Wei et al., 2022  ###reference_b40###) and\nReAct (Yao et al., 2022  ###reference_b43###). We clarify that our implementation primarily serves as a baseline to inspect the significance of language agent architectures in performing social tasks. There remains ample room for improvement in the designation of agent architectures."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this section, we present the experiments conducted on our benchmarks to assess the social intelligence of language agents through performing social tasks. Our experimentation centers on addressing two key questions: (1) To what extent can existing language models and agents execute social tasks within our STSS benchmark? (2) What is the significance of the agent architecture in the context of social intelligence?"
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Experiment Setup",
            "text": "We incorporate 4 language models in the evaluation: GPT-35 111gpt-35-turbo-0613 (Ouyang et al., 2022  ###reference_b23###), GPT-4 222gpt-4-0613 (OpenAI, 2023  ###reference_b22###), Llama-2-13b-chat (Touvron et al., 2023  ###reference_b34###), and Baichuan-2-13b-chat Yang et al. (2023  ###reference_b41###).\nFor the agents, we utilize prompt templates and parameters of generative agents (Park et al., 2023  ###reference_b24###) with minor adjustments for robustness in both levels of the benchmark.\nIn the simulation, GPT-35 is employed as the language model for background characters, as this practice is known to provide human-level intelligence (Park et al., 2023  ###reference_b24###). The evaluated model is used to generate target-driven plans, conversations, and high-level planning for the performer agent, while the low-level simulation is delegated to GPT-35.\nFor language-level evaluation, the task performer is operated by the evaluated language agent, while the others are handled by vanilla generative agents with GPT-35. We also use GPT-35 as the evaluator of conversation and summary.\nDue to the minimum requirement of the instruction-following ability in the simulation, we exclusively GPT-35 and GPT-4 in the complete simulation. Since Llama2 and Baichuan2 exhibit instability when following the complex instructions from the generative agent, we simplify the prompt template of conversation generation in their evaluation. Please refer to the appendix for more details about the experimental setup to reproduce the empirical results.\nIn addition, to study the significance of the agent architecture, we conduct experiments on all models using the vanilla generative agent (GA) and the one with target-driven planning (GA + TDP). For GA, we adhere to the practice of Park et al. (2023  ###reference_b24###), injecting the social goal description into the currently field of the agent persona."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Overall performance",
            "text": "We present the language-level evaluation results in Table 3  ###reference_### and the action-level evaluation in Table 4  ###reference_###.\nIn general, at the language level, GPT-4 performs the best, followed by GPT-35, Baichuan2, and Llama2.\nWhen evaluating the conversation, GPT-4 with TDP achieves a success rate of 81.2% and a goal-condition success rate of 90.2%, demonstrating its robust ability in target-oriented conversations. Surprisingly, Baichuan2 outperforms GPT-35 in the vanilla setting, although GPT-35 performs better with the TDP. We hypothesize that Baichuan2 possesses a better innate inferential capability for planning, whereas GPT-35 excels in generating higher-quality English utterances.\nEven though GPT-35 surprisingly performs comparably to GPT-4 with TDP in the conversation-level evaluation, it falls short when evaluating the summary, implying GPT-4 still produces higher-quality content.\nThe consistent discrepancy between conversation and summary scores indicates a ubiquitous need for improvement in conversation quality.\nWhile GPT-4 showcases commendable performance in the sampled situational dialogues, executing social tasks in a sandbox simulation remains challenging. Even the most proficient agent (GPT-4 with TDP) only achieves an average score of 0.550. When comparing task types, we find that making appointments and inviting companions emerge as the most challenging tasks. In these tasks, agents must inform others and negotiate regarding time or locations, necessitating the seamless integration of all abilities mentioned in Section 3.1  ###reference_###."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Comparing Language-Level and Action-Level Evaluations",
            "text": "We observe a general consistency between the action-level and the language-level evaluations, as reflected in the positive correlation between the scores achieved by each agent. However, all agents experience a decrease in performance from language-level to action-level.\nThis supports our hypothesis that the sandbox simulation is more complex and challenging. By inspecting the number of initiated conversations, we identify threefold challenges: (1) The agent may not always meet enough other agents to chat with in the simulation, which depends on its itinerary and fundamentally its ability to plan; (2) The agent may struggle to effectively communicate information or determine an appropriate time and location during a conversation; (3) Even when information is accurately conveyed and understood, other agents may not always correctly ground it into actions.\nSandboxes can simulate the nature of the real world, exposing new drawbacks of language agents that can not be reflected by language-level evaluation alone. For instance, GPT-4 with TDP unexpectedly failed in an inviting companions task where Wolfgang needs to invite friends to exercise with at Johnson Park. In the simulation, Wolfgang achieved a verbal agreement with several agents to meet at Johnson Park tomorrow at 6:00 am. However, despite the appointment, most agents failed to wake up until 7:00 am the next day, resulting in the appointment being missed.\nHi Tom, how\u2019s your day going? I\u2019m planning a Valentine\u2019s Day party at the cafe tomorrow. Would you like to come?\nHi Tom, I hope you\u2019re doing well. I\u2019m planning a Valentine\u2019s Day party at Hobbs Cafe tomorrow at 7pm. I would love it if you could come and bring some friends along. The more, the merrier!"
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Effectiveness of TDP",
            "text": "For all language models, the GA + TDP architecture consistently outperforms the vanilla GA. The TDP module yields notable improvements in both levels of evaluation, and can even compensate for the shortage of model capabilities. For instance, GPT-35 and Baichuan2 agents with the TDP module can outperform the vanilla GA using GPT-4. Despite GPT-35 being empirically weaker than GPT-4, their conversation scores become almost comparable after equipping the TDP module. This suggests that when a language model exhibits capabilities above a certain threshold, the agent architecture may be the more noteworthy part for further improvement.\nThe case of Isabella\u2019s Valentine\u2019s Day party at Hobbs Cafe, previously presented in Figure 2  ###reference_###, provides insight into the performance boost facilitated by TDP.\nA comparison between invitations from Isabella with/without the TDP module is demonstrated in Table 5  ###reference_###. The conversation reminder generated by the TDP prompts the agent to clearly state the date, time, and location of the party, which is vital for the success of the task. The invitation generated with the TDP also encourages the invitee to bring more friends, further facilitating the success of the social task. Please refer to the appendix for more details."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we introduce the Social Tasks in Sandbox Simulation (STSS) benchmark, which is designed to assess language agents by engaging them in social tasks within a sandbox simulation. The accompanying language-level benchmark also serves as a preliminary evaluation for weaker models. Additionally, we propose a target-driven planning module for generative agents to investigate the significance of designing a specialized agent architecture in social intelligence. A comprehensive evaluation involving four prominent language models validates the efficacy of our benchmark for assessing both language models and agent architectures."
        }
    ],
    "url": "http://arxiv.org/html/2404.05337v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "4"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.1",
            "5.2",
            "5.3",
            "5.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "1",
            "5.3",
            "5.4"
        ]
    },
    "research_context": {
        "paper_id": "2404.05337v1",
        "paper_title": "Towards Objectively Benchmarking Social Intelligence for Language Agents at Action Level",
        "research_background": "### Motivation:\n\nThe paper is motivated by the evolving capabilities of large language models (LLMs) like the GPT series, which have demonstrated human-level performance in a variety of natural language tasks. As the field progresses, crafting comprehensive language agents that can simulate human behavior has garnered significant interest, particularly in the realm of evaluating social intelligence. Despite numerous benchmarks and evaluation metrics introduced by prior research, current evaluations predominantly focus on the language level and rely heavily on subjective metrics. This approach is seen as inadequate since it doesn\u2019t necessarily reflect actual behaviors and actions. Instead, the authors argue for the necessity of an objective benchmark that evaluates social intelligence at the action level.\n\n### Research Problem:\n\nThe research problem addressed in this paper is the quantitative evaluation of emergent social intelligence in language agents. Specifically, the paper identifies two main issues with current evaluation methods:\n1. Existing evaluations are predominantly language-level assessments, which pose questions to the agents but do not effectively measure if the agents genuinely carry out the actions they claim they would.\n2. Many evaluations rely on subjective metrics, which can compromise the reliability of the results.\n\nTo resolve these issues, the paper proposes designing a social intelligence benchmark that can be evaluated objectively at the action level rather than subjectively at the language level.\n\n### Relevant Prior Work:\n\n1. **Large Language Models (LLMs) Performance:**\n   - The GPT series has showcased human-level capabilities in a wide range of tasks within the natural language domain (Brown et al., 2020; Ouyang et al., 2022; OpenAI, 2023).\n\n2. **Language Agents\u2019 Abilities:**\n   - Derived language agents exhibit human-level abilities in a variety of scenarios (Argyle et al., 2023; Aher et al., 2023; Park et al., 2023; Li et al., 2023; Wang et al., 2023a).\n\n3. **Simulators for Social Simulation:**\n   - Multiple social simulators have been created to simulate language agents (Park et al., 2022, 2023; Wang et al., 2023c; Li et al., 2023).\n\n4. **Benchmarks and Evaluation Metrics:**\n   - Prior works have proposed numerous benchmarks and evaluation metrics, although they primarily evaluate at the language level and tend to be subjective (Zhou et al., 2023; Li et al., 2023).\n\nThe paper contributes a new benchmark called Social Tasks in Sandbox Simulation (STSS), which emphasizes action-level evaluation and introduces an objective metric to address the highlighted shortcomings of existing benchmarks.",
        "methodology": "### Methodology:\n\n#### Proposed Benchmark:\nTo objectively assess the social intelligence of language agents at the action level, a novel benchmarking approach using interactive simulators is proposed. These simulators translate textual outputs into tangible actions, allowing for a quantitative evaluation through the state and trajectory analysis of the simulation environment.\n\n#### Integration and Extension:\nBuilding upon the interactive sandbox environment and the generative agent architecture introduced by Park et al. (2023), modifications have been made to fit the social intelligence evaluation. A set of social task templates and corresponding metrics are devised, which enable quantifiable analysis based on the agents' actions. For instance, in a task like hosting a Valentine\u2019s Day party, a metric could be the number of attendees arriving at the correct time and location.\n\n#### Task and Evaluation:\nTasks are instantiated with precise initial states within the simulation to assess language agents. Performance is measured through action trajectories, with specific rules established for each task type to generate quantitative scores. \n\n- **Collective Activities:** Tasks like public activities, online activities, and inviting companions are scored based on the ratio between actual and expected participant numbers.\n- **Appointments:** Full scores are given for complete success (achievement of goals), whereas only half scores are awarded if there\u2019s a location mismatch.\n- **Help Requests:** Scores are determined by the ratio of correctly completed sub-tasks.\n\nA keyword filter is utilized to ensure agents engage in relevant actions, filtering out irrelevant agents mistakenly passing by.\n\n#### Objective Focus:\nThe benchmark emphasizes objective and event-oriented evaluations by concentrating on task success, excluding subjective feelings, personalization, or higher-level motivations, which are suggested as future work.\n\n#### Conversation Task Alignment:\nFor conversation tasks, an event-oriented assessment with goal achievement conditions is implemented, akin to reading comprehension problems solvable by language models. \n\nMetrics introduced include: \n- **Success Rate (SR):** A binary metric where tasks are either deemed successful (score of 1) or unsuccessful (score of 0) if all goal conditions are met.\n- **Goal Condition SR (GCSR):** A metric borrowed from embodied vision-and-language tasks, scoring based on the ratio of achieved conditions.\n\n#### Summary-Level Evaluation:\nTo enhance conversational effectiveness, a summary-level evaluation by an improved generative agent is adopted. This ensures that the conversation summaries are informative, fluent, clear, and concise, thereby promoting effective communication.\n\n#### Conversation Frequency and Scoring:\nGiven the varying number of conversations across tasks (e.g., fewer for appointments versus many for social events like parties), both micro and macro averages are utilized for scoring. The micro average score aggregates scores of individual conversations, whereas the macro average score averages across different tasks.\n\n#### Scoring Definitions:\n- **Micro Average Score:** Aggregation of all conversation scores within the set of tasks.\n- **Macro Average Score:** Averaging across tasks to account for score variations due to the number of conversations per task.\n\nOverall, this methodology provides a structured way to objectively benchmark the social intelligence of language agents through actionable and quantifiable means within interactive simulations.",
        "main_experiment_and_results": "**Main Experiment Setup:**\n\nThe main experiment in the study evaluates the ability of language models and agents to perform social tasks as defined by the Social Task Suite for Social Skills (STSS) benchmark. This setup particularly focuses on examining the degree to which current language models and agents can carry out these tasks and assessing the impact of different agent architectures on social intelligence.\n\n1. **Datasets:**\n   - The experiment utilizes the STSS benchmark, which comprises various social tasks designed to test the social intelligence of language agents. \n   \n2. **Baselines:**\n   - The language models and agents considered as baselines in the experiment include prevalent and advanced models in the field. Specific names or versions of these models are not detailed in the provided text, but they are likely popular models like GPT, BERT, or other state-of-the-art language processing agents.\n\n3. **Evaluation Metrics:**\n   - Performance on the social tasks is measured using metrics tailored to the benchmarks. Such metrics typically assess the accuracy, relevance, and appropriateness of the agents' responses in the context of the social interactions defined by STSS.\n\n**Main Experimental Results:**\n\nThe results of the main experiment reveal the following:\n\n- **Execution of Social Tasks:** Existing language models and agents show varying degrees of success in performing social tasks outlined in the STSS benchmark. The results suggest that while some models can handle certain aspects of social intelligence, others struggle, thereby highlighting gaps in current capabilities.\n  \n- **Significance of Agent Architecture:** The architecture of the agents significantly influences their performance in social intelligence tasks. Different architectural choices impact how well an agent can understand and respond to social cues and contexts, underscoring the importance of architecture in developing socially intelligent agents.\n\nThese findings underscore the current limitations and potential of existing language models in terms of social intelligence, pointing towards areas for further research and improvement."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "The goal is to compare the effectiveness of language-level evaluations and action-level evaluations for language agents in sandbox simulations.",
            "experiment_process": "A comparative study is conducted between the language-level evaluations and the action-level evaluations using the Social Tasks in Sandbox Simulation (STSS) benchmark. The experiments involve various state-of-the-art language agents, including GPT-4, tested across both levels. The evaluation metrics are based on goal achievements within the simulation, and the study examines the number of initiated conversations and task completions.",
            "result_discussion": "The study finds a general consistency between language-level and action-level evaluations, though a noticeable decrease in performance is observed from language-level to action-level evaluations. The sandbox simulation reveals unique challenges: meeting other agents, effective communication, and grounding information into actions. An example showcases GPT-4 with TDP failing a task due to agents' failure to wake up on time despite verbal agreements, highlighting complexities unobservable in language-level evaluations alone.",
            "ablation_id": "2404.05337v1.No1"
        },
        {
            "research_objective": "To assess the impact of the Target-Driven Planning (TDP) module on the performance of language agents.",
            "experiment_process": "Language models are tested within a generative agent (GA) architecture with and without the TDP module across both language-level and action-level evaluations. The models assessed include GPT-35, Baichuan2, and GPT-4. Performance metrics involve evaluating conversation success, task completion rates, and specific social scenarios such as party invitations.",
            "result_discussion": "The findings reveal that the GA + TDP architecture consistently outperforms the vanilla GA across all language models. The TDP module significantly enhances evaluation scores, even allowing empirically weaker models (e.g., GPT-35) to perform on par with stronger ones (e.g., GPT-4). The TDP module improves task performance by generating clearer, more detailed conversational reminders, thus emphasizing the importance of agent architecture alongside model capabilities.",
            "ablation_id": "2404.05337v1.No2"
        }
    ]
}