{
    "title": "Global Contrastive Training for Multimodal Electronic Health Records with Language Supervision",
    "abstract": "Modern electronic health records (EHRs) hold immense promise in tracking personalized patient health trajectories through sequential deep learning, owing to their extensive breadth, scale, and temporal granularity. Nonetheless, how to effectively leverage multiple modalities from EHRs poses significant challenges, given its complex characteristics such as high dimensionality, multimodality, sparsity, varied recording frequencies, and temporal irregularities. To this end, this paper introduces a novel multimodal contrastive learning framework, specifically focusing on medical time series and clinical notes. To tackle the challenge of sparsity and irregular time intervals in medical time series, the framework integrates temporal cross-attention transformers with a dynamic embedding and tokenization scheme for learning multimodal feature representations. To harness the interconnected relationships between medical time series and clinical notes, the framework equips a global contrastive loss, aligning a patient\u2019s multimodal feature representations with the corresponding discharge summaries. Since discharge summaries uniquely pertain to individual patients and represent a holistic view of the patient\u2019s hospital stay, machine learning models are led to learn discriminative multimodal features via global contrasting. Extensive experiments with a real-world EHR dataset demonstrated that our framework outperformed state-of-the-art approaches on the exemplar task of predicting the occurrence of nine postoperative complications for more than 120,000 major inpatient surgeries using multimodal data from UF health system split among three hospitals (UF Health Gainesville, UF Health Jacksonville, and UF Health Jacksonville-North).",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Electronic health records (EHRs) contain important information about patient encounters that support real-world healthcare delivery [1  ###reference_b1###]. While artificial intelligence and machine learning have the potential to support clinical decision-making based on contextual representations of patient data [2  ###reference_b2###], modeling real-world EHRs remains challenging.\nOne of the challenges lies in modeling multivariate medical time series in EHRs, which inherently characterized by sparsity and irregular time intervals [3  ###reference_b3###] . Popular approaches such as recurrent neural networks (RNN) with long short-term memory (LSTM) [4  ###reference_b4###] and gated recurrent networks [5  ###reference_b5###] seek to account for the temporal complexities of medical time series, but may be suboptimal when learning long-term (e.g., over the duration of an entire hospital stay) temporal dynamics of patient health trajectories [6  ###reference_b6###]. Recently, transformers have been used for modeling temporal EHR data [7  ###reference_b7###, 8  ###reference_b8###, 9  ###reference_b9###] and have been established as state-of-the-art approaches for predicting clinical outcomes from patient data sequences [6  ###reference_b6###, 10  ###reference_b10###]. However, additional challenges persist when modeling EHR data with transformers, such as capturing temporal dependency across very long sequences [11  ###reference_b11###] and modeling heterogeneous dependencies across variables [12  ###reference_b12###].\nAnother challenge lies in effectively leveraging different modalities in EHRs and ensuring that their clinical aspects are meaningfully represented [13  ###reference_b13###]. The complexity increases with the multimodal nature of EHR data, encompassing diverse clinical data from medical time series to radiology imaging data and unstructured clinical notes. Recent studies have shown the promise of using contrastive pre-training to jointly model different modalities for various multimodal clinical prediction tasks [14  ###reference_b14###, 15  ###reference_b15###]. Given two or more modalities, these contrastive pre-training methods generally learn bi-modal similarity scores, so that the data (e.g., chest radiographs and radiology report) from the same patient are pulled closer while those from different patients are pushed away. While effective, these methods may result in suboptimal learning when applied to real-world datasets, particularly when different modalities only capture specific perspectives of the patient\u2019s overall health trajectory [16  ###reference_b16###]. Therefore, in such scenarios, suboptimal performance maybe achieved when attempting to align two unimodal representations lacking sufficient shared information (See Figure 1  ###reference_###).\n###figure_1### To this end, we propose a novel global contrastive learning framework for multiomdal EHRs, specifically focusing on medical time series and clinical notes. To tackle the challenge of sparsity and irregular time intervals in medical time series, the framework integrates a dynamic embedding and tokenization scheme, using flexible positional encoding and a learnable time embedding to address the challenge of sparsity and irregular sampling, and a variable-specific encoding strategy for capturing distinct characteristics and relationships between temporal variables. To learn multimodal representations from unaligned medical time series and clinical notes, the framework equips a global contrastive loss, aligning a patient\u2019s multimodal feature representations with the corresponding discharge summaries. representing a holistic view of the patient\u2019s hospital stay. We demonstrate the effectiveness of our approach and analyze the relative contributions of each component of our framework using the benchmark task of predicting the onset of multiple postoperative complications following major inpatient surgery with a real-world EHR dataset ."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Multimodal Representation Learning for Health",
            "text": "Combining diverse sources of data sources in medical domain is promising for more comprehensive understanding of patients\u2019 health conditions [17  ###reference_b17###], more accurate health outcome predictions [18  ###reference_b18###], and building next-generation foundational medical models for generative AI [19  ###reference_b19###]. The core of this research effort is multimodal representation learning where all the modalities are projected to a common space while preserving information from the given modalities [20  ###reference_b20###]. Traditional data fusion methods, such as early concatenation fusion and late weighted average fusion [21  ###reference_b21###], are insufficient to learn the correlations and dependencies among different modalities [22  ###reference_b22###]. Recently, transformer-based architecture, thanks to its superior ability to capture cross-modal interactions by self-attention and its variants [23  ###reference_b23###], has achieved great success in various multimodal machine learning tasks in different domains, such as multimodal action recognition [24  ###reference_b24###], image segmentation [25  ###reference_b25###], and affect detection [26  ###reference_b26###]. Despite the promise, additional challenges persist when modeling multimodal EHR data with transformers, such as capturing temporal dependency across very long sequences [11  ###reference_b11###, 27  ###reference_b27###] and modeling heterogeneous dependencies across modalities [12  ###reference_b12###]. \n\nIn the realm of autonomous vehicles, AI has become pivotal in enhancing navigation and safety systems. Advanced algorithms process inputs from multiple sensors such as cameras, LiDAR, and radar, contributing to real-time decision-making and obstacle detection. Challenges remain in ensuring robust performance in diverse driving conditions and handling unpredictable road scenarios [28  ###reference_b28###]."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Contrastive Learning in Health",
            "text": "Contrastive learning is a technique aimed at developing an embedding function capable of encoding input samples, potentially derived from various modalities, in a manner where samples originating from identical categories are proximally aligned, while those from disparate categories are distinctly separated within the embedding space. This approach supports both supervised and self-supervised learning paradigms, offering a versatile framework for data representation. There are a variety of loss functions in the literature that align with the contrastive training objective, starting with Contrastive Loss [28  ###reference_b28###], which takes only pairs of samples from the input and minimizes embedding distance for the samples in the same class and maximizes the distance for samples in different classes. Triplet Loss [29  ###reference_b29###] and N-pair Loss [30  ###reference_b30###] use the samples as anchors and select positive and negative samples with respect to them, minimizing and maximizing the distances between them, respectively. The triplet loss uses only one positive and negative sample per anchor, while the N-pair loss uses multiple negative samples with one positive sample. Contrastive learning is being used for better representing images [31  ###reference_b31###], texts [32  ###reference_b32###] and others by training the respective embeddings on the loss functions. It has also been widely adopted for multimodal representation learning by contrasting positive and negative pairs of instances [33  ###reference_b33###].\nIn the healthcare domain, contrastive learning has demonstrated significant efficacy, particularly in tasks where labeled data is either scarce or entails high curation costs [34  ###reference_b34###]. This approach is not only beneficial in such data-constrained environments but also enhances supervised learning tasks. For instance, Azizi et al. [35  ###reference_b35###] effectively applied contrastive learning for the pretraining of models, subsequently improving their performance in medical image classification tasks. The methodology gained further traction following its successful application in the CLIP model [36  ###reference_b36###], which adeptly aligns images with their textual captions. Its application has since expanded to the realm of multimodal EHRs, facilitating the alignment of disparate data forms, including chest radiographs with radiology reports [14  ###reference_b14###, 37  ###reference_b37###], medical time series with clinical notes [15  ###reference_b15###], ICD codes with clinical notes [38  ###reference_b38###], and the correlation of retinal images with genomic data [39  ###reference_b39###], thereby illustrating the broad applicability of contrastive learning in complex data integration within healthcare."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "Consider a cohort  with multimodal EHRs, where , in which for each patient , the dataset contains the patient\u2019s static tabular data , such as demographic information, a multivariate medical time series , such as physiological vital signs, corresponding unstructured clinical notes , such as history and physical (H&P) notes, and clinical outcomes , such as in-hospital mortality.\nAn overview of our proposed framework for multimodal modeling of EHRs is shown in Figure 2  ###reference_###, which consists of three sub-networks: (1) multimodal feature learning, (2) cross-modal fusion, and (3) model optimization. Next, we will describe the details of each of these sub-networks.\n###figure_2### Multivariate EHR time series contain variables measured at different frequencies. To adapt to this unique challenge, we propose to use non-unique absolute positional indices based on the recorded timestamps so that variable tokens measured at the same time will be assigned the same positional index; in addition, we add a relative positional encoding to each token embedding [40  ###reference_b40###], which can help capture local token dependencies, especially for processing long sequences[41  ###reference_b41###, 42  ###reference_b42###], to model the relationships between clusters of short-term activity across a long timeframe.\nPositional embeddings alone omit critical information about the relative time between events. For applications of transformers to time series, time embeddings can help capture important temporal patterns. We propose to use Time2Vec [43  ###reference_b43###] to learn a model-agnostic vector representation for time. In Time2Vec, a time t is encoded to generate one non-periodic , and one periodic  time dependent vector, where  and  are learnable parameters [44  ###reference_b44###].\nA multivariate clinical time series often includes different categories of health variables (e.g., vital signs, laboratory tests, medications) that tend to exhibit distinct characteristics, numerical ranges, and temporal patterns. To learn the unique characteristics of each time series, we propose to use a separate encoder for each clinical variable for intra-variable temporal dynamics, and then concatenate the outputs of the separate encoders to learn the inter-variable correlation and dependencies.\nWe embedded patient\u2019s static tabular data  and aggregated into each token in the sequence as a default contextualized information. We extended the notion of \u201cCLS\u201d token and prepended the global token to the tokenized sequence. The global token self-attended to all sequence elements, and was used to generate the latent feature representation of medical time series ."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Multimodal Feature Learning",
            "text": "The multimodal feature learning sub-network is designed to effectively learn the latent unimodal feature representation from each type of data in cohort , for medical time series  and clinical notes .\nMultivariate EHR time series contain variables measured at different frequencies. To adapt to this unique challenge, we propose to use non-unique absolute positional indices based on the recorded timestamps so that variable tokens measured at the same time will be assigned the same positional index; in addition, we add a relative positional encoding to each token embedding [40  ###reference_b40###  ###reference_b40###], which can help capture local token dependencies, especially for processing long sequences[41  ###reference_b41###  ###reference_b41###, 42  ###reference_b42###  ###reference_b42###], to model the relationships between clusters of short-term activity across a long timeframe.\nPositional embeddings alone omit critical information about the relative time between events. For applications of transformers to time series, time embeddings can help capture important temporal patterns. We propose to use Time2Vec [43  ###reference_b43###  ###reference_b43###] to learn a model-agnostic vector representation for time. In Time2Vec, a time t is encoded to generate one non-periodic , and one periodic  time dependent vector, where  and  are learnable parameters [44  ###reference_b44###  ###reference_b44###].\nA multivariate clinical time series often includes different categories of health variables (e.g., vital signs, laboratory tests, medications) that tend to exhibit distinct characteristics, numerical ranges, and temporal patterns. To learn the unique characteristics of each time series, we propose to use a separate encoder for each clinical variable for intra-variable temporal dynamics, and then concatenate the outputs of the separate encoders to learn the inter-variable correlation and dependencies.\nWe embedded patient\u2019s static tabular data  and aggregated into each token in the sequence as a default contextualized information. We extended the notion of \u201cCLS\u201d token and prepended the global token to the tokenized sequence. The global token self-attended to all sequence elements, and was used to generate the latent feature representation of medical time series ."
        },
        {
            "section_id": "3.1.1",
            "parent_section_id": "3.1",
            "section_name": "3.1.1 Medical Time Series",
            "text": "Learning useful representations of medical time series  is challenging due to its high dimensionality, sparsity, irregular and variable-specific recording frequency, and timestamp duplication when multiple measurements are recorded simultaneously. Popular approaches such as recurrent neural networks (RNN) with long short-term memory (LSTM) [4  ###reference_b4###] and gated recurrent networks [5  ###reference_b5###] do not account for the temporal complexities of EHR data and may be suboptimal when learning temporal dynamics of patient health trajectories. To tackle this challenge, we adopted and modified transformer-based models by introducing a dynamic embedding and tokenization scheme to enable transformers to adapt to the above-mentioned unique challenges of medical time series.\nMultivariate EHR time series contain variables measured at different frequencies. To adapt to this unique challenge, we propose to use non-unique absolute positional indices based on the recorded timestamps so that variable tokens measured at the same time will be assigned the same positional index; in addition, we add a relative positional encoding to each token embedding [40  ###reference_b40###  ###reference_b40###  ###reference_b40###], which can help capture local token dependencies, especially for processing long sequences[41  ###reference_b41###  ###reference_b41###  ###reference_b41###, 42  ###reference_b42###  ###reference_b42###  ###reference_b42###], to model the relationships between clusters of short-term activity across a long timeframe.\nPositional embeddings alone omit critical information about the relative time between events. For applications of transformers to time series, time embeddings can help capture important temporal patterns. We propose to use Time2Vec [43  ###reference_b43###  ###reference_b43###  ###reference_b43###] to learn a model-agnostic vector representation for time. In Time2Vec, a time t is encoded to generate one non-periodic , and one periodic  time dependent vector, where  and  are learnable parameters [44  ###reference_b44###  ###reference_b44###  ###reference_b44###].\nA multivariate clinical time series often includes different categories of health variables (e.g., vital signs, laboratory tests, medications) that tend to exhibit distinct characteristics, numerical ranges, and temporal patterns. To learn the unique characteristics of each time series, we propose to use a separate encoder for each clinical variable for intra-variable temporal dynamics, and then concatenate the outputs of the separate encoders to learn the inter-variable correlation and dependencies.\nWe embedded patient\u2019s static tabular data  and aggregated into each token in the sequence as a default contextualized information. We extended the notion of \u201cCLS\u201d token and prepended the global token to the tokenized sequence. The global token self-attended to all sequence elements, and was used to generate the latent feature representation of medical time series ."
        },
        {
            "section_id": "3.1.2",
            "parent_section_id": "3.1",
            "section_name": "3.1.2 Clinical Notes",
            "text": "We encoded clinical notes with pre-trained clinical large language models. Clinical large language models are pre-trained to learn generally useful representations from the knowledge encoded in medical corpora, and have shown state-of-the-art performance in medical NLP tasks such as medical information extraction and question answering [45  ###reference_b45###]. In this work, we experimented with popular clinical large language models including BioBERT [46  ###reference_b46###], Clinical BERT [47  ###reference_b47###], Clinical Longformer [48  ###reference_b48###], and GatorTron [49  ###reference_b49###], and selected the one which yielded the best performance.\nThe outputs from time series transformer encoder  and clinical notes encoder  are later passed through a linear projection layer respectively. This step maps each encoder\u2019s representation to the multimodal embedding space, preparing the inputs for the next cross-modal fusion sub-network."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Cross-modal Fusion",
            "text": "To learn multimodal representations, we merged the embeddings of medical time series  and clinical notes  using a validated cross-attention-based approach [50  ###reference_b50###], in which each feature embedding in one modality is enriched by searching for the most relevant feature in the other modality. For example,\nrepresent a scoring matrix, whose -th element measures the attention given by the information from the -th time step from modality  and the -th time step from modality . Then, , the final enriched feature sequence for  as:\nwhere , , and  denote Query, Key, and Value, respectively. Finally, the enriched feature sequence  and  are concatenated and projected to lower feature space as  for downstream classification or prediction tasks with linear layers:"
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Contrasting Multimodal Representation and Discharge Summaries",
            "text": "Previous research in the medical field employing contrastive learning has yielded encouraging outcomes, particularly in contrasting cross-modal data such as medical images and texts [14  ###reference_b14###]. A key factor contributing to the effective joint training of medical images and texts lies in the significant shared semantics observed between chest radiographs and their corresponding radiology reports [51  ###reference_b51###], with the latter serving as explicit textual representations of the former. However, the application of contrastive learning to medical time series and clinical notes presents a distinct challenge. The semantic relationship between these modalities is not necessarily one of direct shared meaning but rather of a complementary nature, with each modality offering a unique perspective on the patient\u2019s health trajectory, insights not seamlessly interchangeable between the modalities, underscoring the complexity of their semantic relationship [52  ###reference_b52###]. Medical time series, typically recorded in intensive care units (ICU), provide a snapshot of a patient\u2019s vital signs over a brief interval. In contrast, clinical notes span the entirety of a patient\u2019s hospital stay, from admission to discharge, encompassing a diverse array of information, including surgery procedures and medications. Although both modalities hold clinical significance, their disparate nature may result in limited shared information, potentially hindering the efficacy of inter-modality alignment and leading to suboptimal learning outcomes when employing a straightforward contrasting approach.\nTo this end, we propose to use the multimodal latent representation  from an individual patient  to pair with the patient\u2019s corresponding discharge summary as the contrastive objective. Employing multimodal representations for contrastive learning is recognized for its efficacy in capturing a global perspective of semantic information within patient health records [53  ###reference_b53###]. An essential step is to define appropriate positive and negative samples for each patient\u2019s multimodal representation . The rationale for selecting discharge summaries lies in their comprehensive nature, encapsulating an all-encompassing overview of a patient\u2019s entire hospitalization [54  ###reference_b54###]. Discharge summaries are particularly rich in information, often encompassing patient demographics, social context, details of admission, physical exam findings, diagnoses, and specifics of any therapeutic or diagnostic procedures undertaken, along with prescribed medications and the summary of hospital course [55  ###reference_b55###]. This wealth of information aligns well with the varied data modalities present in EHRs. Moreover, discharge summaries are commonly excluded from the development of predictive models using multimodal EHRs, primarily due to concerns about data leakage, particularly as they often contain conclusive assessments of patients\u2019 critical conditions [56  ###reference_b56###]. This exclusion, however, positions them as a potentially advantageous addition for learning objectives during model training. Finally, since each patient has a unique corresponding discharge summary, in doing so, we can easily set the positive pairs to the batch size  during training. We define our contrastive learning objective as the addition of two asymmetric losses:\n\nwhere  denotes the loss contrasting multimodal representation with discharge summaries, and  denotes the loss contrasting discharge summaries with multimodal representation.  is calculated as:\n\nand  is calculated as:\n\nHere  is cosine similarity,  is the temperature hyperparameter modulating distribution\u2019s concentration and Softmax function\u2019s gradient, and  is the batch size. By minimizing this loss, the similarity between each patient\u2019s multimodal representation and the corresponding discharge summaries increased while the similarity between the multimodal representation and any other patients\u2019 discharge summaries are decreased.\nThe final loss function for model optimization  is a weighted sum of the contrastive loss  and the cross-entropy loss  for any downstream tasks between ground truth and prediction:\nwhere  and  are parameters that balance the different loss terms [38  ###reference_b38###]."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Improving Discharge Summaries with LLMs",
            "text": "Discharge summaries represent a holistic view of a patient\u2019s visit beginning admission through discharge, hence corresponds well to the patient\u2019s clinical notes, it may not correspond to medical time series well. Medical time series are collected in high-frequency and the hidden temporal patterns from medical time series are important indicators to a patient\u2019s health trajectory [57  ###reference_b57###]. Yet, discharge summaries often omit the textual description for medical time series. Hence, this could lead to suboptimal contrastive learning performance when aligning multimodal representation  with discharge summaries.\nTo better align the textual semantics of discharge notes with multimodal representation , we propose to further improve discharge summaries with additional zero-shot LLM-generated textual descriptions for medical time series. An example of prompting templates is shown in Figure  3  ###reference_###\n###figure_3### . In this example, we prompted an LLM to answer the question with regard to a specific medical time series, with the role of LLM set as \u201ca professor in medicine\u201d. We also provided the LLM with the downstream learning task (in this case, \u201cpredicting postoperative surgery risk using intraoperative medical time series\u201d) as a general context, and the ground truth for a patient in the learning task as an individualized context (in this case, \u201cAKI\u201d). Additional instructions were included to alleviate hallucination. Finally, we prompted the LLM with the question regarding the temporal patterns of a specific physiological vital sign.\nAfter generating the textual description for medical time series from LLM, we combined it with the patient\u2019s discharge summaries, and utilized the improved discharge summaries for each patient to contrast with the patient\u2019s multimodal representation . With the additional inserted texts regarding medical time series, the improved discharge summaries are designed to be more contextualized matched with the patient\u2019s multimodal representation  compared to the original discharge summaries."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Cohort Selection and Data Preprocessing",
            "text": "In this section, we describe the retrospective dataset for evaluating our approach on the benchmark task of predicting multiple in-hospital complications of major inpatient surgery using a real-world EHR dataset.\nOur dataset consists of complete EHR records for all major inpatient surgeries occurring at three medical centers (UF Health Gainseville, UF Health Jacksonville, and UF Health North Jacksonville) between 2014 and 2019. The combined cohort consisted of 113,953 adult patients who underwent 124,777 inpatient surgeries. When a patient had multiple surgeries during one admission, only the first surgery was included. For each inpatient surgery, our dataset consists of:\n9 preoperative demographic and admission information from 113,953 patients, including age (Mean 51 y, Min 18 y, Max 106 y), sex (48% male, 52% female), language, ethnicity, race, smoking status, zip code, and body mass index.\n14 intraoperative temporal vital signs, including systolic blood pressure, diastolic blood pressure, mean arterial pressure, heart rate, respiratory rate, oxygen flow rate, fraction of inspired oxygen (FIO2), oxygen saturation (SPO2), end-tidal carbon dioxide (ETCO2), minimum alveolar concentration (MAC), positive end-expiratory pressure (PEEP), peak inspiratory pressure (PIP), tidal volume, and body temperature.\n173 types of all preoperative and intraopertive clinical notes for an encounter, such as History and Physical (H&P notes) and operative reports.\n9 major postoperative complications [57  ###reference_b57###] as binary clinical outcomes, the incidence of complications include 23.29% ICU stay (for 48 h or more), 13.09% acute kidney injury, 8.64% prolonged mechanical ventilation, 2.00% in-hospital mortality, 13.48% wound complications, 15.09% neurological complications, 8.20% sepsis, 12.18% cardiovascular complications, and 4.51% venous thromboembolism.\nFor demographic and admission information, we converted each to one-hot vectors, and concatenated with remaining numerical values. Missing static features was imputed with cohort medians.\nFor 14 intraoperative time series data, their variable names were converted to unique integer identifiers; the measured values for each variable were normalized to zero mean and unit variance based on the values from the training set; their measurement time, in the format of \u201cmonth/day/year hour:min:sec\u201d, were first converted to unix timestamps and then also normalized similarly. For absolute positional indices, we assign one integer positional index for each token yet not enforcing the restriction that positional indices are unique and if different variables were measured at the same time. For relative positional embeddings, we generated the relative positional representation based on the GitHub code for the original paper [40  ###reference_b40###]. The maximum sequence length of tokens in our dataset is 14,126, and the mean sequence length is 2,023.\nFor clinical notes, in the preprocessing phase, we merged all types of notes per surgery, converted the text to lowercase, and removed special characters and de-identification placeholders. Subsequently, we generated embeddings by first tokenizing the whole text using the clinically pretrained tokenizer. The tokens were then chunked to fit the pretrained clinical LLM, and the last hidden layer output for the CLS token was extracted as the embedding for each chunk. The final representation for each surgery was obtained by calculating the average of all these embeddings. We fixated on the Clinical Longformer [48  ###reference_b48###] for generating the embeddings due to its superior performance in classifying with clinical notes, following extensive testing with various models from Huggingface including BioBERT [46  ###reference_b46###], BiomedBERT [58  ###reference_b58###], ClinicalBERT [47  ###reference_b47###], Clinical Longformer [48  ###reference_b48###], and GatorTron [49  ###reference_b49###]."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": ""
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Benchmark Multitask Classification",
            "text": "The goal is to predict the onset of nine postoperative complications following major inpatient surgery: prolonged ( 48 hours) intensive care unit (ICU) stay, acute kidney injury (AKI), prolonged mechanical ventilation (MV), wound complications, neurological complications, sepsis, cardiovascular complications, venous thromboembolism (VTE), and in-hospital mortality. Models are trained on data available in the EHR up to the recorded surgery end timestamp.\nOur model was trained with the multi-task fashion for predicting 9 postoperative outcomes. To do this, we expanded the notion of \u201c[CLS]\u201d token for text classification and prepended 9 global tokens to our tokenized sequences, one for each of our postoperative outcomes, so that self-attentions were computed among all sequence elements for each clinical outcome token."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Experimental Setup",
            "text": "We used the following hyperparameters for optimization and regularization, Adam optimizer with a learning rate of 1e-4, dropout of 0.2, and weight decay of 1e-4. For the transformer models, including the Longformer, we limited the models to only 1 attention head and 1 layer per head, as this configuration produced the best results. We trained the models on two NVIDIA A100-SXM4-80GB GPUs for 30 epochs to leverage hardware acceleration. We used a batch size of 32 per GPU for the best performing model."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Examining the Performance of Modeling Medical Time Series",
            "text": "In this paper, we first proposed a novel dynamic embedding and tokenization scheme to modeling medical time series, which introduces three novelties to existing approaches: a flexible positional encoding, a learnable time encoding, and variable-specific encoding. In this work, we experimented with three types of variable-specific encoders, including 1-D convolutional encoder [59  ###reference_b59###], transformer encoder [9  ###reference_b9###], and linear encoder. This subsection examines the performance of modeling clinical time series by comparing the model trained with the dynamic tokenization scheme introduced in this paper with a few widely adopted baselines: (1) Tokenized gated recurrent units (GRUs) with attention: GRU is a popular sequential network for tasks involving modeling clinical time series [60  ###reference_b60###, 61  ###reference_b61###]. (2) Tokenized XGBoost: XGBoost gradient boosting algorithm employs gradient boosting on decision trees for regression and classification tasks [62  ###reference_b62###, 63  ###reference_b63###]. (3) BEHRT [7  ###reference_b7###]: BEHRT is a transformer-based model with traditional tokenization scheme, a widely used baseline transformer model for EHR data. (4): Hi-BEHRT [11  ###reference_b11###]: Hi-BEHRT is a hierarchical transformer model extending from BEHRT, specifically designed for processing longer EHR sequences. Hi-BEHRT uses a sliding window to segment the full sequence into smaller segments and applied transformers as local feature extractor for the temporal interaction within each segment. (5) Self-supervised Transformer for Time-Series (STraTS) [9  ###reference_b9###]: STraTS uses a unique transformer to encode each variable and then uses a self-attention layer to generate the time-series embedding."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Examining the Performance of Multimodal Contrastive Learning",
            "text": "Another novelty of this work is the contrastive loss between multimodal representation and improved discharge summaries by LLMs. Despite the success of multimodal contrastive learning between medical images and clinical notes (e.g., MedCLIP [14  ###reference_b14###]), extending this to the alignment between medical time series and clinical notes remains challenging, due to the potential lack of shared semantic information between these two modalities as described in Subsection 3.3  ###reference_###. In this paper, we took a different approach and proposed to align multimodal representation with the semantics of corresponding discharge summaries. We compared the performance of the models trained with our approach with the baseline model incorporating the inter-modality contrastive loss. The inter-modality contrastive loss calculates the bi-modal contrastive loss pairing each two modalities of data in EHRs during the training process. This baseline approach extends the CLIP model [36  ###reference_b36###] and have been widely adopted on the task of pre-training between medical time series and clinical notes [15  ###reference_b15###] and between medical images and clinical notes. We also trained the model only using the ground truth labels in a supervised fashion without the contrastive loss.\nModel\nMean\nICU\nAKI\nMV\nMortality\nWound\nNeurological\nSepsis\nCardiovascular\nVTE\n\n\n\nGRU + Attention [61  ###reference_b61###]\n0.771\n0.857\n0.718\n0.783\n0.816\n0.712\n0.753\n0.791\n0.762\n0.747\n\nXGBoost [63  ###reference_b63###]\n0.765\n0.851\n0.716\n0.771\n0.815\n0.709\n0.748\n0.788\n0.760\n0.727\n\nBEHRT [7  ###reference_b7###]\n0.749\n0.843\n0.701\n0.765\n0.800\n0.701\n0.725\n0.770\n0.748\n0.699\n\nHi-BEHRT [11  ###reference_b11###]\n0.781\n0.863\n0.730\n0.789\n0.835\n0.721\n0.769\n0.801\n0.780\n0.769\n\nSTraTS [9  ###reference_b9###]\n0.797\n0.881\n0.742\n0.803\n0.857\n0.734\n0.797\n0.813\n0.791\n0.772\n\nLongformer + Single-shared Encoder\n0.780\n0.860\n0.731\n0.787\n0.832\n0.718\n0.765\n0.798\n0.781\n0.766\n\nLongformer + Variable-specific Encoder (1-D CNN)\n0.796\n0.880\n0.743\n0.797\n0.850\n0.734\n0.794\n0.808\n0.793\n0.771\n\nLongformer + Variable-specific Encoder (Transformer)\n0.800\n0.882\n0.745\n0.806\n0.854\n0.736\n0.798\n0.810\n0.795\n0.772\n\nLongformer + Variable-specific Encoder (Linear)\n0.798\n0.880\n0.743\n0.803\n0.853\n0.735\n0.796\n0.808\n0.793\n0.771\n\nLongformer + Variable-specific Encoder (Linear) + Time Embedding\n0.801\n0.883\n0.749\n0.810\n0.853\n0.739\n0.800\n0.811\n0.797\n0.774\nTraining Loss\nMean\nICU\nAKI\nMV\nMortality\nWound\nNeurological\nSepsis\nCardiovascular\nVTE\n\n\n\nCross-entropy loss (time series + notes  ground-truth) [64  ###reference_b64###]\n0.845\n0.908\n0.781\n0.845\n0.905\n0.780\n0.855\n0.882\n0.823\n0.838\n\nInter-modality contrastive loss (time series  notes ) [15  ###reference_b15###]\n0.738\n0.812\n0.687\n0.756\n0.813\n0.660\n0.750\n0.795\n0.722\n0.736\n\nCross-entropy loss + Inter-modality contrastive loss [38  ###reference_b38###]\n0.843\n0.908\n0.780\n0.843\n0.901\n0.780\n0.850\n0.880\n0.818\n0.836\n\nMultimodal contrastive loss (time series + notes  discharge summaries)\n0.803\n0.877\n0.752\n0.821\n0.878\n0.725\n0.815\n0.860\n0.787\n0.801\n\nCross-entropy loss + Multimodal contrastive loss\n0.859\n0.916\n0.801\n0.855\n0.915\n0.789\n0.871\n0.894\n0.831\n0.849\n\nCross-entropy loss + Multimodal contrastive loss (LLM-improved)\n0.862\n0.918\n0.805\n0.857\n0.918\n0.796\n0.875\n0.904\n0.834\n0.851"
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Results and Discussion",
            "text": "This paper proposed a contrastive learning framework for modeling multimodal EHRs, specifically focusing on medical time series and clinical notes. To tackle the challenge of modeling medical time series, we modified Longformer and introduced a dynamic embedding and tokenization scheme. For improving the multimodal fusion of medical time series and clinical notes, we proposed to use the multimodal contrastive loss (medical time series + clinical notes) with discharge summaries, instead of aligning between these two modalities. This section summarizes and discusses the experimental results for the framework."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "The Dynamic Tokenization and Embedding Scheme for Time Series",
            "text": "Table 1  ###reference_### compares the area under receiver operating characteristic (AUROC) scores of our proposed Longformer-based approach with several popular baselines for time series modeling. As shown in the table, our dynamic tokenization scheme-based Longformer model with additional variable-specific encoders and time embedding outperformed all baseline models with the highest mean AUROC of 0.801. STraTS [9  ###reference_b9###] slightly under-performed our approach, suggesting the effectiveness of incorporating extra relative positional embeddings in our approach. With the same tokenized sequence, GRU + Attention (AUROC: 0.771) performed better than transformer models with traditional tokenization scheme (AUROC: 0.749), indicating the superior performance from transformer-based models for this task.\nWe further ablated the variable-specific encoder and time embedding to examine the benefits brought by each of these components. For encoding each variable separately, as described in Subsection 3.1.1  ###reference_.SSS1###, we introduced variable-specific encoders to Longformer for capturing temporal dynamics within each variable, instead of using a single-shared encoder. Table 1  ###reference_### reported the performance of using Longformer with the default single-shared encoder against Longformer with three different types of variable-specific encoders (1-D convolutional layers [59  ###reference_b59###] and transformer layers [9  ###reference_b9###]). The results showed the effectiveness of using a variable-specific encoder indicated by the performance lift, and the model performance of using different types of encoders were similar (For 1-D CNN, transformer, and linear, the mean AUROC score was 0.796, 0.800, and 0.798, respectively). Considering the time complexity of 1-D CNN and transformer encoders as well as generalizing this approach to high-counts of temporal variables, we chose linear encoder for the experiments reported in the rest of the paper. For examining the effectiveness of time embedding, experimental results of adding additional time embedding further improved the AUROC by 0.03."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Intermodal Contrasting versus Multimodal Contrasting",
            "text": "Table 2  ###reference_### compares the AUROC scores of our proposed multimodal contrastive learning approach with several baseline training objectives. As shown in the table, we first experimented several baseline models trained with the cross-entropy loss (between ground truth postoperative outcomes and prediction) and inter-modality contrastive loss. The models only trained with inter-modality contrastive loss yielded the poor AUROC score of 0.738, indicating low extent of shared semantic information between medical time series and clinical notes in our dataset. In addition, the model trained with the combined loss of cross-entropy and inter-modality contrasting (AUROC of 0.843) did not outperform the model only trained with the cross-entropy loss (AUROC of 0.845).\nThe models trained with our approach (multimodal representation contrasting to dis- charge summaries) achieved better performance. The models only trained with the proposed multimodal contrastive loss achieved the AUROC score of 0.803; the model trained with the combined loss of cross-entropy and multimodal contrasting outperformed all baseline models with the AUROC score of 0.859. The models trained with the LLM-improved discharge summaries performed the best, establishing a state-of-the-art mean AUROC of 0.862 for this task.\nAlthough the traditional intermodal contrastive learning is successful in benchmarking public datasets such as MIMIC-III [65  ###reference_b65###], it achieved a relatively poor performance on our dataset, where medical time series were patients\u2019 physiological vitals signs measured during surgery and clinical notes were taken throughout the encounter. In our dataset, the textual descriptions of vital signs appeared only in progress notes, in which the most recent values were auto-populated. Compared to the time series with high-frequency recording, textual descriptions do not provide useful information related to the temporal patterns. Therefore, these two modalities of data lack enough shared information for machine learning models to align, although they were both clinically relevant.\nThis problem has also been discussed in recent literature, as intermodal contrasting based on unimodal representation could be inaccurate, failing to reveal the global information in multimodal objects [53  ###reference_b53###]. Compared to the traditional inter-modal contrastive learning, our framework took a different approach by using patients\u2019 discharge summaries as a global supervision, directly aligning with multimodal representation combining medical time series and clinical notes, establishing the state-of-the-art performance on this task. In addition, our approach is easy to scale up, incorporating diverse modalities of health data, as intermodal contrastive learning requires quadratic time complexity as the number of modalities in EHRs increases."
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "Improving Discharge Notes with LLMs",
            "text": "Considering the scarcity of textual descriptions about medical time series in discharge summaries, our framework used LLMs to generate relevant texts describing these temporal dynamics. Similar LLM-based text augmentation approaches have also been successfully studied in other tasks, such as in-context rewriting for more robust sentiment analysis [66  ###reference_b66###].\nIn this study, we prompted \u201cgpt-4-1106-preview\u201d to generate the temporal patterns for each of the vital signs in our dataset, using patients\u2019 diagnoses as context. This was designed for adding more contextual information to medical time series by inserting texts related to temporal patterns of medical time series during the model training process, in which machine learning models were trained to align the multimodal representation from both medical time series and clinical notes with discharge summaries. By improving the discharge summaries, the experimental results showed a slight performance improvement."
        },
        {
            "section_id": "6.4",
            "parent_section_id": "6",
            "section_name": "Limitations and Future Work",
            "text": "This work has several important limitations. First, the global contrastive learning framework was designed for the challenge of information unalignment in multimodal EHRs, a common problem with medical time series and clinical notes in real-world EHR datasets. It has not yet been applied to other modalities in EHRs, such as medical images, medication orders, and lab results. Future work needs to investigate the generalizability of the proposed framework on other datasets with more modalities of EHR data. Second, the framework is not suitable for online applications in prospective, in-patient early prediction tasks since it requires discharge summaries as the contrasting learning objective. A prospective dataset would likely be adversely affected by backlogging of vital signs and delays in documentation (e.g., for examples, procedure notes for inpatient surgery are typically written after surgery end time). Nevertheless, our experiments serve as a proof-of-concept for using clinical text as a contrastive medium for multimodal datasets. Third, discharge summaries themselves are not necessarily the ground truth for clinical events during a hospital stay and suffer from many of the limitations of clinical notes, including incompleteness and factual inaccuracies. Finally, the prompting methods for improving discharge summaries with LLMs was relatively simple. Future work needs to investigate more effective prompting techniques for improving discharge summaries."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "EHRs offer potential for tracking personalized patient health trajectories using deep learning, but face challenges due to EHR data being characterized by high dimensionality, sparsity, irregular time intervals, and multiple modalities. Traditional contrastive pre-training methods have shown promise in jointly modeling multiple modalities for clinical prediction tasks, but they may be sub-optimal when the modalities represent specific perspectives of a patient\u2019s overall health trajectory, lacking enough shared information for alignment. To address these challenges, this paper introduces a global contrastive learning framework for multimodal electronic health records using temporal cross-attention transformers with a dynamic embedding and tokenization scheme , and a global contrastive loss to align a patient\u2019s multimodal feature representations to discharge summaries. Using a real-world dataset with multimodal data collected to predict postoperative complications, our framework established a new state-of-the-art performance."
        }
    ],
    "url": "http://arxiv.org/html/2404.06723v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.1.1",
            "3.1.2",
            "3.2",
            "3.3",
            "3.4"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "5",
            "5.1",
            "5.2",
            "5.3",
            "5.4",
            "6",
            "6.1",
            "6.2",
            "6.3",
            "6.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "6.1",
            "6.2",
            "5.3",
            "5.4"
        ]
    },
    "research_context": {
        "paper_id": "2404.06723v1",
        "paper_title": "Global Contrastive Training for Multimodal Electronic Health Records with Language Supervision",
        "research_background": "### Paper\u2019s Motivation:\n\nThe primary motivation for this paper is to address the challenges of modeling real-world electronic health records (EHRs) to support clinical decision-making effectively. Current approaches struggle with the complexities inherent to EHRs, such as the sparsity, irregular time intervals, and multimodal nature of the data. The authors aim to develop a more effective framework for handling these challenges, specifically focusing on medical time series and clinical notes within EHRs, to enhance predictive modeling and overall healthcare delivery.\n\n### Research Problem:\n\nThe research problem centers on two main challenges:\n1. **Modeling medical time series data within EHRs**: Current models like recurrent neural networks (RNNs) and transformers face difficulties in capturing long-term temporal dynamics and heterogeneous dependencies in medical time series.\n2. **Leveraging multimodal data in EHRs**: Existing contrastive pre-training methods for multimodal EHR data often fail to optimally align different modalities due to the limited shared information and heterogeneous nature of the data.\n\nThe proposed solution involves developing a global contrastive learning framework that effectively integrates medical time series and clinical notes to capture a comprehensive representation of a patient\u2019s health trajectory.\n\n### Relevant Prior Work:\n\n1. **Challenges in EHR Modeling**: Prior work has highlighted the difficulties in handling sparsity and irregular time intervals in medical time series (Reference 3).\n2. **Current Approaches**: Traditional models like RNNs with long short-term memory (LSTM) (References 4, 5) and more modern approaches like transformers (References 7, 8, 9) have been used to model temporal EHR data, although they have limitations in capturing long-term dependencies and heterogeneous variable relationships.\n3. **Multimodal EHR Data**: Previous studies (References 14, 15) have shown the potential of contrastive pre-training for multimodal clinical prediction tasks. These methods use bi-modal similarity scores to align data from different modalities but may perform suboptimally when those modalities capture only specific aspects of the patient\u2019s health (Reference 16).\n\nBy building upon these prior efforts, the proposed framework seeks to introduce innovations in dynamic embedding, tokenization, and global contrastive loss to better handle the complexities of EHR data.",
        "methodology": "### Methodology\n\nIn this study, we propose a method for multimodal Electronic Health Records (EHR) modeling, leveraging global contrastive training with language supervision. Consider a cohort with multimodal EHRs, where each patient \\( p \\) has the following data:\n- Static tabular data \\( T_p \\), such as demographic information.\n- Multivariate medical time series \\( S_p \\), such as physiological vital signs.\n- Unstructured clinical notes \\( U_p \\), such as history and physical (H&P) notes.\n- Clinical outcomes \\( R_p \\), such as in-hospital mortality.\n\nOur framework consists of three main sub-networks:\n1. Multimodal Feature Learning\n2. Cross-Modal Fusion\n3. Model Optimization\n\n#### Multimodal Feature Learning\n\nMultivariate EHR time series involve variables recorded at different frequencies. To address this, we propose using non-unique absolute positional indices based on recorded timestamps. This approach assigns the same positional index to variable tokens measured simultaneously. Additionally, we apply relative positional encoding to each token embedding, following the work of [reference_b40###], which is particularly useful for handling long sequences by capturing local token dependencies. Incorporating these techniques helps model relationships between short-term activities over extended periods [reference_b41###, reference_b42###].\n\nTime embeddings are crucial for temporal representations in transformer models. To capture significant temporal patterns, we adopt Time2Vec [reference_b43###], encoding time \\( t \\) into both non-periodic \\( W_0t+b_0 \\) and periodic \\( f(W_k t + b_k) \\) components, where \\( W \\) and \\( b \\) are learnable parameters [reference_b44###].\n\nGiven the diverse nature of health variables in multivariate clinical time series (e.g., vital signs, lab tests, medications), we suggest using separate encoders for each clinical variable to learn intra-variable temporal dynamics. The outputs of these encoders are then concatenated to capture inter-variable correlations and dependencies.\n\n#### Cross-Modal Fusion\n\nFor static tabular data \\( T_p \\), we embed and aggregate this data into each token in the sequence, integrating it as contextualized information. We extend the concept of a \u201cCLS\u201d token by prepending a global token to the tokenized sequence. This global token self-attends to all sequence elements, acting as a summary vector that generates the latent feature representation of the medical time series \\( F_p \\).\n\n#### Model Optimization\n\nIn the optimization phase, we utilize contrastive learning with language supervision to align representations across various modalities effectively. This technique aims to enhance the alignment between multimodal data and clinical outcomes, ultimately improving the model\u2019s predictive performance on critical clinical tasks such as in-hospital mortality prediction. \n\nBy following the outlined methodologies and employing innovative strategies like non-unique absolute positional indices, Time2Vec embeddings, and separate encoders for distinct clinical variables, our framework aims to improve the robustness and accuracy of multimodal EHR modeling.",
        "main_experiment_and_results": "**Main Experiment Setup and Results**\n\n**1. Dataset:**\nThe dataset comprises complete electronic health records (EHR) for major inpatient surgeries at three medical centers (UF Health Gainesville, UF Health Jacksonville, and UF Health North Jacksonville) from 2014 to 2019. The dataset includes:\n- **Patients:** 113,953 adults who underwent 124,777 inpatient surgeries. Only the first surgery per patient per admission was included.\n- **Inputs:**\n  - **Demographic and Admission Information (9 types):** Age, sex, language, ethnicity, race, smoking status, zip code, body mass index.\n  - **Intraoperative Temporal Vital Signs (14 types):** Variables such as systolic blood pressure, heart rate, and oxygen saturation.\n  - **Clinical Notes (173 types):** Various notes per encounter, like History and Physical notes and operative reports.\n- **Outputs:**\n  - **Postoperative Complications (9 types):** Binary outcomes like ICU stay (>48 hours), acute kidney injury, prolonged mechanical ventilation, in-hospital mortality, wound complications, neurological complications, sepsis, cardiovascular complications, and venous thromboembolism.\n\n**2. Data Preprocessing:**\n- **Demographic and Admission Information:** Converted to one-hot vectors and concatenated with numerical values; missing features imputed with cohort medians.\n- **Intraoperative Time Series:**\n  - Variable names: Converted to unique integers.\n  - Measurement values and timestamps: Normalized to zero mean and unit variance.\n  - Positional indices: Assigned integer positional indices without enforcing uniqueness.\n  - Relative positional embeddings: Generated using an existing codebase.\n- **Clinical Notes:**\n  - Text Preprocessing: Merged per surgery, lowercased, and cleaned.\n  - Tokenization: Used clinically pretrained tokenizer.\n  - Embeddings: Generated using Clinical Longformer with average embeddings from text chunks, following extensive testing of various models.\n\n**3. Baselines:**\nWhile the exact baselines are not mentioned, the paper alludes to extensive testing with various models from Huggingface, including notable models like BioBERT, BiomedBERT, ClinicalBERT, Clinical Longformer, and GatorTron. It implies that these transformer-based models have been utilized as reference points for evaluating embedding quality.\n\n**4. Evaluation Metrics:**\nThe paper focuses on predicting multiple in-hospital complications. Although specific metrics are not listed in the provided text, common evaluation metrics for such predictive tasks would include:\n- **Accuracy:** Measures the proportion of true results among the total cases examined.\n- **Precision, Recall, and F1-Score:** For binary classifications of complications.\n- **ROC-AUC (Receiver Operating Characteristic - Area Under Curve):** Measures overall performance of a binary classifier system.\n\n**Main Experimental Results:**\nWhile the detailed results are not provided in the provided text, the choice of Clinical Longformer for generating embeddings implies its superior performance specifically with clinical notes, compared to other model baselines like BioBERT, BiomedBERT, ClinicalBERT, and GatorTron. The results likely emphasize the effectiveness of this model in improving the prediction accuracy of multiple in-hospital complications via better representation of multimodal EHR data."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the performance of the proposed dynamic embedding and tokenization scheme for modeling medical time series.",
            "experiment_process": "We experimented with three types of variable-specific encoders, including 1-D convolutional encoder, transformer encoder, and linear encoder. The comparative models included tokenized gated recurrent units (GRUs) with attention, tokenized XGBoost, BEHRT, Hi-BEHRT, and Self-supervised Transformer for Time-Series (STraTS). These models utilized different techniques and architectures to encode and model the clinical time series data. The performance was evaluated on predicting the occurrence of nine postoperative complications for a dataset stemming from the UF health system using the area under receiver operating characteristic (AUROC) scores.",
            "result_discussion": "Table 1 shows that the Longformer-based model with variable-specific encoders and time embedding attained the highest mean AUROC score of 0.801 across all tasks, outperforming STraTS, which had a mean AUROC of 0.797. GRU + Attention had a mean AUROC of 0.771, indicating that transformer-based models provide superior performance for this task. Further ablations of the variable-specific encoder and time embedding highlighted their benefits, with the models utilizing these components achieving mean AUROCs of 0.796 (1-D CNN), 0.800 (transformer), and 0.798 (linear). The time embedding further improved AUROC by 0.03, indicating its effectiveness.",
            "ablation_id": "2404.06723v1.No1"
        },
        {
            "research_objective": "To compare the performance of the proposed multimodal contrastive loss approach with traditional inter-modality contrastive loss methods.",
            "experiment_process": "We compared several baseline models trained with cross-entropy loss (ground truth postoperative outcomes and predictions), inter-modality contrastive loss, and a combination of both. Contrastive models were trained to align multimodal representation with the semantics of corresponding discharge summaries, evaluated versus models trained without contrastive loss. AUROC scores were computed for these models across occurences of postoperative complications.",
            "result_discussion": "The models trained with traditional inter-modality contrastive loss alone yielded a low mean AUROC score of 0.738. When combined with cross-entropy loss, the AUROC score was 0.843, which did not outperform models trained only with cross-entropy loss (AUROC of 0.845). The proposed multimodal contrastive loss approach achieved a higher mean AUROC of 0.803, and the combination with cross-entropy loss resulted in an AUROC of 0.859. Models trained with LLM-improved discharge summaries attained the best performance, with a mean AUROC of 0.862. The findings suggest that the proposed methodology aligns multimodal data more effectively by leveraging the comprehensive information in discharge summaries.",
            "ablation_id": "2404.06723v1.No2"
        }
    ]
}