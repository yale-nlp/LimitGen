{
    "title": "On the True Distribution Approximation of Minimum Bayes-Risk Decoding",
    "abstract": "Minimum Bayes-risk (MBR) decoding has recently gained renewed attention in text generation.\nMBR decoding considers texts sampled from a model as pseudo-references and selects the text with the highest similarity to the others.\nTherefore, sampling is one of the key elements of MBR decoding, and previous studies reported that the performance varies by sampling methods.\nFrom a theoretical standpoint, this performance variation is likely tied to how closely the samples approximate the true distribution of references.\nHowever, this approximation has not been the subject of in-depth study.\nIn this study, we propose using anomaly detection to measure the degree of approximation.\nWe first closely examine the performance variation and then show that previous hypotheses about samples do not correlate well with the variation, but our introduced anomaly scores do.\nThe results are the first to empirically support the link between the performance and the core assumption of MBR decoding.111The code is available at https://github.com/CyberAgentAILab/mbr-anomaly.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Minimum Bayes-risk (MBR) decoding has recently re-emerged as a better alternative to beam search in text generation such as neural machine translation (NMT), text summarization, and image captioning (Eikema and Aziz, 2020  ###reference_b9###; Freitag et al., 2022  ###reference_b14###; Fernandes et al., 2022  ###reference_b11###; Suzgun et al., 2023  ###reference_b34###; Bertsch et al., 2023  ###reference_b4###).\nMBR decoding first samples texts from a model and then selects the text most similar to the others, considering the text samples as substitutes for references.\nTherefore, sampling plays an important role in MBR decoding, and previous studies have reported that the performance varies by sampling methods (Eikema and Aziz, 2020  ###reference_b9###, 2022  ###reference_b10###; Fernandes et al., 2022  ###reference_b11###; Freitag et al., 2023  ###reference_b13###).\nFrom a theoretical standpoint, the samples are assumed to approximate the true distribution, the distribution of human-quality translations (Kumar and Byrne, 2002  ###reference_b23###, 2004  ###reference_b24###).\nIf the approximation deviates, biases can emerge in results of MBR decoding.\nThis implies a significant link between the performance variation and approximation quality.\nAlthough previous studies explained the performance variation by some properties of samples, e.g., sampling bias and cumulative probability mass (Eikema and Aziz, 2020  ###reference_b9###; Freitag et al., 2023  ###reference_b13###), those properties have no clear relation with the true distribution.\nConsequently, the relation between the performance gains by sampling methods and the core assumption remains unclear.\nThis study aims to empirically support the link between the performance and the approximation of true distribution.\nTo this end, we introduce measures for the degree of approximation.\nIf the assumption for samples holds, references, which are drawn from the true distribution by definition, should not deviate from the majority of the samples.\nBased on this recasting, we employ anomaly detection (also called outlier or novelty detection) for the measure.\nOur hypothesis is that references achieve lower anomaly scores among samples obtained with a higher-performance sampling method.\nWe first closely examine the performance variation by sampling methods.\nThen, we show that the variation highly correlates with the anomaly scores but not so with the properties based on previous hypotheses.\nThe results are the first to provide empirical evidence for the link between the performance and core assumption, which is an important step to understanding the connection between the actual performance and the theory of MBR decoding.\n###figure_1###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Preliminaries",
            "text": "Let  be a utility function to measure the quality of model translation  (candidate; Freitag et al., 2022  ###reference_b14###) given its reference translation .\nAmong a set of candidates , MBR decoding selects the one that minimizes the expected error or, equivalently, maximizes the expected utility (Kumar and Byrne, 2002  ###reference_b23###, 2004  ###reference_b24###; Freitag et al., 2022  ###reference_b14###):\nHere,  is the true distribution over translations of an input text  (Kumar and Byrne, 2002  ###reference_b23###, 2004  ###reference_b24###), which describes human-quality translations in the space of all translations.\nSince the true distribution is unknown, MBR decoding approximates Eq. (1  ###reference_###) with finite samples drawn from a model .\nThat is, MBR decoding assumes that the samples drawn from a model approximate the true distribution of references.\nThe samples are called pseudo-references (Freitag et al., 2022  ###reference_b14###), which subsequently serve as alternatives to references in the computation of MBR as follows:\nIn practice, candidates  and pseudo-references  can be the same or different sets of samples.\nFigure 1  ###reference_### shows an example of the above procedure."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Performance Variation by Sampling",
            "text": "Previous studies reported that performance varies by sampling methods in NMT.\nHowever, they used the same set of model translations for both candidates and pseudo-references (Eikema and Aziz, 2020  ###reference_b9###; Fernandes et al., 2022  ###reference_b11###; Freitag et al., 2023  ###reference_b13###) or explored sampling methods only for candidates (Eikema and Aziz, 2022  ###reference_b10###).\nThese settings obscure the effect of pseudo-references, for which the true distribution is assumed, on the performance variation.\nThis section shows the effect of pseudo-reference sampling on performance by evaluating pseudo-references separately from candidates.\nSince we focus on sampling for pseudo-references, we first search for the best sampling method for candidates and fix it.\nThe objective of searching for the best is to prevent the pseudo-reference\u2019s contribution to scores from being capped and obscured by the candidate\u2019s quality.\nTo this end, we conduct the search on the same newstest19 as the subsequent experiments.444\nIf the objective is to find the best combination of sampling methods, which is not our focus, then it is desirable to use different splits to explore and test the combination to ensure the generalization.\nNevertheless, our subsequent results in Tables 1  ###reference_###, 2  ###reference_###, 4  ###reference_###, and 5  ###reference_### suggest the generalization of the found best combination as it consistently performs the best across almost all language pairs.\n\nFollowing Fernandes et al. (2022  ###reference_b11###), we search for the sampling method that achieves the highest oracle score, , on average.\nTable 1  ###reference_### shows that epsilon sampling achieves the highest oracle score across the language pairs.\nBased on these results, we fixed the sampling method of candidates to epsilon sampling in all the following experiments.\nThen, we evaluate the effect of pseudo-references on performance by varying their sampling methods.\nTable 2  ###reference_### shows the results.\nAs expected from previous studies, the performance of MBR decoding varies even when only changing the sampling methods of pseudo-references.\nThe variation is nearly consistent across language pairs, indicating the pervasive effect of pseudo-reference on performance.\nThe best sampling method for candidates (epsilon sampling) is not the best for pseudo-references.\nThis shows that the desirable properties for candidates and pseudo-references are different.\nTable 2  ###reference_### also shows the results of beam search just for the comparison with MBR decoding.\nHere, the beam size was set to 5.\nMBR decoding significantly outperforms beam search and even outperforms the ensemble model, which was the winner of WMT19 (Barrault et al., 2019  ###reference_b3###).\nSince the effectiveness of epsilon sampling was reported on the other WMT dataset (Freitag et al., 2023  ###reference_b13###), we have a good reason to use epsilon sampling for this comparison.\nCandidate\nde-en\nen-de\nru-en\nen-ru\n\n\n\nAncestral\n85.82\n86.32\n82.11\n86.13\n\nBeam\n88.47\n89.32\n84.16\n89.44\n\nEpsilon ( = 0.02)\n88.51\n89.47\n84.36\n90.17\n\nNucleus ( = 0.6)\n88.01\n89.12\n83.76\n89.96\n\nNucleus ( = 0.9)\n88.02\n89.04\n83.98\n89.57\nEpsilon ( = 0.02)\n\nPseudo-Reference\nde-en\nen-de\nru-en\nen-ru\n\nAncestral\n85.82\n87.51\n82.02\n88.41\n\nBeam\n85.62\n87.40\n81.64\n87.78\n\nEpsilon ( = 0.02)\n85.89\n87.74\n82.01\n88.46\n\nEpsilon ( = 0.02)\u2217\n85.87\n87.74\n81.98\n88.46\n\nNucleus ( = 0.6)\n85.69\n87.57\n81.76\n88.26\n\nNucleus ( = 0.9)\n86.04\n87.82\n82.18\n88.61\n\n\nBeam Search\n84.38\n86.13\n80.76\n85.69\n\n\nBeam Search (ensemble)\n84.30\n86.06\n80.91\n85.74"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Setup",
            "text": "Following Fernandes et al. (2022  ###reference_b11###), we use publicly-available Transformer models (Vaswani et al., 2017  ###reference_b35###) trained by Ng et al. (2019  ###reference_b27###)222https://github.com/facebookresearch/fairseq/blob/7409af7f9a7b6ddac4cbfe7cafccc715b3c1b21e/examples/translation/README.md  ###reference_q/blob/7409af7f9a7b6ddac4cbfe7cafccc715b3c1b21e/examples/translation/README.md### for the WMT19 news translation task (Barrault et al., 2019  ###reference_b3###).\nThe models were trained in four directions between English (en) and German (de) or Russian (ru).\nWe conducted our experiments on the test set of WMT19 (newstest19), which was used as the development set in the previous work (Fernandes et al., 2022  ###reference_b11###).\nDue to the quadratic computational cost of MBR decoding, we drew 100 samples of  and  for each of the 1,000 examples of newstest19.\nWe employ COMET22 for the utility function , which is the state-of-the-art evaluation metric in machine translation (Rei et al., 2022  ###reference_b28###, 2020  ###reference_b29###).333COMET22 improved robustness to the deviation in numbers and named entities, which was the weakness of the previous COMET (Amrhein and Sennrich, 2022  ###reference_b1###).\nFor sampling methods, we use those that have been reported to be effective: ancestral sampling (Eikema and Aziz, 2020  ###reference_b9###; Freitag et al., 2022  ###reference_b14###), beam search, nucleus sampling (Eikema and Aziz, 2022  ###reference_b10###; Fernandes et al., 2022  ###reference_b11###), and epsilon sampling (Freitag et al., 2023  ###reference_b13###).\nAncestral sampling draws samples from  without modification, while nucleus sampling restricts sampling to words with top- probabilities (Holtzman et al., 2020  ###reference_b18###) and epsilon sampling truncates words with probabilities lower than  (Hewitt et al., 2022  ###reference_b17###).\nWe adopt the best hyperparameters reported for  and  (Fernandes et al., 2022  ###reference_b11###; Freitag et al., 2023  ###reference_b13###).\nThe beam size was set to 100 to collect 100 samples."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Results",
            "text": "Since we focus on sampling for pseudo-references, we first search for the best sampling method for candidates and fix it.\nThe objective of searching for the best is to prevent the pseudo-reference\u2019s contribution to scores from being capped and obscured by the candidate\u2019s quality.\nTo this end, we conduct the search on the same newstest19 as the subsequent experiments.444\nIf the objective is to find the best combination of sampling methods, which is not our focus, then it is desirable to use different splits to explore and test the combination to ensure the generalization.\nNevertheless, our subsequent results in Tables 1  ###reference_###  ###reference_###, 2  ###reference_###  ###reference_###, 4  ###reference_###  ###reference_###, and 5  ###reference_###  ###reference_### suggest the generalization of the found best combination as it consistently performs the best across almost all language pairs.\n\nFollowing Fernandes et al. (2022  ###reference_b11###  ###reference_b11###), we search for the sampling method that achieves the highest oracle score, , on average.\nTable 1  ###reference_###  ###reference_### shows that epsilon sampling achieves the highest oracle score across the language pairs.\nBased on these results, we fixed the sampling method of candidates to epsilon sampling in all the following experiments.\nThen, we evaluate the effect of pseudo-references on performance by varying their sampling methods.\nTable 2  ###reference_###  ###reference_### shows the results.\nAs expected from previous studies, the performance of MBR decoding varies even when only changing the sampling methods of pseudo-references.\nThe variation is nearly consistent across language pairs, indicating the pervasive effect of pseudo-reference on performance.\nThe best sampling method for candidates (epsilon sampling) is not the best for pseudo-references.\nThis shows that the desirable properties for candidates and pseudo-references are different.\nTable 2  ###reference_###  ###reference_### also shows the results of beam search just for the comparison with MBR decoding.\nHere, the beam size was set to 5.\nMBR decoding significantly outperforms beam search and even outperforms the ensemble model, which was the winner of WMT19 (Barrault et al., 2019  ###reference_b3###  ###reference_b3###).\nSince the effectiveness of epsilon sampling was reported on the other WMT dataset (Freitag et al., 2023  ###reference_b13###  ###reference_b13###), we have a good reason to use epsilon sampling for this comparison.\nCandidate\nde-en\nen-de\nru-en\nen-ru\n\n\n\nAncestral\n85.82\n86.32\n82.11\n86.13\n\nBeam\n88.47\n89.32\n84.16\n89.44\n\nEpsilon ( = 0.02)\n88.51\n89.47\n84.36\n90.17\n\nNucleus ( = 0.6)\n88.01\n89.12\n83.76\n89.96\n\nNucleus ( = 0.9)\n88.02\n89.04\n83.98\n89.57\nEpsilon ( = 0.02)\n\nPseudo-Reference\nde-en\nen-de\nru-en\nen-ru\n\nAncestral\n85.82\n87.51\n82.02\n88.41\n\nBeam\n85.62\n87.40\n81.64\n87.78\n\nEpsilon ( = 0.02)\n85.89\n87.74\n82.01\n88.46\n\nEpsilon ( = 0.02)\u2217\n85.87\n87.74\n81.98\n88.46\n\nNucleus ( = 0.6)\n85.69\n87.57\n81.76\n88.26\n\nNucleus ( = 0.9)\n86.04\n87.82\n82.18\n88.61\n\n\nBeam Search\n84.38\n86.13\n80.76\n85.69\n\n\nBeam Search (ensemble)\n84.30\n86.06\n80.91\n85.74"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Hypotheses for Performance Variation",
            "text": "The previous section confirmed that the performance varies by sampling pseudo-references.\nThe question that naturally arises in response to the results is: why does this variation occur?"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Previous Hypotheses",
            "text": "Eikema and Aziz (2022  ###reference_b10###) hypothesized that unbiased sampling is desirable for pseudo-references.\nSince the biased sampling methods limit the sampling to words of high probability, we use the average log probability (Avg. Prob.) of samples as a continuous proxy of bias existence in sampling.\nEikema and Aziz (2020  ###reference_b9###) and Freitag et al. (2023  ###reference_b13###) did not distinguish between candidates and pseudo-references but referred to the larger cumulative probability mass (Cum. Prob.) of unique samples as a desirable property because it indicates diverse and probable samples.\nEikema and Aziz (2022  ###reference_b10###) employed candidate sampling that achieved high expected utility.\nIf this criterion applies to pseudo-references, performance should be higher when the expected utility against candidates (Cand. Sim.) or references (Ref. Sim.) is high."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Our Hypothesis",
            "text": "Given the relaxation from Eq. (1  ###reference_###) to Eq. (2  ###reference_###), a better approximation of the true distribution by pseudo-references should be associated with higher performance.\nTo examine the relation, we propose using anomaly detection to quantitatively evaluate the approximation.\nIf a better approximation is achieved, references should deviate less from the majority of the samples since references are drawn from the true distribution by definition.\nThis recasting allows us to use anomaly scores of anomaly detection for measuring the degree of approximation.\nWe then hypothesize that a higher-performance sampling method forms samples where references achieve lower anomaly scores."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We test the hypotheses discussed in the previous section by evaluating the correlation between the performance variation and the properties or anomaly scores."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Setup",
            "text": "The setup is the same as described in Section 3.1  ###reference_###.\nWe run each sampling method with three different seeds and then calculate the Spearman\u2019s rank correlation coefficient  between their averaged properties or anomaly scores (see Section 4  ###reference_###) with the COMET22 scores reported in Table 2  ###reference_###."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Anomaly Scores",
            "text": "To test our hypothesis, we employ three popular methods used in unsupervised anomaly detection (Kriegel et al., 2011  ###reference_b22###; Gu et al., 2019  ###reference_b16###; Ruff et al., 2021  ###reference_b30###).\nThe first uses Mahalanobis distance (; Mahalanobis, 1936  ###reference_b25###) as an anomaly score, a classical distance measure between a data point and a distribution.\nIn our context, the distance is between reference  and pseudo-references  in a feature space: , where  and  are mean and inverse covariance matrix of , respectively.\nMahalanobis distance assumes that the data is normally distributed, but this assumption does not necessarily hold.\n-nearest neighbors (; Angiulli and Pizzuti, 2002  ###reference_b2###) does not have the assumption and is applicable to other data, such as the one with multimodal distribution.\n is a simple algorithm to consider the local density of a given data point.\nStill, it is known to perform favorably to some state-of-the-art algorithms for anomaly detection (Gu et al., 2019  ###reference_b16###).\nLet  be the th nearest pseudo-reference to  in Euclidean distance.\n takes the average of Euclidean distance from  to its -nearest pseudo-references : .\nLocal outlier factor (; Breunig et al., 2000  ###reference_b7###) additionally considers the local density of the -nearest data points themselves.\n in our setting measures how the local density of  deviates from that of its -nearest pseudo-references.\nTo better illustrate the relationship with , we show a simplified version of  (Schubert et al., 2014  ###reference_b31###) here: .\n\nSee Breunig et al. (2000  ###reference_b7###) for the complete formula we used.\nTo calculate the anomaly scores, samples needs to be represented in a feature space.\nWe obtain the representation in the space of utility by measuring the utility of references or pseudo-references given a set of candidates .\nA reference  in the space is then defined as .\nSame for a pseudo-reference .\nde-en\nen-de\nru-en\nen-ru\n\nAvg. Prob.(-)\n0.580(\u2713)\n0.290(\u2713)\n0.870(\u2713)\n0.638(\u2713)\n\nCum. Prob.(+)\n0.058(\u00d7)\n0.116(\u00d7)\n0.348(\u00d7)\n0.058(\u00d7)\n\nCand. Sim.(+)\n0.543(\u00d7)\n0.314(\u00d7)\n0.829(\u00d7)\n0.657(\u00d7)\n\nRef. Sim.(+)\n0.580(\u00d7)\n0.290(\u00d7)\n0.870(\u00d7)\n0.638(\u00d7)\n\n\n0.771(\u2713)\n0.486(\u2713)\n0.886(\u2713)\n0.771(\u2713)\n\n\n\n\n\n\n\n       = 5\n0.771(\u2713)\n0.829(\u2713)\n0.886(\u2713)\n0.829(\u2713)\n\n       = 25\n0.943(\u2713)\n0.943(\u2713)\n0.886(\u2713)\n0.943(\u2713)\n\n       = 50\n0.771(\u2713)\n0.943(\u2713)\n0.943(\u2713)\n0.829(\u2713)\n\n       = 75\n0.771(\u2713)\n0.943(\u2713)\n0.371(\u2713)\n0.829(\u2713)\n\n       = 100\n0.086(\u2713)\n0.314(\u2713)\n0.371(\u2713)\n0.029(\u2713)\n\n\n\n\n\n\n\n       = 5\n0.829(\u2713)\n0.600(\u2713)\n0.943(\u2713)\n0.771(\u2713)\n\n       = 25\n0.829(\u2713)\n0.714(\u2713)\n0.943(\u2713)\n0.829(\u2713)\n\n       = 50\n1.000(\u2713)\n0.886(\u2713)\n0.943(\u2713)\n0.829(\u2713)\n\n       = 75\n1.000(\u2713)\n0.886(\u2713)\n0.943(\u2713)\n0.829(\u2713)\n\n       = 100\n0.600(\u2713)\n0.371(\u2713)\n0.886(\u2713)\n0.657(\u2713)"
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Results",
            "text": "Table 3  ###reference_### shows the results.\nAs expected, the anomaly scores are clearly more correlated than the properties based on previous hypotheses.\nExcept for Cum. Prob., Cand. Sim., and Ref. Sim., the signs of  are all as expected, including the anomaly scores.\nSee Table 7  ###reference_### in Appendix for the results used to calculate .\nAmong the anomaly scores,555We took the median of  and  scores instead of the mean because they are unstable due to the inverse covariant matrix  and division, respectively. For , we removed duplicated  from the position vector and added an identity matrix not to drop the rank of  and stabilize the computation of . The value of the elements of the identity matrix was set to 1e-5, taking into account that the average value of the diagonal components of  was 1e-3.  and  with  = 50 stably correlate with the performance variation better than those with  = 100 and .\nWe speculate that the significant degradation of NN with  = 100 is caused by outliers in pseudo-references.\nWhile NN with  < 100 can effectively avoid including these outliers in the calculation of anomaly scores, NN with  = 100 cannot, and its anomaly scores are likely to be distorted by the outliers.\nThese results suggest that even if some pseudo-references are outliers against a reference, the performance tends to be higher if the rest of the pseudo-reference is close to the reference.\nIn other words, pseudo-references do not have to be close to references in entirety to perform well."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "MBR decoding has been used in automatic speech recognition (Goel and Byrne, 2000  ###reference_b15###), statistical machine translation (Kumar and Byrne, 2002  ###reference_b23###, 2004  ###reference_b24###), and NMT (Stahlberg et al., 2017  ###reference_b33###; Shu and Nakayama, 2017  ###reference_b32###; Blain et al., 2017  ###reference_b5###). Recently, MBR decoding has gained prominence again in NMT because of the following two innovations. (1) Eikema and Aziz (2020  ###reference_b9###) showed that MBR decoding with stochastic sampling has a potential to outperform MAP decoding methods, including beam search; (2) Freitag et al. (2022  ###reference_b14###) and Fernandes et al. (2022  ###reference_b11###) explored utility functions and found that using neural reference-based metrics as the utility function significantly enhances the quality of output texts. M\u00fcller and Sennrich (2021  ###reference_b26###) reported domain robustness and less hallucination in the outputs of MBR decoding. Other text generation tasks such as text summarization, image captioning, and diversity-aware text generation also benefit from MBR decoding (Suzgun et al., 2023  ###reference_b34###; Borgeaud and Emerson, 2020  ###reference_b6###; Jinnai et al., 2024  ###reference_b20###). Recent studies have focused on improving the efficiency of MBR decoding (Cheng and Vlachos, 2023  ###reference_b8###; Finkelstein and Freitag, 2023  ###reference_b12###; Yang et al., 2023  ###reference_b36###; Jinnai et al., 2023  ###reference_b21###; Jinnai and Ariu, 2024  ###reference_b19###). The most related studies explored sampling methods for MBR decoding and raised hypotheses to explain the difference in performance by sampling methods (Eikema and Aziz, 2020  ###reference_b9###, 2022  ###reference_b10###; Fernandes et al., 2022  ###reference_b11###; Freitag et al., 2023  ###reference_b13###). We also explored sampling methods but differed in that we did it more closely by focusing on pseudo-references. Furthermore, we introduced anomaly scores that correlate with the performance variation better than previous hypotheses.\n\nIn the realm of AI ethics, bias in machine learning models is a prominent concern. Research by Binns (2018 ###reference_b4###) and Barocas et al. (2019 ###reference_b3###) highlights how bias can originate from biased data or algorithms lacking fairness considerations. Efforts by Mitchell et al. (2021 ###reference_b27###) focus on embedding transparency and accountability in AI systems to mitigate these issues."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "This study investigated the relation between the performance of MBR decoding and the core assumption about samples: samples follow the true distribution of references.\nWe introduced anomaly scores used in anomaly detection to evaluate the approximation of the true distribution.\nExperimental results demonstrated that the anomaly scores correlate with the performance significantly better than the properties hypothesized to explain the performance variation in prior literature.\nThe previous hypotheses assumed that unbiased sampling (Avg. Prob.), diverse and probable samples (Cum. Prob.), or high expected utility (Cand. and Ref. Sim.) are the key properties of samples to achieve high performance.\nHowever, these properties do not have an obvious relationship to approximating the true distribution of references, in contrast to the anomaly scores we employed.\nThese results show the insufficiency of existing hypotheses about the properties that samples should possess.\nThe results are also the first to empirically support the link between the actual performance and the key assumption of MBR decoding.\nWe believe this serves as an essential step to understanding the connection between the actual performance and the theory of MBR decoding."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Limitations and Risks",
            "text": "The limitation of the study is that it is solely a thorough analysis of MBR decoding, not accompanied by an algorithm to improve the performance of MBR decoding.\nHowever, our analysis empirically shows that previous hypotheses about the properties of samples are insufficient and that following the assumption of the MBR decoding is the key to improving performance.\nWe believe this is an important contribution that modifies the direction of future development of MBR decoding.\nOur investigation is limited to Transformer models provided by Ng et al. (2019  ###reference_b27###) and the task is limited to machine translation.\nFuture work will extend the analysis to a wider range of models and text generation tasks.\nHowever, it is worth noting that some studies support the general applicability of MBR decoding findings obtained in NMT to other text generation tasks and models.\nSome hyperparameters (Suzgun et al., 2023  ###reference_b34###), efficiency-boosting techniques (Jinnai et al., 2023  ###reference_b21###; Jinnai and Ariu, 2024  ###reference_b19###), or diversity-aware extensions (Jinnai et al., 2024  ###reference_b20###) for MBR decoding consistently perform well across machine translation, summarization, image captioning, and data-to-text generation with different models.\nBertsch et al. (2023  ###reference_b4###) shows that MBR decoding works well even in open-ended text generation tasks.\nWe do not foresee any ethical concerns in our analysis."
        }
    ],
    "url": "http://arxiv.org/html/2404.00752v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "6"
        ],
        "methodology_sections": [
            "4.1",
            "4.2"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "3.1",
            "3.2",
            "5.1",
            "5.2",
            "5.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "3.1",
            "3.2",
            "5",
            "5.1",
            "5.2",
            "5.3"
        ]
    },
    "research_context": {
        "paper_id": "2404.00752v1",
        "paper_title": "On the True Distribution Approximation of Minimum Bayes-Risk Decoding",
        "research_background": "**Paper's Motivation:**\nThe motivation of this paper is based on the observation that Minimum Bayes-Risk (MBR) decoding has re-emerged as a superior method compared to beam search for tasks such as neural machine translation (NMT), text summarization, and image captioning. Despite its advantages, there is still a need to understand the substantial performance variations that occur when different sampling methods are used in MBR decoding. This study is driven by the desire to empirically demonstrate a connection between these performance variations and the quality of approximating the true distribution, which should ideally correspond to the distribution of human-quality translations.\n\n**Research Problem:**\nThe primary research problem addressed in this paper is the lack of clarity in the relationship between performance gains from various sampling methods in MBR decoding and the theoretical assumption that these samples approximate the true distribution of human-quality translations. Specifically, the existing properties used to explain performance variation, such as sampling bias and cumulative probability mass, do not clearly relate to the true distribution. The paper seeks to bridge this gap by empirically supporting the link between performance and the quality of approximation to the true distribution.\n\n**Relevant Prior Work:**\n1. The resurgence of MBR decoding as a preferable alternative to beam search in several tasks is noted in studies by Eikema and Aziz (2020) and (2022), Fernandes et al. (2022), Freitag et al. (2022), Suzgun et al. (2023), and Bertsch et al. (2023).\n2. The importance of sampling in achieving good performance with MBR decoding and the recognition that performance varies with different sampling methods is highlighted in the works of Eikema and Aziz (2020, 2022), Fernandes et al. (2022), and Freitag et al. (2023).\n3. From a theoretical perspective, Kumar and Byrne (2002, 2004) have discussed the assumption that samples should approximate the true distribution of human-quality translations.\n4. Previous studies that attempted to explain performance variations by properties of samples (e.g., sampling bias, cumulative probability mass) include the works by Eikema and Aziz (2020) and Freitag et al. (2023). However, these properties lack a clear relation to the true distribution, leading to the conclusion that the core assumption's relation to performance gains remains unclear.\n\nThus, this paper aims to fill this gap by introducing measures for the degree of approximation and incorporating anomaly detection to evaluate the performance of different sampling methods in relation to their approximation of the true distribution.",
        "methodology": "The proposed method differentiates itself in several key aspects drawn from prior works, including Eikema and Aziz (2022) and related studies. The approach is structured around the concepts of unbiased sampling, average log probability, cumulative probability mass, and the relationship between candidates and pseudo-references. Below is a detailed description of the methodology:\n\n1. **Unbiased Sampling for Pseudo-References**: The method begins with the hypothesis that unbiased sampling is advantageous for generating pseudo-references. Unbiased sampling refers to a sampling process that does not overly favor high-probability words, allowing for a more diverse and realistic set of pseudo-references.\n\n2. **Average Log Probability (Avg. Prob.)**: To measure the presence of bias in the sampling process, the average log probability of samples is used as a continuous proxy. This metric helps evaluate whether the sampling method tends to select high-probability words excessively, which would indicate a biased process.\n\n3. **Cumulative Probability Mass (Cum. Prob.)**: Referencing the work by Eikema and Aziz (2020) and Freitag et al. (2023), the methodology emphasizes the importance of cumulative probability mass of unique samples. A higher cumulative probability mass suggests that the sampling method produces a diverse array of probable samples, representing the true distribution more accurately.\n\n4. **Distinction Between Candidates and Pseudo-References**: Unlike previous studies that did not distinguish between candidates and pseudo-references, this methodology makes a clear distinction. Candidates are potential solutions that the model considers, while pseudo-references are generated samples used to approximate the true distribution.\n\n5. **High Expected Utility**: Building on the candidate sampling approach from Eikema and Aziz (2022), the method prioritizes achieving high expected utility. This criterion is crucial for pseudo-references as well; when the expected utility is high against either candidates (Cand. Sim.) or references (Ref. Sim.), the performance of the model is indicated to be superior.\n\nBy integrating these components, the proposed methodology aims to produce more reliable pseudo-references that better approximate the true distribution, ultimately enhancing the performance of minimum Bayes-risk decoding.",
        "main_experiment_and_results": "The main experiment aims to evaluate the performance impact of different sampling methods for pseudo-references in Minimum Bayes-Risk (MBR) decoding in Neural Machine Translation (NMT).\n\n**Datasets:**\nThe experiments are conducted using the `newstest19` dataset, which is part of the WMT19 benchmark.\n\n**Baselines:**\n1. **Candidate Sampling Methods:**\n   - **Ancestral Sampling**\n   - **Beam Search**\n   - **Epsilon Sampling (\n   - **Nucleus Sampling (with different values of )**\n\n2. **Pseudo-Reference Sampling Methods:**\n   - **Ancestral Sampling**\n   - **Beam Search**\n   - **Epsilon Sampling (\n   - **Nucleus Sampling (with different values of )**\n\nThe study also includes a comparison with the beam search and the ensemble model, which was the winner of WMT19.\n\n**Evaluation Metrics:**\nPerformance is measured using the `oracle score`, which is the highest score achievable by any candidate in the set. The results are aggregated and compared across multiple language pairs: German-English (de-en), English-German (en-de), Russian-English (ru-en), and English-Russian (en-ru).\n\n**Experiment Results:**\n1. **Candidate Sampling**:\n   - Results for candidates were: `de-en (88.51)`, `en-de (89.47)`, `ru-en (84.36)`, `en-ru (90.17)`.\n\n2. **Comparison with Beam Search**:\n   - MBR decoding significantly outperformed beam search and even surpassed the WMT19 ensemble model.\n   - Beam search results: `de-en (84.38)`, `en-de (86.13)`, `ru-en (80.76)`, `en-ru (85.69)`.\n   - Ensemble model results: `de-en (84.30)`, `en-de (86.06)`, `ru-en (80.91)`, `en-ru (85.74)`.\n\nIn summary, the main results indicate that the sampling method for pseudo-references has a significant impact on MBR decoding performance and that the optimal sampling method for candidates (epsilon sampling) is not necessarily the best for pseudo-references."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To examine the effect of pseudo-reference sampling methods on the performance of Minimum Bayes-Risk (MBR) decoding and to determine if different sampling methods impact performance similarly across different language pairs.",
            "experiment_process": "The study first selects the best sampling method for candidates by testing sampling methods on the newstest19 dataset to avoid the pseudo-reference's contribution being capped by candidate quality. Epsilon sampling was found to be the best for candidates. The experiments use publicly-available Transformer models trained for the WMT19 news translation task in four language directions: English to/from German and Russian. The models are tested on the newstest19 test set, drawing 100 samples for each of the 1,000 examples. COMET22 scores are used as the evaluation metric. The performance of MBR decoding is analyzed by varying the sampling methods of pseudo-references and comparing the results against beam search and an ensemble model.",
            "result_discussion": "The performance of MBR decoding varies significantly when changing pseudo-reference sampling methods, and this variation is consistent across language pairs. The results indicate that the best sampling method for candidates (epsilon sampling) is not the optimal choice for pseudo-references, highlighting different desirable properties for candidates and pseudo-references. MBR decoding outperforms beam search and the WMT19-winning ensemble model. Epsilon sampling is validated as an effective sampling method for MBR decoding.",
            "ablation_id": "2404.00752v1.No1"
        },
        {
            "research_objective": "To test the hypothesis that anomaly scores correlate better with performance variation in Minimum Bayes-Risk (MBR) decoding than previously proposed sampling properties.",
            "experiment_process": "The setup from the earlier experiments is reused, including the publicly available Transformer models trained on the WMT19 dataset and tested on newstest19 with 100 samples for each example. Spearman's rank correlation coefficient is calculated between averaged properties or anomaly scores across different seeds and COMET22 scores. Anomaly detection methods such as Mahalanobis distance, -nearest neighbors (NN), and local outlier factor (LOF) are employed to compute anomaly scores based on the utility representation of references/pseudo-references.",
            "result_discussion": "Anomaly scores show a higher correlation with performance variation compared to properties from previous hypotheses, except for Cum. Prob., Cand. Sim., and Ref. Sim. Among the anomaly scores, NN with a = 50 correlates better with performance variation, suggesting stability against including outliers in calculations. The results imply that pseudo-references close to the reference contribute more to performance, even if some are outliers.",
            "ablation_id": "2404.00752v1.No2"
        }
    ]
}