{
    "title": "DiLM: Distilling Dataset into Language Model for Text-level Dataset Distillation",
    "abstract": "Dataset distillation aims to compress a training dataset by creating a small number of informative synthetic samples such that neural networks trained on them perform as well as those trained on the original training dataset.\nCurrent text dataset distillation methods create each synthetic sample as a sequence of word embeddings instead of a text to apply gradient-based optimization; however, such embedding-level distilled datasets cannot be used for training other models whose word embedding weights are different from the model used for distillation.\nTo address this issue, we propose a novel text dataset distillation approach, called Distilling dataset into Language Model (DiLM), which trains a language model to generate informative synthetic training samples as text data, instead of directly optimizing synthetic samples.\nWe evaluated DiLM on various text classification datasets and showed that distilled synthetic datasets from DiLM outperform those from current coreset selection methods. DiLM achieved remarkable generalization performance in training different types of models and in-context learning of large language models.\nOur code will be available at https://github.com/arumaekawa/DiLM.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The successful advancements in machine learning in a wide range of fields are due to the scaling-up of deep neural networks and large training datasets.\nIn the natural language processing (NLP) field, large language models (LLMs), which are pre-trained with a huge amount of text, such as BERT- and GPT-family models Devlin et al. (2019  ###reference_b7###); Liu et al. (2019  ###reference_b14###); Radford et al. (2019  ###reference_b20###); Brown et al. (2020  ###reference_b2###), have shown remarkable capabilities for various NLP tasks.\nHowever, training such large-scale models requires large computational resources and a long time, which makes it difficult to develop new LLMs, and even to fine-tune them.\n###figure_1### To address this issue, dataset distillation Wang et al. (2018b  ###reference_b30###) has attracted much attention in the machine learning community, which aims to reduce training costs by compressing training datasets.\nIn contrast to traditional coreset selection approaches Wolf (2011  ###reference_b33###); Sener and Savarese (2018  ###reference_b23###); Welling (2009  ###reference_b31###), which heuristically select a small subset of representative training samples from the original dataset, dataset distillation creates more informative synthetic samples by distilling the knowledge from the original dataset.\nWith this approach, synthetic samples are optimized with gradient descent according to objective functions for dataset distillation, including meta-learning Wang et al. (2018b  ###reference_b30###), gradient matching Zhao et al. (2021  ###reference_b45###), training trajectory matching Cazenavette et al. (2022  ###reference_b3###), and feature distribution matching Wang et al. (2022  ###reference_b29###); Zhao and Bilen (2023  ###reference_b44###).\nThe recent remarkable performance of dataset distillation, especially in the computer vision (CV) field, has also led to studies of its various applications, including neural architecture search (Such et al., 2020  ###reference_b24###; Medvedev and D\u2019yakonov, 2021  ###reference_b18###), federated learning (Zhang et al., 2022a  ###reference_b38###; Xiong et al., 2023  ###reference_b34###), continual learning (Wiewel and Yang, 2021  ###reference_b32###; Sangermano et al., 2022  ###reference_b22###), and privacy preservation (Dong et al., 2022  ###reference_b8###; Chen et al., 2022  ###reference_b5###).\nWhile most previous studies applied dataset distillation only to image classification datasets, some studies focused on text dataset distillation Sucholutsky and Schonlau (2021  ###reference_b25###); Li and Li (2021  ###reference_b12###); Maekawa et al. (2023  ###reference_b17###); Sahni and Patel (2023  ###reference_b21###).\nIn contrast to the image, which can be applied gradient-based optimization by considering it as a pixel-wise continuous data, the discrete nature of text makes dataset distillation challenging Geng et al. (2023  ###reference_b9###); Yu et al. (2023  ###reference_b37###).\nTo address this issue, all existing text dataset distillation methods used the widely used neural NLP technique called embedding, i.e., optimizing a synthetic dataset as continuous input word embeddings instead of discrete text.\nHowever, such embedding-level distilled synthetic datasets cannot be used for training other models that have different word embedding weights, which is a crucial issue in terms of practical applications.\nFurthermore, distilled word embedding sequences are also completely unreadable to humans, which makes it difficult to interpret and analyze the original training dataset by observing distilled synthetic samples.\nMotivated by these shortcomings, this paper explores the text dataset distillation to obtain distilled synthetic datasets at the text-level as the first study.\nWe propose the first text-level dataset distillation approach called \u201cDistilling dataset into Language Model (DiLM)\u201d.\nTo overcome the optimization difficulty of discrete text, DiLM uses a language model as a surrogate continuous optimization target instead of directly optimizing a synthetic sample\u2019s text.\nSpecifically, DiLM trains a language model to minimize the gradient matching loss Zhao et al. (2021  ###reference_b45###) of generated synthetic samples as a dataset distillation objective.\nTo enable back-propagating the gradient matching loss to the language model, we design a differentiable backward pass via loss weighting with generation probabilities to bypass the non-differentiable generated text (Figure 1  ###reference_###).\nIn our experiments, we applied DiLM to distill three text classification datasets from the GLUE benchmark Wang et al. (2018a  ###reference_b27###), SST-2, QQP, and MNLI-m.\nThe results indicate that the synthetic datasets distilled with DiLM outperformed representative real samples selected from the original datasets with current coreset selection methods.\nOur distilled datasets also achieved remarkable generalization performance not only for training different types of pre-trained models but also for in-context learning of LLMs as few-shot prompts.\nOur main contributions are as follows:\nTo the best of our knowledge, this is the first study to distill a text dataset into a text-level synthetic dataset that are applicable for training models independent of word embedding weights.\nWe present DiLM, which addresses the discreteness of text by using a language model as a surrogate optimization target and back-propagating the distillation loss to the model, bypassing non-differentiable generated text.\nOur experimental results indicate that DiLM outperformed the current coreset selection methods not only for training the same model used for distillation, but also for training different models independent of the word embedding weights, architectures, and training processes."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Dataset Distillation",
            "text": "Dataset distillation was first proposed by Wang et al. (2018b  ###reference_b30###), motivated by theoretical interests as well as practical applications for reducing network training costs. Inspired by meta-learning based hyperparameter optimization Maclaurin et al. (2015  ###reference_b16###), Wang et al. (2018b  ###reference_b30###) optimized a small synthetic dataset by gradient descent such that models trained on it have a lower training loss for the original dataset. Recently, several surrogate objectives have been proposed to improve the performance and efficiency of dataset distillation. DC Zhao et al. (2021  ###reference_b45###) and DSA Zhao and Bilen (2021  ###reference_b42###) focused on gradient matching between real and synthetic samples. DM Zhao and Bilen (2023  ###reference_b44###) and CAFE Wang et al. (2022  ###reference_b29###) proposed feature distribution matching, which requires less GPU memory for optimizing synthetic datasets. MTT Cazenavette et al. (2022  ###reference_b3###) and TESLA Cui et al. (2023  ###reference_b6###) optimized synthetic samples to approximate trajectories of model parameters trained with real data. SLDD Sucholutsky and Schonlau (2021  ###reference_b25###) and LDD Bohdal et al. (2020  ###reference_b1###) introduced learnable soft-labels, which are optimized together with input images to make each synthetic sample more informative. While the most current research on dataset distillation involves only image classification datasets, some studies also focused on text classification datasets. Sucholutsky and Schonlau (2021  ###reference_b25###) and Li and Li (2021  ###reference_b12###) applied the original meta-learning based method by Wang et al. (2018b  ###reference_b30###) to text datasets. To overcome the discrete nature of text, which makes applying gradient-based methods difficult, they optimized synthetic samples in the pre-trained GloVe word embedding space Pennington et al. (2014  ###reference_b19###) instead of actual words of text as the optimization target. Maekawa et al. (2023  ###reference_b17###) extended the text dataset distillation to the pre-trained BERT model and improved its performance by introducing learnable attention labels, which directly guide the self-attention probabilities of the models. Sahni and Patel (2023  ###reference_b21###) explored dataset distillation in multilingual text classification datasets in the context of fairness, interpretability, and cross-architecture generalization. Although these methods perform well for text classification datasets, distilled synthetic datasets obtained with them cannot be used for training other models that have different word embedding weights. Although Sucholutsky and Schonlau (2021  ###reference_b25###) and Sahni and Patel (2023  ###reference_b21###) transformed their distilled synthetic samples to text by finding a word that has the nearest neighbor embedding, the converted text consists of unrelated words and does not make sense, which makes it difficult to interpret and analyze them. Moreover, the performance of distilled datasets after being converted to text has also not been investigated.\n\nAdvancements in AI have significantly impacted robotics, with applications ranging from autonomous navigation and manipulation to human-robot interaction (Levine et al., 2016 ###reference_b11###; Silver et al., 2018 ###reference_b23###). These techniques improve robots' ability to learn from environments and adapt to complex tasks with enhanced precision and efficiency."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Generative Models",
            "text": "Recent studies on dataset distillation in the CV field used generative adversarial networks (GANs) Goodfellow et al. (2014  ###reference_b10###), i.e., training the model parameters and/or their latent input noises instead of synthetic images. These methods generalize distilled synthetic images to different model architectures by restricting them to the generative distribution learned from the original dataset. DiM Wang et al. (2023  ###reference_b28###) fine-tuned a GAN to generate informative synthetic images from randomly sampled latent noises, where distilled datasets of different sizes can be produced without retraining the model. GTNs Such et al. (2020  ###reference_b24###) trained a GAN to generate informative images, instead of realistic images, to accelerate neural architecture search. GTNs also learned a latent noise for each synthetic image as a curriculum of training learner networks. IT-GAN Zhao and Bilen (2022  ###reference_b43###) and GLaD Cazenavette et al. (2023  ###reference_b4###) used a pre-trained GAN as a generative prior of synthetic samples and only optimized the latent noises. Inspired by these studies, we also introduce a generative model with a different motivation for text dataset distillation: to avoid the difficulties of directly optimizing discrete text, we instead optimize the continuous parameters of a generative model to generate distilled synthetic samples. However, since all previous studies that used generative models for image dataset distillation trained them and/or their input latent noises by back-propagating the distillation loss to them via generated images, none of them can be applied to text data, which are non-differentiable due to their discrete nature.\n\nAI in robotics has revolutionized automation by enhancing robotic perception, decision-making, and interaction capabilities. Techniques such as reinforcement learning allow robots to learn tasks autonomously, while neural networks improve sensory processing. These advancements enable intricate human-robot collaboration and adaptive behaviors in dynamic environments, highlighting AI\u2019s potential to significantly expand robotic functionalities."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "In this section, we introduce DiLM, which distills text datasets into text data, not word embeddings, for the model-agnostic applicability and interpretability of the distilled synthetic datasets.\nThe main idea of DiLM is to avoid the optimization difficulties of discrete text by instead training continuous parameters of a language model as a surrogate optimization target of dataset distillation."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Overview",
            "text": "Given a training dataset , the goal of DiLM is to obtain a generator model, parameterized by , that generates a distilled synthetic dataset  (), such that a learner model, parameterized by , trained on  performs well.\nTo achieve this goal, the overall procedure of DiLM is composed of the following three steps:\nWe first simply train the generator model to generate synthetic training samples that belong to the same distribution as in the original dataset  (Section 3.2  ###reference_###).\nWe then fine-tune the generator model to generate \u201cinformative\u201d training samples by minimizing the gradient matching loss between generated and real samples (Section 3.3  ###reference_###).\nWe obtain distilled dataset  by generating synthetic samples with the generator model and selecting representative samples from them by using a clustering-based coreset selection method (Section 3.4  ###reference_###).\nWe describe the details of each step in the following sections."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Synthetic Training Data Generation with Language Model",
            "text": "Inspired by the remarkable text generation capability of pre-trained transformer language models Radford et al. (2019  ###reference_b20###), we use them as the generator model to generate synthetic training samples of sufficient quality to be used for training models.\nBefore training the generator model to generate more informative synthetic samples than real samples in the original dataset, we first simply train a language model to generate training samples that belong to the same distribution as in the original training dataset for the initial parameters of the generator model.\nWhen we target at text classification tasks, we need to control the generator model to generate samples for each specific class.\nTherefore, we introduce class-specific beginning-of-sentence tokens <bos_i>, which are added to the head of each training sample to train the generator model to generate samples of the corresponding class following it.\nFor each training sample, an end-of-sentence token <eos> is also added, and the sample is fed to the generator model as follows:\n<bos_i> sentence of class  <eos>.\nTo involve text classification tasks that specify the relation between two sentences, such as semantic similarity and natural language inference (NLI), we use a separate token <sep> to split two sentences as\n<bos_i> sentence 1 <sep> sentence 2 <eos>.\nThe generator model is trained on them with the language modeling loss  as\nwhere  is a token in  and  is the length of . In this way, we pre-train the generator model parameters  to generate synthetic training data like real data, and use them as the initial parameter for training for gradient matching, described in the following section."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Training for Gradient Matching",
            "text": "In this section, we explain how to fine-tune the pre-trained generator model, described in Section 3.2  ###reference_###, to generate synthetic training samples that are more informative than real samples in the original dataset.\nSpecifically, we describe gradient matching, which is an optimization objective for dataset distillation, and the model updating procedure to deal with the discreteness of text.\nWe also introduce two techniques to improve DiLM: representative teacher and diverse mini-batch sampling.\nGradient Matching.\nTo distill the knowledge of the original dataset  into generated synthetic samples from the generator model, we optimize the gradient matching loss Zhao et al. (2021  ###reference_b45###) as the objective for dataset distillation.\nGiven a mini-batch of real samples  and a mini-batch of synthetic samples , which is generated from the generator model, the gradient matching loss  on the learner model parameters  is calculated as\nwhere  is the loss function for learning tasks such as cross-entropy loss, and  is the cosine similarity-based distance function, expressed as\nFollowing a previous study Zhao et al. (2021  ###reference_b45###), we separately calculate the gradient matching loss for each class and combine them to update the generator model parameters .\nTo consider the gradient on the learner model parameters  throughout the entire training process, the generator model is trained with the nested loop algorithm, including the outer loop, which initializes  at the beginning, and the inner loop, which updates  for  steps with real samples (see Algorithm 1  ###reference_thm1###).\nGenerator Updating.\nAs we described in Section 2.2  ###reference_###, the gradient matching loss  cannot be directly back-propagated to the generator model parameters  via generated samples , like the case with image datasets, because they consist of discrete text.\nAlthough some solutions to the discrete back-propagation issue in text generation have been explored in the NLP research field, most of standard approaches, including soft-argmax Zhang et al. (2017  ###reference_b41###) and policy gradient Yu et al. (2017  ###reference_b36###), cannot be applied to this case (see details in Appendix A  ###reference_###).\nTo address this issue, we design an alternative backward pass, inspired by a previous study Hiraoka et al. (2020  ###reference_b11###), which optimizes a tokenization model for the downstream task\u2019s loss through a non-differentiable procedure.\nWhen computing the generated sample\u2019s loss , instead of simply averaging the losses for each generated sample as in Eq. (2  ###reference_###), we weight them with their generation probabilities  as\nTherefore,  can be back-propagated to  through the differentiable pass via loss weights , as illustrated in Figure 1  ###reference_###.\nIntuitively, the generator model is updated to increase its generation probabilities of synthetic samples that improve gradient similarity.\nRepresentative Teacher.\nTo improve DiLM, we consider enhancing the gradient teacher of real samples by using representative samples for each mini-batch of real samples instead of randomly selected ones.\nInspired by Liu et al. (2023  ###reference_b13###), we select the representative samples with K-centers Wolf (2011  ###reference_b33###); Sener and Savarese (2018  ###reference_b23###), a clustering-based coreset selection method (Figure 1  ###reference_###a).\nSpecifically, we divide all the real training samples for each class into  sub-clusters by using the K-means algorithm on the feature space of the learner model, and choose the center sample of each sub-cluster.\nAs shown in Liu et al. (2023  ###reference_b13###), the representative samples selected by K-centers provide the proper teacher gradient by including diverse samples that cover the overall distribution for each class and eliminating samples near the decision boundaries, which have dominant gradients with large norms.\nConsidering coverage and robustness, we generate 10 representative sample sets by running the K-means algorithm with different random seeds at the beginning of training and use one as a mini-batch of real samples in each training step.111Liu et al. (2023  ###reference_b13###) repeatedly re-generated the K-center representative samples by conducting clustering on the feature space of the different learner model\u2019s states throughout the inner loop. However, it is very time consuming with BERT as the learner model, as in our study.\nDiverse Mini-batch Sampling.\nDiversity in a mini-batch of generated samples for each step affects the sample space that the generator model explores in training.\nIf the generator model only generates many samples that are similar to each other, this leads to the biased optimization of the generator model.\nTo address this issue, we introduce diverse mini-batch sampling of generated samples in the training process of DiLM (Figure 1  ###reference_###b).\nInstead of generating  synthetic samples for each step, the generator model generates  synthetic samples at the same time, where  is the generation interval. The generated synthetic samples are then divided into  sub-clusters with the K-means algorithm, and a mini-batch of synthetic samples for each step is constructed by randomly choosing one sample from each sub-cluster."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Generate Synthetic Dataset",
            "text": "We obtain distilled dataset  by generating synthetic samples with the trained generator model.\nTo include representative samples of the model\u2019s generative distribution , we use the coreset selection method again to select generated synthetic samples.\nSpecifically, we generate 100 times as many synthetic samples as the distilled dataset size  by top- sampling with , considering the diversity, and then construct  with K-center representative samples.\nThis makes  to include diverse synthetic samples by removing redundant samples caused by the biased generative distribution of the model."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experimental Settings",
            "text": "Datasets.\nWe evaluated DiLM in distilling three major text classification datasets, SST-2, QQP, and MNLI-m, from the GLUE benchmark Wang et al. (2018a  ###reference_b27###).\nFollowing Wang et al. (2018a  ###reference_b27###), we report accuracy for SST-2 and MNLI-m, and the average of accuracy and F1 score for QQP as our results.\nMore details about each dataset are shown in Appendix B  ###reference_###.\nBaselines. Following previous studies on dataset distillation in the CV field, we compared the performance of DiLM with three coreset selection methods, Random, K-centers Wolf (2011  ###reference_b33###); Sener and Savarese (2018  ###reference_b23###), and Herding Welling (2009  ###reference_b31###), as well as TDD Sucholutsky and Schonlau (2021  ###reference_b25###), which is a recent embedding-level distillation method.\nNote that TDD also trains the learnable soft-labels and learning rates for each training step together with the input word embeddings.\nWe also evaluated the vanilla LM, which skips training for gradient matching (in Section 3.3  ###reference_###), to validate its effectiveness. Note that we applied K-center representative sample selection (in Section 3.4  ###reference_###) to the vanilla LM as well.\nThe details of each baseline are given in Appendix C  ###reference_###.\nEvaluation.\nFor evaluation, we used BERT and other three pre-trained models, RoBERTa, BERT, and XLNet, as learner models (see more details in Appendix D  ###reference_###).\nWe trained a learner model on the distilled datasets for 200 steps by using AdamW Loshchilov and Hutter (2019  ###reference_b15###) with a learning rate of  and a batch size of 64.222We did not follow this training protocol for TDD, since TDD optimizes learning rates as well for each step with a specific synthetic sample order.\nFor Herding and TDD, we trained the learner model on their datasets for 100 times. For other methods, we generated 20 datasets with different random seeds and trained the learner model on each of them for 5 times.\nWe report the average and standard deviation for these 100 models.\nIn the result tables, \u2018\u2019 indicates significant difference of DiLM from K-centers (, Welch\u2019s t-test). Note that the standard deviations in our results inevitably become large because we trained models with few selected/generated samples from different initial model parameters. However, our evaluation procedure, which includes 100 runs, supports the reliability of our experimental results enough to discuss the effectiveness of the proposed method.\nImplementation.\nWe used the 128M parameter version of GPT-2333https://huggingface.co/gpt2  ###reference_huggingface.co/gpt2### Radford et al. (2019  ###reference_b20###) as the generator model of DiLM,\nand used BERT Devlin et al. (2019  ###reference_b7###) as the learner model, on which we calculated the gradient matching loss.\nTo reduce the computational costs, we calculated the gradient matching loss only for the randomly initialized last layer parameters, which tend to have dominantly larger gradient than the pre-trained parameters.\nWe set the number of each loop for training DiLM to , , and , and the generation interval to  according to our preliminary experiments.\nThe mini-batch size of real and synthetic samples were respectively set to  and .\nMore details of our implementation are given in Appendix E  ###reference_###."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Results and Discussion",
            "text": ""
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Performance for BERT",
            "text": "SST-2 (2 classes, 67.3k)\nQQP (2 classes, 364k)\nMNLI-m (3 classes, 393k)\n\nData/class\n5\n10\n20\n5\n10\n20\n5\n10\n20\n\nRandom\n\n\n\n\n\n\n\n\n\n\nK-centers\n\n\n\n\n\n\n\n\n\n\nHerding\n\n\n\n\n\n\n\n\n\n\nTDD (embed.)\n\n-\n-\n\n-\n-\n\n-\n-\n\nTDD (text)\n\n-\n-\n\n-\n-\n\n-\n-\n\nVanilla LM\n\n\n\n\n\n\n\n\n\n\nDiLM\n\n\n\n\n\n\n\n\n\n\nFull dataset\n92.7\n89.6\n86.7\nAs shown in Table 1  ###reference_###, we first compared DiLM with the other baselines for training BERT, on which DiLM trained gradient matching.\nWe evaluated them for different sizes of distilled synthetic datasets of 5/10/20 data-per-class (DPC) settings.\nWe first found that the vanilla LM, which was only trained for synthetic training sample generation without gradient matching, clearly underperformed the coreset selection methods.\nThis indicates that, as can be expected, the quality of the generated synthetic samples becomes lower than that of real samples in the original datasets.\nHowever, DiLM, which fine-tuned the vanilla LM with gradient matching, improved its performance and even outperformed the coreset selection methods overall.\nNote that the performance gains from K-centers indicate that DiLM generated synthetic training samples that are more effective for model training than the real samples in the original datasets.\nWhen focusing on the difference between the three datasets, the performance gains of DiLM on QQP and MNLI-m were larger than that on SST-2.\nWe believe this is because QQP and MNLI-m, which are the tasks to specify the relationship between two sentences, are intuitively less likely to have real samples that represent the task than SST-2, which is a relatively simple negative/positive classification task.\nIn addition, it may also be related to the size of the original training dataset of QQP and MNLI-m, which is five times larger than that of SST-2.\nSince the generator model was trained by gradient matching with self-generated synthetic samples, it can explore broader sample space by pre-training with the original dataset that contains enough diversity samples, which results in the effective performance of DiLM.\nFor TDD, we also evaluated its distilled datasets as text data by converting them to discrete tokens that have nearest neighbor embeddings.\nWhen directly using the distilled datasets as word embeddings, TDD achieved remarkable performance even compared with the full datasets.\nHowever, after converting to text, its performance catastrophically degraded even to the lower-bound performances with random prediction.\nThis suggests that the distilled datasets from TDD are strictly overfitted at the word embedding level and cannot be converted to text without acceptable performance degradation, which is necessary for applying them to other models.\nThis point is the clear advantage of DiLM, which distills synthetic datasets at the text-level."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Cross-model Generalization",
            "text": "Dataset\nModel\n\n\nRandom\n\n\n\nK-centers\n\n\n\nDiLM\n\n\nSST-2\nBERT (S)\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoBERTa\n\n\n\n\n\n\n\n\n\n\n\n\n\nBERT\n\n\n\n\n\n\n\n\n\n\n\n\n\nXLNet\n\n\n\n\n\n\n\n\n\n\n\n\n\nQQP\nBERT (S)\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoBERTa\n\n\n\n\n\n\n\n\n\n\n\n\n\nBERT\n\n\n\n\n\n\n\n\n\n\n\n\n\nXLNet\n\n\n\n\n\n\n\n\n\n\n\n\n\nMNLI-m\nBERT (S)\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoBERTa\n\n\n\n\n\n\n\n\n\n\n\n\n\nBERT\n\n\n\n\n\n\n\n\n\n\n\n\n\nXLNet\nIn contrast to the current embedding-level distillation methods, text-level synthetic datasets from DiLM can be leveraged for training different models independent of their word embedding weights.\nTo emphasize this advantage, we evaluated the distilled synthetic datasets for training three models different from BERT, with which the distilled synthetic datasets were obtained,\ni.e., RoBERTa, BERT, and XLNet.\nTable 2  ###reference_### summarizes the performances of Random, K-centers, and DiLM with DPC=20, where DiLM achieved stably good performances.444We also show the results with other DPC settings in Appendix F  ###reference_###.\nThe results indicate that the distilled datasets from DiLM consistently performed well for training the different models, even though DiLM trained gradient matching only for the BERT model\u2019s parameters.\nIt is worth noting that our distilled datasets show successful generalization performance not only for training RoBERTa and BERT, which have the same model architecture as BERT, but also for training XLNet, which is an autoregressive model using the hidden state of the <eos> token for classification, while BERT is an autoencoding model using the hidden state of the [CLS] token.\nModels\nRandom\nK-centers\nDiLM\n\n\n\nGPT-2-XL (1.5B)\n\n\n\n\nOPT (2.7B)\n\n\n\n\nLlama 2 (7B)\nWe also evaluated the distilled datasets from DiLM as few-shot prompts for in-context learning of LLMs.\nTable 3  ###reference_### shows the performance of Random, K-centers, and DiLM for in-context learning for SST-2 with three different sizes of LLMs, GPT-2-XL Radford et al. (2019  ###reference_b20###), OPT Zhang et al. (2022b  ###reference_b39###), and Llama 2 Touvron et al. (2023  ###reference_b26###).\nSurprisingly, the distilled datasets from DiLM consistently performed well for the in-context learning, compared with Random and K-centers.\nThese remarkable generalization performances across models and training processes strongly support the advantage of DiLM to distill datasets at the text-level."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Analysis and Discussion",
            "text": "Ablation Study. Table 4  ###reference_### shows the results of the ablation study for the performance improvement techniques of the representative teacher for gradient matching, the diverse mini-batch sampling of synthetic samples during training of DiLM (in Section 3.3  ###reference_###), and the representative sample selection with K-centers during synthetic dataset generation (in Section 3.4  ###reference_###).\nThe results demonstrated that all the three techniques are consistently effective for DiLM.\nRT\n\n\n\nDMS\n\n\n\nSelection\n\n\n\nSST-2\n\n\n\nQQP\n\n\n\nMNLI-m\n\n\n\n\n\u2713\n\n\n\n\u2713\n\n\n\n\u2713\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n-\n\n\n\n\u2713\n\n\n\n\u2713\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u2713\n\n\n\n-\n\n\n\n\u2713\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u2713\n\n\n\n\u2713\n\n\n\n-\nScaling of DPC.\nWe investigated the performance of DiLM when increasing the size of synthetic datasets.\nNote that DiLM does not require retraining the generator model for generating distilled synthetic datasets for different DPCs, which is also the advantage of using generative models for dataset distillation.\nAs shown in Figure 2  ###reference_###, the performance of the distilled datasets generally scaled with increasing DPC.\n###figure_2### Distilled Data Examples.\nWe gave examples of distilled synthetic samples for each dataset in Appendix G  ###reference_###.\nWe found that DiLM successfully generated interpretable synthetic samples that are appropriate for the tasks of the original datasets.\nAlthough DiLM consistently generated high quality synthetic samples for SST-2 and QQP, the repetition problem can be observed in some lengthy samples for MNLI-m.\nThis suggests that there is still room for performance improvements of DiLM by using a larger and more sophisticated pre-trained language model for the generator model than the small GPT-2 used in our current experiments."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We proposed the first text-level dataset distillation approach, called DiLM, which trains a language model to generate informative synthetic samples as text data for model-agnostic applicability and interpretability of distilled datasets.\nExperimental results across various text classification datasets indicated that the distilled datasets from DiLM achieve successful performance for training various types of models beyond the source model used for distillation, even for in-context learning of LLMs."
        }
    ],
    "url": "http://arxiv.org/html/2404.00264v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.4"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "5.1",
            "5.2",
            "5.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "5.3"
        ]
    },
    "research_context": {
        "paper_id": "2404.00264v1",
        "paper_title": "DiLM: Distilling Dataset into Language Model for Text-level Dataset Distillation",
        "research_background": "### Paper's Motivation:\nThe primary motivation of the paper springs from the computational challenges and limitations associated with training large language models (LLMs) on extensive datasets. The authors identify the significant resources and time needed for training and fine-tuning LLMs, which hinders the development and deployment of new models. Therefore, the paper aims to explore an efficient alternative through dataset distillation, particularly focusing on text-level dataset distillation to make the process more practical and interpretable.\n\n### Research Problem:\nThe principal research problem the paper attempts to tackle is the creation of synthetic text datasets that are not only optimized for training but also retain the compatibility for training various models with different word embedding weights. The challenge lies in overcoming the discrete nature of text, which complicates the dataset distillation process. Current methods that optimize at the embedding level fail to provide usable and interpretable datasets across different models, thus limiting their practical applicability.\n\n### Relevant Prior Work:\n1. **Large Language Models (LLMs) and Training Costs:**\n   - The paper acknowledges prior work on large language models such as BERT and GPT families, highlighting their capabilities and the associated high computational costs (Devlin et al., 2019; Liu et al., 2019; Radford et al., 2019; Brown et al., 2020).\n\n2. **Dataset Distillation:**\n   - Previous dataset distillation techniques aimed at reducing training costs by compressing training datasets, demonstrating significant reductions in training data size while maintaining performance (Wang et al., 2018b; Zhao et al., 2021; Cazenavette et al., 2022; Wang et al., 2022; Zhao and Bilen, 2023).\n\n3. **Applications of Dataset Distillation:**\n   - The paper references various applications of dataset distillation in fields such as neural architecture search (Such et al., 2020; Medvedev and D\u2019yakonov, 2021), federated learning (Zhang et al., 2022a; Xiong et al., 2023), continual learning (Wiewel and Yang, 2021; Sangermano et al., 2022), and privacy preservation (Dong et al., 2022; Chen et al., 2022).\n\n4. **Text Dataset Distillation:**\n   - Prior studies focused on text dataset distillation, mostly relying on embedding-level distillation techniques due to the challenges posed by the discrete nature of text (Sucholutsky and Schonlau, 2021; Li and Li, 2021; Maekawa et al., 2023; Sahni and Patel, 2023). \n\n5. **Challenges with Discrete Text:**\n   - The difficulty with text dataset distillation arises from optimizing discrete text, which necessitates novel approaches to circumvent non-differentiable elements (Geng et al., 2023; Yu et al., 2023).\n\n6. **Optimization Objectives:**\n   - Different optimization techniques such as meta-learning, gradient matching, training trajectory matching, and feature distribution matching have been explored for effective dataset distillation (Wang et al., 2018b; Zhao et al., 2021; Cazenavette et al., 2022; Wang et al., 2022; Zhao and Bilen, 2023).\n\n### Conclusion:\nIn light of these challenges and prior efforts, the paper proposes \"Distilling dataset into Language Model (DiLM)\" to address the core limitations of existing methods by employing a language model as a continuous optimization target, allowing for effective back-propagation and gradient matching loss minimization, ultimately leading to practical, interpretable, and versatile distilled text datasets suitable for a variety of NLP model training scenarios.",
        "methodology": "DiLM: Distilling Dataset into Language Model for Text-level Dataset Distillation\n\nMethodology:\nIn this section, we introduce DiLM, which distills text datasets into text data, not word embeddings, for the model-agnostic applicability and interpretability of the distilled synthetic datasets. The main idea of DiLM is to avoid the optimization difficulties of discrete text by instead training continuous parameters of a language model as a surrogate optimization target of dataset distillation.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n#### Datasets\nThe main experiment evaluated DiLM by distilling three significant text classification datasets from the GLUE benchmark: SST-2, QQP, and MNLI-m. According to Wang et al. (2018), accuracy was reported for SST-2 and MNLI-m, while for QQP, the average of accuracy and F1 score was reported.\n\n#### Baselines\nThe performance of DiLM was compared against three coreset selection methods commonly used in the computer vision field:\n1. **Random**\n2. **K-centers** (Wolf, 2011; Sener and Savarese, 2018)\n3. **Herding** (Welling, 2009)\n\nAdditionally, the performance was compared with TDD (Sucholutsky and Schonlau, 2021), a recent embedding-level distillation method. TDD involves training learnable soft-labels and learning rates for each training step along with the input word embeddings.\n\nThe baseline evaluations included a vanilla LM that skipped training for gradient matching and applied K-center representative sample selection.\n\n#### Evaluation Metrics\nThe evaluation was performed using BERT and three other models: RoBERTa, BERT, and XLNet. These models were trained on the distilled datasets for 200 steps using AdamW (Loshchilov and Hutter, 2019) with a learning rate and a batch size of 64. However, for TDD, a different training protocol was followed since it optimizes learning rates for each step with specific synthetic sample orders. \n\nHerding and TDD models were trained on their datasets 100 times, while other methods generated 20 datasets with varying random seeds and trained the models on each dataset five times. The results are reported as an average and standard deviation of these 100 models.\n\n#### Implementation Details\n- **Generator Model:** 128M parameter version of GPT-2.\n- **Learner Model:** BERT, on which the gradient matching loss was calculated.\n- **Gradient Matching Loss Calculation:** Only performed for the randomly initialized last layer parameters to reduce computational cost.\n- **Training Parameters:** Number of loops for training DiLM was set to different intervals; the mini-batch sizes of real and synthetic samples were respectively set to certain values.\nFurther details are documented in Appendix E.\n\n### Main Experimental Results\nSignificant differences were observed between DiLM and the baseline methods, particularly K-centers, indicated by the Welch\u2019s t-test. Despite the large standard deviations due to training models with few selected/generated samples, the reliability of the results is supported by a robust procedure involving 100 runs per model. This allows for a thorough discussion on the effectiveness of the proposed DiLM method. Additional results and statistical validation can be found in the result tables provided."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To assess the effectiveness of different performance improvement techniques for DiLM, including gradient matching with a representative teacher, diverse mini-batch sampling during training, and representative sample selection using K-centers.",
            "experiment_process": "The researchers conducted ablation experiments to determine the impact of three specific techniques on the performance of DiLM: (1) gradient matching with a representative teacher (RT), (2) diverse mini-batch sampling (DMS) during DiLM's training process, and (3) representative sample selection using K-centers during synthetic dataset generation. The experiments were carried out on several text classification datasets, including SST-2, QQP, and MNLI-m. The study assessed the models' performance without each technique to ascertain their individual contributions by referring to Table 4 results.",
            "result_discussion": "The results demonstrated that all three techniques are consistently effective for improving DiLM's performance. When any of the techniques were omitted, there was a noticeable decline in performance across the datasets. This indicates that gradient matching with a representative teacher, diverse mini-batch sampling, and K-centers sample selection are integral to the effectiveness of DiLM.",
            "ablation_id": "2404.00264v1.No1"
        },
        {
            "research_objective": "To evaluate the scalability of DiLM's performance concerning the size of synthetic datasets, termed Distilled Dataset Proportion (DPC).",
            "experiment_process": "The researchers investigated DiLM's performance as the size of synthetic datasets (DPC) increased. Notably, DiLM did not require retraining of the generator model to produce distilled synthetic datasets for different DPCs, leveraging the advantage of using generative models. The performance scaling was visualized in Figure 2, illustrating how the dataset size impacted the model's effectiveness.",
            "result_discussion": "The performance of the distilled datasets generally improved with increasing DPC, demonstrating DiLM's scalability. This shows that larger synthetic datasets generated by DiLM can further enhance the model's performance without necessitating additional generator model training.",
            "ablation_id": "2404.00264v1.No2"
        },
        {
            "research_objective": "To provide examples and assess the interpretability and task-appropriateness of synthetic samples generated by DiLM.",
            "experiment_process": "Examples of distilled synthetic samples were provided for each dataset, documented in Appendix G. These samples were scrutinized to determine if DiLM could generate interpretable and task-appropriate samples for the original datasets\u2019 tasks.",
            "result_discussion": "DiLM successfully generated high-quality synthetic samples for SST-2 and QQP, which were appropriate and interpretable for the respective tasks. However, some lengthy samples for MNLI-m displayed a repetition problem, indicating potential areas for performance improvement. The study suggested that a larger and more sophisticated pre-trained language model than the currently used small GPT-2 could mitigate this issue and enhance sample quality.",
            "ablation_id": "2404.00264v1.No3"
        }
    ]
}