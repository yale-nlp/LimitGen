{
    "title": "Controlled Training Data Generation with Diffusion Models",
    "abstract": "In this work, we present a method to control a text-to-image generative model to produce training data specifically \u201cuseful\u201d for supervised learning.\nUnlike previous works that employ an open-loop approach and pre-define prompts to generate new data using either a language model or human expertise, we develop an automated closed-loop system which involves two feedback mechanisms.\nThe first mechanism uses feedback from a given supervised model and finds adversarial prompts that result in image generations that maximize the model loss.\nWhile these adversarial prompts result in diverse data informed by the model, they are not informed of the target distribution, which can be inefficient.\nTherefore, we introduce the second feedback mechanism that guides the generation process towards a certain target distribution.\nWe call the method combining these two mechanisms Guided Adversarial Prompts.\nWe perform our evaluations on different tasks, datasets and architectures, with different types of distribution shifts (spuriously correlated data, unseen domains) and demonstrate the efficiency of the proposed feedback mechanisms compared to open-loop approaches.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The quality of data plays a crucial role in training generalizable deep learning models [74  ###reference_b74###, 52  ###reference_b52###, 21  ###reference_b21###].\nFor a model to generalize well, its training data should be representative of the test distribution where it will be deployed.\nHowever, real world test conditions change over time, while training datasets are typically collected once and remain static due to high collection costs.\nWe, therefore, focus on generating datasets that can adapt to novel test distributions and are more cost-efficient.\nDiffusion generative models [61  ###reference_b61###, 33  ###reference_b33###, 70  ###reference_b70###, 55  ###reference_b55###, 67  ###reference_b67###] are trained on large-scale collections of images [69  ###reference_b69###] and exhibit remarkable generalization abilities by being able to produce realistic images not seen during training.\nAdditionally, unlike static datasets that they are trained on, these generative models allow us to adapt the generation process to produce images that follow a certain conditioning. For example, they can be conditioned on textual prompts [61  ###reference_b61###] or geometric information such as depth maps [82  ###reference_b82###].\nRecent works explore the use of diffusion models to generate training data for supervised learning with promising results [68  ###reference_b68###, 18  ###reference_b18###, 28  ###reference_b28###].\nThey guide the generation process using text prompts to accomplish two goals: produce aligned image-label pairs for supervised training and adapt the generated images to a certain target distribution.\nThese proposed methods, however, find conditioning text prompts in an open-loop way by either using a language model [18  ###reference_b18###] or heuristics [68  ###reference_b68###].\nTherefore, they lack an automatic feedback mechanism that can refine the found text prompts to produce more curated and useful training data. Furthermore, it has been argued that being able to control the input data is a key contributor to how children are able to learn with few examples [6  ###reference_b6###, 44  ###reference_b44###].\nIn this work, we propose two feedback mechanisms to find prompts for generating useful training data.\nThe first mechanism finds prompts that result in generations that maximize the loss of a particular supervised model, thus, reflecting its failure modes.\nWe call them Adversarial Prompts (AP).\nThis mechanism ensures that we find not only novel prompts, which may produce images that the model already performs well on, but adversarial prompts that produce images with high loss, and, thus, useful for improving the model [14  ###reference_b14###] (see exemplar generations for AP in Figs. 1  ###reference_###, 3  ###reference_### and 5  ###reference_###)\nA given model can perform poorly on multiple distribution shifts, and traversing all of them with adversarial optimization to adapt it to a specific target distribution can be inefficient (e.g., see the difference between AP generations and the illustrated target distributions in Fig. 1  ###reference_###-right).\nTherefore, we introduce an additional target-informed feedback mechanism that finds prompts that generate images similar to those from the target distribution we want to adapt to.\nTo implement this, we assume either access to a textual description of the target distribution or a few sampled (unlabeled) images from it.\nWe then optimize a similarity metric between CLIP [57  ###reference_b57###] embeddings of the generated examples and the target description. We call these prompts Guided Adversarial Prompts (GAP).\nCompare the columns Adversarial Prompts and Guided Adversarial Prompts in Figs. 1  ###reference_###, 3  ###reference_###, 4  ###reference_### and 5  ###reference_### to see the effect of CLIP guidance in steering the generations towards a specific target distribution.\nWe demonstrate the effectiveness of our method on different tasks (image classification, depth estimation), datasets with distribution shifts (Waterbirds [66  ###reference_b66###], iWildCam [4  ###reference_b4###, 41  ###reference_b41###], Common Corruptions [29  ###reference_b29###], 3D Common Corruptions [37  ###reference_b37###]), and architectures (convolutional and transformer) with supportive results."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Open-loop data generation methods use pre-defined controls to guide the generative process and produce novel training examples. One line of work uses GANs [35, 8, 59] and pre-defines a perturbation in their latent space to generate novel examples. More recent works adopt text-to-image diffusion models and use pre-defined prompt templates [68, 77, 27] or use a language model to generative variations of a given prompt [77]. These methods require anticipating the kind of data that will be seen at test-time when defining the prompts. On the other hand, our CLIP guidance mechanism allows us to generate images similar to the target distribution. [18] also approach this problem by using a captioning and language model to summarize a target distribution shift into a text prompt. However, this summarization process is not informed of the generations, and, thus, does not guarantee that the text prompt will guide the generation process to images related to the target distribution. Finally, these methods are not model-informed and do not necessarily generate images useful for training a given model. Closed-loop data generation methods guide the generation process via an automatic feedback mechanism. They control the latent space of GANs [5] or VAEs [76] models, NeRF [15], or the space of hand-crafted augmentations [10] to generate data that maximizes the loss of the network on the generated data. Similarly, [36] uses an SVM to identify the failure modes of a given model and uses this information to generate training data with a diffusion model. Our method employs a similar adversarial formulation (in conjunction with target distribution guidance) but performs the optimization in the text prompt space of recently developed diffusion models. \u201cShallow\u201d data augmentation techniques apply simple hand-crafted transformations to training images to increase data diversity and improve the model\u2019s generalization. Examples of such transformations are color jitter, random crop, and flipping, etc. To produce more diverse augmentations, methods like RandAugment [11] and AugMix [30] combine multiple of such simple transformations, and Mixup [81] and CutMix [78] methods use transformations that can combine multiple images. AutoAugment [10] and adversarial training [49] build a closed system to tune the parameters of the applied augmentations but are inherently limited by the expressiveness of the simple transformations. In contrast, our method uses expressive diffusion models, which results in images that are more diverse and realistic than those produced by \u201cshallow\u201d augmentations. Controlling diffusion models. Methods like ControlNet [82] and T2I-Adapter [54] adapt a pre-trained diffusion model to allow for additional conditioning e.g., edge, segmentation, and depth maps. We employ these models for generation as it allows us to generate paired data for different tasks, given the labels from an existing dataset. Editing methods aim to modify a given image, either via the prompt [32], masks [9], instructions [7] or inversion of the latent space [53, 34]. In contrast, personalization methods aim to adapt diffusion models to a given concept e.g., an object, individual, or style. Popular examples include textual inversion [22] and DreamBooth [64], which aim to find a token to represent a concept given several images of that concept. The former freezes the diffusion model, while the latter fine-tunes it. Extensions of these works learn to represent multiple concepts [1, 24]. In our work, we adopt an approach similar to textual inversion to steer the diffusion model, but our method can also be used with other controlling mechanisms.\n\nAI ethics in facial recognition technology has raised concerns due to biases inherent in training datasets, privacy invasions, and misidentification risks [12, 23, 33]. Studies urge for more transparent algorithms, robust privacy protocols, and inclusive data collection [44, 56]. Researchers continue to explore ethical frameworks to balance technological advances with social responsibility [67, 89]."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Method",
            "text": "We begin this section by formalizing our problem setting and describing how diffusion models can be used to generate training data (Sec. 3.1  ###reference_###).\nWe then introduce two feedback mechanisms to find prompts that are informed of the failure modes of a given model (Sec. 3.2  ###reference_###) and relevant to a given target distribution (Sec. 3.3  ###reference_###)."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Preliminaries",
            "text": "Problem Formulation.\nWe consider the problem of supervised learning, where a model  learns a mapping from the image space , to a target space , e.g., a depth estimation or semantic classification problem.\nThe model  is trained using training dataset  and tested on a new set  that exhibits a distribution shift w.r.t. the training data.\nOur goal is to generate additional synthetic training data  to adapt the model and improve its performance under the distribution shift.\nTo apply the target-informed feedback mechanism described in Sec. 3.3  ###reference_###, we assume access to some information about the test distribution, either from text descriptions or a few samples of unlabeled images.\nText-to-image Diffusion Models.\nWe use the Stable Diffusion [62  ###reference_b62###] text-to-image diffusion model as the basis for our generator .\nGiven a textual prompt , Stable Diffusion is capable of synthesizing realistic images following the textual conditioning.\nHowever, in general, for a given task, e.g., depth estimation, a textual prompt alone may not be sufficient for controlling the generation well enough to produce aligned image-label examples.\nGenerating aligned training examples.\nWe employ the following two approaches to condition the generative model  on the label  and sample aligned training examples .\nFor the depth estimation task, we use the ControlNet [83  ###reference_b83###] model which extends the conditioning mechanisms of the Stable Diffusion to accept various spatial modalities, e.g., depth maps, segmentation masks, or edges.\nSpecifically, we use ControlNet v1.0 with depth conditioning111https://github.com/lllyasviel/ControlNet  ###reference_###.\nFor semantic classification tasks, we utilize the foreground object masks and use an in-painting technique proposed in [47  ###reference_b47###] that preserves the masked region throughout the denoising process, essentially keeping it intact.\nThese mechanisms provide us with a generative model conditioned both on a text prompt  and label .\nWe denote the resulting distribution modeled by this generative model as ."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Model-Informed Generation with Adversarial Prompt Optimization",
            "text": "Our first feedback mechanism aims at generating training examples that reflect the failure modes of a given model .\nAn automatic way to do so is via adversarial optimization, which finds the \u201cworst case\u201d failure modes of .\nMore precisely, we find a text prompt  that generates images  that maximize the supervised loss , e.g.,  loss for depth estimation.\nSince the usual prompt space is discrete (text tokens) and challenging to optimize over, we employ the approach introduced in Textual Inversion [22  ###reference_b22###] and instead optimize over the corresponding continuous embedding space. For ease of notation, \u201cprompt space\u201d will implicitly refer to the continuous embedding space instead of the discrete token space. We construct a prompt  out of  new \u201cplaceholder\u201d tokens, i.e., , and find their corresponding embedding weights  by solving the following optimization problem:\nwhere  and  is sampled from .\nNote that the sample  is differentiable w.r.t. the embeddings  which allows us to use gradient-based optimization. We call the prompts that result from solving the above optimization problem Adversarial Prompts (AP).\nAvoiding  alignment collapse.\n\nThe adversarial objective in Eq. 1  ###reference_### aims to fool the model . However, it may instead fool the label-conditioning mechanism of the generative model , resulting in  generating samples  that are not faithful to  (see Fig. 2  ###reference_###).\n###figure_1### To avoid this, we further constrain the expressiveness of the generation process. There are several ways to do so.\nOne way is to use the SDEdit method [50  ###reference_b50###], which conditions the generation process on the original image by starting the denoising process from a noised version of  instead of pure noise. Thus, it constrains the expressive power of the generative model to produce samples closer to the original image .\nAdditionally, some constraints can be implemented w.r.t . For the depth estimation task, we employ an early stopping criterion and stop the adversarial optimization when the loss reaches a certain task-specific threshold. For semantic classification, choosing  to be the negative cross-entropy loss, although natural, may not be a good choice. Indeed, for iWildCam, although we keep the class mask intact, we observed that optimizing the negative cross-entropy loss may lead to the generation of another class somewhere else in the image, e.g. an elephant is generated next to a giraffe, destroying the  alignment. Thus, for iWildCam, we choose to maximize the uncertainty or entropy of the model\u2019s prediction on the generated images. We provide more details in the Appendix Sec. 7.5.2  ###reference_.SSS2###.\nFinally, our CLIP [57  ###reference_b57###] guidance loss introduced in Sec. 3.3  ###reference_### further constrains possible perturbations to a target distribution and helps to avoid the generation of non-realistic images."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Target Distribution Informed Generation",
            "text": "The adversarial formulation above finds prompts that reflect the failure modes of .\nWithout any information about the target distribution, improving the model on the worst-performing distributions is one of the best strategies one can do and, indeed, improves performance in some cases (see Fig. 4  ###reference_### and Fig. 5(a)  ###reference_sf1###).\nHowever, there are typically multiple failure modes of a given model and many possible distribution shifts that can occur at test-time.\nAdapting to all of them using only the first feedback mechanism could be inefficient when the goal is to adapt to a specific target distribution instead of improving the performance on average.\nThus, we introduce the second feedback mechanism to inform the prompt optimization process of the target distribution.\nThis only requires access to simple text descriptions (e.g., \u2018fog\u2019 to adapt to foggy images) or a small number () of unlabelled images.\nWe implement the target-informed feedback mechanism using CLIP [57  ###reference_b57###] guidance.\nSpecifically, we assume access to either textual descriptions of the target image distribution , a few unlabeled image samples  or both.\nWe then construct the corresponding text and image guidance embeddings as  and , where  and  denote, respectively, the CLIP text and image encoders, and  stand for averaging.\nWe then use the following guidance loss:\nwhere we take  to be  norm between two embeddings and  to be the negative cosine similarity, as we found it to perform the best. See the Appendix Sec. 7.7  ###reference_### for the results of this ablation.\nNote, that based on the available information, one can also use only one of the two guidance losses.\nFinally, we combine both adversarial, Eq. 1  ###reference_###, and CLIP guidance, Eq. 2  ###reference_###, losses to form the final objective:\nWe call the prompts that result from solving Eq. 3  ###reference_###, Guided Adversarial Prompts\n(GAP).\nSee the Appendix Secs. 7.4.2  ###reference_.SSS2### and 7.5.3  ###reference_.SSS3### for further implementation details.\n###figure_2###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We perform experiments in three settings: domain generalization via camera trap animal classification on the iWildCam [4  ###reference_b4###] dataset, bird classification with spurious correlation with the Waterbirds [66  ###reference_b66###] dataset, and depth estimation with the Taskonomy dataset [80  ###reference_b80###, 79  ###reference_b79###].\nFor depth estimation, the considered distribution shifts are Common\nCorruptions [29  ###reference_b29###] (CC), 3D Common Corruptions [37  ###reference_b37###] (3DCC) applied on the Taskonomy [80  ###reference_b80###] test set and cross dataset shift from the Replica [73  ###reference_b73###] dataset."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Semantic Classification",
            "text": "Waterbirds [66  ###reference_b66###] is a dataset constructed by pasting an image of either a waterbird or landbird from the CUB [75  ###reference_b75###] dataset, which represents the label , onto a \u201cland\u201d or \u201cwater\u201d background image from the Places [84  ###reference_b84###] dataset.\nWe follow [18  ###reference_b18###] and take only images of waterbirds appearing on water and landbirds on land background as , i.e., in the training data, the background alone is predictive of the bird class.\nThe examples from the test distribution  contain all four combinations of bird class and background.\niWildCam [3  ###reference_b3###, 42  ###reference_b42###] is a domain generalization dataset, made up of a large-scale collection of images captured from camera traps placed in various locations around the world. We seek to learn a model that generalizes to photos taken from new camera deployments. We follow [18  ###reference_b18###] and subsample the dataset to create a 7-way classification task (background, cattle, elephant, impala, zebra, giraffe, dik-dik), with 2 test locations that are not in the training or validation set. We also fixed the number of additional generated images for finetuning to 2224 images in order to match the setting of [18  ###reference_b18###].\nFor each dataset, we compare the following methods (we provide more details in Appendix Sec. 7  ###reference_###):\nNo Extra Data: We train a ResNet50 [26  ###reference_b26###] model using the original training data  without using any extra data.\nAugmentation baselines: We compare to two data augmentation baselines taken from recent literature: CutMix [78  ###reference_b78###] and RandAugment [12  ###reference_b12###].\nAgnostic Prompts: We use a prompt that is not informed of the model or the target distribution. Similar to ALIA [18  ###reference_b18###], we use a prompt \u201cnature\u201d for Waterbirds and the prompt template \u201ca camera trap photo of {class name}\u201d.\nGuided Prompts (ALIA [18  ###reference_b18###]): This approach uses a captioning and language model to summarize a target distribution shift into text prompts. Specifically, we use the prompts found by the ALIA method. This results in seven prompts for Waterbirds and four prompts for iWildCam. See Appendix Sec. 7.2  ###reference_### for more details and a discussion on the differences between ALIA and our method.\nAdversarial Prompts: We use the model trained without extra data as the target model  and find adversarial prompts following Eq. 1  ###reference_###. We find four prompts per class\nfor Waterbirds, eight in total, and four prompts in total applied to all classes for iWildCam.\nGuided Adversarial Prompts: We use the same setting as in Adversarial Prompts and apply additional CLIP guidance to adapt to a target distribution shift, following Eq. 3  ###reference_###. For Waterbirds, we apply text guidance using ALIA prompts as the textual description of the target distribution. For iWildCam, we use image guidance and partition the target test distribution into four groups based on two attributes that have significant impact on the visual characteristics of the data; the test location (first or second) and time of the day (day or night). We sample 64 unlabelled images randomly from each group. We optimize for one guided adversarial prompt per group.\nTraining details.\nFor both datasets, we perform adversarial optimization with constant learning rate of 1e-3 using Adam [38  ###reference_b38###]. We use the DDIM [72  ###reference_b72###] scheduler. See Tab. 1  ###reference_### for a summary of the experimental parameters. The adversarial loss  from Eq. 1  ###reference_### is defined as the negative cross-entropy loss for Waterbirds. As mentioned in Sec. 3.2  ###reference_###, we optimize the entropy loss for iWildCam. More precisely, this loss is equal to the cross entropy loss where the target label  is replaced by the soft label , the uniform distribution over all classes. This loss explicitly encourages generations that either (1) do not contain new animals (2) contain new animals that are not accounted for in the label space . For more details, see the Appendix.\nWaterbirds results.\nFig. 3  ###reference_###-left shows the performance of each method, and Fig. 3  ###reference_###-right demonstrates the corresponding generated examples for each method.\nFirst, we find that while Adversarial Prompts works on par with the Agnostic Prompts in a low-data regime, it performs worse with more generated data.\nLooking at the corresponding generations, we see that, while being adversarial, these images are different from the target distribution, which can explain the inferior performance on this particular target distribution.\nSecond, using text prompts informed of the target leads to a consistent improvement over the Agnostic Prompts, and the corresponding images look more similar to the real images from the target distribution. Finally, Guided Adversarial Prompts combining both feedback mechanisms results in more data-efficient generations outperforming all other methods in the low-data regime.\nLooking at the corresponding generations, we can see that GAP tends to generate only useful combinations, i.e., waterbirds on land and landbirds on water that are missing in the training dataset.\nThis can explain its higher sample efficiency.\niWildCam results.\nFig. 4  ###reference_### shows that having model-informed feedback (Adversarial Prompts) helps to generate more useful data than no feedback mechanism (Agnostic Prompts) and also improves the performance of the target-only informed Guided Prompts method in the low-data regime.\nGuided Adversarial Prompts combines the benefits of both model- and target-informed feedback mechanisms, consistently outperforming other methods.\nFig. 4  ###reference_### shows exemplar generations for each method.\nWe find that while AP generates images distinct from the target distribution and images generated by target-informed methods (snow background vs. grass background), training a model using these examples in the low-data regime performs better than GP and similar to GAP.\n###figure_3### Fine-tuning with Guided Adversarial Prompts from a different model.\nTo evaluate our method\u2019s ability to customize data to a model, , and assess its resilience to changes in model architecture, we changed the model from a ResNet50 [26  ###reference_b26###] to a ViT-B-16 [16  ###reference_b16###] model.\nIn Tab. 2  ###reference_###, we investigate whether prompts optimized using one model feedback can be useful for finetuning another model, and conclude that model feedback does indeed generate data more useful for that particular model.\nFurthermore, the data generated from one model when used to fine-tune another model can still significantly improve performance over Agnostic Prompts.\nSee Appendix Fig. 14  ###reference_### for the results of Fig. 4  ###reference_### with a ViT-B-16 model, similar trends hold."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Depth Estimation",
            "text": "For depth estimation, we consider the following pre-trained models as : 1) a U-Net [63  ###reference_b63###] model trained on the Taskonomy dataset [80  ###reference_b80###, 79  ###reference_b79###] and 2) a dense prediction transformer (DPT) [58  ###reference_b58###] model trained on Omnidata [20  ###reference_b20###].\nWe compare the following methods. They all involve fine-tuning , but on different datasets. We use ControlNet v1.0 with depth conditioning for the experiments in this section. See Fig. 5  ###reference_### for a comparison of the generations:\nControl (No extra data): We fine-tune  on the original training data. This baseline is to ensure that the difference in performance is due to the generated data, rather than e.g., longer training or optimization hyperparameters.\nAgnostic Prompts: This baseline generates data that is agnostic to the model or the target distribution. We generate images with the prompt \u201croom\u201d as the datasets consist of indoor images from mostly residential buildings.\nAgnostic Prompts (Random): We generate data with \u201crandom\u201d prompts. In our proposed method, we optimize for  embedding vectors, resulting in a prompt, . Thus, to match this setting, from a Gaussian distribution fitted on the embeddings from the vocabulary, we sample  random embeddings to create a random prompt to be used in the data generation.\nAdversarial Prompts: We perform the optimization as described in Eq. 1  ###reference_### and fine-tune on this data.\nGuided Prompts: We perform the optimization using only the loss described in Eq. 2  ###reference_### and fine-tune on this data.\nGuided Adversarial Prompts: In addition to optimizing the adversarial loss, we also optimize the CLIP guidance loss as described in Eq. 3  ###reference_###. This allows us to generate data that is also informed of a certain distribution shift.\nTraining details.\nThe adversarial optimization was done with AdamW [46  ###reference_b46###], learning rate of , weight decay of , and batch size of 8. We set the early stopping threshold (mentioned in Sec. 3.2  ###reference_###) to 0.08 for the UNet model and 1.0 for the DPT model. They were trained with  and Midas loss [20  ###reference_b20###] respectively.\nWe perform a total of 30 runs to get different Adversarial Prompts. We use the DDIM [72  ###reference_b72###] scheduler. During optimization, we use only 5 denoising steps, as it is more stable.\nFor Guided Adversarial Optimization, the guidance coefficient for text and image guidance is 1 and 5 respectively.\nFor fine-tuning, we generate images with 15 steps. For the GP runs with SDEdit, we used strength 0.6, for the GAP runs, strength 0.9. See the Appendix Sec. 8.1  ###reference_### for further details.\n###figure_4### U-Net\nDPT\n\n\nTaskonomy\nReplica\nTaskonomy\n\nShift\nClean\nCC\n3DCC\nCDS\nCC\n3DCC\n\nControl (No extra data)\n2.35\n4.93\n4.79\n5.38\n3.76\n3.42\n\nAgnostic Prompts\n2.47\n5.03\n4.17\n5.30\n4.06\n3.58\n\nAgnostic Prompts (Random)\n2.38\n4.96\n4.11\n5.14\n3.88\n3.51\n\nAdversarial Prompts\n2.49\n4.36\n4.02\n5.12\n3.40\n3.28\n\nAdversarial Prompts (SDEdit)\n2.59\n4.20\n3.88\n4.96\n3.35\n3.25\n###figure_5### Comparing the generated images with different prompts.\nFig. 5  ###reference_###-left shows the results of the generations for the baselines and our method, optimized on the Taskonomy dataset. The generation with Agnostic Prompts are visually different from that of the original image, however, they tend to have similar styles. In contrast, the generations with Adversarial Prompts have more complex styles and are more diverse.\nUsing SDEdit during the optimization and generation results in generations that are closer to the original image, as it was also used as conditioning.\nThe last four columns show the results of using CLIP text guidance for the target distribution shift fog and blur, as described in Sec. 3.3  ###reference_###, with and without adversarial optimization.\nThe generations with Guided Prompts involve passing the depth conditioning and prompt \u201cfog\u201d or \u201cblur\u201d to the diffusion model. In both cases, the generations result in a mild level of fog or blur.\nIn contrast, Guided Adversarial Prompts results in more severe fog and blur corruptions.\nSee Appendix Sec. 8.4.2  ###reference_.SSS2### for generations using image guidance.\nNote that all the generations follow the conditioning, i.e., depth labels (see first column).\nSee Sec. 3.2  ###reference_### for the discussion on how we prevent the generations from collapsing.\nThus, this gives us aligned training data, i.e., RGB images and depth labels that we can use for fine-tuning.\nPerformance on OOD data.\nWe evaluate our method and the baselines after fine-tuning on their respective generated datasets. Tab. 5(a)  ###reference_sf1### demonstrated that data generated with Adversarial Prompts improve the performance of the model under different distribution shifts.\nFurthermore, we show that the trend holds with the DPT model.\nThus, our method is able to successfully find useful Adversarial Prompts for different architectures.\nPerformance of GAP against amount of generated data. Fig. 5  ###reference_###-right shows the performance of our method on the defocus blur corruption over the amount of extra generated data (See Appendix Fig. 18  ###reference_### for results on other corruptions).\nGuided Prompts or Guided Adversarial Prompts results in a large improvement in performance, compared to Adversarial Prompts or the baseline with only 10 extra data points.\nThis suggests that the guidance loss successfully steered the generations toward producing training data relevant to the distribution shift.\nThis experiment was performed with image guidance. In Appendix Sec. 8.4.2  ###reference_.SSS2###, we compare the qualitative and quantitative differences from image and text guidance. Text guided prompts tends to generate corruptions that are more realistic (see Appendix Fig. 18  ###reference_### for comparisons). However, image guidance tends to perform better quantitatively across the different target distributions.\nUnlike classification, the performance of Guided Adversarial Prompts for depth is not consistent across all distribution shifts. The diffusion model was not able to generate certain shifts e.g., noise corruptions, as the text descriptions and unlabelled images was too ambiguous e.g., \u2018noise\u2019 or having common attributes other than the corruption. We leave further analysis to future work.\nRunning multiple iterations of adversarial optimization vs a single iteration.\nWe define an iteration as one round of adversarial optimization, i.e. optimizing Eq. 1  ###reference_### or Eq. 3  ###reference_###, generation and fine-tuning.\nGiven that all of the above results were obtained with a single iteration, we aim to see if there are benefits in performing multiple iterations.\nWe perform a total of 8 iterations and we compare this to performing a single iteration. The experimental settings for 8 iterations and a single iteration are similar, e.g., in total, over the 8 iterations, we optimize for the same number of Adversarial Prompts, and fine-tune on the same datapoints, etc.\nFig. 5(b)  ###reference_sf2### shows that the first iteration of the 8 iterations setting resulted in the largest improvement in performance, eventually converging to the performance of the single iteration approach.\nThus, we chose to perform a single iteration for the results in Fig. 5(a)  ###reference_sf1###. See the Appendix Sec. 8.4.1  ###reference_.SSS1### for further implementation details and results."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion and Limitations",
            "text": "In this work, we aim to generate training data useful for training a supervised model by steering a text-to-image generative model.\nWe introduced two feedback mechanisms to find prompts that are informed by both the given model and the target distribution.\nEvaluations on a diverse set of tasks and distribution shifts show the effectiveness of the proposed closed-loop approach in comparison to open-loop ones.\nBelow we briefly discuss some of the limitations:\nLabel shift:\nIn this work, we focus on generating novel images.\nHowever, some distribution shifts can also change the label distribution, e.g., for depth estimation, changing from indoor to outdoor scenes would result in a shift in depth maps.\nOne possible approach could be learning a generative model over the label space [43  ###reference_b43###] to control the generation in both the label and image space.\nComputational cost:\nEstimating the gradient of the loss in Eq. 3  ###reference_### requires backpropagation through the denoising process of the diffusion model, which can be computationally demanding.\nUsing approaches that reduce the number of denoising steps [71  ###reference_b71###, 48  ###reference_b48###] may be able to reduce this computational cost.\nLabel Conditioning:\nAs discussed in Sec. 3.2  ###reference_###, our method is limited by the faithfulness of the generation conditioned on the given label.\nFor example, we found that the semantic segmentation ControlNet does not follow the conditioning accurately enough to be useful for the supervised model.\nFurther developments in more robust conditioning mechanisms are needed to successfully apply our method to other tasks."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Outline",
            "text": "We provide further discussions, details, and evaluations in the appendix, as outlined below.\nSecs. 7  ###reference_### and 8  ###reference_### describe additional implementation details for our classification and depth estimation experiments, respectively.\nSec. 7.6  ###reference_### describes an image guidance mechanism using Textual Inversion [22  ###reference_b22###] and compares it with the CLIP guidance mechanism, on Waterbirds.\nSec. 8.2  ###reference_### provide additional results for \u201cstandard\u201d augmentation baselines for the depth estimation experiments.\nSecs. 7.4.3  ###reference_.SSS3###, 7.5.4  ###reference_.SSS4### and 8.3  ###reference_### provide qualitative generations from all the Adversarial Prompts and Guided Adversarial Prompts used in the Waterbirds, iWildCam, and depth estimation experiments.\nAdditionally, for depth estimation, we provide a qualitative comparison of Adversarial Prompts generations optimized on different models (UNet [63  ###reference_b63###], DPT [58  ###reference_b58###]). For iWildCam, we also provide additional results using a ViT-B-16 [17  ###reference_b17###] instead of a ResNet50 [26  ###reference_b26###].\nSec. 8.4  ###reference_### provides additional analysis on the depth estimation experiments:\nthe single iteration vs. multi-iteration setting\na comparison of CLIP image and text guidance\nan assessment of the generalization of Adversarial Prompts from one model to another."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Classification",
            "text": "As mentioned in main paper Sec. 3.1  ###reference_###, for semantic classification tasks, we utilize the foreground object masks and use an in-painting technique proposed in [47  ###reference_b47###] that preserves the masked region throughout the denoising process.\nIn this section, we briefly describe this procedure and refer the reader to the original work for more details.\nLet  be a binary pixel mask, where a pixel is equal to 1 if the pixel contains the object and 0 otherwise, and  be the original image from a training dataset.\nDuring generation, after obtaining a denoised sample  at time  we update it as , where  is the original image noised to have the correct properties of the expected Gaussian distribution at time .\nHowever, because we are using Stable Diffusion [61  ###reference_b61###], the denoising process is done in latent space (using an encoder ), not pixel space. This means that to apply inpainting, we must resize the mask  to the latent space dimensions, and apply the above-described procedure in the latent space: , where  and  is its corresponding noised version.\nWhile this procedure usually performs well in preserving the original region of interest, we also paste the original masked region in the pixel space to obtain the final sample .\nIn addition to inpainting, depending on the setting, we also use SDEdit [50  ###reference_b50###], a mechanism available to all diffusion models that allows to use an initial image to condition the generation of new images to be closer to the initial image. The mechanism is parametrized by the SDEdit strength , which indicates the extent by which the model can deviate from the original image.\nFor our diffusion model, we use Stable Diffusion v1.5 222https://huggingface.co/runwayml/stable-diffusion-v1-5.\nWe report our generation parameters in Tab. 3  ###reference_###. We use the DDIM [71  ###reference_b71###] scheduler. We generate 384x384 resolution images. Those parameters were chosen based on visual inspection, ease of optimization and downstream performance (validation accuracy).\n\nTraining data.\nAfter generation, ALIA\u2019s method consists of an additional filtering step to remove \u201cbad\u201d generations. This step relies on using a pretrained model to measure confidence on the generated images. However, given our method creates images that are adversarial to an iWildCam pretrained model, the filtering part of ALIA\u2019s pipeline is not usable on our data. Thus, to keep things comparable, we decided not to apply filtering both our method generated data and ALIA\u2019s generated data. However, it must be noted that [18  ###reference_b18###] only reports a 2% absolute accuracy drop between fitlering and no filtering on iWildCam (1.4% on Waterbirds), thus we do not expect a big difference in performance with ALIA\u2019s reported results and our results.\nWe report our training parameters in Tab. 4  ###reference_###. We use ALIA\u2019s codebase to finetune our models, which ensures fair comparison to the ALIA baselines. For everything except the generated data, the settings are the same as in ALIA. For both datasets, the starting model is a ResNet50 [25  ###reference_b25###] model, pretrained on ImageNet [13  ###reference_b13###].\nThe reported test accuracy is chosen according to the best checkpoint, measured by validation accuracy.\nFor adversarial feedback, we use the model trained only using the original training data  with complete spurious correlation. It is taken from ALIA checkpoints333https://api.wandb.ai/files/clipinvariance/ALIA-Waterbirds/y6zc932x/checkpoint/ckpt-Waterbirds-none-filtered-resnet50-1-0.001-0.0001/best.pth.\nAs the task is the binary classification, we use the cross-entropy loss for the opposite class as the adversarial loss: , assuming . This is equivalent to the negative cross-entropy loss referred in the text.\nWe find four prompts per each class, i.e., eight prompts in total.\nEach prompt is composed of five new learnable tokens.\nWe perform adversarial optimization for 1000 steps with learning rate 1e-3 using Adam [38  ###reference_b38###].\nWe use five denoising steps during adversarial optimization and generate images for training with 15 steps.\nWe do not use SDEdit for Waterbirds.\nSee Tab. 3  ###reference_### for summary.\nFor Waterbirds, we use CLIP text guidance by encoding each of ALIA\u2019s summarized prompts (see Sec. 7.2  ###reference_###) with the CLIP text encoder as described in main paper Sec. 3.3  ###reference_###. In addition, we renormalize the averaged target text embedding to have the norm equal to the mean norm of the original prompts, and use the resulting vector as the target .\nWe use  guidance loss: .\nWe use  and  (i.e., no image guidance).\nWe describe here the parameters and settings used for optimization. If not precised, the same parameters were used for Adversarial Prompts and Guided Adversarial Prompts. As said in main paper Sec. 4.1  ###reference_###, we optimize 4 prompts. Each prompt is composed of 10 placeholder tokens.\nFor optimization, we use a constant learning rate of 0.001, and a batch size of 8.\nWe use the \u201centropy\u201d loss, described previously. For adversarial prompts, we train for 2000 steps. For guided adversarial prompts, we use CLIP guidance coefficient with  and and  (i.e., no text guidance). We train for a total of 10000 gradient steps. However, we don\u2019t optimize the adversarial loss for the first 2000 steps to allow the prompt to first converge to the target distribution region.\nFor adversarial prompts, to generate 4 different prompts, we simply change the seed. For guided (adversarial) prompts, each prompt is w.r.t a new location & time of the day of the test distribution.\nThe generation settings are the same as the ones used during adversarial optimization. For each target domain-guided adversarial prompt, (i.e. location & time of the day), the source images (used to condition the generation with an object mask and through SDEdit [50  ###reference_b50###]) are only images that match the time of the day of the target domain used during generation. Furthermore, for each prompt, we only generate one image per source image.\nFor ALIA, for each prompt, we generate one image per source image, from the whole training dataset. For the generation settings, given we use a slightly different generation process (inpainting) compared to their original implementation, we search ALIA\u2019s best-performing generation parameters (according to validation accuracy) over SDEDit strength [0.4, 0.5, 0.8] and guidance scale [5.0, 7.5]. We found the best-performing parameters for ALIA to be the same as the one reported by ALIA in their Github444https://github.com/lisadunlap/ALIA i.e. SDEdit strength of 0.5 and guidance of 7.5.\nThe learning rate scheduler is a cosine scheduler, updated every epoch. The batch size is 128.\nOur iWildCam pretrained model is taken from ALIA checkpoints555https://api.wandb.ai/files/clipinvariance/ALIA-iWildCamMini/brr7b3ks/checkpoint/ckpt-iWildCamMini-randaug-filtered-resnet50-0-0.001-0.0001/best.pth. ALIA trains the model from \u201cscratch\u201d (i.e. the model has never seen iWildCam data), for 100 epochs, on the combination of real + generated data. For our method, given we optimize the prompts based on a finetuned model feedback, it may not make as much sense to train the model from \u201dscratch\u201d. Thus, we also introduce the variant where the iWildCam pretrained model is finetuned on the combination of real + generated data for 20 epochs, where finetuning means that every layer, except the last, is frozen.\nFor a fair comparison, both training settings are tested for ALIA and our method. We found that ALIA worked best when training from scratch and our method worked best when using the finetuning setting.\nFinally, in their iWildCam experiment, ALIA fixed the number of extra generated points to be used in combination with real data during training to 2224 images. For the sake of comparison, we adopt the same limit in our experiments, with the added variant where the limit is 556 images, showcasing the data efficiency of our method."
        },
        {
            "section_id": "7.1",
            "parent_section_id": "7",
            "section_name": "Training data generation",
            "text": "As mentioned in main paper Sec. 3.1  ###reference_###  ###reference_###, for semantic classification tasks, we utilize the foreground object masks and use an in-painting technique proposed in [47  ###reference_b47###  ###reference_b47###] that preserves the masked region throughout the denoising process.\nIn this section, we briefly describe this procedure and refer the reader to the original work for more details.\nLet  be a binary pixel mask, where a pixel is equal to 1 if the pixel contains the object and 0 otherwise, and  be the original image from a training dataset.\nDuring generation, after obtaining a denoised sample  at time  we update it as , where  is the original image noised to have the correct properties of the expected Gaussian distribution at time .\nHowever, because we are using Stable Diffusion [61  ###reference_b61###  ###reference_b61###], the denoising process is done in latent space (using an encoder ), not pixel space. This means that to apply inpainting, we must resize the mask  to the latent space dimensions, and apply the above-described procedure in the latent space: , where  and  is its corresponding noised version.\nWhile this procedure usually performs well in preserving the original region of interest, we also paste the original masked region in the pixel space to obtain the final sample .\nIn addition to inpainting, depending on the setting, we also use SDEdit [50  ###reference_b50###  ###reference_b50###], a mechanism available to all diffusion models that allows to use an initial image to condition the generation of new images to be closer to the initial image. The mechanism is parametrized by the SDEdit strength , which indicates the extent by which the model can deviate from the original image.\nFor our diffusion model, we use Stable Diffusion v1.5 222https://huggingface.co/runwayml/stable-diffusion-v1-5."
        },
        {
            "section_id": "7.2",
            "parent_section_id": "7",
            "section_name": "ALIA",
            "text": "Here, we give more details on the ALIA [18  ###reference_b18###] (Automated Language-guided Image Augmentation) baseline method, which aims at generating images targeting a particular test distribution similar to our guidance mechanism (main paper Sec. 3.3  ###reference_###).\nGiven exemplar images from the test distribution, ALIA first captions each image using the BLIP [45  ###reference_b45###] captioning model.\nThen, it uses the GPT-4 [56  ###reference_b56###] LLM to summarize these captions into a list of domains asking it to produce descriptions that are agnostic to the class information.\n[18  ###reference_b18###] then use these prompts to generate additional training data.\nIn order to preserve the original class information in their generations, they use SDEdit [50  ###reference_b50###] or Instruct Pix2Pix [7  ###reference_b7###].\nWe refer the original paper for further implementation details.\nBelow, we summarize resulting prompts we use for comparison in our results.\nFor Waterbirds [66  ###reference_b66###], we found that removing the prefix \u201ca photo of a {class name}\u201d from the original prompts when using the inpainting technique (Sec. 7.1  ###reference_.SSS0.Px1###) to work slightly better for both the ALIA baseline and our CLIP text guidance (main paper Eq. 2  ###reference_###).\nWe, therefore, use the following prompts:\n\u201cin a bamboo forest with a green background.\u201d\n\u201cflying over the water with a city skyline in the background.\u201d\n\u201cperched on a car window.\u201d\n\u201cstanding in the snow in a forest.\u201d,\n\u201cstanding on a tree stump in the woods.\u201d\n\u201cswimming in a lake with mountains in the background.\u201d,\n\u201cstanding on the beach looking up.\u201d\nFor iWildCam [3  ###reference_b3###], we keep the original prompts intact:\n\u201ca camera trap photo of a {class name} in a grassy field with trees and bushes.\u201d\n\u201ca camera trap photo of a {class name} in a forest in the dark.\u201d\n\u201ca camera trap photo of a {class name} near a large body of water in the middle of a field.\u201d\n\u201ca camera trap photo of a {class name} walking on a dirt trail with twigs and branches.\u201d\nThere are two main differences between ALIA and our method:\nThe target distribution feedback. ALIA aligns its prompts with the target distribution by utilizing captioning and summarizing. However, this summarizing process is not informed of the produced generations when using such prompts, and, thus, does not guarantee that the text prompt will accurately guide the generation process to images related to the target distribution.\nModel feedback. ALIA is not model-informed. Thus, it doesn\u2019t necessarily generate images useful for training a given model.\nThose two differences originate from the fact that ALIA is an open-loop method, i.e, it lacks the mechanism to refine the prompt based on the generated images. In contrast, our method uses model and target distribution feedback in a closed-loop. This allows our method to outperform ALIA and be more data-efficient."
        },
        {
            "section_id": "7.3",
            "parent_section_id": "7",
            "section_name": "General implementation details",
            "text": "We report our generation parameters in Tab. 3  ###reference_###  ###reference_###. We use the DDIM [71  ###reference_b71###  ###reference_b71###] scheduler. We generate 384x384 resolution images. Those parameters were chosen based on visual inspection, ease of optimization and downstream performance (validation accuracy).\n\nTraining data.\nAfter generation, ALIA\u2019s method consists of an additional filtering step to remove \u201cbad\u201d generations. This step relies on using a pretrained model to measure confidence on the generated images. However, given our method creates images that are adversarial to an iWildCam pretrained model, the filtering part of ALIA\u2019s pipeline is not usable on our data. Thus, to keep things comparable, we decided not to apply filtering both our method generated data and ALIA\u2019s generated data. However, it must be noted that [18  ###reference_b18###  ###reference_b18###] only reports a 2% absolute accuracy drop between fitlering and no filtering on iWildCam (1.4% on Waterbirds), thus we do not expect a big difference in performance with ALIA\u2019s reported results and our results.\nWe report our training parameters in Tab. 4  ###reference_###  ###reference_###. We use ALIA\u2019s codebase to finetune our models, which ensures fair comparison to the ALIA baselines. For everything except the generated data, the settings are the same as in ALIA. For both datasets, the starting model is a ResNet50 [25  ###reference_b25###  ###reference_b25###] model, pretrained on ImageNet [13  ###reference_b13###  ###reference_b13###].\nThe reported test accuracy is chosen according to the best checkpoint, measured by validation accuracy."
        },
        {
            "section_id": "7.4",
            "parent_section_id": "7",
            "section_name": "Waterbirds",
            "text": "For adversarial feedback, we use the model trained only using the original training data  with complete spurious correlation. It is taken from ALIA checkpoints333https://api.wandb.ai/files/clipinvariance/ALIA-Waterbirds/y6zc932x/checkpoint/ckpt-Waterbirds-none-filtered-resnet50-1-0.001-0.0001/best.pth.\nAs the task is the binary classification, we use the cross-entropy loss for the opposite class as the adversarial loss: , assuming . This is equivalent to the negative cross-entropy loss referred in the text.\nWe find four prompts per each class, i.e., eight prompts in total.\nEach prompt is composed of five new learnable tokens.\nWe perform adversarial optimization for 1000 steps with learning rate 1e-3 using Adam [38  ###reference_b38###  ###reference_b38###].\nWe use five denoising steps during adversarial optimization and generate images for training with 15 steps.\nWe do not use SDEdit for Waterbirds.\nSee Tab. 3  ###reference_###  ###reference_### for summary.\nFor Waterbirds, we use CLIP text guidance by encoding each of ALIA\u2019s summarized prompts (see Sec. 7.2  ###reference_###  ###reference_###) with the CLIP text encoder as described in main paper Sec. 3.3  ###reference_###  ###reference_###. In addition, we renormalize the averaged target text embedding to have the norm equal to the mean norm of the original prompts, and use the resulting vector as the target .\nWe use  guidance loss: .\nWe use  and  (i.e., no image guidance)."
        },
        {
            "section_id": "7.4.1",
            "parent_section_id": "7.4",
            "section_name": "7.4.1 Dataset details",
            "text": "Fig. 7  ###reference_### demonstrates the shift between train and test distributions in the Waterbirds dataset [65  ###reference_b65###].\nWe follow the setting suggested in [18  ###reference_b18###] and use 1139 images as , where waterbirds appear only on water background and landbirds on land background.\nWe add additional 839 examples either from the original dataset, where waterbirds appear only on land background and landbirds on water background (\u201dReal OOD data\u201d), or generated by Stable Diffusion with prompts obtained by one of the methods.\nFor the data-efficiency plots (e.g., Fig. 3  ###reference_###) we reduced the number of added examples by a factor of {1/2, 1/4, 1/8, 1/16}.\nSince the original Waterbirds dataset does not provide masks for the exact generated images, we used the SAM [40  ###reference_b40###] segmentation model to obtain bird segmentation masks for training images.\nWe use these masks to condition the generative model on the class by using inpainting as described in Sec. 7.1  ###reference_.SSS0.Px1###."
        },
        {
            "section_id": "7.4.2",
            "parent_section_id": "7.4",
            "section_name": "7.4.2 Implementation Details",
            "text": "For adversarial feedback, we use the model trained only using the original training data  with complete spurious correlation. It is taken from ALIA checkpoints333https://api.wandb.ai/files/clipinvariance/ALIA-Waterbirds/y6zc932x/checkpoint/ckpt-Waterbirds-none-filtered-resnet50-1-0.001-0.0001/best.pth.\nAs the task is the binary classification, we use the cross-entropy loss for the opposite class as the adversarial loss: , assuming . This is equivalent to the negative cross-entropy loss referred in the text.\nWe find four prompts per each class, i.e., eight prompts in total.\nEach prompt is composed of five new learnable tokens.\nWe perform adversarial optimization for 1000 steps with learning rate 1e-3 using Adam [38  ###reference_b38###  ###reference_b38###  ###reference_b38###].\nWe use five denoising steps during adversarial optimization and generate images for training with 15 steps.\nWe do not use SDEdit for Waterbirds.\nSee Tab. 3  ###reference_###  ###reference_###  ###reference_### for summary.\nFor Waterbirds, we use CLIP text guidance by encoding each of ALIA\u2019s summarized prompts (see Sec. 7.2  ###reference_###  ###reference_###  ###reference_###) with the CLIP text encoder as described in main paper Sec. 3.3  ###reference_###  ###reference_###  ###reference_###. In addition, we renormalize the averaged target text embedding to have the norm equal to the mean norm of the original prompts, and use the resulting vector as the target .\nWe use  guidance loss: .\nWe use  and  (i.e., no image guidance)."
        },
        {
            "section_id": "7.4.3",
            "parent_section_id": "7.4",
            "section_name": "7.4.3 Additional Qualitative Results.",
            "text": "In Fig. 8  ###reference_### and Fig. 9  ###reference_###, we show a few generations using all 8 prompts used in the Waterbirds experiments for Guided Adversarial Prompts and Adversarial Prompts, respectively.\n###figure_6### ###figure_7### ###figure_8###"
        },
        {
            "section_id": "7.5",
            "parent_section_id": "7",
            "section_name": "iWildCam",
            "text": "We describe here the parameters and settings used for optimization. If not precised, the same parameters were used for Adversarial Prompts and Guided Adversarial Prompts. As said in main paper Sec. 4.1  ###reference_###  ###reference_###, we optimize 4 prompts. Each prompt is composed of 10 placeholder tokens.\nFor optimization, we use a constant learning rate of 0.001, and a batch size of 8.\nWe use the \u201centropy\u201d loss, described previously. For adversarial prompts, we train for 2000 steps. For guided adversarial prompts, we use CLIP guidance coefficient with  and and  (i.e., no text guidance). We train for a total of 10000 gradient steps. However, we don\u2019t optimize the adversarial loss for the first 2000 steps to allow the prompt to first converge to the target distribution region.\nFor adversarial prompts, to generate 4 different prompts, we simply change the seed. For guided (adversarial) prompts, each prompt is w.r.t a new location & time of the day of the test distribution.\nThe generation settings are the same as the ones used during adversarial optimization. For each target domain-guided adversarial prompt, (i.e. location & time of the day), the source images (used to condition the generation with an object mask and through SDEdit [50  ###reference_b50###  ###reference_b50###]) are only images that match the time of the day of the target domain used during generation. Furthermore, for each prompt, we only generate one image per source image.\nFor ALIA, for each prompt, we generate one image per source image, from the whole training dataset. For the generation settings, given we use a slightly different generation process (inpainting) compared to their original implementation, we search ALIA\u2019s best-performing generation parameters (according to validation accuracy) over SDEDit strength [0.4, 0.5, 0.8] and guidance scale [5.0, 7.5]. We found the best-performing parameters for ALIA to be the same as the one reported by ALIA in their Github444https://github.com/lisadunlap/ALIA i.e. SDEdit strength of 0.5 and guidance of 7.5.\nThe learning rate scheduler is a cosine scheduler, updated every epoch. The batch size is 128.\nOur iWildCam pretrained model is taken from ALIA checkpoints555https://api.wandb.ai/files/clipinvariance/ALIA-iWildCamMini/brr7b3ks/checkpoint/ckpt-iWildCamMini-randaug-filtered-resnet50-0-0.001-0.0001/best.pth. ALIA trains the model from \u201cscratch\u201d (i.e. the model has never seen iWildCam data), for 100 epochs, on the combination of real + generated data. For our method, given we optimize the prompts based on a finetuned model feedback, it may not make as much sense to train the model from \u201dscratch\u201d. Thus, we also introduce the variant where the iWildCam pretrained model is finetuned on the combination of real + generated data for 20 epochs, where finetuning means that every layer, except the last, is frozen.\nFor a fair comparison, both training settings are tested for ALIA and our method. We found that ALIA worked best when training from scratch and our method worked best when using the finetuning setting.\nFinally, in their iWildCam experiment, ALIA fixed the number of extra generated points to be used in combination with real data during training to 2224 images. For the sake of comparison, we adopt the same limit in our experiments, with the added variant where the limit is 556 images, showcasing the data efficiency of our method."
        },
        {
            "section_id": "7.5.1",
            "parent_section_id": "7.5",
            "section_name": "7.5.1 Dataset details.",
            "text": "The original iWildCam [3  ###reference_b3###] dataset is subsampled to create a 7-way classification task (background, cattle, elephant, impala, zebra, giraffe, dik-dik). The training set has 6,000 images with some classes having as few as 50 images per example. There are 2 test locations that are not in the training or validation set. Additionally, given , the hour at which an image was taken, we define an image to be during \u201cdaytime\u201d if , and \u201cnighttime\u201d if . As said in main paper Sec. 4.1  ###reference_###, for image CLIP guidance, we separate the target test locations into four groups (location={1,2}, time={daytime, nighttime}). We provide visualisation of the test locations (at day & night) in Fig. 10  ###reference_###. For more details on the iWildCam subset construction, we refer to [18  ###reference_b18###] Section 8.3. For inpainting, the object masks are obtained from MegaDetector [2  ###reference_b2###].\n###figure_9###"
        },
        {
            "section_id": "7.5.2",
            "parent_section_id": "7.5",
            "section_name": "7.5.2 Alignment collapse solution for iWildCam.",
            "text": "As mentioned in main paper Sec. 3.1  ###reference_###, choosing  to be the negative cross entropy loss, i.e. minimizing the probability that the model predicts , may not be the best choice. Indeed, given we use a random sample of 64 images to create our target embedding for the image CLIP guidance, the likelihood that animals were present on these 64 images is very high. This means that the target embedding, although mostly containing the \u201clocation concept\u201d, also partly contains an \u201canimal concept\u201d. This means that the image CLIP guidance does not explicitly forbid the generation of new animals. Combined with optimizing the negative cross entropy loss, this leads to adversarial animal insertions at generation time, where a new animal of class  appears alongside the original animal of class , destroying the  alignment. In Fig. 11  ###reference_###, we provide qualitative examples for this behaviour. To counter this behaviour, we choose  to be the \u201centropy\u201d loss, or uncertainty loss. More precisely, this loss is equal to the cross entropy loss where the target label  is replaced by the soft label , the uniform distribution over all classes. This loss explicitly encourages generations that either (1) do not contain new animals (2) contain new animals that are not accounted for in the label space .\n###figure_10###"
        },
        {
            "section_id": "7.5.3",
            "parent_section_id": "7.5",
            "section_name": "7.5.3 Implementation details",
            "text": "We describe here the parameters and settings used for optimization. If not precised, the same parameters were used for Adversarial Prompts and Guided Adversarial Prompts. As said in main paper Sec. 4.1  ###reference_###  ###reference_###  ###reference_###, we optimize 4 prompts. Each prompt is composed of 10 placeholder tokens.\nFor optimization, we use a constant learning rate of 0.001, and a batch size of 8.\nWe use the \u201centropy\u201d loss, described previously. For adversarial prompts, we train for 2000 steps. For guided adversarial prompts, we use CLIP guidance coefficient with  and and  (i.e., no text guidance). We train for a total of 10000 gradient steps. However, we don\u2019t optimize the adversarial loss for the first 2000 steps to allow the prompt to first converge to the target distribution region.\nFor adversarial prompts, to generate 4 different prompts, we simply change the seed. For guided (adversarial) prompts, each prompt is w.r.t a new location & time of the day of the test distribution.\nThe generation settings are the same as the ones used during adversarial optimization. For each target domain-guided adversarial prompt, (i.e. location & time of the day), the source images (used to condition the generation with an object mask and through SDEdit [50  ###reference_b50###  ###reference_b50###  ###reference_b50###]) are only images that match the time of the day of the target domain used during generation. Furthermore, for each prompt, we only generate one image per source image.\nFor ALIA, for each prompt, we generate one image per source image, from the whole training dataset. For the generation settings, given we use a slightly different generation process (inpainting) compared to their original implementation, we search ALIA\u2019s best-performing generation parameters (according to validation accuracy) over SDEDit strength [0.4, 0.5, 0.8] and guidance scale [5.0, 7.5]. We found the best-performing parameters for ALIA to be the same as the one reported by ALIA in their Github444https://github.com/lisadunlap/ALIA i.e. SDEdit strength of 0.5 and guidance of 7.5.\nThe learning rate scheduler is a cosine scheduler, updated every epoch. The batch size is 128.\nOur iWildCam pretrained model is taken from ALIA checkpoints555https://api.wandb.ai/files/clipinvariance/ALIA-iWildCamMini/brr7b3ks/checkpoint/ckpt-iWildCamMini-randaug-filtered-resnet50-0-0.001-0.0001/best.pth. ALIA trains the model from \u201cscratch\u201d (i.e. the model has never seen iWildCam data), for 100 epochs, on the combination of real + generated data. For our method, given we optimize the prompts based on a finetuned model feedback, it may not make as much sense to train the model from \u201dscratch\u201d. Thus, we also introduce the variant where the iWildCam pretrained model is finetuned on the combination of real + generated data for 20 epochs, where finetuning means that every layer, except the last, is frozen.\nFor a fair comparison, both training settings are tested for ALIA and our method. We found that ALIA worked best when training from scratch and our method worked best when using the finetuning setting.\nFinally, in their iWildCam experiment, ALIA fixed the number of extra generated points to be used in combination with real data during training to 2224 images. For the sake of comparison, we adopt the same limit in our experiments, with the added variant where the limit is 556 images, showcasing the data efficiency of our method."
        },
        {
            "section_id": "7.5.4",
            "parent_section_id": "7.5",
            "section_name": "7.5.4 Additional Qualitative Results.",
            "text": "In Fig. 12  ###reference_### and Fig. 13  ###reference_###, we show a few generations using each of the 4 Guided Adversarial Prompts and Adversarial Prompts used in the iWildCam experiments.\n###figure_11### ###figure_12###"
        },
        {
            "section_id": "7.5.5",
            "parent_section_id": "7.5",
            "section_name": "7.5.5 Using ViT model.",
            "text": "In Fig. 14  ###reference_###, we repeat the iWildCam experiment from the main paper (Fig. 4  ###reference_###) with a ViT-B-16 [16  ###reference_b16###] model. Additionally, we provide qualitative results for generations from adversarial prompts optimized with a ViT-B-16 model in Fig. 15  ###reference_###.\n###figure_13### ###figure_14###"
        },
        {
            "section_id": "7.6",
            "parent_section_id": "7",
            "section_name": "Image Guidance using Textual Inversion",
            "text": "In addition to the CLIP image guidance introduced in main paper Sec. 3.3  ###reference_###, we also explore using Textual Inversion (TI) [22  ###reference_b22###] as an image guidance mechanism.\nSimilar to the CLIP guidance, we use a few images  from the target distribution.\nNow, instead of the similarity in a CLIP embedding space, we use the denoising loss between a generated image and one of the target images as in [22  ###reference_b22###] (see Eq. (2)):\nwhere  is the VAE [39  ###reference_b39###] image encoder and  is the denoising UNet model from the Stable Diffusion model [61  ###reference_b61###], and  is sampled randomly from the set of available target images.\nWe test the TI image guidance on the Waterbirds dataset.\nWe use the guidance loss from Eq. 4  ###reference_### with the weight 1000 (we found lower values to result in generations less faithful to the target images) and randomly sample 50 (unlabeled) images from the validation split of the original Waterbirds dataset where both types of birds appear on both types of backgrounds.\nWe keep other settings the same as for GAP and AP.\n###figure_15### Fig. 16  ###reference_### shows that TI guidance works on par with or better than CLIP guidance on the Waterbirds dataset.\nWe found, however, that the TI guidance does not result in faithful generations for iWildsCam dataset, and further investigations are needed."
        },
        {
            "section_id": "7.7",
            "parent_section_id": "7",
            "section_name": "Additional Analysis",
            "text": "We ablate hyperparameters like 1) using  or cosine loss for , 2) different ways of incorporating guidance e.g., text or image with CLIP or textual inversion (T.I).\nThe table shows the accuracy from different combinations of 1 and 2 on the Waterbirds dataset. Using  and text guidance worked best, thus, we used this setting for our results in Fig. 3  ###reference_###.\nT.I. compares generations to the original images in the pixel space, and Image in the CLIP embedding space, resulting in different guidance mechanisms and, hence, performance."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Depth Estimation",
            "text": "In this setting, , we use . For every , we construct  and  to be generated using  Adversarial Prompts. We fine-tune the model on  and and validate both on  and  validation sets during the fine-tuning. The results are shown in Fig. 17  ###reference_###.\nIn this setting,  are respectively. For every  we fine-tune on  and compute the validation loss on  and . The results are shown in Fig. 17  ###reference_###. As the loss for  tends to be more similar then when , we chose to increase the number of tokens used per prompt in our multi-iteration setting.\n###figure_16###"
        },
        {
            "section_id": "8.1",
            "parent_section_id": "8",
            "section_name": "Depth training details",
            "text": "Adversarial Optimization.\nThe adversarial optimization was done with AdamW [46  ###reference_b46###], learning rate of , weight decay of , and batch size of 8. The token embeddings at the start of optimization are randomly sampled from  where  and  is the mean and standard deviation of all embeddings in the vocabulary. We set the early stopping threshold to 0.08 for the UNet model and 1.0 for the DPT model. Note that these models were trained with different losses,  for the former and Midas loss [20  ###reference_b20###] for the later. Adversarial optimization is performed with the same loss as was used for training these models. One run takes approximately 30 mins on one A100. We perform a total of 32 runs, to get 32 Adversarial Prompts for the UNet model and 30 runs for the DPT model. As the DPT model was trained on Omnidata, which is a mix of 5 datasets, we have 6 runs for each dataset. Different number of placeholder tokens were also used for each run as suggested in Fig. 5(b)  ###reference_sf2### of the main paper. For the DPT model, we do 1, 8, 16 tokens runs for each dataset and also 3 runs with 32 tokens for each dataset. For the UNet model, 4 runs of 1, 8 and 16 tokens each and 16 runs of 32 tokens were used, to get a total of 32 prompts. We also use a reduced number of denoising steps during optimization i.e., 5, as we found it to be more stable.\nGuided Adversarial Optimization. The CLIP guidance coefficient for text and image guidance is set to 1 and 5 respectively. For image guidance, we randomly sampled 100 images from the target distribution. For text guidance, we used target distribution\u2019s name in the prompt, e.g., \u201cfog\u201d for the fog corruption from CC.\nGeneration.\nGeneration is performed with the DDIM [72  ###reference_b72###] scheduler and 15 sampling steps. We generate 80k images for the UNet model and 60k images for the DPT model for fine-tuning.\nFine-tuning. For fine-tuning, we optimize the UNet model with AMSGrad [60  ###reference_b60###] with a learning rate of , weight decay of  and batch size 128. For the DPT model, a learning rate of , weight decay of  and batch size 32."
        },
        {
            "section_id": "8.2",
            "parent_section_id": "8",
            "section_name": "Additional Quantitative Results",
            "text": "Performance of non-SD baselines. In Tab. 6  ###reference_###, we show the results for depth estimation for two additional baselines, deep augmentation [31  ###reference_b31###] and style augmentation [23  ###reference_b23###] that do not make use of generative models. Deep augmentation distorts a given image by passing it through an image-to-image model e.g., VAE [39  ###reference_b39###], while perturbing its representations. Style augmentation involves involves applying style transfer to the original training images. They perform comparably to Adversarial Prompts.\nU-Net\nDPT\n\n\nTaskonomy\nReplica\nTaskonomy\n\nShift\nClean\nCC\n3DCC\nCDS\nCC\n3DCC\n\nControl (No extra data)\n2.35\n4.93\n4.79\n5.38\n3.76\n3.42\n\nAgnostic Prompts\n2.47\n5.03\n4.17\n5.30\n4.06\n3.58\n\nAgnostic Prompts (Random)\n2.38\n4.96\n4.11\n5.14\n3.88\n3.51\n\nAdversarial Prompts\n2.49\n4.36\n4.02\n5.12\n3.40\n3.28\n\nAdversarial Prompts (SDEdit)\n2.59\n4.20\n3.88\n4.96\n3.35\n3.25\n\nDeep Augmentation\n2.42\n4.24\n3.70\n5.01\n2.83\n3.70\n\nStyle Augmentation\n2.42\n4.15\n3.85\n5.16\n2.80\n3.10"
        },
        {
            "section_id": "8.3",
            "parent_section_id": "8",
            "section_name": "Additional Qualitative Results",
            "text": "Generations from all adversarial prompts & comparison of generations from different models.\nWe show the generations from all Adversarial Prompts from the UNet model, without SDEDit (Fig. 19  ###reference_###), with SDEdit (Fig. 20  ###reference_###), and multi-iteration (Fig. 21  ###reference_###). Additionally, we provide the generations from two DPT models, allowing us to assess the difference the model feedback has on generations. The first DPT model was only trained on Omnidata (Fig. 22  ###reference_###) and second was trained on Omnidata with augmentations from CC and 3DCC and with consistency constrains [79  ###reference_b79###] (Fig. 23  ###reference_###). The quantitative results in the paper were reported only on the former DPT model.\nThere does not seem to be obvious differences in the styles generated between the two DPT models. However, between the Adversarial Prompts from the UNet model with and without multi-iteration, the Adversarial Prompts from the latter seems to result in much more diverse styles."
        },
        {
            "section_id": "8.4",
            "parent_section_id": "8",
            "section_name": "Additional Analysis",
            "text": "###figure_17### In this setting, , we use . For every , we construct  and  to be generated using  Adversarial Prompts. We fine-tune the model on  and and validate both on  and  validation sets during the fine-tuning. The results are shown in Fig. 17  ###reference_###  ###reference_###.\nIn this setting,  are respectively. For every  we fine-tune on  and compute the validation loss on  and . The results are shown in Fig. 17  ###reference_###  ###reference_###. As the loss for  tends to be more similar then when , we chose to increase the number of tokens used per prompt in our multi-iteration setting.\n###figure_18###"
        },
        {
            "section_id": "8.4.1",
            "parent_section_id": "8.4",
            "section_name": "8.4.1 Running multiple iterations of adversarial optimization vs a single iteration",
            "text": "Here, we provide additional analysis for the multi-iteration experiments in Fig. 5(b)  ###reference_sf2### in the main paper.\nWe optimize for 4 prompts in each iteration and noticed that if the number of placeholder tokens in a given prompt is kept fixed throughout the iterations, the optimization to find new Adversarial Prompts becomes more difficult. However, if we increase the number of tokens at each iteration e.g.,  token per prompt for 1st,  per prompt for 2nd, etc, we are able to consistently find new Adversarial Prompts.\nThus, we aim to investigate the generalization of a given model to different Adversarial Prompts, e.g., is a model more likely to generalize to Adversarial Prompts with the same number of tokens.\nTo perform this analysis, we generated data  using AP with  and  tokens per prompt respectively and measured the performance of a model fine-tuned on  on .\nIn this setting, , we use . For every , we construct  and  to be generated using  Adversarial Prompts. We fine-tune the model on  and and validate both on  and  validation sets during the fine-tuning. The results are shown in Fig. 17  ###reference_###  ###reference_###  ###reference_###.\nIn this setting,  are respectively. For every  we fine-tune on  and compute the validation loss on  and . The results are shown in Fig. 17  ###reference_###  ###reference_###  ###reference_###. As the loss for  tends to be more similar then when , we chose to increase the number of tokens used per prompt in our multi-iteration setting.\n###figure_19###"
        },
        {
            "section_id": "8.4.2",
            "parent_section_id": "8.4",
            "section_name": "8.4.2 CLIP image vs text guidance.",
            "text": "In Fig. 18  ###reference_###, we compare the qualitative (top) and quantitative (bottom) differences in generations from text guidance and image guidance on defocus blur and fog. Note that image guidance uses sample (unlabelled) images from the corresponding target distribution that it is evaluated on, i.e., fog samples images from the fog corruption from the CC benchmark and fog (3D) samples images from the fog corruption of the 3DCC benchmark. If the target distribution name has (3D) appended to it, it is from the CC benchmark, otherwise it is from the 3DCC benchmark.\nWe observed some differences in the generations with text vs. image guidance (Fig. 18  ###reference_###, top). Text Guided Prompts generates corruptions that are more realistic that image Guided Prompts. For example, fog gets denser further away from the camera or around the floor when text Guided Prompts are used for generations. For image Guided Prompts, as it was guided by the image samples from CC where the corruption is applied uniformly over the image, it learns to also apply a more uniform corruption over the image.\nQuantitatively, we observed that image guidance tends to perform the best across the target distributions, with large enough extra data (Fig. 18  ###reference_###, bottom)."
        },
        {
            "section_id": "8.4.3",
            "parent_section_id": "8.4",
            "section_name": "8.4.3 Generalization of Adversarial Prompts to different models.",
            "text": "We show how adversarial generations from Adversarial Prompts found for one model are for another model in Tab. 7  ###reference_###. The generations from Adversarial Prompts found for e.g., the UNet model result in the highest loss when evaluated on the UNet model. However, the generations from Adversarial Prompts from the DPT model also result in similar loss. Similar trends hold for the DPT model. Thus, Adversarial Prompts found for one model are also able to result in high loss for another model.\nAP from\\Eval on\nOriginal data\nUNet\nDPT\n\n\n\nUNet\n2.55\n7.63\n5.39\n\nDPT\n1.76\n7.17\n6.46\n###figure_20### ###figure_21### ###figure_22### ###figure_23### ###figure_24###"
        }
    ],
    "appendix": [],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T1\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S4.T1.3.1.1\" style=\"font-size:90%;\">Table 1</span>: </span><span class=\"ltx_text\" id=\"S4.T1.4.2\" style=\"font-size:90%;\">Adversarial optimization parameters for the classification experiments.</span></figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T1.1\" style=\"width:422.8pt;height:47.1pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-30.8pt,3.4pt) scale(0.872869612138151,0.872869612138151) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T1.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T1.1.1.1.2\">Dataset</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T1.1.1.1.3\">SDEdit strength</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T1.1.1.1.4\">denoising steps</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T1.1.1.1.5\">guidance scale</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T1.1.1.1.6\"># placeholder tokens</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T1.1.1.1.7\">Opt. steps (AP/GAP)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T1.1.1.1.1\">\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.2.1.1\">Waterbirds</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.1.2.1.2\">1.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.1.2.1.3\">5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.1.2.1.4\">7.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.1.2.1.5\">5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.1.2.1.6\">1000/1000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.1.2.1.7\">20/0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.3.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T1.1.1.3.2.1\">iWildCam</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.1.1.3.2.2\">0.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.1.1.3.2.3\">5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.1.1.3.2.4\">5.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.1.1.3.2.5\">10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.1.1.3.2.6\">2000/10000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.1.1.3.2.7\">0/10</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 1: Adversarial optimization parameters for the classification experiments."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T2\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S4.T2.3.1.1\" style=\"font-size:90%;\">Table 2</span>: </span><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.4.2\" style=\"font-size:80%;\">Model feedback generates data tailored to a specific model.<span class=\"ltx_text ltx_font_medium\" id=\"S4.T2.4.2.1\"> We show test accuracy (averaged over 3 seeds) on iWildCam for different combinations of model and generated data. Each row represents the model that was used for model-based feedback to get Guided Adversarial Prompts. Each column represents the model used for finetuning on the generated data.\n<span class=\"ltx_text ltx_font_italic\" id=\"S4.T2.4.2.1.1\">The best dataset for each model is the one generated using that model. </span></span></span></figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T2.5\" style=\"width:149.1pt;height:96.9pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(5.3pt,-3.5pt) scale(1.07698819167184,1.07698819167184) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T2.5.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T2.5.1.1.1\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt\" id=\"S4.T2.5.1.1.1.1\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S4.T2.5.1.1.1.2\">Finetuned model</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.5.1.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.5.1.2.2.1\">Data</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.5.1.2.2.2\">ResNet</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.2.2.3\">ViT</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.5.1.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T2.5.1.3.3.1\">Agnostic Prompts</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.5.1.3.3.2\">72.94</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.5.1.3.3.3\">73.07</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.5.1.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T2.5.1.4.4.1\">GAP from ResNet</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.5.1.4.4.2\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.5.1.4.4.2.1\">83.97</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.5.1.4.4.3\">72.61</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.5.1.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S4.T2.5.1.5.5.1\">GAP from ViT</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T2.5.1.5.5.2\">83.87</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.5.1.5.5.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.5.1.5.5.3.1\">77.21</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 2: Model feedback generates data tailored to a specific model. We show test accuracy (averaged over 3 seeds) on iWildCam for different combinations of model and generated data. Each row represents the model that was used for model-based feedback to get Guided Adversarial Prompts. Each column represents the model used for finetuning on the generated data.\nThe best dataset for each model is the one generated using that model. "
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S7.T3\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S7.T3.2\" style=\"width:210.3pt;height:24pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-210.5pt,24.0pt) scale(0.3331414708518,0.3331414708518) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S7.T3.2.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S7.T3.2.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" id=\"S7.T3.2.1.1.1.1\"><span class=\"ltx_text\" id=\"S7.T3.2.1.1.1.1.1\" style=\"font-size:144%;\">Dataset</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"3\" id=\"S7.T3.2.1.1.1.2\"><span class=\"ltx_text\" id=\"S7.T3.2.1.1.1.2.1\" style=\"font-size:144%;\">ALIA</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" id=\"S7.T3.2.1.1.1.3\"><span class=\"ltx_text\" id=\"S7.T3.2.1.1.1.3.1\" style=\"font-size:144%;\">Ours</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T3.2.1.2.2\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r\" id=\"S7.T3.2.1.2.2.1\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S7.T3.2.1.2.2.2\"><span class=\"ltx_text ltx_font_italic\" id=\"S7.T3.2.1.2.2.2.1\" style=\"font-size:144%;\">SDEdit strength</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S7.T3.2.1.2.2.3\"><span class=\"ltx_text ltx_font_italic\" id=\"S7.T3.2.1.2.2.3.1\" style=\"font-size:144%;\">sampling steps</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\" id=\"S7.T3.2.1.2.2.4\"><span class=\"ltx_text ltx_font_italic\" id=\"S7.T3.2.1.2.2.4.1\" style=\"font-size:144%;\">text guidance</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S7.T3.2.1.2.2.5\"><span class=\"ltx_text ltx_font_italic\" id=\"S7.T3.2.1.2.2.5.1\" style=\"font-size:144%;\">SDEdit strength</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S7.T3.2.1.2.2.6\"><span class=\"ltx_text ltx_font_italic\" id=\"S7.T3.2.1.2.2.6.1\" style=\"font-size:144%;\">sampling steps</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S7.T3.2.1.2.2.7\"><span class=\"ltx_text ltx_font_italic\" id=\"S7.T3.2.1.2.2.7.1\" style=\"font-size:144%;\">text guidance</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S7.T3.2.1.3.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S7.T3.2.1.3.1.1\"><span class=\"ltx_text\" id=\"S7.T3.2.1.3.1.1.1\" style=\"font-size:144%;\">Waterbirds</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S7.T3.2.1.3.1.2\"><span class=\"ltx_text\" id=\"S7.T3.2.1.3.1.2.1\" style=\"font-size:144%;\">0.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S7.T3.2.1.3.1.3\"><span class=\"ltx_text\" id=\"S7.T3.2.1.3.1.3.1\" style=\"font-size:144%;\">50</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S7.T3.2.1.3.1.4\"><span class=\"ltx_text\" id=\"S7.T3.2.1.3.1.4.1\" style=\"font-size:144%;\">7.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S7.T3.2.1.3.1.5\"><span class=\"ltx_text\" id=\"S7.T3.2.1.3.1.5.1\" style=\"font-size:144%;\">1.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S7.T3.2.1.3.1.6\"><span class=\"ltx_text\" id=\"S7.T3.2.1.3.1.6.1\" style=\"font-size:144%;\">15</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S7.T3.2.1.3.1.7\"><span class=\"ltx_text\" id=\"S7.T3.2.1.3.1.7.1\" style=\"font-size:144%;\">7.0</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T3.2.1.4.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S7.T3.2.1.4.2.1\"><span class=\"ltx_text\" id=\"S7.T3.2.1.4.2.1.1\" style=\"font-size:144%;\">iWildCam</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S7.T3.2.1.4.2.2\"><span class=\"ltx_text\" id=\"S7.T3.2.1.4.2.2.1\" style=\"font-size:144%;\">0.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S7.T3.2.1.4.2.3\"><span class=\"ltx_text\" id=\"S7.T3.2.1.4.2.3.1\" style=\"font-size:144%;\">50</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S7.T3.2.1.4.2.4\"><span class=\"ltx_text\" id=\"S7.T3.2.1.4.2.4.1\" style=\"font-size:144%;\">7.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S7.T3.2.1.4.2.5\"><span class=\"ltx_text\" id=\"S7.T3.2.1.4.2.5.1\" style=\"font-size:144%;\">0.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S7.T3.2.1.4.2.6\"><span class=\"ltx_text\" id=\"S7.T3.2.1.4.2.6.1\" style=\"font-size:144%;\">5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S7.T3.2.1.4.2.7\"><span class=\"ltx_text\" id=\"S7.T3.2.1.4.2.7.1\" style=\"font-size:144%;\">5.0</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:144%;\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S7.T3.5.1.1\" style=\"font-size:63%;\">Table 3</span>: </span><span class=\"ltx_text\" id=\"S7.T3.6.2\" style=\"font-size:63%;\">Generation parameters for the classification experiments</span></figcaption>\n</figure>",
            "capture": "Table 3: Generation parameters for the classification experiments"
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S7.T4\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S7.T4.2\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S7.T4.2.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" id=\"S7.T4.2.1.1.1\"><span class=\"ltx_text\" id=\"S7.T4.2.1.1.1.1\" style=\"font-size:144%;\">Dataset</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" id=\"S7.T4.2.1.1.2\"><span class=\"ltx_text\" id=\"S7.T4.2.1.1.2.1\" style=\"font-size:144%;\">Training</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S7.T4.2.2.1\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r\" id=\"S7.T4.2.2.1.1\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T4.2.2.1.2\"><span class=\"ltx_text ltx_font_italic\" id=\"S7.T4.2.2.1.2.1\" style=\"font-size:144%;\">learning rate</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T4.2.2.1.3\"><span class=\"ltx_text ltx_font_italic\" id=\"S7.T4.2.2.1.3.1\" style=\"font-size:144%;\">weight decay</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T4.2.2.1.4\"><span class=\"ltx_text ltx_font_italic\" id=\"S7.T4.2.2.1.4.1\" style=\"font-size:144%;\">epochs</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T4.2.3.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" id=\"S7.T4.2.3.2.1\"><span class=\"ltx_text\" id=\"S7.T4.2.3.2.1.1\" style=\"font-size:144%;\">Waterbirds</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S7.T4.2.3.2.2\"><span class=\"ltx_text\" id=\"S7.T4.2.3.2.2.1\" style=\"font-size:144%;\">0.001</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S7.T4.2.3.2.3\"><span class=\"ltx_text\" id=\"S7.T4.2.3.2.3.1\" style=\"font-size:144%;\">1e-4</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S7.T4.2.3.2.4\"><span class=\"ltx_text\" id=\"S7.T4.2.3.2.4.1\" style=\"font-size:144%;\">100</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T4.2.4.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S7.T4.2.4.3.1\"><span class=\"ltx_text\" id=\"S7.T4.2.4.3.1.1\" style=\"font-size:144%;\">iWildCam</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S7.T4.2.4.3.2\"><span class=\"ltx_text\" id=\"S7.T4.2.4.3.2.1\" style=\"font-size:144%;\">0.0001</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S7.T4.2.4.3.3\"><span class=\"ltx_text\" id=\"S7.T4.2.4.3.3.1\" style=\"font-size:144%;\">1e-4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S7.T4.2.4.3.4\"><span class=\"ltx_text\" id=\"S7.T4.2.4.3.4.1\" style=\"font-size:144%;\">100/20</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:144%;\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S7.T4.5.1.1\" style=\"font-size:63%;\">Table 4</span>: </span><span class=\"ltx_text\" id=\"S7.T4.6.2\" style=\"font-size:63%;\">Training parameters for the classification experiments</span></figcaption>\n</figure>",
            "capture": "Table 4: Training parameters for the classification experiments"
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"S7.T5\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S7.T5.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S7.T5.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" id=\"S7.T5.1.2.1.1\"><span class=\"ltx_text\" id=\"S7.T5.1.2.1.1.1\" style=\"font-size:80%;\">Loss\\Guide.</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S7.T5.1.2.1.2\"><span class=\"ltx_text\" id=\"S7.T5.1.2.1.2.1\" style=\"font-size:80%;\">Text</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S7.T5.1.2.1.3\"><span class=\"ltx_text\" id=\"S7.T5.1.2.1.3.1\" style=\"font-size:80%;\">Image</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S7.T5.1.2.1.4\"><span class=\"ltx_text\" id=\"S7.T5.1.2.1.4.1\" style=\"font-size:80%;\">T.I.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S7.T5.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S7.T5.1.1.1\"></th>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S7.T5.1.1.2\"><span class=\"ltx_text\" id=\"S7.T5.1.1.2.1\" style=\"font-size:80%;\">84.0</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S7.T5.1.1.3\"><span class=\"ltx_text\" id=\"S7.T5.1.1.3.1\" style=\"font-size:80%;\">77.3</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S7.T5.1.1.4\"><span class=\"ltx_text\" id=\"S7.T5.1.1.4.1\" style=\"font-size:80%;\">83.6</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T5.1.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S7.T5.1.3.1.1\"><span class=\"ltx_text\" id=\"S7.T5.1.3.1.1.1\" style=\"font-size:80%;\">Cosine sim.</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" id=\"S7.T5.1.3.1.2\"><span class=\"ltx_text\" id=\"S7.T5.1.3.1.2.1\" style=\"font-size:80%;\">82.5</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" id=\"S7.T5.1.3.1.3\"><span class=\"ltx_text\" id=\"S7.T5.1.3.1.3.1\" style=\"font-size:80%;\">72.7</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S7.T5.1.3.1.4\"><span class=\"ltx_text\" id=\"S7.T5.1.3.1.4.1\" style=\"font-size:80%;\">\u2014</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S7.T5.5.1.1\" style=\"font-size:113%;\">Table 5</span>: </span><span class=\"ltx_text\" id=\"S7.T5.6.2\" style=\"font-size:113%;\">Results from different hyperparameters, namely, loss for CLIP guidance and how guidance is incorporated, on the Waterbird dataset.</span></figcaption>\n</figure>",
            "capture": "Table 5: Results from different hyperparameters, namely, loss for CLIP guidance and how guidance is incorporated, on the Waterbird dataset."
        },
        "6": {
            "table_html": "<figure class=\"ltx_table\" id=\"S8.T6\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S8.T6.4\" style=\"width:409.5pt;height:180pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<p class=\"ltx_p\" id=\"S8.T6.4.1\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1\" style=\"font-size:144%;\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S8.T6.4.1.1.1\">\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\" id=\"S8.T6.4.1.1.1.1.1\">\n<span class=\"ltx_td ltx_border_r ltx_border_tt\" id=\"S8.T6.4.1.1.1.1.1.1\"></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_colspan ltx_colspan_4\" id=\"S8.T6.4.1.1.1.1.1.2\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.1.1.2.1\" style=\"color:#000000;\">U-Net</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2\" id=\"S8.T6.4.1.1.1.1.1.3\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.1.1.3.1\" style=\"color:#000000;\">DPT</span></span></span>\n<span class=\"ltx_tr\" id=\"S8.T6.4.1.1.1.2.2\">\n<span class=\"ltx_td ltx_border_r\" id=\"S8.T6.4.1.1.1.2.2.1\"></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_3\" id=\"S8.T6.4.1.1.1.2.2.2\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.2.2.2.1\" style=\"color:#000000;\">Taskonomy</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S8.T6.4.1.1.1.2.2.3\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.2.2.3.1\" style=\"color:#000000;\">Replica</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_2\" id=\"S8.T6.4.1.1.1.2.2.4\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.2.2.4.1\" style=\"color:#000000;\">Taskonomy</span></span></span>\n<span class=\"ltx_tr\" id=\"S8.T6.4.1.1.1.3.3\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S8.T6.4.1.1.1.3.3.1\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.3.3.1.1\" style=\"color:#000000;\">Shift</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S8.T6.4.1.1.1.3.3.2\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.3.3.2.1\" style=\"color:#000000;\">Clean</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S8.T6.4.1.1.1.3.3.3\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.3.3.3.1\" style=\"color:#000000;\">CC</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S8.T6.4.1.1.1.3.3.4\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.3.3.4.1\" style=\"color:#000000;\">3DCC</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S8.T6.4.1.1.1.3.3.5\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.3.3.5.1\" style=\"color:#000000;\">CDS</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S8.T6.4.1.1.1.3.3.6\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.3.3.6.1\" style=\"color:#000000;\">CC</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S8.T6.4.1.1.1.3.3.7\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.3.3.7.1\" style=\"color:#000000;\">3DCC</span></span></span>\n<span class=\"ltx_tr\" id=\"S8.T6.4.1.1.1.4.4\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S8.T6.4.1.1.1.4.4.1\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.4.4.1.1\" style=\"color:#000000;\">Control (No extra data)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S8.T6.4.1.1.1.4.4.2\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.4.4.2.1\" style=\"color:#000000;\">2.35</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S8.T6.4.1.1.1.4.4.3\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.4.4.3.1\" style=\"color:#000000;\">4.93</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S8.T6.4.1.1.1.4.4.4\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.4.4.4.1\" style=\"color:#000000;\">4.79</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S8.T6.4.1.1.1.4.4.5\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.4.4.5.1\" style=\"color:#000000;\">5.38</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S8.T6.4.1.1.1.4.4.6\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.4.4.6.1\" style=\"color:#000000;\">3.76</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S8.T6.4.1.1.1.4.4.7\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.4.4.7.1\" style=\"color:#000000;\">3.42</span></span></span>\n<span class=\"ltx_tr\" id=\"S8.T6.4.1.1.1.5.5\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S8.T6.4.1.1.1.5.5.1\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.5.5.1.1\" style=\"color:#4878D0;\">Agnostic Prompts</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S8.T6.4.1.1.1.5.5.2\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.5.5.2.1\" style=\"color:#000000;\">2.47</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S8.T6.4.1.1.1.5.5.3\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.5.5.3.1\" style=\"color:#000000;\">5.03</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S8.T6.4.1.1.1.5.5.4\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.5.5.4.1\" style=\"color:#000000;\">4.17</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S8.T6.4.1.1.1.5.5.5\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.5.5.5.1\" style=\"color:#000000;\">5.30</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S8.T6.4.1.1.1.5.5.6\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.5.5.6.1\" style=\"color:#000000;\">4.06</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S8.T6.4.1.1.1.5.5.7\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.5.5.7.1\" style=\"color:#000000;\">3.58</span></span></span>\n<span class=\"ltx_tr\" id=\"S8.T6.4.1.1.1.6.6\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S8.T6.4.1.1.1.6.6.1\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.6.6.1.1\" style=\"color:#4878D0;\">Agnostic Prompts</span><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.6.6.1.2\" style=\"color:#000000;\"> (Random)</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S8.T6.4.1.1.1.6.6.2\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.6.6.2.1\" style=\"color:#000000;\">2.38</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S8.T6.4.1.1.1.6.6.3\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.6.6.3.1\" style=\"color:#000000;\">4.96</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S8.T6.4.1.1.1.6.6.4\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.6.6.4.1\" style=\"color:#000000;\">4.11</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S8.T6.4.1.1.1.6.6.5\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.6.6.5.1\" style=\"color:#000000;\">5.14</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S8.T6.4.1.1.1.6.6.6\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.6.6.6.1\" style=\"color:#000000;\">3.88</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S8.T6.4.1.1.1.6.6.7\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.6.6.7.1\" style=\"color:#000000;\">3.51</span></span></span>\n<span class=\"ltx_tr\" id=\"S8.T6.4.1.1.1.7.7\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S8.T6.4.1.1.1.7.7.1\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.7.7.1.1\" style=\"color:#EE854A;\">Adversarial Prompts</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S8.T6.4.1.1.1.7.7.2\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.7.7.2.1\" style=\"color:#000000;\">2.49</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S8.T6.4.1.1.1.7.7.3\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.7.7.3.1\" style=\"color:#000000;\">4.36</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S8.T6.4.1.1.1.7.7.4\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.7.7.4.1\" style=\"color:#000000;\">4.02</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S8.T6.4.1.1.1.7.7.5\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.7.7.5.1\" style=\"color:#000000;\">5.12</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S8.T6.4.1.1.1.7.7.6\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.7.7.6.1\" style=\"color:#000000;\">3.40</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S8.T6.4.1.1.1.7.7.7\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.7.7.7.1\" style=\"color:#000000;\">3.28</span></span></span>\n<span class=\"ltx_tr\" id=\"S8.T6.4.1.1.1.8.8\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S8.T6.4.1.1.1.8.8.1\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.8.8.1.1\" style=\"color:#EE854A;\">Adversarial Prompts</span><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.8.8.1.2\" style=\"color:#000000;\"> (SDEdit)</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S8.T6.4.1.1.1.8.8.2\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.8.8.2.1\" style=\"color:#000000;\">2.59</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S8.T6.4.1.1.1.8.8.3\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.8.8.3.1\" style=\"color:#000000;\">4.20</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S8.T6.4.1.1.1.8.8.4\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.8.8.4.1\" style=\"color:#000000;\">3.88</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S8.T6.4.1.1.1.8.8.5\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.8.8.5.1\" style=\"color:#000000;\">4.96</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S8.T6.4.1.1.1.8.8.6\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.8.8.6.1\" style=\"color:#000000;\">3.35</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S8.T6.4.1.1.1.8.8.7\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.8.8.7.1\" style=\"color:#000000;\">3.25</span></span></span>\n<span class=\"ltx_tr\" id=\"S8.T6.4.1.1.1.9.9\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S8.T6.4.1.1.1.9.9.1\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.9.9.1.1\" style=\"color:#000000;\">Deep Augmentation</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S8.T6.4.1.1.1.9.9.2\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.9.9.2.1\" style=\"color:#000000;\">2.42</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S8.T6.4.1.1.1.9.9.3\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.9.9.3.1\" style=\"color:#000000;\">4.24</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S8.T6.4.1.1.1.9.9.4\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.9.9.4.1\" style=\"color:#000000;\">3.70</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S8.T6.4.1.1.1.9.9.5\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.9.9.5.1\" style=\"color:#000000;\">5.01</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S8.T6.4.1.1.1.9.9.6\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.9.9.6.1\" style=\"color:#000000;\">2.83</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S8.T6.4.1.1.1.9.9.7\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.9.9.7.1\" style=\"color:#000000;\">3.70</span></span></span>\n<span class=\"ltx_tr\" id=\"S8.T6.4.1.1.1.10.10\">\n<span class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" id=\"S8.T6.4.1.1.1.10.10.1\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.10.10.1.1\" style=\"color:#000000;\">Style Augmentation</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S8.T6.4.1.1.1.10.10.2\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.10.10.2.1\" style=\"color:#000000;\">2.42</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S8.T6.4.1.1.1.10.10.3\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.10.10.3.1\" style=\"color:#000000;\">4.15</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S8.T6.4.1.1.1.10.10.4\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.10.10.4.1\" style=\"color:#000000;\">3.85</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S8.T6.4.1.1.1.10.10.5\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.10.10.5.1\" style=\"color:#000000;\">5.16</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S8.T6.4.1.1.1.10.10.6\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.10.10.6.1\" style=\"color:#000000;\">2.80</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S8.T6.4.1.1.1.10.10.7\"><span class=\"ltx_text\" id=\"S8.T6.4.1.1.1.10.10.7.1\" style=\"color:#000000;\">3.10</span></span></span>\n</span>\n</span><span class=\"ltx_text\" id=\"S8.T6.4.1.1.2\" style=\"color:#000000;\"></span></span></p>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:144%;\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S8.T6.26.2.1\" style=\"font-size:63%;\">Table 6</span>: </span><span class=\"ltx_text ltx_font_bold\" id=\"S8.T6.2.1\" style=\"font-size:63%;\">Quantitative results on depth estimation.<span class=\"ltx_text ltx_font_medium\" id=\"S8.T6.2.1.1\">  errors on the depth prediction task for a pre-trained U-Net and DPT model. (Lower is better. UNet losses are multiplied by 100 and DPT losses by 10, for readability. Note that the two models were trained with different losses, thus their numbers are not comparable to each other.). We evaluate on distribution shifts from Common Corruptions\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.15309v1#bib.bib29\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">29</span></a>]</cite> (CC), 3D Common Corruptions\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.15309v1#bib.bib37\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">37</span></a>]</cite> (3DCC) and cross-datasets\u00a0(CDS), Replica\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.15309v1#bib.bib73\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">73</span></a>]</cite>. The results from CC and 3DCC are averaged over all distortions and severity levels on Taskonomy. Our method is able to generate training data that can improve results over the baselines on several distribution shifts. Generations with <span class=\"ltx_text\" id=\"S8.T6.2.1.1.1\" style=\"color:#EE854A;\">AP</span> (SDEdit) gives better results than <span class=\"ltx_text\" id=\"S8.T6.2.1.1.2\" style=\"color:#EE854A;\">AP</span> under distribution shifts. Thus, also conditioning on the original image seems to be helpful for these shifts. For the DPT model, the trends are similar, <span class=\"ltx_text\" id=\"S8.T6.2.1.1.3\" style=\"color:#EE854A;\">AP</span> performs better than the baselines. Deep augmentation and style augmentation do not make use of a generative model for generating extra data. They perform comparably to <span class=\"ltx_text\" id=\"S8.T6.2.1.1.4\" style=\"color:#EE854A;\">AP</span>.</span></span></figcaption>\n</figure>",
            "capture": "Table 6: Quantitative results on depth estimation.  errors on the depth prediction task for a pre-trained U-Net and DPT model. (Lower is better. UNet losses are multiplied by 100 and DPT losses by 10, for readability. Note that the two models were trained with different losses, thus their numbers are not comparable to each other.). We evaluate on distribution shifts from Common Corruptions\u00a0[29] (CC), 3D Common Corruptions\u00a0[37] (3DCC) and cross-datasets\u00a0(CDS), Replica\u00a0[73]. The results from CC and 3DCC are averaged over all distortions and severity levels on Taskonomy. Our method is able to generate training data that can improve results over the baselines on several distribution shifts. Generations with AP (SDEdit) gives better results than AP under distribution shifts. Thus, also conditioning on the original image seems to be helpful for these shifts. For the DPT model, the trends are similar, AP performs better than the baselines. Deep augmentation and style augmentation do not make use of a generative model for generating extra data. They perform comparably to AP."
        },
        "7": {
            "table_html": "<figure class=\"ltx_table\" id=\"S8.T7\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S8.T7.8\" style=\"width:263.3pt;height:54pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<p class=\"ltx_p\" id=\"S8.T7.8.1\"><span class=\"ltx_text\" id=\"S8.T7.8.1.1\" style=\"font-size:144%;\">\n<span class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S8.T7.8.1.1.1\">\n<span class=\"ltx_thead\">\n<span class=\"ltx_tr\" id=\"S8.T7.8.1.1.1.1.1\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" id=\"S8.T7.8.1.1.1.1.1.1\"><span class=\"ltx_text\" id=\"S8.T7.8.1.1.1.1.1.1.1\" style=\"color:#000000;\">AP from\\Eval on</span></span>\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S8.T7.8.1.1.1.1.1.2\"><span class=\"ltx_text\" id=\"S8.T7.8.1.1.1.1.1.2.1\" style=\"color:#000000;\">Original data</span></span>\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S8.T7.8.1.1.1.1.1.3\"><span class=\"ltx_text\" id=\"S8.T7.8.1.1.1.1.1.3.1\" style=\"color:#000000;\">UNet</span></span>\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S8.T7.8.1.1.1.1.1.4\"><span class=\"ltx_text\" id=\"S8.T7.8.1.1.1.1.1.4.1\" style=\"color:#000000;\">DPT</span></span></span>\n</span>\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\" id=\"S8.T7.8.1.1.1.2.1\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S8.T7.8.1.1.1.2.1.1\"><span class=\"ltx_text\" id=\"S8.T7.8.1.1.1.2.1.1.1\" style=\"color:#000000;\">UNet</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S8.T7.8.1.1.1.2.1.2\"><span class=\"ltx_text\" id=\"S8.T7.8.1.1.1.2.1.2.1\" style=\"color:#000000;\">2.55</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S8.T7.8.1.1.1.2.1.3\"><span class=\"ltx_text\" id=\"S8.T7.8.1.1.1.2.1.3.1\" style=\"color:#000000;\">7.63</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S8.T7.8.1.1.1.2.1.4\"><span class=\"ltx_text\" id=\"S8.T7.8.1.1.1.2.1.4.1\" style=\"color:#000000;\">5.39</span></span></span>\n<span class=\"ltx_tr\" id=\"S8.T7.8.1.1.1.3.2\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S8.T7.8.1.1.1.3.2.1\"><span class=\"ltx_text\" id=\"S8.T7.8.1.1.1.3.2.1.1\" style=\"color:#000000;\">DPT</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S8.T7.8.1.1.1.3.2.2\"><span class=\"ltx_text\" id=\"S8.T7.8.1.1.1.3.2.2.1\" style=\"color:#000000;\">1.76</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S8.T7.8.1.1.1.3.2.3\"><span class=\"ltx_text\" id=\"S8.T7.8.1.1.1.3.2.3.1\" style=\"color:#000000;\">7.17</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S8.T7.8.1.1.1.3.2.4\"><span class=\"ltx_text\" id=\"S8.T7.8.1.1.1.3.2.4.1\" style=\"color:#000000;\">6.46</span></span></span>\n</span>\n</span><span class=\"ltx_text\" id=\"S8.T7.8.1.1.2\" style=\"color:#000000;\"></span></span></p>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:144%;\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S8.T7.14.4.1\" style=\"font-size:63%;\">Table 7</span>: </span><span class=\"ltx_text\" id=\"S8.T7.6.3\" style=\"font-size:63%;\">Performance of a model on generated images from Adversarial Prompts\u00a0from another model. For the UNet model we report the  loss ( for readability) and the DPT model, the Midas loss ( for readability). The Adversarial Prompts\u00a0attained from performing adversarial optimization on a UNet model and evaluated on the same model result in a loss of 7.63. Generations from the DPT model evaluated on the UNet model result in a loss of 7.17. Thus, the adversarial prompts found for one model seems to also be adversarial for another. We also report the loss on the original images for comparison.</span></figcaption>\n</figure>",
            "capture": "Table 7: Performance of a model on generated images from Adversarial Prompts\u00a0from another model. For the UNet model we report the  loss ( for readability) and the DPT model, the Midas loss ( for readability). The Adversarial Prompts\u00a0attained from performing adversarial optimization on a UNet model and evaluated on the same model result in a loss of 7.63. Generations from the DPT model evaluated on the UNet model result in a loss of 7.17. Thus, the adversarial prompts found for one model seems to also be adversarial for another. We also report the loss on the original images for comparison."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.15309v1_figure_1.png",
            "caption": "Figure 1: A framework to generate model- and target distribution-informed training examples.\nLeft:\nAn overview of how we generate training data for a given supervised model f\ud835\udc53fitalic_f and target distribution. Suppose g\ud835\udc54gitalic_g is a text-to-image generative model that generates images conditioned on a text prompt, S\ud835\udc46Sitalic_S and label, y\ud835\udc66yitalic_y.\nA supervised model trained to perform a task, e.g., image classification or depth estimation, is denoted by f\ud835\udc53fitalic_f.\nWe aim to find prompts that would generate training data useful for f\ud835\udc53fitalic_f, and we do so via two feedback mechanisms.\nThe first mechanism makes use of feedback from f\ud835\udc53fitalic_f. In particular, we maximize the loss on the predictions from f\ud835\udc53fitalic_f to get Adversarial Prompts.\nThe space of Adversarial Prompts and corresponding adversarial examples are represented by orange circles.\nSince there can be many adversarial examples not relevant to a particular target distribution, we introduce a second feedback mechanism that guides the prompts towards a certain target distribution (green circle).\nCombining the two mechanisms results in Guided Adversarial Prompts, represented by S*superscript\ud835\udc46S^{*}italic_S start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT. This results in generations that are both relevant to the target distribution and where f\ud835\udc53fitalic_f does not perform well.\nRight:\nWe show some examples of generations attained by our method on several datasets and tasks. Namely, Taskonomy [80] (depth estimation), iWildCam [3] (animal classification), and Waterbirds [66] (bird classification). An exemplar training image is shown in the first row and the third row illustrates examples of target distributions. The second row shows the generations from Adversarial Prompts which uses only model feedback.\nIt finds novel \u201cstyles\u201d that fool a given model f\ud835\udc53fitalic_f, but may not match a target distribution (third row).\nGuided Adversarial Prompts (fourth row) uses feedback from the target distribution and generates images that both fool the model and are similar to the target."
        },
        "2": {
            "figure_path": "2403.15309v1_figure_2.png",
            "caption": "Figure 2: Ways to alleviate the misalignment of the generation with its conditioning. The third column onwards shows some examples of generations from depth maps that do not follow the depth conditioning. See the first and second columns for the original image and its depth label. There are several ways to constrain the generation to alleviate this misalignment.\n1. Early stopping involves stopping the adversarial optimization when the loss reaches a certain threshold. The resulting generations from early stopping are shown in the fourth column.\n2. SDEdit [51] involves conditioning the generation process on the original image. This mechanism is applied during generation with the adversarial prompts i.e., applying SDEdit to the prompts that generated the images in the third column results in the last column generations. Both SDEdit and early stopping are able to improve the alignment of the generations with depth conditioning."
        },
        "3": {
            "figure_path": "2403.15309v1_figure_3.png",
            "caption": "Figure 3:  On Waterbirds, Guided Adversarial Prompts are able to generate counterfactual examples not present in the original training dataset and improves the data-efficiency over other methods.\nLeft: We train a classification model on the original spuriously correlated dataset (see examples on the right) with varying number of extra data points generated using different types of prompts.\nWe measure the accuracy on a balanced set where waterbirds and landbirds appear on both land and water.\nWe run each experiment with three seeds and report the mean and standard deviation.\nRight: We show examples of generated images with each type of prompts on the Waterbirds. We also provide original training examples where the background is a perfectly predictive feature of the bird type.\nA red frame signifies that the model trained only on the original data (No Extra Data) misclassifies the image, and a green frame stands for the correct prediction. We observe that having a guidance mechanism towards the target image distribution consistently improves on top of the Agnostic Prompts baseline (\u201cnature\u201d prompt).\nAdversarial Prompts, while fooling the model, generates images that are different from the target distribution and, thus, not useful to adapt the model to it.\nCombining both mechanisms in Guided Adversarial Prompts leads to improved data efficiency.\nUnlike Guided Prompts that uses the same prompts to generate images for both classes, GAP finds prompts that generate images the model fails on, this leads to generation of waterbirds on land and landbirds on water, the combinations not present in the original training data, which are necessary data samples for the model to learn the bird predictive feature."
        },
        "4": {
            "figure_path": "2403.15309v1_figure_4.png",
            "caption": "Figure 4: Left: For iWildCam, Guided Adversarial Prompts are superior in performance and data-efficiency.\nWe train a model on the combination of the original training data and extra data generated using different types of prompts.\nWe show the average accuracy on two iWildCam test camera trap locations.\nWe run each experiment with three seeds and report the mean and standard deviation.\n(1) Guided Adversarial Prompts consistently outperforms Guided Prompts and Adversarial Prompts.\nFor AP and GAP, there is a much lower performance drop (1-3%) when reducing the amount of generated data compared to GP (9%). This suggests that being only target-informed requires a more exhaustive exploration of the image space to find samples that are \u201duseful\u201d for the pre-trained model, compared to being model-informed.\nRight: Qualitative results and comparison with ALIA on the iWildCam dataset. From left to right: the real training data from iWildCam; generation using the Agnostic Prompts template: \u201ca camera trap photo of {class name}\u201d; Guided Prompts (ALIA) generations with target-informed prompt template: \u201ca photo of {class name} in a grassy field with trees and bushes\u201d; Adversarial Prompts generations; and Guided Adversarial Prompts generations with the same target distribution as Guided Prompts. GAP generations, while being visually coherent with the \u201cgrassy field\u201d location, introduce an adversarial \u201ccamouflage\u201d effect. White arrows point to the animals for clarity. From top to bottom, they are (mis)classified as cattle, dik-dik, cattle, cattle."
        },
        "5": {
            "figure_path": "2403.15309v1_figure_5.png",
            "caption": "Figure 5: Left: A comparison of generations with different prompts. A comparison of the generated data for the baselines and our method. Generations with Adversarial Prompts results in diverse styles that are distinct from the original training data (see fifth column). Adversarial Prompts with SDEdit results in generations shown in the fourth column. As these generations are conditioned on the original image, they look more similar to them.\nThe last 4 columns show the generations with text guidance for target shifts fog and blur. Using Guided Prompts alone, in this case \u201cfog\u201d or \u201cblur\u201d, results in generations with a mild fog or blur. Performing Guided Adversarial Prompts results in generations with more severe fog or blur.\nRight: Performance of GAP with different amount of added data. The distribution shift, in this case, is defocus blur applied on the Taskonomy test set. The plot shows the \u21131subscriptnormal-\u21131\\ell_{1}roman_\u2113 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT loss (\u00d7100absent100\\times 100\u00d7 100) of the U-Net model versus the amount of extra data generated and used for fine-tuning. In this example, we performed image guidance with a set of 100 unlabelled corrupted RGB images. The plot shows that both GP and GAP were able to guide the optimization toward generating training data relevant to the distribution shift. Furthermore, there is a large improvement compared to using AP or the baselines with as little as 10 extra generated samples. See Appendix Fig. 18 for results on other corruptions."
        },
        "6": {
            "figure_path": "2403.15309v1_figure_6.png",
            "caption": "(b) Comparing the performance from running multiple iterations versus a single iteration of adversarial optimization, generation, and fine-tuning. The plot shows the \u21131subscriptnormal-\u21131\\ell_{1}roman_\u2113 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT loss (\u00d7100absent100\\times 100\u00d7 100) of the U-Net model against the number of iterations on the depth prediction task. The \u21131subscriptnormal-\u21131\\ell_{1}roman_\u2113 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT loss is computed on the Taskonomy dataset under common corruptions, averaged over all corruptions and severity levels. The single- and multi-iteration runs have similar settings in terms of the total number of AP used, the same number of tokens per prompt, etc. See Appendix Sec. 8.4.1 for further details. The first iteration of the multi-iteration run resulted in the largest improvement in performance. However, it converges to a similar performance as the single iteration run. Thus, we chose to perform only a single iteration in Fig. 5(a)."
        },
        "7": {
            "figure_path": "2403.15309v1_figure_7.png",
            "caption": "Figure 7: \nDistribution shift in the Waterbirds dataset. The background is a perfectly predictive spurious feature on the training distribution, but loses predictive power in the test distribution."
        },
        "8": {
            "figure_path": "2403.15309v1_figure_8.png",
            "caption": "Figure 8: \nGeneration examples for Guided Adversarial Prompts.\nEach column shows generations for the corresponding class.\nEach row in a column (separated by gray lines) shows generations for one found token.\nThere are eight tokens in total, four for each class.\nGAP tend to generate landbirds on water (left) and of waterbirds on land (right), the combinations not present in the original training data (see Fig. 7)"
        },
        "9": {
            "figure_path": "2403.15309v1_figure_9.png",
            "caption": "Figure 9: \nGeneration examples for Adversarial Prompts.\nEach column shows generations for the corresponding class.\nEach row in a column (separated by gray lines) shows generations for one found token.\nThere are eight tokens in total, four for each class.\nWhile AP finds tokens that fool the model, the generated images are different from the target distribution (land or water background)."
        },
        "10": {
            "figure_path": "2403.15309v1_figure_10.png",
            "caption": "Figure 10: iWildCam Test Locations. Random samples from the four target distributions. First row is location=1, day & night. Second row is location=2, day & night."
        },
        "11": {
            "figure_path": "2403.15309v1_figure_11.png",
            "caption": "Figure 11: Using the negative cross-entropy loss may lead to adversarial animal (e.g., elephants) insertions, destroying the alignment between x~normal-~\ud835\udc65\\tilde{x}over~ start_ARG italic_x end_ARG and y\ud835\udc66yitalic_y. First row contains the original training images. The labels are [background, background, cattle]. Second row contains the corresponding generated samples using a guided adversarial prompt, optimized with negative cross-entropy loss as the adversarial loss."
        },
        "12": {
            "figure_path": "2403.15309v1_figure_12.png",
            "caption": "Figure 12: Generations with the 4 guided adversarial prompts for iWildCam. 1st column is the original image. Then from left from right, we have the guided adversarial prompts for location 1 during the day, during the night, and then location 2 during the day, during the night. SDEdit (strength 0.80.80.80.8) was used during the\nadversarial optimization and generation."
        },
        "13": {
            "figure_path": "2403.15309v1_figure_13.png",
            "caption": "Figure 13: Generations with the 4 adversarial prompts for iWildCam. 1st column is the original image. Then, each column is an adversarial prompt initialized with a different seed. SDEdit (strength 0.80.80.80.8) was used during the\nadversarial optimization and generation."
        },
        "14": {
            "figure_path": "2403.15309v1_figure_14.png",
            "caption": "Figure 14: \nWe train a ViT-B/16 model on the combination of the original training data and extra data generated using different types of prompts.\nWe show the average accuracy on two iWildCam test camera trap locations.\nWe run each experiment with three seeds and report the mean and standard deviation."
        },
        "15": {
            "figure_path": "2403.15309v1_figure_15.png",
            "caption": "Figure 15:  A comparison of generations with adversarial optimization on different models. First row is original image. Second to fith row are generated using the four ResNet-based adversarial prompts. The four last rows are generated using the four ViT-based adversarial prompts."
        },
        "16": {
            "figure_path": "2403.15309v1_figure_16.png",
            "caption": "Figure 16: \nResults of using Textual Inversion image guidance.\nWe use the same plot as in\nFig. 3.\nWe add two additional methods using Textual Inversion image guidance described in Sec. 7.6.\nTI GAP is the Guided Adversarial Prompts with TI image guidance instead of the CLIP guidance.\nTI GP uses only TI guidance to find prompts.\nThe TI guidance works on par with the CLIP guidance when used with adversarial optimization (TI GAP).\nHowever, using only TI guidance (TI GP) results in worse performance than using only text guidance with prompts found by the ALIA method [18]."
        },
        "17": {
            "figure_path": "2403.15309v1_figure_17.png",
            "caption": "Figure 17: Generalization to generated images from similar or larger number of tokens. This plot shows the performance of the UNet model when trained on Adversarial Prompts with n\ud835\udc5bnitalic_n number of tokens and tested on Adversarial Prompts with m\ud835\udc5amitalic_m number of tokens (denoted in the plot as n\u2192m\u2192\ud835\udc5b\ud835\udc5an\\rightarrow mitalic_n \u2192 italic_m). With the exception of 1\u21921\u2192111\\rightarrow 11 \u2192 1 and 1\u21928\u2192181\\rightarrow 81 \u2192 8, training on Adversarial Prompts with n\ud835\udc5bnitalic_n tokens and testing on m,n\u2260m\ud835\udc5a\ud835\udc5b\ud835\udc5am,n\\neq mitalic_m , italic_n \u2260 italic_m results in higher loss than training and testing on the same number of tokens."
        },
        "18": {
            "figure_path": "2403.15309v1_figure_18.png",
            "caption": "Figure 18: A comparison of text and image guidance for two distribution shifts fog and blur. The base model used here is the UNet model.\nTop: Generated images from Guided Prompts and Guided Adversarial Prompts. All images were generated with SDEdit, strength 0.9, thus, they tend to look similar to the original image (2nd column).\nUsing Guided Prompts alone for either text or image guidance results in generations with a mild fog or blur. With Guided Adversarial Prompts, we get generations with more severe fog or blur.\nFor image guidance, we sampled random (unlabelled) images with blur and fog from the Common Corruptions evaluation set. See the last two columns for some sample images.\nBottom: The plots show the results from fine-tuning with text and image guidance, evaluated on fog and blur for CC and 3DCC. Note that the image guidance here uses (unlabelled) samples from the same target distribution that it was sampled on. In all cases, Guided Adversarial Prompts outperforms Guided Prompts with large enough extra data."
        },
        "19": {
            "figure_path": "2403.15309v1_figure_19.png",
            "caption": "Figure 19: Generations from all Adversarial Prompts with the UNet model as the base model."
        },
        "20": {
            "figure_path": "2403.15309v1_figure_20.png",
            "caption": "Figure 20: Generations from all Adversarial Prompts with the UNet model as the base model. SDEdit (strength 0.6) was used during the adversarial optimization and generation, thus, the generations look similar to the original image. Zoom in to see the different perturbations generated."
        },
        "21": {
            "figure_path": "2403.15309v1_figure_21.png",
            "caption": "Figure 21: Generations from all Adversarial Prompts with the UNet model as the base model for the multi-iteration case i.e., multiple iterations of adversarial optimization, generation and fine-tuning. The colored borders denote the iteration number. Note that we set the early stopping threshold to be 0.1 for the first 3 iterations and 0.08 for the other iterations. We optimized for 4 prompts for each iteration, with an increasing number of tokens for each prompt."
        },
        "22": {
            "figure_path": "2403.15309v1_figure_22.png",
            "caption": "Figure 22: Generations from all Adversarial Prompts with the DPT model as the base model. The model was trained on Omnidata which consists of 5 datasets and we optimized for 6 Adversarial Prompts for each data. Each row shows the generation from the 6 different prompts for that dataset."
        },
        "23": {
            "figure_path": "2403.15309v1_figure_23.png",
            "caption": "Figure 23: Generations from all Adversarial Prompts with a DPT model, also trained on Omnidata. However, this model was also trained with CC and 3DCC augmentations and consistency constraints."
        }
    },
    "references": [
        {
            "1": {
                "title": "Break-a-scene: Extracting multiple concepts from a single image.",
                "author": "Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-Or, and Dani Lischinski.",
                "venue": "arXiv preprint arXiv:2305.16311, 2023.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Efficient Pipeline for Camera Trap Image Review, 2019.",
                "author": "Sara Beery, Dan Morris, and Siyu Yang.",
                "venue": "arXiv:1907.06772 [cs].",
                "url": null
            }
        },
        {
            "3": {
                "title": "The iwildcam 2021 competition dataset.",
                "author": "Sara Beery, Arushi Agarwal, Elijah Cole, and Vighnesh Birodkar.",
                "venue": "arXiv preprint arXiv:2105.03494, 2021a.",
                "url": null
            }
        },
        {
            "4": {
                "title": "The iwildcam 2021 competition dataset.",
                "author": "Sara Beery, Arushi Agarwal, Elijah Cole, and Vighnesh Birodkar.",
                "venue": "arXiv preprint arXiv:2105.03494, 2021b.",
                "url": null
            }
        },
        {
            "5": {
                "title": "This dataset does not exist: training models from generated images.",
                "author": "Victor Besnier, Himalaya Jain, Andrei Bursuc, Matthieu Cord, and Patrick\nP\u00e9rez.",
                "venue": "In ICASSP 2020-2020 IEEE international conference on\nacoustics, speech and signal processing (ICASSP), pages 1\u20135. IEEE, 2020.",
                "url": null
            }
        },
        {
            "6": {
                "title": "Visual control of manual actions: brain mechanisms in typical\ndevelopment and developmental disorders.",
                "author": "Oliver Braddick and Janette Atkinson.",
                "venue": "Developmental Medicine and Child Neurology, 55 Suppl\n4:13\u201318, 2013.",
                "url": null
            }
        },
        {
            "7": {
                "title": "Instructpix2pix: Learning to follow image editing instructions.",
                "author": "Tim Brooks, Aleksander Holynski, and Alexei A Efros.",
                "venue": "In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pages 18392\u201318402, 2023.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Ensembling with Deep Generative Views, 2021.",
                "author": "Lucy Chai, Jun-Yan Zhu, Eli Shechtman, Phillip Isola, and Richard Zhang.",
                "venue": "arXiv:2104.14551 [cs].",
                "url": null
            }
        },
        {
            "9": {
                "title": "Diffedit: Diffusion-based semantic image editing with mask\nguidance.",
                "author": "Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord.",
                "venue": "arXiv preprint arXiv:2210.11427, 2022.",
                "url": null
            }
        },
        {
            "10": {
                "title": "Autoaugment: Learning augmentation policies from data.",
                "author": "Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le.",
                "venue": "arXiv preprint arXiv:1805.09501, 2018.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Randaugment: Practical automated data augmentation with a reduced\nsearch space.",
                "author": "Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le.",
                "venue": "In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition workshops, pages 702\u2013703,\n2020a.",
                "url": null
            }
        },
        {
            "12": {
                "title": "Randaugment: Practical automated data augmentation with a reduced\nsearch space.",
                "author": "Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le.",
                "venue": "In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition workshops, pages 702\u2013703,\n2020b.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Imagenet: A large-scale hierarchical image database.",
                "author": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.",
                "venue": "In 2009 IEEE conference on computer vision and pattern\nrecognition, pages 248\u2013255. Ieee, 2009.",
                "url": null
            }
        },
        {
            "14": {
                "title": "Adversarial Training Helps Transfer Learning via Better\nRepresentations, 2021.",
                "author": "Zhun Deng, Linjun Zhang, Kailas Vodrahalli, Kenji Kawaguchi, and James Zou.",
                "venue": "arXiv:2106.10189 [cs].",
                "url": null
            }
        },
        {
            "15": {
                "title": "Viewfool: Evaluating the robustness of visual recognition to\nadversarial viewpoints.",
                "author": "Yinpeng Dong, Shouwei Ruan, Hang Su, Caixin Kang, Xingxing Wei, and Jun Zhu.",
                "venue": "Advances in Neural Information Processing Systems,\n35:36789\u201336803, 2022.",
                "url": null
            }
        },
        {
            "16": {
                "title": "An image is worth 16x16 words: Transformers for image recognition\nat scale.",
                "author": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,\nXiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg\nHeigold, Sylvain Gelly, and others.",
                "venue": "arXiv preprint arXiv:2010.11929, 2020a.",
                "url": null
            }
        },
        {
            "17": {
                "title": "An image is worth 16x16 words: Transformers for image recognition\nat scale.",
                "author": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,\nXiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg\nHeigold, Sylvain Gelly, and others.",
                "venue": "arXiv preprint arXiv:2010.11929, 2020b.",
                "url": null
            }
        },
        {
            "18": {
                "title": "Diversify your vision datasets with automatic diffusion-based\naugmentation.",
                "author": "Lisa Dunlap, Alyssa Umino, Han Zhang, Jiezhi Yang, Joseph E Gonzalez, and\nTrevor Darrell.",
                "venue": "arXiv preprint arXiv:2305.16289, 2023.",
                "url": null
            }
        },
        {
            "19": {
                "title": "Omnidata: A scalable pipeline for making multi-task mid-level\nvision datasets from 3D scans.",
                "author": "Ainaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir Zamir.",
                "venue": "In Proceedings of the IEEE/CVF international conference on\ncomputer vision, pages 10786\u201310796, 2021a.",
                "url": null
            }
        },
        {
            "20": {
                "title": "Omnidata: A Scalable Pipeline for Making Multi-Task\nMid-Level Vision Datasets from 3D Scans.",
                "author": "Ainaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir Zamir.",
                "venue": "In Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 10786\u201310796, 2021b.",
                "url": null
            }
        },
        {
            "21": {
                "title": "DataComp: In search of the next generation of multimodal\ndatasets, 2023.",
                "author": "Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios\nSmyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu\nZhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek\nRamanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu,\nMehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander Ratner,\nShuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh,\nAlex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig\nSchmidt.",
                "venue": "arXiv:2304.14108 [cs].",
                "url": null
            }
        },
        {
            "22": {
                "title": "An image is worth one word: Personalizing text-to-image generation\nusing textual inversion.",
                "author": "Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal\nChechik, and Daniel Cohen-Or.",
                "venue": "arXiv preprint arXiv:2208.01618, 2022.",
                "url": null
            }
        },
        {
            "23": {
                "title": "ImageNet-trained CNNs are biased towards texture; increasing\nshape bias improves accuracy and robustness.",
                "author": "Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A\nWichmann, and Wieland Brendel.",
                "venue": "arXiv preprint arXiv:1811.12231, 2018.",
                "url": null
            }
        },
        {
            "24": {
                "title": "Svdiff: Compact parameter space for diffusion fine-tuning.",
                "author": "Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, and Feng\nYang.",
                "venue": "arXiv preprint arXiv:2303.11305, 2023.",
                "url": null
            }
        },
        {
            "25": {
                "title": "Deep residual learning for image recognition.",
                "author": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.",
                "venue": "In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 770\u2013778, 2016a.",
                "url": null
            }
        },
        {
            "26": {
                "title": "Deep residual learning for image recognition.",
                "author": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.",
                "venue": "In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 770\u2013778, 2016b.",
                "url": null
            }
        },
        {
            "27": {
                "title": "Is synthetic data from generative models ready for image recognition?",
                "author": "Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip Torr, Song\nBai, and Xiaojuan Qi.",
                "venue": "arXiv preprint arXiv:2210.07574, 2022a.",
                "url": null
            }
        },
        {
            "28": {
                "title": "Is synthetic data from generative models ready for image recognition?",
                "author": "Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip Torr, Song\nBai, and Xiaojuan Qi.",
                "venue": "arXiv preprint arXiv:2210.07574, 2022b.",
                "url": null
            }
        },
        {
            "29": {
                "title": "Benchmarking neural network robustness to common corruptions and\nperturbations.",
                "author": "Dan Hendrycks and Thomas Dietterich.",
                "venue": "arXiv preprint arXiv:1903.12261, 2019.",
                "url": null
            }
        },
        {
            "30": {
                "title": "Augmix: A simple data processing method to improve robustness and\nuncertainty.",
                "author": "Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji\nLakshminarayanan.",
                "venue": "arXiv preprint arXiv:1912.02781, 2019.",
                "url": null
            }
        },
        {
            "31": {
                "title": "The many faces of robustness: A critical analysis of\nout-of-distribution generalization.",
                "author": "Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan\nDorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, and others.",
                "venue": "In Proceedings of the IEEE/CVF international conference on\ncomputer vision, pages 8340\u20138349, 2021.",
                "url": null
            }
        },
        {
            "32": {
                "title": "Prompt-to-prompt image editing with cross attention control.",
                "author": "Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel\nCohen-Or.",
                "venue": "arXiv preprint arXiv:2208.01626, 2022.",
                "url": null
            }
        },
        {
            "33": {
                "title": "Denoising diffusion probabilistic models.",
                "author": "Jonathan Ho, Ajay Jain, and Pieter Abbeel.",
                "venue": "Advances in neural information processing systems,\n33:6840\u20136851, 2020.",
                "url": null
            }
        },
        {
            "34": {
                "title": "An edit friendly DDPM noise space: Inversion and manipulations.",
                "author": "Inbar Huberman-Spiegelglas, Vladimir Kulikov, and Tomer Michaeli.",
                "venue": "arXiv preprint arXiv:2304.06140, 2023.",
                "url": null
            }
        },
        {
            "35": {
                "title": "Generative models as a data source for multiview representation\nlearning.",
                "author": "Ali Jahanian, Xavier Puig, Yonglong Tian, and Phillip Isola.",
                "venue": "arXiv preprint arXiv:2106.05258, 2021.",
                "url": null
            }
        },
        {
            "36": {
                "title": "Distilling model failures as directions in latent space.",
                "author": "Saachi Jain, Hannah Lawrence, Ankur Moitra, and Aleksander Madry.",
                "venue": "arXiv preprint arXiv:2206.14754, 2022.",
                "url": null
            }
        },
        {
            "37": {
                "title": "3D Common Corruptions for Object Recognition.",
                "author": "O\u011fuzhan Fatih Kar, Teresa Yeo, and Amir Zamir.",
                "venue": "In ICML 2022 Shift Happens Workshop, 2022.",
                "url": null
            }
        },
        {
            "38": {
                "title": "Adam: A Method for Stochastic Optimization, 2017.",
                "author": "Diederik P. Kingma and Jimmy Ba.",
                "venue": "arXiv:1412.6980 [cs].",
                "url": null
            }
        },
        {
            "39": {
                "title": "Auto-Encoding Variational Bayes, 2022.",
                "author": "Diederik P. Kingma and Max Welling.",
                "venue": "arXiv:1312.6114 [cs, stat].",
                "url": null
            }
        },
        {
            "40": {
                "title": "Segment Anything, 2023.",
                "author": "Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura\nGustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr\nDoll\u00e1r, and Ross Girshick.",
                "venue": "arXiv:2304.02643 [cs].",
                "url": null
            }
        },
        {
            "41": {
                "title": "Wilds: A benchmark of in-the-wild distribution shifts.",
                "author": "Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang,\nAkshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips,\nIrena Gao, and others.",
                "venue": "In International conference on machine learning, pages\n5637\u20135664. PMLR, 2021a.",
                "url": null
            }
        },
        {
            "42": {
                "title": "Wilds: A benchmark of in-the-wild distribution shifts.",
                "author": "Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang,\nAkshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips,\nIrena Gao, and others.",
                "venue": "In International Conference on Machine Learning, pages\n5637\u20135664. PMLR, 2021b.",
                "url": null
            }
        },
        {
            "43": {
                "title": "Semantic palette: Guiding scene generation with class proportions.",
                "author": "Guillaume Le Moing, Tuan-Hung Vu, Himalaya Jain, Patrick P\u00e9rez, and Matthieu\nCord.",
                "venue": "In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pages 9342\u20139350, 2021.",
                "url": null
            }
        },
        {
            "44": {
                "title": "Multiple sensitive periods in human visual development: evidence from\nvisually deprived children.",
                "author": "Terri L. Lewis and Daphne Maurer.",
                "venue": "Developmental Psychobiology, 46(3):163\u2013183, 2005.",
                "url": null
            }
        },
        {
            "45": {
                "title": "BLIP: Bootstrapping Language-Image Pre-training for\nUnified Vision-Language Understanding and Generation, 2022.",
                "author": "Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.",
                "venue": "arXiv:2201.12086 [cs].",
                "url": null
            }
        },
        {
            "46": {
                "title": "Decoupled Weight Decay Regularization, 2019.",
                "author": "Ilya Loshchilov and Frank Hutter.",
                "venue": "arXiv:1711.05101 [cs, math].",
                "url": null
            }
        },
        {
            "47": {
                "title": "RePaint: Inpainting using Denoising Diffusion Probabilistic\nModels, 2022.",
                "author": "Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and\nLuc Van Gool.",
                "venue": "arXiv:2201.09865 [cs].",
                "url": null
            }
        },
        {
            "48": {
                "title": "Latent Consistency Models: Synthesizing High-Resolution\nImages with Few-Step Inference, 2023.",
                "author": "Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao.",
                "venue": "arXiv:2310.04378 [cs].",
                "url": null
            }
        },
        {
            "49": {
                "title": "Towards deep learning models resistant to adversarial attacks.",
                "author": "Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and\nAdrian Vladu.",
                "venue": "arXiv preprint arXiv:1706.06083, 2017.",
                "url": null
            }
        },
        {
            "50": {
                "title": "Sdedit: Guided image synthesis and editing with stochastic\ndifferential equations.",
                "author": "Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and\nStefano Ermon.",
                "venue": "arXiv preprint arXiv:2108.01073, 2021a.",
                "url": null
            }
        },
        {
            "51": {
                "title": "Sdedit: Guided image synthesis and editing with stochastic\ndifferential equations.",
                "author": "Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and\nStefano Ermon.",
                "venue": "arXiv preprint arXiv:2108.01073, 2021b.",
                "url": null
            }
        },
        {
            "52": {
                "title": "Accuracy on the line: On the strong correlation between\nout-of-distribution and in-distribution generalization.",
                "author": "John P Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang Wei Koh,\nVaishaal Shankar, Percy Liang, Yair Carmon, and Ludwig Schmidt.",
                "venue": "In International conference on machine learning, pages\n7721\u20137735. PMLR, 2021.",
                "url": null
            }
        },
        {
            "53": {
                "title": "Null-text inversion for editing real images using guided diffusion\nmodels.",
                "author": "Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or.",
                "venue": "In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pages 6038\u20136047, 2023.",
                "url": null
            }
        },
        {
            "54": {
                "title": "T2i-adapter: Learning adapters to dig out more controllable ability\nfor text-to-image diffusion models.",
                "author": "Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and\nXiaohu Qie.",
                "venue": "arXiv preprint arXiv:2302.08453, 2023.",
                "url": null
            }
        },
        {
            "55": {
                "title": "GLIDE: Towards Photorealistic Image Generation and\nEditing with Text-Guided Diffusion Models, 2022.",
                "author": "Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin,\nBob McGrew, Ilya Sutskever, and Mark Chen.",
                "venue": "arXiv:2112.10741 [cs].",
                "url": null
            }
        },
        {
            "56": {
                "title": "GPT-4 Technical Report, 2023.",
                "author": "OpenAI.",
                "venue": "arXiv:2303.08774 [cs].",
                "url": null
            }
        },
        {
            "57": {
                "title": "Learning transferable visual models from natural language\nsupervision.",
                "author": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,\nSandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,\nand others.",
                "venue": "In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021.",
                "url": null
            }
        },
        {
            "58": {
                "title": "Vision transformers for dense prediction.",
                "author": "Ren\u00e9 Ranftl, Alexey Bochkovskiy, and Vladlen Koltun.",
                "venue": "In Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 12179\u201312188, 2021.",
                "url": null
            }
        },
        {
            "59": {
                "title": "Classification Accuracy Score for Conditional Generative\nModels, 2019.",
                "author": "Suman Ravuri and Oriol Vinyals.",
                "venue": "arXiv:1905.10887 [cs, stat].",
                "url": null
            }
        },
        {
            "60": {
                "title": "On the convergence of adam and beyond.",
                "author": "Sashank J Reddi, Satyen Kale, and Sanjiv Kumar.",
                "venue": "arXiv preprint arXiv:1904.09237, 2019.",
                "url": null
            }
        },
        {
            "61": {
                "title": "High-resolution image synthesis with latent diffusion models.",
                "author": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn\nOmmer.",
                "venue": "In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pages 10684\u201310695, 2022a.",
                "url": null
            }
        },
        {
            "62": {
                "title": "High-resolution image synthesis with latent diffusion models.",
                "author": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn\nOmmer.",
                "venue": "In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pages 10684\u201310695, 2022b.",
                "url": null
            }
        },
        {
            "63": {
                "title": "U-net: Convolutional networks for biomedical image segmentation.",
                "author": "Olaf Ronneberger, Philipp Fischer, and Thomas Brox.",
                "venue": "In International conference on medical image computing and\ncomputer-assisted intervention, pages 234\u2013241. Springer, 2015.",
                "url": null
            }
        },
        {
            "64": {
                "title": "Dreambooth: Fine tuning text-to-image diffusion models for\nsubject-driven generation.",
                "author": "Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and\nKfir Aberman.",
                "venue": "In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pages 22500\u201322510, 2023.",
                "url": null
            }
        },
        {
            "65": {
                "title": "Distributionally robust neural networks for group shifts: On the\nimportance of regularization for worst-case generalization.",
                "author": "Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang.",
                "venue": "arXiv preprint arXiv:1911.08731, 2019a.",
                "url": null
            }
        },
        {
            "66": {
                "title": "Distributionally robust neural networks for group shifts: On the\nimportance of regularization for worst-case generalization.",
                "author": "Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang.",
                "venue": "arXiv preprint arXiv:1911.08731, 2019b.",
                "url": null
            }
        },
        {
            "67": {
                "title": "Photorealistic Text-to-Image Diffusion Models with Deep\nLanguage Understanding, 2022.",
                "author": "Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily\nDenton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi,\nRapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad\nNorouzi.",
                "venue": "arXiv:2205.11487 [cs].",
                "url": null
            }
        },
        {
            "68": {
                "title": "Fake it till you make it: Learning transferable representations\nfrom synthetic ImageNet clones.",
                "author": "Mert Bulent Sariyildiz, Karteek Alahari, Diane Larlus, and Yannis Kalantidis.",
                "venue": "In CVPR 2023\u2013IEEE/CVF conference on computer vision and\npattern recognition, 2023.",
                "url": null
            }
        },
        {
            "69": {
                "title": "LAION-5B: An open large-scale dataset for training next\ngeneration image-text models, 2022.",
                "author": "Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross\nWightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell\nWortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig\nSchmidt, Robert Kaczmarczyk, and Jenia Jitsev.",
                "venue": "arXiv:2210.08402 [cs].",
                "url": null
            }
        },
        {
            "70": {
                "title": "Deep unsupervised learning using nonequilibrium thermodynamics.",
                "author": "Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli.",
                "venue": "In International conference on machine learning, pages\n2256\u20132265. PMLR, 2015.",
                "url": null
            }
        },
        {
            "71": {
                "title": "Denoising diffusion implicit models.",
                "author": "Jiaming Song, Chenlin Meng, and Stefano Ermon.",
                "venue": "arXiv preprint arXiv:2010.02502, 2020a.",
                "url": null
            }
        },
        {
            "72": {
                "title": "Denoising diffusion implicit models.",
                "author": "Jiaming Song, Chenlin Meng, and Stefano Ermon.",
                "venue": "arXiv preprint arXiv:2010.02502, 2020b.",
                "url": null
            }
        },
        {
            "73": {
                "title": "The Replica Dataset: A Digital Replica of Indoor\nSpaces.",
                "author": "Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik Wijmans, Simon Green,\nJakob J. Engel, Raul Mur-Artal, Carl Ren, Shobhit Verma, Anton Clarkson,\nMingfei Yan, Brian Budge, Yajie Yan, Xiaqing Pan, June Yon, Yuyang Zou,\nKimberly Leon, Nigel Carter, Jesus Briales, Tyler Gillingham, Elias Mueggler,\nLuis Pesqueira, Manolis Savva, Dhruv Batra, Hauke M. Strasdat, Renzo De\nNardi, Michael Goesele, Steven Lovegrove, and Richard Newcombe.",
                "venue": "arXiv preprint arXiv:1906.05797, 2019.",
                "url": null
            }
        },
        {
            "74": {
                "title": "Measuring robustness to natural distribution shifts in image\nclassification.",
                "author": "Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht,\nand Ludwig Schmidt.",
                "venue": "arXiv preprint arXiv:2007.00644, 2020.",
                "url": null
            }
        },
        {
            "75": {
                "title": "The Caltech-UCSD Birds-200-2011 Dataset.",
                "author": "Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge\nBelongie.",
                "venue": "California Institute of Technology, 2011.",
                "url": null
            }
        },
        {
            "76": {
                "title": "Learning perturbation sets for robust machine learning.",
                "author": "Eric Wong and J Zico Kolter.",
                "venue": "arXiv preprint arXiv:2007.08450, 2020.",
                "url": null
            }
        },
        {
            "77": {
                "title": "Not just pretty pictures: Text-to-image generators enable\ninterpretable interventions for robust representations.",
                "author": "Jianhao Yuan, Francesco Pinto, Adam Davies, Aarushi Gupta, and Philip Torr.",
                "venue": "arXiv preprint arXiv:2212.11237, 2022.",
                "url": null
            }
        },
        {
            "78": {
                "title": "Cutmix: Regularization strategy to train strong classifiers with\nlocalizable features.",
                "author": "Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and\nYoungjoon Yoo.",
                "venue": "In Proceedings of the IEEE/CVF international conference on\ncomputer vision, pages 6023\u20136032, 2019.",
                "url": null
            }
        },
        {
            "79": {
                "title": "Robust Learning Through Cross-Task Consistency.",
                "author": "Amir Zamir, Alexander Sax, Teresa Yeo, O\u011fuzhan Kar, Nikhil Cheerla, Rohan\nSuri, Zhangjie Cao, Jitendra Malik, and Leonidas Guibas.",
                "venue": "arXiv preprint arXiv:2006.04096, 2020.",
                "url": null
            }
        },
        {
            "80": {
                "title": "Taskonomy: Disentangling task transfer learning.",
                "author": "Amir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik,\nand Silvio Savarese.",
                "venue": "In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 3712\u20133722, 2018.",
                "url": null
            }
        },
        {
            "81": {
                "title": "mixup: Beyond empirical risk minimization.",
                "author": "Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz.",
                "venue": "arXiv preprint arXiv:1710.09412, 2017.",
                "url": null
            }
        },
        {
            "82": {
                "title": "Adding conditional control to text-to-image diffusion models.",
                "author": "Lvmin Zhang and Maneesh Agrawala.",
                "venue": "arXiv preprint arXiv:2302.05543, 2023a.",
                "url": null
            }
        },
        {
            "83": {
                "title": "Adding conditional control to text-to-image diffusion models.",
                "author": "Lvmin Zhang and Maneesh Agrawala.",
                "venue": "arXiv preprint arXiv:2302.05543, 2023b.",
                "url": null
            }
        },
        {
            "84": {
                "title": "Learning Deep Features for Scene Recognition using Places\nDatabase.",
                "author": "Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva.",
                "venue": "In Advances in Neural Information Processing Systems.\nCurran Associates, Inc., 2014.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.15309v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.3",
            "7.7",
            "8.4.2"
        ]
    },
    "research_context": {
        "paper_id": "2403.15309v1",
        "paper_title": "Controlled Training Data Generation with Diffusion Models",
        "research_background": "Motivation:\nThe primary motivation for this paper is to address the challenge of generating training datasets that can adapt to the evolving test distributions in real-world scenarios. Unlike static datasets that are costly to collect and remain unchanged, the authors focus on generating adaptive and cost-efficient datasets using diffusion generative models. The goal is to produce training data that generalizes well across different and novel test distributions, thereby enhancing the performance and robustness of deep learning models.\n\nResearch Problem:\nThe core research problem tackled in this paper is the creation of effective feedback mechanisms to condition the generation process of diffusion models. The existing methods lack an automatic feedback mechanism to refine the text prompts that condition image generation. Consequently, these methods use open-loop strategies with either language models or heuristics, which do not ensure the generation of the most useful training data. This paper proposes a solution by introducing two feedback mechanisms aimed at finding prompts that produce either adversarial examples to reveal model failure modes or examples that match a specific target distribution.\n\nRelevant Prior Work:\n1. The importance of data quality in training deep learning models has been well recognized in past studies [reference_b74, reference_b52, reference_b21].\n2. Diffusion generative models have been highlighted for their ability to produce realistic and diverse images, drawing attention to their potential for image synthesis [reference_b61, reference_b33, reference_b70, reference_b55, reference_b67, reference_b69].\n3. Recent works have started exploring the use of diffusion models to generate training data with promising results. These studies have employed text prompts to guide the generation process, aligning image-label pairs for supervised learning and adapting images to target distributions [reference_b68, reference_b18, reference_b28].\n4. Prior approaches to guiding the generation process have relied on open-loop methods using language models or heuristics, which lack an automatic refinement mechanism for the prompts [reference_b18, reference_b68].\n5. The concept of controlling input data to facilitate learning has been suggested as beneficial, particularly in the context of children learning with few examples [reference_b6, reference_b44].\n6. The use of CLIP embeddings for optimizing similarity metrics in image generation tasks has been noted in the literature [reference_b57].\n\nBy addressing these gaps and building upon the previous work, the paper aims to advance the field of controlled training data generation to better support deep learning model development and adaptation.",
        "methodology": "Controlled Training Data Generation with Diffusion Models\n\nMethodology: We begin this section by formalizing our problem setting and describing how diffusion models can be used to generate training data (Sec. 3.1  ###reference_###). We then introduce two feedback mechanisms to find prompts that are informed of the failure modes of a given model (Sec. 3.2  ###reference_###) and relevant to a given target distribution (Sec. 3.3  ###reference_###).\n\nKey components and innovations:\n\n1. **Problem Setting and Diffusion Models for Data Generation**:\n\n   - Formalizes the problem setting.\n   - Utilizes diffusion models to generate training data.\n   \n2. **Feedback Mechanisms**:\n\n   - **Failure Modes Feedback** (Sec. 3.2  ###reference_###): Introduces a feedback mechanism to identify prompts that are informed by the failure modes of a given model, thus helping in generating data that highlights and addresses the weaknesses of the model.\n   \n   - **Target Distribution Feedback** (Sec. 3.3  ###reference_###): Develops a feedback system to find prompts relevant to a given target distribution, ensuring that the generated data aligns well with the intended distribution and contributes to more targeted and effective model training.",
        "main_experiment_and_results": "**Main Experiment Setup:**\n\n- **Domains and Tasks:**\n  - **Domain Generalization:**\n    - **Dataset:** iWildCam\n    - **Task:** Camera trap animal classification\n\n  - **Bird Classification with Spurious Correlation:**\n    - **Dataset:** Waterbirds\n    - **Task:** Bird classification\n\n  - **Depth Estimation:**\n    - **Dataset:** Taskonomy\n    - **Task:** Depth estimation\n    - **Distribution Shifts:** \n      - Common Corruptions (CC)\n      - 3D Common Corruptions (3DCC)\n      - Cross dataset shift from the Replica dataset\n\n**Baselines:**\n\n(Not directly mentioned in the provided text, but typically includes standard training approaches and possibly prior state-of-the-art methods specific to domain generalization, handling spurious correlations, and depth estimation tasks.)\n\n**Evaluation Metrics:**\n\n(Not explicitly detailed, but likely includes accuracy for classification tasks and metrics relevant to depth estimation such as mean absolute error (MAE) or root mean square error (RMSE).)\n\n**Main Experimental Results:**\n\n(Not provided in the text but would typically summarize the performance improvement or the effectiveness of the proposed method compared to the baselines, across the mentioned tasks and datasets.)"
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To determine the optimal hyperparameters for the target distribution informed generation process in terms of CLIP guidance embedding methods and corresponding guidance losses.",
            "experiment_process": "The hyperparameters analyzed include: 1) using L2 norm or cosine similarity loss for guidance loss Lg, 2) incorporating guidance using text or image with CLIP or textual inversion (T.I). The experiments were conducted on the Waterbirds dataset, with a table presenting the accuracy from various combinations of guidance loss and integration methods.",
            "result_discussion": "The results indicate that using the L2 norm and text guidance provided the best performance, and this setting was subsequently used for the results presented in Fig. 3. Comparisons showed different performance based on guidance mechanisms due to variations in how generations are compared in the pixel space vs. CLIP embedding space.",
            "ablation_id": "2403.15309v1.No1"
        },
        {
            "research_objective": "To compare the qualitative and quantitative differences in generations produced by text guidance and image guidance on defocus blur and fog.",
            "experiment_process": "Qualitative and quantitative comparisons were made using defocus blur and fog generations from both text guidance and image guidance. Image-guided generations utilized sample (unlabelled) images from the corresponding target distribution, while text-guided generations relied on textual descriptions. The evaluations were conducted on target distributions extracted from the CC and 3DCC benchmarks, with qualitative results shown at the top of Fig. 18 and quantitative results at the bottom.",
            "result_discussion": "Qualitative observations revealed that text Guided Prompts produced more realistic corruptions (e.g., fog getting denser further away from the camera or around the floor). Conversely, image Guided Prompts applied a more uniform corruption across the image. Quantitatively, image guidance showed superior performance across target distributions when a sufficient amount of extra data was available.",
            "ablation_id": "2403.15309v1.No2"
        }
    ]
}