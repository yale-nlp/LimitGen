{
    "title": "Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy with Semantic Search and Hybrid Query-Based Retrievers",
    "abstract": "Retrieval-Augmented Generation (RAG) is a prevalent approach to infuse a private knowledge base of documents with Large Language Models (LLM) to build Generative Q&A (Question-Answering) systems. However, RAG accuracy becomes increasingly challenging as the corpus of documents scales up, with Retrievers playing an outsized role in the overall RAG accuracy by extracting the most relevant document from the corpus to provide context to the LLM. In this paper, we propose the \u2019Blended RAG\u2019 method of leveraging semantic search techniques, such as Dense Vector indexes and Sparse Encoder indexes, blended with hybrid query strategies. Our study achieves better retrieval results and sets new benchmarks for IR (Information Retrieval) datasets like NQ and TREC-COVID datasets. We further extend such a \u2019Blended Retriever\u2019 to the RAG system to demonstrate far superior results on Generative Q&A datasets like SQUAD, even surpassing fine-tuning performance.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "RAG represents an approach to text generation that is based not only on patterns learned during training but also on dynamically retrieved external knowledge. This method combines the creative flair of generative models with the encyclopedic recall of a search engine. The efficacy of the RAG system relies fundamentally on two components: the Retriever (R) and the Generator (G), the latter representing the size and type of LLM.\nThe language model can easily craft sentences, but it might not always have all the facts. This is where the Retriever (R) steps in, quickly sifting through vast amounts of documents to find relevant information that can be used to inform and enrich the language model's output. Think of the retriever as a researcher part of the AI, which feeds the contextually grounded text to generate knowledgeable answers to Generator (G). Without the retriever, RAG would be like a well-spoken individual who delivers irrelevant information."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "II Related Work",
            "text": "Search has been a focal point of research in information retrieval, with numerous studies exploring various methodologies. Historically, the BM25 (Best Match) algorithm, which uses similarity search, has been a cornerstone in this field, as explored by Robertson and Zaragoza (2009).[1  ###reference_b1###]. BM25 prioritizes documents according to their pertinence to a query, capitalizing on Term Frequency (TF), Inverse Document Frequency (IDF), and Document Length to compute a relevance score.\nDense vector models, particularly those employing KNN (k Nearest Neighbours) algorithms, have gained attention for their ability to capture deep semantic relationships in data. Studies by Johnson et al. (2019) demonstrated the efficacy of dense vector representations in large-scale search applications. The kinship between data entities (including the search query) is assessed by computing the vectorial proximity (via cosine similarity etc.). During search execution, the model discerns the \u2019k\u2019 vectors closest in resemblance to the query vector, hence returning the corresponding data entities as results. Their ability to transform text into vector space models, where semantic similarities can be quantitatively assessed, marks a significant advancement over traditional keyword-based approaches.[2  ###reference_b2###]\nOn the other hand, sparse encoder based vector models have also been explored for their precision in representing document semantics. The work of Zaharia et al. (2010) illustrates the potential of these models in efficiently handling high-dimensional data while maintaining interpretability, a challenge often faced in dense vector representations. In Sparse Encoder indexes the indexed documents, and the user\u2019s search query maps into an extensive array of associated terms derived from a vast corpus of training data to encapsulate relationships and contextual use of concepts. The resultant expanded terms for documents and queries are encoded into sparse vectors, an efficient data representation format when handling an extensive vocabulary.\n\nAI ethics in autonomous decision-making systems is critical as these systems increasingly influence real-world outcomes. Studies highlight concerns over bias, transparency, and accountability (Awad et al., 2018). The challenge lies in designing algorithms that align with ethical principles while maintaining autonomy, a topic of intense research and debate in the AI ethics community."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "II-A Limitations in the current RAG system",
            "text": "Most current retrieval methodologies employed in Retrieval-Augmented Generation (RAG) pipelines rely on keyword and similarity-based searches, which can restrict the RAG system\u2019s overall accuracy. Table 1 provides a summary of the current benchmarks for retriever accuracy.\n\n| Dataset    | Benchmark Metrics | NDCG@10 | p@20 | F1  |\n|------------|-------------------|---------|------|-----|\n| NQDataset  | P@20              | 0.633   | 86   | 79.6|\n| Trec Covid | NDCG@10           | 80.4    |      |     |\n| HotpotQA   | F1, EM            | 0.85    |      |     |\n\nWhile most of the prior efforts in improving RAG accuracy focus on the G part, by tweaking LLM prompts, tuning, etc., [9] they have limited impact on the overall accuracy of the RAG system, since if the R part feeds irrelevant context, the answer will be inaccurate. Furthermore, most retrieval methodologies employed in RAG pipelines rely on keyword and similarity-based searches, which can restrict the system's overall accuracy. Finding the best search method for RAG is still an emerging area of research. The goal of this study is to enhance retriever and RAG accuracy by incorporating Semantic Search-Based Retrievers and Hybrid Search Queries.\n\nThe ethical considerations of AI in autonomous decision-making systems represent an expanding frontier in technological governance. This includes concerns over accountability, the biases in machine learning algorithms, and the transparency of decision processes. Addressing these issues is imperative to ensure responsible deployment and maintain trust in AI systems."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "III Blended Retrievers",
            "text": "For RAG systems, we explored three distinct search strategies: keyword-based similarity search, dense vector-based, and semantic-based sparse encoders, integrating these to formulate hybrid queries. Unlike conventional keyword matching, semantic search delves into the nuances of a user\u2019s query, deciphering context and intent. This study systematically evaluates an array of search techniques across three primary indices: BM25[3  ###reference_b3###] for keyword-based, KNN[4  ###reference_b4###] for vector-based, and Elastic Learned Sparse Encoder (ELSER) for sparse encoder-based semantic search.\n###figure_1### BM25 Index: The BM25 index is adept at employing full-text search capabilities enhanced by fuzzy matching techniques, laying the groundwork for more sophisticated query operations.\nDense Vector Index: We construct a dense vector index empowered by sentence transformers. It identifies the proximity of vector representations derived from document and query content.\nSparse Encoder Index: The Sparse EncodeR Retriever Model index is an amalgam of semantic understanding and similarity-based retrieval to encapsulate the nuanced relationships between terms, thereby capturing a more authentic representation of user intent and document relevance."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "III-A Methodology",
            "text": "Our methodology unfolds in a sequence of progressive steps, commencing with the elementary match query within the BM25 index. We then escalate to hybrid queries that amalgamate diverse search techniques across multiple fields, leveraging the multi-match query within the Sparse Encoder-Based Index. This method proves invaluable when the exact location of the query text within the document corpus is indeterminate, hence ensuring a comprehensive match retrieval.\nThe multi-match queries are categorized as follows:\nCross Fields: Targets concurrence across multiple fields\nMost Fields: Seeks text representation through different lenses across various fields.\nBest Fields: Pursues the aggregation of words within a singular field.\nPhrase Prefix: Operates similarly to Best Fields but prioritizes phrases over keywords.\nAfter initial match queries, we incorporate dense vector (KNN) and sparse encoder indices, each with their bespoke hybrid queries. This strategic approach synthesizes the strengths of each index, channeling them towards the unified goal of refining retrieval accuracy within our RAG system. We calculate the top-k retrieval accuracy metric to distill the essence of each query type.\nIn Figure 1, we introduce a scheme designed to create Blended Retrievers by blending semantic search with hybrid queries."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "III-B Constructing RAG System",
            "text": "From the plethora of possible permutations, a select sextet (top 6) of hybrid queries\u2014those exhibiting paramount retrieval efficacy\u2014were chosen for further scrutiny. These queries were then subjected to rigorous evaluation across the benchmark datasets to ascertain the precision of the retrieval component within RAG. The sextet queries represent the culmination of retriever experimentation, embodying the synthesis of our finest query strategies aligned with various index types. The six blended queries are then fed to generative question-answering systems. This process finds the best retrievers to feed to the Generator of RAG, given the exponential growth in the number of potential query combinations stemming from the integration with distinct index types.\nThe intricacies of constructing an effective RAG system are multi-fold, particularly when source datasets have diverse and complex landscapes. We undertook a comprehensive evaluation of a myriad of hybrid query formulations, scrutinizing their performance across benchmark datasets, including the Natural Questions (NQ), TREC-COVID, Stanford Question Answering Dataset (SqUAD), and HotPotQA."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "IV Experimentation for Retriever Evaluation",
            "text": "We used top-10 retrieval accuracy to narrow down the six best types of blended retrievers (index + hybrid query) for comparison for each benchmark dataset."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "IV-A Retriever Benchmarking",
            "text": "Now that we have identified the best set of combinations of Index + Query types, we will use these sextet queries on IR datasets for benchmarking using NDCG@10[8  ###reference_b8###] scores (Normalised Discounted Cumulative Gain metric)."
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "IV-A1 NQ dataset benchmarking",
            "text": "###figure_2### The results for NDCG@10 using sextet queries and the current benchmark on the NQ dataset are shown in the chart Figure 7. Our pipeline provides the best NDCG@10 score of 0.67, which is 5.8% higher than the current benchmark score of 0.633 achieved by the monoT5-3B model. Table II shows that all semantic search-based hybrid queries outperform the current benchmark score, which indicates that our hybrid queries are a better candidate for developing the RAG pipeline.\nDataset\nModel/Pipeline\nNDCG@10\nTrec-covid\nCOCO-DR Large\n0.804\nTrec-covid\nBlended RAG\n0.87\nNQ dataset\nmonoT5-3B\n0.633\nNQ dataset\nBlended RAG\n0.67"
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "IV-A2 TREC-Covid Dataset Benchmarking",
            "text": "###figure_3### In our research, the suite of hybrid queries devised has demonstrably exceeded the current benchmark of 0.80 NDCG@10 score, signaling their superior candidature for the RAG pipeline. Figure 7 shows the results for NDCG@10 using sextet queries. Blended Retrievers achieved an NDCG@10 score of 0.87, which marks an 8.2% increment over the benchmark score of 0.804 established by the COCO-DR Large model (Table II)."
        },
        {
            "section_id": "4.1.3",
            "parent_section_id": "4.1",
            "section_name": "IV-A3 SqUAD Dataset Benchmarking",
            "text": "The SqUAD (Stanford Question Answering Dataset) [9  ###reference_b9###] is not an IR dataset, but we evaluated the retrieval accuracy of the SquAD dataset for consistency. Firstly, we created a corpus from the SqUAD dataset using the title and context fields in the dataset. Then, we indexed the corpus using BM25, dense vector, and Sparse Encoder. The top-k (k=5,10, and 20) retrieval accuracy results for the SqUAD dataset are calculated. Table III illustrates that for SQuAD, dense vector (KNN)-based semantic searches achieve higher accuracy than sparse vector-based semantic searches and traditional similarity-based searches, particularly for top-k retrieval performance with k values of 5, 10, and 20. (See Appendix for more details)\n###table_1### SqUAD\nBM25+MQ\nBM25+BF\nKNN+MQ\nKNN+BF\nSPARSE ENCODER+MQ\nSPARSE ENCODER+BF\nTop-5\n91.5\n91.52\n94.86\n94.89\n90.7\n90.7\nTop-10\n94.43\n94.49\n97.43\n97.43\n94.13\n94.16\nTop-20\n96.3\n96.36\n98.57\n98.58\n96.49\n96.52"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "IV-B Summary of Retriever Evaluation",
            "text": "We evaluated the retrieval accuracy using our approach, quantified by Top-k metrics where , across NQ, TREC-COVID, SQUAD, and CoQA datasets. This synopsis demonstrates the capability of our Blended Retrieval methodology within diverse informational contexts. Key observations are\nEnhanced retrieval accuracy is exhibited in all datasets except for CoQA[10  ###reference_b10###]. This enhancement is attributable to the capability of our hybrid queries to effectively utilize available metadata to source the most pertinent results.\nImplementing dense vector-based (KNN) semantic search results in a marked improvement over keyword-based search approaches.\nEmploying semantic search-based hybrid queries realizes better retrieval precision compared to all conventional keyword-based or vector-based searches.\nFurthermore, it is discernible that the Sparse Encoder-based semantic search, when amalgamated with the \u2019Best Fields\u2019 hybrid query, often provides superior results than any other method.\n###figure_4###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "RAG Experimentation",
            "text": "From the retriever evaluation experiments, we know the best retriever, i.e., the best combination of indices + query. In this section, we extend this knowledge to evaluate the RAG pipeline. To avoid the effect of LLM size or type, we perform all experiments using FLAN-T5-XXL."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "RAG Evaluation on the SqUAD Dataset",
            "text": "SqUAD is a commonly bench-marked dataset for RAG systems or Generative Q&A using LLMs. Our study juxtaposes three variations of the RAG pipeline from prior work using the evaluation metrics of Exact Match (EM) and F1 scores to gauge the accuracy of answer generation, as well as Top-5 and Top-10 for retrieval accuracy.\nRAG-original [11  ###reference_b11###]: This variant, a model fine-tuned on the Natural Questions dataset, has been appraised without domain-specific adaptation.\nRAG-end2end [11  ###reference_b11###]: As an extension of RAG-original, this model undergoes additional fine-tuning, tailored for domain adaptation to the SQuAD.\nBlended RAG: Distinctively, our Blended RAG variant has not undergone training on the SQuAD dataset or any related corpora. It harnesses an optimized amalgamation of field selections and hybrid query formulations with semantic indices to feed LLMs to render the most precise responses possible.\n###table_2### Model/Pipeline\nEM\nF1\nTop-5\nTop-20\nRAG-original\n28.12\n39.42\n59.64\n72.38\nRAG-end2end\n40.02\n52.63\n75.79\n85.57\nBlended RAG\n57.63\n68.4\n94.89\n98.58\nConsequently, as shown in Table IV, our Blended RAG showcases enhanced performance for Generative Q&A with F1 scores higher by 50%, even without dataset-specific fine-tuning. This characteristic is particularly advantageous for large enterprise datasets, where fine-tuning may be impractical or unfeasible, underscoring this research\u2019s principal application."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "RAG Evaluation on the NQ Dataset",
            "text": "Natual Questions (NQ) is another commonly studied dataset for RAG. The Blended RAG pipeline, utilizing zero-shot learning, was evaluated to ascertain its efficacy against other non-fine-tuned models. The assessment focused on the following metrics: Exact Match (EM), F1 Score, and retrieval accuracy (Top-5 and Top-20) in Table V.\n###table_3### Model/Pipeline\nEM\nF1\nTop-5\nTop-20\nGLaM (Oneshot)[12  ###reference_b12###]\n26.3\nGLaM (Zeroshot)[12  ###reference_b12###]\n24.7\nPaLM540B (Oneshot)[13  ###reference_b13###]\n29.3\nBlended RAG (Zeroshot)\n42.63\n53.96\n88.22\n88.88\nBlended RAG (Zero-shot): Demonstrated superior performance with an EM of 42.63, improving the prior benchmark by 35%."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "VI Discussion",
            "text": "While RAG is a commonly used approach in the industry, we realized during the course of this study that various challenges still exist, like there are no standard datasets on which both R (Retriever) and RAG benchmarks are available. Retriever is often studied as a separate problem in the IR domain, while RAG is studied in the LLM domain. We thus attempted to bring synergy between the two domains with this work. In this section, we share some learning on limitations and appropriate use of this method."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "VI-A Trade-off between Sparse and Dense Vector Indices",
            "text": "The HotPotQA corpus presents substantial computational challenges with 5M documents, generating a dense vector index to an approximate size of 50GB, a factor that significantly hampers processing efficiency. Dense vector indexing, characterized by its rapid indexing capability, is offset by a relatively sluggish querying performance. Conversely, sparse vector indexing, despite its slower indexing process, offers expeditious querying advantages. Furthermore, a stark contrast in storage requirements is observed; for instance, the sparse vector index of the HotPotQA corpus occupied a mere 10.5GB as opposed to the 50GB required for the dense vector equivalent.\nIn such cases, we recommend sparse encoder indexes. Furthermore, for enterprises with this volume, we found it better to use multi-tenancy with federated search queries."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "VI-B Blended Retrievers without Metadata",
            "text": "When datasets are enriched with metadata or other relevant informational facets, they improve the efficacy of blended retrievers. Conversely, for datasets devoid of metadata, such as CoQA, it is not as impressive.\n###figure_5### The absence of metadata in the CoQA dataset resulted in hybrid queries offering no improvement over basic queries. This limitation underscores the critical role of metadata in enhancing the efficacy of complex query structures. However, Sparse Encoder-based semantic searches still yield the most favorable outcomes than traditional methods.\nAdditionally, we would like to note that while NDCG@10 scores for Retriever and F1,EM scores for RAG are commonly used metrics, we found them to be poor proxies of Generative Q&A systems for human alignment. Better metrics to evaluate the RAG system is a key area of future work."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "VII Conclusion",
            "text": "Blended RAG pipeline is highly effective across multiple datasets despite not being specifically trained on them. Notably, this approach does not necessitate exemplars for prompt engineering which are often required in few-shot learning, indicating a robust generalization capability within the zero-shot paradigm. This study demonstrated:\nOptimization of R with Blended Search: Incorporating Semantic Search, specifically Sparse Encoder indices coupled with \u2019Best Fields\u2019 queries, has emerged as the superior construct across all, setting a new benchmark of 87% for Retriever Accuracy on TREC-COVID.\nEnhancement of RAG via Blended Retrievers: The significant amplification in retrieval accuracy is particularly pronounced for the overall evaluation of the RAG pipeline, surpassing prior benchmarks on fine-tuned sets by a wide margin. Blended RAG sets a new benchmark at 68% F1 Score on SQUAD and 42% EM Score on NQ dataset; for non-tuned Q&A systems.\nThe empirical findings endorse the potency of Blended Retrievers in refining RAG systems beyond focusing on LLM size & type, getting better results with relatively smaller LLM and thus setting a foundation for more intelligent and contextually aware Generative Q&A systems."
        }
    ],
    "url": "http://arxiv.org/html/2404.07220v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.1.1",
            "4.1.2",
            "4.1.3",
            "4.2",
            "5",
            "5.1",
            "5.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "4.1",
            "4.1.1",
            "4.1.2",
            "4.1.3",
            "5.1",
            "5.2"
        ]
    },
    "research_context": {
        "paper_id": "2404.07220v1",
        "paper_title": "Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy with Semantic Search and Hybrid Query-Based Retrievers",
        "research_background": "### Motivation\nThe motivation behind the paper is to enhance the accuracy of Retriever-Augmented Generation (RAG) systems. The RAG system enhances text generation beyond the capabilities of generative models alone by incorporating external knowledge dynamically retrieved from vast amounts of documents. The primary drive is to ensure the generative model produces more factually correct and contextually relevant information. The paper aims to combine the generative prowess of language models with the extensive factual repository available through retrieval mechanisms, thereby overcoming the limitations of generative models, which may not always possess all necessary facts.\n\n### Research Problem\nThe research problem the paper addresses is to improve the accuracy of RAG systems, which heavily relies on the effectiveness of its two core components: the Retriever (R) and the Generator (G). The challenge lies in developing a robust retriever that can efficiently and accurately sift through large datasets to provide contextually relevant information that enhances the output of generative models. Essentially, the question is how to optimize the retrieval mechanism to better inform the language model, thereby improving the overall performance and reliability of RAG systems.\n\n### Relevant Prior Work\nThe paper builds on prior work in the fields of generative models and information retrieval. Traditional generative models are proficient in crafting coherent and fluent text but often fall short in factual accuracy because they rely solely on patterns learned during training. The concept of augmentation through retrieval is crucial, with early works highlighting the potential of integrating external knowledge into language models. Specifically, the RAG approach represents a significant step towards combining retrieval-based methods with generative capabilities, although it continues to face challenges in ensuring the accuracy and relevance of retrieved information. Prior works have also touched upon the importance of the retriever's design and effectiveness, which this paper seeks to advance further by proposing enhancements in semantic search and hybrid query-based retrieval methods.",
        "methodology": "**Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy with Semantic Search and Hybrid Query-Based Retrievers**\n\n**Methodology**: The proposed model, Blended RAG, seeks to enhance the accuracy of Retriever-Augmented Generation (RAG) systems through a combination of three distinct search strategies: keyword-based similarity search, dense vector-based retrieval, and semantic-based sparse encoder-based retrieval. By integrating these strategies, hybrid queries are formed to leverage the strengths of each approach.\n\nThe key components of the methodology include:\n\n1. **Keyword-Based Similarity Search**: This approach utilizes the BM25 index to perform full-text search operations. BM25 enhances traditional keyword matching by incorporating fuzzy matching techniques, which allow for a more sophisticated analysis of query operations.\n\n2. **Dense Vector-Based Retrieval**: This method constructs a dense vector index using sentence transformers. Dense vector retrieval measures the proximity of vector representations derived from the content of documents and queries. This technique aims to capture the semantic similarity between the user's query and relevant documents.\n\n3. **Semantic-Based Sparse Encoders**: The sparse encoder, specifically the Elastic Learned Sparse Encoder (ELSER), is employed to create a sparse representation of the semantic content in documents. This index merges semantic comprehension with similarity-based retrieval, capturing the nuanced relationships between terms to reflect user intent and document relevance more accurately.\n\nThe study systematically evaluates these search strategies across three primary indices:\n\n- **BM25 Index**: A keyword-based index that uses full-text search capabilities and fuzzy matching enhancements.\n- **Dense Vector Index**: An index built using dense vector embeddings from sentence transformers to measure semantic proximity.\n- **Sparse Encoder Index**: An index generated by the Sparse EncodeR Retriever Model, which combines semantic understanding with similarity-based retrieval for more precise user intent and document relevance interpretation.\n\nBy integrating these indices into hybrid queries, Blended RAG aims to improve the accuracy and effectiveness of the retrieval process in RAG systems, ultimately enhancing the overall model performance.",
        "main_experiment_and_results": "**Main Experiment Setup:**\n\n**Datasets:**\n- The paper uses benchmark datasets to evaluate the performance of the blended retrievers. However, specific datasets are not listed directly in the provided snippet.\n\n**Baselines:**\n- Blended retrievers that combine an index-based retriever with a hybrid query-based retriever are the focus.\n- Six different types of blended retrievers are evaluated.\n\n**Evaluation Metrics:**\n- Top-10 retrieval accuracy is used as the primary metric for narrowing down the best types of blended retrievers.\n\n**Main Experimental Results:**\n- The results include a comparison of the top-10 retrieval accuracy across the six best types of blended retrievers for each benchmark dataset.\n- Detailed results for specific performance and comparisons to other methods or baselines are not included in the provided text."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Evaluate the retrieval accuracy of different semantic search-based hybrid queries for the NQ dataset to determine the best candidates for developing the RAG pipeline.",
            "experiment_process": "Six types of blended retrievers (index + hybrid query) narrowed down based on top-10 retrieval accuracy were compared. NDCG@10 (Normalized Discounted Cumulative Gain) scores were used for benchmarking on the NQ dataset.",
            "result_discussion": "Blended RAG achieves an NDCG@10 score of 0.67, 5.8% higher than the current benchmark score of 0.633 by the monoT5-3B model. All semantic search-based hybrid queries outperform the current benchmark score.",
            "ablation_id": "2404.07220v1.No1"
        },
        {
            "research_objective": "Benchmark the retrieval accuracy of hybrid queries on the TREC-COVID dataset to validate their efficacy for the RAG pipeline.",
            "experiment_process": "Hybrid queries devised for the TREC-COVID dataset were benchmarked using NDCG@10 scores. Comparisons were made against the COCO-DR Large model.",
            "result_discussion": "Blended Retrievers achieved an NDCG@10 score of 0.87, which is an 8.2% improvement over the benchmark score of 0.804 established by the COCO-DR Large model.",
            "ablation_id": "2404.07220v1.No2"
        },
        {
            "research_objective": "Assess the retrieval performance of different indexing methods on the SQuAD dataset to identify the most effective method for generative Q&A.",
            "experiment_process": "A corpus was created from the SQuAD dataset using title and context fields. The corpus was indexed using BM25, dense vector, and Sparse Encoder methods. Top-k retrieval accuracy (k=5,10,20) was calculated and compared across these indexing methods.",
            "result_discussion": "Dense vector (KNN)-based semantic searches showed higher accuracy than sparse vector-based and traditional similarity-based searches, particularly for top-k retrieval performance with k values of 5, 10, and 20.",
            "ablation_id": "2404.07220v1.No3"
        },
        {
            "research_objective": "Evaluate the effectiveness of the Blended RAG pipeline for generative Q&A on the SQuAD dataset compared to previously established RAG variants.",
            "experiment_process": "Compared RAG-original, RAG-end2end, and Blended RAG pipelines using Exact Match (EM) and F1 scores for answer generation accuracy, along with top-5 and top-10 retrieval accuracy. RAG-original was fine-tuned on the Natural Questions dataset, RAG-end2end was additionally fine-tuned for domain adaptation to the SQuAD dataset, and Blended RAG used field selections and hybrid query formulations without SQuAD-specific training.",
            "result_discussion": "Blended RAG showed a 50% higher F1 score compared to other variants without dataset-specific fine-tuning. This result highlights the potential utility of Blended RAG in scenarios where fine-tuning is impractical.",
            "ablation_id": "2404.07220v1.No4"
        },
        {
            "research_objective": "Assess the zero-shot learning performance of the Blended RAG pipeline on the Natural Questions (NQ) dataset compared to other non-fine-tuned models.",
            "experiment_process": "Blended RAG was evaluated using Exact Match (EM), F1 Score, and top-5 and top-20 retrieval accuracy metrics on the NQ dataset. Comparisons were made with GLaM and PaLM540B (One-shot and Zero-shot) models.",
            "result_discussion": "Blended RAG achieved a superior EM score of 42.63, indicating a 35% improvement over previous benchmarks.",
            "ablation_id": "2404.07220v1.No5"
        }
    ]
}