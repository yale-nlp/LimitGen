{
    "title": "Will the Real Linda Please Stand up\u2026to Large Language Models? Examining the Representativeness Heuristic in LLMs",
    "abstract": "Although large language models (LLMs) have demonstrated remarkable proficiency in \u201cunderstanding\u201c text and generating human-like text, they may exhibit biases acquired from training data in doing so.\nSpecifically, LLMs may be susceptible to a common cognitive trap in human decision-making called the representativeness heuristic.\nThis is a concept in psychology that refers to judging the likelihood of an event based on how closely it resembles a well-known prototype or typical example versus considering broader facts or statistical evidence.\nThis work investigates the impact of the representativeness heuristic on LLM reasoning.\nWe created ReHeAT (Representativeness Heuristic AI Testing), a dataset containing a series of problems spanning six common types of representativeness heuristics.\nExperiments reveal that four LLMs applied to ReHeAT all exhibited representativeness heuristic biases.\nWe further identify that the model\u2019s reasoning steps are often incorrectly based on a stereotype rather than the problem\u2019s description.\nInterestingly, the performance improves when adding a hint in the prompt to remind the model of using its knowledge.\nThis suggests the uniqueness of the representativeness heuristic compared to traditional biases.\nIt can occur even when LLMs possess the correct knowledge while failing in a cognitive trap.\nThis highlights the importance of future research focusing on the representativeness heuristic in model reasoning and decision-making and on developing solutions to address it.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "\u201cLinda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice and also participated in anti-nuclear demonstrations.\u201c\nWe have seven statements below: [1] Linda is a teacher in elementary school. [2] Linda works in a bookstore and takes Yoga classes. [3] Linda is a psychiatric social worker. [4] Linda is a member of the League of Women Voters. [5] Linda is a bank teller. [6] Linda is an insurance salesperson. [7] Linda is a bank teller and is active in the feminist movement.\nQuestion: Rank the seven statements associated with each description by the degree to which Linda resembles the typical member of that class.\n- Tversky & Kahneman (1983  ###reference_b29###)\nThis is one experiment known as the \u201cLinda problem\u201d devised by Tversky and Kahneman; it was demonstrated that people are influenced by specific descriptions, such as being deeply concerned with issues of discrimination and social justice and also participating in anti-nuclear demonstrations to rank [7] higher than [5]. However, [7] combines [5] and an additional event. This means that, from a statistical perspective, [5] is more likely to occur than [7] because it is more general and less restrictive.\nKahneman & Tversky (1973  ###reference_b13###) introduced this phenomenon as the \u201crepresentativeness heuristic,\u201d which involves estimating the likelihood of an event by comparing it to an existing prototype in our minds.\nIt offers a convenient shortcut in decision-making by aligning with intuitive thinking, leading people to rely on it frequently.\nGenerally, this heuristic is quite beneficial, as it simplifies complex judgments.\nHowever, it is important to recognize that it can also result in significant errors, given that people are prone to assessing the likelihood of an object\u2019s category membership based on superficial similarities while neglecting actual statistical evidence.\nFor example, people categorize individuals based on their looks, actions, or background description from language,\nleading to skewed perceptions and decisions. This stereotyping phenomenon is widespread among humans (Spencer et al., 2016  ###reference_b25###).\nLLMs trained on real-world data and instructed to emulate human behavior may capture the representativeness heuristic.\nPrevious work has mainly focused on the biases within training data (Bender et al., 2021  ###reference_b4###; Bolukbasi et al., 2016  ###reference_b5###; Garg et al., 2018  ###reference_b9###; Sheng et al., 2019  ###reference_b24###; Zhao et al., 2019  ###reference_b35###).\nThese biases often stem from data distributions that do not reflect the desired proportions that would drive unbiased decision-making.\nIn contrast, the representativeness heuristic represents a type of cognitive bias that has yet been thoroughly investigated in LLMs.\nIt is unique in leading the model to make mistakes even when it possesses the knowledge necessary to solve the problem.\nAs illustrated in Figure 1  ###reference_###, the model is able to answer the Statistical Prototype Question (SPQ), yet it tends to fail to answer the Representativeness Heuristic Question (RHQ).\nThe SPQ and RHQ are intrinsically equivalent, where SPQ is expressed statistically, and RHQ expresses the same statistical logic in a scenario.\nThis indicates that LLMs can engage in erroneous cognitive reasoning even with the knowledge of statistical probability.\nInterestingly, providing a hint can prompt the model to use the knowledge to make a correct prediction.\nThis indicates that the representativeness heuristic can block the model from following a correct reasoning path; instead, it relies on the cognitive shortcut to make a trivial decision.\n###figure_1### To investigate the representativeness heuristic in LLMs, we construct a dataset111We will release the dataset upon the completion of the review process to facilitate future research. , ReHeAT (Representativeness Heuristic AI Testing), which contains 202 RHQs that span six types of representativeness heuristics\u2014Base Rate Fallacy, Conjunction Fallacy, Disjunction Fallacy, Insensitivity to Sample Size, Misconceptions of Chance, and Regression Fallacy.\nThe questions we designed are adapted from those used in prior investigations into heuristics within the field of psychology (Bar-Hillel & Neter, 1993  ###reference_b2###; Kahneman & Tversky, 1973  ###reference_b13###; Kahneman et al., 1982  ###reference_b14###; Tversky & Kahneman, 1974  ###reference_b28###; 1983  ###reference_b29###).\nTo the best of our knowledge, our dataset is the first to offer extensive and comprehensive coverage of RHQs, enabling exploration of LLMs\u2019 capabilities in countering the cognitive bias.\nWe evaluate four LLMs: GPT-3.5 (Ouyang et al., 2022  ###reference_b22###), GPT-4 (OpenAI, 2023  ###reference_b21###), PaLM 2 (Anil et al., 2023  ###reference_b1###), and LLaMA 2 (Touvron et al., 2023  ###reference_b27###), on ReHeAT using different prompts.\nOur findings indicate that these LLMs exhibit behaviors that closely mirror human heuristic behavior.\nAdditionally, advanced prompting techniques such as chain of thought (CoT) (Wei et al., 2022  ###reference_b31###), in-context learning (Brown et al., 2020  ###reference_b6###), and self-consistency (Huang et al., 2022  ###reference_b11###) prompting, offer marginal improvements.\nNevertheless, when explicitly prompted to recall its knowledge, the model shows an improvement in performance.\nThis underscores the significance for future research to address the representativeness heuristic, guiding LLMs toward correct reasoning and decision-making."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Social biases in natural language processing (NLP) systems and related data have been studied with respect to their fairness, inclusivity, and accuracy (Hutchinson et al., 2020 ###reference_b12###; Maass, 1999 ###reference_b19###; Zhao et al., 2018 ###reference_b34###). For example, Bolukbasi et al. (2016 ###reference_b5###); Garg et al. (2018 ###reference_b9###) are among the pioneers in demonstrating gender biases embedded in word embeddings, showing how these representations could reinforce stereotypical associations. Caliskan et al. (2017 ###reference_b7###) concludes standard machine learning methods for NLP could acquire societal biases from textual data. Some works expanded the understanding of bias sources in NLP systems, including those from data collection (Bender & Friedman, 2018 ###reference_b3###), annotation processes (Gebru et al., 2018 ###reference_b10###), and model architecture choices (Zhao et al., 2017 ###reference_b33###). To date, numerous efforts have been made to mitigate social biases in systems through a variety of methods, including data augmentation (Lu et al., 2018 ###reference_b18###), changes in model architecture (Liang et al., 2020 ###reference_b16###), and training objectives (Liu et al., 2021 ###reference_b17###; Romanov et al., 2019 ###reference_b23###). In a similar vein, although recent advancements in LLMs are exciting, researchers are concerned about whether LLMs inherit social biases from the trillions of tokens they have been trained on. Weidinger et al. (2022 ###reference_b32###) gives a comprehensive taxonomy of social risks within LLMs. Although the research community has documented numerous social biases in LLMs (Ferrara, 2023 ###reference_b8###; Mei et al., 2023 ###reference_b20###), few LLM researchers have examined these biases from the human mind\u2019s perspective. In this work, we study the bias issue in LLMs from a new angle, representative heuristics, a concept originating in psychology (Kahneman & Tversky, 1973 ###reference_b13###; Kahneman et al., 1982 ###reference_b14###; Tversky & Kahneman, 1974 ###reference_b28###; 1983 ###reference_b29###). Contrary to the very recent work of Suri et al. (2024 ###reference_b26###), which explores the decision-making heuristics of GPT-3.5, our current research emphasizes a more in-depth and comprehensive exploration of the application of the representativeness heuristic within LLMs. For example, we have compiled a dataset encompassing a much larger number of questions (a total of 202 compared to 9) and we have benchmarked performance across a wider range of LLMs based on diverse prompting strategies.\n\nAI in creative writing has emerged as an intriguing domain, combining computational creativity with language models (Thakkar, 2021 ###reference_b27###). Systems like GPT-3 are being used to draft narratives and poetry, exploring human-like expression and creativity (Clark, 2022 ###reference_b6###). However, the challenge of aligning AI-generated content with human creativity and ethics persists (Johnson & Vera, 2023 ###reference_b11###)."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Representativeness Heuristic",
            "text": "Drawing from prior research, we organize our research around a framework that categorizes the representativeness heuristic into six types (Kahneman & Tversky, 1973  ###reference_b13###; Kahneman et al., 1982  ###reference_b14###; Tversky & Kahneman, 1974  ###reference_b28###; 1983  ###reference_b29###).\nThese categories vary in their fundamental logical approach and their impact on decision-making processes.\n###figure_2### ###figure_3### ###figure_4### ###figure_5### ###figure_6### ###figure_7### Base Rate Fallacy occurs when individuals overlook or insufficiently account for the population base rate of an event (i.e., its overall prevalence or frequency) in favor of specific instances or recent information. Figure 2(a)  ###reference_sf1### presents an example where  represents the proportion of individuals with a symptom (all blue points within the large circle),  denotes the rate of illness (all points within the small circle), and  indicates the proportion of the sick who have the symptom (all blue points within the small circle).\n represents the illness rate among those with the symptom.\nMost people would assume that because  is high,  would also be high.\nYet, according to Bayes\u2019 theorem,\nmeaning  is greatly influenced by the base rates of  and , showing the importance of considering general prevalence in evaluating specific probabilities.\nAn example question can be seen in Table 17  ###reference_### of the Appendix.\nConjunction Fallacy occurs when people mistakenly believe that the chance of two events happening together is greater than the chance of either event happening alone.\nSee Figure 2(b)  ###reference_sf2###, consider the example at the beginning of the article.\n represents the probability that the person is a bank teller (all points within the small circle, less relevant to the description).\n is the probability that the person is active in the feminist movement (all points within the large circle, more relevant to the description).\n the probability that the person is both a bank teller and active in the feminist movement (all purple points).\n will always be no larger than  and , no matter which one is closer to the description.\nAn example question can be seen in Table 18  ###reference_### of the Appendix.\nDisjunction Fallacy occurs when people incorrectly judge the probability of a broader category to be smaller than one of its specific components.\nIn Figure 2(c)  ###reference_sf3###, we can imagine the small circle representing ice cream and the large circle representing frozen food.\nSince ice cream is a subset of frozen food, the probability of frozen food is higher than ice cream.\nHowever, when people talk about summer refreshments, they often think of ice cream rather than frozen food.\nThis choice illustrates a common tendency to differentiate specific items from their general classifications based on contextual associations.\nAn example question can be seen in Table 19  ###reference_### of the Appendix.\nInsensitivity to Sample Size occurs when people underestimate how important sample size is in data evaluation, potentially leading to incorrect conclusions.\nFigure 2(d)  ###reference_sf4### presents an example where a small group (small circle) and a large group (large circle) both have 50% blue dots and 50% red dots (2:2 and 12:12).\nIf we add one red dot to each sample, then the ratio of blue to red becomes 40% and 60% (2:3) in the small group, versus 48% and 52% (12:13) in the large group.\nIt should be recognized that smaller groups are more prone to skewed outcomes because even small changes have a larger impact on the overall dynamics of a small group.\nAn example question can be seen in Table 20  ###reference_### of the Appendix.\nMisconceptions of Chance involve misunderstanding how randomness works, especially thinking that past outcomes will affect future outcomes, in cases where the outcomes are in fact independent. For example,\nFigure 2(e)  ###reference_sf5### presents a dice-rolling example.\nPeople wrongly believe that if a specific outcome has occurred frequently, it is less likely to happen again shortly, or vice versa if it has occurred rarely.\nHowever, in truly random events, such as rolling a fair die, the probability of any given outcome (1-6) remains constant at , unaffected by the sequence of previous results.\nAn example question can be seen in Table 21  ###reference_### of the Appendix.\nRegression Fallacy occurs when individuals overlook the natural tendency for extreme situations to move back towards the average (regress to the mean) and instead, erroneously assign this regression to a particular cause, see Figure 2(f)  ###reference_sf6###.\nFor instance, if an athlete shows a lackluster performance following a perfect game, it might be incorrectly ascribed to external factors, neglecting the likelihood of natural variance in performance.\nAn example question can be seen in Table 22  ###reference_### of the Appendix."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Dataset and Experimental Setup",
            "text": "We begin by constructing a dataset intended to reflect a diverse array of questions and scenarios (\u00a74.1  ###reference_###).\nFollowing this, we introduce the models and prompting strategies (\u00a74.2  ###reference_###), along with the evaluation methods (\u00a74.3  ###reference_###)."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Data Collection",
            "text": "Our main resource for creating test questions on representativeness heuristic comes from the academic studies by Kahneman and Tversky (Kahneman & Tversky, 1973  ###reference_b13###; Kahneman et al., 1982  ###reference_b14###; Tversky & Kahneman, 1974  ###reference_b28###; 1983  ###reference_b29###).\nTheir work introduced various question types and insights into the design of cognitive heuristic measures.\nBuilding on their foundation, we design questions that extensively explore the representativeness heuristic in LLMs.\nSpecifically, our test set contains 49 questions drawn directly from previous research and 153 new questions that have been carefully adapted.\nTo support the usefulness of our adapted items, we pay special attention to retaining the essence of the representativeness heuristic being tested, while changing the situation and context of the original items.\nThe total 202 examples are all in English.\nAs shown in Table 1  ###reference_###, each example is composed of the following characteristics:\nQuery: Provide the question\u2019s description and requirements.\nType: Indicate the question\u2019s category, such as Disjunction Fallacy.\nFeature: Highlight unique aspects, such as choose one, or the question\u2019s source, such as original problem in Bar-Hillel & Neter, 1993.\nGround Truth: Represents the standard answer.\nHuman Response: Document the outcomes of human responses from academic research when available, or denoting N/A otherwise."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Models and Prompting Strategies",
            "text": "We investigate four LLMs, encompassing both closed-source and open-source models: GPT-3.5  (gpt-3.5-turbo-0613) (Ouyang et al., 2022  ###reference_b22###), GPT-4 (gpt-4-0613) (OpenAI, 2023  ###reference_b21###), PaLM 2 (chat-bison-001) (Anil et al., 2023  ###reference_b1###), and LLaMA 2 (llama-2-70b-chat) (Touvron et al., 2023  ###reference_b27###). We apply four different prompting strategies to each of them to generate answers.\nStandard: We ask the model to directly answer the query without explicit reasoning instructions with a greedy decoding method.\nZero-shot Chain-of-Thoughts (CoT): We first ask the model to generate its reasoning with an instruction222Step 1: Let\u2019s think step by step, but don\u2019t give the answer directly., then direct it to answer the query with the context of reasoning steps333Step 2: Therefore, the answer is..\nThis two-step CoT strategy is developed based on CoT (Wei et al., 2022  ###reference_b31###) and its zero-shot variation (Kojima et al., 2022  ###reference_b15###).\nSelf-Consistency: We prompt the model to generate ten answers with a temperature sampling parameter of , using majority voting to finalize the model decision via diverse reasoning paths. This prompting strategy is also known as self-consistency prompting (Wang et al., 2023  ###reference_b30###).\nFew-shot In-Context Learning (ICL):\nThe model will be prompted with a few selected examples from the same category of representativeness heuristic so that the model can learn a task from demonstrations.\nSamples used as exemplars will not participate in accuracy calculations.\nThis ability to learn from context is known as in-context learning (Brown et al., 2020  ###reference_b6###)."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Evaluation Methods",
            "text": "Automatic evaluation: All query questions in ReHeAT are either in multiple-choice or ranking format.\nWe adopt precise prompt templates in Table 13  ###reference_### of the Appendix to instruct models to generate responses.\nFor a multiple-choice question, a response is deemed correct if and only if it contains the ground-truth option.\nFor a ranking question, the exact match between relative permutations of response options and ground-truth options qualifies for a true positive model prediction.\nHuman evaluation: In addition, one of the present authors with expertise in psychology conducted the human evaluation to assess the output reasoning steps from zero-shot CoT prompting.\nWe report the proportions of the four possible LLM outcomes: both reasoning and prediction are correct; the reasoning is correct, but the prediction is incorrect; the prediction is correct, but the reasoning is incorrect; both reasoning and prediction are incorrect."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experimental Results",
            "text": "In addition to evaluating the models\u2019 performance on the ReHeAT dataset and analyzing their reasoning abilities (\u00a75.1  ###reference_###; \u00a75.2  ###reference_###), we further investigated the potential performance boost that could be achieved by providing hints to the models (\u00a75.4  ###reference_###), as well as how situational similarity influences model performance (\u00a75.3  ###reference_###).\nAs shown in Table 2  ###reference_###, one-shot ICL performs the best among various prompting strategies. This has piqued our curiosity: Could providing more examples further improve performance?\nTo explore whether LLMs can acquire knowledge related to the representativeness heuristic through ICL, we report the -shot performance of four types of LLMs in Figure 3  ###reference_###.\nFrom  to , both GPT-4 and LLaMA-2 exhibit a noticeable improvement in accuracy from adding an example in context. However, such an uptrend becomes saturated and begins to fluctuate with further increases in the number of shots.\nIn contrast, GPT-3.5 and PaLM 2 do not display a clear pattern of improvement with the addition of more in-context examples, indicating a weak or non-existent correlation between the number of exemplars and accuracy.\nWe also check each type of representativeness heuristic (Table 6  ###reference_### - 9  ###reference_### of the Appendix).\nWe have observed that in most cases, most models perform relatively poorly when dealing with questions of Conjunction Fallacy and Disjunction Fallacy.\nCompared to other types of questions, such as Base Rate Fallacy, the challenge with Conjunction Fallacy and Disjunction Fallacy lies in the fact that the statistical reasoning required is embedded within the connotations and combinations of texts (e.g., South Africa is a subset of Africa) rather than being directly indicated by expressions like large sample size or far more than.\nWe examine LLMs\u2019 knowledge of statistical principles in answering SPQs.\nThe four models possess statistical knowledge, demonstrating a comprehensive understanding across all categories of statistical principles (Table 4  ###reference_### of the Appendix; SPQs example in Table 14  ###reference_### of the Appendix).\nThis significantly differs from our observations on the accuracy of the four models\u2019 performance on RHQs, indicating that the models indeed made errors related to the representativeness heuristic.\nOverall, LLMs have demonstrated a representativeness heuristic bias similar to humans, as found in the research findings reported by Bar-Hillel & Neter (1993  ###reference_b2###); Kahneman & Tversky (1973  ###reference_b13###); Kahneman et al. (1982  ###reference_b14###); Tversky & Kahneman (1974  ###reference_b28###; 1983  ###reference_b29###), where human accuracy across various tasks was within the 10% to 50% range.\nExample questions and model answers are in Table 17  ###reference_### - 22  ###reference_### of the Appendix."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "LLMs Exhibit the Representativeness Heuristic",
            "text": "We report the model\u2019s performance on the ReHeAT dataset with different prompting strategies, shown in Table 2  ###reference_###.\nAlthough GPT-3.5 shows the strongest performance in standard prompting, which we will elaborate on later in \u00a75.2  ###reference_###, the advanced prompting methods of zero-shot CoT, self-consistency, and one-shot ICL have a negative impact on its performance.\nGPT-4 and LLaMA-2-70B benefit from these prompting strategies with noticeable growth in prediction accuracy, but it is still not substantial.\nLLaMA2-70B demonstrates the most significant improvement (+14.5%) with one-shot ICL prompting, possibly due to the standard prompting on LLaMA2-70B results in response formatting issues.\nPaLM 2 is the least effective LLM, with its performance lagging behind other models using standard prompts, despite applying prompt strategies.\n###figure_8### As shown in Table 2  ###reference_###  ###reference_###, one-shot ICL performs the best among various prompting strategies. This has piqued our curiosity: Could providing more examples further improve performance?\nTo explore whether LLMs can acquire knowledge related to the representativeness heuristic through ICL, we report the -shot performance of four types of LLMs in Figure 3  ###reference_###  ###reference_###.\nFrom  to , both GPT-4 and LLaMA-2 exhibit a noticeable improvement in accuracy from adding an example in context. However, such an uptrend becomes saturated and begins to fluctuate with further increases in the number of shots.\nIn contrast, GPT-3.5 and PaLM 2 do not display a clear pattern of improvement with the addition of more in-context examples, indicating a weak or non-existent correlation between the number of exemplars and accuracy.\nWe also check each type of representativeness heuristic (Table 6  ###reference_###  ###reference_### - 9  ###reference_###  ###reference_### of the Appendix).\nWe have observed that in most cases, most models perform relatively poorly when dealing with questions of Conjunction Fallacy and Disjunction Fallacy.\nCompared to other types of questions, such as Base Rate Fallacy, the challenge with Conjunction Fallacy and Disjunction Fallacy lies in the fact that the statistical reasoning required is embedded within the connotations and combinations of texts (e.g., South Africa is a subset of Africa) rather than being directly indicated by expressions like large sample size or far more than.\nWe examine LLMs\u2019 knowledge of statistical principles in answering SPQs.\nThe four models possess statistical knowledge, demonstrating a comprehensive understanding across all categories of statistical principles (Table 4  ###reference_###  ###reference_### of the Appendix; SPQs example in Table 14  ###reference_###  ###reference_### of the Appendix).\nThis significantly differs from our observations on the accuracy of the four models\u2019 performance on RHQs, indicating that the models indeed made errors related to the representativeness heuristic.\nOverall, LLMs have demonstrated a representativeness heuristic bias similar to humans, as found in the research findings reported by Bar-Hillel & Neter (1993  ###reference_b2###  ###reference_b2###); Kahneman & Tversky (1973  ###reference_b13###  ###reference_b13###); Kahneman et al. (1982  ###reference_b14###  ###reference_b14###); Tversky & Kahneman (1974  ###reference_b28###  ###reference_b28###; 1983  ###reference_b29###  ###reference_b29###), where human accuracy across various tasks was within the 10% to 50% range.\nExample questions and model answers are in Table 17  ###reference_###  ###reference_### - 22  ###reference_###  ###reference_### of the Appendix."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Discrepancies in Model Reasoning: Beyond Predictions",
            "text": "While testing the ReHeAT dataset, we also conducted human evaluations on the reasoning steps produced by the GPT-4 and LLaMA2-70B models under CoT prompting.\nAs before, we form four possible combinations of outcomes, as shown in Figure 4  ###reference_### and Table 10  ###reference_### of the Appendix.\nGenerally, GPT-4 outperformed LLaMA2-70B in all types of RHQs.\nHowever, this improvement is accompanied by a side effect: an increased proportion of instances where GPT-4\u2019s reasoning process is accurate, but the outcome is incorrect.\nThis primarily occurs because when faced with ambiguous questions, GPT-4 often preferred to express the need for additional information before making a decision.\nThis is also why GPT-4 accuracy is slightly lower than GPT-3.5 under some prompts.\nAlthough we mark such cautious answers as incorrect outcomes\u2014because they did not adhere to the directive of providing a definitive answer\u2014cautious answers may remain beneficial in practical settings.\nConversely, LLaMA2-70B more frequently produces correct outcomes from incorrect reasoning processes; the reasons are often incorrectly based on a stereotype rather than the problem\u2019s description.\nWe have conducted case studies on some interesting reasoning, we include the case study in Appendix A  ###reference_###.\n###figure_9###"
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Improving Performance by Hinting that LLMs to use Their Knowledge",
            "text": "How can the model\u2019s cognitive process be put back on track?\nWe test whether LLMs show enhanced performance with prompts that hint them to utilize their existing knowledge.\nWe tested two types of hints: one general and the other based on more detailed cues for each representativeness heuristic type (Table 16  ###reference_### of the Appendix).\nThese prompts aimed to hint at the model to recall the knowledge it possessed.\nThe results are presented in Table 3  ###reference_###.\nWe also provide detailed results for each type of representativeness heuristic in Table 11  ###reference_### and Table 12  ###reference_### of the Appendix.\nWe found that both types of hints provided a noticeable improvement for most models, with the specific type of hints providing a more significant boost in performance compared to the general prompts."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "The Impact of Situational Similarity on Model Performance",
            "text": "After observing the performance differences between the model on statistical SPQs and the contextual RHQ counterparts found in ReHeAT, we delve further into the transition between these two - Intermediate Questions (IQs) (Table 15  ###reference_### of the Appendix).\nThis type of question integrates specific situational contexts and statistical data, making it necessary to consider both concrete data and the potential impact of the situation when making decisions.\nFor example, for Linda\u2019s problem, we assign a probability to each independent event to offer the model with more explicit statistical information.\nNevertheless, the model has to infer the information based on the similarity manifested in the meaning of the statements in RHQs.\nThe performance of the models on IQs is reported in Table 5  ###reference_### of the Appendix.\nCompared with SPQs\u2019 results (Table 4  ###reference_### of the Appendix), more errors were made for IQs.\nThis indicates that introducing scenarios interferes with the model\u2019s statistical decision-making process.\nThis also explains why LLMs exhibit the representativeness heuristic more frequently when responding to RHQs."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We introduce a novel dataset (ReHeAT) for the representativeness heuristic to assess whether models, like humans, make representativeness heuristic errors in decision-making - specifically, by overlooking known information and relying on similarity for judgment.\nOur research reveals that even the most advanced LLMs tend to repeat human heuristic mistakes when addressing issues related to the representativeness heuristic, highlighting the necessity for a deep understanding of these biases in model decision-making processes.\nFurthermore, we explored how models perform differently when faced with various types of questions.\nFor example, questions regarding the Conjunction Fallacy and Disjunction Fallacy present a significant challenge.\nThis is due to the model\u2019s difficulty discerning the latent probabilistic relationships embedded within the text.\nWe also found that hints to stimulate the model to recall its existing knowledge can, to some extent, enhance its performance.\nMore specific and detailed prompts tend to lead the model to demonstrate better performance.\nHowever, although this method is effective, the model\u2019s potential is far from fully tapped, and there remains significant room for improvement."
        }
    ],
    "url": "http://arxiv.org/html/2404.01461v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.1",
            "5.2",
            "5.3",
            "5.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.2",
            "4.3",
            "5",
            "5.1",
            "5.2",
            "5.3",
            "5.4"
        ]
    },
    "research_context": {
        "paper_id": "2404.01461v2",
        "paper_title": "Will the Real Linda Please Stand up\u2026to Large Language Models? Examining the Representativeness Heuristic in LLMs",
        "research_background": "### Motivations:\n\nThe paper's motivation revolves around understanding and addressing the cognitive biases present in Large Language Models (LLMs). Specifically, it investigates the representativeness heuristic, a phenomenon where individuals make judgments based on how much something resembles a known stereotype or prototype rather than on statistical probability. Given that LLMs are trained to emulate human behavior and decision-making, they might also exhibit similar heuristic biases. This could lead to instances where LLMs make decisions based on cognitive shortcuts rather than accurate statistical reasoning.\n\n### Research Problem:\n\nThe primary research problem is to evaluate the extent to which LLMs, such as GPT-3.5, GPT-4, PaLM 2, and LLaMA 2, display the representativeness heuristic when making decisions. The study aims to determine whether these models are prone to the same type of cognitive errors as humans are, even when they possess the necessary knowledge to solve the problems correctly. Moreover, it investigates how advanced prompting techniques can mitigate these heuristic biases, with the goal of guiding LLMs toward better reasoning and decision-making.\n\n### Relevant Prior Work:\n\n1. **Tversky & Kahneman (1983)**: Introduced the \"Linda problem,\" demonstrating that people are influenced by specific descriptions and tend to rank improbable but stereotypically matching scenarios higher than more likely but less descriptively fitting ones. This is pivotal in understanding the representativeness heuristic.\n\n2. **Kahneman & Tversky (1973)**: Introduced the concept of the representativeness heuristic, explaining how people estimate the likelihood of an event by comparing it to an existing prototype, which often neglects actual statistical evidence.\n\n3. **Spencer et al., (2016)**: Discussed stereotyping, a common occurrence among humans, which aligns with the concept of heuristics influencing decision-making based on superficial similarities.\n\n4. **Bender et al., (2021); Bolukbasi et al., (2016); Garg et al., (2018); Sheng et al., (2019); Zhao et al., (2019)**: Focused on biases in training data for LLMs, showing how skewed data distributions can lead to biased outcomes, setting a foundation for studying cognitive biases like the representativeness heuristic.\n\n5. **Bar-Hillel & Neter (1993); Kahneman et al., (1982); Tversky & Kahneman (1974, 1983)**: Provided foundational analyses and experiments on various heuristics and biases, crucial for developing the RHQs used in this study's newly constructed dataset.\n\n6. **Wei et al., (2022); Brown et al., (2020); Huang et al., (2022)**: Explored advanced prompting techniques such as chain of thought (CoT), in-context learning, and self-consistency, which have been used to analyze and potentially mitigate heuristic biases in LLMs.\n\nThe paper leverages this prior work to build upon the understanding of cognitive heuristics and apply it to the context of LLMs, aiming to reveal the extent of heuristic influence and suggest pathways for improvement.",
        "methodology": "In the methodology section of the paper titled \"Will the Real Linda Please Stand up\u2026to Large Language Models? Examining the Representativeness Heuristic in LLMs,\" the proposed method is outlined as follows:\n\n1. **Dataset Construction**:\n   - The authors start by creating a dataset that is designed to encapsulate a wide variety of questions and scenarios (\u00a74.1 ###reference_###). This ensures that the evaluation process comprehensively assesses the Large Language Models (LLMs) across different contexts and types of queries.\n\n2. **Models and Prompting Strategies**:\n   - The next step involves the introduction of the models and the strategies used for prompting (\u00a74.2 ###reference_###). This provides clarity on which LLMs are being tested and how they are being prompted to generate responses. It includes details on the specific models selected for the study and the tailored prompting techniques employed to elicit answers for the constructed dataset.\n\n3. **Evaluation Methods**:\n   - Finally, the authors detail the evaluation methods (\u00a74.3 ###reference_###). This section covers how the responses from the LLMs are assessed, the criteria used to judge their performance, and any metrics or benchmarks applied to measure the effectiveness of the models in handling the representativeness heuristic.\n\nBy maintaining the original phrasing and inclusion of the section references, this summary faithfully represents the key components and innovations of the proposed method described in the paper.",
        "main_experiment_and_results": "### Main Experiment Setup\n\nThe primary goal of the main experiment was to assess the performance of various Large Language Models (LLMs) on tasks related to the representativeness heuristic. The experiment utilized several datasets and evaluation metrics to gauge model accuracy and reasoning abilities:\n\n- **Datasets**: The ReHeAT dataset was used to evaluate the models' performance.\n- **Models Tested**: Four types of LLMs were tested:\n  - GPT-4\n  - GPT-3.5\n  - LLaMA-2\n  - PaLM 2\n- **Prompting Strategies**: Different in-context learning (ICL) prompt strategies were employed, ranging from zero-shot to multiple-shot setups.\n- **Evaluation Metrics**: Model accuracy was measured, and detailed analysis on specific question types such as Conjunction Fallacy, Disjunction Fallacy, and Base Rate Fallacy questions were conducted. Statistical Principle Questions (SPQs) were used to assess the models' understanding of statistical principles.\n\n### Main Experimental Results\n\n- **One-shot ICL Performance**: One-shot ICL showed the best performance among various prompting strategies.\n- **Multi-shot Performance**: \n  - Both **GPT-4 and LLaMA-2** showed improved accuracy with the addition of a single example but exhibited performance saturation and fluctuation with more shots.\n  - **GPT-3.5 and PaLM 2** did not show a clear improvement pattern with more in-context examples, suggesting a weaker or non-existent correlation between the number of examples and accuracy.\n  \n- **Question Type Performance**: \n  - Most models performed poorly on Conjunction Fallacy and Disjunction Fallacy questions compared to other types like Base Rate Fallacy questions.\n  - The difficulty with Conjunction and Disjunction Fallacies is attributed to the embedded statistical reasoning within the text, rather than explicit terms.\n\n- **Statistical Knowledge**: The four models displayed a comprehensive understanding of statistical principles across all categories of SPQs, but this knowledge did not consistently translate into accurate reasoning on representativeness heuristic questions.\n\n### Final Observations\n\n- **Representativeness Heuristic Bias**: The models exhibited a similar representativeness heuristic bias to humans, consistent with findings from past research by Bar-Hillel & Neter (1993) and Kahneman & Tversky (1973, 1983).\n- **Example Questions and Model Answers**: Detailed examples and model responses are available, providing insights into the types of errors the models made related to the representativeness heuristic.\n\nIn summary, while LLMs show some capability in understanding and applying statistical principles, their performance on representativeness heuristic tasks varies significantly, with notable difficulties in tasks involving Conjunction and Disjunction Fallacies."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To explore whether providing hints that prompt LLMs to utilize their existing knowledge can enhance their performance and reduce representativeness heuristic biases.",
            "experiment_process": "Two types of hints were tested: a general hint and more detailed cues for each representativeness heuristic type. These prompts aimed to hint at the model to recall the knowledge it possessed. Detailed results for each representativeness heuristic were presented, specifically in Table 11 and Table 12 of the Appendix.",
            "result_discussion": "Both types of hints provided a noticeable improvement for most models. The specific type of hints provided a more significant boost in performance compared to the general prompts.",
            "ablation_id": "2404.01461v2.No1"
        },
        {
            "research_objective": "To understand how situational similarity influences LLM performance and their susceptibility to the representativeness heuristic.",
            "experiment_process": "The performance on Intermediate Questions (IQs) was analyzed. These questions integrate specific situational contexts with statistical data, requiring the models to consider both explicit statistical information and situational context. For instance, for Linda's problem, a probability was assigned to each independent event to provide more explicit statistical information. The model's performance was then compared between IQs and standard SPQs, as reported in Table 5 of the Appendix.",
            "result_discussion": "More errors were made with IQs compared to SPQs, indicating that introducing scenarios interferes with the model's statistical decision-making process. This explains why LLMs exhibit the representativeness heuristic more frequently when responding to situational RHQs.",
            "ablation_id": "2404.01461v2.No2"
        }
    ]
}