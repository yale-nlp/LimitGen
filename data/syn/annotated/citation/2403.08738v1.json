{
    "title": "Improving Acoustic Word Embeddings through Correspondence Training of Self-supervised Speech Representations",
    "abstract": "Acoustic word embeddings (AWEs) are vector representations of spoken words. An effective method for obtaining AWEs is the Correspondence Auto-Encoder (CAE). In the past, the CAE method has been associated with traditional MFCC features. Representations obtained from self-supervised learning (SSL)-based speech models such as HuBERT, Wav2vec2, etc., are outperforming MFCC in many downstream tasks. However, they have not been well studied in the context of learning AWEs. This work explores the effectiveness of CAE with SSL-based speech representations to obtain improved AWEs. Additionally, the capabilities of SSL-based speech models are explored in cross-lingual scenarios for obtaining AWEs. Experiments are conducted on five languages: Polish, Portuguese, Spanish, French, and English. HuBERT-based CAE model achieves the best results for word discrimination in all languages, despite HuBERT being pre-trained on English only. Also, the HuBERT-based CAE model works well in cross-lingual settings. It outperforms MFCC-based CAE models trained on the target languages when trained on one source language and tested on target languages.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Self-supervised learning (SSL)-based speech representations are becoming popular in speech processing and producing state-of-the-art results in many downstream tasks such as automatic speech recognition, speaker verification, keyword spotting, voice conversion, etc Yang et al. (2021  ###reference_b27###). These representations are obtained using self-supervised learning on large amounts of unlabelled speech data. Wav2vec2 Baevski et al. (2020  ###reference_b4###), HuBERT Hsu et al. (2021  ###reference_b11###), and WavLM Chen et al. (2022  ###reference_b7###) are a few examples of such SSL-based speech models. However, representations obtained from these models have not been extensively explored in the context of learning acoustic word embeddings (AWEs). AWEs are fixed-dimensional vector representations of spoken words that find applications in various downstream tasks, such as query-by-example search Settle et al. (2017  ###reference_b25###); Yuan et al. (2018  ###reference_b28###); Hu et al. (2021  ###reference_b12###), keyword spotting Barakat et al. (2011  ###reference_b5###), providing clues for human lexical processing Matusevych et al. (2020  ###reference_b18###), hate speech detection in low resource settings Jacobs et al. (2023  ###reference_b13###), etc.\nRecently, the work Sanabria et al. (2023  ###reference_b23###) proposed extracting AWEs from SSL-based speech representations using a mean pooling mechanism. The authors suggest that SSL-based speech representations, which are contextualized, can be effectively converted into AWEs using a straightforward pooling mechanism. On the other hand, Correspondence Auto-Encoder (CAE) based training strategies for AWEs Kamper (2019  ###reference_b14###) using MFCC Davis and Mermelstein (1980  ###reference_b9###) features are shown to be promising in the literature. Correspondence training involves an auto-encoder where a spoken word serves as the input to the encoder, and the target output of the decoder is a different instance of the same spoken word. This approach helps to preserve acoustic-phonetic information while filtering out unnecessary details such as speaker, acoustic environment, and duration, etc. Both encoder and decoder are typically recurrent neural networks (RNNs). More details about the model will be presented in Sec. 2  ###reference_### and Sec. 4.2  ###reference_###. Correspondence training has also been explored in the work Meghanani and Hain (2024  ###reference_b20###) to improve content representations of SSL-based speech models.\nThe work Lin et al. (2023  ###reference_b17###) uses a Correspondence Transformer Encoder (CTE) for obtaining robust AWEs, trained from scratch and a large-scale unlabelled speech corpus. In contrast, in this work, pre-trained SSL speech models are coupled with a simple RNN based auto-encoder for correspondence training to obtain robust AWEs. This work attempts to use the correspondence training of auto-encoder to obtain the AWEs by leveraging SSL-based speech representations instead of MFCC features as input features to the CAE model. Further, cross-lingual capabilities are also examined for SSL-based AWEs trained with CAE method. The SSL models (HuBERT, Wav2vec2, and WavLM) used in this work are pre-trained on English data. However, it has been demonstrated that these models work well as feature extractors for the all the languages considered in this study. The performances on the word-discrimination task for all the languages (Polish, Portuguese, Spanish, and French) are as good as on the English language (Sec. 5  ###reference_###). A detailed analysis is also conducted to assess the importance of contextual information in spoken words by comparing feature extraction with and without context. For this work, we obtain spoken words for all five languages from the subset of Multilingual LibriSpeech (MLS) Pratap et al. (2020  ###reference_b22###) dataset. The derived dataset consists of five languages (Polish, Portuguese, Spanish, French, English) with start and end timestamps of spoken words 111https://github.com/Trikaldarshi/SSL_AWE  ###reference_###.\nWe chose MLS dataset for our experiments as many previous works Matusevych et al. (2020  ###reference_b18###); Abdullah et al. (2021a  ###reference_b1###, 2022  ###reference_b3###, b  ###reference_b2###); Kamper et al. (2021  ###reference_b16###) on AWEs rely on GobalPhone Schultz et al. (2013  ###reference_b24###) dataset, which is not freely available.\nThe main contributions of this work are as follows:\nUtilizing corresponding training with SSL-based speech representations to obtain highly discriminative AWEs.\nShowing effectiveness of SSL models, pre-trained only on English, as feature extractors in cross-lingual scenarios for obtaining high-quality AWEs.\nQuantitatively demonstrating that incorporating the context of the spoken word in SSL-based speech representations leads to the production of more robust AWEs.\nThe rest of the paper is as follows: Sec. 2  ###reference_### describes the correspondence auto-encoder methodology to obtain AWEs; Sec. 3  ###reference_### describes the data preparation and data statistics; Sec. 4  ###reference_### describes the details of the experiments; Sec. 5  ###reference_### describes the results and analysis; Sec. 6  ###reference_### concludes the work with possible future directions.\nSec. 7  ###reference_### describes the limitations of the work."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "Correspondence auto-encoder is trained with input as a spoken word and target output as a different instance of the same spoken word. Typically, Recurrent Neural Network (RNN) based encoder and decoder are used, hence the model is referred to as CAE-RNN. The rationale behind this training method is that CAE-RNN will preserve only the acoustic-phonetic information and filter out the other unnecessary information factors such as speaker, duration, acoustic environment, etc Kamper (2019  ###reference_b14###). Fig. 1  ###reference_### shows the CAE-RNN model setup. The input to the ENC is a sequence of acoustic feature vectors () of a spoken word. The target output is the sequence of acoustic feature vectors of the different instance of the same spoken word (). The encoder produces the AWE (e) of the spoken word , which is then fed to the decoder to reconstruct . The output of the decoder is represented as . The mean squared loss function for a single training pair () can be described as following:\n###figure_1### , where  is the target output and  is the output of the decoder as shown in Fig. 1  ###reference_###."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Data Preparation",
            "text": "The Multilingual LibriSpeech (MLS) dataset Pratap et al. (2020  ###reference_b22###) is utilized to obtain spoken words. Five languages, namely Polish, Portuguese, Spanish, French, and English, are selected from MLS. For each language, approximately 25,000 utterances are selected for the training set, 500 for the development set, and 500 for the test set. These selected utterances are force-aligned to obtain the spoken word boundaries using the Montreal Forced Aligner toolkit McAuliffe et al. (2017  ###reference_b19###). Only spoken words with a duration of 0.5 seconds or longer are included in the derived dataset, following the standard practice in the literature He et al. (2017  ###reference_b10###). Spoken words with a frequency greater than 50 or less than 5 are excluded from the derived dataset. Table 1  ###reference_### presents a summary of the statistics for the final extracted dataset\n, encompassing all five languages. The speakers across different sets are non-overlapping, which is a desirable characteristic for evaluating AWEs as they should be robust to speaker variations. Polish language had limited available data and consequently has the fewest number of speakers, while English has the highest number. The duration in Table 1  ###reference_### represents the total time duration of spoken words across the different sets."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experimental Setup",
            "text": "Experiments are conducted on all the five languages with SSL-based speech representations as input features extracted from Wav2vec2, HuBERT, and WavLM. Experiments are also conducted with MFCC as input features.\nFirst, the feature extraction methods for various SSL-based speech representations and MFCCs are described. Then, the configuration of the CAE-RNN model is explained, along with the mean pooling baseline Sanabria et al. (2023  ###reference_b23###) and the AE-RNN method (without correspondence training), for comparison. Next, the word discrimination task is described, which is used for evaluating the quality of the extracted AWEs. Finally, the training details of the CAE-RNN and other models are provided."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Feature Extraction",
            "text": ""
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1 SSL-based Speech Representations",
            "text": "SSL models are pre-trained on large amount of unlabelled speech data. The task defined for the pre-training is known as the pretext task. Each model differs based on how the pretext task was defined, the data used for pre-training, and the model architecture. In this work, the \u201cBASE\u201d architectures of Wav2vec2, HuBERT, and WavLM model 222https://github.com/pytorch/fairseq  ###reference_### (all with  95M parameters) are used for feature extraction. All these models are pre-trained on 960 hours of LibriSpeech data Panayotov et al. (2015  ###reference_b21###). A \u201cBASE\" architecture typically has a multi-layer CNN-based feature encoder followed by 12 Transformer layers. In this work, representations from each model are extracted from the final (i.e. 12th) Transformer layer. For all the above mentioned SSL models, 768-dimensional feature vectors are obtained for each spoken word at a framerate of 20 ms.\nSSL-based speech representations are extracted in two different ways: the first one is extracted using the context around the spoken word, and the other one is extracted without the context, as described here:\nWith context: In this case, first the SSL-based speech representations of the entire spoken utterance are computed and then the time boundaries of the spoken word is used to get the representations of the segment belonging to the spoken word. This ensures that the extracted representations capture the context around the spoken word as the entire utterance is processed by the SSL model. Let us assume  represents an utterance and  represents a spoken word instance present in the utterance  with start and end timestamps denoted as  and . If  represents the SSL model, then the SSL-based speech representation for the entire utterance is computed . Then the speech representations for the spoken word  will be .\nWithout context: In this case, no context is considered and SSL-based speech representations are extracted by inputting only speech segments belonging to the spoken words to the SSL models. Hence, in this case, the SSL-based speech representation for the spoken word  will be ."
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "4.1.2 MFCC Features",
            "text": "For each spoken word, 20-dimensional MFCC features are extracted with 30 ms window size and 20 ms shift along with delta and delta-delta features, which leads to 60-dimensional MFCC feature vectors. In the realm of AI in healthcare diagnostics, machine learning models have demonstrated significant promise in automating and enhancing diagnostic processes. This includes leveraging deep learning algorithms to accurately interpret medical imaging and longitudinal data, thereby improving diagnostic precision and efficiency."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Model Details",
            "text": "A 4-layer Bidirectional GRU with a hidden dimension of 256 is used for both the encoder and decoder in the CAE-RNN model. Dropout rate is set to 0.2. The final hidden state of the encoder-GRU is fed to a fully connected layer to obtain 128-dimensional AWE (e) as shown in Fig. 1  ###reference_###. This embedding is then fed to the decoder at each time step as input to the decoder Kamper (2019  ###reference_b14###). The output of the decoder is then fed to a fully connected layer to produce the target output.\nA regular auto-encoder RNN (AE-RNN) model is also used as one of the baselines with similar configurations. AE-RNN model is an auto-encoder model where input and target output is exactly the same spoken word, i.e. input-output training pair is (). A mean pooling model is also used as baseline Sanabria et al. (2023  ###reference_b23###), which does not require any training. This method computes the mean of the SSL-based speech representations to get the 768-dimensional AWE of a spoken word."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Word Discrimination Task",
            "text": "To evaluate the AWEs, the same-different word-discrimination task is used Kamper et al. (2015  ###reference_b15###); Carlin et al. (2011  ###reference_b6###). First, all possible spoken word pairs are generated. For example, if there are total N spoken words, then the total generated spoken word pairs for comparison will be . After that, the cosine distance between the AWEs of these pairs are computed and compared with a threshold to decide whether the spoken words are same or different. The average precision (AP) is calculated by varying all the possible threshold values, which is the area under the precision-recall curve. AP is reported for the same-different word discrimination task.\nWord-discrimination task is applied on the test set, which has unseen speakers during training.\nAlso, a subset of the test set is created for all five languages in such a way that none of the words in the subset are encountered during training. This particular subset is referred to as . The word-discrimination task is also conducted on . The total number of generated spoken word pairs for both the test and  sets is described in Table 2  ###reference_### for all languages."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Training Details",
            "text": "The total number of generated correspondence training pairs () for each language is as follows: 9,55,106 for Polish, 11,63,468 for Portuguese, 7,80,197 for Spanish, 7,95,613 for French, and 9,72,532 for English. The remaining training details are as follows for various inputs:\nSSL-based Speech Representations as Input:\nCAE-RNN models are trained for 30 epochs, using a learning rate of 0.0001 with Adam optimizer and a batch size of 512. In each run, the model with the best performance on the development set in terms of word-discrimination is selected as the final model for evaluation on the test set. AE-RNN models are trained for 50 epochs, keeping all other parameters same as mentioned above for the CAE-RNN models.\nMFCC as Input: Both AE-RNN and CAE-RNN models with MFCC as inputs are trained for 100 epochs, using a learning rate of 0.0001 with Adam optimizer. The batch size for the AE-RNN model was chosen as 64, while for the CAE-RNN model it was set to 256 based on preliminary experiments for better convergence. Similarly to the previous case, the model with the best performance on the development set in terms of word-discrimination is selected as the final model for evaluation on the test set."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Results and Analysis",
            "text": "Table 3  ###reference_### shows the baseline results with MFCC features as input for the AE-RNN and CAE-RNN models. This demonstrates the effectiveness of the CAE-RNN model over the AE-RNN model for the word-discrimination task, as the CAE-RNN consistently outperforms the AE-RNN for all languages. Table 4  ###reference_### presents the results for the derived subset of the test set () with similar trends. It is worth noting that the AP on the  set is relatively better than that of the original test set in most cases. This is likely due to the fact that the number of spoken word pairs generated for the evaluation on the  is significantly fewer compared to the original test set, as mentioned in Table 2  ###reference_###.\nTable 5  ###reference_### displays the results obtained from using various SSL-based speech representations (Wav2vec2, WavLM, and HuBERT) as input features, combined with different AWE extraction methods (mean pooling, CAE-RNN, and AE-RNN). The results presented in Table 5  ###reference_### represent the AP for the word-discrimination task on the test set, employing different SSL-based speech representation feature extraction setups (\u2018with context\u2019 and \u2018without context\u2019).\nFrom Table 5  ###reference_###, it is evident that the AWEs derived \u2018with context\u2019 exhibit greater robustness. The AP on the test set for all languages is significantly better when utilizing SSL-based speech representations \u2018with context\u2019 compared to the feature extraction \u2018without context\u2019.\nAs shown in Table 5  ###reference_###, the CAE-RNN model demonstrates superior performance when using SSL-based speech representations as input features compared to the MFCC-based baseline model (Table 3  ###reference_###) across all languages. Furthermore, Table 5  ###reference_### provides a comparison of the CAE-RNN model with other baseline models (mean pooling and AE-RNN) when utilizing SSL-based speech representations as input features. CAE-RNN consistently outperforms both the AE-RNN and mean pooling methods for all languages and SSL models. Another advantage of the CAE-RNN model over mean pooling is that the AWEs obtained from CAE-RNN have a dimension of 128, while mean pooling-based AWEs are 768-dimensional. Based on the results presented in Table 5  ###reference_###, it is evident that the HuBERT features consistently achieve the best performance across all configurations and languages. Specifically, when using the CAE-RNN method for AWE extraction and SSL-based speech representations extracted \u2018with context\u2019, the HuBERT achieves the highest AP on the test set: 0.90 for Polish, 0.88 for Portuguese, 0.95 for Spanish, 0.74 for French, and 0.86 for English. The performance order can be sorted as HuBERT  Wav2vec2  WavLM  MFCCs when using the CAE-RNN-based AWE model and SSL-based speech representations extracted \u2018with context\u2019.\nTable 6  ###reference_### presents the results for the  set, which includes unseen words and speakers. The models exhibit similar trends in performance in this case as well. This provides evidence that the proposed methodology performs equally well on unseen words.\nOne interesting finding is that the SSL-based speech representations considered in this work were pre-trained solely on English language. Despite this, they are capable of generating meaningful features for other languages, resulting in good performance as demonstrated in Table 5  ###reference_### and 6  ###reference_### for the word-discrimination task."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Cross-lingual Analysis",
            "text": "To assess the effectiveness of SSL speech representation-based CAE-RNN models in cross-lingual settings, a CAE-RNN model trained on one source language (English in this case) is evaluated on four different target languages. This evaluation can be considered a \u2018zero-shot\u2019 evaluation, as no training data from the target languages is required. Table 7  ###reference_### displays the results in terms of AP for the word-discrimination task on the test set and  set for the four target languages (Polish, Portuguese, Spanish, and French).\nIn this scenario as well, the HuBERT-based CAE-RNN model achieves the best performance across all languages, except for French where Wav2vec2 performs the best. It is worth noting that the CAE-RNN model in the \u2018zero-shot\u2019 setting outperforms the mean pooling method (Table 5  ###reference_### and 6  ###reference_###) Sanabria et al. (2023  ###reference_b23###) and the CAE-RNN model trained on the target language with MFCC features (Table 3  ###reference_### and 4  ###reference_###).\nThe mean pooling method Sanabria et al. (2023  ###reference_b23###) can be considered a \u2018zero-shot\u2019 AWE extraction method, as it does not involve additional training on top of the pre-trained SSL models. In a \u2018zero-shot\u2019 setup for target languages, using a CAE-RNN trained on a well-resourced source language can offer an advantage over the mean pooling method. In conclusion, SSL-based CAE-RNN models have fairly good performance when used crosslingually. There have been earlier studies Kamper et al. (2021  ###reference_b16###) on acoustic word embeddings for zero-resource languages using multilingual transfer with MFCC features, which worked well. Also, intuitively, some generalisation was expected as the aim of modelling is to compress a small segment of speech into a fixed dimensional vector. There might be a language effect on pre-trained SSL speech models but the basic speech properties are still invariant to changes in the language. The cross-lingual ability of SSL-based CAE-RNN models to obtain AWEs can support many applications such as speech search, indexing and discovery systems for languages with low-resources Kamper et al. (2021  ###reference_b16###).\n###figure_2### ###figure_3### ###figure_4### ###figure_5### ###figure_6###"
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Analysis of Anagram Pairs",
            "text": "Anagrams are words that can be formed by rearranging the letters of another word. Analysing anagram pairs provides insights into the impact of letter order on AWE representation. Robust AWEs should capture the letter order in spoken words. For this analysis, same spoken word pairs and anagram pairs are chosen from different speakers. Ideally, the cosine distance between the same word pairs should be close to 0, while anagram word pairs should be close to 1. In Table 8  ###reference_###, HuBERT-based CAE-RNN AWEs demonstrate cosine distances of approximately 0.01, 0.11, and 0.02 for the same spoken word pairs \u2018aside\u2019, \u2018this\u2019, and \u2018no\u2019, respectively. The anagram pairs of the words \u2018aside\u2019, \u2018this\u2019, and \u2018no\u2019 (i.e., \u2018ideas\u2019, \u2018hits\u2019, and \u2018on\u2019) have distances of 0.99, 0.50, and 0.69, respectively, for the HuBERT-based CAE-RNN model. These values are significantly better for both the same word pairs and anagram word pairs when compared to the HuBERT-based mean pooling method Sanabria et al. (2023  ###reference_b23###). This indicates that the HuBERT-based CAE-RNN model accurately captures the letter order in a word compared to the mean pooling baseline Sanabria et al. (2023  ###reference_b23###)."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "AWE Visualisation",
            "text": "t-SNE visualization is used to plot the 2-dimensional representations of the derived AWEs for all five languages. From each language, all spoken instances of the top 7 words with the highest frequency count from the test set are chosen. The plots demonstrate distinct and well-separated clusters for each spoken word across all languages. One interesting pattern can be observed for the Polish language, where the clusters of the spoken words \u2018Owad\u00f3w\u2019 and \u2018Owady\u2019 share the boundary and are closely related in the AWE space. This is likely due to the fact that the first four letters of both the words (o, w, a, d) are shared and these words only differ in their endings."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusions and Future Work",
            "text": "It has been demonstrated that SSL-based speech representations with CAE-RNN models outperform mean pooling and AE-RNN models across all languages. They also outperform MFCC-based models. Among all the SSL models, HuBERT performs the best when used as input for the CAE-RNN model, outperforming models such as Wav2vec2 and WavLM. Notably, despite being pre-trained on English data, the SSL models exhibit excellent performance on other languages, showcasing their cross-lingual generalization capability for AWE extraction.\nFurthermore, quantitative analysis reveals that incorporating context information of the spoken word leads to more robust AWEs. The HuBERT-based CAE-RNN model trained on English language and tested on other target languages outperforms the mean pooling method and the CAE-RNN model trained on the target language using MFCC features. This \u2018zero-shot\u2019 method to obtain robust AWEs for the target language can be useful in applications for low-resource languages Jacobs et al. (2023  ###reference_b13###). An analysis was also conducted to show that the CAE-RNN model effectively captures the order of letters in a word.\nIn future work, experiments will be conducted with the \u201cLARGE\" variation of SSL models, as well as multilingual pre-trained SSL models such as Wav2vec2-XLSR Conneau et al. (2020  ###reference_b8###). Additionally, an interesting experiment would involve training a single universal AWE model on all languages and comparing its performance with language-specific AWE models. Further research will focus on measuring the performance gains of SSL-based CAE-RNN models on downstream tasks such as query-by-example search Settle et al. (2017  ###reference_b25###); Yuan et al. (2018  ###reference_b28###); Hu et al. (2021  ###reference_b12###) and keyword spotting Shin et al. (2022  ###reference_b26###)."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "This work is focused on the extraction of AWEs and measuring their quality solely based on the word discrimination task. No downstream applications such as query-by-example search and keyword spotting, have been discussed using the improved AWEs. In this work, only the \u201cBASE\u201d versions of the SSL-based speech models are explored for experiments and analysis. There are other variations, such as \u201cLARGE\u201d version, for which this study can be extended. All the languages considered in this work belong to the Indo-European language family. This work does not contain the analysis of languages that belong to another language family, such as Dravidian or Afroasiatic language families. This work does not deal with layer-wise analysis, which can provide better insights for further improving the AWEs."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Acknowledgments",
            "text": "This work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1]. This work was also funded in part by LivePerson, Inc."
        }
    ],
    "appendix": [],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T1\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" id=\"S3.T1.1\" style=\"width:433.6pt;height:104.8pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-6.7pt,1.6pt) scale(0.970004157152871,0.970004157152871) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S3.T1.1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.1.1.1.1.1\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T1.1.1.1.1.1.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.1.1.1.1.1.1.1\">\n<span class=\"ltx_tr\" id=\"S3.T1.1.1.1.1.1.1.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.1.1.1.1.1.1.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.1.1.1.1.1.1.1\">Data</span></span></span>\n<span class=\"ltx_tr\" id=\"S3.T1.1.1.1.1.1.1.1.2\">\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.1.1.1.1.1.2.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.1.1.1.1.2.1.1\">Statistics</span></span></span>\n</span></span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"3\" id=\"S3.T1.1.1.1.1.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.1.2.1\">Polish</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"3\" id=\"S3.T1.1.1.1.1.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.1.3.1\">Portuguese</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"3\" id=\"S3.T1.1.1.1.1.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.1.4.1\">Spanish</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"3\" id=\"S3.T1.1.1.1.1.5\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.1.5.1\">French</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"3\" id=\"S3.T1.1.1.1.1.6\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.1.6.1\">English</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.2.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.2.2.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Train</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.2.2.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Dev</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.2.2.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Test</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.2.2.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Train</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.2.2.5\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Dev</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.2.2.6\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Test</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.2.2.7\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Train</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.2.2.8\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Dev</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.2.2.9\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Test</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.2.2.10\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Train</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.2.2.11\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Dev</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.2.2.12\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Test</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.2.2.13\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Train</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.2.2.14\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Dev</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.2.2.15\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Test</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.3.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.1.1.3.3.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"># Spoken Words</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.3.3.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">104448</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.3.3.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">4595</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.3.3.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">4563</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.3.3.5\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">117820</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.3.3.6\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">4964</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.3.3.7\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">4659</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.3.3.8\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">82258</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.3.3.9\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3721</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.3.3.10\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3601</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.3.3.11\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">84267</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.3.3.12\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3004</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.3.3.13\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3114</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.3.3.14\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">92352</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.3.3.15\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3147</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.3.3.16\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3192</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.4.4\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S3.T1.1.1.4.4.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"># Unique Spoken Words</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.4.4.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">9346</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.4.4.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3818</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.4.4.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3887</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.4.4.5\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">9785</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.4.4.6\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3696</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.4.4.7\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3539</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.4.4.8\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">7085</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.4.4.9\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2678</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.4.4.10\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2769</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.4.4.11\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">7221</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.4.4.12\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2394</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.4.4.13\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2433</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.4.4.14\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">7157</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.4.4.15\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2448</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.4.4.16\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2527</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.5.5\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S3.T1.1.1.5.5.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"># Speaker</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.5.5.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">11</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.5.5.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.5.5.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.5.5.5\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">42</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.5.5.6\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">10</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.5.5.7\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">10</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.5.5.8\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">79</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.5.5.9\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">20</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.5.5.10\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">20</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.5.5.11\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">120</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.5.5.12\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">18</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.5.5.13\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">18</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.5.5.14\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">182</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.5.5.15\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">42</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.5.5.16\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">41</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.6.6\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b\" id=\"S3.T1.1.1.6.6.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Total Duration (hours)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S3.T1.1.1.6.6.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">18.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S3.T1.1.1.6.6.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S3.T1.1.1.6.6.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S3.T1.1.1.6.6.5\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">21.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S3.T1.1.1.6.6.6\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.93</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S3.T1.1.1.6.6.7\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.87</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S3.T1.1.1.6.6.8\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">14.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S3.T1.1.1.6.6.9\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S3.T1.1.1.6.6.10\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S3.T1.1.1.6.6.11\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">14.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S3.T1.1.1.6.6.12\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S3.T1.1.1.6.6.13\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S3.T1.1.1.6.6.14\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">16.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S3.T1.1.1.6.6.15\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S3.T1.1.1.6.6.16\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.57</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>A summary of the data statistics for all five languages across the train, dev, and test splits</figcaption>\n</figure>",
            "capture": "Table 1: A summary of the data statistics for all five languages across the train, dev, and test splits"
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T2\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" id=\"S4.T2.1\" style=\"width:433.6pt;height:132.9pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(90.5pt,-27.8pt) scale(1.7171144241631,1.7171144241631) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T2.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.2.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\" id=\"S4.T2.1.1.2.1.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.1.1.2.1.1.1\">\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.2.1.1.1.1\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.2.1.1.1.1.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.2.1.1.1.1.1.1\">Spoken Word</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.2.1.1.1.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.2.1.1.1.2.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.2.1.1.1.2.1.1\">Pairs (in million)</span></td>\n</tr>\n</table>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.1.1.2.1.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.2.1.2.1\">Polish</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.1.1.2.1.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.2.1.3.1\">Portuguese</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.1.1.2.1.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.2.1.4.1\">Spanish</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.1.1.2.1.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.2.1.5.1\">French</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.1.1.2.1.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.2.1.6.1\">English</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.3.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" id=\"S4.T2.1.1.3.1.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.3.1.1.1\">test</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.3.1.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">10.4 M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.3.1.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">10.8 M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.3.1.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">6.4 M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.3.1.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">4.8 M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.3.1.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">5.1 M</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b\" id=\"S4.T2.1.1.1.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\u2006 \n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T2.1.1.1.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">4.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T2.1.1.1.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">2.6 M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T2.1.1.1.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">1.8 M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T2.1.1.1.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">1.3 M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T2.1.1.1.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">1.4 M</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>The total number of spoken word pairs generated for the <span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.5.1\">test</span> and  sets.</figcaption>\n</figure>",
            "capture": "Table 2: The total number of spoken word pairs generated for the test and  sets."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T3\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T3.1\" style=\"width:433.6pt;height:105.9pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(106.3pt,-25.9pt) scale(1.96109177120739,1.96109177120739) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T3.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.1.1.1.1.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.1.1.1.1.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.2.1\">Polish</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.1.1.1.1.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.3.1\">Portuguese</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.1.1.1.1.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.4.1\">Spanish</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.1.1.1.1.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.5.1\">French</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.1.1.1.1.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.6.1\">English</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.2.1.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">AE-RNN</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.2.1.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.2.1.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.2.1.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.2.1.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.2.1.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.01</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.3.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T3.1.1.3.2.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">CAE-RNN</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T3.1.1.3.2.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T3.1.1.3.2.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T3.1.1.3.2.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T3.1.1.3.2.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.43</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T3.1.1.3.2.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.24</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>AP on the <span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.3.1\">test</span> set for word-discrimination task using MFCCs as input features for AE-RNN and CAE-RNN models in different languages.</figcaption>\n</figure>",
            "capture": "Table 3: AP on the test set for word-discrimination task using MFCCs as input features for AE-RNN and CAE-RNN models in different languages."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T4\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T4.3\" style=\"width:433.6pt;height:105.9pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(106.3pt,-25.9pt) scale(1.96109177120739,1.96109177120739) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T4.3.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T4.3.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T4.3.1.1.1.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.3.1.1.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T4.3.1.1.1.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.3.1.1.1.2.1\">Polish</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T4.3.1.1.1.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.3.1.1.1.3.1\">Portuguese</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T4.3.1.1.1.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.3.1.1.1.4.1\">Spanish</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T4.3.1.1.1.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.3.1.1.1.5.1\">French</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T4.3.1.1.1.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.3.1.1.1.6.1\">English</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T4.3.1.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.3.1.2.1.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">AE-RNN</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.3.1.2.1.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.3.1.2.1.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.3.1.2.1.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.3.1.2.1.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.3.1.2.1.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.01</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.3.1.3.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T4.3.1.3.2.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">CAE-RNN</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T4.3.1.3.2.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T4.3.1.3.2.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T4.3.1.3.2.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.63</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T4.3.1.3.2.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T4.3.1.3.2.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.33</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>AP on the  set for word-discrimination task using MFCCs as input features for AE-RNN and CAE-RNN models in different languages.</figcaption>\n</figure>",
            "capture": "Table 4: AP on the  set for word-discrimination task using MFCCs as input features for AE-RNN and CAE-RNN models in different languages."
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T5\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S5.T5.1\" style=\"width:433.6pt;height:211.3pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-10.3pt,5.0pt) scale(0.954465680863876,0.954465680863876) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T5.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T5.1.1.1.1.1\" rowspan=\"2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" id=\"S5.T5.1.1.1.1.1.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T5.1.1.1.1.1.1.1\">\n<span class=\"ltx_tr\" id=\"S5.T5.1.1.1.1.1.1.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.1.1.1.1.1.1.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.1.1.1.1.1.1.1.1\">AWE</span></span></span>\n<span class=\"ltx_tr\" id=\"S5.T5.1.1.1.1.1.1.1.2\">\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.1.1.1.1.1.2.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.1.1.1.1.1.2.1.1\">Extraction</span></span></span>\n<span class=\"ltx_tr\" id=\"S5.T5.1.1.1.1.1.1.1.3\">\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.1.1.1.1.1.3.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.1.1.1.1.1.3.1.1\">Method</span></span></span>\n</span></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T5.1.1.1.1.2\" rowspan=\"2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" id=\"S5.T5.1.1.1.1.2.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T5.1.1.1.1.2.1.1\">\n<span class=\"ltx_tr\" id=\"S5.T5.1.1.1.1.2.1.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.1.1.2.1.1.1.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.1.1.2.1.1.1.1.1\">Input</span></span></span>\n<span class=\"ltx_tr\" id=\"S5.T5.1.1.1.1.2.1.1.2\">\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.1.1.2.1.1.2.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.1.1.2.1.1.2.1.1\">Features</span></span></span>\n</span></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\" id=\"S5.T5.1.1.1.1.3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.1.1.3.1\">Polish</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\" id=\"S5.T5.1.1.1.1.4\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.1.1.4.1\">Portuguese</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\" id=\"S5.T5.1.1.1.1.5\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.1.1.5.1\">Spanish</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\" id=\"S5.T5.1.1.1.1.6\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.1.1.6.1\">French</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\" id=\"S5.T5.1.1.1.1.7\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.1.1.7.1\">English</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T5.1.1.2.2.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T5.1.1.2.2.1.1\">\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.2.2.1.1.1\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.2.2.1.1.1.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.2.2.1.1.1.1.1\">with</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.2.2.1.1.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.2.2.1.1.2.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.2.2.1.1.2.1.1\">context</span></td>\n</tr>\n</table></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T5.1.1.2.2.2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T5.1.1.2.2.2.1\">\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.2.2.2.1.1\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.2.2.2.1.1.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.2.2.2.1.1.1.1\">without</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.2.2.2.1.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.2.2.2.1.2.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.2.2.2.1.2.1.1\">context</span></td>\n</tr>\n</table>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T5.1.1.2.2.3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T5.1.1.2.2.3.1\">\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.2.2.3.1.1\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.2.2.3.1.1.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.2.2.3.1.1.1.1\">with</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.2.2.3.1.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.2.2.3.1.2.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.2.2.3.1.2.1.1\">context</span></td>\n</tr>\n</table></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T5.1.1.2.2.4\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T5.1.1.2.2.4.1\">\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.2.2.4.1.1\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.2.2.4.1.1.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.2.2.4.1.1.1.1\">without</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.2.2.4.1.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.2.2.4.1.2.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.2.2.4.1.2.1.1\">context</span></td>\n</tr>\n</table>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T5.1.1.2.2.5\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T5.1.1.2.2.5.1\">\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.2.2.5.1.1\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.2.2.5.1.1.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.2.2.5.1.1.1.1\">with</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.2.2.5.1.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.2.2.5.1.2.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.2.2.5.1.2.1.1\">context</span></td>\n</tr>\n</table></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T5.1.1.2.2.6\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T5.1.1.2.2.6.1\">\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.2.2.6.1.1\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.2.2.6.1.1.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.2.2.6.1.1.1.1\">without</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.2.2.6.1.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.2.2.6.1.2.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.2.2.6.1.2.1.1\">context</span></td>\n</tr>\n</table>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T5.1.1.2.2.7\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T5.1.1.2.2.7.1\">\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.2.2.7.1.1\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.2.2.7.1.1.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.2.2.7.1.1.1.1\">with</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.2.2.7.1.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.2.2.7.1.2.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.2.2.7.1.2.1.1\">context</span></td>\n</tr>\n</table></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T5.1.1.2.2.8\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T5.1.1.2.2.8.1\">\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.2.2.8.1.1\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.2.2.8.1.1.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.2.2.8.1.1.1.1\">without</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.2.2.8.1.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.2.2.8.1.2.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.2.2.8.1.2.1.1\">context</span></td>\n</tr>\n</table>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T5.1.1.2.2.9\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T5.1.1.2.2.9.1\">\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.2.2.9.1.1\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.2.2.9.1.1.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.2.2.9.1.1.1.1\">with</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.2.2.9.1.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.2.2.9.1.2.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.2.2.9.1.2.1.1\">context</span></td>\n</tr>\n</table></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T5.1.1.2.2.10\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T5.1.1.2.2.10.1\">\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.2.2.10.1.1\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.2.2.10.1.1.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.2.2.10.1.1.1.1\">without</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.2.2.10.1.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.2.2.10.1.2.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.2.2.10.1.2.1.1\">context</span></td>\n</tr>\n</table>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.3.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.3.1.1\" rowspan=\"3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" id=\"S5.T5.1.1.3.1.1.1\">Mean Pooling</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.3.1.2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Wav2vec2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.3.1.3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.3.1.4\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.3.1.5\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.3.1.6\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.3.1.7\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.3.1.8\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.3.1.9\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.3.1.10\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.3.1.11\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.3.1.12\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.00</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.4.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.4.2.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">WavLM</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.4.2.2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.07</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.4.2.3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.00</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.4.2.4\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.01</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.4.2.5\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.00</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.4.2.6\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.03</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.4.2.7\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.00</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.4.2.8\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.05</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.4.2.9\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.02</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.4.2.10\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.07</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.4.2.11\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.03</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.5.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.5.3.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">HuBERT</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.5.3.2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.17</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.5.3.3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.33</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.5.3.4\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.10</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.5.3.5\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.15</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.5.3.6\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.14</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.5.3.7\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.32</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.5.3.8\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.22</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.5.3.9\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.31</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.5.3.10\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.15</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.5.3.11\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.24</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.6.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.6.4.1\" rowspan=\"3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" id=\"S5.T5.1.1.6.4.1.1\">AE-RNN</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.6.4.2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Wav2vec2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.6.4.3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.6.4.4\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.6.4.5\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.6.4.6\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.6.4.7\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.6.4.8\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.6.4.9\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.6.4.10\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.6.4.11\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.6.4.12\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.00</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.7.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.7.5.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">WavLM</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.7.5.2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.34</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.7.5.3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.14</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.7.5.4\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.21</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.7.5.5\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.11</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.7.5.6\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.43</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.7.5.7\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.20</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.7.5.8\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.34</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.7.5.9\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.23</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.7.5.10\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.26</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.7.5.11\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.17</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.8.6\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.8.6.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">HuBERT</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.8.6.2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.44</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.8.6.3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.40</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.8.6.4\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.36</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.8.6.5\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.27</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.8.6.6\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.58</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.8.6.7\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.52</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.8.6.8\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.45</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.8.6.9\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.40</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.8.6.10\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.36</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.8.6.11\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.34</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.9.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S5.T5.1.1.9.7.1\" rowspan=\"3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" id=\"S5.T5.1.1.9.7.1.1\">CAE-RNN</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.9.7.2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Wav2vec2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.9.7.3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.86</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.9.7.4\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.9.7.5\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.86</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.9.7.6\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.63</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.9.7.7\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.93</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.9.7.8\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.79</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.9.7.9\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.9.7.10\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.9.7.11\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.82</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.9.7.12\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.52</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.10.8\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.10.8.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">WavLM</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.10.8.2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.86</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.10.8.3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.72</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.10.8.4\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.76</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.10.8.5\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.63</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.10.8.6\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.92</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.10.8.7\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.85</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.10.8.8\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.70</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.10.8.9\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.61</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.10.8.10\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.66</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.10.8.11\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.51</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.11.9\">\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T5.1.1.11.9.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">HuBERT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T5.1.1.11.9.2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.11.9.2.1\">0.90</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T5.1.1.11.9.3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.11.9.3.1\">0.82</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T5.1.1.11.9.4\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.11.9.4.1\">0.88</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T5.1.1.11.9.5\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.11.9.5.1\">0.71</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T5.1.1.11.9.6\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.11.9.6.1\">0.95</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T5.1.1.11.9.7\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.11.9.7.1\">0.89</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T5.1.1.11.9.8\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.11.9.8.1\">0.74</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T5.1.1.11.9.9\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.11.9.9.1\">0.65</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T5.1.1.11.9.10\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.11.9.10.1\">0.86</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T5.1.1.11.9.11\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.11.9.11.1\">0.65</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>AP scores for the word-discrimination task on the <span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.3.1\">test</span> set using SSL-based speech representations as input features for all five languages. AWE extraction methods include mean pooling <cite class=\"ltx_cite ltx_citemacro_cite\">Sanabria et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.08738v1#bib.bib23\" title=\"\">2023</a>)</cite>, AE-RNN, and CAE-RNN.</figcaption>\n</figure>",
            "capture": "Table 5: AP scores for the word-discrimination task on the test set using SSL-based speech representations as input features for all five languages. AWE extraction methods include mean pooling Sanabria et\u00a0al. (2023), AE-RNN, and CAE-RNN."
        },
        "6": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T6\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S5.T6.3\" style=\"width:433.6pt;height:211.3pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-10.3pt,5.0pt) scale(0.954465680863876,0.954465680863876) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T6.3.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T6.3.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T6.3.1.1.1.1\" rowspan=\"2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" id=\"S5.T6.3.1.1.1.1.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T6.3.1.1.1.1.1.1\">\n<span class=\"ltx_tr\" id=\"S5.T6.3.1.1.1.1.1.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.1.1.1.1.1.1.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.3.1.1.1.1.1.1.1.1.1\">AWE</span></span></span>\n<span class=\"ltx_tr\" id=\"S5.T6.3.1.1.1.1.1.1.2\">\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.1.1.1.1.1.2.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.3.1.1.1.1.1.1.2.1.1\">Extraction</span></span></span>\n<span class=\"ltx_tr\" id=\"S5.T6.3.1.1.1.1.1.1.3\">\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.1.1.1.1.1.3.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.3.1.1.1.1.1.1.3.1.1\">Method</span></span></span>\n</span></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T6.3.1.1.1.2\" rowspan=\"2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" id=\"S5.T6.3.1.1.1.2.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T6.3.1.1.1.2.1.1\">\n<span class=\"ltx_tr\" id=\"S5.T6.3.1.1.1.2.1.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.1.1.2.1.1.1.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.3.1.1.1.2.1.1.1.1.1\">Input</span></span></span>\n<span class=\"ltx_tr\" id=\"S5.T6.3.1.1.1.2.1.1.2\">\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.1.1.2.1.1.2.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.3.1.1.1.2.1.1.2.1.1\">Features</span></span></span>\n</span></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\" id=\"S5.T6.3.1.1.1.3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.3.1.1.1.3.1\">Polish</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\" id=\"S5.T6.3.1.1.1.4\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.3.1.1.1.4.1\">Portuguese</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\" id=\"S5.T6.3.1.1.1.5\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.3.1.1.1.5.1\">Spanish</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\" id=\"S5.T6.3.1.1.1.6\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.3.1.1.1.6.1\">French</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\" id=\"S5.T6.3.1.1.1.7\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.3.1.1.1.7.1\">English</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.3.1.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T6.3.1.2.2.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T6.3.1.2.2.1.1\">\n<tr class=\"ltx_tr\" id=\"S5.T6.3.1.2.2.1.1.1\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.2.2.1.1.1.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.3.1.2.2.1.1.1.1.1\">with</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.3.1.2.2.1.1.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.2.2.1.1.2.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.3.1.2.2.1.1.2.1.1\">context</span></td>\n</tr>\n</table></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T6.3.1.2.2.2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T6.3.1.2.2.2.1\">\n<tr class=\"ltx_tr\" id=\"S5.T6.3.1.2.2.2.1.1\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.2.2.2.1.1.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.3.1.2.2.2.1.1.1.1\">without</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.3.1.2.2.2.1.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.2.2.2.1.2.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.3.1.2.2.2.1.2.1.1\">context</span></td>\n</tr>\n</table>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T6.3.1.2.2.3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T6.3.1.2.2.3.1\">\n<tr class=\"ltx_tr\" id=\"S5.T6.3.1.2.2.3.1.1\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.2.2.3.1.1.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.3.1.2.2.3.1.1.1.1\">with</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.3.1.2.2.3.1.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.2.2.3.1.2.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.3.1.2.2.3.1.2.1.1\">context</span></td>\n</tr>\n</table></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T6.3.1.2.2.4\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T6.3.1.2.2.4.1\">\n<tr class=\"ltx_tr\" id=\"S5.T6.3.1.2.2.4.1.1\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.2.2.4.1.1.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.3.1.2.2.4.1.1.1.1\">without</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.3.1.2.2.4.1.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.2.2.4.1.2.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.3.1.2.2.4.1.2.1.1\">context</span></td>\n</tr>\n</table>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T6.3.1.2.2.5\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T6.3.1.2.2.5.1\">\n<tr class=\"ltx_tr\" id=\"S5.T6.3.1.2.2.5.1.1\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.2.2.5.1.1.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.3.1.2.2.5.1.1.1.1\">with</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.3.1.2.2.5.1.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.2.2.5.1.2.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.3.1.2.2.5.1.2.1.1\">context</span></td>\n</tr>\n</table></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T6.3.1.2.2.6\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T6.3.1.2.2.6.1\">\n<tr class=\"ltx_tr\" id=\"S5.T6.3.1.2.2.6.1.1\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.2.2.6.1.1.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.3.1.2.2.6.1.1.1.1\">without</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.3.1.2.2.6.1.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.2.2.6.1.2.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.3.1.2.2.6.1.2.1.1\">context</span></td>\n</tr>\n</table>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T6.3.1.2.2.7\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T6.3.1.2.2.7.1\">\n<tr class=\"ltx_tr\" id=\"S5.T6.3.1.2.2.7.1.1\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.2.2.7.1.1.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.3.1.2.2.7.1.1.1.1\">with</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.3.1.2.2.7.1.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.2.2.7.1.2.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.3.1.2.2.7.1.2.1.1\">context</span></td>\n</tr>\n</table></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T6.3.1.2.2.8\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T6.3.1.2.2.8.1\">\n<tr class=\"ltx_tr\" id=\"S5.T6.3.1.2.2.8.1.1\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.2.2.8.1.1.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.3.1.2.2.8.1.1.1.1\">without</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.3.1.2.2.8.1.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.2.2.8.1.2.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.3.1.2.2.8.1.2.1.1\">context</span></td>\n</tr>\n</table>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T6.3.1.2.2.9\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T6.3.1.2.2.9.1\">\n<tr class=\"ltx_tr\" id=\"S5.T6.3.1.2.2.9.1.1\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.2.2.9.1.1.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.3.1.2.2.9.1.1.1.1\">with</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.3.1.2.2.9.1.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.2.2.9.1.2.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.3.1.2.2.9.1.2.1.1\">context</span></td>\n</tr>\n</table></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T6.3.1.2.2.10\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T6.3.1.2.2.10.1\">\n<tr class=\"ltx_tr\" id=\"S5.T6.3.1.2.2.10.1.1\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.2.2.10.1.1.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.3.1.2.2.10.1.1.1.1\">without</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.3.1.2.2.10.1.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.2.2.10.1.2.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.3.1.2.2.10.1.2.1.1\">context</span></td>\n</tr>\n</table>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T6.3.1.3.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.3.1.3.1.1\" rowspan=\"3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" id=\"S5.T6.3.1.3.1.1.1\">Mean Pooling</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.3.1.3.1.2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Wav2vec2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.3.1.3.1.3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.3.1.3.1.4\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.3.1.3.1.5\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.3.1.3.1.6\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.3.1.3.1.7\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.3.1.3.1.8\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.3.1.3.1.9\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.06</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.3.1.3.1.10\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.3.1.3.1.11\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.3.1.3.1.12\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.00</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.3.1.4.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.4.2.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">WavLM</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.4.2.2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.05</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.4.2.3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.00</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.4.2.4\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.02</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.4.2.5\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.01</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.4.2.6\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.05</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.4.2.7\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.01</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.4.2.8\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.10</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.4.2.9\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.05</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.4.2.10\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.09</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.4.2.11\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.04</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.3.1.5.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.5.3.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">HuBERT</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.5.3.2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.18</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.5.3.3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.31</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.5.3.4\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.11</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.5.3.5\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.17</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.5.3.6\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.19</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.5.3.7\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.40</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.5.3.8\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.30</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.5.3.9\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.40</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.5.3.10\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.18</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.5.3.11\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.29</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.3.1.6.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.3.1.6.4.1\" rowspan=\"3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" id=\"S5.T6.3.1.6.4.1.1\">AE-RNN</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.3.1.6.4.2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Wav2vec2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.3.1.6.4.3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.3.1.6.4.4\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.3.1.6.4.5\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.3.1.6.4.6\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.3.1.6.4.7\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.3.1.6.4.8\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.3.1.6.4.9\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.3.1.6.4.10\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.3.1.6.4.11\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.3.1.6.4.12\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.01</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.3.1.7.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.7.5.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">WavLM</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.7.5.2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.33</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.7.5.3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.12</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.7.5.4\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.25</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.7.5.5\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.12</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.7.5.6\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.55</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.7.5.7\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.28</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.7.5.8\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.44</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.7.5.9\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.33</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.7.5.10\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.29</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.7.5.11\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.23</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.3.1.8.6\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.8.6.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">HuBERT</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.8.6.2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.44</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.8.6.3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.41</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.8.6.4\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.41</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.8.6.5\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.31</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.8.6.6\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.69</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.8.6.7\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.64</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.8.6.8\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.55</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.8.6.9\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.50</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.8.6.10\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.43</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.8.6.11\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.41</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.3.1.9.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S5.T6.3.1.9.7.1\" rowspan=\"3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" id=\"S5.T6.3.1.9.7.1.1\">CAE-RNN</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.3.1.9.7.2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Wav2vec2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.3.1.9.7.3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.87</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.3.1.9.7.4\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.3.1.9.7.5\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.3.1.9.7.6\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.3.1.9.7.7\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.96</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.3.1.9.7.8\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.3.1.9.7.9\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.3.1.9.7.10\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.3.1.9.7.11\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.89</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.3.1.9.7.12\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.67</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.3.1.10.8\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.10.8.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">WavLM</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.10.8.2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.85</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.10.8.3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.72</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.10.8.4\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.81</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.10.8.5\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.68</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.10.8.6\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.95</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.10.8.7\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.90</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.10.8.8\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.83</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.10.8.9\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.75</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.10.8.10\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.75</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.3.1.10.8.11\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.62</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.3.1.11.9\">\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T6.3.1.11.9.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">HuBERT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T6.3.1.11.9.2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.3.1.11.9.2.1\">0.90</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T6.3.1.11.9.3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.3.1.11.9.3.1\">0.83</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T6.3.1.11.9.4\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.3.1.11.9.4.1\">0.91</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T6.3.1.11.9.5\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.3.1.11.9.5.1\">0.75</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T6.3.1.11.9.6\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.3.1.11.9.6.1\">0.97</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T6.3.1.11.9.7\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.3.1.11.9.7.1\">0.93</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T6.3.1.11.9.8\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.3.1.11.9.8.1\">0.86</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T6.3.1.11.9.9\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.3.1.11.9.9.1\">0.81</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T6.3.1.11.9.10\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.3.1.11.9.10.1\">0.93</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T6.3.1.11.9.11\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.3.1.11.9.11.1\">0.75</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 6: </span>AP scores for the word-discrimination task on the  set using SSL-based speech representations as input features for all five languages. AWE extraction methods include mean pooling <cite class=\"ltx_cite ltx_citemacro_cite\">Sanabria et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.08738v1#bib.bib23\" title=\"\">2023</a>)</cite>, AE-RNN, and CAE-RNN..</figcaption>\n</figure>",
            "capture": "Table 6: AP scores for the word-discrimination task on the  set using SSL-based speech representations as input features for all five languages. AWE extraction methods include mean pooling Sanabria et\u00a0al. (2023), AE-RNN, and CAE-RNN.."
        },
        "7": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T7\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S5.T7.4\" style=\"width:433.6pt;height:231.6pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(115.7pt,-61.8pt) scale(2.14426945171415,2.14426945171415) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T7.4.4\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T7.4.4.5.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T7.4.4.5.1.1\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S5.T7.4.4.5.1.1.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T7.4.4.5.1.1.1.1\">\n<span class=\"ltx_tr\" id=\"S5.T7.4.4.5.1.1.1.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.4.5.1.1.1.1.1.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.4.4.5.1.1.1.1.1.1.1\">Input</span></span></span>\n<span class=\"ltx_tr\" id=\"S5.T7.4.4.5.1.1.1.1.2\">\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.4.5.1.1.1.1.2.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.4.4.5.1.1.1.1.2.1.1\">Features</span></span></span>\n</span></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\" id=\"S5.T7.4.4.5.1.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.4.4.5.1.2.1\">Polish</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\" id=\"S5.T7.4.4.5.1.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.4.4.5.1.3.1\">Portuguese</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\" id=\"S5.T7.4.4.5.1.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.4.4.5.1.4.1\">Spanish</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\" id=\"S5.T7.4.4.5.1.5\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.4.4.5.1.5.1\">French</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.4.4.4\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T7.4.4.4.5\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.4.4.4.5.1\">test</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T7.1.1.1.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S5.T7.1.1.1.1.1\"></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T7.4.4.4.6\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.4.4.4.6.1\">test</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T7.2.2.2.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S5.T7.2.2.2.2.1\"></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T7.4.4.4.7\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.4.4.4.7.1\">test</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T7.3.3.3.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S5.T7.3.3.3.3.1\"></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T7.4.4.4.8\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.4.4.4.8.1\">test</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T7.4.4.4.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S5.T7.4.4.4.4.1\"></span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T7.4.4.6.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T7.4.4.6.1.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Wav2vec2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T7.4.4.6.1.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T7.4.4.6.1.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T7.4.4.6.1.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.48</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T7.4.4.6.1.5\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T7.4.4.6.1.6\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T7.4.4.6.1.7\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T7.4.4.6.1.8\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.4.4.6.1.8.1\">0.52</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T7.4.4.6.1.9\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.4.4.6.1.9.1\">0.68</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.4.4.7.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.4.7.2.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">WavLM</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.4.7.2.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.48</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.4.7.2.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.47</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.4.7.2.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.36</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.4.7.2.5\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.40</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.4.7.2.6\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.54</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.4.7.2.7\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.63</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.4.7.2.8\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.50</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.4.7.2.9\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.64</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.4.4.8.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.4.8.3.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">HuBERT</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.4.8.3.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.4.4.8.3.2.1\">0.59</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.4.8.3.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.4.4.8.3.3.1\">0.60</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.4.8.3.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.4.4.8.3.4.1\">0.50</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.4.8.3.5\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.4.4.8.3.5.1\">0.56</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.4.8.3.6\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.4.4.8.3.6.1\">0.62</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.4.8.3.7\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.4.4.8.3.7.1\">0.69</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.4.8.3.8\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.48</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.4.8.3.9\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.65</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.4.4.9.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T7.4.4.9.4.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MFCC</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T7.4.4.9.4.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T7.4.4.9.4.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T7.4.4.9.4.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T7.4.4.9.4.5\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T7.4.4.9.4.6\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T7.4.4.9.4.7\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T7.4.4.9.4.8\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T7.4.4.9.4.9\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.35</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 7: </span>AP for the word-discrimination task with CAE-RNN model trained on English language with various input features and tested on other four languages.</figcaption>\n</figure>",
            "capture": "Table 7: AP for the word-discrimination task with CAE-RNN model trained on English language with various input features and tested on other four languages."
        },
        "8": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T8\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S5.T8.1\" style=\"width:433.6pt;height:247.3pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(85.8pt,-48.9pt) scale(1.65503516125337,1.65503516125337) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T8.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T8.1.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T8.1.1.1.1.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.1.1.1.1.1.1\">Word1</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T8.1.1.1.1.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.1.1.1.1.2.1\">Word2</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T8.1.1.1.1.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T8.1.1.1.1.3.1\">\n<tr class=\"ltx_tr\" id=\"S5.T8.1.1.1.1.3.1.1\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.1.1.1.1.3.1.1.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.1.1.1.1.3.1.1.1.1\">Cosine Distance</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.1.1.1.1.3.1.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.1.1.1.1.3.1.2.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.1.1.1.1.3.1.2.1.1\">(Mean Pooling)</span></td>\n</tr>\n</table>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T8.1.1.1.1.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T8.1.1.1.1.4.1\">\n<tr class=\"ltx_tr\" id=\"S5.T8.1.1.1.1.4.1.1\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.1.1.1.1.4.1.1.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.1.1.1.1.4.1.1.1.1\">Cosine Distance</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.1.1.1.1.4.1.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.1.1.1.1.4.1.2.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.1.1.1.1.4.1.2.1.1\">(CAE-RNN)</span></td>\n</tr>\n</table>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T8.1.1.1.1.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.1.1.1.1.5.1\">Description</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T8.1.1.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T8.1.1.2.1.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">aside</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T8.1.1.2.1.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">aside</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T8.1.1.2.1.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T8.1.1.2.1.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T8.1.1.2.1.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Same word</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.1.1.3.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.1.1.3.2.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">aside</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.1.1.3.2.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">ideas</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.1.1.3.2.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.56</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.1.1.3.2.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.99</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.1.1.3.2.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Anagram pair</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.1.1.4.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.1.1.4.3.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">this</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.1.1.4.3.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">this</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.1.1.4.3.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.33</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.1.1.4.3.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.11</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.1.1.4.3.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Same word</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.1.1.5.4\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.1.1.5.4.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">this</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.1.1.5.4.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">hits</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.1.1.5.4.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.38</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.1.1.5.4.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.50</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.1.1.5.4.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Anagram pair</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.1.1.6.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.1.1.6.5.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">no</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.1.1.6.5.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">no</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.1.1.6.5.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.30</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.1.1.6.5.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.02</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.1.1.6.5.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Same word</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.1.1.7.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T8.1.1.7.6.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">no</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T8.1.1.7.6.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">on</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T8.1.1.7.6.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.63</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T8.1.1.7.6.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T8.1.1.7.6.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Anagram pair</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 8: </span>Comparison of cosine distances between AWEs of same\nspoken word pairs and anagram pairs. HuBERT features are\nused for both CAE-RNN and mean pooling method.</figcaption>\n</figure>",
            "capture": "Table 8: Comparison of cosine distances between AWEs of same\nspoken word pairs and anagram pairs. HuBERT features are\nused for both CAE-RNN and mean pooling method."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.08738v1_figure_1.png",
            "caption": "Figure 1: CAE-RNN training setup for extracting AWEs Kamper (2019)."
        },
        "2": {
            "figure_path": "2403.08738v1_figure_2.png",
            "caption": "Figure 2: t-SNE visualisation of the AWEs derived from HuBERT-based CAE-RNN model for all five languages. From each language, all spoken instances of the top 7 words with\nthe highest frequency count from the test set are chosen."
        },
        "3": {
            "figure_path": "2403.08738v1_figure_3.png",
            "caption": "Figure 2: t-SNE visualisation of the AWEs derived from HuBERT-based CAE-RNN model for all five languages. From each language, all spoken instances of the top 7 words with\nthe highest frequency count from the test set are chosen."
        },
        "4": {
            "figure_path": "2403.08738v1_figure_4.png",
            "caption": "Figure 2: t-SNE visualisation of the AWEs derived from HuBERT-based CAE-RNN model for all five languages. From each language, all spoken instances of the top 7 words with\nthe highest frequency count from the test set are chosen."
        },
        "5": {
            "figure_path": "2403.08738v1_figure_5.png",
            "caption": "Figure 2: t-SNE visualisation of the AWEs derived from HuBERT-based CAE-RNN model for all five languages. From each language, all spoken instances of the top 7 words with\nthe highest frequency count from the test set are chosen."
        },
        "6": {
            "figure_path": "2403.08738v1_figure_6.png",
            "caption": "Figure 2: t-SNE visualisation of the AWEs derived from HuBERT-based CAE-RNN model for all five languages. From each language, all spoken instances of the top 7 words with\nthe highest frequency count from the test set are chosen."
        }
    },
    "references": [
        {
            "1": {
                "title": "How familiar does that sound? cross-lingual representational similarity analysis of acoustic word embeddings.",
                "author": "Badr Abdullah, Iuliia Zaitova, Tania Avgustinova, Bernd M\u00f6bius, and Dietrich Klakow. 2021a.",
                "venue": "In Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 407\u2013419, Punta Cana, Dominican Republic. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.blackboxnlp-1.32"
            }
        },
        {
            "2": {
                "title": "Do Acoustic Word Embeddings Capture Phonological Similarity? An Empirical Study.",
                "author": "Badr M. Abdullah, Marius Mosbach, Iuliia Zaitova, Bernd M\u00f6bius, and Dietrich Klakow. 2021b.",
                "venue": "In Proc. Interspeech 2021, pages 4194\u20134198.",
                "url": "https://doi.org/10.21437/Interspeech.2021-678"
            }
        },
        {
            "3": {
                "title": "Integrating Form and Meaning: A Multi-Task Learning Model for Acoustic Word Embeddings.",
                "author": "Badr M. Abdullah, Bernd M\u00f6bius, and Dietrich Klakow. 2022.",
                "venue": "In Proc. Interspeech 2022, pages 1876\u20131880.",
                "url": "https://doi.org/10.21437/Interspeech.2022-626"
            }
        },
        {
            "4": {
                "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations.",
                "author": "Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. 2020.",
                "venue": "In Advances in Neural Information Processing Systems, volume 33, pages 12449\u201312460. Curran Associates, Inc.",
                "url": "https://proceedings.neurips.cc/paper_files/paper/2020/file/92d1e1eb1cd6f9fba3227870bb6d7f07-Paper.pdf"
            }
        },
        {
            "5": {
                "title": "Keyword spotting based on the analysis of template matching distances.",
                "author": "M. S. Barakat, C. H. Ritz, and D. A. Stirling. 2011.",
                "venue": "In 2011 5th International Conference on Signal Processing and Communication Systems (ICSPCS), pages 1\u20136.",
                "url": "https://doi.org/10.1109/ICSPCS.2011.6140822"
            }
        },
        {
            "6": {
                "title": "Rapid evaluation of speech representations for spoken term discovery.",
                "author": "Michael A. Carlin, Samuel Thomas, Aren Jansen, and Hynek Hermansky. 2011.",
                "venue": "In Proc. Interspeech 2011, pages 821\u2013824.",
                "url": "https://doi.org/10.21437/Interspeech.2011-304"
            }
        },
        {
            "7": {
                "title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing.",
                "author": "Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Xiangzhan Yu, and Furu Wei. 2022.",
                "venue": "IEEE Journal of Selected Topics in Signal Processing, 16(6):1505\u20131518.",
                "url": "https://www.microsoft.com/en-us/research/publication/wavlm-large-scale-self-supervised-pre-training-for-full-stack-speech-processing/"
            }
        },
        {
            "8": {
                "title": "Unsupervised cross-lingual representation learning for speech recognition.",
                "author": "Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdel rahman Mohamed, and Michael Auli. 2020.",
                "venue": "In Interspeech.",
                "url": null
            }
        },
        {
            "9": {
                "title": "Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences.",
                "author": "S Davis and P Mermelstein. 1980.",
                "venue": "IEEE Trans. Acoust., Speech, Signal Process., 28(4):357\u2013366.",
                "url": null
            }
        },
        {
            "10": {
                "title": "Multi-view recurrent neural acoustic word embeddings.",
                "author": "Wanjia He, Weiran Wang, and Karen Livescu. 2017.",
                "venue": "In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units.",
                "author": "Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. 2021.",
                "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:3451\u20133460.",
                "url": null
            }
        },
        {
            "12": {
                "title": "Acoustic span embeddings for multilingual query-by-example search.",
                "author": "Yushi Hu, Shane Settle, and Karen Livescu. 2021.",
                "venue": "In 2021 IEEE Spoken Language Technology Workshop (SLT), pages 935\u2013942.",
                "url": "https://doi.org/10.1109/SLT48900.2021.9383545"
            }
        },
        {
            "13": {
                "title": "Towards hate speech detection in low-resource languages: Comparing asr to acoustic word embeddings on wolof and swahili.",
                "author": "Christiaan Jacobs, Nathana\u00ebl Carraz Rakotonirina, Everlyn Asiko Chimoto, Bruce A. Bassett, and Herman Kamper. 2023.",
                "venue": "ArXiv, abs/2306.00410.",
                "url": null
            }
        },
        {
            "14": {
                "title": "Truly unsupervised acoustic word embeddings using weak top-down constraints in encoder-decoder models.",
                "author": "Herman Kamper. 2019.",
                "venue": "ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6535\u20133539.",
                "url": null
            }
        },
        {
            "15": {
                "title": "Unsupervised neural network based feature extraction using weak top-down constraints.",
                "author": "Herman Kamper, Micha Elsner, Aren Jansen, and Sharon Goldwater. 2015.",
                "venue": "In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5818\u20135822.",
                "url": "https://doi.org/10.1109/ICASSP.2015.7179087"
            }
        },
        {
            "16": {
                "title": "Improved acoustic word embeddings for zero-resource languages using multilingual transfer.",
                "author": "Herman Kamper, Yevgen Matusevych, and Sharon Goldwater. 2021.",
                "venue": "IEEE/ACM Trans. Audio, Speech and Lang. Proc., 29:1107\u20131118.",
                "url": "https://doi.org/10.1109/TASLP.2021.3060805"
            }
        },
        {
            "17": {
                "title": "Self-Supervised Acoustic Word Embedding Learning via Correspondence Transformer Encoder.",
                "author": "Jingru Lin, Xianghu Yue, Junyi Ao, and Haizhou Li. 2023.",
                "venue": "In Proc. INTERSPEECH 2023, pages 2988\u20132992.",
                "url": "https://doi.org/10.21437/Interspeech.2023-359"
            }
        },
        {
            "18": {
                "title": "Analyzing autoencoder-based acoustic word embeddings.",
                "author": "Yevgen Matusevych, Herman Kamper, and Sharon Goldwater. 2020.",
                "venue": "CoRR, abs/2004.01647.",
                "url": null
            }
        },
        {
            "19": {
                "title": "Montreal forced aligner: Trainable text-speech alignment using kaldi.",
                "author": "Michael McAuliffe, Michaela Socolof, Sarah Mihuc, Michael Wagner, and Morgan Sonderegger. 2017.",
                "venue": "In Interspeech.",
                "url": null
            }
        },
        {
            "20": {
                "title": "Score: Self-supervised correspondence fine-tuning for improved content representations.",
                "author": "Amit Meghanani and Thomas Hain. 2024.",
                "venue": "In ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).",
                "url": null
            }
        },
        {
            "21": {
                "title": "Librispeech: An asr corpus based on public domain audio books.",
                "author": "Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015.",
                "venue": "In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5206\u20135210.",
                "url": "https://doi.org/10.1109/ICASSP.2015.7178964"
            }
        },
        {
            "22": {
                "title": "Mls: A large-scale multilingual dataset for speech research.",
                "author": "Vineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan Collobert. 2020.",
                "venue": "Interspeech 2020.",
                "url": "https://doi.org/10.21437/interspeech.2020-2826"
            }
        },
        {
            "23": {
                "title": "Analyzing acoustic word embeddings from pre-trained self-supervised speech models.",
                "author": "Ramon Sanabria, Hao Tang, and Sharon Goldwater. 2023.",
                "venue": "In ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1\u20135.",
                "url": "https://doi.org/10.1109/ICASSP49357.2023.10096099"
            }
        },
        {
            "24": {
                "title": "Globalphone: A multilingual text & speech database in 20 languages.",
                "author": "Tanja Schultz, Ngoc Thang Vu, and Tim Schlippe. 2013.",
                "venue": "In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 8126\u20138130.",
                "url": "https://doi.org/10.1109/ICASSP.2013.6639248"
            }
        },
        {
            "25": {
                "title": "Query-by-example search with discriminative neural acoustic word embeddings.",
                "author": "Shane Settle, Keith Levin, Herman Kamper, and Karen Livescu. 2017.",
                "venue": "In INTERSPEECH.",
                "url": null
            }
        },
        {
            "26": {
                "title": "Learning audio-text agreement for open-vocabulary keyword spotting.",
                "author": "Hyeon-Kyeong Shin, Hyewon Han, DoYeon Kim, Soo-Whan Chung, and Hong-Goo Kang. 2022.",
                "venue": "In Interspeech.",
                "url": null
            }
        },
        {
            "27": {
                "title": "Superb: Speech processing universal performance benchmark.",
                "author": "Shu-Wen Yang, Po-Han Chi, Yung-Sung Chuang, Cheng-I Lai, Kushal Lakhotia, Yist Y. Lin, Andy T. Liu, Jiatong Shi, Xuankai Chang, Guan-Ting Lin, Tzu hsien Huang, Wei-Cheng Tseng, Ko tik Lee, Da-Rong Liu, Zili Huang, Shuyan Dong, Shang-Wen Li, Shinji Watanabe, Abdel rahman Mohamed, and Hung yi Lee. 2021.",
                "venue": "In Interspeech.",
                "url": null
            }
        },
        {
            "28": {
                "title": "Learning Acoustic Word Embeddings with Temporal Context for Query-by-Example Speech Search.",
                "author": "Yougen Yuan, Cheung-Chi Leung, Lei Xie, Hongjie Chen, Bin Ma, and Haizhou Li. 2018.",
                "venue": "In Proc. Interspeech 2018, pages 97\u2013101.",
                "url": "https://doi.org/10.21437/Interspeech.2018-1010"
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.08738v1",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "2",
            "4.2"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "5",
            "5.1"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "5",
            "5.1"
        ]
    },
    "research_context": {
        "paper_id": "2403.08738v1",
        "paper_title": "Improving Acoustic Word Embeddings through Correspondence Training of Self-supervised Speech Representations",
        "research_background": "The paper \"Improving Acoustic Word Embeddings through Correspondence Training of Self-supervised Speech Representations\" is motivated by the rapid advancements in self-supervised learning (SSL)-based speech representations, which have demonstrated state-of-the-art performance in numerous speech processing tasks. Despite their broad applicability, these representations have not been deeply explored for creating acoustic word embeddings (AWEs). AWEs are critical for tasks such as query-by-example search, keyword spotting, and even more specialized tasks like hate speech detection in low-resource settings. \n\nThe research problem addressed in this paper is how to harness the powerful, contextualized features of SSL-based speech representations to generate robust and discriminative acoustic word embeddings through correspondence training. Traditional correspondence training has shown significant promise, particularly using MFCC features, but integrating this with SSL-based features opens new opportunities for improving AWEs.\n\nRelevant prior work includes:\n1. The development of key SSL-based speech models such as Wav2vec2, HuBERT, and WavLM, which have revolutionized speech processing by utilizing large amounts of unlabelled data to create effective speech representations.\n2. Sanabria et al. (2023), who explored extracting AWEs from SSL-based representations using a mean pooling mechanism.\n3. Kamper (2019), who demonstrated the effectiveness of using Correspondence Auto-Encoder (CAE) with MFCC features to maintain acoustic-phonetic information while discarding irrelevant details.\n4. Meghanani and Hain (2024), who improved SSL-based speech models' content representations using correspondence training.\n5. Lin et al. (2023), who utilized a Correspondence Transformer Encoder (CTE) to obtain robust AWEs from scratch and unlabelled speech data.\n\nThis paper advances prior work by utilizing pre-trained SSL models coupled with a simple RNN-based auto-encoder for correspondence training to derive robust AWEs. It further investigates cross-lingual capabilities of these models and examines the impact of contextual information in the derived embeddings. The study relies on a subset of the Multilingual LibriSpeech (MLS) dataset for its multilingual experiments.\n\nThe key contributions of the paper are:\n1. Utilizing correspondence training combined with SSL-based speech representations to derive highly discriminative AWEs.\n2. Demonstrating the effectiveness of SSL models pre-trained on English for creating high-quality AWEs in cross-lingual scenarios.\n3. Quantitatively showing that contextual information in SSL-based speech representations is crucial for generating robust AWEs.",
        "methodology": "The proposed method in the paper focuses on improving Acoustic Word Embeddings (AWEs) by utilizing a Correspondence Autoencoder (CAE) for training self-supervised speech representations. The key components and innovations of the method are summarized as follows:\n\n### Components:\n1. **CAE-RNN Model:**\n   - **Encoder (ENC):** Takes as input a sequence of acoustic feature vectors of a spoken word.\n   - **Decoder:** Takes the encoded representation from the encoder and reconstructs the target sequence.\n   - **Output of Decoder:** The reconstructed sequence of acoustic feature vectors.\n\n2. **Training Input and Target:**\n   - **Input:** A spoken word represented as a sequence of acoustic feature vectors.\n   - **Target Output:** A different instance of the same spoken word, represented as another sequence of acoustic feature vectors.\n\n3. **Acoustic Word Embedding (AWE):**\n   - The encoder produces an AWE for the input spoken word, which is used by the decoder to reconstruct the target sequence.\n\n### Innovations:\n1. **Correspondence Training Approach:**\n   - The input and target output are different instances of the same spoken word, ensuring that the model focuses on preserving acoustic-phonetic information rather than irrelevant factors such as speaker differences, duration, or acoustic environment.\n\n2. **Loss Function:**\n   - The mean squared loss function is used to measure the difference between the target output and the decoder's reconstructed output for a training pair.\n\n3. **Filtering Out Unnecessary Information:**\n   - The CAE-RNN method aims to filter out extraneous factors not pertinent to the spoken words\u2019 acoustic-phonetic information, such as speaker variation and environmental noise.\n\nBy concentrating on encoding and reconstructing different instances of the same word, this method ensures that the learned representations are robust and focused on essential phonetic information, leading to potentially improved AWEs.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\n**Setup:**\n\n1. **Datasets and Languages:** Experiments are conducted on five different languages.\n   \n2. **Input Features:**\n   - **SSL-based speech representations:**\n     - Wav2vec2\n     - HuBERT\n     - WavLM\n   - **Traditional features:**\n     - MFCC (Mel-frequency cepstral coefficients)\n\n3. **Models and Baselines:**\n   - **CAE-RNN model:** This is the primary model under investigation.\n   - **Baselines:**\n     - **Mean pooling baseline:** Sanabria et al. (2023) \n     - **AE-RNN method:** Similar to CAE-RNN but without correspondence training.\n\n4. **Evaluation Task:**\n   - **Word Discrimination Task:** This is utilized to evaluate the quality of the Acoustic Word Embeddings (AWEs).\n\n5. **Training Details:**\n   - The training specifics for the CAE-RNN and other models are thoroughly described to ensure replicability and clarity.\n\n**Main Experimental Results:**\n\nThe results of the main experiments are not explicitly detailed in this summary. To fully understand and interpret the impact and efficacy of the CAE-RNN model compared to the baselines, one would need to look into the paper's results section."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To explore the effectiveness of Correspondence Auto-Encoders (CAE) with self-supervised learning (SSL)-based speech representations in obtaining improved Acoustic Word Embeddings (AWEs), and to investigate their performance in cross-lingual scenarios.",
            "experiment_process": "Experiments are conducted on five languages (Polish, Portuguese, Spanish, French, and English) with SSL-based speech representations (Wav2vec2, HuBERT, WavLM) as input features. MFCC features are also used for comparison. Feature extraction methods for SSL-based speech representations and MFCCs are described, followed by the configuration of the CAE-RNN model, mean pooling baseline, and AE-RNN method. The word discrimination task is used for evaluating AWE quality. The experiments include different extraction setups ('with context' and 'without context').",
            "result_discussion": "The CAE-RNN model consistently outperforms the AE-RNN model on the word discrimination task across all languages. When using SSL-based speech representations 'with context,' the performance is significantly better than 'without context.' HuBERT features achieve the best performance, especially when using the CAE-RNN method for AWE extraction. Specifically, HuBERT achieves the highest AP on the test set: 0.90 for Polish, 0.88 for Portuguese, 0.95 for Spanish, 0.74 for French, and 0.86 for English. The performance is ordered as HuBERT > Wav2vec2 > WavLM > MFCCs when using the CAE-RNN model 'with context.' Additionally, the trained CAE-RNN models exhibit robustness on unseen words and speakers.",
            "ablation_id": "2403.08738v1.No1"
        },
        {
            "research_objective": "To evaluate the performance of SSL-based CAE-RNN models in cross-lingual settings and determine their effectiveness in 'zero-shot' scenarios.",
            "experiment_process": "A CAE-RNN model trained on one source language (English) is evaluated on four target languages (Polish, Portuguese, Spanish, and French) to assess zero-shot performance. AP for the word-discrimination task is computed for both the test set and a derived subset of the test set containing unseen words and speakers. Results are compared against the mean pooling method and CAE-RNN models trained on target languages using MFCC features.",
            "result_discussion": "In the cross-lingual evaluation, the HuBERT-based CAE-RNN model achieves the best performance across most languages except for French, where Wav2vec2 performs best. The CAE-RNN model in the 'zero-shot' setting outperforms both the mean pooling method and CAE-RNN models trained on target languages using MFCC features. This indicates that the SSL-based CAE-RNN models have a considerable capacity for cross-lingual generalization, making them suitable for applications in speech search, indexing, and discovery systems for low-resource languages.",
            "ablation_id": "2403.08738v1.No2"
        }
    ]
}