{
    "title": "LastResort at SemEval-2024 Task 3: Exploring Multimodal Emotion Cause Pair Extraction as Sequence Labelling Task",
    "abstract": "Conversation is the most natural form of human communication, where each utterance can range over a variety of possible emotions. While significant work has been done towards the detection of emotions in text, relatively little work has been done towards finding the cause of the said emotions, especially in multimodal settings. SemEval 2024 introduces the task of Multimodal Emotion Cause Analysis in Conversations, which aims to extract emotions reflected in individual utterances in a conversation involving multiple modalities (textual, audio, and visual modalities) along with the corresponding utterances that were the cause for the emotion. In this paper, we propose models that tackle this task as an utterance labeling and a sequence labeling problem and perform a comparative study of these models, involving baselines using different encoders, using BiLSTM for adding contextual information of the conversation, and finally adding a CRF layer to try to model the inter-dependencies between adjacent utterances more effectively. In the official leaderboard for the task, our architecture was ranked 8th, achieving an F1-score of 0.1759 on the leaderboard. We also release our code here111github.com/akshettrj/semeval2024_task03.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Emotion Analysis is one of the fundamental and earliest sub-fields of NLP that focus on identifying and categorizing emotions that are expressed in text. Earlier, research in this domain focused on Emotion Detection in news articles and headlines (Lei et al., 2014  ###reference_b20###; Abdul-Mageed and Ungar, 2017  ###reference_b1###). However, later Emotion Recognition in Conversation gained popularity due to the widespread availability of public conversation data (Gupta et al., 2017  ###reference_b14###). Recently, the task of emotion cause analysis has gained traction, which tries to identify the causes behind certain emotions (Xia and Ding, 2019a  ###reference_b35###). This has widespread application such as building chatbots that can identify the emotions of the user and even identify the cause behind the emotions to perform certain actions (Pamungkas, 2019  ###reference_b26###). For instance, companies can identify causes behind dissatisfaction in customer interactions and take appropriate measures (Yun and Park, 2022  ###reference_b37###), AI-driven therapeutic insights can be gained using such models (D\u2019Alfonso, 2020  ###reference_b10###), social media content moderation can be better done (Sawhney et al., 2021  ###reference_b30###), work management and team management by companies can be improved (Benke et al., 2020  ###reference_b3###).\nIn the task Wang et al., 2024  ###reference_b33###, we tackle the problem of Multimodal Emotion Cause Pair Extraction, where given a set of utterances in a conversation, we must identify the following:\n1. Emotion of every utterance (if any). These emotions can be one of Ekman\u2019s six basic emotions (Ekman et al., 1999  ###reference_b11###).\n2. Cause of these emotions, which is considered as the utterance that explicitly expresses an event or argument that is highly linked to the corresponding emotion.\nOur proposed system tackles the task in a 3-step fashion \u2013 (a) First, we train a model to identify the emotions that are expressed in individual utterances in a conversation. (b) Next, we train a model to identify whether an utterance can be a cause of an emotion expressed in another/same utterance (candidate causes). (c) Finally, we train a model to pair emotion-utterances with their causes among the possible candidate causes. For both the (a) and (b) models we experiment with 3 basic architectures \u2013 (i) a simple Neural Network to determine the class of emotion (N-class classifier) and another Neural Network to identify whether the utterance is a candidate cause or not (binary classification). (ii) A BiLSTM (Sak et al., 2014  ###reference_b29###) architecture that accounts for the surrounding context of the conversation while doing the N-class and binary-classification. (iii) A BiLSTM CRF (Lafferty et al., 2001  ###reference_b18###) architecture which accounts for the surrounding emotions as well while doing the N-class classification. We also experiment with different encoders for the three modalities."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Dataset",
            "text": "The dataset used for this problem is Emotion-Cause-in-Friends prepared by Wang et al., 2023  ###reference_b32### specifically for this task. It has been prepared using conversations from the popular 1994 sitcom Friends as the source. This dataset contains 1,344 conversations made up of a total of 13,509 utterances, each conversation containing an average of 10 utterances. For each utterance, the dataset has an annotated transcript (covering text modality) and the corresponding video clip (covering visual and auditory modalities) from the show.\nEach utterance is annotated with the emotion depicted by it, which is one of: anger, disgust, fear, joy, neutral, sadness and surprise. The dataset is highly skewed in terms of the frequency of different emotions in the dataset (see Figure 2  ###reference_###).\nFurther, the emotion-causes pairs for all the non-neutral utterances are provided in the dataset in a separate list.\n###figure_1### ###figure_2### The task MC-ECPE expects the model to take a list of such conversations and predict the emotion and emotion-cause pairs labels."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Related Work",
            "text": "A lot of work has been done in the field of emotion analysis in textual settings. Soon, work began on extracting not only the emotion but also the cause of that extracted emotion. People employed mainly two approaches for emotion cause analysis - 1. Extracting the potential causes given an emotion (Lee et al., 2010  ###reference_b19###; Chen et al., 2010  ###reference_b6###; Gui et al., 2016b  ###reference_b13###) and 2. Extracting the emotion-cause pairs jointly (Xia and Ding, 2019b  ###reference_b36###; Ding et al., 2020  ###reference_b9###; Wei et al., 2020  ###reference_b34###). Poria et al., 2020  ###reference_b28### was the first to introduce the task of extracting emotion-cause in conversations but their focus was also only on the textual dialogues. However, in our natural way of conversation, we rely on things like facial expressions, voice intonations for determining the emotion of the speaker. We also rely on auditory and visual scenes to determine the cause of the speaker\u2019s emotions. Hence, it is clear that Emotion-Cause Pair Extraction (ECPE) is a multimodal task requiring at least three modalities: text, audio and video. Busso et al., 2008  ###reference_b4###; McKeown et al., 2012  ###reference_b25###; Li et al., 2022a  ###reference_b21### and Poria et al., 2019  ###reference_b27### worked in the field of multimodal emotion analysis in conversations but they did not consider the emotion causes. The task of MC-ECPE was first worked on by Wang et al., 2021  ###reference_b31###.\n\nArtificial intelligence plays a pivotal role in predicting agricultural yield, with models leveraging data from weather patterns, soil composition, and historical yields to enhance accuracy. Recent advancements incorporate machine learning algorithms, offering scalable solutions for farmers and agronomists to optimize resource allocation and improve crop productivity (Kamilaris and Prenafeta-Bold\u00fa, 2018; Liakos et al., 2018)."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "System Overview",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Baseline I: Utterance labeling",
            "text": "Our baseline model treats the problem as a simple utterance labeling task. We use pre-trained text, audio, and image encoders to encode the individual modalities and use these to train three models that can identify the emotions in the utterances, the candidate cause utterances, and finally identify valid emotion and cause utterance(s) pairs.\nText Encoding: For encoding the transcription of each utterance, we use pre-trained BERT (Devlin et al., 2018  ###reference_b8###) embeddings as the baseline embeddings. Additionally, we finetune DeBERTa-Base (He et al., 2020  ###reference_b15###) on the training data for our experiments. DeBERTa makes use of a disentangled attention mechanism and an enhanced masked encoder to improve upon BERT\u2019s performance in a variety of tasks. Finally, we also tried RoBERTa-Large and (Liu et al., 2019  ###reference_b24###) pre-trained EmotionRoBERTa-Base222https://huggingface.co/SamLowe/roberta-base-go_emotions  ###reference_e-go_emotions### which is publicly available RoBERTa-base model finetuned on the Go Emotions dataset (Demszky et al., 2020  ###reference_b7###). For every text encoder, we perform mean-pooling of the word embeddings to get the textual representation of the utterance.\nVideo Encodings: For encoding the videos, we sampled 16 equally spaced frames from the video and mean-pooled the embeddings for the 16 frames. For encoding these 16 images, we used MViTv2-small (Li et al., 2022b  ###reference_b22###) encoder, which achieves state-of-the-art performance on the Kinetics video detection task (Kay et al., 2017  ###reference_b17###), which makes it an obvious choice for recognizing activities happening in the conversations relevant for emotion/cause detection.\nAudio Encodings: We used WavLM (Chen et al., 2022  ###reference_b5###) for generating audio embeddings, which is trained on large audio data using masked speech representation and denoising in pre-training, making it suitable for various downstream speech tasks. We also try Wav2Vec2-Large (Baevski et al., 2020  ###reference_b2###), which is trained by masking speech input in latent space and solving a contrastive task defined over a quantization of the latent representations which are jointly learned.\nThe model architecture is a combination of three steps, each of which is described below:"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Baseline II: BiLSTM Architecture",
            "text": "The BiLSTM architecture is inspired by the work in Wang et al., 2021  ###reference_b31###. While the Baseline I architecture treats the emotion and cause classification independently for each utterance, it is dependent on the surrounding context of the conversation too. Thus, the BiLSTM architecture models the problem as a Sequence Labeling task. We use the best encoders in the Baseline I architecture for generating the embeddings in this architecture."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Baseline III: BiLSTM-CRF Architecture",
            "text": "In the BiLSTM model, each classification decision was conditionally independent. Linear-chain CRFs are models generally used to model structured data where one output influences its neighboring outputs as it models the various transition probabilities, and have been extensively used with BiLSTMs for sequence labeling (Huang et al., 2015  ###reference_b16###). This could be useful for emotion predictions because the emotion of one utterance is generally influenced by the emotions in its previous utterances. For instance, an utterance with happiness generally tends to be followed by another happiness utterance."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experimental Setup",
            "text": "We perform a random shuffle and use a 90-10% split for the train-validation split. The test set was provided by the authors, but its gold labels have not been made public.\nThe experiments involving Baseline II and III use EmotionRoBERTa + WavLM + MViTv2 configuration. All the experiments involve applying a dropout of 0.3 on the audio, visual and textual embeddings before they are passed on to the main architectures. The BiLSTM for emotion detection consists of 4 layers while the one for candidate cause identification contains 3 layers. The dropout between the stacked layers of the BiLSTM is kept 0.3 as well. We use AdamW optimizer for all the three models, and use a linear learning rate scheduler with warmup for training the models. The Emotion Classification model is trained for 60 epochs, the Candidate Cause Identification model is trained for 40 epochs, and the Emotion-Cause Pairing Model is trained for 40 epochs as well.\nIn order to train the Emotion-Cause pairing model, we create positive and negative pairs during training. However, while the number of positive pairs is of the order N, the number of negative pairs comes to the order of N2, and thus we perform a random sampling of the negative pairs to keep the positive and negative samples in the ratio 1:5. This helps us to maintain balance between the positive and negative classes."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Results and Analysis",
            "text": "The performance of the three Baselines can be seen in Table 1  ###reference_###. During the Evaluation phase, our best ranked submission of Baseline II had Wt. F1 score of 0.1836 and Macro F1 score of 0.1759, ranking 8th on the leaderboard."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In conclusion, we observe that the utterance labeling systems perform as good as sequence labeling systems for this specific dataset. Further, we also see that encoders which are trained on other emotion-related tasks tend to perform better on similar emotion-related tasks.\nIn future, it is possible to learn joint embeddings over the 3 modalities, which should provide better representations for each utterance (Girdhar et al., 2023  ###reference_b12###). Further, it can be experimented to utilize the speaker information for each utterance while creating utterance representations (Liang et al., 2023  ###reference_b23###)."
        }
    ],
    "url": "http://arxiv.org/html/2404.02088v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2.2"
        ],
        "methodology_sections": [
            "3.1",
            "3.2",
            "3.3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "1",
            "3.1",
            "3.2",
            "3.3",
            "4"
        ]
    },
    "research_context": {
        "paper_id": "2404.02088v1",
        "paper_title": "LastResort at SemEval-2024 Task 3: Exploring Multimodal Emotion Cause Pair Extraction as Sequence Labelling Task",
        "research_background": "The paper \"LastResort at SemEval-2024 Task 3: Exploring Multimodal Emotion Cause Pair Extraction as Sequence Labelling Task\" primarily addresses the emerging task of Multimodal Emotion Cause Pair Extraction (ME-CPE). Here\u2019s a summary of the paper's motivation, research problem, and relevant prior work:\n\n### Motivation\nEmotion Analysis has long been a key focus in NLP, traditionally emphasizing the identification of emotions from text. This has evolved over time, from early efforts targeting emotion detection in news articles to more recent advancements in recognizing emotions within conversations. The motivation stems from the increasing importance and utility of understanding not just emotions, but the underlying causes behind these emotions. This knowledge can enhance applications such as AI-driven chatbots, customer interaction systems, therapeutic models, social media moderation, and corporate team management.\n\n### Research Problem\nThe specific research problem tackled in this paper is the extraction of emotion-cause pairs in a conversation using multimodal data. The task requires identifying:\n1. The emotion expressed in each utterance of a conversation.\n2. The underlying causes of these emotions, defined as the utterances explicitly linking certain events or arguments to the identified emotions.\n\nAchieving this involves a sequence of steps and models that identify emotions, assess candidate causes, and pair the detected emotions with their respective causes.\n\n### Relevant Prior Work\n1. **Emotion Detection:** \n   Early work in emotion detection from text, such as research by Lei et al. (2014) and Abdul-Mageed and Ungar (2017), laid the foundation for understanding emotions in various contexts.\n   \n2. **Emotion Recognition in Conversations:**\n   Later advancements, particularly highlighted by Gupta et al. (2017), focused on leveraging public conversation data to recognize emotions, demonstrating the feasible utility of conversational contexts in emotion analysis.\n\n3. **Emotion Cause Analysis:**\n   The shift towards analyzing the causes behind emotions is more recent, notably advanced by Xia and Ding (2019a), who emphasized the importance of understanding these causal relations.\n\n4. **Application Areas:**\n   Various applications highlight the importance of emotion cause analysis:\n   - Chatbots identifying user emotions and their triggers (Pamungkas, 2019).\n   - Customer satisfaction analysis in interactions for responsive management (Yun and Park, 2022).\n   - AI-driven therapeutic tools (D\u2019Alfonso, 2020).\n   - Enhanced social media content moderation (Sawhney et al., 2021).\n   - Improved work and team management in corporate settings (Benke et al., 2020).\n\n### Proposed Approach\nThe authors propose a three-step methodology to address the ME-CPE task:\n1. **Emotion Identification:** Training a model to classify emotions in individual utterances.\n2. **Candidate Cause Identification:** Training another model to determine if an utterance can be a cause of an emotion (binary classification).\n3. **Pairing Emotions with Causes:** Identifying and pairing emotions with their corresponding causes.\n\nThey use different model architectures to approach these tasks, experimenting with:\n- Simple Neural Networks for classification tasks.\n- BiLSTM architectures to incorporate conversational context.\n- BiLSTM CRF to account for surrounding emotions and contexts.\n\nTheir approach also includes experimenting with various encoders to manage the multimodal nature of the input data.\n\nIn essence, the paper brings forward an integrated system leveraging advanced NLP techniques to robustly tackle the sophisticated challenge of multimodal emotion cause pair extraction within conversations.",
        "methodology": "Our approach addresses the problem of Multimodal Emotion Cause Pair Extraction by framing it as a sequence labeling task. Here, we outline the key components and innovations of our methodology:\n\n### Text Encoding:\n1. **Baseline Embeddings**: We utilize pre-trained BERT (Devlin et al., 2018) embeddings to encode the transcription of each utterance.\n2. **Finetuning**: We fine-tune DeBERTa-Base (He et al., 2020) on the training data. DeBERTa employs a disentangled attention mechanism and an enhanced masked encoder, offering improvements over BERT in various tasks.\n3. **Textual Representation**: For each text encoder, we performed mean-pooling on the word embeddings to generate a single textual representation of the utterance.\n\n### Video Encodings:\n1. **Frame Sampling**: From each video, we sampled 16 equally spaced frames.\n2. **Representation Aggregation**: The embeddings of the 16 frames were mean-pooled to create a unified video representation.\n\n### Audio Encodings:\n1. **Primary Encoder**: We used WavLM (Chen et al., 2022) to generate audio embeddings. WavLM is designed to handle large volumes of audio data through masked speech representation and denoising during pre-training.\n\n### Model Architecture:\nOur architecture consists of three interconnected steps, which we will detail as follows:\n\n1. **Emotion Identification**: The first step involves training models to identify emotions in the utterances using encoded text, audio, and video data.\n2. **Candidate Cause Identification**: In the second step, separate models are trained to recognize candidate cause utterances using similar multimodal encoding techniques.\n3. **Pairing Emotion and Cause Utterances**: The final step integrates the identified emotions and cause utterances, establishing valid emotion-cause pairs.\n\n### Innovation:\n1. **Multimodal Fusion**: The model combines text, video, and audio representations for a comprehensive understanding of emotional context and causes.\n2. **Pre-trained Representations**: Leveraging state-of-the-art pre-trained models across modalities and fine-tuning them for the task at hand enhances the model's accuracy.\n3. **Efficient Encoding Techniques**: Employing mean-pooling and state-of-the-art encoders ensures the efficient and effective transformation of input data into useful embeddings.\n\nThese elements together contribute to a robust framework for tackling the Multimodal Emotion Cause Pair Extraction task in a sequence labeling format.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n**Datasets**:\n- **Train-Validation Split**: A 90-10% random shuffle split is performed for the train and validation sets.\n- **Test Set**: Provided by the authors of the task, with gold labels not made public.\n\n**Baseline Systems**:\n- **Baselines II and III**: Use a combination of EmotionRoBERTa, WavLM, and MViTv2 for the multimodal emotion cause pair extraction task.\n\n**Model Configuration**:\n- **Dropout**: A dropout rate of 0.3 is applied to audio, visual, and textual embeddings.\n- **BiLSTM Architecture**:\n  - Emotion Detection: 4 layers with a dropout of 0.3 between stacked layers.\n  - Candidate Cause Identification: 3 layers with a dropout of 0.3.\n- **Optimization**:\n  - Optimizer: AdamW\n  - Learning Rate Scheduler: Linear with warmup\n- **Training**:\n  - Emotion Classification Model: 60 epochs\n  - Candidate Cause Identification Model: 40 epochs\n  - Emotion-Cause Pairing Model: 40 epochs\n\n**Negative Sample Balancing**: \n- Positive pairs are naturally fewer than negative pairs, which are of the order N2. During training, negative pairs are randomly sampled to maintain a ratio of 1:5 with positive pairs, ensuring balance between the positive and negative classes.\n\n**Evaluation Metrics and Main Results**:\n- The task presumably evaluates models based on metrics like Precision, Recall, and F1-score, though the exact metrics are not explicitly stated in the setup. The key findings or main results from these experiments are not provided in the experiment setup description.\n\nOverall, the experimental setup involves thorough balancing and tuning considerations to handle the imbalance problem inherent in negative pair sampling, as well as utilizing advanced multimodal models and sequence labeling architectures to address the emotion cause pair extraction."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Examine the performance of different model architectures for multimodal emotion cause pair extraction in conversations.",
            "experiment_process": "The baseline model treats the problem as a simple utterance labeling task. We used pre-trained text, audio, and image encoders to train three models for identifying emotions in utterances, candidate cause utterances, and valid emotion-cause pairs. Text encoders included BERT, DeBERTa-Base, RoBERTa-Large, and EmotionRoBERTa-Base, with mean-pooling of word embeddings for textual representation. Video encoding involved sampling and mean-pooling 16 frames using the MViTv2-small encoder. Audio encoders used were WavLM and Wav2Vec2-Large. This architecture independently classifies each utterance's emotion and cause.",
            "result_discussion": "The findings from this baseline suggest that treating the problem independently for each utterance lacks contextual information, indicating a need for models that can take surrounding context into account.",
            "ablation_id": "2404.02088v1.No1"
        },
        {
            "research_objective": "Assess the impact of incorporating sequential context using a BiLSTM architecture for the problem of multimodal emotion cause pair extraction.",
            "experiment_process": "The BiLSTM architecture considers the problem as a Sequence Labeling task, accounting for the surrounding context within conversations. It used the best-performing encoders from the baseline architecture for generating embeddings. The BiLSTM model for emotion detection consisted of 4 layers, and the one for candidate cause identification contained 3 layers, both applying a dropout of 0.3 between layers. AdamW optimizer and a linear learning rate scheduler with warmup were used for training the models across up to 60 epochs, with an experimental setup involving a 90-10% train-validation split.",
            "result_discussion": "The BiLSTM architecture demonstrated improved performance by leveraging the surrounding context within conversations, which is significant for more accurate emotion and cause identification.",
            "ablation_id": "2404.02088v1.No2"
        },
        {
            "research_objective": "Explore the benefits of using a BiLSTM-CRF architecture to enhance sequence labeling performance by modeling dependencies between neighboring utterances.",
            "experiment_process": "The BiLSTM-CRF architecture combines BiLSTM with Linear-chain CRFs to model structured data, enabling each output to influence its neighboring outputs via transition probabilities. This setup included using EmotionRoBERTa for textual encoding, WavLM for audio encoding, and MViTv2 for visual encoding, with a 0.3 dropout rate applied to embeddings before passing to the main architecture. The model applied CRF technique to BiLSTM outputs to consider the interdependencies between utterances. The experimental setup followed the same train-validation split and optimization methods as the BiLSTM architecture.",
            "result_discussion": "The BiLSTM-CRF architecture showed enhanced performance over the other models by better modeling the dependencies between sequential utterances, reflecting the influence of one utterance's emotion on subsequent ones, thereby improving emotion prediction accuracy.",
            "ablation_id": "2404.02088v1.No3"
        }
    ]
}