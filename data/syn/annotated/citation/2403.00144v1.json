{
    "title": "EBBS: An Ensemble with Bi-Level Beam Searchfor Zero-Shot Machine Translation",
    "abstract": "The ability of zero-shot translation emerges when we train a multilingual model with certain translation directions; the model can then directly translate in unseen directions. Alternatively, zero-shot translation can be accomplished by pivoting through a third language (e.g., English). In our work, we observe that both direct and pivot translations are noisy and achieve less satisfactory performance. We propose EBBS, an ensemble method with a novel bi-level beam search algorithm, where each ensemble component explores its own prediction step by step at the lower level but they are synchronized by a \u201csoft voting\u201d mechanism at the upper level. Results on two popular multilingual translation datasets show that EBBS consistently outperforms direct and pivot translations as well as existing ensemble techniques. Further, we can distill the ensemble\u2019s knowledge back to the multilingual model to improve inference efficiency; profoundly, our EBBS-based distillation does not sacrifice, or even improves, the translation quality.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Machine translation is a widely applicable NLP task that translates a text from a source language to a target language Brown et al. (1990  ###reference_b6###); Bahdanau et al. (2015  ###reference_b2###).\nThe Transformer architecture Vaswani et al. (2017  ###reference_b49###) and pretrained large language models Radford et al. (2019  ###reference_b38###); Raffel et al. (2020  ###reference_b39###); Lewis et al. (2020  ###reference_b29###) have largely improved translation performance, especially in the supervised setting, where a model can learn from large volumes of parallel corpora.\nHowever, machine translation remains challenging for low-resource languages, because there are not enough data for large neural networks to learn these languages.\nWe specifically focus on multilingual translation in the zero-shot setting, where the system is required to translate between unseen language pairs.\nSince collecting parallel data and training individual models for every translation pair are prohibitively expensive, it is common to build a single multilingual system Johnson et al. (2017  ###reference_b23###); Fan et al. (2021  ###reference_b14###) that can perform translation for all language pairs, most of which are zero-shot translation directions with few exceptions (e.g., English).\nThese models work by prepending a language-indicator token, and zero-shot ability emerges as the model generalizes from trained language pairs to unseen ones (Liu et al., 2021  ###reference_b30###; Wicks and Duh, 2022  ###reference_b55###).\nThe main drawback of such a multilingual model is that they are noisy in the zero-shot setting due to the lack of supervision, and as a result, they tend to generate less satisfactory outputs (Zhang et al., 2020  ###reference_b63###; Liu et al., 2021  ###reference_b30###).\nAlternatively, zero-shot translation can be performed by pivoting (Wu and Wang, 2007  ###reference_b59###, 2009  ###reference_b60###), where the model first translates the input into a high-resource language such as English, which is then translated to the target language.\nHowever, pivoting requires two translation steps, often leading to an accumulation of errors Babych et al. (2007  ###reference_b1###); Gu et al. (2019  ###reference_b17###).\nIn this paper, we propose a multi-pivot ensemble approach, where we design a novel ensemble method to aggregate direct translation and the predictions made by pivoting through different languages.\nOur ensemble approach overcomes the drawback of direct and pivot translations as it can utilize these multiple weak models, yielding high-quality output text despite lacking parallel data.\nBuilding an ensemble for text generation is nuanced as it involves a sequence of word predictions.\nWord-level ensembles aggregate predictions at each generation step, which is usually achieved by averaging the predicted probabilities Sennrich et al. (2016a  ###reference_b43###); Freitag et al. (2017  ###reference_b15###); Shanbhogue et al. (2023  ###reference_b45###).\nThis may not be ideal for zero-shot translation as the predictions are too noisy, making the averaged probabilities overly smooth.\nOn the other hand, minimum Bayes risk decoding (Bickel and Doksum, 2015  ###reference_b4###, MBR) can be considered a sequence-level voting ensemble, but it is only able to select from weak and noisy candidates given by the direct and pivot translations.\nTo this end, we propose an ensemble decoding algorithm with bi-level beam search (EBBS). Our EBBS performs two levels of beam search at each generation step: at the lower level, beam search is applied individually to each ensemble component; at the upper level, the ensemble maintains a shared beam by voting and synchronizing the candidates (sub-sequences) in individual beams.\nUnlike word-level ensembles Freitag et al. (2017  ###reference_b15###); Shanbhogue et al. (2023  ###reference_b45###), EBBS does not average the predicted distributions, encouraging individual predictors to explore their own preferences;\nunlike sequence-level ensembles Kobayashi (2018  ###reference_b25###); Eikema and Aziz (2020  ###reference_b13###), EBBS does not select from a candidate set, and thus is more flexible since voting is performed throughout the generation process.\nWe conducted experiments on IWSLT (Cettolo et al., 2017  ###reference_b8###) and Europarl (Koehn, 2005  ###reference_b26###), two popular multilingual datasets for zero-shot machine translation.\nResults show that EBBS can generate high-quality translations and outperform existing ensemble techniques.\nIn addition, we used EBBS-generated outputs as the distillation signal to further finetune our multilingual translation model, which largely improves the efficiency while retaining the performance or even surpassing EBBS on Europarl."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Zero-shot machine translation. With monolingual data, it is possible to accomplish zero-shot machine translation by training language-specific autoencoders that share the same latent space (Lample et al., 2018a  ###reference_b27###, b  ###reference_b28###; Mohiuddin and Joty, 2020  ###reference_b32###). In a multilingual setting, zero-shot ability emerges when a single model is trained with multiple translation directions, allowing it to generalize to unseen directions (Dabre et al., 2020  ###reference_b11###; Ranathunga et al., 2023  ###reference_b40###; Johnson et al., 2017  ###reference_b23###; Fan et al., 2021  ###reference_b14###). A key research direction for multilingual translation is to alleviate task interference among different translation directions (Zaremoodi et al., 2018  ###reference_b62###; Wang et al., 2020  ###reference_b51###). Researchers have developed various parameter-sharing schemes Baziotis et al. (2022  ###reference_b3###); Chronopoulou et al. (2023  ###reference_b9###) and applied optimization-based methods to locate language-specific neurons Wang and Zhang (2022  ###reference_b50###); He et al. (2023  ###reference_b20###); such a model is able to better learn individual translation directions while pooling the knowledge of all languages together. In our work, we use a standard Transformer model following Johnson et al. (2017  ###reference_b23###) and Liu et al. (2021  ###reference_b30###). Our proposed ensemble algorithm EBBS is compatible with the above methods, as it is agnostic to model architectures. Ensemble learning. Ensemble methods integrate multiple machine learning systems to form a stronger one (Dong et al., 2020  ###reference_b12###; Yang et al., 2023  ###reference_b61###). Bagging, a classic ensemble technique, works by training multiple models with different portions of data and combining their predictions by averaging or voting (Breiman, 1996  ###reference_b5###; B\u00fchlmann and Yu, 2002  ###reference_b7###). Another popular ensemble approach is boosting, where multiple models are trained sequentially, with each subsequent model focusing on the mistakes of the previous ones (Schapire, 2003  ###reference_b42###; Hastie et al., 2009  ###reference_b19###; Natekin and Knoll, 2013  ###reference_b34###). Unfortunately, bagging and boosting are not compatible with our setting, because we build a self-ensemble with only a single model. Alternatively, stacking combines the outputs by training a meta-model (Wolpert, 1992  ###reference_b57###; Ganaie et al., 2022  ###reference_b16###), but this does not apply in our zero-shot setting either because we do not have groundtruth signal to train the meta-model. These ensemble techniques nevertheless may be applied to supervised text generation (Freitag et al., 2017  ###reference_b15###; Kobayashi, 2018  ###reference_b25###; Hendy et al., 2021  ###reference_b21###). That being said, they are not ideal as they do not take advantage of the sequential nature of sentences. Unlike previous work, our EBBS performs bi-level beam search, allowing each component to explore its own predictions at every step but synchronizing all individuals by a \u201csoft voting\u201d mechanism. Our approach is specifically suited to the sequence generation process. \n\nAI in autonomous vehicle navigation. Recent advancements in AI have significantly enhanced autonomous vehicle navigation by utilizing deep learning techniques for environment perception and decision-making (Kendall et al., 2021; Chen et al., 2022). Techniques like reinforcement learning enable vehicles to learn from real-world environments, improving route optimization and safe maneuvering (Zhu et al., 2023)."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Approach",
            "text": "In this section, we first explain our ensemble components in \u00a73.1  ###reference_###.\nWe then propose EBBS, a novel ensemble decoding algorithm in \u00a73.2  ###reference_###.\nFinally, we describe in \u00a73.3  ###reference_### knowledge distillation with EBBS-decoded outputs to improve efficiency."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Ensemble Components",
            "text": "In this work, we focus on zero-shot multilingual machine translation, which requires a system to perform translations for multiple languages, where some translation directions are unseen.\nSpecifically, our multilingual model is an encoder\u2013decoder Transformer with a byte pair encoding tokenizer (Sennrich et al., 2016b  ###reference_b44###) shared among all languages. The encoder can capture the semantics of tokens in any language, whereas the decoder translates the encoded text into the desired language based on a target-language indicator Johnson et al. (2017  ###reference_b23###); Fan et al. (2021  ###reference_b14###).\nWe follow the standard English-centric training (Johnson et al., 2017  ###reference_b23###; Liu et al., 2021  ###reference_b30###), where the multilingual model is trained with parallel data with English on one of the sides (e.g., German-to-English and English-to-Romanian). The zero-shot ability emerges during such training, and the model is able to translate directly between unseen language pairs (e.g., German-to-Romanian), known as direct translation Dabre et al. (2020  ###reference_b11###); Ranathunga et al. (2023  ###reference_b40###). An alternative approach is pivot translation, where the multilingual model performs two translations using a high-resource language as a pivot (e.g., first translating German to English, and then English to Romanian).\nHowever, both direct and pivot translations have major weaknesses: the quality of direct translation tends to be low due to the lack of parallel data, whereas pivot translation suffers from an error accumulation as it requires two translation steps (Babych et al., 2007  ###reference_b1###; Gu et al., 2019  ###reference_b17###).\nIn this paper, we would like to build a self-ensemble of the multilingual model to boost translation quality, where direct and pivot translations are our ensemble components.\nHowever, standard ensemble methods such as averaging and voting may not work well for text generation. Voting, for example, chooses the most voted prediction, but in text generation, the components\u2019 votes often do not share anything in common, because there could be tens of thousands of tokens in the vocabulary.\nAn averaging ensemble, on the other hand, averages the predicted distributions of all components, potentially leading to an overly smooth distribution. As reported in a pilot study in the appendix of Fan et al. (2021  ###reference_b14###), a multilingual averaging ensemble only achieves a small improvement over direct translation. Overall, ensemble methods have not been widely explored for zero-shot multilingual translation in previous literature."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Our Proposed EBBS Algorithm",
            "text": "###figure_1### To this end, we propose an ensemble with bi-level beam search (EBBS), a novel decoding framework that enables different ensemble components to collaborate and vote on each other\u2019s partial generations with two levels of beam search.\nAt the lower level, each ensemble component performs beam search individually, exploring its own preferred regions of the sentence space.\nAt the upper level, EBBS synchronizes the lower-level beam candidates through a voting mechanism, only keeping the most promising partial generations in a shared, upper-level beam.\nThis allows the ensemble components to vote out spurious partial candidates and improve zero-shot translation performance.\nConcretely, we assume there are  ensemble components , each predicting the probability of the next word given some prefix.\nFor the th decoding step, EBBS initializes a shared, upper-level beam , suggesting that a sequence is forced to start with a special token bos with probability .\nFor step , each ensemble component performs a lower-level beam search individually, based on the prefixes in the last step\u2019s shared beam :\nfor . Here,  selects -many sequences with the highest probabilities,  represents string concatenation,  is the vocabulary, and  is the th ensemble component\u2019s predicted probability at step  given the prefix  and input .\nAt the upper level, EBBS synchronizes the individual beams , for , into a single, shared,\nupper-level beam through a soft voting mechanism, where the candidate set  is the union of the sequences in individual beams:\nThen, we evaluate each candidate in  and compute its overall vote as the sum of the probabilities.\nIn this way, the upper level synchronizes every ensemble component with the shared beam  for the next step of generation.\nIntuitively, our voting scheme gives an ensemble component -many votes, each weighted by the predicted probability. The votes (probabilities) are then tallied (summed) for each candidate to form the upper-level beam.\nOur bi-level beam search terminates when we have -many terminated sequences in the shared beam, and returns the sequence with the highest score111For selecting the final output, we follow standard implementations and normalize the joint probabilities by length, i.e., taking the geometric mean of step-wise probabilities (Wolf et al., 2019  ###reference_b56###; Ott et al., 2019  ###reference_b35###). Otherwise, beam search algorithms are often biased towards short sequences.\n as the ensemble output. We provide the detailed pseudocode for EBBS in Algorithm 1  ###reference_### and an illustration in Figure 1  ###reference_###.\nDiscussion. As shown in Algorithm 2  ###reference_### (Appendix A  ###reference_###), traditional beam search keeps a fixed-size beam of high-likelihood partial sequences. It is tempting to combine such vanilla beam search with multiple predictors by directly averaging their probabilities\n as the score for beam search (Line 8, Algorithm 2  ###reference_###), which has been experimented in previous work Sennrich et al. (2016a  ###reference_b43###); Shanbhogue et al. (2023  ###reference_b45###).\nHowever, our intuition suggests that such an approach may suffer from the over-smoothing problem Wei et al. (2019  ###reference_b52###); Wen et al. (2023b  ###reference_b54###): when multiple translations (known as modes) are plausible given an input, the ensemble process will overly smooth out the modes by probability averaging.\nBy contrast, EBBS allows each ensemble component to explore its own mode (Lines 4\u201311, Algorithm 1  ###reference_###). In Figure 1  ###reference_###, for example, the top sequence yields two plausible next tokens, suggested by each component in the lower level. Their probabilities are not smoothed out in our approach, unlike averaging ensembles. The upper level performs soft voting (Lines 12\u201319, Algorithm 1  ###reference_###) so as to maintain tractable inference."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "EBBS-Based Distillation",
            "text": "To improve inference efficiency, we may perform knowledge distillation based on the outputs of our EBBS algorithm.\nIn particular, we follow Kim and Rush (2016  ###reference_b24###) and apply a sequence-level knowledge distillation loss, treating the output  of our ensemble (called a teacher) as the pseudo-groundtruth for finetuning the multilingual translation model (called a student):\nOur self-distillation is an ensemble-then-distill process. This differs from a straightforward practice of multi-teacher distillation, where the student learns from the union of teachers\u2019 outputs Wu et al. (2021  ###reference_b58###). The commonly applied cross-entropy loss is known to yield overly smooth distributions Wen et al. (2023a  ###reference_b53###, b  ###reference_b54###), and the problem becomes more severe with multiple teachers, leading to unsatisfactory performance of union distillation Shayegh et al. (2024  ###reference_b46###). By contrast, our approach provides the student with a consolidated pseudo-groundtruth translation for an input, causing less confusion during the distillation process especially when teachers disagree with each other."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Settings",
            "text": "Datasets. We evaluated EBBS on two popular benchmark datasets for zero-shot machine translation: IWSLT (Cettolo et al., 2017  ###reference_b8###), which contains 4 languages (with English) and 6 zero-shot directions; and Europarl v7 (Koehn, 2005  ###reference_b26###), which contains 9 languages and 56 zero-shot directions.\nMetrics. We evaluated translation quality with BLEU scores (Papineni et al., 2002  ###reference_b36###), which measure the -gram overlap between generated and reference translations.\nIn particular, BLEU denotes the -gram overlap, and BLEU denotes the geometric mean of BLEU for  with a brevity penalty.\nWe used the standard SacreBLEU Post (2018  ###reference_b37###), following recent translation studies Liu et al. (2021  ###reference_b30###); Scao et al. (2022  ###reference_b41###).\nModel architecture.\nWe replicated Liu et al. (2021  ###reference_b30###) and trained a multilingual system as our base model.\nSpecifically, we had a 5-layer encoder\u2013decoder Transformer for IWSLT, but used an 8-layer Transformer for Europarl to accommodate more training data and languages.\nWe also followed Liu et al. (2021  ###reference_b30###) and removed residual connections in certain middle layers of the encoder (2nd layer for IWSLT; 2nd and 4th layers for Europarl). Liu et al. (2021  ###reference_b30###) show weakening positional correspondence between layers can significantly improve zero-shot performance, which was also observed in our replication experiments.\nMore details about our experimental setup can be found in Appendix B  ###reference_###."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Competing Methods",
            "text": "We compare our EBBS against the following methods, including pivoting and different ensembles.\nEnglish pivoting. Pivoting is commonly applied for translation (Wu and Wang, 2007  ###reference_b59###, 2009  ###reference_b60###; Vamvas and Sennrich, 2022  ###reference_b47###), and we used English as the pivot in our setting because we have parallel data for translations both from and to English.\nWord-level averaging ensemble. Averaging is one of the most widely used ensemble techniques in text generation (Sennrich et al., 2016a  ###reference_b43###; Freitag et al., 2017  ###reference_b15###; Shanbhogue et al., 2023  ###reference_b45###). Essentially, the ensemble components\u2019 probabilities are first averaged before being fed to the standard beam search (Algorithm 2  ###reference_###).\nWord-level voting ensemble. The voting ensemble, common in classification tasks, picks the output class based on the number of votes from ensemble components (given by ).\nHowever, voting is not common in text generation, because  may select completely different words by the ensemble components due to the large vocabulary size, making voting ineffective.\nAs a remedy, we pick the word by the highest probability when there is a tie for votes.\nSequence-level voting ensemble. Minimum Bayes risk decoding is originally designed as a single-model decoding algorithm, where it selects a sequence from a set of beam search results based on similarity (Eikema and Aziz, 2020  ###reference_b13###; M\u00fcller and Sennrich, 2021  ###reference_b33###). Here, we use it as a sequence-level ensemble technique, where the candidates are the output sequences from different ensemble components.\nLet  be the set of candidate outputs given by  ensemble components. The best output is selected as\nwhere  computes the BLEU score between a hypothesis  and a reference .\nIn essence, MBR selects an output that resembles others most, using BLEU as the similarity metric."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Results and Analysis",
            "text": "Main results.\nWe first replicated the base multilingual model Liu et al. (2021  ###reference_b30###). As shown in Rows 1\u20132, Table 1  ###reference_###, the results are generally close, indicating that the replication is successful and ready for our ensemble research. Further, we tried English pivoting (Row 3), a common zero-shot translation method.\nIn our experiments, we find that it does not outperform direct translation, as pivoting methods may suffer from the error accumulation problem since it requires two steps of translation.\nWe then compare different ensemble techniques, including the proposed EBBS and alternative ensemble methods.\nSpecifically, we notice that IWSLT contains four languages (with English); thus we have two available pivoting directions (excluding source and target), which, along with direct translation, are our three ensemble components. For Europarl, it contains nine languages; for performance and efficiency concerns (to be shown in Figure 2  ###reference_###), we also consider three translation paths as our ensemble components: direction translation, English pivoting, and a second pivot.222We use the first available language in the order of Spanish, German, and French. For example, Spanish-to-German translation will have to use French as the pivot. These languages are chosen because they have the most content on the Internet according to the Web Technology Surveys (https://w3techs.com/technologies/overview/content_language  ###reference_content_language###), allowing us to better simulate real-world applications.\nWe start by studying the standard ensemble technique of word-level averaging (Row 4), which has been used in previous translation research (Freitag et al., 2017  ###reference_b15###).\nIn particular, our averaging ensemble performs worse than direct translation on IWSLT, but is slightly better on Europarl. Our results are not consistent to Freitag et al. (2017  ###reference_b15###), who show a word-level averaging ensemble of random seeds can improve performance. This is understandable because models trained with random seeds exhibit similar behavior, and averaging their probabilities achieves a denoising effect. However, our ensemble components are drastically different in terms of their strengths and expertise, because they learn diverse translation patterns from different languages.\nThus, word averaging fails to improve translation quality in our setting.\nAlternatively, voting ensembles can also be applied, at either the word level or the sequence level.\nAs seen, word-level voting is not effective, as it does not achieve significant improvements (Row 5).\nThis is expected because the voted words (top predictions) by the ensemble components may not overlap due to the large vocabulary size.\nIn such cases, the algorithm defaults to choosing the word with the highest probability, causing the ensemble to follow the most peaked distributions.\nSequence-level voting should also be done in a soft manner, and minimum Bayes risk (MBR) decoding can be thought of as using a Bayes risk to softly \u201cvote\u201d the candidate outputs. As seen from Row 6, such a method works relatively well on Europarl, achieving the second-highest performance across all competing methods; however, it works poorly on the IWSLT dataset.\nThe main drawback of sequence-level voting is that it can only select one of the ensemble components\u2019 outputs.\nThis may not work well when the individual ensemble components are weak, as in our zero-shot setting, and especially with the IWSLT dataset due to a relatively small training set.\nSuch a selective sequence-level ensemble cannot integrate different expertise of its components during generation.\nUnlike existing ensemble methods, our EBBS algorithm achieves the highest performance in most directions on both datasets. Notice that Europarl contains 56 zero-shot directions; due to the space limit, we can only present in Table 1  ###reference_### the first seven directions based on the order provided by the dataset. Table 2  ###reference_### further shows a pairwise comparison against direct translation (a strong baseline in our experiment) in all zero-shot directions.\nAs seen, EBBS achieves higher performance in 56 out of 62 cases across two datasets, showing strong statistical evidence for its effectiveness, with a -value of 3e-11 in a two-sided binomial test.\nAnalysis of voting methods in EBBS. In our EBBS algorithm, the lower-level beams are synchronized into a shared upper-level beam by voting.\nSpecifically, EBBS uses a sum-voting mechanism, where we add the ensemble components\u2019 probabilities for each appearance of a candidate in the lower-level beam, shown in Eqn. (3  ###reference_###).\nHere, we analyze a few alternative voting methods for EBBS.\nIf EBBS adopts total-sum voting, it still uses lower-level beams to find candidates, but adds all components\u2019 probabilities together. This is equivalent to applying the common averaging ensemble to the top- candidates. However, it\ndiffers from our approach, because in total-sum voting, a component will vote even if the candidate does not appear in its own lower-level beam; the probability after voting in Eqn. (3  ###reference_###) is substituted with .\nAs shown in Table 3  ###reference_###, EBBS with total-sum voting performs worse than direct translation, suggesting the importance of ignoring the components whose lower-level beam does not contain the candidate. This is analogous to nucleus sampling Holtzman et al. (2019  ###reference_b22###), where the long tail of a distribution is mainly noise and should be ignored.\nOther voting schemes that EBBS may use include 0/1 voting and max voting.\nThe former selects the candidates that appear most in the lower-level beams, disregarding the probability values (unless for ties);\nthe latter chooses the maximum probability across the lower-level beams, which gives preference to sequences through a maximization bias (Hasselt, 2010  ###reference_b18###; van Hasselt et al., 2016  ###reference_b48###).\nAs seen in Table 3  ###reference_###, EBBS performs relatively well with both of these voting schemes, achieving a decent improvement over the baseline approach;\nhowever, their performance is consistently worse than our sum voting scheme.\nOverall, our bi-level beam search ensemble is effective with different voting schemes (except for the total-sum voting), and our sum voting works the best among these variants.\nAnalysis of ensemble components. In Table 4  ###reference_###, we analyze our ensemble components to better understand our ensemble technique for zero-shot machine translation. As seen, direct translation is an effective approach, which is consistent with previous literature Fan et al. (2021  ###reference_b14###); Liu et al. (2021  ###reference_b30###). English pivoting achieves higher BLEU scores for some  but lower for others; its overall BLEU is slightly worse than direct translation in our experiment, as it has the error accumulation problem. Pivoting through non-English languages degrades the performance to a large extent because lacking supervision along the pivoting path leads to two steps of zero-shot translation.\nNotably, EBBS can achieve a solid improvement (~0.6 BLEU points) over the baseline despite including weak ensemble components (e.g., Finnish pivoting, which is almost 5 BLEU points lower than direct translation).\nIf we limit the ensemble components to the strong ones (top-3), EBBS can achieve an even higher improvement of ~0.8 BLEU points.\nThis analysis highlights the robustness of our EBBS algorithm.\n###figure_2### We further study how EBBS performs with different numbers of ensemble components.\nSpecifically, we analyze two incremental ensemble settings: best-to-worst and worst-to-best. In both cases, we start with direct translation; then we incrementally add the next \u201cbest\u201d or \u201cworst\u201d pivot translation according to Table 4  ###reference_###.\nFigure 2  ###reference_### shows the trends of incremental ensembles. If we add best pivot directions, the performance peaks at three ensemble components; interestingly, the inclusion of weaker components does not affect EBBS much.\nOn the other hand, adding the worst pivot translation leads to an immediate drop of 1.6 BLEU points, which then largely recovers with the second pivot.\nThis is reasonable because the worst pivot (Finnish) is 4.6 BLEU points lower than direct translation, and EBBS cannot decide on which of the two ensemble components to trust; despite this, the performance of EBBS is still much better than the average performance of the components.\nWith a second pivot, it gives a third \u201copinion\u201d when the first two components \u201cdisagree.\u201d\nThe performance continues to rise if more and stronger components are added.\nIn fact, our ensemble even surpasses the baseline with 4 weakest pivot translations, each of which is at least 1 BLEU point lower than the baseline. This demonstrates that EBBS is flexible and works well with both strong and weak ensemble components.\nDistillation results. We apply distillation techniques to our ensemble outputs to speed up inference. We follow the same ensemble setting (three components) as in Table 1  ###reference_###, but only focus on Italian-to-Dutch translation because generating the training samples for all 66 zero-shot directions (each containing more than 100K samples) requires significant computing resources.\nWe compare our EBBS-based distillation with two alternatives. A straightforward way to distill from multiple teachers (here, translation paths) is to take the union of teachers\u2019 outputs for training. We also include self-distillation, which does not utilize different translation paths but generates samples by standard beam search for self-training.\nAs seen in Table 5  ###reference_###, self-distillation improves zero-shot translation on both IWSLT and Europarl.\nThis is because the base model has never learned from parallel data between Italian and Dutch, and the self-distillation process enables the multilingual model to further refine its translation capabilities in this zero-shot direction. Similarly, we observe that union distillation also improves performance to some degree, but it is not better than self-distillation.\nThis is consistent with the challenges of one-to-many learning seen in previous work (Wen et al., 2023a  ###reference_b53###, b  ###reference_b54###). In our case, learning from the outputs of multiple teachers in a straightforward manner may confuse the student, failing to consolidate and transfer the teachers\u2019 knowledge.\nIn comparison, EBBS-based distillation consistently outperforms both self-distillation and union distillation, achieving near-EBBS performance on Europarl. For IWSLT, our model even outperforms EBBS, as IWSLT is a small dataset and the high-quality outputs generated by EBBS can largely improve the base model (no distillation) by 2.7 points.\nWe provide an efficiency analysis in Appendix D  ###reference_###, showing that the inference time of EBBS is linear to the number of ensemble components. Overall, our EBBS-based distillation achieves a significant speed up while maintaining, or even improving, translation quality.\nAdditional results. We show additional results in the appendices.\nC  ###reference_###: Analysis of beam size, D  ###reference_###: Analysis of inference efficiency, E  ###reference_###: Entropy of distilled models, and F  ###reference_###: Case study."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this work, we address ensemble-based zero-shot machine translation by directly translating and pivoting through different languages. We further design a novel bi-level beam search algorithm (called EBBS) for decoding. We evaluated EBBS on two popular zero-shot translation datasets, IWSLT and Europarl.\nResults show that EBBS outperforms existing ensemble techniques, and that the high-quality translations produced by EBBS can be used for distillation to improve translation efficiency (and sometimes also quality over EBBS)."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "Our work features both algorithmic design and empirical effectiveness, but may also have limitations.\nFirst, our work focuses on zero-shot translation, due to the background of our project. Nevertheless, the EBBS algorithm that we have developed is a general decoding method and is potentially applicable to various text generation tasks. We are happy to explore this direction as future work, such as building ensembles of large language models.\nSecond, our distillation experiments were conducted on Italian-to-Dutch translation because it appears in both IWSLT and Europarl datasets. We are unable to perform all-direction distillation in this project, because Europarl, say, has 56 zero-shot directions, and distillation techniques require extensive computing resources to generate pseudo-groundtruth sentences. Nevertheless, our distillation results are consistent on both datasets, and therefore, not including all-direction distillation does not hurt the validity of our method. Moreover, our work points to interesting future work, such as iteratively applying our ensemble-then-distill procedure to build a self-improving multilingual machine translation system.\nThird, we have mainly used BLEU (in particular, SacreBLEU) scores to evaluate translation quality, which is standard in multilingual translation research Johnson et al. (2017  ###reference_b23###); Liu et al. (2021  ###reference_b30###); Fan et al. (2021  ###reference_b14###). We are aware of language model-based scores, such as BERTScore Zhang et al. (2019  ###reference_b64###), for evaluating the translation in certain languages, but they may not be appropriate for low-resource languages and are not commonly used in our setting. We have not performed human evaluation either, due to both practical and ethical concerns.\nPractically, it is difficult to find qualified annotators because our multilingual setting requires an annotator to know a large number of languages in addition to English. Ethically, it is inappropriate to ask annotators to study these languages before or during annotation, which is exhausting and may last for extended periods of time. That being said, our work uses the most standard metrics and results are consistent in most BLEU variants on both datasets, with a strong statistical confidence level based on 62 translation directions.\nIn addition, we have provided case studies to further illustrate how our EBBS works.\nTherefore, we deem our evaluation ethical, appropriate, and adequate."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A Beam Search",
            "text": "We show the standard beam search in Algorithm 2  ###reference_### for a comparison with our proposed EBBS.\nIn general, beam search takes a scorer  as the input and approximately finds the highest-scored sequence, by expanding its search tree with all the vocabulary (Lines 6\u20139) but only keeping the top- partial candidates (Line 10) at each generation step.\nUnlike EBBS, beam search is not specifically designed to work with multiple scorers, and we show in our main analysis that applying beam search with averaged probabilities of the ensemble components is not an ideal approach for ensemble decoding."
        },
        {
            "section_id": "Appendix 2",
            "parent_section_id": null,
            "section_name": "Appendix B Details of Our Experiments",
            "text": "Dataset details. We evaluated our methods using IWSLT 2017 Cettolo et al. (2017  ###reference_b8###) and Europarl v7 Koehn (2005  ###reference_b26###). Table 6  ###reference_### provides a summary of the languages.\nThe IWSLT 2017 translation dataset features multilingual data derived from TED talks.\nWe followed previous work and used a standard split for zero-shot evaluation (Dabre and Kurohashi, 2017  ###reference_b10###; Liu et al., 2021  ###reference_b30###).\nIn particular, IWSLT contains English-centric training data for Italian, Dutch, and Romanian, while evaluation is performed in six zero-shot directions.\nIWSLT is a relatively small dataset, which tests our method\u2019s ability to generalize from few languages.\nEuroparl is a multilingual dataset crawled from the proceedings of the European Parliament.\nWe again followed previous work (Liu et al., 2021  ###reference_b30###) and evaluated our methods with a standard split for the zero-shot setting, containing English-centric data for eight languages with a total of 56 zero-shot evaluation directions. We specifically adopted the non-overlapping setting: in the original corpus, a sentence may be translated into multiple languages, and the non-overlapping setup chooses only one target translation for each input. This prevents potential data leaking problems.\nEuroparl contains more data and languages than IWSLT, which further tests our method\u2019s ability to generalize across multiple languages.\nImplementation details. We directly adopt the neural architecture and hyperparameters in Liu et al. (2021  ###reference_b30###). In particular, we used 5- and 8-layer encoder\u2013decoder models for IWSLT and Europarl, respectively. For both datasets, we had 512 hidden units and 8 attention heads. Our BLEU scores are based on SacreBLEU Post (2018  ###reference_b37###) with the following specifications: \\seqsplitBLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.5.1. These setups establish a controlled comparison against previous work.\nIn our presentation of beam search and the proposed EBBS, we describe the scorer as the multiplication of step-wise probabilities. In implementation, we used the sum of log-probabilities for numerical stability.\nMoreover, our EBBS is built on top of the popular fairseq framework (Ott et al., 2019  ###reference_b35###), using their beam search implementation as the backbone.\nConsequently, we inherit standard beam search implementation techniques such as length normalization and max length constraints, which are not detailed in our pseudocode."
        },
        {
            "section_id": "Appendix 3",
            "parent_section_id": null,
            "section_name": "Appendix C Analysis of Beam Size",
            "text": "We analyze the effect of the beam size and show the results in Table 7  ###reference_###. As seen, performance rises with a larger beam size and eventually plateaus at around five, which is consistent with the practice of standard beam search Meister et al. (2020  ###reference_b31###). This analysis also shows the robustness of our EBBS algorithm. Based on this experiment and efficiency considerations, we used a beam size of five in our main experiments.\nIt should be mentioned that in principle our lower-level and upper-level beams need not have the same size. In our experiments, we simply set them to be the same. We have not tried full-grid hyperparameter tuning because the focus of our paper is algorithmic design, and as shown, our proposed EBBS is mostly robust to the beam size.\n###figure_3###"
        },
        {
            "section_id": "Appendix 4",
            "parent_section_id": null,
            "section_name": "Appendix D Analysis of Inference Efficiency",
            "text": "We analyze the efficiency of our ensemble approach. As seen in Figure 3  ###reference_###, the inference scales almost linearly, which is reasonable as we need to perform inference for all the components. The trend shows that it is computationally feasible to build an ensemble of even more components.\nFurther, the analysis suggests that our EBB-distilled models achieves a speedup of multiple times depending on the number of ensemble components. Recall from \u00a74.3  ###reference_### that our our EBBS-based distillation is able to transfer the ensemble\u2019s knowledge to a single multilingual model, without hurting, if not improving, the translation quality."
        },
        {
            "section_id": "Appendix 5",
            "parent_section_id": null,
            "section_name": "Appendix E Entropy of Distilled Models",
            "text": "We would like to understand why EBBS-based distillation largely outperforms other methods, such as union distillation (\u00a74.3  ###reference_###). The hypothesis is that cross-entropy distillation loss with diverse samples may lead to an overly smooth distribution, which in turn would affect the model performance Wen et al. (2023b  ###reference_b54###); Shayegh et al. (2024  ###reference_b46###).\nWe show the average prediction entropy of our distilled models in Table 8  ###reference_###.\nFor some input  and generation step , the prediction entropy is\nHere, a large entropy generally indicates that the model is less certain, producing a more uniform prediction, whereas a low entropy indicates that the model is confident, producing a more peaked distribution.\nAs seen in Table 8  ###reference_###, the model without distillation yields the highest entropy, suggesting that it is uncertain about zero-shot translation probably due to a lack of training signals.\nUnion distillation trains the model from the union of ensemble components\u2019 outputs. It reduces prediction entropy compared with no distillation, but due to the nature of cross-entropy loss, it remains the highest among all distillation variants.\nSelf-distillation is based on direction translation only, reinforcing the model\u2019s current belief and thus producing the lowest entropy. On the contrary, our EBBS-based distillation achieves a moderate entropy on both datasets.\nIt should be emphasized that the entropy analysis only merely light on how different distillation methods behave, but the entropy itself does not indicate the quality of a model.\nWe quote BLEU scores from Table 5  ###reference_###, which has suggested that our EBBS-based distillation achieves similar or higher performance compared with EBBS, consistently outperforming other distillation methods."
        },
        {
            "section_id": "Appendix 6",
            "parent_section_id": null,
            "section_name": "Appendix F Case Study",
            "text": "Table 9  ###reference_### shows examples of direct, pivot, and EBBS translations.\nAs seen, pivot and direction translations are prone to low-quality output, but EBBS enables them to correct each other\u2019s mistakes.\nIn the first example, say, our EBBS generally follows the sentence structure of direct translation, where the Italian word \u201cdivertimento\u201d (fun) is mistranslated to the Dutch word \u201contspanning\u201d (relaxation), but our EBBS corrects it to \u201cplezier\u201d (pleasure), advocated by English pivoting and voted by all ensemble components."
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T1\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T1.2\" style=\"width:390.3pt;height:175.8pt;vertical-align:-0.6pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-125.7pt,56.4pt) scale(0.608176252443124,0.608176252443124) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T1.2.2\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T1.2.2.3.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" id=\"S4.T1.2.2.3.1.1\" rowspan=\"8\"><span class=\"ltx_text\" id=\"S4.T1.2.2.3.1.1.1\">IWSLT</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T1.2.2.3.1.2\">#</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T1.2.2.3.1.3\">Model</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.2.2.3.1.4\">Average</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.2.2.3.1.5\">it-nl</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.2.2.3.1.6\">it-ro</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.2.2.3.1.7\">nl-it</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.2.2.3.1.8\">nl-ro</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.2.2.3.1.9\">ro-it</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.2.2.3.1.10\">ro-nl</td>\n<td class=\"ltx_td\" id=\"S4.T1.2.2.3.1.11\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.2\">1</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1\">Direct translation\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.00144v1#bib.bib30\" title=\"\">2021</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T1.1.1.1.3\">17.7</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T1.1.1.1.4\">18.5</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T1.1.1.1.5\">17.8</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T1.1.1.1.6\">17.9</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T1.1.1.1.7\">15.5</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T1.1.1.1.8\">19.6</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.9\">16.8</td>\n<td class=\"ltx_td\" id=\"S4.T1.1.1.1.10\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.2.2.4.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.2.2.4.2.1\">2</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.2.2.4.2.2\">Direct translation (our replication)</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.4.2.3\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T1.2.2.4.2.3.1\">\\ul</span>17.29</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.4.2.4\">17.46</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.4.2.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.2.2.4.2.5.1\">17.48</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.4.2.6\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T1.2.2.4.2.6.1\">\\ul</span>18.23</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.4.2.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.2.2.4.2.7.1\">14.63</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.4.2.8\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T1.2.2.4.2.8.1\">\\ul</span>19.65</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T1.2.2.4.2.9\">16.26</td>\n<td class=\"ltx_td\" id=\"S4.T1.2.2.4.2.10\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.2.2.5.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.2.2.5.3.1\">3</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.2.2.5.3.2\">Pivoting (en)</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.5.3.3\">16.19</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.5.3.4\">17.49</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.5.3.5\">15.09</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.5.3.6\">16.79</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.5.3.7\">13.05</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.5.3.8\">18.34</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T1.2.2.5.3.9\">16.37</td>\n<td class=\"ltx_td\" id=\"S4.T1.2.2.5.3.10\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.2.2.6.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.2.2.6.4.1\">4</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T1.2.2.6.4.2\">Word-level averaging ensemble</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T1.2.2.6.4.3\">16.52</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T1.2.2.6.4.4\">16.48</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T1.2.2.6.4.5\">16.49</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T1.2.2.6.4.6\">17.53</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T1.2.2.6.4.7\">13.80</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T1.2.2.6.4.8\">19.07</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T1.2.2.6.4.9\">15.77</td>\n<td class=\"ltx_td\" id=\"S4.T1.2.2.6.4.10\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.2.2.7.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.2.2.7.5.1\">5</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.2.2.7.5.2\">Word-level voting ensemble</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.7.5.3\">16.99</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.7.5.4\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T1.2.2.7.5.4.1\">\\ul</span>17.58</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.7.5.5\">16.38</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.7.5.6\">17.78</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.7.5.7\">14.13</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.7.5.8\">19.21</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T1.2.2.7.5.9\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T1.2.2.7.5.9.1\">\\ul</span>16.84</td>\n<td class=\"ltx_td\" id=\"S4.T1.2.2.7.5.10\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.2.2.8.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.2.2.8.6.1\">6</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.2.2.8.6.2\">Sequence-level voting ensemble (MBR)</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.8.6.3\">16.66</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.8.6.4\">16.54</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.8.6.5\">16.51</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.8.6.6\">17.72</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.8.6.7\">13.64</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.8.6.8\">19.58</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T1.2.2.8.6.9\">15.98</td>\n<td class=\"ltx_td\" id=\"S4.T1.2.2.8.6.10\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.2.2.9.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.2.2.9.7.1\">7</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.2.2.9.7.2\">EBBS (ours)</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.9.7.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.2.2.9.7.3.1\">18.24</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.9.7.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.2.2.9.7.4.1\">19.52</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.9.7.5\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T1.2.2.9.7.5.1\">\\ul</span>17.09</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.9.7.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.2.2.9.7.6.1\">19.06</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.9.7.7\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T1.2.2.9.7.7.1\">\\ul</span>14.58</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.9.7.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.2.2.9.7.8.1\">20.75</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T1.2.2.9.7.9\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.2.2.9.7.9.1\">18.45</span></td>\n<td class=\"ltx_td\" id=\"S4.T1.2.2.9.7.10\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.2.2.10.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_tt\" id=\"S4.T1.2.2.10.8.1\" rowspan=\"8\"><span class=\"ltx_text\" id=\"S4.T1.2.2.10.8.1.1\">Europarl</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S4.T1.2.2.10.8.2\">#</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S4.T1.2.2.10.8.3\">Model</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T1.2.2.10.8.4\">Average</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T1.2.2.10.8.5\">da-de</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T1.2.2.10.8.6\">da-es</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T1.2.2.10.8.7\">da-fi</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T1.2.2.10.8.8\">da-fr</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T1.2.2.10.8.9\">da-it</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T1.2.2.10.8.10\">da-nl</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.2.2.10.8.11\">da-pt</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.2.2.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.2.2.2.2\">1</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T1.2.2.2.1\">Direct translation\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.00144v1#bib.bib30\" title=\"\">2021</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T1.2.2.2.3\">26.9</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T1.2.2.2.4\">24.2</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T1.2.2.2.5\">33.1</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T1.2.2.2.6\">18.1</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T1.2.2.2.7\">30.6</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T1.2.2.2.8\">26.1</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T1.2.2.2.9\">26.3</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T1.2.2.2.10\">29.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.2.2.11.9\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.2.2.11.9.1\">2</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.2.2.11.9.2\">Direct translation (our replication)</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.11.9.3\">27.74</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.11.9.4\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T1.2.2.11.9.4.1\">\\ul</span>26.24</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.11.9.5\">33.64</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.11.9.6\">18.95</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.11.9.7\">31.01</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.11.9.8\">26.58</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.11.9.9\">27.36</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T1.2.2.11.9.10\">30.38</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.2.2.12.10\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.2.2.12.10.1\">3</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.2.2.12.10.2\">Pivoting (en)</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.12.10.3\">27.67</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.12.10.4\">25.15</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.12.10.5\">33.79</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.12.10.6\">18.63</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.12.10.7\">31.45</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.12.10.8\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T1.2.2.12.10.8.1\">\\ul</span>27.12</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.12.10.9\">26.71</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T1.2.2.12.10.10\">30.82</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.2.2.13.11\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.2.2.13.11.1\">4</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T1.2.2.13.11.2\">Word-level averaging ensemble</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T1.2.2.13.11.3\">27.76</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T1.2.2.13.11.4\">26.21</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T1.2.2.13.11.5\">33.64</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T1.2.2.13.11.6\">18.88</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T1.2.2.13.11.7\">31.09</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T1.2.2.13.11.8\">26.71</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T1.2.2.13.11.9\">27.40</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T1.2.2.13.11.10\">30.37</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.2.2.14.12\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.2.2.14.12.1\">5</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.2.2.14.12.2\">Word-level voting ensemble</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.14.12.3\">27.75</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.14.12.4\">25.93</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.14.12.5\">33.90</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.14.12.6\">18.47</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.14.12.7\">31.29</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.14.12.8\">27.08</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.14.12.9\">26.74</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T1.2.2.14.12.10\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T1.2.2.14.12.10.1\">\\ul</span>30.84</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.2.2.15.13\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.2.2.15.13.1\">6</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.2.2.15.13.2\">Sequence-level voting ensemble (MBR)</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.15.13.3\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T1.2.2.15.13.3.1\">\\ul</span>27.85</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.15.13.4\">25.88</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.15.13.5\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T1.2.2.15.13.5.1\">\\ul</span>33.93</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.15.13.6\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T1.2.2.15.13.6.1\">\\ul</span>19.10</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.15.13.7\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T1.2.2.15.13.7.1\">\\ul</span>31.47</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.15.13.8\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T1.2.2.15.13.8.1\">\\ul</span>27.12</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T1.2.2.15.13.9\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T1.2.2.15.13.9.1\">\\ul</span>26.98</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T1.2.2.15.13.10\">30.49</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.2.2.16.14\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S4.T1.2.2.16.14.1\">7</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\" id=\"S4.T1.2.2.16.14.2\">EBBS (ours)</td>\n<td class=\"ltx_td ltx_align_right ltx_border_b\" id=\"S4.T1.2.2.16.14.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.2.2.16.14.3.1\">28.36</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_b\" id=\"S4.T1.2.2.16.14.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.2.2.16.14.4.1\">26.32</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_b\" id=\"S4.T1.2.2.16.14.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.2.2.16.14.5.1\">34.28</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_b\" id=\"S4.T1.2.2.16.14.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.2.2.16.14.6.1\">19.43</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_b\" id=\"S4.T1.2.2.16.14.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.2.2.16.14.7.1\">31.97</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_b\" id=\"S4.T1.2.2.16.14.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.2.2.16.14.8.1\">27.67</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_b\" id=\"S4.T1.2.2.16.14.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.2.2.16.14.9.1\">27.78</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_r\" id=\"S4.T1.2.2.16.14.10\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.2.2.16.14.10.1\">31.08</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Main results on the IWSLT and Europarl datasets.\nThe best results are in <span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.7.1\">bold</span>, and the second best results are <span class=\"ltx_text ltx_ulem_uline\" id=\"S4.T1.8.2\">underlined</span>.  indicates cited results; others were obtained by our experimentation.\n</figcaption>\n</figure>",
            "capture": "Table 1: Main results on the IWSLT and Europarl datasets.\nThe best results are in bold, and the second best results are underlined.  indicates cited results; others were obtained by our experimentation.\n"
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T2\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T2.1\" style=\"width:173.4pt;height:130.3pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-9.1pt,6.9pt) scale(0.904668313233288,0.904668313233288) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.2.1.1\">Dataset</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.2.1.2\">Method</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.2.1.3\">Avg. BLEU</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.2.1.4\">Wins</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.2.1.5\">Losses</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.3.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_tt\" id=\"S4.T2.1.1.3.2.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.1.1.3.2.1.1\">IWSLT</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S4.T2.1.1.3.2.2\">Direct</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.1.1.3.2.3\">17.28</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.1.1.3.2.4\">2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S4.T2.1.1.3.2.5\">4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.4.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T2.1.1.4.3.1\">Ensemble</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.4.3.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.4.3.2.1\">18.23</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.4.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.4.3.3.1\">4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.4.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.4.3.4.1\">2</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.5.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.5.4.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.1.1.5.4.1.1\">Europarl</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.5.4.2\">Direct</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.5.4.3\">27.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.5.4.4\">4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.5.4.5\">52</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.6.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T2.1.1.6.5.1\">Ensemble</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.6.5.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.6.5.2.1\">28.44</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.6.5.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.6.5.3.1\">52</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.6.5.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.6.5.4.1\">4</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.7.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_tt\" id=\"S4.T2.1.1.7.6.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.1.1.7.6.1.1\">Overall</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S4.T2.1.1.7.6.2\">Direct</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.1.1.7.6.3\">26.83</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.1.1.7.6.4\">6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S4.T2.1.1.7.6.5\">56</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.8.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T2.1.1.8.7.1\">Ensemble</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.8.7.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.8.7.2.1\">27.45</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.8.7.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.8.7.3.1\">56</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.8.7.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.8.7.4.1\">6</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1\">\n-value</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" colspan=\"4\" id=\"S4.T2.1.1.1.2\">3e-11</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Performance statistics on all 66 zero-shot directions in both datasets. -value is given by a two-sided binomial test.</figcaption>\n</figure>",
            "capture": "Table 2: Performance statistics on all 66 zero-shot directions in both datasets. -value is given by a two-sided binomial test."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T3\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T3.1\" style=\"width:216.8pt;height:91.8pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-19.1pt,8.1pt) scale(0.849955605548041,0.849955605548041) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T3.1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"S4.T3.1.1.2.1.1\">Voting scheme</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.2.1.2\">BLEU</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.2.1.3\">BLEU1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.2.1.4\">BLEU2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.2.1.5\">BLEU3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.1.1.2.1.6\">BLEU4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"S4.T3.1.1.3.2.1\">None (beam search)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.3.2.2\">25.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.3.2.3\">56.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.3.2.4\">30.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.3.2.5\">19.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.1.1.3.2.6\">12.78</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"S4.T3.1.1.4.3.1\">Total-sum</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.4.3.2\">25.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.4.3.3\">56.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.4.3.4\">30.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.4.3.5\">19.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.1.1.4.3.6\">12.65</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S4.T3.1.1.5.4.1\">Max</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.5.4.2\">25.81</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.5.4.3\">56.89</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.5.4.4\">30.76</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.5.4.5\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T3.1.1.5.4.5.1\">\\ul</span>19.51</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.1.5.4.6\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T3.1.1.5.4.6.1\">\\ul</span>13.09</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.6.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S4.T3.1.1.6.5.1\">0/1</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.6.5.2\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T3.1.1.6.5.2.1\">\\ul</span>25.84</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.6.5.3\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T3.1.1.6.5.3.1\">\\ul</span>56.99</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.6.5.4\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T3.1.1.6.5.4.1\">\\ul</span>30.78</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.6.5.5\">19.49</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.1.6.5.6\">13.05</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\" id=\"S4.T3.1.1.1.1\">Top- sum (ours)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T3.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.2.1\">26.10</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T3.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.3.1\">57.07</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T3.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.4.1\">31.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T3.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.5.1\">19.76</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S4.T3.1.1.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.6.1\">13.28</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Comparison of different ensemble variants. We used Italian-to-Dutch translation in the Europarl dataset as a testbed due to the limit of time and resources.</figcaption>\n</figure>",
            "capture": "Table 3: Comparison of different ensemble variants. We used Italian-to-Dutch translation in the Europarl dataset as a testbed due to the limit of time and resources."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T4\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T4.1\" style=\"width:216.8pt;height:172.2pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-16.3pt,12.9pt) scale(0.869460287412418,0.869460287412418) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T4.1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T4.1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"S4.T4.1.1.1.1.1\">Method</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.1.1.1.1.2\">BLEU</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.1.1.1.1.3\">BLEU1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.1.1.1.1.4\">BLEU2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.1.1.1.1.5\">BLEU3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.1.1.1.6\">BLEU4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.1.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"S4.T4.1.1.2.2.1\">Direct</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T4.1.1.2.2.2\">25.33</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T4.1.1.2.2.3\">56.32</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T4.1.1.2.2.4\">30.08</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T4.1.1.2.2.5\">19.01</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T4.1.1.2.2.6\">12.78</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.1.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S4.T4.1.1.3.3.1\">Pivoting (en)</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.1.1.3.3.2\">25.08</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.1.1.3.3.3\">56.76</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.1.1.3.3.4\">30.29</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.1.1.3.3.5\">19.06</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T4.1.1.3.3.6\">12.66</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.1.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S4.T4.1.1.4.4.1\">Pivoting (es)</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.1.1.4.4.2\">24.40</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.1.1.4.4.3\">55.38</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.1.1.4.4.4\">29.08</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.1.1.4.4.5\">18.22</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T4.1.1.4.4.6\">12.09</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.1.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S4.T4.1.1.5.5.1\">Pivoting (pt)</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.1.1.5.5.2\">24.34</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.1.1.5.5.3\">55.46</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.1.1.5.5.4\">29.02</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.1.1.5.5.5\">18.13</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T4.1.1.5.5.6\">12.02</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.1.6.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S4.T4.1.1.6.6.1\">Pivoting (fr)</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.1.1.6.6.2\">24.20</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.1.1.6.6.3\">55.41</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.1.1.6.6.4\">29.02</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.1.1.6.6.5\">18.00</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T4.1.1.6.6.6\">11.84</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.1.7.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S4.T4.1.1.7.7.1\">Pivoting (de)</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.1.1.7.7.2\">23.65</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.1.1.7.7.3\">55.33</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.1.1.7.7.4\">28.69</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.1.1.7.7.5\">17.67</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T4.1.1.7.7.6\">11.54</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.1.8.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S4.T4.1.1.8.8.1\">Pivoting (da)</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.1.1.8.8.2\">23.12</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.1.1.8.8.3\">54.81</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.1.1.8.8.4\">27.96</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.1.1.8.8.5\">17.12</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T4.1.1.8.8.6\">11.12</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.1.9.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S4.T4.1.1.9.9.1\">Pivoting (fi)</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.1.1.9.9.2\">20.74</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.1.1.9.9.3\">53.54</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.1.1.9.9.4\">26.10</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.1.1.9.9.5\">15.43</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T4.1.1.9.9.6\">9.79</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.1.10.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"S4.T4.1.1.10.10.1\">All ensemble</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T4.1.1.10.10.2\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T4.1.1.10.10.2.1\">\\ul</span>25.94</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T4.1.1.10.10.3\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T4.1.1.10.10.3.1\">\\ul</span>57.04</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T4.1.1.10.10.4\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T4.1.1.10.10.4.1\">\\ul</span>30.85</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T4.1.1.10.10.5\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T4.1.1.10.10.5.1\">\\ul</span>19.61</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T4.1.1.10.10.6\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T4.1.1.10.10.6.1\">\\ul</span>13.18</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.1.11.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\" id=\"S4.T4.1.1.11.11.1\">Top-3 ensemble</th>\n<td class=\"ltx_td ltx_align_right ltx_border_b\" id=\"S4.T4.1.1.11.11.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.1.11.11.2.1\">26.10</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_b\" id=\"S4.T4.1.1.11.11.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.1.11.11.3.1\">57.07</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_b\" id=\"S4.T4.1.1.11.11.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.1.11.11.4.1\">31.00</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_b\" id=\"S4.T4.1.1.11.11.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.1.11.11.5.1\">19.76</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_r\" id=\"S4.T4.1.1.11.11.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.1.11.11.6.1\">13.28</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>The performance of individuals and their ensembles for Italian-to-Dutch translation on Europarl.</figcaption>\n</figure>",
            "capture": "Table 4: The performance of individuals and their ensembles for Italian-to-Dutch translation on Europarl."
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T5\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T5.1\" style=\"width:216.8pt;height:163.1pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-23.2pt,17.4pt) scale(0.823935596815561,0.823935596815561) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T5.1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.1.1\">\n<td class=\"ltx_td ltx_border_l ltx_border_r ltx_border_t\" id=\"S4.T5.1.1.1.1.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\" id=\"S4.T5.1.1.1.1.2\">Method</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.1.1.1.3\">BLEU</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.1.1.1.4\">BLEU1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.1.1.1.5\">BLEU2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.1.1.1.6\">BLEU3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T5.1.1.1.1.7\">BLEU4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.2.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"S4.T5.1.1.2.2.1\" rowspan=\"5\"><span class=\"ltx_text\" id=\"S4.T5.1.1.2.2.1.1\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" id=\"S4.T5.1.1.2.2.1.1.1\" style=\"width:6.8pt;height:32.1pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:32.1pt;transform:translate(-12.63pt,-12.63pt) rotate(-90deg) ;\">\n<span class=\"ltx_p\" id=\"S4.T5.1.1.2.2.1.1.1.1\">IWSLT</span>\n</span></span></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" colspan=\"2\" id=\"S4.T5.1.1.2.2.2\">EBBS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.1.2.2.3\">19.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.1.2.2.4\">51.87</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.1.2.2.5\">25.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.1.2.2.6\">13.88</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T5.1.1.2.2.7\">8.02</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.3.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T5.1.1.3.3.1\" rowspan=\"4\">\n<span class=\"ltx_text\" id=\"S4.T5.1.1.3.3.1.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T5.1.1.3.3.1.1.1\">\n<span class=\"ltx_tr\" id=\"S4.T5.1.1.3.3.1.1.1.1\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T5.1.1.3.3.1.1.1.1.1\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" id=\"S4.T5.1.1.3.3.1.1.1.1.1.1\" style=\"width:6.8pt;height:27.1pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:27.1pt;transform:translate(-10.14pt,-10.14pt) rotate(-90deg) ;\">\n<span class=\"ltx_p\" id=\"S4.T5.1.1.3.3.1.1.1.1.1.1.1\">Direct</span>\n</span></span></span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T5.1.1.3.3.2\">No distillation</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.1.3.3.3\">17.46</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.1.3.3.4\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T5.1.1.3.3.4.1\">\\ul</span>50.49</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.1.3.3.5\">23.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.1.3.3.6\">12.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T5.1.1.3.3.7\">6.66</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.4.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T5.1.1.4.4.1\">Self-distillation</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.4.4.2\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T5.1.1.4.4.2.1\">\\ul</span>18.10</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.4.4.3\">50.37</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.4.4.4\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T5.1.1.4.4.4.1\">\\ul</span>23.53</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.4.4.5\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T5.1.1.4.4.5.1\">\\ul</span>12.63</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T5.1.1.4.4.6\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T5.1.1.4.4.6.1\">\\ul</span>7.17</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.5.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T5.1.1.5.5.1\">Union distillation</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.5.5.2\">17.80</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.5.5.3\">49.21</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.5.5.4\">23.01</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.5.5.5\">12.51</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T5.1.1.5.5.6\">7.10</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.6.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T5.1.1.6.6.1\">EBBS distillation</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.6.6.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.6.6.2.1\">20.13</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.6.6.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.6.6.3.1\">53.20</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.6.6.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.6.6.4.1\">26.06</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.6.6.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.6.6.5.1\">14.33</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T5.1.1.6.6.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.6.6.6.1\">8.26</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.7.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_tt\" id=\"S4.T5.1.1.7.7.1\" rowspan=\"5\"><span class=\"ltx_text\" id=\"S4.T5.1.1.7.7.1.1\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" id=\"S4.T5.1.1.7.7.1.1.1\" style=\"width:8.9pt;height:38.5pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:38.5pt;transform:translate(-14.82pt,-13.85pt) rotate(-90deg) ;\">\n<span class=\"ltx_p\" id=\"S4.T5.1.1.7.7.1.1.1.1\">Europarl</span>\n</span></span></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" colspan=\"2\" id=\"S4.T5.1.1.7.7.2\">EBBS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T5.1.1.7.7.3\">26.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T5.1.1.7.7.4\">57.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T5.1.1.7.7.5\">31.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T5.1.1.7.7.6\">19.76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S4.T5.1.1.7.7.7\">13.28</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.8.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\" id=\"S4.T5.1.1.8.8.1\" rowspan=\"4\">\n<span class=\"ltx_text\" id=\"S4.T5.1.1.8.8.1.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T5.1.1.8.8.1.1.1\">\n<span class=\"ltx_tr\" id=\"S4.T5.1.1.8.8.1.1.1.1\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T5.1.1.8.8.1.1.1.1.1\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" id=\"S4.T5.1.1.8.8.1.1.1.1.1.1\" style=\"width:6.8pt;height:27.1pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:27.1pt;transform:translate(-10.14pt,-10.14pt) rotate(-90deg) ;\">\n<span class=\"ltx_p\" id=\"S4.T5.1.1.8.8.1.1.1.1.1.1.1\">Direct</span>\n</span></span></span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T5.1.1.8.8.2\">No distillation</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.1.8.8.3\">25.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.1.8.8.4\">56.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.1.8.8.5\">30.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.1.8.8.6\">19.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T5.1.1.8.8.7\">12.78</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.9.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T5.1.1.9.9.1\">Self-distillation</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.9.9.2\">25.44</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.9.9.3\">56.54</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.9.9.4\">30.28</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.9.9.5\">19.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T5.1.1.9.9.6\">12.79</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.10.10\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T5.1.1.10.10.1\">Union distillation</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.10.10.2\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T5.1.1.10.10.2.1\">\\ul</span>25.53</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.10.10.3\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T5.1.1.10.10.3.1\">\\ul</span>56.58</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.10.10.4\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T5.1.1.10.10.4.1\">\\ul</span>30.34</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.10.10.5\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T5.1.1.10.10.5.1\">\\ul</span>19.18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T5.1.1.10.10.6\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T5.1.1.10.10.6.1\">\\ul</span>12.91</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.11.11\">\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\" id=\"S4.T5.1.1.11.11.1\">EBBS distillation</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T5.1.1.11.11.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.11.11.2.1\">25.92</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T5.1.1.11.11.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.11.11.3.1\">56.76</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T5.1.1.11.11.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.11.11.4.1\">30.68</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T5.1.1.11.11.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.11.11.5.1\">19.57</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S4.T5.1.1.11.11.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.11.11.6.1\">13.24</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>Comparison of various distillation methods for Italian-to-Dutch translation.</figcaption>\n</figure>",
            "capture": "Table 5: Comparison of various distillation methods for Italian-to-Dutch translation."
        },
        "6": {
            "table_html": "<figure class=\"ltx_table\" id=\"A2.T6\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"A2.T6.13\" style=\"width:151.8pt;height:207.4pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(3.4pt,-4.7pt) scale(1.04728691736727,1.04728691736727) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"A2.T6.13.13\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A2.T6.13.13.14.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_t\" id=\"A2.T6.13.13.14.1.1\">Code</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"A2.T6.13.13.14.1.2\">Language</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T6.13.13.14.1.3\">IWSLT</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"A2.T6.13.13.14.1.4\">Europarl</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A2.T6.1.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_t\" id=\"A2.T6.1.1.1.2\">da</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A2.T6.1.1.1.3\">Danish</td>\n<td class=\"ltx_td ltx_border_t\" id=\"A2.T6.1.1.1.4\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A2.T6.1.1.1.1\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T6.2.2.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_l\" id=\"A2.T6.2.2.2.2\">de</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A2.T6.2.2.2.3\">German</td>\n<td class=\"ltx_td\" id=\"A2.T6.2.2.2.4\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A2.T6.2.2.2.1\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T6.4.4.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_l\" id=\"A2.T6.4.4.4.3\">en</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A2.T6.4.4.4.4\">English</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T6.3.3.3.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A2.T6.4.4.4.2\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T6.5.5.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_l\" id=\"A2.T6.5.5.5.2\">es</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A2.T6.5.5.5.3\">Spanish</td>\n<td class=\"ltx_td\" id=\"A2.T6.5.5.5.4\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A2.T6.5.5.5.1\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T6.6.6.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_l\" id=\"A2.T6.6.6.6.2\">fi</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A2.T6.6.6.6.3\">Finnish</td>\n<td class=\"ltx_td\" id=\"A2.T6.6.6.6.4\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A2.T6.6.6.6.1\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T6.7.7.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_l\" id=\"A2.T6.7.7.7.2\">fr</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A2.T6.7.7.7.3\">French</td>\n<td class=\"ltx_td\" id=\"A2.T6.7.7.7.4\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A2.T6.7.7.7.1\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T6.9.9.9\">\n<td class=\"ltx_td ltx_align_center ltx_border_l\" id=\"A2.T6.9.9.9.3\">it</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A2.T6.9.9.9.4\">Italian</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T6.8.8.8.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A2.T6.9.9.9.2\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T6.11.11.11\">\n<td class=\"ltx_td ltx_align_center ltx_border_l\" id=\"A2.T6.11.11.11.3\">nl</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A2.T6.11.11.11.4\">Dutch</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T6.10.10.10.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A2.T6.11.11.11.2\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T6.12.12.12\">\n<td class=\"ltx_td ltx_align_center ltx_border_l\" id=\"A2.T6.12.12.12.2\">pt</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A2.T6.12.12.12.3\">Portugese</td>\n<td class=\"ltx_td\" id=\"A2.T6.12.12.12.4\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A2.T6.12.12.12.1\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T6.13.13.13\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l\" id=\"A2.T6.13.13.13.2\">ro</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\" id=\"A2.T6.13.13.13.3\">Romanian</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"A2.T6.13.13.13.1\"></td>\n<td class=\"ltx_td ltx_border_b ltx_border_r\" id=\"A2.T6.13.13.13.4\"></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 6: </span>Summary of the language codes.</figcaption>\n</figure>",
            "capture": "Table 6: Summary of the language codes."
        },
        "7": {
            "table_html": "<figure class=\"ltx_table\" id=\"A2.T7\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"A2.T7.1\" style=\"width:216.8pt;height:110pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(1.9pt,-1.0pt) scale(1.01828327029169,1.01828327029169) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"A2.T7.1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A2.T7.1.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"A2.T7.1.1.1.1.1\">Beam Size</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.1.1.1.1.2\">BLEU</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.1.1.1.1.3\">BLEU1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.1.1.1.1.4\">BLEU2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.1.1.1.1.5\">BLEU3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A2.T7.1.1.1.1.6\">BLEU4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T7.1.1.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"A2.T7.1.1.2.2.1\">1</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.1.1.2.2.2\">24.98</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.1.1.2.2.3\">56.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.1.1.2.2.4\">30.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.1.1.2.2.5\">18.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A2.T7.1.1.2.2.6\">12.46</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T7.1.1.3.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"A2.T7.1.1.3.3.1\">3</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T7.1.1.3.3.2\">25.99</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T7.1.1.3.3.3\">57.04</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T7.1.1.3.3.4\">30.92</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T7.1.1.3.3.5\">19.65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A2.T7.1.1.3.3.6\">13.17</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T7.1.1.4.4\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"A2.T7.1.1.4.4.1\">5</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T7.1.1.4.4.2\">\n<span class=\"ltx_ERROR undefined\" id=\"A2.T7.1.1.4.4.2.1\">\\ul</span>26.10</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T7.1.1.4.4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T7.1.1.4.4.3.1\">57.07</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T7.1.1.4.4.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T7.1.1.4.4.4.1\">31.00</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T7.1.1.4.4.5\">\n<span class=\"ltx_ERROR undefined\" id=\"A2.T7.1.1.4.4.5.1\">\\ul</span>19.76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A2.T7.1.1.4.4.6\">13.28</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T7.1.1.5.5\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"A2.T7.1.1.5.5.1\">7</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T7.1.1.5.5.2\">\n<span class=\"ltx_ERROR undefined\" id=\"A2.T7.1.1.5.5.2.1\">\\ul</span>26.10</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T7.1.1.5.5.3\">\n<span class=\"ltx_ERROR undefined\" id=\"A2.T7.1.1.5.5.3.1\">\\ul</span>57.06</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T7.1.1.5.5.4\">30.98</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T7.1.1.5.5.5\">19.74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A2.T7.1.1.5.5.6\">\n<span class=\"ltx_ERROR undefined\" id=\"A2.T7.1.1.5.5.6.1\">\\ul</span>13.30</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T7.1.1.6.6\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\" id=\"A2.T7.1.1.6.6.1\">9</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"A2.T7.1.1.6.6.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T7.1.1.6.6.2.1\">26.12</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"A2.T7.1.1.6.6.3\">57.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"A2.T7.1.1.6.6.4\">\n<span class=\"ltx_ERROR undefined\" id=\"A2.T7.1.1.6.6.4.1\">\\ul</span>30.99</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"A2.T7.1.1.6.6.5\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T7.1.1.6.6.5.1\">19.78</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"A2.T7.1.1.6.6.6\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T7.1.1.6.6.6.1\">13.31</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 7: </span>Beam size analysis for Italian-to-Dutch translation on the Europarl dataset.</figcaption>\n</figure>",
            "capture": "Table 7: Beam size analysis for Italian-to-Dutch translation on the Europarl dataset."
        },
        "8": {
            "table_html": "<figure class=\"ltx_table\" id=\"A4.T8\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"A4.T8.1\" style=\"width:216.8pt;height:180.3pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-10.7pt,8.9pt) scale(0.910432814503045,0.910432814503045) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"A4.T8.1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A4.T8.1.1.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" id=\"A4.T8.1.1.1.1.1\">Dataset</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\" id=\"A4.T8.1.1.1.1.2\">Method</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A4.T8.1.1.1.1.3\">BLEU</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A4.T8.1.1.1.1.4\">Entropy</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T8.1.1.2.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" id=\"A4.T8.1.1.2.2.1\" rowspan=\"5\"><span class=\"ltx_text\" id=\"A4.T8.1.1.2.2.1.1\">IWSLT</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" colspan=\"2\" id=\"A4.T8.1.1.2.2.2\">EBBS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A4.T8.1.1.2.2.3\">\n<span class=\"ltx_ERROR undefined\" id=\"A4.T8.1.1.2.2.3.1\">\\ul</span>19.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A4.T8.1.1.2.2.4\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T8.1.1.3.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A4.T8.1.1.3.3.1\" rowspan=\"4\">\n<span class=\"ltx_text\" id=\"A4.T8.1.1.3.3.1.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"A4.T8.1.1.3.3.1.1.1\">\n<span class=\"ltx_tr\" id=\"A4.T8.1.1.3.3.1.1.1.1\">\n<span class=\"ltx_td ltx_align_left\" id=\"A4.T8.1.1.3.3.1.1.1.1.1\">Direct</span></span>\n<span class=\"ltx_tr\" id=\"A4.T8.1.1.3.3.1.1.1.2\">\n<span class=\"ltx_td ltx_align_left\" id=\"A4.T8.1.1.3.3.1.1.1.2.1\">translation</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A4.T8.1.1.3.3.2\">No distillation</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A4.T8.1.1.3.3.3\">17.46</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A4.T8.1.1.3.3.4\">2.46</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T8.1.1.4.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A4.T8.1.1.4.4.1\">Self-distillation</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A4.T8.1.1.4.4.2\">18.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A4.T8.1.1.4.4.3\">1.62</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T8.1.1.5.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A4.T8.1.1.5.5.1\">Union distillation</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A4.T8.1.1.5.5.2\">17.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A4.T8.1.1.5.5.3\">1.80</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T8.1.1.6.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A4.T8.1.1.6.6.1\">EBBS distillation</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A4.T8.1.1.6.6.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A4.T8.1.1.6.6.2.1\">20.13</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A4.T8.1.1.6.6.3\">1.70</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T8.1.1.7.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_tt\" id=\"A4.T8.1.1.7.7.1\" rowspan=\"5\"><span class=\"ltx_text\" id=\"A4.T8.1.1.7.7.1.1\">Europarl</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" colspan=\"2\" id=\"A4.T8.1.1.7.7.2\">EBBS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A4.T8.1.1.7.7.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A4.T8.1.1.7.7.3.1\">26.10</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"A4.T8.1.1.7.7.4\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T8.1.1.8.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\" id=\"A4.T8.1.1.8.8.1\" rowspan=\"4\">\n<span class=\"ltx_text\" id=\"A4.T8.1.1.8.8.1.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"A4.T8.1.1.8.8.1.1.1\">\n<span class=\"ltx_tr\" id=\"A4.T8.1.1.8.8.1.1.1.1\">\n<span class=\"ltx_td ltx_align_left\" id=\"A4.T8.1.1.8.8.1.1.1.1.1\">Direct</span></span>\n<span class=\"ltx_tr\" id=\"A4.T8.1.1.8.8.1.1.1.2\">\n<span class=\"ltx_td ltx_align_left\" id=\"A4.T8.1.1.8.8.1.1.1.2.1\">translation</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A4.T8.1.1.8.8.2\">No distillation</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A4.T8.1.1.8.8.3\">25.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A4.T8.1.1.8.8.4\">2.06</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T8.1.1.9.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A4.T8.1.1.9.9.1\">Self-distillation</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A4.T8.1.1.9.9.2\">25.44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A4.T8.1.1.9.9.3\">1.44</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T8.1.1.10.10\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A4.T8.1.1.10.10.1\">Union distillation</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A4.T8.1.1.10.10.2\">25.53</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A4.T8.1.1.10.10.3\">1.59</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T8.1.1.11.11\">\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\" id=\"A4.T8.1.1.11.11.1\">EBBS distillation</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"A4.T8.1.1.11.11.2\">\n<span class=\"ltx_ERROR undefined\" id=\"A4.T8.1.1.11.11.2.1\">\\ul</span>25.92</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"A4.T8.1.1.11.11.3\">1.51</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 8: </span>Entropy of various distillation techniques on Italian-to-Dutch translation.</figcaption>\n</figure>",
            "capture": "Table 8: Entropy of various distillation techniques on Italian-to-Dutch translation."
        },
        "9": {
            "table_html": "<figure class=\"ltx_table\" id=\"A4.T9\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"A4.T9.1\" style=\"width:433.6pt;height:446.9pt;vertical-align:-0.8pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-45.2pt,46.5pt) scale(0.827517484238196,0.827517484238196) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"A4.T9.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A4.T9.1.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" colspan=\"2\" id=\"A4.T9.1.1.1.1.1\">\n<span class=\"ltx_text ltx_font_bold\" id=\"A4.T9.1.1.1.1.1.1\">IWSLT</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A4.T9.1.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"A4.T9.1.1.2.1.1\" style=\"padding-bottom:10.0pt;\">Input</th>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A4.T9.1.1.2.1.2\" style=\"padding-bottom:10.0pt;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"A4.T9.1.1.2.1.2.1\">\n<tr class=\"ltx_tr\" id=\"A4.T9.1.1.2.1.2.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T9.1.1.2.1.2.1.1.1\">ho sempre creduto che trasformare la paura in divertimento sia il dono della creativit\u00e0.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T9.1.1.2.1.2.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T9.1.1.2.1.2.1.2.1\">(<span class=\"ltx_text ltx_font_italic\" id=\"A4.T9.1.1.2.1.2.1.2.1.1\">I have always believed that turning fear into fun is the gift of creativity.</span>)</td>\n</tr>\n</table>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T9.1.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"A4.T9.1.1.3.2.1\">Reference</th>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A4.T9.1.1.3.2.2\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"A4.T9.1.1.3.2.2.1\">\n<tr class=\"ltx_tr\" id=\"A4.T9.1.1.3.2.2.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T9.1.1.3.2.2.1.1.1\">Ik heb altijd geloofd dat het omzetten van angst in plezier de gift is van creativiteit.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T9.1.1.3.2.2.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T9.1.1.3.2.2.1.2.1\">(<span class=\"ltx_text ltx_font_italic\" id=\"A4.T9.1.1.3.2.2.1.2.1.1\">I have always believed that turning fear into joy is the gift of creativity.</span>)</td>\n</tr>\n</table>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T9.1.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"A4.T9.1.1.4.3.1\" style=\"padding-bottom:10.0pt;\">Direct translation</th>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A4.T9.1.1.4.3.2\" style=\"padding-bottom:10.0pt;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"A4.T9.1.1.4.3.2.1\">\n<tr class=\"ltx_tr\" id=\"A4.T9.1.1.4.3.2.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T9.1.1.4.3.2.1.1.1\">Ik geloofde altijd dat het transformeren van angst in ontspanning de gift van creativiteit is</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T9.1.1.4.3.2.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T9.1.1.4.3.2.1.2.1\">(<span class=\"ltx_text ltx_font_italic\" id=\"A4.T9.1.1.4.3.2.1.2.1.1\">I always believed that transforming anxiety into relaxation is the gift of creativity.</span>)</td>\n</tr>\n</table>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T9.1.1.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"A4.T9.1.1.5.4.1\" style=\"padding-bottom:10.0pt;\">English-pivoting</th>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A4.T9.1.1.5.4.2\" style=\"padding-bottom:10.0pt;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"A4.T9.1.1.5.4.2.1\">\n<tr class=\"ltx_tr\" id=\"A4.T9.1.1.5.4.2.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T9.1.1.5.4.2.1.1.1\">Omdat ik altijd geloofde om angst in plezier te transformeren, is het geschenk van creativiteit.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T9.1.1.5.4.2.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T9.1.1.5.4.2.1.2.1\">(<span class=\"ltx_text ltx_font_italic\" id=\"A4.T9.1.1.5.4.2.1.2.1.1\">Because I always believed to transform fear into pleasure is the gift of creativity.</span>)</td>\n</tr>\n</table>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T9.1.1.6.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"A4.T9.1.1.6.5.1\" style=\"padding-bottom:10.0pt;\">Romanian-pivoting</th>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A4.T9.1.1.6.5.2\" style=\"padding-bottom:10.0pt;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"A4.T9.1.1.6.5.2.1\">\n<tr class=\"ltx_tr\" id=\"A4.T9.1.1.6.5.2.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T9.1.1.6.5.2.1.1.1\">In feite hebben we altijd gedacht dat het transformeren van angst in divergentie de gift van creativiteit is.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T9.1.1.6.5.2.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T9.1.1.6.5.2.1.2.1\">(<span class=\"ltx_text ltx_font_italic\" id=\"A4.T9.1.1.6.5.2.1.2.1.1\">In fact, we have always thought that transforming fear into divergence is the gift of creativity</span>)</td>\n</tr>\n</table>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T9.1.1.7.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"A4.T9.1.1.7.6.1\">EBBS</th>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A4.T9.1.1.7.6.2\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"A4.T9.1.1.7.6.2.1\">\n<tr class=\"ltx_tr\" id=\"A4.T9.1.1.7.6.2.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T9.1.1.7.6.2.1.1.1\">Ik geloofde altijd dat het transformeren van angst in plezier de gift van creativiteit is.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T9.1.1.7.6.2.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T9.1.1.7.6.2.1.2.1\">(<span class=\"ltx_text ltx_font_italic\" id=\"A4.T9.1.1.7.6.2.1.2.1.1\">I always believed that transforming fear into pleasure is the gift of creativity.</span>)</td>\n</tr>\n</table>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T9.1.1.8.7\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_tt\" colspan=\"2\" id=\"A4.T9.1.1.8.7.1\">\n<span class=\"ltx_text ltx_font_bold\" id=\"A4.T9.1.1.8.7.1.1\">Europarl</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T9.1.1.9.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"A4.T9.1.1.9.8.1\" style=\"padding-bottom:10.0pt;\">Input</th>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A4.T9.1.1.9.8.2\" style=\"padding-bottom:10.0pt;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"A4.T9.1.1.9.8.2.1\">\n<tr class=\"ltx_tr\" id=\"A4.T9.1.1.9.8.2.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T9.1.1.9.8.2.1.1.1\">si poteva avvertire una forte tensione.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T9.1.1.9.8.2.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T9.1.1.9.8.2.1.2.1\">(<span class=\"ltx_text ltx_font_italic\" id=\"A4.T9.1.1.9.8.2.1.2.1.1\">a strong tension could be felt.</span>)</td>\n</tr>\n</table>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T9.1.1.10.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"A4.T9.1.1.10.9.1\">Reference</th>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A4.T9.1.1.10.9.2\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"A4.T9.1.1.10.9.2.1\">\n<tr class=\"ltx_tr\" id=\"A4.T9.1.1.10.9.2.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T9.1.1.10.9.2.1.1.1\">Er was veel spanning zichtbaar.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T9.1.1.10.9.2.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T9.1.1.10.9.2.1.2.1\">(<span class=\"ltx_text ltx_font_italic\" id=\"A4.T9.1.1.10.9.2.1.2.1.1\"> There was a lot of tension visible.</span>)</td>\n</tr>\n</table>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T9.1.1.11.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"A4.T9.1.1.11.10.1\" style=\"padding-bottom:10.0pt;\">Direct translation</th>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A4.T9.1.1.11.10.2\" style=\"padding-bottom:10.0pt;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"A4.T9.1.1.11.10.2.1\">\n<tr class=\"ltx_tr\" id=\"A4.T9.1.1.11.10.2.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T9.1.1.11.10.2.1.1.1\">Er was grote spanning te ontgaan.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T9.1.1.11.10.2.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T9.1.1.11.10.2.1.2.1\">(<span class=\"ltx_text ltx_font_italic\" id=\"A4.T9.1.1.11.10.2.1.2.1.1\">There was great tension to be escaped.</span>)</td>\n</tr>\n</table>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T9.1.1.12.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"A4.T9.1.1.12.11.1\" style=\"padding-bottom:10.0pt;\">English-pivoting</th>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A4.T9.1.1.12.11.2\" style=\"padding-bottom:10.0pt;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"A4.T9.1.1.12.11.2.1\">\n<tr class=\"ltx_tr\" id=\"A4.T9.1.1.12.11.2.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T9.1.1.12.11.2.1.1.1\">Er zou veel spanningen kunnen zijn ontstaan.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T9.1.1.12.11.2.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T9.1.1.12.11.2.1.2.1\">(<span class=\"ltx_text ltx_font_italic\" id=\"A4.T9.1.1.12.11.2.1.2.1.1\">A lot of tensions could have arisen.</span>)</td>\n</tr>\n</table>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T9.1.1.13.12\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"A4.T9.1.1.13.12.1\" style=\"padding-bottom:10.0pt;\">Spanish-pivoting</th>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A4.T9.1.1.13.12.2\" style=\"padding-bottom:10.0pt;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"A4.T9.1.1.13.12.2.1\">\n<tr class=\"ltx_tr\" id=\"A4.T9.1.1.13.12.2.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T9.1.1.13.12.2.1.1.1\">Mocht een sterke spanning kunnen worden aangekondigd.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T9.1.1.13.12.2.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T9.1.1.13.12.2.1.2.1\">(<span class=\"ltx_text ltx_font_italic\" id=\"A4.T9.1.1.13.12.2.1.2.1.1\">Should a strong tension can be announced.</span>)</td>\n</tr>\n</table>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T9.1.1.14.13\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" id=\"A4.T9.1.1.14.13.1\">EBBS</th>\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\" id=\"A4.T9.1.1.14.13.2\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"A4.T9.1.1.14.13.2.1\">\n<tr class=\"ltx_tr\" id=\"A4.T9.1.1.14.13.2.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T9.1.1.14.13.2.1.1.1\">Er was veel spanning geweest.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T9.1.1.14.13.2.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T9.1.1.14.13.2.1.2.1\">(<span class=\"ltx_text ltx_font_italic\" id=\"A4.T9.1.1.14.13.2.1.2.1.1\">There had been a lot of tension.</span>)</td>\n</tr>\n</table>\n</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 9: </span>Case studies, where the source language is Italian and the target is Dutch. We provide English interpretations in (<span class=\"ltx_text ltx_font_italic\" id=\"A4.T9.3.1\">italic</span>) for non-English text using Google Translate.</figcaption>\n</figure>",
            "capture": "Table 9: Case studies, where the source language is Italian and the target is Dutch. We provide English interpretations in (italic) for non-English text using Google Translate."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.00144v1_figure_1.png",
            "caption": "Figure 1: Overview of our EBBS algorithm."
        },
        "2": {
            "figure_path": "2403.00144v1_figure_2.png",
            "caption": "Figure 1: Overview of our EBBS algorithm."
        },
        "3": {
            "figure_path": "2403.00144v1_figure_3.png",
            "caption": "Figure 2: Analysis of the number of ensemble components for Italian-to-Dutch translation on Europarl."
        },
        "4": {
            "figure_path": "2403.00144v1_figure_4.png",
            "caption": "Figure 3: Inference time analysis on the test set of Italian to Dutch translation from the Europarl dataset. Experiments were conducted on an AMD EPYC 7313 CPU and an NVIDIA RTX A6000 GPU, with an inference batch size of 300 samples."
        }
    },
    "references": [
        {
            "1": {
                "title": "Translating from under-resourced languages: comparing direct transfer against pivot translation.",
                "author": "Bogdan Babych, Anthony Hartley, and Serge Sharoff.",
                "venue": "In Proceedings of Machine Translation Summit, 2007.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Neural machine translation by jointly learning to align and translate.",
                "author": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.",
                "venue": "In International Conference on Learning Representations, 2015.",
                "url": null
            }
        },
        {
            "3": {
                "title": "Multilingual machine translation with hyper-adapters.",
                "author": "Christos Baziotis, Mikel Artetxe, James Cross, and Shruti Bhosale.",
                "venue": "In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1170\u20131185, 2022.",
                "url": null
            }
        },
        {
            "4": {
                "title": "Mathematical Statistics: Basic Ideas and Selected Topics.",
                "author": "Peter J Bickel and Kjell A Doksum.",
                "venue": "CRC Press, 2015.",
                "url": null
            }
        },
        {
            "5": {
                "title": "Bagging predictors.",
                "author": "Leo Breiman.",
                "venue": "Machine Learning, 24:123\u2013140, 1996.",
                "url": null
            }
        },
        {
            "6": {
                "title": "A statistical approach to machine translation.",
                "author": "Peter F. Brown, John Cocke, Stephen A. Della Pietra, Vincent J. Della Pietra, Fredrick Jelinek, John D. Lafferty, Robert L. Mercer, and Paul S. Roossin.",
                "venue": "Computational Linguistics, 16(2):79\u201385, 1990.",
                "url": null
            }
        },
        {
            "7": {
                "title": "Analyzing bagging.",
                "author": "Peter B\u00fchlmann and Bin Yu.",
                "venue": "The Annals of Statistics, 30(4):927\u2013961, 2002.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Overview of the IWSLT 2017 evaluation campaign.",
                "author": "Mauro Cettolo, Marcello Federico, Luisa Bentivogli, Jan Niehues, Sebastian St\u00fcker, Katsuhito Sudoh, Koichiro Yoshino, and Christian Federmann.",
                "venue": "In Sakriani Sakti and Masao Utiyama, editors, Proceedings of the International Conference on Spoken Language Translation, page 2\u201314, 2017.",
                "url": null
            }
        },
        {
            "9": {
                "title": "Language-family adapters for low-resource multilingual neural machine translation.",
                "author": "Alexandra Chronopoulou, Dario Stojanovski, and Alexander Fraser.",
                "venue": "In Atul Kr. Ojha, Chao-hong Liu, Ekaterina Vylomova, Flammie Pirinen, Jade Abbott, Jonathan Washington, Nathaniel Oco, Valentin Malykh, Varvara Logacheva, and Xiaobing Zhao, editors, Proceedings of the the Workshop on Technologies for Machine Translation of Low-Resource Languages, pages 59\u201372, 2023.",
                "url": null
            }
        },
        {
            "10": {
                "title": "MMCR4NLP: Multilingual multiway corpora repository for natural language processing.",
                "author": "Raj Dabre and Sadao Kurohashi.",
                "venue": "arXiv preprint arXiv:1710.01025, 2017.",
                "url": null
            }
        },
        {
            "11": {
                "title": "A survey of multilingual neural machine translation.",
                "author": "Raj Dabre, Chenhui Chu, and Anoop Kunchukuttan.",
                "venue": "ACM Computing Surveys, 53(5), 2020.",
                "url": null
            }
        },
        {
            "12": {
                "title": "A survey on ensemble learning.",
                "author": "Xibin Dong, Zhiwen Yu, Wenming Cao, Yifan Shi, and Qianli Ma.",
                "venue": "Frontiers of Computer Science, 14:241\u2013258, 2020.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Is MAP decoding all you need? The inadequacy of the mode in neural machine translation.",
                "author": "Bryan Eikema and Wilker Aziz.",
                "venue": "In Proceedings of the International Conference on Computational Linguistics, pages 4506\u20134520, 2020.",
                "url": null
            }
        },
        {
            "14": {
                "title": "Beyond English-centric multilingual machine translation.",
                "author": "Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Michael Auli, and Armand Joulin.",
                "venue": "Journal of Machine Learning Research, 22(107):1\u201348, 2021.",
                "url": null
            }
        },
        {
            "15": {
                "title": "Ensemble distillation for neural machine translation.",
                "author": "Markus Freitag, Yaser Al-Onaizan, and Baskaran Sankaran.",
                "venue": "arXiv preprint arXiv:1702.01802, 2017.",
                "url": null
            }
        },
        {
            "16": {
                "title": "Ensemble deep learning: A review.",
                "author": "M.A. Ganaie, Minghui Hu, A.K. Malik, M. Tanveer, and P.N. Suganthan.",
                "venue": "Engineering Applications of Artificial Intelligence, 115:105151, 2022.",
                "url": null
            }
        },
        {
            "17": {
                "title": "Improved zero-shot neural machine translation via ignoring spurious correlations.",
                "author": "Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li.",
                "venue": "In Anna Korhonen, David Traum, and Llu\u00eds M\u00e0rquez, editors, Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 1258\u20131268, 2019.",
                "url": null
            }
        },
        {
            "18": {
                "title": "Double q-learning.",
                "author": "Hado Hasselt.",
                "venue": "In J. Lafferty, C. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems, 2010.",
                "url": null
            }
        },
        {
            "19": {
                "title": "Multi-class AdaBoost.",
                "author": "Trevor Hastie, Saharon Rosset, Ji Zhu, and Hui Zou.",
                "venue": "Statistics and its Interface, 2(3):349\u2013360, 2009.",
                "url": null
            }
        },
        {
            "20": {
                "title": "Gradient-based gradual pruning for language-specific multilingual neural machine translation.",
                "author": "Dan He, Minh-Quang Pham, Thanh-Le Ha, and Marco Turchi.",
                "venue": "In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 654\u2013670, 2023.",
                "url": null
            }
        },
        {
            "21": {
                "title": "Ensembling of distilled models from multi-task teachers for constrained resource language pairs.",
                "author": "Amr Hendy, Esraa A. Gad, Mohamed Abdelghaffar, Jailan S. ElMosalami, Mohamed Afify, Ahmed Y. Tawfik, and Hany Hassan Awadalla.",
                "venue": "In Proceedings of the Conference on Machine Translation, pages 130\u2013135, 2021.",
                "url": null
            }
        },
        {
            "22": {
                "title": "The curious case of neural text degeneration.",
                "author": "Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi.",
                "venue": "In International Conference on Learning Representations, 2019.",
                "url": null
            }
        },
        {
            "23": {
                "title": "Google\u2019s multilingual neural machine translation system: Enabling zero-shot translation.",
                "author": "Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Vi\u00e9gas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jeffrey Dean.",
                "venue": "Transactions of the Association for Computational Linguistics, 5:339\u2013351, 2017.",
                "url": null
            }
        },
        {
            "24": {
                "title": "Sequence-level knowledge distillation.",
                "author": "Yoon Kim and Alexander M. Rush.",
                "venue": "In Jian Su, Kevin Duh, and Xavier Carreras, editors, Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1317\u20131327, 2016.",
                "url": null
            }
        },
        {
            "25": {
                "title": "Frustratingly easy model ensemble for abstractive summarization.",
                "author": "Hayato Kobayashi.",
                "venue": "In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun\u2019ichi Tsujii, editors, Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 4165\u20134176, 2018.",
                "url": null
            }
        },
        {
            "26": {
                "title": "Europarl: A parallel corpus for statistical machine translation.",
                "author": "Philipp Koehn.",
                "venue": "In Proceedings of Machine Translation Summit, pages 79\u201386, 2005.",
                "url": null
            }
        },
        {
            "27": {
                "title": "Unsupervised machine translation using monolingual corpora only.",
                "author": "Guillaume Lample, Alexis Conneau, Ludovic Denoyer, and Marc\u2019Aurelio Ranzato.",
                "venue": "In International Conference on Learning Representations, 2018a.",
                "url": null
            }
        },
        {
            "28": {
                "title": "Phrase-based & neural unsupervised machine translation.",
                "author": "Guillaume Lample, Myle Ott, Alexis Conneau, Ludovic Denoyer, and Marc\u2019Aurelio Ranzato.",
                "venue": "In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun\u2019ichi Tsujii, editors, Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 5039\u20135049, 2018b.",
                "url": null
            }
        },
        {
            "29": {
                "title": "BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.",
                "author": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer.",
                "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 7871\u20137880, 2020.",
                "url": null
            }
        },
        {
            "30": {
                "title": "Improving zero-shot translation by disentangling positional information.",
                "author": "Danni Liu, Jan Niehues, James Cross, Francisco Guzm\u00e1n, and Xian Li.",
                "venue": "In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing, pages 1259\u20131273, 2021.",
                "url": null
            }
        },
        {
            "31": {
                "title": "If beam search is the answer, what was the question?",
                "author": "Clara Meister, Ryan Cotterell, and Tim Vieira.",
                "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 2173\u20132185, 2020.",
                "url": null
            }
        },
        {
            "32": {
                "title": "Unsupervised word translation with adversarial autoencoder.",
                "author": "Tasnim Mohiuddin and Shafiq Joty.",
                "venue": "Computational Linguistics, 46(2):257\u2013288, 2020.",
                "url": null
            }
        },
        {
            "33": {
                "title": "Understanding the properties of minimum Bayes risk decoding in neural machine translation.",
                "author": "Mathias M\u00fcller and Rico Sennrich.",
                "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing, pages 259\u2013272, 2021.",
                "url": null
            }
        },
        {
            "34": {
                "title": "Gradient boosting machines, a tutorial.",
                "author": "Alexey Natekin and Alois Knoll.",
                "venue": "Frontiers in Neurorobotics, 7:21, 2013.",
                "url": null
            }
        },
        {
            "35": {
                "title": "fairseq: A fast, extensible toolkit for sequence modeling.",
                "author": "Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli.",
                "venue": "In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics, pages 48\u201353, 2019.",
                "url": null
            }
        },
        {
            "36": {
                "title": "BLEU: A method for automatic evaluation of machine translation.",
                "author": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.",
                "venue": "In Pierre Isabelle, Eugene Charniak, and Dekang Lin, editors, Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, 2002.",
                "url": null
            }
        },
        {
            "37": {
                "title": "A call for clarity in reporting BLEU scores.",
                "author": "Matt Post.",
                "venue": "In Proceedings of the Conference on Machine Translation, pages 186\u2013191, 2018.",
                "url": null
            }
        },
        {
            "38": {
                "title": "Language models are unsupervised multitask learners.",
                "author": "Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.",
                "venue": "OpenAI Blog, 2019.",
                "url": null
            }
        },
        {
            "39": {
                "title": "Exploring the limits of transfer learning with a unified text-to-text Transformer.",
                "author": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu.",
                "venue": "Journal of Machine Learning Research, 21(140):1\u201367, 2020.",
                "url": null
            }
        },
        {
            "40": {
                "title": "Neural machine translation for low-resource languages: A survey.",
                "author": "Surangika Ranathunga, En-Shiun Annie Lee, Marjana Prifti Skenduli, Ravi Shekhar, Mehreen Alam, and Rishemjit Kaur.",
                "venue": "ACM Computing Survey, 55(11), 2023.",
                "url": null
            }
        },
        {
            "41": {
                "title": "BLOOM: A 176B-parameter open-access multilingual language model.",
                "author": "Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, et al.",
                "venue": "arXiv preprint arXiv:2211.05100, 2022.",
                "url": null
            }
        },
        {
            "42": {
                "title": "The boosting approach to machine learning: An overview.",
                "author": "Robert E Schapire.",
                "venue": "Nonlinear Estimation and Classification, pages 149\u2013171, 2003.",
                "url": null
            }
        },
        {
            "43": {
                "title": "Edinburgh neural machine translation systems for WMT 16.",
                "author": "Rico Sennrich, Barry Haddow, and Alexandra Birch.",
                "venue": "In Proceedings of the Conference on Machine Translation, pages 371\u2013376, 2016a.",
                "url": null
            }
        },
        {
            "44": {
                "title": "Neural machine translation of rare words with subword units.",
                "author": "Rico Sennrich, Barry Haddow, and Alexandra Birch.",
                "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 1715\u20131725, 2016b.",
                "url": null
            }
        },
        {
            "45": {
                "title": "Improving low resource speech translation with data augmentation and ensemble strategies.",
                "author": "Akshaya Vishnu Kudlu Shanbhogue, Ran Xue, Soumya Saha, Daniel Zhang, and Ashwinkumar Ganesan.",
                "venue": "In Proceedings of the International Conference on Spoken Language Translation, pages 241\u2013250, 2023.",
                "url": null
            }
        },
        {
            "46": {
                "title": "Ensemble distillation for unsupervised constituency parsing.",
                "author": "Behzad Shayegh, Yanshuai Cao, Xiaodan Zhu, Jackie CK Cheung, and Lili Mou.",
                "venue": "In International Conference on Learning Representations, 2024.",
                "url": null
            }
        },
        {
            "47": {
                "title": "NMTScore: A multilingual analysis of translation-based text similarity measures.",
                "author": "Jannis Vamvas and Rico Sennrich.",
                "venue": "In Findings of the Association for Computational Linguistics: EMNLP, pages 198\u2013213, 2022.",
                "url": null
            }
        },
        {
            "48": {
                "title": "Deep reinforcement learning with double q-learning.",
                "author": "Hado van Hasselt, Arthur Guez, and David Silver.",
                "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence, pages 2094\u20132100, 2016.",
                "url": null
            }
        },
        {
            "49": {
                "title": "Attention is all you need.",
                "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin.",
                "venue": "In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, 2017.",
                "url": null
            }
        },
        {
            "50": {
                "title": "Parameter differentiation based multilingual neural machine translation.",
                "author": "Qian Wang and Jiajun Zhang.",
                "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence, pages 11440\u201311448, 2022.",
                "url": null
            }
        },
        {
            "51": {
                "title": "On negative interference in multilingual models: Findings and a meta-learning treatment.",
                "author": "Zirui Wang, Zachary C. Lipton, and Yulia Tsvetkov.",
                "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 4438\u20134450, 2020.",
                "url": null
            }
        },
        {
            "52": {
                "title": "Why do neural dialog systems generate short and meaningless replies? A comparison between dialog and translation.",
                "author": "Bolin Wei, Shuai Lu, Lili Mou, Hao Zhou, Pascal Poupart, Ge Li, and Zhi Jin.",
                "venue": "In Proceedings of the International Conference on Acoustics, Speech and Signal Processing, pages 7290\u20137294, 2019.",
                "url": null
            }
        },
        {
            "53": {
                "title": "An equal-size hard EM algorithm for diverse dialogue generation.",
                "author": "Yuqiao Wen, Yongchang Hao, Yanshuai Cao, and Lili Mou.",
                "venue": "In International Conference on Learning Representations, 2023a.",
                "url": null
            }
        },
        {
            "54": {
                "title": "-divergence minimization for sequence-level knowledge distillation.",
                "author": "Yuqiao Wen, Zichao Li, Wenyu Du, and Lili Mou.",
                "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 10817\u201310834, 2023b.",
                "url": null
            }
        },
        {
            "55": {
                "title": "The effects of language token prefixing for multilingual machine translation.",
                "author": "Rachel Wicks and Kevin Duh.",
                "venue": "In Proceedings of the Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing, pages 148\u2013153, 2022.",
                "url": null
            }
        },
        {
            "56": {
                "title": "Huggingface\u2019s Transformers: State-of-the-art natural language processing.",
                "author": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, et al.",
                "venue": "arXiv preprint arXiv:1910.03771, 2019.",
                "url": null
            }
        },
        {
            "57": {
                "title": "Stacked generalization.",
                "author": "David H. Wolpert.",
                "venue": "Neural Networks, 5(2):241\u2013259, 1992.",
                "url": null
            }
        },
        {
            "58": {
                "title": "One teacher is enough? Pre-trained language model distillation from multiple teachers.",
                "author": "Chuhan Wu, Fangzhao Wu, and Yongfeng Huang.",
                "venue": "In Findings of the Association for Computational Linguistics: ACL-IJCNLP, pages 4408\u20134413, 2021.",
                "url": null
            }
        },
        {
            "59": {
                "title": "Pivot language approach for phrase-based statistical machine translation.",
                "author": "Hua Wu and Haifeng Wang.",
                "venue": "In Proceedings of the Annual Meeting of the Association of Computational Linguistics, pages 856\u2013863, 2007.",
                "url": null
            }
        },
        {
            "60": {
                "title": "Revisiting pivot language approach for machine translation.",
                "author": "Hua Wu and Haifeng Wang.",
                "venue": "In Proceedings of the Joint Conference of the Annual Meeting of the ACL and the International Joint Conference on Natural Language Processing of the AFNLP, pages 154\u2013162, 2009.",
                "url": null
            }
        },
        {
            "61": {
                "title": "A survey on ensemble learning under the era of deep learning.",
                "author": "Yongquan Yang, Haijun Lv, and Ning Chen.",
                "venue": "Artificial Intelligence Review, 56(6):5545\u20135589, 2023.",
                "url": null
            }
        },
        {
            "62": {
                "title": "Adaptive knowledge sharing in multi-task learning: Improving low-resource neural machine translation.",
                "author": "Poorya Zaremoodi, Wray Buntine, and Gholamreza Haffari.",
                "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 656\u2013661, 2018.",
                "url": null
            }
        },
        {
            "63": {
                "title": "Improving massively multilingual neural machine translation and zero-shot translation.",
                "author": "Biao Zhang, Philip Williams, Ivan Titov, and Rico Sennrich.",
                "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 1628\u20131639, 2020.",
                "url": null
            }
        },
        {
            "64": {
                "title": "BERTScore: Evaluating text generation with BERT.",
                "author": "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi.",
                "venue": "In International Conference on Learning Representations, 2019.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.00144v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3.1",
            "3.2",
            "3.3"
        ],
        "main_experiment_and_results_sections": [
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.2",
            "4.3"
        ]
    },
    "research_context": {
        "paper_id": "2403.00144v1",
        "paper_title": "EBBS: An Ensemble with Bi-Level Beam Searchfor Zero-Shot Machine Translation",
        "research_background": "The paper \"EBBS: An Ensemble with Bi-Level Beam Search for Zero-Shot Machine Translation\" addresses several key points in its introduction and related work sections.\n\n### Motivation:\n\nThe paper aims to tackle the challenge of machine translation in zero-shot settings, particularly for low-resource languages. The authors note the difficulty of translating between language pairs that lack substantial parallel corpora for model training. Traditional approaches involving supervised learning, using architectures like the Transformer or large language models, are hindered by limited data in such contexts.\n\n### Research Problem:\n\nThe core research problem is improving the quality of zero-shot machine translation for unseen language pairs. The standard multilingual models, which often prepend a language-indicator token, are noisy and deliver less satisfactory results due to the scarcity of supervision. Moreover, alternative methods like pivot translation, which rely on translating through a high-resource language, tend to accumulate errors over the two translation steps. Thus, the need is to find a method that enhances translation quality without being heavily dependent on massive parallel data.\n\n### Proposed Method:\n\nThe authors introduce a new ensemble approach called EBBS (Ensemble with Bi-Level Beam Search). This method combines direct translation and pivot translations across different languages into a single aggregated prediction. The novel approach involves a bi-level beam search mechanism:\n- **Lower Level**: Beam search is applied within each individual ensemble component.\n- **Upper Level**: The ensemble maintains a shared beam via voting from the individual beams, ensuring that the candidates remain synchronized and diverse.\n\nThis setup diverges from word-level ensembles that average predicted probabilities (leading to overly smooth predictions) and sequence-level ensembles restricted to weak candidate sets.\n\n### Relevant Prior Work:\n\n1. **Supervised Neural Machine Translation**:\n   - The Transformer: Vaswani et al. (2017)\n   - Pretrained large language models: Radford et al. (2019), Raffel et al. (2020), Lewis et al. (2020)\n\n2. **Multilingual Translation Systems**:\n   - Utilizing a single multilingual model: Johnson et al. (2017), Fan et al. (2021)\n   - Zero-shot generalization: Liu et al. (2021), Wicks and Duh (2022)\n\n3. **Pivot Translation**:\n   - Foundational works: Wu and Wang (2007, 2009)\n   - Two-step error accumulation: Babych et al. (2007), Gu et al. (2019)\n\n4. **Ensemble Approaches**:\n   - Word-level probability averaging: Sennrich et al. (2016a), Freitag et al. (2017), Shanbhogue et al. (2023)\n   - Sequence-level voting and selection: Bickel and Doksum (2015), Kobayashi (2018), Eikema and Aziz (2020)\n\n### Empirical Validation:\n\nThe proposed EBBS method is tested on well-known zero-shot translation benchmarks like IWSLT and Europarl datasets. The results indicate that EBBS achieves superior translation quality compared to existing ensemble techniques. Additionally, employing EBBS outputs for model distillation further enhances multilingual model performance.",
        "methodology": "Methodology: In this work, we focus on zero-shot multilingual machine translation, which requires translating between multiple languages, even those pairs that have not been directly seen during training.\n\nOur multilingual model is based on an encoder-decoder architecture using the Transformer model, enhanced with a byte pair encoding (BPE) tokenizer. The tokenizer, developed by Sennrich et al. (2016), is shared among all languages to ensure a unified approach to tokenization. The encoder is designed to grasp the semantics of tokens across any language, while the decoder is responsible for translating the encoded text into the target language. The operation is guided by a target-language indicator as outlined by Johnson et al. (2017) and furthered by Fan et al. (2021).\n\nThe model is trained using an English-centric approach, as suggested by Johnson et al. (2017) and Liu et al. (2021). Here, the system is fed parallel data where English appears as one of the translation pairs (e.g., German-to-English or English-to-Romanian). This training facilitates the zero-shot capability allowing the model to directly translate between unseen language pairs, such as German-to-Romanian. This method of direct translation was explored by Dabre et al. (2020) and Ranathunga et al. (2023). Alternatively, pivot translation uses English or another high-resource language as an intermediary step, translating in two phases (e.g., German to English, then English to Romanian).\n\nDirect translation often suffers due to the lack of direct parallel data for training, leading to lower translation quality. On the other hand, pivot translation might compound errors across its two-step process. These limitations are well-documented by Babych et al. (2007) and Gu et al. (2019).\n\nOur proposition is to create a self-ensemble of the multilingual model to enhance translation quality by combining both direct and pivot translations. However, typical ensemble techniques like averaging and voting may not be optimal for text generation. Voting involves selecting the most common prediction, which is impractical given the vast vocabulary involved. Averaging distributes predictions across components, potentially resulting in a diluted prediction distribution. This issue was partially corroborated by a pilot study reported in the appendix of Fan et al. (2021), where a multilingual averaging ensemble only showed marginal improvement over direct translation.\n\nEnsemble methods have not been widely explored for zero-shot multilingual translation, and this work aims to pioneer in addressing this gap.",
        "main_experiment_and_results": "**Main Experiment Setup:**\n\n1. **Datasets:**\n   - **IWSLT (2017)**: \n     - Contains 4 languages (with English).\n     - Includes 6 zero-shot directions.\n   - **Europarl v7 (2005)**: \n     - Contains 9 languages.\n     - Includes 56 zero-shot directions.\n\n2. **Evaluation Metrics:**\n   - **BLEU Scores (Papineni et al., 2002)**: Measures the n-gram overlap between the generated and reference translations.\n   - **SacreBLEU (Post, 2018)**: A standard BLEU score calculation tool, used to ensure consistency with recent translation studies.\n\n3. **Model Architecture:**\n   - For the **IWSLT dataset**: Used a 5-layer encoder\u2013decoder Transformer.\n   - For the **Europarl dataset**: Used an 8-layer Transformer to handle the increased amount of training data and languages.\n   - The architecture and residual connection strategy was based on Liu et al. (2021).\n\n**Main Experimental Results:**\n   - The main results and detailed performance metrics can be found in Appendix B."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To compare different ensemble techniques for zero-shot machine translation, determining which methods best enhance translation performance.",
            "experiment_process": "The study involves replicating a base multilingual model (Liu et al., 2021) and performing translation tasks using direct translation, English pivoting, word-level averaging ensemble, word-level voting ensemble, sequence-level voting ensemble, and the proposed EBBS method. The experiments use two datasets, IWSLT and Europarl, encompassing multiple languages and translation directions, and the evaluation is based on BLEU scores.",
            "result_discussion": "EBBS achieves the highest performance across most directions on both datasets. Specifically, it outperforms direct translation in 56 of 62 cases and exhibits superior performance over other ensemble methods. Word-level averaging and word-level voting ensembles do not show significant improvements, while sequence-level voting works better in Europarl but poorly on IWSLT. The study confirms that incorporating weak components minimally impacts EBBS, showcasing its robustness. Additionally, distilling EBBS outputs to the multilingual model improves inference efficiency and translation quality.",
            "ablation_id": "2403.00144v1.No1"
        },
        {
            "research_objective": "To analyze alternative voting methods within the EBBS algorithm and determine which voting mechanism provides the best performance.",
            "experiment_process": "The study compares different voting schemes within the EBBS framework, including sum-voting, total-sum voting, 0/1 voting, and max voting. The evaluation involves re-calculating performance metrics like BLEU scores using these different voting schemes.",
            "result_discussion": "Sum voting delivers the best performance among the evaluated voting schemes. Total-sum voting performs worse than direct translation, suggesting the importance of excluding candidate translations not present in lower-level beams. Both 0/1 voting and max voting provide decent improvements over the baseline but are consistently outperformed by sum voting. This analysis highlights the efficacy of the sum voting scheme within the bi-level beam search ensemble approach.",
            "ablation_id": "2403.00144v1.No2"
        },
        {
            "research_objective": "To evaluate the benefits of EBBS-based distillation for improving inference efficiency and translation quality in zero-shot machine translation.",
            "experiment_process": "The study implements EBBS-based distillation, union distillation, and self-distillation, focusing on Italian-to-Dutch translation. Comparisons are made by measuring translation performance on IWSLT and Europarl datasets under these different distillation techniques. Each distillation approach involves generating training samples and evaluating the BLEU scores of the distilled models.",
            "result_discussion": "EBBS-based distillation consistently outperforms both self-distillation and union distillation, achieving performance close to or even surpassing EBBS itself in some cases. On IWSLT, EBBS-based distillation improves the base model by 2.7 BLEU points while providing significant gains in inference efficiency. These results underscore the effectiveness of EBBS-based distillation in maintaining or enhancing translation performance while improving computational efficiency.",
            "ablation_id": "2403.00144v1.No3"
        }
    ]
}