{
    "title": "What Is Missing in Multilingual Visual Reasoning and How to Fix It",
    "abstract": "NLP models today strive for supporting multiple languages and modalities, improving accessibility for diverse users. In this paper, we evaluate their multilingual, multimodal capabilities by testing on a visual reasoning task. We observe that proprietary systems like GPT-4V obtain the best performance on this task now, but open models lag in comparison. Surprisingly, GPT-4V exhibits similar performance between English and other languages, indicating the potential for equitable system development across languages. Our analysis on model failures reveals three key aspects that make this task challenging: multilinguality, complex reasoning, and multimodality. To address these challenges, we propose three targeted interventions including a translate-test approach to tackle multilinguality, a visual programming approach to break down complex reasoning, and a novel method that leverages image captioning to address multimodality. Our interventions achieve the best open performance on this task in a zero-shot setting, boosting open model LLaVA by 13.4%, while also minorly improving GPT-4V\u2019s performance.111The code implementations and prompts can be found at https://github.com/yueqis/Multilingual_Visual_Reasoning.git.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Language technology today is continually evolving to be more inclusive, with models becoming increasing multilingual Lai et al. (2023  ###reference_b19###); Li et al. (2022  ###reference_b20###), multimodal Yang et al. (2023  ###reference_b38###), or both (Chen et al., 2020  ###reference_b6###; Zeng et al., 2023  ###reference_b40###; Geigle et al., 2023  ###reference_b13###; Achiam et al., 2023  ###reference_b2###).\nEven though this promotes broader user accessibility, past research has consistently highlighted differences in model performance across languages (Blasi et al., 2022  ###reference_b4###) and cultures (Liu et al., 2021  ###reference_b21###). Notably, these models often favor North American or Western contexts, potentially leaving behind users from other regions. (Liu et al., 2021  ###reference_b21###; Hershcovich et al., 2022  ###reference_b15###).\n###figure_1### The NLP community is currently witnessing a trend of moving away from openly releasing models to limiting their access through paid web APIs Abdalla et al. (2023  ###reference_b1###). Additionally, the cost to use these services is often higher for low-resourced languages, despite poorer performance Ahia et al. (2023  ###reference_b3###). While it is certainly desirable to have strong and inclusive models available regardless of the access method, open, well-documented, and reasonably sized models have advantages from the point of view of control, ownership, cost, and advancing scientific understanding.\nIn this work, we first compare and contrast the multilingual, multicultural capabilities of the proprietary system GPT-4V(ision) Achiam et al. (2023  ###reference_b2###) with a plethora of open models like LLaVA Liu et al. (2023c  ###reference_b24###, a  ###reference_b22###), mBLIP Geigle et al. (2023  ###reference_b13###), CCLM Zeng et al. (2023  ###reference_b40###), using two datasets on the same task of reasoning over texts and pairs of images, NLVR2 (Suhr et al., 2019  ###reference_b32###) and MaRVL Liu et al. (2021  ###reference_b21###). We discuss this setup in more details in \u00a72  ###reference_### and \u00a73  ###reference_###. We find that GPT-4V significantly outperforms all open models. One additional unprecedented and surprising result is, as shown in Figure 1  ###reference_###, GPT-4V\u2019s consistency in performance across all languages, with performance on some even surpassing that on the NLVR2 dataset in English.\nIn contrast, as we will discuss in \u00a74  ###reference_###, most open models still show a significant gap between English and other languages, perhaps due to deficiencies in training data, or due to the well-known \u201ccurse of multilinguality\u201d, where smaller models are less adept at processing low-resource languages (Conneau et al., 2020  ###reference_b9###).\nThis begs the question: \u201chow can we take open models, and bring them closer to achieving the exciting language-equitable multimodal reasoning results demonstrated by the opaque (and presumably bigger) GPT-4V?\u201d\nTowards this end, we conduct a careful analysis of the results from testing models on the multilingual visual reasoning task and discover that failures can arise from any of the three challenging aspects of the task: multilinguality, reasoning, and multimodality. For multilinguality, we find a significant gap in performance for other languages as compared to English. For reasoning, we find a negative correlation of performance and the compositionality of the statement. For multimodality, we find that models were typically pretrained on single image-text pairs, but haven\u2019t seen pairs of images in pretraining, which may lead to a mismatch between pretraining and evaluation objectives. We will discuss this in more details in Section 5  ###reference_###.\nBased on our analysis, we design three interventions that address these challenges in section 6  ###reference_###. The first simply tackles multilinguality \u2013 we translate the MaRVL statements to English. Surprisingly, translation leads to a drop in performance for GPT-4V (which might indicate its advanced multilingual capabilities), but helps improve performance for all open models. Our next intervention tackles both multilinguality and reasoning, by generating programs to reason over the set of images using the translated statements, inspired by Gupta and Kembhavi (2023  ###reference_b14###)\u2019s VisProg method. Our third (and most effective) intervention tackles all three challenges by first captioning images conditioned on the statement, and then reasoning over the captions, rather than the images, using chain-of-thought capabilities of text-modality LLMs Wei et al. (2022  ###reference_b35###). Using this intervention, we obtain state-of-the-art zero-shot performance on the MaRVL dataset for open models, and also slightly improve the performance of GPT-4V itself, as shown in Figure 1  ###reference_###."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Dataset Description",
            "text": "###figure_2### We evaluate on two visual reasoning datasets, both containing a statement in natural language and a pair of images. The task is to reason whether the statement is true based on the images, requiring reasoning over both images and the statement together.\nFigure 2  ###reference_### shows an example of this task.\nNLVR2 contains 107,292 examples of English sentences with web photographs. Annotators paired visually-rich images and were encouraged to come up with compositional and linguistically diverse statements for each pair. The dataset contains a train-validation-test split. Images were collected using search queries generated from synsets derived from the ILSVRC2014 ImageNet challenge Russakovsky et al. (2015  ###reference_b30###), with each query resulting in 4 pairs of images from Google Images222https://images.google.com/  ###reference_images.google.com/###. Queries for ImageNet Deng et al. (2009  ###reference_b10###) are based on the English WordNet Poli et al. (2010  ###reference_b29###), whose concepts are more reflective of the North-American or Western cultures.\nMaRVL explores the same task as NLVR2 in multilingual multicultural contexts. MaRVL is a test-only dataset collected for five diverse languages: Indonesian, Swahili, Tamil, Turkish, and Mandarin Chinese. Native speakers first select concepts that are reflective of their speaking population. Next, they curate images from the web that reflect those concepts within their specific cultural context. Finally, native speakers pair and write statements for each image pair, following the same protocol as that laid out for NLVR2."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Models and Evaluation Protocols",
            "text": "We evaluate various open models, including mBLIP (mt0-xl) Geigle et al. (2023  ###reference_b13###), LLaVA Liu et al. (2023c  ###reference_b24###, a  ###reference_b22###), CCLM Zeng et al. (2023  ###reference_b40###), and UNITERs Chen et al. (2020  ###reference_b6###); and a proprietary model GPT-4V(ision).333gpt-4-vision-preview (https://openai.com/research/gpt-4v-system-card  ###reference_card###), abbreviated as \u201dGPT-4V\u201d. We describe these models in Section A  ###reference_###.\nWe evaluate them in two settings:\nZero-shot.\nIn this setting, the models are not specifically fine-tuned for the task of multimodal visual reasoning.\nThis setting is academically interesting, as it more generally tests the ability of models to perform reasoning tasks, and the results are more likely to be representative of performance on datasets for which training data is not explicitly available.\nIn addition, it is practically useful in that it can also be applied to LMs that cannot as easily by fine-tuned, such as GPT-4V (due to its closed nature), and LLaVa (due to its relatively large size).\nWe test LLaVA, mBLIP, and GPT-4V in this setting.\nFinetuned.\nIn addition, for models that can more easily be finetuned, we finetune them on the English NLVR2 dataset, and test them on both NLVR2 and MaRVL.\nThis represents the realistic setting of adapting a multilingual model to a particular reasoning task using only English data, which is relatively available.\nWe test mBLIP, CCLM-4M, xUNITER, and mUNITER in this setting."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "How well do proprietary and open models perform on multilingual visual reasoning?",
            "text": "In this section, we perform an examination of how-well these various models perform on multilingual multimodal reasoning tasks.\nTable 1  ###reference_### shows performance of humans, open models, and proprietary models. For the models, we use the experiment protocols as in \u00a73  ###reference_### in the zero-shot and finetuned settings. We ask the following questions:\nWhich model performs the best? Answer: GPT-4V on MaRVL, and mBLIP (mT0-XL) on English post-fintuning. However, in the zero-shot setting, the proprietary model GPT-4V performs the best across all languages,444We put GPT-4V in the zero-shot category because we evaluate the performance of GPT-4V on NLVR2 and MaRVL without finetuning on the NLVR2 training data. However, we do not know if GPT-4V has seen examples of NLVR2 or MaRVL during pretraining. and open models lag behind. Note that despite GPT-4V\u2019s impressive performance, it still lags behind human performance by 10% to 20% across all languages, emphasizing that this task still is not completely solved.\nWhich open model performs the best? Answer: mBLIP (mT0-XL), regardless of whether it is finetuned. The other open LLM, LLaVA, is not explicitly trained on multilingual data, so the gap in MaRVL and NLVR2 performance is expected.\nDo models perform equitably across languages?\nUnder zero-shot setting, GPT-4V and mBLIP both show equitable performance across languages, which is encouraging, although the latter significantly lags in overall performance compared to GPT-4V. Interestingly, post finetuning on NLVR2, mBLIP shows better performance on NLVR2 than GPT-4V, but still has lower performance on MaRVL. The gap between English and MaRVL languages also significantly increases for mBLIP from the zero-shot to finetuned setting. Maintaining the equity in performance across languages during finetuning is an interesting future direction, which may help models surpass GPT-4V\u2019s performance on multilingual visual reasoning. Other models lag mBLIP, both in overall performance and equity with English."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "What makes multilingual visual reasoning challenging?",
            "text": "As noted in Table 1  ###reference_###, the best model still lags human performance by 10% to 20%. In this section, we aim to analyze what makes multilingual visual reasoning so challenging, and identify three key aspects as detailed below:"
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Multilinguality and Sub-Optimal Cross-Lingual Transfer",
            "text": "In the finetuned setting, we observe a significant drop in performance for MaRVL languages as compared to NLVR2 in English. This is expected since models are finetuned only in English but not in these languages due to lack of training data.\nWe also note that performance on Swahili is consistently lower across models (excluding GPT-4V), which is the lowest-resource language amongst MaRVL languages, as laid out by the language resource taxonomy Joshi et al. (2020  ###reference_b17###). This observation motivates us to evaluate models with MaRVL data translated to English, as we discuss in \u00a76.1  ###reference_###.\nIn the zero-shot setting, GPT-4V and mBLIP both exhibit equitable performance on MaRVL as with NLVR2. While LLaVa is not expected to perform as well for non-English languages, it has poorer performance than mBLIP on NLVR2. While mBLIP is pretrained on multilingual multimodal data, LLaVA is not specifically trained on multilingual data, but it is generally believed that LLaVA has multilingual abilities as it has seen multilingual data during pretraining Liu et al. (2023c  ###reference_b24###, a  ###reference_b22###).\nOverall, models have better visual reasoning capabilities when given English inputs and US/European-centric cultures, while still lagging behind when facing multilingual text and multicultural image inputs."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Complex Reasoning",
            "text": "Data points in both NLVR2 and MaRVL require complex reasoning. An example statement from NLVR2 is \u201done image includes a silver stylus and a device with a blue keyboard base and an open screen propped up like an easel\u201d, which is semantically diverse, long in length, and has a compositional structure, requiring models to perform compositional and complex reasoning to infer the label.\nAs a proxy to the complexity of reasoning, we measure the number of words of the NLVR2 and MaRVL statements (translated to English), and find that model performances drop as the number of words of the statement increases. Figure 3  ###reference_### shows a graph of the performance of GPT-4V plotted against the number of words in each statement. We can clearly see a downward trend in the graph. Based on this, we are motivated to examine methods that break down long, compositional statements, as will be discussed in \u00a76.2  ###reference_###.\n###figure_3### ###figure_4###"
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Multimodality and Mismatch between Pretraining & Evaluation",
            "text": "Multimodal reasoning is known to be harder than reasoning over text alone Mogadala et al. (2021  ###reference_b25###); Park and Kim (2023  ###reference_b28###). Further, NLVR2 and MaRVL contain two images per instance, along with a statement describing both images, while vision-language models are typically pretrained on a single image-text pair Cao et al. (2020  ###reference_b5###), leading to a mismatch in the input between pretraining and evaluation. This, and the inherent difficulty of aligning image data and text data during the reasoning process make this multimodal task particularly challenging. This motivates us to (1) move from processing a pair of images together to processing each image separately; and (2) break down the two modalities of image and text in the reasoning process, as in \u00a76.3  ###reference_###."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "How can we address these challenges?",
            "text": "Based on our analysis from the previous section, we now move on to examining whether we can devise methods to further improve multilingual multimodal reasoning abilities, particularly those of open models. We examine three research questions, which we discuss in more details in the following subsections respectively. Figure 4  ###reference_### shows a flow chart visualizing the interventions we perform to address the research questions.\nRQ1) (multilinguality) Does translating the text to English and reducing the cross-lingual gap aid performance? Short Answer: it depends.\nRQ2) (multilinguality+reasoning) Can we break down the complex reasoning into modular programs which can be executed on a vision-text input? Short Answer: yes, we adopt the Visual Programming approach Gupta and Kembhavi (2023  ###reference_b14###).\nRQ3) (multilinguality+reasoning+multimodality) Can we alleviate the need for multimodal interaction during the reasoning process? Short Answer: yes, we propose a new approach utilizing captions."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Addressing Multilinguality: Translate-Test",
            "text": "In \u00a75.1  ###reference_###, we find performance on NLVR2 is much better than that on MaRVL. While both are visual reasoning datasets, MaRVL is multi-cultural and contains statements in 5 diverse languages. Since NLP systems perform significantly better with English data Song et al. (2023  ###reference_b31###), we first simply translate the reasoning statements to English using the Google Translate API Wu et al. (2016  ###reference_b37###). A visualization of the process of translate test can be found in Figure 4  ###reference_###.\nIn addition to the models we evaluate in \u00a73  ###reference_###, we also evaluate ViLT Kim et al. (2021  ###reference_b18###) for better comparisons, as our next proposed intervention in \u00a76.2  ###reference_### uses ViLT. We didn\u2019t evaluate ViLT on MaRVL before translate test, since it doesn\u2019t support the MaRVL languages. Our evaluation protocols follows the ones introduced in \u00a73  ###reference_### and results are shown in Table 2  ###reference_###.\nAll prior works, as per our knowledge, have observed a gain in performance post translating to English Liu et al. (2021  ###reference_b21###). Our observation is consistent with prior findings for all models, except GPT-4V(ision). All models except for GPT-4V sees an increase in accuracy after performing translate test; while surprisingly, GPT-4V shows a sharp decrease in performance across all MaRVL languages after translate test. However, this is encouraging, because it speaks for the multilingual capabilities of this model, and indicates that the gains provided by translating to English are lower than the errors made in translating cultural-specific nuances in meaning.\nFor example, the MaRVL statement \u201d\u53f3\u56fe\u6709\u9752\u7eff\u8272\u7684\u82f9\u679c\u201d is translated to \u201dthe picture on the right has turquoise apples\u201d, where \u201d\u9752\u7eff\u8272\u201d is translated to \u201dturquoise\u201d. However, the color \u201d\u9752\u7eff\u8272\u201d means pure green with a little bit cyan in Mandarin Chinese, which is different from \u201dturquoise\u201d. Given this, GPT-4V reasons correctly when provided the statement in Mandarin, but makes mistakes when given the translated statement."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Addressing Multilinguality + Reasoning: Visual Programming",
            "text": "To improve performance of LLMs on reasoning tasks, beyond naive prompting, several methods have been introduced Nye et al. (2021  ###reference_b26###); Zhou et al. (2022  ###reference_b41###); Wei et al. (2022  ###reference_b35###); Gao et al. (2023  ###reference_b12###). Particularly, PAL Gao et al. (2023  ###reference_b12###) provides significant improvements by decomposing a natural language instruction into multiple programmatic sub-modules, executed in an inference step to obtain the final answer. Most recently, efforts like VisProg Gupta and Kembhavi (2023  ###reference_b14###), ViperGPT Sur\u00eds et al. (2023  ###reference_b33###), Visual ChatGPT Wu et al. (2023  ###reference_b36###) have followed suit to solve multimodal reasoning using LLMs to generate visual programs, that leverage off-the-shelf computer vision models for image processing during inference. Hence, we use VisProg to generate visual programs given translated statements as obtained in \u00a76.1  ###reference_###. VisProg uses ViLT Kim et al. (2021  ###reference_b18###) as its inherent vision module.\nFigure 4  ###reference_### shows the flow of VisProg. For example, given the translated statement: There is no one in the bedroom on the left, and there is someone in the bedroom on the right, the generated visual program is:\nIf this program is executed on the images in Figure 5  ###reference_###, then it will have ANSWER0 , ANSWER1 , so the final result is .\n###figure_5### For this intervention, we use text-davinci-003555text-davinci-003 is the model that the VisProg authors utilized when running VisProg. as a representative of proprietary LLMs and LLaMA2-70B Touvron et al. (2023  ###reference_b34###) to represent open LLMs. Table 3  ###reference_### shows results to this method. Although this method does not achieve as high accuracy as models evaluated end-to-end in Table 1  ###reference_###, this approach provides valuable insights of breaking down complex reasoning into modular modules and utilizing prompts to make use of LLMs\u2019 strong in-context abilities. In addition, this approach, without any additional training, achieves on par performance on MaRVL, as compared to ViLT post-fintuning."
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "Addressing Multilinguality + Reasoning + Multimodality: Reasoning with Captions",
            "text": "When analyzing errors for NLVR2, Gupta and Kembhavi (2023  ###reference_b14###) note that 69% of them are caused by the vision module. This might be potentially worse for MaRVL, because open visual modules used in VisProg Kim et al. (2021  ###reference_b18###) are trained on Western-centric datasets like Imagenet Russakovsky et al. (2015  ###reference_b30###). Text-based LLMs, on the other hand, are trained on trillions of tokens, and are known to exhibit cultural awareness, albeit it may be limited Yao et al. (2023  ###reference_b39###). Hence, here we target the last remaining challenge, that of multimodal interaction needed for the reasoning process, by working with image captions instead of images. Concretely, we first caption both images, and use an LLM to reason about the given statement with the two captions, rather than with the two images. Figure 4  ###reference_### shows a flow chart of how this pipeline works.\nTo make sure the captions capture necessary information needed for reasoning about the statement, as a first step of this intervention we use LLMs to generate targeted instructions based on the statement. For instance, consider the statement \u201dThe picture on the left has several pencils of different colors, and the picture on the right has only one pencil\u201d from MaRVL-zh, the targeted instructions for generating captions are:\nLeft image - \u201dWrite a short caption describing the number and colors of pencils;\u201d\nRight image - \u201dWrite a short caption describing the number of pencils\u201d.\n###figure_6### As a second step of this method, we generate captions following the targeted instructions in step 1, using multiple captioning models, including InstructBLIP Liu et al. (2023b  ###reference_b23###), PromptCap Hu et al. (2022  ###reference_b16###), GPT-4V, and LLaVA-13B Liu et al. (2023a  ###reference_b22###). The instructions can point the captioning models to focus on targeted contents in the image. For instance, using the same statement in the first step and the images in Figure 6  ###reference_### as an example, the captions generated by GPT-4V are: \nLeft image - A pencil case containing a single natural wood colored pencil along with other stationery items.; \nRight image - A young mind sketches ideas with a classic green pencil, one among a vast array of tools to capture creativity on paper..\nFor the last step, we prompt LLMs to reason whether the statement match the pair of captions. For instance, using the same example above, GPT-4 reasons as follows:\nReasoning - The statement is False. This is because the left image is described to include only a single two-toned pencil, which implies there are not several pencils of different colors in the left image. However, the statement claims that there should be several pencils of different colors in the left image. Meanwhile, the right image correctly only shows a single green pencil, but the statement is negated by the inaccuracy of the first part regarding the left image.\nUsing this approach, visio-linguistic models only need to process one image at a time, rather then processing a pair of images simultaneously. In addition, reasoning is performed only over text, instead of over both image and text modalities.\nTable 4  ###reference_### show the performance of this intervention. Performance of InstructBLIP and PromptCap under zero-shot setting is on par with UNITER models post-finetuning. For GPT-4V, the overall performance improved minorly; for LLaVA-13B, performance improved by 10% to 16%, depending on language. On average, our pipeline boost LLaVA\u2019s performance on MaRVL by 13.4%, and improving LLaVA\u2019s performance on NLVR2 by 8%. This captioning intervention improves LLaVA\u2019s performance, achieving the best performance under zero-shot setting (without training on pairs of images)."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Previous research on instruction tuning sparks multiple works to finetune models on instructions, and create general-purpose models that are good at performing tasks under zero-shot settings Ouyang et al. (2022  ###reference_b27###); Liu et al. (2023b  ###reference_b23###); Geigle et al. (2023  ###reference_b13###). However, instruction tuning data is mostly in English Touvron et al. (2023  ###reference_b34###); Liu et al. (2023b  ###reference_b23###). Due to the absence of multilingual instruction tuning data, models may struggle to effectively process multilingual inputs. Past research efforts has predominantly centered around English language models, highlighting differences in model performance across languages (Blasi et al., 2022  ###reference_b4###; Song et al., 2023  ###reference_b31###). In the visio-linguistic domain, research in instruction tuning also center on English, due to a lack of multilingual instruction training data Geigle et al. (2023  ###reference_b13###). To this end, mBLIP Geigle et al. (2023  ###reference_b13###) translated instruction training data to various languages, and perform instruction tuning. This is the first multilingual instruction tuned vision LLM. Currently, there is a trend of shifting from openly releasing models to paid APIs Abdalla et al. (2023  ###reference_b1###). Previous research on examining GPT-4V demonstrates its unprecedented multimodal capabilities, and there is still a gap between this proprietary model and other open source models Yang et al. (2023  ###reference_b38###). However, it is still important for the community to have as strong open source multimodal models.\n\nAI in healthcare diagnostics has witnessed significant strides, particularly with deep learning models enhancing diagnostic accuracy (Esteva et al., 2017; Rajpurkar et al., 2018). These models have shown potential in fields like radiology and pathology, yet challenges remain in ensuring reliability and addressing biases inherent in clinical settings (Pesapane et al., 2018; Naylor et al., 2023)."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In conclusion, our study explores the evolving domain of multilingual visual reasoning. We observe a trend towards inclusivity in models, yet recognize persistent disparities in performance across languages and cultures. While proprietary systems like GPT-4V exhibit notable accuracy and equity across languages, open models still face challenges in bridging the gap, especially for low-resource languages. Our comparative analysis highlights the superior performance of GPT-4V but also underscores the need for advancements in open models. Through strategic interventions addressing multilinguality, multimodality, and reasoning, we showcase promising avenues for improvement. Leveraging our interventions, we demonstrate significant enhancements in open model performance, achieving state-of-the-art results under zero-shot settings for open models. Our findings emphasizes the potential for further advancements in language-equitable multimodal reasoning, with the aim of narrowing down the gap between human and machine performance, and the gap between proprietary and open models.\nThe authors thank the members of CMU who gave useful feedback on an early version of the draft. This work was supported in part by grants from Google and DSTA Singapore."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A Models and Evaluation Protocols",
            "text": "In this section, we introduce all multimodal models that we evaluate in Section 4  ###reference_###."
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S2.T1\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" id=\"S2.T1.1\" style=\"width:433.6pt;height:241.2pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(38.8pt,-21.6pt) scale(1.21812605547068,1.21812605547068) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S2.T1.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S2.T1.1.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.1.1.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S2.T1.1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.1.1.1.2.1\">NLVR2-en</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S2.T1.1.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.1.1.1.3.1\">id</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S2.T1.1.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.1.1.1.4.1\">sw</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S2.T1.1.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.1.1.1.5.1\">ta</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S2.T1.1.1.1.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.1.1.1.6.1\">tr</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S2.T1.1.1.1.1.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.1.1.1.7.1\">zh</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S2.T1.1.1.1.1.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.1.1.1.8.1\">MaRVL-Avg.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S2.T1.1.1.1.1.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.1.1.1.9.1\">MaRVL-Avg. - EN</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.2.1.1\"><span class=\"ltx_text\" id=\"S2.T1.1.1.2.1.1.1\" style=\"background-color:#D0DFFA;\">Human</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.2.1.2\">96.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.2.1.3\">96.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.2.1.4\">93.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.2.1.5\">98.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.2.1.6\">97.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.2.1.7\">95.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.2.1.8\">96.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.2.1.9\">-0.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.3.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"9\" id=\"S2.T1.1.1.3.2.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S2.T1.1.1.3.2.1.1\">Zero-Shot</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.4.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.4.3.1\"><span class=\"ltx_text\" id=\"S2.T1.1.1.4.3.1.1\" style=\"background-color:#FEB4E7;\">GPT-4V</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.4.3.2\">81.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.4.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.1.4.3.3.1\">80.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.4.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.1.4.3.4.1\">81.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.4.3.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.1.4.3.5.1\">78.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.4.3.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.1.4.3.6.1\">87.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.4.3.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.1.4.3.7.1\">83.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.4.3.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.1.4.3.8.1\">82.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.4.3.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.1.4.3.9.1\">0.7</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.5.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.5.4.1\">mBLIP (mT0-XL)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.5.4.2\">67.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.5.4.3\">64.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.5.4.4\">64.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.5.4.5\">69.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.5.4.6\">68.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.5.4.7\">65.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.5.4.8\">66.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.5.4.9\">-0.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.6.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.6.5.1\">LLaVA-13B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.6.5.2\">60.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.6.5.3\">54.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.6.5.4\">52.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.6.5.5\">50.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.6.5.6\">55.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.6.5.7\">52.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.6.5.8\">53.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.6.5.9\">-6.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.7.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"9\" id=\"S2.T1.1.1.7.6.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S2.T1.1.1.7.6.1.1\">Finetuned</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.8.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.8.7.1\">mBLIP (mT0-XL)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.8.7.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.1.8.7.2.1\">85.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.8.7.3\">75.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.8.7.4\">74.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.8.7.5\">75.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.8.7.6\">74.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.8.7.7\">75.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.8.7.8\">75.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.8.7.9\">-10.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.9.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.9.8.1\">CCLM-4M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.9.8.2\">80.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.9.8.3\">67.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.9.8.4\">64.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.9.8.5\">60.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.9.8.6\">69.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.9.8.7\">69.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.9.8.8\">66.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.9.8.9\">-14.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.10.9\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.10.9.1\">xUNITER</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.10.9.2\">72.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.10.9.3\">57.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.10.9.4\">56.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.10.9.5\">54.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.10.9.6\">57.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.10.9.7\">54.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.10.9.8\">56.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.10.9.9\">-16.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.11.10\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S2.T1.1.1.11.10.1\">mUNITER</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S2.T1.1.1.11.10.2\">73.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S2.T1.1.1.11.10.3\">55.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S2.T1.1.1.11.10.4\">51.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S2.T1.1.1.11.10.5\">52.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S2.T1.1.1.11.10.6\">54.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S2.T1.1.1.11.10.7\">56.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S2.T1.1.1.11.10.8\">54.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S2.T1.1.1.11.10.9\">-19.2</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>NLVR2 and MaRVL performance across <span class=\"ltx_text\" id=\"S2.T1.4.1\" style=\"background-color:#D0DFFA;\">Human</span>, <span class=\"ltx_text\" id=\"S2.T1.5.2\" style=\"background-color:#FEB4E7;\">Proprietary Models</span>, and Open Models. Overall, mBLIP outperforms GPT-4V in NLVR2 post finetuning, while GPT-4V shows the best performance across all other languages without finetuning.</figcaption>\n</figure>",
            "capture": "Table 1: NLVR2 and MaRVL performance across Human, Proprietary Models, and Open Models. Overall, mBLIP outperforms GPT-4V in NLVR2 post finetuning, while GPT-4V shows the best performance across all other languages without finetuning."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T2\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" id=\"S5.T2.1\" style=\"width:433.6pt;height:241.2pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(38.8pt,-21.6pt) scale(1.21812605547068,1.21812605547068) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T2.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T2.1.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T2.1.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.1.1.1.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T2.1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.1.1.1.1.2.1\">NLVR2-en</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T2.1.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.1.1.1.1.3.1\">id</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T2.1.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.1.1.1.1.4.1\">sw</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T2.1.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.1.1.1.1.5.1\">ta</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T2.1.1.1.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.1.1.1.1.6.1\">tr</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T2.1.1.1.1.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.1.1.1.1.7.1\">zh</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T2.1.1.1.1.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.1.1.1.1.8.1\">MaRVL-Avg.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T2.1.1.1.1.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.1.1.1.1.9.1\">MaRVL-Avg. - EN</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T2.1.1.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"8\" id=\"S5.T2.1.1.2.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T2.1.1.2.1.1.1\">Zero-Shot</span></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S5.T2.1.1.2.1.2\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.1.3.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.3.2.1\"><span class=\"ltx_text\" id=\"S5.T2.1.1.3.2.1.1\" style=\"background-color:#FEB4E7;\">GPT-4V</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.3.2.2\">81.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.3.2.3\">78.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.3.2.4\">75.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.3.2.5\">70.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.3.2.6\">78.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.3.2.7\">78.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.3.2.8\">76.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.3.2.9\">-5.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.1.4.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.4.3.1\"><span class=\"ltx_text\" id=\"S5.T2.1.1.4.3.1.1\" style=\"background-color:#D0DFFA;\">mBLIP (mT0-XL)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.4.3.2\">67.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.4.3.3\">65.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.4.3.4\">66.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.4.3.5\">68.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.4.3.6\">68.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.4.3.7\">67.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.4.3.8\">67.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.4.3.9\">-0.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.1.5.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.5.4.1\"><span class=\"ltx_text\" id=\"S5.T2.1.1.5.4.1.1\" style=\"background-color:#D0DFFA;\">LLaVA-13B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.5.4.2\">60.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.5.4.3\">53.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.5.4.4\">53.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.5.4.5\">54.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.5.4.6\">58.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.5.4.7\">54.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.5.4.8\">54.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.5.4.9\">-5.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.1.6.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"8\" id=\"S5.T2.1.1.6.5.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T2.1.1.6.5.1.1\">Finetuned</span></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S5.T2.1.1.6.5.2\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.1.7.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.7.6.1\"><span class=\"ltx_text\" id=\"S5.T2.1.1.7.6.1.1\" style=\"background-color:#D0DFFA;\">mBLIP (mT0-XL)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.7.6.2\">85.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.7.6.3\">77.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.7.6.4\">78.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.7.6.5\">78.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.7.6.6\">73.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.7.6.7\">75.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.7.6.8\">76.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.7.6.9\">-8.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.1.8.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.8.7.1\"><span class=\"ltx_text\" id=\"S5.T2.1.1.8.7.1.1\" style=\"background-color:#D0DFFA;\">CCLM-4M</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.8.7.2\">80.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.8.7.3\">72.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.8.7.4\">69.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.8.7.5\">69.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.8.7.6\">77.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.8.7.7\">71.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.8.7.8\">72.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.8.7.9\">-8.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.1.9.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.9.8.1\"><span class=\"ltx_text\" id=\"S5.T2.1.1.9.8.1.1\" style=\"background-color:#D0DFFA;\">xUNITER</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.9.8.2\">72.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.9.8.3\">63.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.9.8.4\">63.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.9.8.5\">62.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.9.8.6\">67.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.9.8.7\">62.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.9.8.8\">63.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.9.8.9\">-8.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.1.10.9\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.10.9.1\"><span class=\"ltx_text\" id=\"S5.T2.1.1.10.9.1.1\" style=\"background-color:#D0DFFA;\">mUNITER</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.10.9.2\">73.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.10.9.3\">59.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.10.9.4\">63.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.10.9.5\">62.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.10.9.6\">69.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.10.9.7\">62.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.10.9.8\">63.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.1.10.9.9\">-9.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.1.11.10\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T2.1.1.11.10.1\"><span class=\"ltx_text\" id=\"S5.T2.1.1.11.10.1.1\" style=\"background-color:#D0DFFA;\">ViLT</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T2.1.1.11.10.2\">73.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T2.1.1.11.10.3\">61.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T2.1.1.11.10.4\">62.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T2.1.1.11.10.5\">65.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T2.1.1.11.10.6\">69.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T2.1.1.11.10.7\">60.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T2.1.1.11.10.8\">63.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T2.1.1.11.10.9\">9.8</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>MaRVL translate-test accuracies across <span class=\"ltx_text\" id=\"S5.T2.4.1\" style=\"background-color:#D0DFFA;\">Open</span> and <span class=\"ltx_text\" id=\"S5.T2.5.2\" style=\"background-color:#FEB4E7;\">Proprietary</span> models.</figcaption>\n</figure>",
            "capture": "Table 2: MaRVL translate-test accuracies across Open and Proprietary models."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S6.T3\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" id=\"S6.T3.1\" style=\"width:433.6pt;height:200.1pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(119.3pt,-55.1pt) scale(2.22368636651952,2.22368636651952) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S6.T3.1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S6.T3.1.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S6.T3.1.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T3.1.1.1.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S6.T3.1.1.1.1.2\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T3.1.1.1.1.2.1\">NLVR</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"6\" id=\"S6.T3.1.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T3.1.1.1.1.3.1\">MaRVL</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T3.1.1.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S6.T3.1.1.2.2.1\" style=\"padding-bottom:-8.61108pt;\">\n<span class=\"ltx_ERROR undefined\" id=\"S6.T3.1.1.2.2.1.1\">\\cdashline</span>3-8</th>\n<th class=\"ltx_td ltx_th ltx_th_column\" id=\"S6.T3.1.1.2.2.2\" style=\"padding-bottom:-8.61108pt;\"></th>\n<th class=\"ltx_td ltx_th ltx_th_column\" id=\"S6.T3.1.1.2.2.3\" style=\"padding-bottom:-8.61108pt;\"></th>\n<th class=\"ltx_td ltx_th ltx_th_column\" id=\"S6.T3.1.1.2.2.4\" style=\"padding-bottom:-8.61108pt;\"></th>\n<th class=\"ltx_td ltx_th ltx_th_column\" id=\"S6.T3.1.1.2.2.5\" style=\"padding-bottom:-8.61108pt;\"></th>\n<th class=\"ltx_td ltx_th ltx_th_column\" id=\"S6.T3.1.1.2.2.6\" style=\"padding-bottom:-8.61108pt;\"></th>\n<td class=\"ltx_td\" id=\"S6.T3.1.1.2.2.7\" style=\"padding-bottom:-8.61108pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T3.1.1.3.3\">\n<td class=\"ltx_td\" id=\"S6.T3.1.1.3.3.1\"></td>\n<td class=\"ltx_td\" id=\"S6.T3.1.1.3.3.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.3.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T3.1.1.3.3.3.1\">id</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.3.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T3.1.1.3.3.4.1\">sw</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.3.3.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T3.1.1.3.3.5.1\">ta</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.3.3.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T3.1.1.3.3.6.1\">tr</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.3.3.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T3.1.1.3.3.7.1\">zh</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.3.3.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T3.1.1.3.3.8.1\">Avg.</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T3.1.1.4.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T3.1.1.4.4.1\">GPT-3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T3.1.1.4.4.2\">67.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T3.1.1.4.4.3\">64.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T3.1.1.4.4.4\">59.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T3.1.1.4.4.5\">60.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T3.1.1.4.4.6\">67.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T3.1.1.4.4.7\">64.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T3.1.1.4.4.8\">63.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T3.1.1.5.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S6.T3.1.1.5.5.1\">LLaMA2-70b</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S6.T3.1.1.5.5.2\">67.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S6.T3.1.1.5.5.3\">58.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S6.T3.1.1.5.5.4\">57.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S6.T3.1.1.5.5.5\">58.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S6.T3.1.1.5.5.6\">65.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S6.T3.1.1.5.5.7\">61.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S6.T3.1.1.5.5.8\">60.2</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>VisProg performance across models.</figcaption>\n</figure>",
            "capture": "Table 3: VisProg performance across models."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S6.T4\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" id=\"S6.T4.1\" style=\"width:433.6pt;height:169.1pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(55.3pt,-21.6pt) scale(1.34235830748386,1.34235830748386) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S6.T4.1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S6.T4.1.1.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S6.T4.1.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T4.1.1.1.1.1.1\">Captioning</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S6.T4.1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T4.1.1.1.1.2.1\">Reasoning</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S6.T4.1.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T4.1.1.1.1.3.1\">NLVR (en)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S6.T4.1.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T4.1.1.1.1.4.1\">id</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S6.T4.1.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T4.1.1.1.1.5.1\">sw</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S6.T4.1.1.1.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T4.1.1.1.1.6.1\">ta</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S6.T4.1.1.1.1.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T4.1.1.1.1.7.1\">tr</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S6.T4.1.1.1.1.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T4.1.1.1.1.8.1\">zh</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S6.T4.1.1.1.1.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T4.1.1.1.1.9.1\">MaRVL-Avg.</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T4.1.1.2.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.2.2.1\">InstructBLIP</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.2.2.2\">LLaMA2-70B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.2.2.3\">65.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.2.2.4\">61.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.2.2.5\">60.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.2.2.6\">60.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.2.2.7\">62.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.2.2.8\">62.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.2.2.9\">61.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T4.1.1.3.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.3.3.1\">PromptCap</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.3.3.2\">LLaMA2-70B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.3.3.3\">63.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.3.3.4\">59.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.3.3.5\">58.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.3.3.6\">58.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.3.3.7\">59.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.3.3.8\">59.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.3.3.9\">59.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T4.1.1.4.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.4.4.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S6.T4.1.1.4.4.1.1\">GPT-4V</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.4.4.2\">No Intervention</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.4.4.3\">81.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.4.4.4\">80.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.4.4.5\">81.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.4.4.6\">78.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.4.4.7\">87.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.4.4.8\">83.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.4.4.9\">82.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T4.1.1.5.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.5.5.1\">GPT4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.5.5.2\">82.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.5.5.3\">81.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.5.5.4\">81.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.5.5.5\">76.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.5.5.6\">90.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.5.5.7\">85.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.5.5.8\">82.92</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T4.1.1.6.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S6.T4.1.1.6.6.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S6.T4.1.1.6.6.1.1\">LLaVA-13B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.6.6.2\">No Intervention</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.6.6.3\">60.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.6.6.4\">54.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.6.6.5\">52.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.6.6.6\">50.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.6.6.7\">55.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.6.6.8\">52.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.6.6.9\">53.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T4.1.1.7.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T4.1.1.7.7.1\">LLaMA2-70B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T4.1.1.7.7.2\">68.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T4.1.1.7.7.3\">65.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T4.1.1.7.7.4\">65.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T4.1.1.7.7.5\">65.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T4.1.1.7.7.6\">69.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T4.1.1.7.7.7\">70.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T4.1.1.7.7.8\">67.6</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>Captioning Pipeline Performance across Models. For rows with \u201dNo Intervention\u201d stated in the \u201dReasoning\u201d column, we pull over the end-to-end results of that model from Table 1, for the sake of comparison.</figcaption>\n</figure>",
            "capture": "Table 4: Captioning Pipeline Performance across Models. For rows with \u201dNo Intervention\u201d stated in the \u201dReasoning\u201d column, we pull over the end-to-end results of that model from Table 1, for the sake of comparison."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.01404v1_figure_1.png",
            "caption": "Figure 1: Our Contributions: First, we evaluate the multilingual visual reasoning capabilities of multimodal models; then, we analyze the challenges where these models are falling short; last but not the least, we propose several interventions to address these challenges."
        },
        "2": {
            "figure_path": "2403.01404v1_figure_2.png",
            "caption": "Figure 2: Example from the MaRVL Dataset: Given two images and a statement, the task is to infer whether the statement is true or false of the given image pair."
        },
        "3": {
            "figure_path": "2403.01404v1_figure_3.png",
            "caption": "Figure 3: Performance of GPT-4V decreases as statement length increases."
        },
        "4": {
            "figure_path": "2403.01404v1_figure_4.png",
            "caption": "Figure 4: Flow chart visualizing the end-to-end testing in \u00a74 and all interventions performed in \u00a76."
        },
        "5": {
            "figure_path": "2403.01404v1_figure_5.png",
            "caption": "Figure 5: VisProg example image pair."
        },
        "6": {
            "figure_path": "2403.01404v1_figure_6.png",
            "caption": "Figure 6: Captioning example image pair."
        }
    },
    "references": [
        {
            "1": {
                "title": "The elephant in the room: Analyzing the presence of big tech in natural language processing research.",
                "author": "Mohamed Abdalla, Jan Philip Wahle, Terry Ruas, Aur\u00e9lie N\u00e9v\u00e9ol, Fanny Ducel, Saif Mohammad, and Karen Fort. 2023.",
                "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13141\u201313160, Toronto, Canada. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.acl-long.734"
            }
        },
        {
            "2": {
                "title": "Gpt-4 technical report.",
                "author": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023.",
                "venue": "arXiv preprint arXiv:2303.08774.",
                "url": null
            }
        },
        {
            "3": {
                "title": "Do all languages cost the same? tokenization in the era of commercial language models.",
                "author": "Orevaoghene Ahia, Sachin Kumar, Hila Gonen, Jungo Kasai, David Mortensen, Noah Smith, and Yulia Tsvetkov. 2023.",
                "venue": "In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 9904\u20139923, Singapore. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.emnlp-main.614"
            }
        },
        {
            "4": {
                "title": "Systematic inequalities in language technology performance across the world\u2019s languages.",
                "author": "Damian Blasi, Antonios Anastasopoulos, and Graham Neubig. 2022.",
                "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5486\u20135505, Dublin, Ireland. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2022.acl-long.376"
            }
        },
        {
            "5": {
                "title": "Behind the scene: Revealing the secrets of pre-trained vision-and-language models.",
                "author": "Jize Cao, Zhe Gan, Yu Cheng, Licheng Yu, Yen-Chun Chen, and Jingjing Liu. 2020.",
                "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part VI 16, pages 565\u2013580. Springer.",
                "url": null
            }
        },
        {
            "6": {
                "title": "Uniter: Universal image-text representation learning.",
                "author": "Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. 2020.",
                "venue": "In European conference on computer vision, pages 104\u2013120. Springer.",
                "url": null
            }
        },
        {
            "7": {
                "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.",
                "author": "Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023.",
                "venue": null,
                "url": "https://lmsys.org/blog/2023-03-30-vicuna/"
            }
        },
        {
            "8": {
                "title": "Scaling instruction-finetuned language models.",
                "author": "Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022.",
                "venue": "arXiv preprint arXiv:2210.11416.",
                "url": null
            }
        },
        {
            "9": {
                "title": "Unsupervised cross-lingual representation learning at scale.",
                "author": "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020.",
                "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440\u20138451, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2020.acl-main.747"
            }
        },
        {
            "10": {
                "title": "Imagenet: A large-scale hierarchical image database.",
                "author": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009.",
                "venue": "In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Bert: Pre-training of deep bidirectional transformers for language understanding.",
                "author": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.",
                "venue": "arXiv preprint arXiv:1810.04805.",
                "url": null
            }
        },
        {
            "12": {
                "title": "Pal: Program-aided language models.",
                "author": "Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023.",
                "venue": "In International Conference on Machine Learning, pages 10764\u201310799. PMLR.",
                "url": null
            }
        },
        {
            "13": {
                "title": "mblip: Efficient bootstrapping of multilingual vision-llms.",
                "author": "Gregor Geigle, Abhay Jain, Radu Timofte, and Goran Glava\u0161. 2023.",
                "venue": "arXiv, abs/2307.06930.",
                "url": "http://arxiv.org/abs/2307.06930"
            }
        },
        {
            "14": {
                "title": "Visual programming: Compositional visual reasoning without training.",
                "author": "Tanmay Gupta and Aniruddha Kembhavi. 2023.",
                "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14953\u201314962.",
                "url": null
            }
        },
        {
            "15": {
                "title": "Challenges and strategies in cross-cultural NLP.",
                "author": "Daniel Hershcovich, Stella Frank, Heather Lent, Miryam de Lhoneux, Mostafa Abdou, Stephanie Brandl, Emanuele Bugliarello, Laura Cabello Piqueras, Ilias Chalkidis, Ruixiang Cui, Constanza Fierro, Katerina Margatina, Phillip Rust, and Anders S\u00f8gaard. 2022.",
                "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6997\u20137013, Dublin, Ireland. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2022.acl-long.482"
            }
        },
        {
            "16": {
                "title": "Promptcap: Prompt-guided task-aware image captioning.",
                "author": "Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah A Smith, and Jiebo Luo. 2022.",
                "venue": "arXiv preprint arXiv:2211.09699.",
                "url": null
            }
        },
        {
            "17": {
                "title": "The state and fate of linguistic diversity and inclusion in the NLP world.",
                "author": "Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. 2020.",
                "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6282\u20136293, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2020.acl-main.560"
            }
        },
        {
            "18": {
                "title": "Vilt: Vision-and-language transformer without convolution or region supervision.",
                "author": "Wonjae Kim, Bokyung Son, and Ildoo Kim. 2021.",
                "venue": "In International Conference on Machine Learning, pages 5583\u20135594. PMLR.",
                "url": null
            }
        },
        {
            "19": {
                "title": "Chatgpt beyond english: Towards a comprehensive evaluation of large language models in multilingual learning.",
                "author": "Viet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben Veyseh, Hieu Man, Franck Dernoncourt, Trung Bui, and Thien Huu Nguyen. 2023.",
                "venue": "arXiv preprint arXiv:2304.05613.",
                "url": null
            }
        },
        {
            "20": {
                "title": "Pretrained language models for text generation: A survey.",
                "author": "Junyi Li, Tianyi Tang, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2022.",
                "venue": "arXiv preprint arXiv:2201.05273.",
                "url": null
            }
        },
        {
            "21": {
                "title": "Visually grounded reasoning across languages and cultures.",
                "author": "Fangyu Liu, Emanuele Bugliarello, Edoardo Maria Ponti, Siva Reddy, Nigel Collier, and Desmond Elliott. 2021.",
                "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10467\u201310485, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.emnlp-main.818"
            }
        },
        {
            "22": {
                "title": "Improved baselines with visual instruction tuning.",
                "author": "Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023a.",
                "venue": null,
                "url": null
            }
        },
        {
            "23": {
                "title": "Improved baselines with visual instruction tuning.",
                "author": "Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023b.",
                "venue": "arXiv preprint arXiv:2310.03744.",
                "url": null
            }
        },
        {
            "24": {
                "title": "Visual instruction tuning.",
                "author": "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023c.",
                "venue": "In NeurIPS.",
                "url": null
            }
        },
        {
            "25": {
                "title": "Trends in integration of vision and language research: A survey of tasks, datasets, and methods.",
                "author": "Aditya Mogadala, Marimuthu Kalimuthu, and Dietrich Klakow. 2021.",
                "venue": "Journal of Artificial Intelligence Research, 71:1183\u20131317.",
                "url": null
            }
        },
        {
            "26": {
                "title": "Show your work: Scratchpads for intermediate computation with language models.",
                "author": "Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021.",
                "venue": "arXiv preprint arXiv:2112.00114.",
                "url": null
            }
        },
        {
            "27": {
                "title": "Training language models to follow instructions with human feedback.",
                "author": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022.",
                "venue": "Advances in Neural Information Processing Systems, 35:27730\u201327744.",
                "url": null
            }
        },
        {
            "28": {
                "title": "Visual language integration: A survey and open challenges.",
                "author": "Sang-Min Park and Young-Gab Kim. 2023.",
                "venue": "Computer Science Review, 48:100548.",
                "url": null
            }
        },
        {
            "29": {
                "title": "Theory and applications of ontology: Computer applications.",
                "author": "Roberto Poli, Michael Healy, and Achilles Kameas. 2010.",
                "venue": "Springer.",
                "url": null
            }
        },
        {
            "30": {
                "title": "Imagenet large scale visual recognition challenge.",
                "author": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. 2015.",
                "venue": "International journal of computer vision, 115:211\u2013252.",
                "url": null
            }
        },
        {
            "31": {
                "title": "GlobalBench: A benchmark for global progress in natural language processing.",
                "author": "Yueqi Song, Simran Khanuja, Pengfei Liu, Fahim Faisal, Alissa Ostapenko, Genta Winata, Alham Aji, Samuel Cahyawijaya, Yulia Tsvetkov, Antonios Anastasopoulos, and Graham Neubig. 2023.",
                "venue": "In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 14157\u201314171, Singapore. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.emnlp-main.875"
            }
        },
        {
            "32": {
                "title": "A corpus for reasoning about natural language grounded in photographs.",
                "author": "Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. 2019.",
                "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6418\u20136428, Florence, Italy. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/P19-1644"
            }
        },
        {
            "33": {
                "title": "Vipergpt: Visual inference via python execution for reasoning.",
                "author": "D\u00eddac Sur\u00eds, Sachit Menon, and Carl Vondrick. 2023.",
                "venue": "Proceedings of IEEE International Conference on Computer Vision (ICCV).",
                "url": null
            }
        },
        {
            "34": {
                "title": "Llama 2: Open foundation and fine-tuned chat models.",
                "author": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023.",
                "venue": "arXiv preprint arXiv:2307.09288.",
                "url": null
            }
        },
        {
            "35": {
                "title": "Chain-of-thought prompting elicits reasoning in large language models.",
                "author": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022.",
                "venue": "Advances in Neural Information Processing Systems, 35:24824\u201324837.",
                "url": null
            }
        },
        {
            "36": {
                "title": "Visual chatgpt: Talking, drawing and editing with visual foundation models.",
                "author": "Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. 2023.",
                "venue": "arXiv preprint arXiv:2303.04671.",
                "url": null
            }
        },
        {
            "37": {
                "title": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation.",
                "author": "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016.",
                "venue": "arXiv preprint arXiv:1609.08144.",
                "url": null
            }
        },
        {
            "38": {
                "title": "The dawn of lmms: Preliminary explorations with gpt-4v(ision).",
                "author": "Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. 2023.",
                "venue": "arXiv, abs/2309.17421.",
                "url": "http://arxiv.org/abs/2309.17421"
            }
        },
        {
            "39": {
                "title": "Empowering llm-based machine translation with cultural awareness.",
                "author": "Binwei Yao, Ming Jiang, Diyi Yang, and Junjie Hu. 2023.",
                "venue": "arXiv preprint arXiv:2305.14328.",
                "url": null
            }
        },
        {
            "40": {
                "title": "Cross-view language modeling: Towards unified cross-lingual cross-modal pre-training.",
                "author": "Yan Zeng, Wangchunshu Zhou, Ao Luo, Ziming Cheng, and Xinsong Zhang. 2023.",
                "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5731\u20135746, Toronto, Canada. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.acl-long.315"
            }
        },
        {
            "41": {
                "title": "Least-to-most prompting enables complex reasoning in large language models.",
                "author": "Denny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. 2022.",
                "venue": "arXiv preprint arXiv:2205.10625.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.01404v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "7"
        ],
        "methodology_sections": [
            "5",
            "6"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "6",
            "6.1",
            "6.2",
            "6.3"
        ]
    },
    "research_context": {
        "paper_id": "2403.01404v1",
        "paper_title": "What Is Missing in Multilingual Visual Reasoning and How to Fix It",
        "research_background": "**Motivation:**\nThe primary motivation behind the paper is to address the persistent issue of model performance disparities across different languages and cultures in multilingual and multimodal language technologies. While recent advancements have made these models more inclusive, there is a significant performance gap favoring North American or Western contexts, which marginalizes users from other regions. This issue is amplified by the trend of limiting model access through paid web APIs, which disproportionately affects low-resource languages due to higher costs and poorer performance. The authors aim to advance language-equitable multimodal reasoning capabilities, especially for open models, which have advantages in terms of control, ownership, cost, and scientific advancement.\n\n**Research Problem:**\nThe research problem centers on understanding why open models exhibit a significant performance gap between English and other languages in multilingual visual reasoning tasks, and to develop methods to bring these open models closer in performance to proprietary systems like GPT-4V. The study investigates three main challenges: (1) Multilinguality, (2) Reasoning, and (3) Multimodality, and proposes interventions to address these challenges to enhance the performance of open models.\n\n**Relevant Prior Work:**\n1. **Multilingual and Multimodal Models:**\n   - The paper builds on the work of models that are increasingly multilingual and multimodal, such as those by Lai et al. (2023), Li et al. (2022), Yang et al. (2023), Chen et al. (2020), Zeng et al. (2023), and Geigle et al. (2023).\n   - It references the trend where these models often perform better in North American or Western contexts while underperforming in other regions, as highlighted by Liu et al. (2021) and Hershcovich et al. (2022).\n\n2. **Access and Cost of Language Models:**\n   - The trend of limiting model access through paid web APIs is discussed in Abdalla et al. (2023).\n   - The economic challenges faced by users of low-resourced languages, who encounter higher costs for these services, are mentioned by Ahia et al. (2023).\n\n3. **Performance Disparities:**\n   - Blasi et al. (2022) underscores the performance disparities across different languages.\n   - Liu et al. (2021) emphasizes how these models cater to North American or Western contexts.\n\n4. **Open Models and Training Data:**\n   - The performance of various open models like LLaVA Liu et al. (2023c, a), mBLIP Geigle et al. (2023), and CCLM Zeng et al. (2023) is contrasted with proprietary systems like GPT-4V.\n   - The \u201ccurse of multilinguality,\u201d where smaller models struggle with low-resource languages, is attributed to Conneau et al. (2020).\n\n5. **Interventions and Methodology:**\n   - Inspired by Gupta and Kembhavi (2023)'s VisProg method, the authors propose generating programs to reason over images using translated statements.\n   - They also incorporate the chain-of-thought capabilities of text-modality LLMs as discussed by Wei et al. (2022).\n\nThe paper identifies measurable gaps in open models and proposes novel, data-backed interventions to bridge these gaps, thereby advancing multilingual visual reasoning capabilities.",
        "methodology": "It is observed that the best model for multilingual visual reasoning still lags behind human performance by 10% to 20%. The authors aim to analyze the challenges inherent in multilingual visual reasoning and identify three key aspects contributing to the performance gap: \n\n1. **Aspect 1:** The model needs to enhance its ability to understand and process multilingual data effectively.\n2. **Aspect 2:** It should improve its visual reasoning capabilities to better interpret and analyze visual inputs.\n3. **Aspect 3:** There should be a focus on integrating both multilingual understanding and visual reasoning in a seamless manner to achieve more accurate results.\n\nBy addressing these three aspects, the proposed method attempts to bridge the performance gap between current models and human-level understanding in multilingual visual reasoning tasks.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\n#### Models Evaluated:\n1. **Open Models:**\n   - mBLIP (mt0-xl) \n   - LLaVA\n   - CCLM\n   - UNITERs\n\n2. **Proprietary Model:**\n   - GPT-4V(ision)\n\n#### Evaluation Settings:\n\n1. **Zero-shot Setting:**\n   - **Description:** Models are tested without specific fine-tuning for multimodal visual reasoning tasks.\n   - **Models Evaluated:** LLaVA, mBLIP, GPT-4V\n   - **Purpose:** \n     - Academically, this setting helps in assessing the general reasoning abilities of the models on datasets without available training data.\n     - Practically, it\u2019s useful for LMs like GPT-4V that are difficult to fine-tune and LLaVA due to its large size.\n    \n2. **Finetuned Setting:**\n   - **Description:** Models are fine-tuned on the English NLVR2 dataset and tested on both NLVR2 and MaRVL datasets.\n   - **Models Evaluated:** mBLIP, CCLM-4M, xUNITER, mUNITER\n   - **Purpose:** This setting simulates the adaptation of a multilingual model to a specific reasoning task using only English data.\n\n#### Evaluation Metrics:\n- The specific evaluation metrics used in the study are not detailed in the given sections. Typically, such evaluations use accuracy, F1 score, precision, recall, or similar measures to assess model performance on reasoning tasks.\n\n### Main Experimental Results:\n- The main results, including quantitative performance data for the models in both zero-shot and finetuned settings, are not provided in the provided sections.\n- Detailed results typically show how well each model performs on the specified datasets (NLVR2 and MaRVL), comparing the zero-shot capabilities against fine-tuned adaptations. \n\nTo summarize, the main experiment involves evaluating multiple models on visual reasoning tasks under zero-shot and fine-tuned conditions, focusing on general versus adapted performance using English-only finetuning data."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Examine whether translating the text to English and reducing the cross-lingual gap aids performance in visual reasoning tasks.",
            "experiment_process": "Translate the reasoning statements from the MaRVL dataset (which contains statements in five diverse languages) to English using the Google Translate API. Evaluate the ViLT model (and other models) on the translated statements using the evaluation protocols introduced in section 3.",
            "result_discussion": "All models except GPT-4V showed an increase in accuracy after performing translate test, whereas GPT-4V experienced a sharp decrease in performance. This suggests that GPT-4V has strong multilingual capabilities and translating to English introduces errors due to cultural-specific nuances in meaning.",
            "ablation_id": "2403.01404v1.No1"
        },
        {
            "research_objective": "Investigate if breaking down complex reasoning into modular programs which can be executed on a vision-text input can improve performance on reasoning tasks.",
            "experiment_process": "Use the Visual Programming approach (VisProg) to generate visual programs given the translated statements. Use text-davinci-003 as a representative proprietary LLM and LLaMA2-70B as a representative open LLM. Execute these programs to obtain final answers and compare performance with models evaluated end-to-end.",
            "result_discussion": "This approach provides valuable insights into breaking down complex reasoning into modular modules, showing on par performance on MaRVL compared to ViLT post-finetuning, without additional training.",
            "ablation_id": "2403.01404v1.No2"
        },
        {
            "research_objective": "Determine if alleviating the need for multimodal interaction during the reasoning process by using captions rather than images can improve performance on visual reasoning tasks.",
            "experiment_process": "Caption both images and use an LLM to reason about the given statement with the two captions. Generate targeted instructions based on the statement to guide caption generation by multiple captioning models. Finally, prompt LLMs to reason whether the statement matches the pair of captions.",
            "result_discussion": "Using this captioning approach, performance of InstructBLIP and PromptCap under zero-shot setting is on par with post-finetuning models. GPT-4V showed minor performance improvements, while LLaVA-13B performance improved by 10% to 16% depending on language. On average, LLaVA's performance on MaRVL improved by 13.4%, achieving the best performance in a zero-shot setting.",
            "ablation_id": "2403.01404v1.No3"
        }
    ]
}