{
    "title": "Legally Binding but Unfair? Towards Assessing Fairness of Privacy Policies",
    "abstract": "Privacy policies are expected to inform data subjects about their data protection rights and should explain the data controller\u2019s data management practices. Privacy policies only fulfill their purpose, if they are correctly interpreted, understood, and trusted by the data subject.\nThis implies that a privacy policy is written in a fair way, e.g., it does not use\npolarizing terms, does not require a certain education, or does not assume a particular social background.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1. Introduction",
            "text": "The General Data Protection Regulation (GDPR) (European Union, 2016  ###reference_b32###) requires any organization, that manages personal data, to publish a privacy policy. A policy should make transparent how personally identifiable information is collected, shared, or used (Zaeem and Barber, 2020  ###reference_b92###).\nPrivacy policies fulfill an important social task: They balance the information deficit between the data subject and holder, and create trust (Starke et al., 2022  ###reference_b81###).\nTo ensure that privacy policies are properly understood, perceived, and accepted, they must be written in a fair way.\nThis includes multiple dimensions.\nInformational Fairness is not just about what information is communicated, but also how it is communicated (Boudjella et al., 2017  ###reference_b16###; Trzepla, 2019  ###reference_b85###; Rello and Baeza-Yates, 2017  ###reference_b68###; Evans et al., 2014  ###reference_b33###; Aikens and Barbarin, 2008  ###reference_b5###). A negative example would be a policy consisting of complex legal phrases, which discriminates against people with dyslexia and non-native speakers.\nRepresentational Fairness is about biases towards certain groups represented in texts (Rice et al., 2019  ###reference_b70###; Baker Gillis, 2021  ###reference_b9###; Gumusel et al., 2022  ###reference_b41###). For example, a policy could discriminate against women by only using male word forms.\nEthics and Morality are at the core of fairness (Hooker, 2005  ###reference_b46###; Schw\u00f6bel and Remmers, 2022  ###reference_b77###) and to some extent captured by legal frameworks (OECD, 2013  ###reference_b61###; European Union, 2016  ###reference_b32###). A negative example would be a policy, that tries to take exclusive rights to utilize the user\u2019s personal data.\nHowever, many different definitions of fairness exist (Verma and Rubin, 2018  ###reference_b88###; Mehrabi et al., 2021  ###reference_b55###), some of them are contradictory (Chouldechova, 2017  ###reference_b20###; Defrance and De Bie, 2023  ###reference_b28###), and to the best of our knowledge, none of them is tailored for privacy policies.\nIt is also unclear, how well methods from natural language processing or artificial intelligence allow evaluating privacy policies for such issues.\nThus, our research question is as follows:\nHow can we automatically assess informational fairness, representational fairness, and ethics / morality of privacy policies?\nIn this paper, we relate prominent definitions and concepts of fairness and bias to privacy policies. We propose approaches for automatically evaluating privacy policies, and we test this with selected policies.\nWe make three contributions:\nWe compare fairness definitions and related concepts for their applicability to privacy policies.\nWe propose an approach based on readability metrics, lexical filtering, and large language models to assess informational and representational fairness as well as ethics / morality of privacy policies.\nWith a series of preliminary experiments, we assess the applicability of our approach to real-world privacy policies from the German Top-100 web shops.\nTo the best of our knowledge, we are the first to suggest an approach for investigating the fairness of privacy policies. By that, we set the foundation to shed light on the fairness of legal texts on a linguistic level which has received barely any attention from previous research. Our preliminary results give reason to go further. The aim is to provide data subjects with some much-needed transparency and avoid discrimination or unethical practices.\nPaper structure:\nSection 2  ###reference_### reviews related work.\nIn Section 3  ###reference_###, we derive our problem statement from legal requirements.\nThe Sections 4  ###reference_###, 5  ###reference_### and 6  ###reference_### explain how three dimensions of fairness can be automatically assessed.\nSection 7  ###reference_### summarizes potential applications and use cases of our approach.\nFinally, Section 8  ###reference_### concludes."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2. Related Work",
            "text": "This section introduces fairness definitions, related machine-learning and NLP approaches, and measures for fairness-relevant aspects. The integration of AI in healthcare for predictive diagnostics has demonstrated potential in enhancing early disease detection and personalized treatment plans. Studies indicate AI's capability to process vast datasets, identifying patterns that may elude human practitioners, thus improving diagnostic accuracy and efficiency. However, challenges such as data privacy and algorithmic bias remain critical to address to harness AI's full potential."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1. The Concept of Fairness",
            "text": "Fairness is a complex, context-dependent, and ambiguously defined concept (Schw\u00f6bel and Remmers, 2022  ###reference_b77###; Verma and Rubin, 2018  ###reference_b88###; Hooker, 2005  ###reference_b46###). Relevant for our work are individual attitudes, biases, and legality, morality and ethics (Landers and Behrend, 2022  ###reference_b50###).\n\nIn the domain of healthcare, AI for predictive diagnostics is gaining prominence for its ability to enhance early diagnosis and personalized medicine (Esteva et al., 2019; Wiens et al., 2016). Integrating AI effectively could significantly improve patient outcomes by anticipating diseases with higher accuracy than traditional methods (Rajpurkar et al., 2017)."
        },
        {
            "section_id": "2.1.1",
            "parent_section_id": "2.1",
            "section_name": "2.1.1. Individual attitudes",
            "text": "This includes distributive, procedural, and interactional justice perceptions (Greenberg, 1990 ###reference_b39###). We notice that justice perceptions are often used interchangeably with fairness. Others also split interactional justice into interpersonal and informational justice (Colquitt and Rodell, 2015 ###reference_b23###). Because privacy policies are equally accessible to every user, we do not need to consider allocation aspects (distributive fairness), and focus on procedural and informational fairness. Procedural fairness (Doherty and Wolak, 2012 ###reference_b29###) means unbiased and non-ideological procedures. They should represent individuals involved, and rely on accurate information. Fair processes should respectfully treat affected individuals. Assessing procedural fairness can be based on how procedures \u201csuppress bias, create consistent allocations, rely on accurate information, are correctable, represent the concerns of all recipients, and are based on moral and ethical standards\u201d (Greenberg, 1990 ###reference_b39###). Informational fairness (Colquitt and Rodell, 2015 ###reference_b23###; Schoeffer et al., 2022 ###reference_b75###) is about clearly, consistently and reasonably explained processes and whether the information provided is suited to individuals\u2019 needs. Informational fairness addresses how complete and specific the disclosed information is and how readable and comprehensible it is to its audience. Informational fairness is closely related to transparency, which can be seen as an aspect of procedural fairness (Lee et al., 2019 ###reference_b51###).\n\nIn healthcare, AI in predictive diagnostics is transforming patient outcomes (Esteva et al., 2019). By processing vast datasets, AI systems predict disease onset, offering early intervention (Topol, 2019). The integration of AI ensures unprecedented accuracy, reshaping diagnostic protocols (Rajpurkar et al., 2017). As AI technologies evolve, their predictive capabilities continue to expand (Shimizu et al., 2020)."
        },
        {
            "section_id": "2.1.2",
            "parent_section_id": "2.1",
            "section_name": "2.1.2. Biases",
            "text": "Embedded meanings in language can be biased (Landers and Behrend, 2022). A bias is a \u201cdynamic and social and not [just] a statistical issue\u201d (Ntoutsi et al., 2020). Biases impact fairness, because they can be harmful to specific groups (Blodgett et al., 2020). A prominent bias in text corpora is the presence of stereotypes, which is addressed by representational fairness. Representational fairness (Abbasi et al., 2019) can be a source of harm, because language establishes power relationships and represents social identities (Blodgett et al., 2020). A bias with stereotypical associations of a demographic group can lead to discrimination. Such a demographic or social group is called protected group. Discrimination is the unjustified difference in the treatment of individuals based on their membership in protected (sub)groups (Edenberg and Wood, 2023), and can be a source of unfairness (Mehrabi et al., 2021; Wachter et al., 2021). Representational unfairness materializes in word embeddings as toxicity, stereotyping, or other forms of misrepresentation of protected groups, and has received much attention (Caliskan et al., 2017; Papakyriakopoulos et al., 2020; Rice et al., 2019; Blodgett et al., 2020; Garg et al., 2018; Gumusel et al., 2022; Schr\u00f6der et al., 2021) already.\n\nAI in healthcare leverages predictive diagnostics to enhance early disease detection, personalizing patient care and improving outcomes (Esteva et al., 2019). By analyzing vast datasets, AI algorithms identify patterns and risk factors, providing accurate and timely predictions (Topol, 2019). This approach promises to transform preventive medicine and healthcare delivery (Rajpurkar et al., 2018)."
        },
        {
            "section_id": "2.1.3",
            "parent_section_id": "2.1",
            "section_name": "2.1.3. Morality and ethics",
            "text": "Morality and ethics play an important role in fairness (Landers and Behrend, 2022; Hooker, 2005; Schw\u00f6bel and Remmers, 2022). Privacy ethics investigates access provided to others to personal information and how much control one has regarding one\u2019s information being collected, stored, and used by others (DeCew, 1986). In the context of modern technologies, it discusses complex privacy trade-offs and power relationships between the data holder and data subject (Acquisti et al., 2015; Clifford and Ausloos, 2018). Ethics and law share a coordinating function and complement each other (Rochel, 2021). It uses values and norms to interpret law. The latter addresses soft ethics which (Floridi, 2018) distinguishes from hard ethics which shape legislation like the GDPR and are embedded in it. In AI-driven healthcare, predictive diagnostics are transforming medical decision-making, offering earlier detection and personalized treatment plans. Studies highlight the potential for AI to reduce human error and improve patient outcomes while addressing privacy and ethical concerns (Esteva et al., 2019; Topol, 2019; Jiang et al., 2017)."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2. Machine Learning and NLP Approaches",
            "text": "Various approaches have been proposed to assess textual features that are correlated with certain fairness aspects. We introduce readability metrics, bias metrics, and morality classifications. In the domain of AI in healthcare, predictive diagnostics leverage machine learning to anticipate disease risk, enabling personalized care. Recent studies emphasize the accuracy and potential of AI systems to predict conditions like diabetes and heart disease, illustrating a transformative shift towards proactive health management."
        },
        {
            "section_id": "2.2.1",
            "parent_section_id": "2.2",
            "section_name": "2.2.1. Readability Metrics",
            "text": "Related metrics exist on three levels. On the word level, foreign words, anglicisms, long words, abbreviations, complex words, etc., are problematic for non-native speakers (Boudjella et al., 2017 ###reference_b16###), elderly (Trzepla, 2019 ###reference_b85###), dyslexics (Rello and Baeza-Yates, 2017 ###reference_b68###) or autistic people (Evans et al., 2014 ###reference_b33###), or socioeconomically less-privileged (Aikens and Barbarin, 2008 ###reference_b5###). Lexical ambiguity of words can be assessed by comparing domain-specific meanings of words with their common meaning, e.g., by using BERT word embeddings with the Bradley-Terry statistical model (Liu et al., 2022 ###reference_b52###). On the document level, the Flesch Reading Ease (FRE) (Flesch, 1948 ###reference_b35###) is a commonly used metric for readability (Becher and Benoliel, 2021 ###reference_b12###). However, FRE does not capture readability holistically (Crossley et al., 2017 ###reference_b26###). Modern NLP-based approaches like TAASSC 2.0 (Kyle et al., 2021 ###reference_b49###) provide metrics on surface indices, syntactic features, and semantics. Comparing the consistency and logic of semantics between sentences allows coherence metrics, e.g., DiscoScore (Zhao et al., 2023 ###reference_b93###), which measure thematic focus frequency and sentence connectivity. Finally, metrics on the structural level consider that a better-structured text improves the understanding (Cocklin et al., 1984 ###reference_b22###). Assessing structure separately is recommended (Power et al., 2003 ###reference_b67###).\n\nAI in healthcare for predictive diagnostics is gaining momentum by analyzing vast datasets to forecast disease risks, enhancing early intervention strategies (Jha et al., 2021 ###reference_b48###). Models like deep learning are pivotal in identifying potential health threats by deciphering patterns in medical data (Esteva et al., 2019 ###reference_b32###), thus refining patient prognosis accuracy."
        },
        {
            "section_id": "2.2.2",
            "parent_section_id": "2.2",
            "section_name": "2.2.2. Bias Metrics",
            "text": "The identification of biases typically needs a list of descriptor terms. Such descriptor terms are word lists in specific demographic axes that capture the different groups that could be misrepresented. For example, HolisticBias (Smith et al., 2022  ###reference_b80###) includes 600 descriptor terms across 13 different demographic axes such as ability, age, body type, nationality, politics, etc. Descriptor terms can be generated automatically, e.g., for assessing gender bias on legal documents (Baker Gillis, 2021  ###reference_b9###). Assessing semantics has become the standard practice in literature for assessing representational fairness (Caliskan et al., 2017  ###reference_b18###; Garg et al., 2018  ###reference_b37###; Rice et al., 2019  ###reference_b70###; Baker Gillis, 2021  ###reference_b9###; Schr\u00f6der et al., 2021  ###reference_b76###; Gumusel et al., 2022  ###reference_b41###). To measure a bias, the text in question is transformed into word embeddings, i.e., a numerical, vectorized representation of the words used. Classical approaches measure cosine similarity between the descriptor terms, utilizing word embeddings to detect biases (Garg et al., 2018  ###reference_b37###; Gumusel et al., 2022  ###reference_b41###; Caliskan et al., 2017  ###reference_b18###; Schr\u00f6der et al., 2021  ###reference_b76###). A popular metric is WEAT (Caliskan et al., 2017  ###reference_b18###), improved in its robustness to SAME (Schr\u00f6der et al., 2021  ###reference_b76###). Other approaches are based on text features (e.g., using all-caps words or bold formatting) or semantic features (e.g., sarcasm or insinuation of dishonesty or deceitfulness). For example, see (Risch et al., 2021  ###reference_b71###), a solution for identifying toxic engaging, and fact-claiming comments. This, however, can be biased in itself and needs bias mitigation (Garg et al., 2023  ###reference_b38###).\n\nAI in healthcare for predictive diagnostics has shown significant potential, leveraging machine learning algorithms to analyze vast datasets for early disease detection (Esteva et al., 2017  ###reference_dx1###; Topol, 2019  ###reference_dx2###). These systems can enhance predictive accuracy by identifying patterns often indiscernible to human clinicians, thereby improving patient outcomes (Rajpurkar et al., 2019  ###reference_dx3###)."
        },
        {
            "section_id": "2.2.3",
            "parent_section_id": "2.2",
            "section_name": "2.2.3. Morality and Ethics Classification",
            "text": "Ethics might question the general morality of processes (Wallimann-Helmer et al., 2021  ###reference_b90###). A judgment could be derived following a set of principles / virtues like autonomy, nonmaleficence, beneficence, and justice (Beauchamp and Childress, 2001  ###reference_b11###). However, research on classifying morality or ethics focuses on moral stance prediction (Pavan et al., 2020  ###reference_b64###). Modern Large Language Models (LLM), like Mixtral-8x7B (Jiang et al., 2024  ###reference_b47###) or GPT4turbo (OpenAI, 2023  ###reference_b62###), show some cross-domain understanding (Hendrycks et al., 2020  ###reference_b44###), capabilities of reasoning (Bubeck et al., 2023  ###reference_b17###) and some understanding of the concepts of morality and ethics (Pock et al., 2023  ###reference_b65###). To evaluate ethics and morality with them, modern prompt engineering approaches (Nori et al., 2023  ###reference_b59###) are required. AI in healthcare for predictive diagnostics is advancing, with systems now able to identify patterns in large datasets, enabling early detection of diseases (Esteva et al., 2017  ###reference_b28###) and improving patient outcomes through personalized care strategies (Miotto et al., 2016  ###reference_b51###)."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "2.3. Evaluation of Privacy Policies",
            "text": "Fairness in privacy policies has been manually assessed in the mobile health domain (Benjumea et al., 2020  ###reference_b13###). A rather narrow definition of fairness based on items of the 14 content requirements stated in Article 13 of the GDPR is embraced. We are unaware of an automated approach to assessing fairness of privacy policies independent of the application domain. Research shows that privacy policies tend to be lengthy and use inaccessible language (Bartelt and Buchmann, 2024  ###reference_b10###; Becher and Benoliel, 2021  ###reference_b12###). For these reasons, approaches summarize or tag key aspects of privacy policies with NLP and machine learning (Harkous et al., 2018  ###reference_b43###; Nokhbeh Zaeem et al., 2020  ###reference_b58###; Tesfay et al., 2018b  ###reference_b83###). The completeness and compliance of privacy policies regarding the GDPR (Contissa et al., 2018  ###reference_b24###; Amaral et al., 2021  ###reference_b6###; Tesfay et al., 2018a  ###reference_b82###; S\u00e1nchez et al., 2021  ###reference_b73###; Vanezi et al., 2021  ###reference_b87###; Xiang et al., 2023  ###reference_b91###; Torre et al., 2020  ###reference_b84###; Elluri et al., 2021  ###reference_b31###) can be automatically assessed. Closest to our work is Claudette (Contissa et al., 2018  ###reference_b24###), which assesses GDPR compliance based on completeness of provided information, substantive compliance of a policy, and clarity of expression. Assessing fairness of policies goes beyond assessing their compliance with law. Our assessment of informational fairness is also concerned with completeness of provided information, assessing readability in our approach also tries to identify vague language but, further, addresses issues like readability for protected groups. Our approach assesses representational fairness, which Claudette does not cover, and ethics, which Claudette partially covers with substantive compliance. The most promising approach for completeness of a policy (Amaral et al., 2021  ###reference_b6###) first identifies informational requirements involving 56 metadata types relevant to the completeness of a policy. Based on that, it identifies 23 criteria to check for policies. The completeness assessment via Machine Learning (ML) and NLP is based on semantic similarity and word embeddings. Other approaches provide criteria or templates for more user-friendly and understandable policies (Renaud and Shepherd, 2018  ###reference_b69###; Feng et al., 2021  ###reference_b34###). To the best of our knowledge, representational fairness has not yet been assessed for privacy policies. However, representational fairness has been successfully assessed in loosely related fields of the legal domain (Rice et al., 2019  ###reference_b70###; Baker Gillis, 2021  ###reference_b9###; Gumusel et al., 2022  ###reference_b41###) or more broadly on textual data (Garg et al., 2018  ###reference_b37###; Caliskan et al., 2017  ###reference_b18###; Schr\u00f6der et al., 2021  ###reference_b76###). Assessing morality and ethics requires looking into what policies enforce. Judging ethics is a field that is typically addressed by ethics councils. Privacy policies have been found to obfuscate unethical data handling practices and use persuasive language (Pollach, 2005  ###reference_b66###). However, the criteria of the GDPR can be used as a first step to evaluate the risks of a privacy policy. For example, policies have been found to lack specificity regarding data use practices (Zaeem and Barber, 2020  ###reference_b92###) and lack protection of minors as well as a clear communication of changes made to their policy (Contissa et al., 2018  ###reference_b24###). A prominent approach to identifying risk is to use a Support Vector Machine to sentence- or paragraph-wise quantify the degree of policy compliance with the data protection goals (Contissa et al., 2018  ###reference_b24###; S\u00e1nchez et al., 2021  ###reference_b73###). Most approaches do not make their data and code available.\n\nIn the realm of AI in healthcare for predictive diagnostics, AI models have demonstrated enhanced predictive capabilities across various medical conditions. By analyzing complex medical datasets, AI can provide early diagnosis, which is crucial in treatment planning (Esteva et al., 2017  ###reference_b##; Rajpurkar et al., 2018  ###reference_b##). The integration of"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3. Problem Statement",
            "text": "In this section, we explore the legal foundation of fairness in privacy policies, we derive three dimensions of fairness, and we propose a working definition for fair privacy policies."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1. Legal Foundation",
            "text": "Equal treatment is a fundamental human right, as declared in anti-discrimination principles in the Universal Declaration of Human Rights (UDHR) (UN, 1948  ###reference_b86###), the European Convention on Human Rights (ECHR) (Council of Europe, 1950  ###reference_b25###), and respective regulations in national law, e.g., the US (Edenberg and Wood, 2023  ###reference_b30###).\nArt. 12 of the UDHR and Art. 8 of the ECHR emphasize the right to privacy. The Privacy Guidelines (OECD, 2013  ###reference_b61###) of the Organization for Economic Co-operation and Development (OECD) provide a framework for data privacy protection that is widely used as a reference for legislation. The guidelines require that \u201cdata should be obtained by lawful and fair means\u201d, and implicitly reference various fairness concepts to establish a balanced approach to privacy protection.\nThe OECD Privacy Guidelines have been widely adopted. For example, the 21 member states of the Asia-Pacific Economic Cooperative mirror the Guidelines (APEC, 2015  ###reference_b7###). In the USA and Canada, the OECD Guidelines are the basis of the Generally Accepted Privacy Principles (GAPP) (AICPA and CICA, 2006  ###reference_b4###). The GAPP are \u201cbased on internationally known fair information practices\u201d (AICPA and CICA, 2006  ###reference_b4###).\nIn the European Union, the GDPR (European Union, 2016  ###reference_b32###) implements these guidelines into EU law. Chap. 2 GDPR specifies the openness and participation principles, which address informational fairness. Chap. 5 GDPR requires \u201censuring that there is no unfair discrimination\u201d. Art. 5(1) GDPR enforces fairness as a key principle for privacy policies (Clifford and Ausloos, 2018  ###reference_b21###; Malgieri, 2020  ###reference_b53###)."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2. Dimensions of Fairness",
            "text": "Legal foundations motivate our three dimensions of fairness:\nRepresentational Fairness: Art. 1 UHDR states that equal treatment is a fundamental human right.\nArt. 2 UHDR and Art. 14 ECHR explicitly forbids \u201cdiscrimination on any ground\u201d.\nInformational Fairness: Art.\n13 and 14 GDPR enforce completeness of the information provided in a privacy policy on how it handles its users\u2019 data and what rights its users have.\nArt. 12(1) GDPR aims at making privacy policies readable by requesting \u201cconcise, transparent, intelligible and easily accessible form, using clear and plain language\u201d. Recital 39 requests \u201cany information and communication of the processing of those personal data to be easily accessible and easy to understand\u201d.\nEthics and Morality: Art. 5(1) point (a), 6(2)-(3) GDPR and Recital 71 states fairness as a fundamental principle, grounded in ethics. The GDPR strives to avoid potential harm to data subjects through safeguards, and represents procedural fairness as a central element of its fairness conception (Malgieri, 2020  ###reference_b53###). This means users should be protected from vulnerabilities and risk. Interests between the data subject and service provider should be fairly balanced (Clifford and Ausloos, 2018  ###reference_b21###; Malgieri, 2020  ###reference_b53###).\nIn summary, a privacy policy introduces (among other aspects) fairness into the interactions between the data controller and data subject. It cannot deliver fairness, if it is unfairly formulated. Therefore, our working definition of fairness in privacy policies is:\nA fair privacy policy complies with informational fairness and representational fairness, as well as ethics and morality.\nWe are open to considering more dimensions of fairness in the future.\nWe want to highlight that our three fairness dimensions are just motivated by the legal foundation and go beyond legal compliance.\nWe propose approaches to assess indicators for each dimension, we show the results of preliminary experiments, and we discuss how this brings forward fairness in privacy policies."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4. Informational Fairness",
            "text": "Our assessment of informational fairness borrows from the approaches described in Section 2.2.1  ###reference_.SSS1###."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1. Approach",
            "text": "Informational fairness in privacy policies is related to completeness of a privacy policy and comprehensibility and readability of vocabulary, sentences, and document structure for protected groups.\nCompleteness:\nTo get an estimate of the completeness of privacy policies, we propose to use supervised learning, as described in (Amaral et al., 2021  ###reference_b6###). We want to identify the relevant 56 metadata types and then assess the embedding space to judge completeness.\nFairness on word level:\nAnglicisms, complex words, tech jargon, etc. induce a socioeconomic, nationality, ableist, and age-related bias, causing unfairness due to discrimination.\nWe propose lexical filtering, either with an English language dictionary, a word-to-word translation library, or a customized dictionary, to detect tech jargon and anglicisms.\nA frequency-based dictionary (Sch\u00e4fer, 2015  ###reference_b78###; Sch\u00e4fer and Bildhauer, 2012  ###reference_b79###) allows us to estimate the proportion of words in a policy, that are not in common usage with a threshold-based approach.\nWe can estimate the use of complex or ambiguous words, when we use a translation service to translate words into another language and back, and measure the proportion of words that remain unchanged.\nHowever, the translation service might be affected by biases (Savoldi et al., 2021  ###reference_b74###) and linguistic limitations, e.g., when translating a gendered language to a gender-neutral one and back.\nWe also propose the Bradley-Terry statistical model with embeddings to assess lexical ambiguity (Liu et al., 2022  ###reference_b52###).\nFairness on sentence level:\nIncomprehensible policies lead to discrimination and unfairness.\nWe propose to estimate the readability of the sentences using statistical text metrics and existing linguistic measures.\nA straightforward text metric is the number of words in the policy. A higher number indicates less readable policies. The Flesch Reading Ease (FRE) (Flesch, 1948  ###reference_b35###) and the TAASSC 2.0 (Kyle et al., 2021  ###reference_b49###) are well-recognized linguistic metrics for readability.\nThe coherence of sentences can be assessed with the DiscoScore metric (Zhao et al., 2023  ###reference_b93###).\nInclusive document structure:\nAn inclusive policy structure mitigates socioeconomic, nationality, and ableist bias and improves fairness. Again, we propose statistical and linguistic measures. An inclusive policy is indicated by short section headings or lengths per paragraph, or a shallow split of the policy into sections (structural depth).\nTo assess how well the headings semantically fit with the whole section text of a policy, embeddings can be used (Kenter and De Rijke, 2015  ###reference_b48###; Han et al., 2021  ###reference_b42###)."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2. Preliminary Results",
            "text": "To provide evidence, that our work-in-progress indeed allows to assess informational fairness in privacy policies automatically, we have conducted a series of prototypical experiments regarding wording, sentences, and document structure. We leave experiments, that require a high implementation effort, to future work.\nAs a test case, we use the German Top-100 most visited web shops. Our data set contains the canonical form of 618 German privacy policies over eight years, starting in 2016. For acquisition, preprocessing, cleansing, and a description of the data set, see (Bartelt and Buchmann, 2024  ###reference_b10###). We conducted all experiments in German, as this is the language of the data set, but translated the results to English for this paper.\nConcerning fairness on word level, we exemplarily selected eight policies, one for each year in the data set, and conducted lexical filtering. A simple dictionary-based approach to filtering for potential anglicisms produces many false positives. Many German stop words in NLTK (Bird et al., 2009  ###reference_b14###) overlap with English words from SpaCy\u2019s (Honnibal and Montani, 2017  ###reference_b45###) English dictionary and can be removed. We then used a word-to-word translator (Choe et al., 2020  ###reference_b19###) with a smaller vocabulary than SpaCy or NLTK, to narrow down our list of anglicisms, and measured the length of this list. In our eight policies, we found 26 anglicisms on average, which impairs informational fairness by discriminating against demographics, where anglicisms are not commonly used.\nFor assessing fairness at sentence level, we used the entire data set of 618 policies.\nWe measured an average FRE of 37, with FRE=14 as the worst and FRE=88 as the best value. In comparison, an FRE 30 or below requires the reading competence of an academic. A fair, non-discriminating policy should read an FRE of 60 and above.\nTable 1  ###reference_### provides statistics on the document structure of all 618 policies.\nThe average policy is very long. A dyslexic, who needs 2 minutes for 250 words (Musch and R\u00f6sler, 2011  ###reference_b57###; Martelli et al., 2014  ###reference_b54###), would need to spend almost 165 minutes reading the longest policy. The average reading time per policy for an average reader (Musch and R\u00f6sler, 2011  ###reference_b57###) would be just under 20 minutes.\nOn average, a policy has 150 paragraphs with 32 words each. It contains 34 headings on 3 levels, with an average heading length of 5 words.\n92% of the policies make use of lists, 72% contain strong formatting, and use 15% italic. However, 6% of the policies are not structured by headings at all. This leaves a mixed impression regarding informational fairness in privacy policies."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "4.3. Discussion",
            "text": "Text statistics, FRE and, to some extent, formatting have already been measured for privacy policies aiming at transparency (Bartelt and Buchmann, 2024  ###reference_b10###).\nHowever, existing work did not use this information to assess key aspects of fairness.\nWhile informational fairness has previously been assessed for the predictions of machine learning models (Schoeffer et al., 2022  ###reference_b75###), privacy policies are a different problem. Privacy policies are intended to balance the information asymmetry between provider and user (completeness), and add the aspect of potential discrimination due to a lack of inclusiveness in the presentation of information.\nMost approaches to privacy policies from the related work (cf. Sec. 2.3  ###reference_###) only focus on completeness of information. Our preliminary results address different issues and allow for no comparison."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5. Representational Fairness",
            "text": "Our approach regarding representational fairness uses the methods outlined in Section 2.2.2  ###reference_.SSS2### to assess the ability-related, socioeconomic, age-related, political, nationality-related, gender-related, sexuality-related, and cultural social / demographic axes."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "5.1. Approach",
            "text": "Assessing representational fairness requires descriptor terms that capture various demographics (cf. Sec. 2  ###reference_###). Based on the descriptor terms, a quantitative and semantic assessment can be carried out.\nFairness descriptor terms:\nWe propose to carefully select existing descriptor terms, e.g., for gender (Muller et al., 2023  ###reference_b56###), or via translation service from other languages (Smith et al., 2022  ###reference_b80###). Finally, we propose to use a multilingual LLM to filter the resulting set, generate contextualized translations, or automatically generate policy-specific descriptor terms.\nQuantitative and semantic fairness assessment:\nThe descriptor terms enable us to estimate how often different groups are represented in a policy. Semantically, we propose to use the descriptor terms to assess the invariance of sentiment toward changed protected group membership by replacing existing entities with changed ones of varying protected groups.\nFinally, we propose to leverage the SAME metric (Schr\u00f6der et al., 2021  ###reference_b76###) for bias detection.\nWe want to adapt approaches for toxicity detection (Risch et al., 2021  ###reference_b71###), and debiasing the detection itself (Garg et al., 2023  ###reference_b38###)."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "5.2. Preliminary Results",
            "text": "Again, our test case is based on German language privacy policies from the Top-100 German web shops. With this series of experiments, we evaluated the entire data set of 618 policies.\nWe translated the descriptor terms from HolisticBias (Smith et al., 2022  ###reference_b80###) to German with deep-translator (Baccouri, 2020  ###reference_b8###) utilizing the Google Translator API, and we manually removed homonyms and other artifacts from the translation.\nWe appended terms from the categories \u201cnationality\u201d and \u201cpolitical\u201d that are specific for a German context, and we integrated German gender descriptor terms (Muller et al., 2023  ###reference_b56###). We perform a quantitative assessment.\nWe found that the policies do not take into account reading-impairing disabilities. No specific disability from our descriptor terms is mentioned even once. Some policies address the reader informally. We find terminology that is typically used to address youth readers mentioned 13325 times overall.\nThe vocabulary is rather gender-neutral (13 male occurrences, 130 female occurrences, 6660 gender-neutral occurrences) based on the German gender descriptor terms. However, policies lack appropriate gendering. For instance, we found the German word for \u201duser\u201d 3057 times not gendered.\nThis could be problematic for the inclusiveness of poor readers, age-related demographics, women, or people who identify as non-binary.\nWe observed difficulties when assessing age-related representational bias, because \u201cold\u201d and similar descriptor terms are frequently but differently used in privacy policies. This needs a refinement of the word list and assessing the semantic space. The quantitative assessment suggests a nationality bias toward the US, Germany, and generally European countries. This also needs to be checked by assessing semantics, as it may just be related to information about the stakeholders\u2019 locations. We could not find quantitative representational bias regarding political, cultural, or sexuality-related demographics."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "5.3. Discussion",
            "text": "To the best of our knowledge, representational fairness has not yet been assessed for privacy policies. We have only investigated quantitatively as a first step. For more nuanced and accurate findings, we want to assess semantics as a next step. This addresses false positives due to homonyms and has been used effectively in recent research (cf. Sec.  2.2.2  ###reference_.SSS2###)."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6. Ethics and Morality",
            "text": "This assessment uses approaches from Section 2.2.3  ###reference_.SSS3### to screen the processes, rights, and obligations declared in a privacy policy."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "6.1. Approach",
            "text": "We propose an LLM to assess to which vulnerabilities or risks a policy exposes a data subject, how proportional those are, and which general ethics issues exist.\nVulnerabilities and Proportionality:\nWe propose to train a specific vulnerability classifier, similar to (S\u00e1nchez et al., 2021  ###reference_b73###). Based on that, the proportionality of a policy depends on the business activities of the data holder. The assessment of proportionality requires weighing the difficult-to-quantify reasons for a case-specific process against its induced vulnerabilities. This raises challenges for typical ML classification models, so we suspect an ethicist\u2019s judgment to be superior (cf. Section 2  ###reference_###). That\u2019s why we propose utilizing an LLM as a new approach to be tested. We use the classification result of the vulnerability classifier to prompt the LLM.\nGeneral Ethics:\nWe also suggest testing general ethics issues with an LLM, using a broad framing of the prompt.\nBecause this is an explorative approach, we propose to request the LLM to state the criteria of its assessment first, and to quantify them on a five-point Likert scale in a subsequent step.\nWe know from prompt engineering (Nori et al., 2023  ###reference_b59###) that this can be achieved by using the entire policy as context and a task description in the prompt, and by asking the LLM to reflect its result.\nAs a supplement, we suggest approaches for sentiment analysis (Guhr et al., 2020  ###reference_b40###) on the LLM outputs, to get an approximate overall proxy for the ethics of a policy."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "6.2. Preliminary Results",
            "text": "Again, we use the German language policies from the German Top-100 web shops.\nFor the time being, we focus on general morality and leave aside the other aspects.\nWe created a prompt with a broad scope (see Figure 1  ###reference_###), and tested it with GPT4turbo (OpenAI, 2023  ###reference_b62###) on eight policies, one for each year in the data set. We asked the LLM not only to make a qualitative assessment, but also to rate it on a five-point Likert scale. Due to the lack of space, we only report the Likert ratings (Table 2  ###reference_###).\nAs the first column of Table 2  ###reference_### shows, we can aggregate the LLM assessments into 12 distinct criteria for the ethics of privacy policies.\nAs desired, the criteria capture general ethical requirements on a broad scope and go beyond the GDPR, e.g., regarding the protection of minors.\nAn LLM answers each prompt with a new internal seed, comparable to an expert asked to make an assessment from a different point of view.\nFor this reason, we let the LLM evaluate each prompt in five runs. The second and third columns of Table 2  ###reference_### show in how many runs and policies the criteria appear. Some criteria just appeared in one of the runs for a policy.\nWhile integrating a fixed set of criteria into the prompt would enable a standardized assessment, running the LLM multiple times results in different aspects being considered, which has some merit for exploration.\nThe fourth column shows the Likert-scores between 1 (worst) and 5 (best). Our most alarming findings are the usage of data, the (international) transmission, and sharing of data, as well as communication and changes.\nSystem prompt: You are a critically scrutinizing, experienced expert in an ethics council on issues around fairness and justice and are committed to consumer protection for fairness. You have 20 years of experience in ethical issues around consumer protection, data protection, cybersecurity and related fairness aspects. You have already provided precise expert opinions in many court proceedings.\nYour task is to uncover aspects in privacy policies that are ethically questionable. You must clearly explain your thoughts and how you came to your conclusion. Explain step by step! Also disclose the ethical and moral principles on which you base your judgment. In particular, point out potential for improvement or risks to users.\nIMPORTANT: Also rate the privacy policy for each of your criteria on a 5-point Likert scale. Check whether the following privacy policy is fair and ethical towards its users.\nThe user wants a critical review and an assessment of vulnerabilities in privacy policies.\nUser prompt: The privacy policy: [Privacy Policy]"
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "6.3. Discussion",
            "text": "Existing work has not automated ethics assessment so far, i.e., an LLM for ethics evaluation of privacy policies is a novel approach. Our findings are in line with related work (cf. Sec.  2.3  ###reference_###) regarding problematic data use.\nWe found the GDPR principles, which are hard ethics, represented in the ethics criteria, which we identified automatically with GPT4turbo.\nAs we exemplarily investigated eight policies with a multilingual LLM, our list of criteria is just a starting point. There may be further criteria worth considering by utilizing a German LLM, more sophisticated prompting and a large and diverse set of policies."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7. Use Cases and Applications",
            "text": "In this section, we explain why our approach has the potential to improve the fairness of privacy policies. In particular, we see two prominent use cases in the areas of analyses and writing support.\nOur approach enables researchers to automatically scan a large corpus of privacy policies for such issues. This allows to obtain an overview of how widespread which kind of fairness issue is in privacy policies. Due to the ties between fairness and the GDPR, this might also help authorities to single out privacy policies that are problematic from a legal point of view. Our data set allows to analyze how such issues have developed or subsided over time. This provides an opportunity for researchers to investigate whether changes in legislation or major privacy events have led to fairer or less fair policies.\nWriting a fair policy is challenging. In the case of AI applications, for example, it is difficult even for experts to assess the personal risks associated with the use of personal data. Our approach allows to automatically identify aspects, that are associated with various kinds of bias, discrimination, etc. It might be useful to integrate this either into existing generators for privacy policies or to develop a stand-alone tool, to support data protection officers in writing complex, but fair privacy policies.\nAlso, our work can be used as a foundation for data assessment for legal NLP models trained on privacy policies."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "8. Conclusion",
            "text": "Assessing privacy policies regarding their fairness is an important, but yet unresolved issue.\nIn this paper, we suggest three dimensions of fairness that should be investigated: informational fairness, representational fairness, and ethics and morality. We base our problem understanding in literature on fairness, privacy policies, and fairness issues in legal text processing. Furthermore, we propose assessment procedures for all three fairness dimensions, utilizing NLP and linguistic analysis.\nWe implemented and tested a small subset of those procedures, with promising results. Our next step is to implement a holistic system of assessments, as suggested in our approach. Beyond that, we seek a close collaboration with ethical and legal experts to identify further relevant criteria that can be used to iterate, find out what is needed for a more complete assessment procedure, and how our method could be used by ethicists and jurists as tool support.\nOur contribution is important as we advance the understanding of fairness in privacy policies. Our approach may help in providing data subjects with some much-needed transparency and in avoiding discrimination or unethical practices."
        }
    ],
    "appendix": [],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T1\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1. </span> Overview of surface measures</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.1\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r\" id=\"S4.T1.1.1.1.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T1.1.1.1.1.1\">\n<span class=\"ltx_p\" id=\"S4.T1.1.1.1.1.1.1\" style=\"width:79.7pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.1.1.1.1.1\">Measure</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column\" id=\"S4.T1.1.1.1.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T1.1.1.1.2.1\">\n<span class=\"ltx_p\" id=\"S4.T1.1.1.1.2.1.1\" style=\"width:108.1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.1.2.1.1.1\">Average or Value</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T1.1.2.1\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T1.1.2.1.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T1.1.2.1.1.1\">\n<span class=\"ltx_p\" id=\"S4.T1.1.2.1.1.1.1\" style=\"width:79.7pt;\">words / policy</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S4.T1.1.2.1.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T1.1.2.1.2.1\">\n<span class=\"ltx_p\" id=\"S4.T1.1.2.1.2.1.1\" style=\"width:108.1pt;\">4809.59</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.3.2\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T1.1.3.2.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T1.1.3.2.1.1\">\n<span class=\"ltx_p\" id=\"S4.T1.1.3.2.1.1.1\" style=\"width:79.7pt;\">paragraphs / policy</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S4.T1.1.3.2.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T1.1.3.2.2.1\">\n<span class=\"ltx_p\" id=\"S4.T1.1.3.2.2.1.1\" style=\"width:108.1pt;\">149.63</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.4.3\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T1.1.4.3.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T1.1.4.3.1.1\">\n<span class=\"ltx_p\" id=\"S4.T1.1.4.3.1.1.1\" style=\"width:79.7pt;\">words / paragraph</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S4.T1.1.4.3.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T1.1.4.3.2.1\">\n<span class=\"ltx_p\" id=\"S4.T1.1.4.3.2.1.1\" style=\"width:108.1pt;\">32.14</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.5.4\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T1.1.5.4.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T1.1.5.4.1.1\">\n<span class=\"ltx_p\" id=\"S4.T1.1.5.4.1.1.1\" style=\"width:79.7pt;\">headings / policy</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S4.T1.1.5.4.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T1.1.5.4.2.1\">\n<span class=\"ltx_p\" id=\"S4.T1.1.5.4.2.1.1\" style=\"width:108.1pt;\">33.82 (6.15% have no heading)</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.6.5\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T1.1.6.5.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T1.1.6.5.1.1\">\n<span class=\"ltx_p\" id=\"S4.T1.1.6.5.1.1.1\" style=\"width:79.7pt;\">heading types</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S4.T1.1.6.5.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T1.1.6.5.2.1\">\n<span class=\"ltx_p\" id=\"S4.T1.1.6.5.2.1.1\" style=\"width:108.1pt;\">2.85</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.7.6\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T1.1.7.6.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T1.1.7.6.1.1\">\n<span class=\"ltx_p\" id=\"S4.T1.1.7.6.1.1.1\" style=\"width:79.7pt;\">words / heading</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S4.T1.1.7.6.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T1.1.7.6.2.1\">\n<span class=\"ltx_p\" id=\"S4.T1.1.7.6.2.1.1\" style=\"width:108.1pt;\">4.83</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.8.7\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T1.1.8.7.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T1.1.8.7.1.1\">\n<span class=\"ltx_p\" id=\"S4.T1.1.8.7.1.1.1\" style=\"width:79.7pt;\">lists</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S4.T1.1.8.7.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T1.1.8.7.2.1\">\n<span class=\"ltx_p\" id=\"S4.T1.1.8.7.2.1.1\" style=\"width:108.1pt;\">91.59 % of policies</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.9.8\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T1.1.9.8.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T1.1.9.8.1.1\">\n<span class=\"ltx_p\" id=\"S4.T1.1.9.8.1.1.1\" style=\"width:79.7pt;\">other text formatting</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S4.T1.1.9.8.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T1.1.9.8.2.1\">\n<span class=\"ltx_p\" id=\"S4.T1.1.9.8.2.1.1\" style=\"width:108.1pt;\">strong: 71.52 % of policies</span>\n<span class=\"ltx_p\" id=\"S4.T1.1.9.8.2.1.2\">italics: 15.37 % of policies</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 1.  Overview of surface measures"
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S6.T2\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2. </span> Overview of LLM ethics assessments</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S6.T2.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S6.T2.1.1.1\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r\" id=\"S6.T2.1.1.1.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.1.1.1.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.1.1.1.1.1\" style=\"width:116.7pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.1.1.1.1.1.1.1\">Criteria</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r\" id=\"S6.T2.1.1.1.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.1.1.2.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.1.1.2.1.1\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.1.1.1.2.1.1.1\">#</span></span>\n<span class=\"ltx_p\" id=\"S6.T2.1.1.1.2.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.1.1.1.2.1.2.1\">Runs</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r\" id=\"S6.T2.1.1.1.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.1.1.3.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.1.1.3.1.1\" style=\"width:28.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.1.1.1.3.1.1.1\">#</span></span>\n<span class=\"ltx_p\" id=\"S6.T2.1.1.1.3.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.1.1.1.3.1.2.1\">Policies</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column\" id=\"S6.T2.1.1.1.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.1.1.4.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.1.1.4.1.1\" style=\"width:25.6pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.1.1.1.4.1.1.1\">Average</span></span>\n<span class=\"ltx_p\" id=\"S6.T2.1.1.1.4.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.1.1.1.4.1.2.1\">Score</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S6.T2.1.2.1\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S6.T2.1.2.1.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.2.1.1.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.2.1.1.1.1\" style=\"width:116.7pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.1.2.1.1.1.1.1\">transparency</span> (precise, complete, comprehensible)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S6.T2.1.2.1.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.2.1.2.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.2.1.2.1.1\" style=\"width:22.8pt;\">38</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S6.T2.1.2.1.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.2.1.3.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.2.1.3.1.1\" style=\"width:28.5pt;\">8</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S6.T2.1.2.1.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.2.1.4.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.2.1.4.1.1\" style=\"width:25.6pt;\">3.20</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.1.3.2\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S6.T2.1.3.2.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.3.2.1.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.3.2.1.1.1\" style=\"width:116.7pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.1.3.2.1.1.1.1\">data subject control and autonomy</span> (user rights, freedom of choice)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S6.T2.1.3.2.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.3.2.2.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.3.2.2.1.1\" style=\"width:22.8pt;\">39</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S6.T2.1.3.2.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.3.2.3.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.3.2.3.1.1\" style=\"width:28.5pt;\">8</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S6.T2.1.3.2.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.3.2.4.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.3.2.4.1.1\" style=\"width:25.6pt;\">3.35</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.1.4.3\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S6.T2.1.4.3.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.4.3.1.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.4.3.1.1.1\" style=\"width:116.7pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.1.4.3.1.1.1.1\">data minimalization and purpose binding</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S6.T2.1.4.3.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.4.3.2.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.4.3.2.1.1\" style=\"width:22.8pt;\">26</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S6.T2.1.4.3.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.4.3.3.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.4.3.3.1.1\" style=\"width:28.5pt;\">8</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S6.T2.1.4.3.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.4.3.4.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.4.3.4.1.1\" style=\"width:25.6pt;\">3.06</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.1.5.4\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S6.T2.1.5.4.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.5.4.1.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.5.4.1.1.1\" style=\"width:116.7pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.1.5.4.1.1.1.1\">data usage</span> (surveillance concerns, automated decisions)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S6.T2.1.5.4.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.5.4.2.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.5.4.2.1.1\" style=\"width:22.8pt;\">21</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S6.T2.1.5.4.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.5.4.3.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.5.4.3.1.1\" style=\"width:28.5pt;\">8</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S6.T2.1.5.4.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.5.4.4.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.5.4.4.1.1\" style=\"width:25.6pt;\">2.71</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.1.6.5\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S6.T2.1.6.5.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.6.5.1.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.6.5.1.1.1\" style=\"width:116.7pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.1.6.5.1.1.1.1\">data storage and deletion</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S6.T2.1.6.5.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.6.5.2.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.6.5.2.1.1\" style=\"width:22.8pt;\">8</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S6.T2.1.6.5.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.6.5.3.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.6.5.3.1.1\" style=\"width:28.5pt;\">4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S6.T2.1.6.5.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.6.5.4.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.6.5.4.1.1\" style=\"width:25.6pt;\">3.38</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.1.7.6\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S6.T2.1.7.6.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.7.6.1.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.7.6.1.1.1\" style=\"width:116.7pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.1.7.6.1.1.1.1\">data protection and security</span> (avoiding misuse or leakage)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S6.T2.1.7.6.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.7.6.2.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.7.6.2.1.1\" style=\"width:22.8pt;\">30</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S6.T2.1.7.6.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.7.6.3.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.7.6.3.1.1\" style=\"width:28.5pt;\">8</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S6.T2.1.7.6.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.7.6.4.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.7.6.4.1.1\" style=\"width:25.6pt;\">3.38</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.1.8.7\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S6.T2.1.8.7.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.8.7.1.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.8.7.1.1.1\" style=\"width:116.7pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.1.8.7.1.1.1.1\">(international) data</span></span>\n<span class=\"ltx_p\" id=\"S6.T2.1.8.7.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.1.8.7.1.1.2.1\">transmission and sharing</span> (third-parties, where is what transferred)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S6.T2.1.8.7.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.8.7.2.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.8.7.2.1.1\" style=\"width:22.8pt;\">27</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S6.T2.1.8.7.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.8.7.3.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.8.7.3.1.1\" style=\"width:28.5pt;\">8</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S6.T2.1.8.7.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.8.7.4.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.8.7.4.1.1\" style=\"width:25.6pt;\">2.74</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.1.9.8\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S6.T2.1.9.8.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.9.8.1.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.9.8.1.1.1\" style=\"width:116.7pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.1.9.8.1.1.1.1\">compliance with data protection regulation</span> (GDPR, relevant countries data protection regulation and standards)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S6.T2.1.9.8.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.9.8.2.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.9.8.2.1.1\" style=\"width:22.8pt;\">9</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S6.T2.1.9.8.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.9.8.3.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.9.8.3.1.1\" style=\"width:28.5pt;\">7</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S6.T2.1.9.8.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.9.8.4.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.9.8.4.1.1\" style=\"width:25.6pt;\">3.56</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.1.10.9\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S6.T2.1.10.9.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.10.9.1.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.10.9.1.1.1\" style=\"width:116.7pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.1.10.9.1.1.1.1\">communication and changes</span> (timeliness, changes, risks)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S6.T2.1.10.9.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.10.9.2.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.10.9.2.1.1\" style=\"width:22.8pt;\">8</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S6.T2.1.10.9.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.10.9.3.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.10.9.3.1.1\" style=\"width:28.5pt;\">5</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S6.T2.1.10.9.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.10.9.4.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.10.9.4.1.1\" style=\"width:25.6pt;\">2.63</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.1.11.10\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S6.T2.1.11.10.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.11.10.1.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.11.10.1.1.1\" style=\"width:116.7pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.1.11.10.1.1.1.1\">protection of minors</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S6.T2.1.11.10.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.11.10.2.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.11.10.2.1.1\" style=\"width:22.8pt;\">4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S6.T2.1.11.10.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.11.10.3.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.11.10.3.1.1\" style=\"width:28.5pt;\">2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S6.T2.1.11.10.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.11.10.4.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.11.10.4.1.1\" style=\"width:25.6pt;\">3.50</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.1.12.11\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S6.T2.1.12.11.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.12.11.1.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.12.11.1.1.1\" style=\"width:116.7pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.1.12.11.1.1.1.1\">fairness</span> (no discrimination, fair use)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S6.T2.1.12.11.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.12.11.2.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.12.11.2.1.1\" style=\"width:22.8pt;\">7</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S6.T2.1.12.11.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.12.11.3.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.12.11.3.1.1\" style=\"width:28.5pt;\">4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S6.T2.1.12.11.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.12.11.4.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.12.11.4.1.1\" style=\"width:25.6pt;\">3.14</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.1.13.12\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S6.T2.1.13.12.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.13.12.1.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.13.12.1.1.1\" style=\"width:116.7pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.1.13.12.1.1.1.1\">assurance, accountability and governance</span> (compliance, reliability, assurance, ability\nto react)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S6.T2.1.13.12.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.13.12.2.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.13.12.2.1.1\" style=\"width:22.8pt;\">9</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S6.T2.1.13.12.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.13.12.3.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.13.12.3.1.1\" style=\"width:28.5pt;\">6</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S6.T2.1.13.12.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S6.T2.1.13.12.4.1\">\n<span class=\"ltx_p\" id=\"S6.T2.1.13.12.4.1.1\" style=\"width:25.6pt;\">3.22</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 2.  Overview of LLM ethics assessments"
        }
    },
    "image_paths": {},
    "references": [
        {
            "1": {
                "title": "Fairness in representation: quantifying stereotyping as a representational harm. In Proceedings of the 2019 SIAM International Conference on Data Mining. SIAM, 801\u2013809.",
                "author": "Mohsen Abbasi, Sorelle A Friedler, Carlos Scheidegger, and Suresh Venkatasubramanian. 2019.",
                "venue": "",
                "url": null
            }
        },
        {
            "2": {
                "title": "Privacy and human behavior in the age of information.",
                "author": "Alessandro Acquisti, Laura Brandimarte, and George Loewenstein. 2015.",
                "venue": "Science 347, 6221 (2015), 509\u2013514.",
                "url": null
            }
        },
        {
            "3": {
                "title": "Generally Accepted Privacy Principles.",
                "author": "AICPA and CICA. 2006.",
                "venue": "Number 0001069 in Reports. The American Institute of Certified Public Accountants, Inc. and The Canadian Institute of Chartered Accountants.",
                "url": null
            }
        },
        {
            "4": {
                "title": "Socioeconomic differences in reading trajectories: The contribution of family, neighborhood, and school contexts.",
                "author": "Nikki L Aikens and Oscar Barbarin. 2008.",
                "venue": "Journal of educational psychology 100, 2 (2008), 235.",
                "url": null
            }
        },
        {
            "5": {
                "title": "AI-Enabled Automation for Completeness Checking of Privacy Policies.",
                "author": "Orlando Amaral, Sallam Abualhaija, Damiano Torre, Mehrdad Sabetzadeh, and Lionel C Briand. 2021.",
                "venue": "IEEE Transactions on Software Engineering 48, 11 (2021), 4647\u20134674.",
                "url": null
            }
        },
        {
            "6": {
                "title": "Asia-Pacific Economic Cooperation Privacy Framework.",
                "author": "APEC. 2015.",
                "venue": "",
                "url": null
            }
        },
        {
            "7": {
                "title": "deep-translator: A Python library for language translation.",
                "author": "Nidhal Baccouri. 2020.",
                "venue": "https://deep-translator.readthedocs.io/en/latest/README.html.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Sexism in the Judiciary: The Importance of Bias Definition in NLP and In Our Courts. In Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing. 45\u201354.",
                "author": "Noa Baker Gillis. 2021.",
                "venue": "",
                "url": null
            }
        },
        {
            "9": {
                "title": "Transparency in Privacy Policies. In 12th International Conference on Building and Exploring Web Based Environments (to appear).",
                "author": "Bianca Bartelt and Erik Buchmann. 2024.",
                "venue": "",
                "url": null
            }
        },
        {
            "10": {
                "title": "Principles of Biomedical Ethics.",
                "author": "Tom L Beauchamp and James F Childress. 2001.",
                "venue": "Oxford University Press, USA.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Law in Books and Law in Action: The Readability of Privacy Policies and the GDPR. In Consumer law and economics. Springer, 179\u2013204.",
                "author": "Shmuel I Becher and Uri Benoliel. 2021.",
                "venue": "",
                "url": null
            }
        },
        {
            "12": {
                "title": "Assessment of the fairness of privacy policies of mobile health apps: scale development and evaluation in cancer apps.",
                "author": "Jaime Benjumea, Jorge Ropero, Octavio Rivera-Romero, Enrique Dorronzoro-Zubiete, Alejandro Carrasco, et al. 2020.",
                "venue": "JMIR mHealth and uHealth 8, 7 (2020), e17134.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Natural language processing with Python: analyzing text with the natural language toolkit.",
                "author": "Steven Bird, Ewan Klein, and Edward Loper. 2009.",
                "venue": "\u201d O\u2019Reilly Media, Inc.\u201d.",
                "url": null
            }
        },
        {
            "14": {
                "title": "Language (Technology) is Power: A Critical Survey of \u201cBias\u201d in NLP. In Proceedings of the 58th annual meeting of the association for computational linguistics. 5454\u20135476.",
                "author": "Su Lin Blodgett, Solon Barocas, Hal Daum\u00e9 III, and Hanna Wallach. 2020.",
                "venue": "",
                "url": null
            }
        },
        {
            "15": {
                "title": "Non-Native English Speaker Readability Metric: Reading Speed and Comprehension.",
                "author": "Aissa Boudjella, Mukti Sharma, and Deepti Sharma. 2017.",
                "venue": "Journal of Applied Mathematics and Physics 5, 6 (2017), 1257\u20131268.",
                "url": null
            }
        },
        {
            "16": {
                "title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4.",
                "author": "S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023.",
                "venue": "arXiv preprint arXiv:2303.12712 (2023).",
                "url": null
            }
        },
        {
            "17": {
                "title": "Semantics derived automatically from language corpora contain human-like biases.",
                "author": "Aylin Caliskan, Joanna J Bryson, and Arvind Narayanan. 2017.",
                "venue": "Science 356, 6334 (2017), 183\u2013186.",
                "url": null
            }
        },
        {
            "18": {
                "title": "word2word: A Collection of Bilingual Lexicons for 3,564 Language Pairs. In Proceedings of the 12th International Conference on Language Resources and Evaluation (LREC 2020).",
                "author": "Yo Joong Choe, Kyubyong Park, and Dongwoo Kim. 2020.",
                "venue": "",
                "url": null
            }
        },
        {
            "19": {
                "title": "Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments.",
                "author": "Alexandra Chouldechova. 2017.",
                "venue": "Big Data 5, 2 (2017), 153\u2013163.",
                "url": null
            }
        },
        {
            "20": {
                "title": "Data protection and the role of fairness.",
                "author": "Damian Clifford and Jef Ausloos. 2018.",
                "venue": "Yearbook of European Law 37 (2018), 130\u2013187.",
                "url": null
            }
        },
        {
            "21": {
                "title": "Factors influencing readability of rapidly presented text segments.",
                "author": "Thomas G Cocklin, Nicklas J Ward, Hsuan-Chih Chen, and James F Juola. 1984.",
                "venue": "Memory & Cognition 12 (1984), 431\u2013442.",
                "url": null
            }
        },
        {
            "22": {
                "title": "187Measuring Justice and Fairness.",
                "author": "Jason A. Colquitt and Jessica B. Rodell. 2015.",
                "venue": "In The Oxford Handbook of Justice in the Workplace. Oxford University Press.",
                "url": null
            }
        },
        {
            "23": {
                "title": "Claudette meets gdpr: Automating the evaluation of privacy policies using artificial intelligence.",
                "author": "Giuseppe Contissa, Koen Docter, Francesca Lagioia, Marco Lippi, Hans-W Micklitz, Przemys\u0142aw Pa\u0142ka, Giovanni Sartor, and Paolo Torroni. 2018.",
                "venue": "Available at SSRN 3208596 (2018).",
                "url": null
            }
        },
        {
            "24": {
                "title": "European Convention for the Protection of Human Rights and Fundamental Freedoms.",
                "author": "Council of Europe. 1950.",
                "venue": "as amended by Protocols Nos. 11 and 14, 4 November.",
                "url": null
            }
        },
        {
            "25": {
                "title": "Predicting Text Comprehension, Processing, and Familiarity in Adult Readers: New Approaches to Readability Formulas.",
                "author": "Scott A Crossley, Stephen Skalicky, Mihai Dascalu, Danielle S McNamara, and Kristopher Kyle. 2017.",
                "venue": "Discourse Processes 54, 5-6 (2017), 340\u2013359.",
                "url": null
            }
        },
        {
            "26": {
                "title": "The scope of privacy in law and ethics.",
                "author": "Judith Wagner DeCew. 1986.",
                "venue": "Law and Philosophy (1986), 145\u2013173.",
                "url": null
            }
        },
        {
            "27": {
                "title": "Maximal Fairness. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency. 851\u2013880.",
                "author": "Marybeth Defrance and Tijl De Bie. 2023.",
                "venue": "",
                "url": null
            }
        },
        {
            "28": {
                "title": "When Do the Ends Justify the Means? Evaluating Procedural Fairness.",
                "author": "David Doherty and Jennifer Wolak. 2012.",
                "venue": "Political Behavior 34 (2012), 301\u2013323.",
                "url": null
            }
        },
        {
            "29": {
                "title": "Disambiguating Algorithmic Bias: From Neutrality to Justice. In Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society. 691\u2013704.",
                "author": "Elizabeth Edenberg and Alexandra Wood. 2023.",
                "venue": "",
                "url": null
            }
        },
        {
            "30": {
                "title": "A bert based approach to measure web services policies compliance with gdpr.",
                "author": "Lavanya Elluri, Sai Sree Laya Chukkapalli, Karuna Pande Joshi, Tim Finin, and Anupam Joshi. 2021.",
                "venue": "IEEE Access 9 (2021), 148004\u2013148016.",
                "url": null
            }
        },
        {
            "31": {
                "title": "REGULATION (EU) 2016/679 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation).",
                "author": "European Union. 2016.",
                "venue": "Official Journal of the European Union L119/1 (2016).",
                "url": null
            }
        },
        {
            "32": {
                "title": "An evaluation of syntactic simplification rules for people with autism. In Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR). 131\u2013140.",
                "author": "Richard Evans, Constantin Or\u0103san, and Iustin Dornescu. 2014.",
                "venue": "",
                "url": null
            }
        },
        {
            "33": {
                "title": "A design space for privacy choices: Towards meaningful privacy control in the internet of things. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1\u201316.",
                "author": "Yuanyuan Feng, Yaxing Yao, and Norman Sadeh. 2021.",
                "venue": "",
                "url": null
            }
        },
        {
            "34": {
                "title": "A new readability yardstick.",
                "author": "Rudolph Flesch. 1948.",
                "venue": "Journal of applied psychology 32, 3 (1948), 221.",
                "url": null
            }
        },
        {
            "35": {
                "title": "Soft ethics, the governance of the digital and the General Data Protection Regulation.",
                "author": "Luciano Floridi. 2018.",
                "venue": "Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 376, 2133 (2018), 20180081.",
                "url": null
            }
        },
        {
            "36": {
                "title": "Word embeddings quantify 100 years of gender and ethnic stereotypes.",
                "author": "Nikhil Garg, Londa Schiebinger, Dan Jurafsky, and James Zou. 2018.",
                "venue": "Proceedings of the National Academy of Sciences 115, 16 (2018), E3635\u2013E3644.",
                "url": null
            }
        },
        {
            "37": {
                "title": "Handling Bias in Toxic Speech Detection: A Survey.",
                "author": "Tanmay Garg, Sarah Masud, Tharun Suresh, and Tanmoy Chakraborty. 2023.",
                "venue": "Comput. Surveys 55, 13s (2023), 1\u201332.",
                "url": null
            }
        },
        {
            "38": {
                "title": "Organizational Justice: Yesterday, Today, and Tomorrow.",
                "author": "Jerald Greenberg. 1990.",
                "venue": "Journal of management 16, 2 (1990), 399\u2013432.",
                "url": null
            }
        },
        {
            "39": {
                "title": "Training a Broad-Coverage German Sentiment Classification Model for Dialog Systems. In Proceedings of The 12th Language Resources and Evaluation Conference. 1620\u20131625.",
                "author": "Oliver Guhr, Anne-Kathrin Schumann, Frank Bahrmann, and Hans Joachim B\u00f6hme. 2020.",
                "venue": "",
                "url": null
            }
        },
        {
            "40": {
                "title": "An Annotation Schema for the Detection of Social Bias in Legal Text Corpora. In Information for a Better World: Shaping the Global Future, Malte Smits (Ed.). Springer International Publishing, 185\u2013194.",
                "author": "Ece Gumusel, Vincent Quirante Malic, Devan Ray Donaldson, Kevin Ashley, and Xiaozhong Liu. 2022.",
                "venue": "",
                "url": null
            }
        },
        {
            "41": {
                "title": "A survey on the techniques, applications, and performance of short text semantic similarity.",
                "author": "Mengting Han, Xuan Zhang, Xin Yuan, Jiahao Jiang, Wei Yun, and Chen Gao. 2021.",
                "venue": "Concurrency and Computation: Practice and Experience 33, 5 (2021), e5971.",
                "url": null
            }
        },
        {
            "42": {
                "title": "Polisis: Automated analysis and presentation of privacy policies using deep learning. In 27th USENIX Security Symposium (USENIX Security 18). 531\u2013548.",
                "author": "Hamza Harkous, Kassem Fawaz, R\u00e9mi Lebret, Florian Schaub, Kang G Shin, and Karl Aberer. 2018.",
                "venue": "",
                "url": null
            }
        },
        {
            "43": {
                "title": "Measuring Massive Multitask Language Understanding.",
                "author": "Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020.",
                "venue": "arXiv preprint arXiv:2009.03300 (2020).",
                "url": null
            }
        },
        {
            "44": {
                "title": "spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing. (2017).",
                "author": "Matthew Honnibal and Ines Montani. 2017.",
                "venue": "",
                "url": null
            }
        },
        {
            "45": {
                "title": "Fairness.",
                "author": "Brad Hooker. 2005.",
                "venue": "Ethical Theory and Moral Practice 8, 4 (08 2005), 329\u2013352.",
                "url": null
            }
        },
        {
            "46": {
                "title": "Mixtral of Experts.",
                "author": "Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. 2024.",
                "venue": "arXiv preprint arXiv:2401.04088 (2024).",
                "url": null
            }
        },
        {
            "47": {
                "title": "Short Text Similarity with Word Embeddings. In Proceedings of the 24th ACM international on conference on information and knowledge management. 1411\u20131420.",
                "author": "Tom Kenter and Maarten De Rijke. 2015.",
                "venue": "",
                "url": null
            }
        },
        {
            "48": {
                "title": "A Comparison of Spoken and Written Language Use in Traditional and Technology-Mediated Learning Environments.",
                "author": "Kristopher Kyle, Ann Tai Choe, Masaki Eguchi, Geoff LaFlair, and Nicole Ziegler. 2021.",
                "venue": "ETS Research Report Series 2021, 1 (2021), 1\u201329.",
                "url": null
            }
        },
        {
            "49": {
                "title": "Auditing the AI auditors: A framework for evaluating fairness and bias in high stakes AI predictive models.",
                "author": "Richard N Landers and Tara S Behrend. 2022.",
                "venue": "American Psychologist (2022).",
                "url": null
            }
        },
        {
            "50": {
                "title": "Procedural Justice in Algorithmic Fairness: Leveraging Transparency and Outcome Control for Fair Algorithmic Mediation.",
                "author": "Min Kyung Lee, Anuraag Jain, Hea Jin Cha, Shashank Ojha, and Daniel Kusbit. 2019.",
                "venue": "Proceedings of the ACM on Human-Computer Interaction 3, CSCW (2019), 1\u201326.",
                "url": null
            }
        },
        {
            "51": {
                "title": "Lexical ambiguity detection in professional discourse.",
                "author": "Yang Liu, Alan Medlar, and Dorota G\u0142owacka. 2022.",
                "venue": "Information Processing & Management 59, 5 (2022), 103000.",
                "url": null
            }
        },
        {
            "52": {
                "title": "The Concept of Fairness in the GDPR: A Linguistic and Contextual Interpretation. In Proceedings of the 2020 Conference on fairness, accountability, and transparency. 154\u2013166.",
                "author": "Gianclaudio Malgieri. 2020.",
                "venue": "",
                "url": null
            }
        },
        {
            "53": {
                "title": "Bridging the gap between different measures of the reading speed deficit in developmental dyslexia.",
                "author": "Marialuisa Martelli, Maria De Luca, Laura Lami, Claudia Pizzoli, Maria Pontillo, Donatella Spinelli, and Pierluigi Zoccolotti. 2014.",
                "venue": "Experimental brain research 232 (2014), 237\u2013252.",
                "url": null
            }
        },
        {
            "54": {
                "title": "A Survey on Bias and Fairness in Machine Learning.",
                "author": "Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. 2021.",
                "venue": "Comput. Surveys 54, 6 (2021), 1\u201335.",
                "url": null
            }
        },
        {
            "55": {
                "title": "The Gender-GAP Pipeline: A Gender-Aware Polyglot Pipeline for Gender Characterisation in 55 Languages.",
                "author": "Benjamin Muller, Belen Alastruey, Prangthip Hansanti, Elahe Kalbassi, Christophe Ropers, Eric Michael Smith, Adina Williams, Luke Zettlemoyer, Pierre Andrews, and Marta R Costa-juss\u00e0. 2023.",
                "venue": "arXiv preprint arXiv:2308.16871 (2023).",
                "url": null
            }
        },
        {
            "56": {
                "title": "Schnell-Lesen: Was ist die Grenze der menschlichen Lesegeschwindigkeit?",
                "author": "Jochen Musch and Peter R\u00f6sler. 2011.",
                "venue": "Kognitive Leistungen: Intelligenz und mentale F\u00e4higkeiten im Spiegel der Neurowissenschaften (2011), 89\u2013106.",
                "url": null
            }
        },
        {
            "57": {
                "title": "PrivacyCheck v2: A Tool that Recaps Privacy Policies for You. In Proceedings of the 29th ACM international conference on information & knowledge management. 3441\u20133444.",
                "author": "Razieh Nokhbeh Zaeem, Safa Anya, Alex Issa, Jake Nimergood, Isabelle Rogers, Vinay Shah, Ayush Srivastava, and K Suzanne Barber. 2020.",
                "venue": "",
                "url": null
            }
        },
        {
            "58": {
                "title": "Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine.",
                "author": "Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, et al. 2023.",
                "venue": "arXiv preprint arXiv:2311.16452 (2023).",
                "url": null
            }
        },
        {
            "59": {
                "title": "Bias in data\u2010driven artificial intelligence systems\u2014an introductory survey.",
                "author": "Eirini Ntoutsi, Pavlos Fafalios, Ujwal Gadiraju, Vasileios Iosifidis, Wolfgang Nejdl, Maria-Esther Vidal, et al. 2020.",
                "venue": "Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 10, 3 (2020), 1\u201314.",
                "url": null
            }
        },
        {
            "60": {
                "title": "Recommendation of the Council concerning Guidelines Governing the Protection of Privacy and Transborder Flows of Personal Data.",
                "author": "OECD. 2013.",
                "venue": "Number OECD/LEGAL/0188.",
                "url": null
            }
        },
        {
            "61": {
                "title": "GPT-4 Turbo.",
                "author": "OpenAI. 2023.",
                "venue": "",
                "url": null
            }
        },
        {
            "62": {
                "title": "Bias in word embeddings. In Proceedings of the 2020 conference on fairness, accountability, and transparency. 446\u2013457.",
                "author": "Orestis Papakyriakopoulos, Simon Hegelich, Juan Carlos Medina Serrano, and Fabian Marco. 2020.",
                "venue": "",
                "url": null
            }
        },
        {
            "63": {
                "title": "Morality Classification in Natural Language Text.",
                "author": "Matheus C Pavan, Vitor G Dos Santos, Alex GJ Lan, Joao Martins, Wesley R Santos, Caio Deutsch, Pablo B Costa, Fernando C Hsieh, and Ivandre Paraboni. 2020.",
                "venue": "IEEE Transactions on Affective Computing (2020).",
                "url": null
            }
        },
        {
            "64": {
                "title": "LLMs grasp morality in concept.",
                "author": "Mark Pock, Andre Ye, and Jared Moore. 2023.",
                "venue": "arXiv preprint arXiv:2311.02294 (2023).",
                "url": null
            }
        },
        {
            "65": {
                "title": "A Typology of Communicative Strategies in Online Privacy Policies: Ethics, Power and Informed Consent.",
                "author": "Irene Pollach. 2005.",
                "venue": "Journal of Business Ethics 62 (2005), 221\u2013235.",
                "url": null
            }
        },
        {
            "66": {
                "title": "Document structure.",
                "author": "Richard Power, Donia Scott, and Nadjet Bouayad-Agha. 2003.",
                "venue": "Computational Linguistics 29, 2 (2003), 211\u2013260.",
                "url": null
            }
        },
        {
            "67": {
                "title": "How to present more readable text for people with dyslexia.",
                "author": "Luz Rello and Ricardo Baeza-Yates. 2017.",
                "venue": "Universal Access in the Information Society 16 (2017), 29\u201349.",
                "url": null
            }
        },
        {
            "68": {
                "title": "How to make privacy policies both GDPR-compliant and usable. In 2018 International Conference On Cyber Situational Awareness, Data Analytics And Assessment (Cyber SA). IEEE, 1\u20138.",
                "author": "Karen Renaud and Lynsay A Shepherd. 2018.",
                "venue": "",
                "url": null
            }
        },
        {
            "69": {
                "title": "Racial bias in legal language.",
                "author": "Douglas Rice, Jesse H. Rhodes, and Tatishe Nteta. 2019.",
                "venue": "Research & Politics 6, 2 (2019), 2053168019848930.",
                "url": null
            }
        },
        {
            "70": {
                "title": "Overview of the GermEval 2021 Shared Task on the Identification of Toxic, Engaging, and Fact-Claiming Comments. In Proceedings of the GermEval 2021 Shared Task on the Identification of Toxic, Engaging, and Fact-Claiming Comments. 1\u201312.",
                "author": "Julian Risch, Anke Stoll, Lena Wilms, and Michael Wiegand. 2021.",
                "venue": "",
                "url": null
            }
        },
        {
            "71": {
                "title": "Ethics in the GDPR: a blueprint for applied legal theory.",
                "author": "Johan Rochel. 2021.",
                "venue": "International Data Privacy Law 11, 2 (2021), 209\u2013223.",
                "url": null
            }
        },
        {
            "72": {
                "title": "Automatic Assessment of Privacy Policies under the GDPR.",
                "author": "David S\u00e1nchez, Alexandre Viejo, and Montserrat Batet. 2021.",
                "venue": "Applied Sciences 11, 4 (2021), 1762.",
                "url": null
            }
        },
        {
            "73": {
                "title": "Gender Bias in Machine Translation.",
                "author": "Beatrice Savoldi, Marco Gaido, Luisa Bentivogli, Matteo Negri, and Marco Turchi. 2021.",
                "venue": "Transactions of the Association for Computational Linguistics 9 (2021), 845\u2013874.",
                "url": null
            }
        },
        {
            "74": {
                "title": "\u201cThere Is Not Enough Information\u201d: On the Effects of Explanations on Perceptions of Informational Fairness and Trustworthiness in Automated Decision-Making. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency. 1616\u20131628.",
                "author": "Jakob Schoeffer, Niklas Kuehl, and Yvette Machowski. 2022.",
                "venue": "",
                "url": null
            }
        },
        {
            "75": {
                "title": "Evaluating Metrics for Bias in Word Embeddings.",
                "author": "Sarah Schr\u00f6der, Alexander Schulz, Philip Kenneweg, Robert Feldhans, Fabian Hinder, and Barbara Hammer. 2021.",
                "venue": "arXiv preprint arXiv:2111.07864 (2021).",
                "url": null
            }
        },
        {
            "76": {
                "title": "The Long Arc of Fairness: Formalisations and Ethical Discourse. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency. 2179\u20132188.",
                "author": "Pola Schw\u00f6bel and Peter Remmers. 2022.",
                "venue": "",
                "url": null
            }
        },
        {
            "77": {
                "title": "Processing and querying large web corpora with the COW14 architecture. In Proceedings of Challenges in the Management of Large Corpora 3 (CMLC-3).",
                "author": "Roland Sch\u00e4fer. 2015.",
                "venue": "",
                "url": null
            }
        },
        {
            "78": {
                "title": "Building Large Corpora from the Web Using a New Efficient Tool Chain. In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC\u201912). 486\u2013493.",
                "author": "Roland Sch\u00e4fer and Felix Bildhauer. 2012.",
                "venue": "",
                "url": null
            }
        },
        {
            "79": {
                "title": "\u201cI\u2019m sorry to hear that\u201d: Finding New Biases in Language Models with a Holistic Descriptor Dataset. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 9180\u20139211.",
                "author": "Eric Michael Smith, Melissa Hall, Melanie Kambadur, Eleonora Presani, and Adina Williams. 2022.",
                "venue": "",
                "url": null
            }
        },
        {
            "80": {
                "title": "Fairness perceptions of algorithmic decision-making: A systematic review of the empirical literature.",
                "author": "Christoph Starke, Janosch Baleis, Bettina Keller, and Frank Marcinkowski. 2022.",
                "venue": "Big Data & Society 9, 2 (2022).",
                "url": null
            }
        },
        {
            "81": {
                "title": "I Read but Don\u2019t Agree: Privacy Policy Benchmarking Using Machine Learning and the EU GDPR. In Companion Proceedings of the The Web Conference 2018. 163\u2013166.",
                "author": "Welderufael B. Tesfay, Peter Hofmann, Toru Nakamura, Shinsaku Kiyomoto, and Jetzabel Serna. 2018a.",
                "venue": "",
                "url": null
            }
        },
        {
            "82": {
                "title": "PrivacyGuide: Towards an Implementation of the EU GDPR on Internet Privacy Policy Evaluation. In Proceedings of the Fourth ACM International Workshop on Security and Privacy Analytics. 15\u201321.",
                "author": "Welderufael B Tesfay, Peter Hofmann, Toru Nakamura, Shinsaku Kiyomoto, and Jetzabel Serna. 2018b.",
                "venue": "",
                "url": null
            }
        },
        {
            "83": {
                "title": "An ai-assisted approach for checking the completeness of privacy policies against gdpr. In 2020 IEEE 28th International Requirements Engineering Conference (RE). IEEE, 136\u2013146.",
                "author": "Damiano Torre, Sallam Abualhaija, Mehrdad Sabetzadeh, Lionel Briand, Katrien Baetens, Peter Goes, and Sylvie Forastier. 2020.",
                "venue": "",
                "url": null
            }
        },
        {
            "84": {
                "title": "UX accessibility for elderly - 12 principles.",
                "author": "Szymon Trzepla. 2019.",
                "venue": "UX Planet (28 07 2019).",
                "url": null
            }
        },
        {
            "85": {
                "title": "Universal Declaration of Human Rights.",
                "author": "UN. 1948.",
                "venue": "General Assembly resolution 217 A (1948).",
                "url": null
            }
        },
        {
            "86": {
                "title": "Complicy: Evaluating the gdpr alignment of privacy policies-a study on web platforms. In International Conference on Research Challenges in Information Science. Springer, 152\u2013168.",
                "author": "Evangelia Vanezi, George Zampa, Christos Mettouris, Alexandros Yeratziotis, and George A Papadopoulos. 2021.",
                "venue": "",
                "url": null
            }
        },
        {
            "87": {
                "title": "Fairness definitions explained. In 2018 ACM/IEEE International Workshop on Software Fairness. 1\u20137.",
                "author": "Sahil Verma and Julia Rubin. 2018.",
                "venue": "",
                "url": null
            }
        },
        {
            "88": {
                "title": "Why fairness cannot be automated: Bridging the gap between EU non-discrimination law and AI.",
                "author": "Sandra Wachter, Brent Mittelstadt, and Chris Russell. 2021.",
                "venue": "Computer Law & Security Review 41 (2021), 105567.",
                "url": null
            }
        },
        {
            "89": {
                "title": "An Integrated Framework for Ethical and Sustainable Digitalization. In 2021 Eighth International Conference on eDemocracy & eGovernment (ICEDEG). IEEE, 156\u2013162.",
                "author": "Ivo Wallimann-Helmer, Luis Ter\u00e1n, Edy Portmann, Hanna Sch\u00fcbel, and Jhonny Pincay. 2021.",
                "venue": "",
                "url": null
            }
        },
        {
            "90": {
                "title": "PolicyChecker: Analyzing the GDPR Completeness of Mobile Apps\u2019 Privacy Policies. In Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security. 3373\u20133387.",
                "author": "Anhao Xiang, Weiping Pei, and Chuan Yue. 2023.",
                "venue": "",
                "url": null
            }
        },
        {
            "91": {
                "title": "The Effect of the GDPR on Privacy Policies: Recent Progress and Future Promise.",
                "author": "Razieh Nokhbeh Zaeem and K. Suzanne Barber. 2020.",
                "venue": "ACM Trans. Manage. Inf. Syst. 12, 1, Article 2 (dec 2020), 20 pages.",
                "url": null
            }
        },
        {
            "92": {
                "title": "DiscoScore: Evaluating Text Generation with BERT and Discourse Coherence. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics. 3865\u20133883.",
                "author": "Wei Zhao, Michael Strube, and Steffen Eger. 2023.",
                "venue": "",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.08115v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "3"
        ],
        "methodology_sections": [
            "4",
            "5",
            "6"
        ],
        "main_experiment_and_results_sections": [
            "4.2",
            "5.2",
            "6.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.2",
            "5.2",
            "6.2"
        ]
    },
    "research_context": {
        "paper_id": "2403.08115v2",
        "paper_title": "Legally Binding but Unfair? Towards Assessing Fairness of Privacy Policies",
        "research_background": "### **Introduction**\n\n**Motivation:**\nThe motivation behind this paper is driven by the need for transparency and fairness in privacy policies as mandated by regulations like the GDPR. Privacy policies are essential for building trust and addressing the information imbalance between data subjects (users) and data holders (organizations). However, ensuring these policies are perceived as fair and are easily understood by diverse user groups is challenging. The authors aim to address this gap by developing methods to automatically assess privacy policies for fairness across several important dimensions.\n\n**Research Problem:**\nThe main research problem the paper addresses is the automatic assessment of the fairness of privacy policies. Specifically, the authors seek to evaluate three dimensions of fairness: \n1. **Informational Fairness**: Ensuring the clarity and understandability of the information provided.\n2. **Representational Fairness**: Avoiding biases and ensuring equitable representation of all user groups.\n3. **Ethics and Morality**: Ensuring ethical usage and avoidance of exploitative practices in data usage terms.\n\nThe problem is motivated by the current lack of definitions and tools specifically tailored to assess the fairness of privacy policies using natural language processing (NLP) and artificial intelligence (AI) techniques.\n\n**Relevant Prior Work:**\nThe paper builds upon significant prior research in several areas:\n- **GDPR and Transparency Requirements**: By referencing the GDPR (European Union, 2016) and related works (Zaeem and Barber, 2020), the paper grounds its motivation in regulatory requirements for privacy policies.\n- **Dimensions of Fairness in Texts**:\n  - **Informational Fairness**: Earlier studies (Boudjella et al., 2017; Trzepla, 2019; Rello and Baeza-Yates, 2017; Evans et al., 2014; Aikens and Barbarin, 2008) elaborate on the need for clear communication of information.\n  - **Representational Fairness**: Prior work (Rice et al., 2019; Baker Gillis, 2021; Gumusel et al., 2022) discusses biases and equitable representation in texts.\n  - **Ethics and Morality**: The ethical considerations (Hooker, 2005; Schw\u00f6bel and Remmers, 2022) and legal frameworks (OECD, 2013; European Union, 2016) have also been foundational in understanding fairness.\n- **Conceptualizing Fairness**: The study acknowledges multiple definitions and sometimes contradictory perspectives on fairness (Verma and Rubin, 2018; Mehrabi et al., 2021; Chouldechova, 2017; Defrance and De Bie, 2023), underscoring a gap in definitions tailored to privacy policies.\n\nThis investigation fills the identified gap by proposing and validating approaches to assess fairness in privacy policies and making a unique contribution to the discourse on the linguistic fairness of legal texts. The authors claim novelty in being the first to suggest a structured approach to this important issue, thereby setting the foundation for future explorations into the fairness of legal texts.",
        "methodology": "I can certainly help with that. However, it looks like the text you've provided is very limited. To give a more thorough explanation, I would need the entire methodology section from the paper \"Legally Binding but Unfair? Towards Assessing Fairness of Privacy Policies.\"\n\nFrom the provided text, it seems like the authors are using an approach outlined in Section 2.2.1 of their paper for assessing informational fairness in privacy policies. This implies that their methodology relies on an established framework or analysis from that earlier section.\n\nCould you please provide the full methodology section, so I can offer a detailed description of the proposed method or model, including its key components and innovations?",
        "main_experiment_and_results": "**Main Experiment Setup and Results:**\n\n* **Experiment Setup:**\n  * **Dataset:** The dataset consists of canonical forms of 618 German privacy policies spanning over eight years, starting from 2016. The policies cover the German Top-100 most visited web shops.\n  * **Language:** All experiments were conducted in German, with results translated into English for reporting.\n  * **Data Preprocessing:** Involved acquisition, preprocessing, and cleansing, as detailed in Bartelt and Buchmann (2024).\n  \n* **Baselines and Techniques:**\n  * **Word-Level Fairness Assessment:**\n    * A dictionary-based lexical filtering was used to identify potential anglicisms.\n    * Tools such as NLTK and SpaCy were utilized for this purpose, alongside a word-to-word translator with a smaller vocabulary to narrow down the list.\n  * **Sentence-Level Fairness Assessment:**\n    * Used the entire dataset of 618 policies.\n    * Measured the Flesch Reading Ease (FRE) scores to gauge readability and fairness.\n\n* **Evaluation Metrics:**\n  * **Lexical Analysis:** The average number of anglicisms found per policy was used to assess potential discrimination.\n  * **Readability (FRE Scores):** Used FRE scores to measure the readability of policies. An FRE of 30 or below indicates that the policy requires academic-level reading competence, whereas an FRE of 60 or above indicates a fair and non-discriminating policy.\n  * **Document Structure Statistics:**\n    * Average reading time calculated for dyslexic individuals and average readers.\n    * The structure parameters such as the number of paragraphs, words per paragraph, heading levels, and the use of lists and formatting.\n\n* **Main Experimental Results:**\n  * **Word-Level Analysis:**\n    * On average, 26 anglicisms were identified per policy. This finding suggests that informational fairness is impaired by discriminating against demographics not familiar with anglicisms.\n  * **Sentence-Level Analysis:**\n    * The average FRE score was 37, with individual policies ranging from an FRE of 14 to 88. \n    * An FRE of 30 or below indicates the necessity for academic-level reading competence, while a fair FRE is 60 and above, suggesting many policies are not easily understandable and thus unfair.\n  * **Document Structure Analysis:**\n    * The longest policy requires nearly 165 minutes for a dyslexic reader to comprehend, while an average reader needs just under 20 minutes.\n    * Each policy averaged 150 paragraphs, each with 32 words, and contained 34 headings across 3 levels, using lists in 92% and strong formatting in 72% of the policies. \n    * Only 6% of the policies lacked headings entirely, indicating varied attention to organizational structure.\n\nThese results highlight significant issues in informational fairness in privacy policies, evidenced by poor readability scores and complex document structures that are not conducive to all reader demographics."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To assess the informational fairness of privacy policies automatically by analyzing wording, sentences, and document structure.",
            "experiment_process": "The experiment utilized the German Top-100 most visited webshops, comprising 618 German privacy policies collected from 2016 to 2024. Lexical filtering was applied to identify potential anglicisms, which involved using a word-to-word translator to narrow down the list. For fairness at the sentence level, the Flesh Reading Ease (FRE) score was calculated across all policies. Document structure analysis included measuring policy length, number of paragraphs, and formatting features such as headings and lists.",
            "result_discussion": "The study found that an average of 26 anglicisms per policy impairs informational fairness by discriminating against demographics where anglicisms are uncommon. The average FRE score was 37, indicating that most policies are difficult to read. The document structure analysis showed that the average policy is very lengthy, potentially requiring significant reading time, particularly for dyslexic individuals, indicating mixed results for informational fairness.",
            "ablation_id": "2403.08115v2.No1"
        },
        {
            "research_objective": "To evaluate inclusive language and representation fairness in privacy policies from German webshops.",
            "experiment_process": "The experiment involved translating descriptor terms from HolisticBias to German and adjusting them for cultural specificity. The evaluation was performed on 618 policies and included counting occurrences of informal language, youth-oriented terminology, and gender-neutral terms. Appropriate gender-specific language usage was also checked, along with vocabulary related to nationality and political terms.",
            "result_discussion": "The study found that no disabilities were specifically mentioned in the policies. Informal and youth-oriented terminology was frequently used. While the policies were generally gender-neutral, there was insufficient appropriate gender-specific language. Indicators of nationality bias towards the US, Germany, and European countries were observed. No significant biases were found concerning political, cultural, or sexuality demographics, but age-related representational bias assessment faced challenges due to varying usage of terms.",
            "ablation_id": "2403.08115v2.No2"
        },
        {
            "research_objective": "To explore the ethical appropriateness of privacy policies using a qualitative assessment metric.",
            "experiment_process": "The experiment focused on general morality and used eight policies, one from each year in the dataset, evaluated using GPT4turbo with a predefined prompt. The large language model (LLM) conducted qualitative assessments and rated aspects on a five-point Likert scale across multiple evaluation runs.",
            "result_discussion": "The LLM was able to identify 12 distinct ethical criteria for privacy policies, beyond GDPR requirements, using varied assessment runs. Some criteria appeared inconsistently across different runs. Major concerns included data usage, international transmission and sharing of data, and communication and changes. The multi-run approach had exploratory merit though it raised the issue of variability in criteria appearance.",
            "ablation_id": "2403.08115v2.No3"
        }
    ]
}