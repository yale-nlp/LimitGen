{
    "title": "Compression Represents Intelligence Linearly",
    "abstract": "There is a belief that learning to compress well will lead to intelligence (Hutter, 2006).\nRecently, language modeling has been shown to be equivalent to compression,\nwhich offers a compelling rationale for the success of large language models (LLMs): the development of more advanced language models is essentially enhancing compression which facilitates intelligence.\nDespite such appealing discussions, little empirical evidence is present for the interplay between compression and intelligence.\nIn this work, we examine\ntheir relationship\nin the context of LLMs, treating LLMs as data compressors.\nGiven the abstract concept of \u201cintelligence\u201d, we adopt the average downstream benchmark scores as a surrogate, specifically targeting intelligence related to knowledge and commonsense, coding, and mathematical reasoning.\nAcross 12 benchmarks, our study brings together 30 public LLMs that originate from diverse organizations.\nRemarkably, we find that LLMs\u2019 intelligence \u2013 reflected by average benchmark scores \u2013 almost linearly correlates with their ability to compress external text corpora.\nThese results provide concrete evidence supporting the belief that superior compression indicates greater intelligence.\nFurthermore, our findings suggest that compression efficiency, as an unsupervised metric derived from raw text corpora, serves as a reliable evaluation measure that is linearly associated with the model capabilities.\nWe open-source our compression datasets as well as our data collection pipelines to facilitate future researchers to assess compression properly.111https://github.com/hkust-nlp/llm-compression-intelligence.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The belief that compression is closely connected to intelligence has been held for a long time, with some researchers suggesting that they are fundamentally equivalent (Hern\u00e1ndez-Orallo & Minaya-Collado, 1998  ###reference_b26###; Mahoney, 1999  ###reference_b40###; Legg et al., 2005  ###reference_b33###; Hutter, 2006  ###reference_b27###; Legg & Hutter, 2007  ###reference_b32###).\nThis notion becomes even more compelling in light of the recent advancements in large language models (LLMs) and their implications for AI, leading to an exploration of language modeling from a compression standpoint.\nAccording to compression theory, any predictive model can be transformed into a lossless compressor and vice versa. Thus, language modeling can be considered a form of compression, with LLMs showing strong capabilities in data compression empirically (Deletang et al., 2024  ###reference_b16###).\nFrom this viewpoint, the argument that compression leads to intelligence becomes more relevant to the current paradigm of AI based on LLMs. Yet, despite these theoretical discussions, empirical evidence on the relationship between compression and intelligence remains limited. In this work, we seek to conduct such empirical study and answer: If a language model can encode a text corpus with fewer bits in a lossless manner, does that indicate greater intelligence?\nWhile the precise definition of \u201cintelligence\u201d often ventures into philosophical territory and can vary significantly, our work approaches \u201cintelligence\u201d from a practical standpoint, focusing on the model\u2019s ability to complete various downstream tasks.\nThis treatment aligns with the definition from Hutter (2006  ###reference_b27###), who notes that \u201cintelligence is an agent\u2019s ability to achieve goals in a wide range of environments\u201d.\nWe measure intelligence along three key abilities: knowledge and commonsense, coding, and mathematical reasoning.\nSpecifically, we begin by collecting external raw corpora in the respective domain (e.g. GitHub code for the coding ability) and evaluate the efficiency of various LLMs in compressing the corpus.\nSubsequently, we assess these models on a range of downstream tasks, using the average benchmark scores to gauge their domain-specific intelligence.\nThe goal is to examine the correlation between the models\u2019 downstream ability and their compression efficiency.\nOn experiments across 30 public LLMs and 12 diverse benchmarks, we uncover a remarkable finding: LLMs\u2019 downstream ability is almost linearly correlated with their compression efficiency, with a Pearson correlation coefficient of around -0.95 for each evaluated domain of intelligence as shown in Figure 1  ###reference_###.\nNotably, this linear relationship even extends to most individual benchmarks as well.\nRecent and concurrent works have explored the relationship between benchmark scores and compression-equivalent metrics like validation loss within the same model series, where the model checkpoints share most configurations such as model designs, tokenizers, and data (Wei et al., 2023  ###reference_b57###; Gadre et al., 2024  ###reference_b19###; Du et al., 2024  ###reference_b17###). Our study, however, is the first to document a linear correlation between compression and intelligence in LLMs across varying model sizes, tokenizers, context window lengths, and pretraining data distributions. Our findings establish the linear correlation between compression and intelligence as a universal principle, providing empirical support for the longstanding belief that superior compression is indicative of greater intelligence.\nFrom a practical perspective, compression efficiency serves as an unsupervised metric for LLMs where the text corpora can be easily updated to avoid overfitting or test contamination.\nOur findings advocate for adopting compression efficiency as a stable, flexible, and reliable metric to evaluate LLMs, which is linearly correlated with the models\u2019 abilities.\nWe open-source our compression corpora, as well as the data collection and processing pipelines so that future researchers can easily collect and update their own corpora to assess compression."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background: Language Modeling is Compression",
            "text": "The discussion on the equivalence between language modeling and compression has a long history (Shannon, 1948  ###reference_b48###; 1951  ###reference_b49###).\nRecently,\nDeletang et al. (2024  ###reference_b16###) have echoed this connection and demonstrated that language models are general-purpose compressors empirically. Below, we briefly introduce the background to acquaint readers with such a relationship.\nSuppose  is the underlying distribution to generate the text sequence data.\nGiven a text sequence , lossless compression seeks to encode it into a code  that is smaller in size while preserving all original information.\nWe typically use binary bits as the coding alphabet of .\nAccording to the source coding theorem (Shannon, 1948  ###reference_b48###), the expected number of bits of an optimal encoder is equal to , which is the minimum average number of bits that one can achieve on compressing text from  in a lossless manner.\nKnowing , a common approach to achieve such optimal compression is arithmetic coding (Rissanen, 1976  ###reference_b45###; Pasco, 1977  ###reference_b42###),\nwhich is a practical approach to optimally encode data with a probabilistic model. We refer the readers to Deletang et al. (2024  ###reference_b16###) for more details on arithmetic coding for autoregressive language models.\nIn practice, however,  is often unknown, and we can use a language model  to help compress  efficiently.\nIntuitively, we seek to utilize  to encode rare sequences with more bits and the frequent ones with fewer bits.\nArithmetic coding is able to achieve the optimal expected code length (in bits) as:\nwhich establishes the connection between  and compressors. To achieve more efficient compression, we aim to optimize  to minimize the average code length in Eq. 1  ###reference_###.\nNoting that Eq. 1  ###reference_### is exactly the cross-entropy loss that is used to train language models, learning to compress well is equivalent to language modeling.222Libraries like PyTorch often implement the cross entropy loss with the log base of  rather than , which causes a constant coefficient difference. Yet, it does not influence the equivalence.\nThis implies that if a language model has a lower loss on the data to be compressed, it will be able to compress it with fewer bits in a lossless manner.\nStrictly speaking, one needs to access the language model parameters to recover the original data from the code, thus the bits required to encode these parameters should be counted as well. However, such a cost can be amortized and becomes negligible when the model is used to compress a substantially large volume of data.\nThere are online or offline settings to compress a data stream. In the online setting, the model is randomly initialized and trained on the data to be compressed. In the offline setting, the model is trained on external data and adopted to compress a different dataset.\nWe focus on the offline setting in this paper, using pretrained language models to compress external corpora."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Examining Compression and Intelligence \u2013 the Experimental Protocols",
            "text": "Our goal is to empirically examine the relationship between compression and intelligence in the context of large language models.\nOur method is to investigate a variety of pretrained LLMs of different sizes and from diverse organizations, assessing their compression efficiency and \u201cintelligence\u201d respectively.\nThen we aim to summarize the potential relationship between compression and intelligence based on the evaluation results.\nOur overall principle is to collect diverse LLMs that are created with different training data, tokenizers, computation, and even architectures (e.g., mixture of experts as in MistralAI (2023  ###reference_b41###)), so that our conclusion on compression and intelligence is general and agnostic to specific model series.\nNext, we detail our experimental protocols to evaluate compression and intelligence.\nAccording to \u00a72  ###reference_###, we evaluate the model\u2019s compression efficiency using the average coding length per message of compressing a given corpus.\nDue to the variety of tokenizers employed by different LLMs, the average bits per token are not directly comparable. Therefore, we utilize the average bits per character (BPC) as the metric. BPC is a commonly used compression metric for compressing text, such as in enwik8 (Mahoney, 2011  ###reference_b39###). Specifically,\nwhere  is the corpus to be compressed,  is the total number of tokens of  tokenized by the model\u2019s tokenizer, and  is the total number of characters of .\nNote that Eq. 2  ###reference_### is equal to the per-character language model loss on the corpus  (with a constant log base shift).\nLLMs can have different context window sizes, and a longer context window gives advantages to compression efficiency. This is because a larger context window offers more information to predict the next token\u2019s distribution and allows for encoding with fewer bits, as illustrated in Eq. 2  ###reference_###.\nHowever, in downstream benchmarks where input lengths are short, the benefit of extended context windows diminishes.\nThis applies to all our benchmarks in Table 1  ###reference_###, where the inputs do not exceed 2048 tokens, even in few-shot in-context learning scenarios.\nBasically, the downstream tasks in our study only reflect the model\u2019s intelligence when operating within relatively short contexts. To study its correlation with compression, it is crucial to assess compression using comparable context length, ensuring consistency in the models\u2019 access to information during both corpus compression and execution of downstream tasks.\nTherefore, for all our compression and benchmark evaluations across different LLMs, we unify the context window size to be 1900 tokens that are sufficient for all benchmark evaluation.\nIdeally, a more holistic evaluation of intelligence should incorporate tasks involving both short and long contexts.\nHowever, in this paper, our focus is on the short- or medium-context regime, which encompasses the majority of benchmark tasks. We leave the exploration of long-context scenarios for future work.\nApart from context window unification, we utilize a sliding window approach to evaluate compression more accurately, as detailed in Appendix A.1  ###reference_###.\nIn the development of LLMs, there are typically two stages: the pre-training stage and the alignment stage.\nThe models are referred to as base models after the pre-training stage and as fine-tuned models following the alignment stage.\nWe note that fine-tuned models are no longer general-purpose compressors since it does not model the next token distribution for arbitrary text, but only for structured (query, response) data.\nBesides, it is commonly believed that the intelligence of LLMs is learned during the pretraining stage and remains relatively fixed during the alignment stage (Zhou et al., 2023  ###reference_b62###).\nTherefore, in this work, we focus on the base models only, while in \u00a75  ###reference_### we will further discuss the case of compression v.s. intelligence for fine-tuned models.\nWhat kind of corpus shall we select to measure the models\u2019 compression?\nFirstly, it is important to recognize that different corpora can illuminate various aspects of models\u2019 abilities, and compressing one corpus well may not generalize to another (Magnusson et al., 2023  ###reference_b38###).\nTherefore, we would like to select the corpora that align with the areas of our focus.\nSecondly, the chosen corpora should not intersect with the models\u2019 pretraining data to avoid data leakage.\nGiven the opaque status of LLMs\u2019 pretraining datasets, we opt to use the newest corpora as a measure.\nConcretely, for assessing knowledge and commonsense, we have compiled texts from the latest Common Crawl dataset.\nTo evaluate coding ability, we have sourced data from GitHub repositories mainly on the Python language since the downstream benchmarks focus on Python coding abilities.\nFor mathematical reasoning, we collect academic papers from ArXiv, specifically selecting those designated with \u201cmath\u201d as their primary category.\nFor each data source, we ensure the recency by utilizing the latest available data at the time of our experiments. The composition of our compression corpora is detailed in Table 1  ###reference_###.\nMore details about data collection and processing are in Appendix A.2  ###reference_.SSS0.Px1###."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Evaluating Intelligence of LLMs",
            "text": "The definition of \u201cintelligence\u201d has been a subject of debate for years.\nAlan Turing firstly proposed the definition based on the Turing Test, which evaluates a machine\u2019s ability to exhibit intelligent behavior indistinguishable from that of a human (Turing, 1950  ###reference_b55###). Although the Turing test is probably the most commonly accepted definition for intelligence, the application of the Turing Test as a measure for AI is deemed impractical, primarily because the evaluation\u2019s outcome relies heavily on the subjective judgment of the interrogator.\nMore recently, Legg & Hutter (2007  ###reference_b32###) proposed a definition termed universal intelligence, a broader interpretation than Turing\u2019s focus on human-like intelligence. They\nposited that an agent\u2019s capacity to achieve goals across a wide array of scenarios should define its intelligence.\nThis practical approach to defining intelligence guides our evaluation of models, wherein we assess their intelligence based on average performance across multiple downstream tasks.\nSuch practical evaluation is commonly adopted by most LLM developers to showcase the potential of their models (Touvron et al., 2023a  ###reference_b53###; b  ###reference_b54###; Jiang et al., 2023  ###reference_b28###; Team et al., 2023  ###reference_b52###), and Anthropic (2024  ###reference_b4###) directly noted the \u201cintelligence\u201d of Claude 3 with the average benchmark scores.\nSpecifically, in this paper, we study intelligence along three key abilities: knowledge and commonsense, coding, and mathematical reasoning.\nThese abilities represent the most focused LLM areas nowadays, and we collect the well-recognized benchmarks for each of these areas, utilizing their average score to indicate domain-specific intelligence. The benchmarks are listed in Table 1  ###reference_###."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Evaluating Compression of LLMs",
            "text": "According to \u00a72  ###reference_###  ###reference_###, we evaluate the model\u2019s compression efficiency using the average coding length per message of compressing a given corpus.\nDue to the variety of tokenizers employed by different LLMs, the average bits per token are not directly comparable. Therefore, we utilize the average bits per character (BPC) as the metric. BPC is a commonly used compression metric for compressing text, such as in enwik8 (Mahoney, 2011  ###reference_b39###  ###reference_b39###). Specifically,\nwhere  is the corpus to be compressed,  is the total number of tokens of  tokenized by the model\u2019s tokenizer, and  is the total number of characters of .\nNote that Eq. 2  ###reference_###  ###reference_### is equal to the per-character language model loss on the corpus  (with a constant log base shift).\nLLMs can have different context window sizes, and a longer context window gives advantages to compression efficiency. This is because a larger context window offers more information to predict the next token\u2019s distribution and allows for encoding with fewer bits, as illustrated in Eq. 2  ###reference_###  ###reference_###.\nHowever, in downstream benchmarks where input lengths are short, the benefit of extended context windows diminishes.\nThis applies to all our benchmarks in Table 1  ###reference_###  ###reference_###, where the inputs do not exceed 2048 tokens, even in few-shot in-context learning scenarios.\nBasically, the downstream tasks in our study only reflect the model\u2019s intelligence when operating within relatively short contexts. To study its correlation with compression, it is crucial to assess compression using comparable context length, ensuring consistency in the models\u2019 access to information during both corpus compression and execution of downstream tasks.\nTherefore, for all our compression and benchmark evaluations across different LLMs, we unify the context window size to be 1900 tokens that are sufficient for all benchmark evaluation.\nIdeally, a more holistic evaluation of intelligence should incorporate tasks involving both short and long contexts.\nHowever, in this paper, our focus is on the short- or medium-context regime, which encompasses the majority of benchmark tasks. We leave the exploration of long-context scenarios for future work.\nApart from context window unification, we utilize a sliding window approach to evaluate compression more accurately, as detailed in Appendix A.1  ###reference_###  ###reference_###.\nIn the development of LLMs, there are typically two stages: the pre-training stage and the alignment stage.\nThe models are referred to as base models after the pre-training stage and as fine-tuned models following the alignment stage.\nWe note that fine-tuned models are no longer general-purpose compressors since it does not model the next token distribution for arbitrary text, but only for structured (query, response) data.\nBesides, it is commonly believed that the intelligence of LLMs is learned during the pretraining stage and remains relatively fixed during the alignment stage (Zhou et al., 2023  ###reference_b62###  ###reference_b62###).\nTherefore, in this work, we focus on the base models only, while in \u00a75  ###reference_###  ###reference_### we will further discuss the case of compression v.s. intelligence for fine-tuned models.\nWhat kind of corpus shall we select to measure the models\u2019 compression?\nFirstly, it is important to recognize that different corpora can illuminate various aspects of models\u2019 abilities, and compressing one corpus well may not generalize to another (Magnusson et al., 2023  ###reference_b38###  ###reference_b38###).\nTherefore, we would like to select the corpora that align with the areas of our focus.\nSecondly, the chosen corpora should not intersect with the models\u2019 pretraining data to avoid data leakage.\nGiven the opaque status of LLMs\u2019 pretraining datasets, we opt to use the newest corpora as a measure.\nConcretely, for assessing knowledge and commonsense, we have compiled texts from the latest Common Crawl dataset.\nTo evaluate coding ability, we have sourced data from GitHub repositories mainly on the Python language since the downstream benchmarks focus on Python coding abilities.\nFor mathematical reasoning, we collect academic papers from ArXiv, specifically selecting those designated with \u201cmath\u201d as their primary category.\nFor each data source, we ensure the recency by utilizing the latest available data at the time of our experiments. The composition of our compression corpora is detailed in Table 1  ###reference_###  ###reference_###.\nMore details about data collection and processing are in Appendix A.2  ###reference_.SSS0.Px1###  ###reference_.SSS0.Px1###."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Overfitting Benchmarks",
            "text": "LLMs may be overoptimized towards certain benchmarks, in this case, the benchmark scores are not a good proxy for intelligence anymore.333\u201cWhen a measure becomes a target, it ceases to be a good measure\u201d \u2013 Goodhart\u2019s law.\nFor example, Bi et al. (2024  ###reference_b9###) show that when adding multi-choice QA training data, the model is able to achieve over 10 absolute points improvement on multi-choice QA benchmarks such as MMLU, while its performance on general QA benchmarks like TriviaQA remains unchanged.\nIn the math domain, recent researches find that some models heavily optimize towards the GSM8K and MATH benchmarks, while performing poorly on held-out math tests (Paster, 2023  ###reference_b43###).\nIn such scenarios, these benchmark scores are not reliable enough to represent the models\u2019 intelligence in the respective area.\nAlthough we focus on base models in this work, where such overfitting phenomenon may be less common, it is still possible that some LLMs are specially trained on the training data of the corresponding benchmarks in the pretraining stage, or even worse, suffer from test data contamination issues as evidenced in Wei et al. (2023  ###reference_b57###).\nTo identify such cases, we adopt the MIN-K% PROB method (Shi et al., 2024  ###reference_b51###) which is proposed to detect whether the given text is exposed to the model during pretraining.\nThe MIN-K% PROB approach selects the  tokens in a given example with minimum probabilities, if the average probability of these tokens is high, then the example is likely to be present in the pretraining data.\nWe define the MIN-K% Score to be the average negative log-likelihood of these selected tokens.\nIn the experiments next, we compute the MIN-K% Score for all the test splits of the benchmarks, and for the training splits as well if available.\nWe will spot and discuss the LLMs with extremely abnormal MIN-K% Scores.\nWe choose  to be 20 in our experiments as suggested by Shi et al. (2024  ###reference_b51###)."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "How is Intelligence Correlated with Compression?",
            "text": "To provide a comprehensive study of the correlation between compression efficiency and intelligence, we include 8 series of general-purpose language models, covering diverse organizations and varying in size, as shown in Table 2  ###reference_###.\nNotably, in addition to dense transformer models, we further include Mixtral-8x7B (MistralAI, 2023  ###reference_b41###), which utilizes a sparse mixture-of-experts (MoE) architecture.\nFurthermore, we incorporate state-of-the-art models specialized in coding and mathematical reasoning in addition to general-purpose models, as outlined in Table 2  ###reference_###.\nWe assess general-purpose LLMs across all benchmarks, while we include code LLMs in the code area only and mathematical LLMs in the mathematical reasoning area only.\nWe note that our investigation focuses exclusively on well-trained LLMs, implying that the intelligence evaluated is likely already manifest, in contrast to models where such abilities have yet to emerge (Wei et al., 2022  ###reference_b56###). This is because LLMs that are not optimized enough tend to perform poorly, where the results are overly noisy and less meaningful, complicating the analysis.\nModels are evaluated using few-shot in-context learning or in a zero-shot manner, adhering to the established norms for the respective benchmarks.\nAll the models are evaluated exactly the same way with the same codebase for a fair comparison. More details on evaluation are included in Appendix A.3  ###reference_###.\nAs described in \u00a73.2  ###reference_###, we utilize bits per character (BPC) as the evaluation metric for compression efficiency.\nTo quantitatively assess the correlation between intelligence and compression, we report the Pearson correlation coefficient (Pearson ) between the average benchmark scores and BPC.\nBased on preliminary observations that the correlation is highly linear, we perform a linear fit between the average benchmark scores and BPC, and report the Root Mean Square Error (RMSE ) that captures how much the models\u2019 benchmark scores deviate from the linear prediction on average.\nWe extend this analysis to individual benchmarks as well in addition to the average.\nQualitatively, we plot the linear regression results to visualize the relationship, with each model being a point in the figure.\nWe have summarized our main results in Figure 1  ###reference_###, where we examine the overall relationship by computing the average benchmark scores across all three areas, and the average BPC across all the compression corpora. The average score and the average BPC demonstrate a highly linear correlation qualitatively in the visualization.\nQuantitative results with a Pearson correlation coefficient of -0.94 and an RMSE of 2.8% further verify the linear relationship.\nWe consider such a linear correlation significant given the noisy nature of evaluating on downstream benchmarks \u2013 it is known that varying the prompts or decoding hyperparameters could easily cause several points of difference for the task scores (Lu et al., 2022  ###reference_b37###; Gonen et al., 2022  ###reference_b22###; Sclar et al., 2024  ###reference_b47###).\nWe also report the results for the three key areas respectively which demonstrate similar phenomena qualitatively and quantitatively.\nWe note that several previous and concurrent works have studied the correlation between benchmark scores and validation losses in limited settings \u2013 they focus on either the model checkpoints of the same model over the course of pretraining or the same model series (Lu et al., 2023  ###reference_b36###; Wei et al., 2023  ###reference_b57###; Gadre et al., 2024  ###reference_b19###; Du et al., 2024  ###reference_b17###). Their studied checkpoints share designs and tokenizers and are pretrained on the same data distributions.\nHowever, our work escapes the constraints on the same model (series) and compares across models that diverge dramatically on tokenizers, model designs, and pretraining data. Our results mark the linear correlation between compression and intelligence in a general manner, establishing it as universal principle.\nThe specific benchmark scores and BPC for every model are reported in Appendix B  ###reference_###.\nNext, we discuss the results in the three areas with more details respectively, and extend the examination to individual benchmarks as well.\n###figure_1### ###figure_2### We present both the average and individual benchmark results for the knowledge and commonsense area in Figure 2  ###reference_###.\nNotably, the scores from individual benchmarks exhibit a strong linear correlation with compression efficiency, indicated by Pearson coefficients of -0.877, -0.947, and -0.888 for HellaSwag, TriviaQA, and MMLU, respectively. The Root Mean Square Error  is approximately 2 absolute points, which falls within the expected range of variation \u2013 Minor fluctuations in evaluation details, such as prompt adjustments, can readily account for a 2-point accuracy shift (Sclar et al., 2024  ###reference_b47###).\nMeanwhile, we observe that the linear correlation in both HellaSwag and MMLU is not as pronounced as in TriviaQA, albeit for different reasons. For HellaSwag, the average accuracy among models has exceeded 0.8, hinting at a saturation point in performance that blurs the distinctions between models. Conversely, MMLU, which draws heavily from real-world examinations and textbooks, presents a slight mismatch in distribution compared to the Common Crawl dataset used to measure compression efficiency. Further experiments on MMLU and textbook compression are detailed in Appendix E  ###reference_###, where a Pearson coefficient of -0.926 is observed.\nWe note that for MMLU evaluation, we employ a cloze-style input format for this popular multiple-choice question (MCQ) benchmark, presenting only the question without the options,\nand subsequently selecting the choice with the highest likelihood.\nWe intentionally opted for this less conventional evaluation style to minimize the risk of models overfitting to the MCQ format, thus over-optimizing the scores. Please see Appendix D  ###reference_### for detailed discussion and ablation.\nSimilar to the knowledge and commonsense area, there is a strong linear correlation between the models\u2019 compression efficiency and its coding ability, with the Pearson correlation coefficient within  for each individual benchmark.\nFurthermore, despite being of different architectures, the MoE model, Mixtral-8x7B, still fits well into this linear correlation.\nWe notice the strongest linear correlation within the DS-1000 benchmark, with a Pearson coefficient of -0.958, surpassing those of HumanEval and MBPP.\nWe think that this is because we collect Python code from the popular GitHub repositories in recent months, which are mostly related to data science and machine learning, aligned closer with DS-1000.\nWe note that the Deepseek-coder model series are consistently well-above the linear fit, demonstrating strong abilities on the HumanEval and MBPP benchmarks.\nWe hypothesize that this is due the fact that Deepseek-coder models are exposed to private, task-specific data during pretraining that help HumanEval and MBPP significantly, yet we do not have reliable methods to detect the existance of such data and verify this hypotheis.\n###figure_3### ###figure_4### Overall the correlation results in the mathematical reasoning area are similar to the other two, as shown in Figure 4  ###reference_###.\nHowever,\noverfitting benchmarks in mathematical reasoning warrants particular attention.\nAs described in \u00a73.3  ###reference_###, we compute the MIN-K% Score for every example \u2013 which is the average negative log likelihood of the  tokens with the lowest probabilities \u2013 on the benchmark test data as well as training data if available.\nAbnormally low MIN-K% Scores imply that the example is likely to be present in the model\u2019s pretraining data.\nWe did not observe abnormal MIN-K% patterns in all the previous experiments, yet, there are models with far lower Min-K% Scores on the GSM8K and MATH datasets than all other models.\nWe visualize the MIN-K% distribution of the training and test examples on both GSM8K and MATH in Figure 5  ###reference_###, which implies that the Qwen model series may be exposed to the GSM8K training data, MATH training data, and even the MATH test data in the pretraining stage.\nTherefore, we spot the Qwen model series in our visualization and exclude them when performing the linear fit and compute the correlation metrics.\nAs expected, the three Qwen models achieve generally higher accuracies than the predicted performance from the linear fit, and appear as outliers in Figure 4  ###reference_###.\nAt last, we highlight that the strongly linear correlation between the single benchmark scores and compression efficiency was originally unexpected, because individual benchmark scores are typically too noisy to represent models\u2019 general ability.\nHowever, current findings indicate that these individual scores may be predictable from the BPC with a minor error in most cases indicated by small RMSE.\nIn the previous experiments, we intentionally selected the appropriate compression corpora aligning with the area of focus. Herein, we further explore the effects of diverse compression corpora on the results. We investigate two representative examples where the compression corpus does not align with the specific domain, to study whether an arbitrary compression corpus would suffice for a strong linear correlation: (1) mathematical reasoning benchmarks v.s. compression on Common Crawl data, and (2) knowledge and commonsense benchmarks v.s. compression on GitHub code data. We present the findings in Figure 7  ###reference_###.\nDue to a substantial ability mismatch between the compression corpus and the benchmarks, the linear correlation is significantly weaker than that with in-domain data. Coupled with the MMLU case discussed in Appendix E  ###reference_###, these instances demonstrate that linear correlation relies on the alignment degree between the compression corpus and the specific area of study.\nHowever, we emphasize that our previous findings are relatively robust since our chosen compression corpora as listed in Table 1  ###reference_### rely on simple heuristics at a coarse granularity without meticulous designing.\nHow to automatically identify the most appropriate compression corpus that correlates with certain benchmarks is an important problem that we leave for future work.\nAs an example, we investigate the effect of size of compression corpus on the knowledge and commonsense ability.\nWe vary the number of characters of the compression corpus from 50K to 100M, observing changes in the Pearson correlation coefficient and the RMSE depicted in Figure 7  ###reference_###. As the number of sampled characters increases, the linear correlation gradually strengthens and reaches saturation at around 30M.\nThis observation suggests that the compression corpus is sufficiently large with only tens of millions of characters.\nWe further provide an analysis of the impact of random sampling on compression evaluation in Appendix C  ###reference_###.\n###figure_5### ###figure_6### ###figure_7### ###figure_8### ###figure_9### The previously discussed experiments focus exclusively on tasks related to a single ability. Nevertheless, numerous tasks require abilities in multiple aspects. For example, incorporating programming as an intermediate step for reasoning has been shown to enhance performance in mathematical tasks, requiring abilities in both coding and mathematical reasoning (Gao et al., 2023b  ###reference_b21###; Chen et al., 2023  ###reference_b12###). Following  Gao et al. (2023b  ###reference_b21###), we apply the Program-Aided Math Reasoning (PAL) method to GSM8K. To align with the abilities involved,we combined the Python and ArXiv-Math corpora to create a mixed corpus. As illustrated in Figure 8  ###reference_###, this mixed corpus exhibits a stronger linear correlation than using either the Python or ArXiv-Math corpora alone."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Main Results \u2013 Compression Represents Intelligence Linearly",
            "text": "Recent advancements in AI have revolutionized art generation, blending creativity with machine learning. Techniques such as Generative Adversarial Networks (GANs) have enabled the creation of art by generating human-like paintings and music compositions. Notably, DALL-E and Midjourney have shown the potential of AI to produce imaginative and unique artworks, by interpreting text prompts and manifesting them into visual masterpieces. These tools exemplify how AI is reshaping artistic expression, pushing boundaries between human creativity and machine interpretation, and opening new avenues for artistic exploration."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Remaining Questions",
            "text": "In the previous experiments, we intentionally selected the appropriate compression corpora aligning with the area of focus. Herein, we further explore the effects of diverse compression corpora on the results. We investigate two representative examples where the compression corpus does not align with the specific domain, to study whether an arbitrary compression corpus would suffice for a strong linear correlation: (1) mathematical reasoning benchmarks v.s. compression on Common Crawl data, and (2) knowledge and commonsense benchmarks v.s. compression on GitHub code data. We present the findings in Figure 7  ###reference_###  ###reference_###.\nDue to a substantial ability mismatch between the compression corpus and the benchmarks, the linear correlation is significantly weaker than that with in-domain data. Coupled with the MMLU case discussed in Appendix E  ###reference_###  ###reference_###, these instances demonstrate that linear correlation relies on the alignment degree between the compression corpus and the specific area of study.\nHowever, we emphasize that our previous findings are relatively robust since our chosen compression corpora as listed in Table 1  ###reference_###  ###reference_### rely on simple heuristics at a coarse granularity without meticulous designing.\nHow to automatically identify the most appropriate compression corpus that correlates with certain benchmarks is an important problem that we leave for future work.\nAs an example, we investigate the effect of size of compression corpus on the knowledge and commonsense ability.\nWe vary the number of characters of the compression corpus from 50K to 100M, observing changes in the Pearson correlation coefficient and the RMSE depicted in Figure 7  ###reference_###  ###reference_###. As the number of sampled characters increases, the linear correlation gradually strengthens and reaches saturation at around 30M.\nThis observation suggests that the compression corpus is sufficiently large with only tens of millions of characters.\nWe further provide an analysis of the impact of random sampling on compression evaluation in Appendix C  ###reference_###  ###reference_###.\n###figure_14### ###figure_15### ###figure_16### ###figure_17### ###figure_18### The previously discussed experiments focus exclusively on tasks related to a single ability. Nevertheless, numerous tasks require abilities in multiple aspects. For example, incorporating programming as an intermediate step for reasoning has been shown to enhance performance in mathematical tasks, requiring abilities in both coding and mathematical reasoning (Gao et al., 2023b  ###reference_b21###  ###reference_b21###; Chen et al., 2023  ###reference_b12###  ###reference_b12###). Following  Gao et al. (2023b  ###reference_b21###  ###reference_b21###), we apply the Program-Aided Math Reasoning (PAL) method to GSM8K. To align with the abilities involved,we combined the Python and ArXiv-Math corpora to create a mixed corpus. As illustrated in Figure 8  ###reference_###  ###reference_###, this mixed corpus exhibits a stronger linear correlation than using either the Python or ArXiv-Math corpora alone."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "Our findings provide evidence for the belief that superior compression is indicative of greater intelligence.\nPractically, our experiments strengthen the rationale to adopt compression efficiency as an unsupervised, flexible, and reliable metric to assess LLMs\u2019 abilities.\nThe compression corpus can be easily updated and composed flexibly, which mitigates the data contamination and benchmark overfitting issues that most evaluations suffer from.\nWhile recent work establishes a large collection of diverse corpora to evaluate LLMs with losses (Magnusson et al., 2023  ###reference_b38###), our work offers strong empirical support for doing so by demonstrating the correlation with intelligence.\nOur study admits several limitations.\nFirst, we only focus on base models because fine-tuned models are not general-purpose compressors for arbitrary text. However, we posit that there are still interesting relationships to be explored about the base model compression efficiency and the benchmark scores of the corresponding fine-tuned models.\n Yuan et al. (2023  ###reference_b59###) initially provides evidence of this correlation in mathematical reasoning ability, albeit limited to LLaMA models.\nSecond,\nour focus lies on the short- to medium-context regimes, deferring the examination of long-context scenarios.\nThird, our conclusion and findings may only apply to well-trained models and not hold for LMs where the evaluated abilities have not emerged yet.\nWe leave study of these problems for future work."
        }
    ],
    "url": "http://arxiv.org/html/2404.09937v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.2"
        ],
        "main_experiment_and_results_sections": [
            "3.1",
            "3.2",
            "4",
            "4.1"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.3",
            "4",
            "4.1",
            "4.2"
        ]
    },
    "research_context": {
        "paper_id": "2404.09937v1",
        "paper_title": "Compression Represents Intelligence Linearly",
        "research_background": "### Paper's Motivation\nThe motivation for this paper stems from the longstanding hypothesis that there is a close connection between compression and intelligence. This idea is particularly relevant now due to significant advancements in large language models (LLMs) and their role in artificial intelligence. Theoretical discussions have suggested that predictive models (like LLMs) can be transformed into lossless compressors, leading to the notion that effective data compression may be indicative of greater intelligence. Despite these theoretical underpinnings, empirical evidence supporting this relationship has been sparse.\n\n### Research Problem\nThe core research problem addressed in this paper is to empirically investigate whether a language model's ability to compress a text corpus efficiently can serve as a reliable indicator of its intelligence. The research seeks to answer: If a language model can encode a text corpus with fewer bits in a lossless manner, does that indicate greater intelligence? Intelligence, in this context, is defined practically, focusing on the model\u2019s performance in completing a variety of downstream tasks.\n\n### Relevant Prior Work\nSeveral works form the foundation of this research:\n1. **Equating Compression and Intelligence**: Previous scholars like Hern\u00e1ndez-Orallo & Minaya-Collado (1998), Mahoney (1999), Legg et al. (2005), Hutter (2006), and Legg & Hutter (2007) have discussed the conceptual similarity between compression and intelligence.\n2. **Empirical Studies on Language Models**: Deletang et al. (2024) empirically demonstrated the strong data compression capabilities of LLMs, laying the groundwork for exploring their intelligence through the lens of compression.\n3. **Benchmark Scores and Compression Metrics**: Recent studies by Wei et al. (2023), Gadre et al. (2024), and Du et al. (2024) examined the relationship between benchmark scores and compression-equivalent metrics within specific model series, providing a partial basis for this paper's broader investigation.\n\n### Contribution of the Paper\nThe paper distinguishes itself by conducting a comprehensive study across 30 public LLMs and 12 diverse benchmarks, thus providing a generalized empirical basis for the hypothesis. This study uncovers a nearly linear correlation between a model's downstream ability and its compression efficiency, establishing this relationship as a universal principle. The findings advocate for using compression efficiency as a stable, flexible, and reliable metric to evaluate the intelligence of LLMs.",
        "methodology": "**Methodology:**\n\n**Objective:** \nThe primary goal of this study is to empirically examine the relationship between compression efficiency and the intelligence of large language models (LLMs). \n\n**Approach:** \n1. **Model Selection:** The study involves investigating various pretrained LLMs of different sizes from diverse organizations. These models differ in training data, tokenizers, computation, and architectures (including mixture of experts as in MistralAI, 2023), ensuring that the findings are generalizable and not specific to any particular model series.\n  \n2. **Metrics:**\n   - **Compression Efficiency:** The average number of bits per character (BPC) is used as the primary metric to measure compression efficiency. This is chosen over the average bits per token due to the variety of tokenizers different models may use. BPC is derived from the average coding length per message of compressing a given corpus.\n   - **Intelligence Evaluation:** Intelligence is judged based on performance in downstream benchmark tasks, which generally involve inputs not exceeding 2048 tokens. Therefore, while assessing compression, the context window size is unified to 1900 tokens to ensure consistency and fairness across models.\n\n3. **Context Window:** \n   - **Standardization:** Context window sizes are standardized to 1900 tokens for all evaluations.\n   - **Sliding Window Approach:** A sliding window approach is used for a more accurate evaluation of compression, which is detailed in Appendix A.1.\n\n4. **Stages of Model Development:** \n   - Only base models (those obtained after pre-training but before alignment) are considered. This is because fine-tuned models are specialized for (query, response) data rather than general-purpose text compression. Moreover, intelligence is believed to be mainly learned during the pre-training stage.\n\n5. **Corpus Selection for Compression Measurement:** \n   - **Diversity:** Different corpora highlight different aspects of model abilities, so a varied set of corpora aligned with specific focus areas is chosen.\n   - **Recent Data:** To avoid data leakage due to overlap with pre-training datasets whose contents are often opaque, the latest available data is used. \n   - **Specific Corpora:** Corpus selection includes:\n     - Common Crawl dataset for assessing knowledge and commonsense.\n     - GitHub repositories mainly focused on Python for evaluating coding ability.\n     - Academic papers from ArXiv labeled with \u201cmath\u201d for mathematical reasoning.\n\n**Data Collection and Processing:** Detailed protocols for data collection and processing are provided in Appendix A.2.\n\n**Conclusion:** \nThis methodology aims to provide robust insights into the relationship between compression and intelligence by using a diverse and recently-sourced dataset, standardizing evaluation conditions, and focusing on the pre-trained (base) models of LLMs.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\n#### Main Experiment Setup:\n- **Objective**: The main objective is to study \"intelligence\" focusing on three key abilities: knowledge and commonsense, coding, and mathematical reasoning.\n- **Datasets**: The evaluation uses well-recognized benchmarks for each of these areas:\n  - **Knowledge and Commonsense**: Specific benchmarks for assessing general knowledge and commonsense intelligence.\n  - **Coding**: Benchmarks designed to evaluate coding skills.\n  - **Mathematical Reasoning**: Benchmarks focusing on mathematical problem-solving abilities.\n- **Evaluation Strategy**: The assessment is based on the average performance of the models across multiple downstream tasks, in line with the practical approach of defining intelligence as the capacity to achieve goals across a broad array of scenarios.\n  \n#### Baselines:\n- The baselines include other large language models and their respective evaluations, referencing prior works that showcase model potential through average benchmark performances (Touvron et al., 2023a; b; Jiang et al., 2023; Team et al., 2023; Anthropic, 2024).\n\n#### Evaluation Metrics:\n- **Average Benchmark Scores**: The primary evaluation metric is the average score across the domains of knowledge and commonsense, coding, and mathematical reasoning, reflecting domain-specific intelligence.\n  \n#### Main Experimental Results:\n- The paper uses the benchmark scores to indicate the specific domain intelligence of the models. The results are indicative of how well models perform across diverse tasks, synthesizing overall intelligence through average performance metrics.\n"
    },
    "reference_ablation_studies": [
        {
            "research_objective": "LLMs may be overoptimized towards certain benchmarks, making their scores unreliable indicators of intelligence. This study seeks to detect and address such overfitting.",
            "experiment_process": "The study adopts the MIN-K% PROB method to detect potential overfitting. This approach involves calculating the average negative log-likelihood of the least probable tokens in a test example to determine if it was likely present during the model's pretraining. The researchers compute this score for both test and training splits of various benchmarks to identify any abnormalities.",
            "result_discussion": "The results indicate that certain LLMs show unusually low MIN-K% Scores on specific benchmarks, suggesting potential overfitting or exposure to test data during pretraining. These findings underline the need for careful evaluation and suggest that overfitting significantly impacts perceived model intelligence.",
            "ablation_id": "2404.09937v1.No1"
        },
        {
            "research_objective": "To examine the correlation between compression efficiency and intelligence, using LLMs' ability to compress text as an indicator of their performance across various benchmarks.",
            "experiment_process": "Eight series of general-purpose language models, including dense transformers and a sparse mixture-of-experts (MoE) model, were evaluated on benchmarks related to knowledge and commonsense, coding, and mathematical reasoning. Models were assessed through few-shot or zero-shot learning, and compression efficiency was measured using bits per character (BPC). Pearson correlation and linear regression were employed to quantify and visualize the relationship between compression and benchmark scores.",
            "result_discussion": "The study found a highly linear correlation between compression efficiency and intelligence, with a Pearson coefficient of -0.94 and an RMSE of 2.8%. This suggests that LLMs' ability to compress data effectively is a reliable indicator of their intelligence, regardless of model architecture. The correlation held across different benchmarks and specific abilities, though slight variations were observed due to overfitting and domain alignment issues in mathematical reasoning.",
            "ablation_id": "2404.09937v1.No2"
        },
        {
            "research_objective": "To explore the impact of mismatched compression corpora on the correlation between compression efficiency and model performance on specific benchmarks.",
            "experiment_process": "The study investigated the effects of using unrelated compression corpora, such as Common Crawl data for mathematical reasoning benchmarks and GitHub code data for knowledge and commonsense benchmarks. Additionally, the impact of corpus size on correlation was examined by varying the number of characters from 50K to 100M and observing changes in Pearson correlation coefficient and RMSE.",
            "result_discussion": "When using mismatched compression corpora, the linear correlation between compression efficiency and benchmark scores weakened significantly, highlighting the importance of corpus alignment. The study also found that larger compression corpora enhanced correlation accuracy, suggesting that tens of millions of characters are sufficient for reliable evaluation.",
            "ablation_id": "2404.09937v1.No3"
        },
        {
            "research_objective": "To investigate the correlation between compression efficiency using mixed-domain corpora and models' abilities on tasks requiring multiple skills, such as programming and mathematical reasoning.",
            "experiment_process": "Following Gao et al. (2023b), the researchers applied the Program-Aided Math Reasoning (PAL) method to GSM8K. They created a mixed compression corpus combining Python and ArXiv-Math text. The study analyzed the correlation between this mixed corpus's compression efficiency and performance on mathematical reasoning benchmarks.",
            "result_discussion": "The mixed compression corpus exhibited a stronger linear correlation with mathematical reasoning benchmarks compared to single-domain corpora. This indicates that using comprehensive, mixed-domain corpora can better capture the multifaceted abilities required for complex tasks.",
            "ablation_id": "2404.09937v1.No4"
        }
    ]
}