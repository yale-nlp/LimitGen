{
    "title": "An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models",
    "abstract": "In this study, we identify the inefficient attention phenomena in Large Vision-Language Models (LVLMs), notably within prominent models like LLaVA-1.5, QwenVL-Chat and Video-LLaVA. We find out that the attention computation over visual tokens is of extreme inefficiency in the deep layers of popular LVLMs, suggesting a need for a sparser approach compared to textual data handling. To this end, we introduce FastV, a versatile plug-and-play method designed to optimize computational efficiency by learning adaptive attention patterns in early layers and pruning visual tokens in subsequent ones.\nOur evaluations demonstrate FastV\u2019s ability to dramatically reduce computational costs (e.g., a 45% reduction in FLOPs for LLaVA-1.5-13B) without sacrificing performance in a wide range of image and video understanding tasks. The computational efficiency and performance trade-off of FastV are highly customizable and pareto-efficient. It can compress the FLOPs of a 13B-parameter model to achieve a lower budget than that of a 7B-parameter model, while still maintaining superior performance. We believe FastV has practical values for the deployment of LVLMs in edge devices and commercial models. Code is released at https://github.com/pkunlp-icler/FastV.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large Vision-Language Models (LVLMs) have become a hit in both computer vision and natural language processing studies. We have witnessed tremendous creative research and applications that are built upon powerful LVLMs Liu et al. (2023c  ###reference_b24###; 2024a  ###reference_b22###); Team et al. (2023  ###reference_b33###); Bai et al. (2023  ###reference_b2###). From describing the given picture to navigating the internet Zheng et al. (2024  ###reference_b42###), using smartphones Wang et al. (2024  ###reference_b36###) and making decision in the real world Driess et al. (2023  ###reference_b8###); Chen et al. (2024  ###reference_b5###), large language models with vision abilities is reshaping how we interact with AI systems, which can not be achieved solely by language or vision uni-modal models.\nCurrently, a majority of popular LVLMs rely on sequential visual representation, where images are transformed to hundreds or thousands of tokens when feeding to LLM together with language prompts OpenAI (2023  ###reference_b29###); Zhu et al. (2023  ###reference_b43###); Liu et al. (2023c  ###reference_b24###; b  ###reference_b23###; 2024b  ###reference_b25###); Zhao et al. (2023  ###reference_b41###); Bai et al. (2023  ###reference_b2###); Li et al. (2023d  ###reference_b16###; c  ###reference_b15###); Lin et al. (2023b  ###reference_b20###).\nAs LVLMs leverage the advanced emergent capabilities inherent in their language components, they concurrently face a surge in computational complexity, correlating with budget increments. This complexity stems from the principle that the proficiency of Large Language Models (LLMs) is predominantly influenced by their scale. Two critical areas remain under-explored in this context: 1) How do language models process and interpret images? and 2) While the efficient training and inference of LLMs have attracted considerable attention, these dimensions within LVLMs are yet to be thoroughly examined and understood.\nIn this paper, we uncover the fact that current LVLMs actually apply an inefficient way while processing image information.\nSpecifically, the image tokens receive strikingly lower attention scores compared to their textual counterparts within the token-based LVLMs like LLaVA. The degree of imbalance also varies between the shallow and deep layers.\nFrom our experiments a wide range of vision-language tasks, we observed that within the deep layers (after layer 2) of renowned LVLMs such as LLaVA 1.5,\nimage tokens receive an average attention score that amounts to only 0.21% of the score attributed to system prompts.\nIn contrast, this figure reaches 50% in the initial two layers.\nThese observations raise questions upon the optimal utilization of visual information within LVLMs.\nTo address the problem, we assume a plausible explanation is that the high redundancy in visual signals leads to the aggregation of image-related, instruction-specific features onto certain \u201canchor\u201d tokens through the self-attention mechanism in the shallow layers. Notably, these anchor tokens might not be image tokens. In deep layers, attentions are focused on those anchor tokens, leading to significantly reduced attention on the image tokens themselves.\nThe phenomena inspires us to propose FastV, a dynamic image tokens pruning method to reduce the inference budget of LVLMs. Our findings suggest an intriguing possibility: Given that image tokens contribute minimally to output generation in deeper layers due to diminished attention, why not consider removing them at these stages? FastV implements an image token pruning strategy at one specific layer of LLM. Prior to this layer, computations proceed as usual. Beyond this selected layer, image tokens are re-evaluated based on their average received attention scores. Tokens falling below a predefined attention score threshold are then selectively discarded in subsequent layers, streamlining the process by focusing on the most impactful tokens.\nCompared to other attention-based methods for accelerating inference, such as sparse attention, FastV\u2019s most notable distinction lies in its direct elimination of tokens. This approach not only bypasses the computational demand of the self-attention module but also the Feed-Forward Network (FFN) module in deeper layers. As a result, FastV achieves a great theoretical reduction in FLOPs while maintaining relatively high performance as shown in Figure 1  ###reference_###\u2019s experiment on LLaVA and Qwen-VL-Chat models. Our experiment on LLaVA-1.5-13B model shows that we can filter out 50% image tokens after layer 2 without sacrificing the average performance on a combination of Vision-Language tasks including captioning tasks like Nocaps Agrawal et al. (2019  ###reference_b1###), Flickr30K Plummer et al. (2015  ###reference_b30###), multimple choice tasks like A-OKVQA Schwenk et al. (2022  ###reference_b32###), MMMU Yue et al. (2023  ###reference_b40###), complex embodied reasoning task like PCA-Bench Chen et al. (2024  ###reference_b5###; 2023  ###reference_b4###), tasks requiring detailed OCR ablitily like OCR-VQA Mishra et al. (2019  ###reference_b28###) and more challenging video understanding tasks Jang et al. (2017  ###reference_b10###); Xu et al. (2017a  ###reference_b38###; b  ###reference_b39###).\nOur latency test experiment on A-OKVQA showed that LLaVA-13B model with FastV could achieve a lower latency than LLaVA-7B model while maintaining superior performance. This result highlights the effectiveness of FastV in balancing the trade-off between speed and accuracy in LVLMs.\nResearches Liu et al. (2023c  ###reference_b24###); Li et al. (2023f  ###reference_b18###) underscore the significance of enhancing image resolution for the performance of LVLMs. However, it\u2019s equally important to note that increased resolution comes with its own challenges, including a rise in the computational budgets such as longer image token sequence and inference latency. We also conduct experiments on training LVLM in different image feature resolution by setting pooling layer of different strides. Specifically, with an equal number of image tokens, models equipped with FastV can process higher resolution images, leading to better performance than models limited to lower resolution features. This finding highlights the potential to enhance downstream performance by increasing image resolution without incurring additional inference costs.\nIn summary, the contribution of the work are three-folds:\nIdentify and analyze the inefficient visual attention phenomena in prevailing LVLMs.\nPropose FastV, a plug-and-play method to significantly reduce inference budget for LVLMs without sacrificing performance inspired by our observation.\nValidate the effectiveness of FastV on a wide range of vision-language tasks across different LVLMs with thorough ablations."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "To benefit from the advancement of LLM and integrate visual information into the LLM, large Vision-Language Models utilize a Visual Prompt Generator Li et al. (2023a  ###reference_b13###) to transform the visual embeddings into prompts that the language model can comprehend Li et al. (2023b  ###reference_b14###); Liu et al. (2023c  ###reference_b24###), resulting in a significant increase in required tokens. Handling higher resolution images inevitably necessitates an exponential increase in the number of needed tokens. For instance, LLAVA process 336x336 images into 576 tokens Liu et al. (2023b  ###reference_b23###) and process images with a greater resolution of 672x672 into 2304 tokens Liu et al. (2024b  ###reference_b25###). Fuyu Bavishi et al. (2023  ###reference_b3###), in a similar vein, translates pixel-level images of 1080x1080 into 1296 tokens. Understanding and generating multiple images or videos also inherently demands an escalated count of tokens for vision information. Both Video-Poet Kondratyuk et al. (2023  ###reference_b11###) and Unified-IO2 Lu et al. (2023  ###reference_b26###) are compelled to reserve thousands of tokens within the context to facilitate the understanding and generation of multiple images or videos. Large multimodal models like Gemini Team et al. (2023  ###reference_b33###) and LWM Liu et al. (2024a  ###reference_b22###) highlights the significance of long context in developing a robust understanding of the world model and extending the context length to 1M to address the issue of escalating context requirements. Nonetheless, considering the limitations of computing resources, this poses a significant challenge in exploiting the available resources for this purpose optimally. Further research and development in computational technologies are needed to surpass these obstacles and fully unlock the potential of LVLMs. Efficient inference in LLMs is challenged by their autoregressive generation where each token prediction depends on the preceding context. Hence, considering the quadratic complexity of computation\u2019s attention during training, as the context length increases, the generation becomes progressively slower. To tackle these challenges, pioneering studies fall into two categories: methods optimizing memory consumption for attention module like FlashAttention, vLLM and RingAttention Dao et al. (2022  ###reference_b7###); Dao (2023  ###reference_b6###); Kwon et al. (2023  ###reference_b12###); Liu et al. (2023a  ###reference_b21###), which ensure no drastic shifts in the results, and methods like StreamingLLM and FastGen Xiao et al. (2023  ###reference_b37###); Ge et al. (2024  ###reference_b9###) that simplify computations by pruning redundant attention computation. We are interested in the second kind of methods since they are proposed inspired by the distinct attention patterns observed in LLM\u2019s inference. While these methods have boosted the inference efficiency of LLMs, they are designed for text-only language models, and whether their effectiveness can be transferred to LVLMs remain under-explored. There is previous work attempt to handle the long-context in LVLMs efficiently, like LLaMA-VID Li et al. (2023e  ###reference_b17###), which utilizes cross-attention to effectively represent each video frame with two key tokens, the requirement for an additional fine-tuning stage obstructs its broad applicability for different LVLMs.\n\nMachine learning in healthcare diagnostics holds transformative potential, leveraging AI for accurate disease identification. Esteva et al. (2017) demonstrated deep learning\u2019s proficiency in dermatology, while Hannun et al. (2019) showed promise in cardiology. Recent advances underline AI's enhancement of diagnostic accuracy across diverse medical fields, heralding a new era in healthcare."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Inefficient Visual Attention in VLLMs",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Preliminaries",
            "text": "In this section, we delve into how LVLMs process visual tokens during output generation from the perspective of self-attention module. For an image-question pair , the given LVLM , usually in the structure of transformer Vaswani et al. (2017  ###reference_b34###) decoder, predicts the answer  in an auto-regressive manner:\n###figure_1### Multimodal information, encompassing both images and text, is transformed into sequential embeddings prior to being processed by the transformer model. For images, a commonly used approach is to employ a pretrained encoder, such as CLIP-VIT Radford et al. (2021  ###reference_b31###), to extract visual features. These features are then linearized by eliminating the spatial dimension. Additional linear transformations Zhu et al. (2023  ###reference_b43###); Liu et al. (2023b  ###reference_b23###) or cross-attention Li et al. (2023b  ###reference_b14###); Bai et al. (2023  ###reference_b2###) modules are utilized to adjust the size of the visual features to match the embedding size of the Large Language Model (LLM) and to achieve semantic alignment. Regarding text, a tokenizer breaks down the natural language into discrete tokens and then performs an embedding lookup to form text embeddings. In the rest of the paper, we refer to \u2019visual tokens\u2019 and \u2019text tokens\u2019 not merely as the discrete units of visual and textual data but as the embeddings derived from these units.\nAs illustrated in Figure 2  ###reference_###, after preprocessing the image and text token to a unified embedding space, they are fed to the transformer decoder to generate output tokens. The input tokens at each decoding step can be categorized into four distinct types: system prompt (sys), image tokens (img), user instruction (ins), and output tokens (out). The system prompts for LVLMs usually inherit the backbone LLM, used as a general message to control the LLM\u2019s behavior, which is decided during the instruction tuning stage of LLM. Image tokens are the linearized image features transformed by a pretrained vision encoder. User instruction specifies the query question for the given image. Output tokens are generated step by step conditioned on the preceding tokens."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Experiment Settings",
            "text": "To explore how LVLMs process image tokens, we first randomly sample  image-text pairs  from a combination of vision langauge tasks including image caption (Flickr30K), embodied reasoning (PCA-Bench), visual question answering (A-OKVQA), multimodal understanding and reasoning (MMMU) and then prompt the LVLM to generate  responses .\nDuring the decoding process of one response, we collect each output tokens\u2019 attention score distribution  in different layers and sum up for different type of input tokens. That is, for the -th token, in the -th layer, we compute  to denote the total attention score current token attends to the system prompt, image tokens, user instruction and output tokens. We have:\nWe compute the total attention allocation  to denote the total attention score one type of tokens received in one layer. For example, the total attention of system prompt in layer  is:\n###figure_2### where  is the number of tokens in the response. Final attention allocation is averaged over all attention heads in the  image-text pairs we sampled.\nNext, we define metric attention efficiency  to denote the average attention score per type\u2019s token received in one layer during the decoding process of one response. For example, the attention efficiency of image tokens in layer  is:\nwhere  is the number of image tokens,  is the number of tokens in the response. Final attention efficiency is averaged over all attention heads in the  image-text pairs we sampled.\nIn our experiment,  is set to 1000 and we use LLaVA1.5-7B as the LVLM. We follow the same generation configuration as the original paper Liu et al. (2023c  ###reference_b24###)."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Results",
            "text": "We have two major findings in the attention pattern statistics regrading attention allocation  and attention efficiency  for different type of input tokens. We define the first 2 layers as shallow layer and the rest 30 layers as deep layers.\nBoth attention allocation and attention efficiency show different degree of imbalance, which is related to the layer depth. The average attention allocation and efficiency in different layer is shown in Figure 3  ###reference_###. In shallow layer the attention allocation is relatively more balanced than in deep layers. In shallow layer, the output tokens tends to attend to the previous output tokens while in deep layers, they tend to attend to the system prompt.\nImage tokens have the lowest attention efficiency in both shallow and deep layers. System prompt is of extremely high attention efficiency in deep layers, which is 472 times that of image tokens, taking up 85% total attention scores.\n###figure_3###"
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Insights",
            "text": "The statistics reveal a surprising trend in the decoding process of LVLMs: despite accounting for the majority of tokens in the input, image tokens receive significantly less attention.\nConversely, system prompts, which provides the minimal semantic information, attract the most of the attention scores.\nTo delve deeper into this phenomenon, we analyze the attention maps of the first, middle, and last layers during during the decoding process of a model response as shown in Figure 4  ###reference_###. The attention maps for all layers are provided in figure-7 of the supplement material.\nFrom the attention visualization results, we can see that in shallow layer, the attention scores distribute more smoothly across different tokens. While in deep layer, there are vertical strong lines (in the system prompt) that takes up most of attention scores. The existence of vertical strong line shows that there are some input tokens that consistently received high attention during the whole decoding process. This also explains the highly imbalanced attention efficiencies in our statistics: A small portion of anchor tokens draw the most attention and the model much favors to attend to those anchor tokens in deep layers. Much attention is aggregated to the beginning system prompt, which leads to the severe inefficient visual attention in LVLMs. Our findings also align with the attention sink phenomena of Large Language Model found in  Xiao et al. (2023  ###reference_b37###)."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "FastV",
            "text": "With insights from the validated phenomena and explanation, we propose FastV as a solution to reduce the inference budgets of LVLMs without sacrificing the performance.\n###figure_4###"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Dynamically Prune Vision Tokens",
            "text": "Figure 5  ###reference_### illustrates the general idea of FastV. The key is the image token re-rank and filtering module. It consists of one ranking function  and two parameters: filtering layer  and filtering ratio . At layer  of the LVLM, the ranking function  takes a sequence of input tokens and rank them by certain importance criteria . The last  tokens after ranking would be pruned out in successive layers. We simply compute the average attention-score one token received from all other tokens as the criteria  in our experiment.\nIn extreme condition,  could be also set to 0, that image tokens are pruned before sending to the language model, we use random ranking as the criteria  where image tokens are randomly dropped.\nFastV is plug-and-play to different token-based LVLMs for various vision language tasks without the need of training the model. We take video understanding tasks with VideoLLaVA Lin et al. (2023a  ###reference_b19###) as example as shown in Figure 5  ###reference_###."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Computing Budget Estimation",
            "text": "###figure_5### We consider the computation of multi-head attention (MHA) and feed-forward network (FFN) module in the FLOPs estimation. For one transformer layer, assume  is the token number,  is the hidden state size,  is the intermediate size of FFN, the total FLOPs can be estimated by . For the whole model, assume FastV prunes tokens from  to  after layer  and there are T layers at all. The theoretical FLOPs reduction ratio related to image tokens is computed as:\nWe plot a 3D graph to show how the FLOPs reduction ratio changes with FastV\u2019s parameter  and  in Figure 6  ###reference_###."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Comparison: Training With Less Visual Tokens",
            "text": "FastV achieves computation reduction through eliminating redundant visual tokens during inference stage. An alternative method to reduce visual tokens is directly training with less visual tokens. This could be simply done by conducting pooling on the output of visual encoder during LVLM\u2019s training process. We compare FastV and this method in our ablation studies (sec. 5.4  ###reference_###)."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiment",
            "text": "Image caption requires the model to generate a description for a given image. We choose Nocaps Agrawal et al. (2019  ###reference_b1###) and Flickr30k Plummer et al. (2015  ###reference_b30###) as benchmarks and report CIDEr score Vedantam et al. (2015  ###reference_b35###) as metric.\nVQA requires the model to generate an answer for a given image-question pair. We select the development set of A-OKVQA Schwenk et al. (2022  ###reference_b32###) and the test set of OCR-VQA  Mishra et al. (2019  ###reference_b28###) as the benchmark and the report the multiple choice (MC) score of AOKVQA and Rouge-L score of OCR-VQA.\nCompared with VQA, multimodal reasoning requires more advanced perception, knowledge and reasoning skills of the model, which are more suitable benchmarks to evaluate the integrated abilities of LVLMs. We choose MMMU and PCA-Bench Chen et al. (2024  ###reference_b5###) as benchmarks. MMMU is a multimodal benchmark featuring multi-discipline tasks\ndemanding college-level subject knowledge and reasoning skills. PCA-Bench is a complex embodied reasoning benchmark with error localization, which features three different domains including autonomous driving, robot and game. We report the multiple choice accuracy for the development set of MMMU and Perception, Cognition, Action, Genuine PCA scores for both the open and closed test set of PCA-Bench.\nSimlar to VQA for single image, Video Question Answering requires the model to generate answer given a video-question pair. Current LVLMs usually deal with video question answering tasks by sampling multiple frames as input, resulting in longer image token sequences. We choose TGIF-QA Jang et al. (2017  ###reference_b10###), MSVD-QA Xu et al. (2017b  ###reference_b39###) and MSRVTT-QA Xu et al. (2017a  ###reference_b38###) as benchmarks following the evaluation pipeline of Video-ChatGPT Maaz et al. (2023  ###reference_b27###) and report the accuracy and chatgpt-score as metrics. We use the first 1K examples in each benchmark in our experiments due to the limited commercial API usage in evaluation.\nThe performance on tasks under different FastV settings are shown in Table 1  ###reference_### (Nocaps, Flickr30k, A-OKVQA, MMMU) and Table 3  ###reference_### (PCA-Bench, OCR-VQA). The result of latency test is shown in Table 2  ###reference_###.\nIn Table 1  ###reference_###, we present the performance trend with FLOPs ratio ranging from 19% to 100% by FastV, for different type and size of models. We also plot the relation between FLOPs Reduction ratio (1-FLOPs Ratio) and average performance in Figure 1  ###reference_###. The results indicate that FastV (K=2, R=50%) could achieve about 45% FLOPs reduction for different LVLMs without sacrificing the performance. The FLOPs-Performance trade-off is is also highly adjustable by lowering  and increasing  if we want to pursue an ultimate speed up. As shown in the latency test (Table 2  ###reference_###), an 13B model with FastV could inference as fast as a 7B model with superior performance for A-OKVQA.\nIn PCA-Bench and OCR-VQA, (Table 3  ###reference_###), which runs finegrained analysis on perception, cognition, action and OCR abilities, we find that FastV (K=2, R=50%) could maintain the sub-scores while significantly decreasing the FLOPs.\nThe results of FastV on different video question answering tasks in shown in table 4  ###reference_### (TGIF, MSVD, MSRVTT). To our surprise, we find FastV could generally improves the Video-QA tasks performance while saving 40%+ computations especially for the TGIF task. We think the main reason is that the redundancy information problem is more severe for video understanding as multiple images from the video are transformed to tokens when sending to the LLM. For example, an image costs 576 tokens in LLaVA1.5 model, while a video costs 2048 tokens in Video-LLaVA. As shown in the case from Figure 5  ###reference_###, setting suitable FastV parameters could lead to much FLOPs reduction for Video-LLaVA while the outputs are nearly identical.\nConsidering the parameters of FastV, a lower  and higher  would generally result in a more significant FLOPs reduction with relative performance drop. Above from the decreased inference budget, it is interesting to note that sometimes FastV could lead to better performance than vanilla decoding. This\nbolsters our theory that FastV can be seen as a redundancy reduction operation.\nWe conduct an ablation experiment on how the parameters (K and R) influence the acceleration and downstream task\u2019s performance. We select OCR-VQA as the task, which necessitates a through understanding of the image. The result is shown in Figure 7  ###reference_###. When K is small, lowering R would improve the performance with a smaller FLOPs reduction ratio. In contrast, when K is large, adjusting R has minimal impact on the overall performance. This observation further proves that in deep layers, there is high redundancy in image tokens.\nFastV reduces computational requirements (FLOPs) by pruning tokens during the inference stage. An alternative approach for token reduction involves training the LVLM at a lower resolution. To facilitate a fair comparison, we retrained two LLaVA1.5-7B models, adhering to the original pretraining and supervised finetuning protocols. The sole modification in the second model\u2019s training process was the incorporation of an average pooling layer (with a stride of 2) following the Clip encoder, leading to a 50% reduction in image tokens during training. A comparison between lines (a) and (b) in Table 5  ###reference_### reveals that reducing the input resolution directly during training results in diminished performance. Conversely, FastV manages to decrease the number of image tokens without compromising performance, showcasing its efficiency in balancing computational savings with model efficacy.\nFastV strategically reduces the number of image tokens during the inference phase of LVLMs, motivated by our observation that image tokens exhibit the lowest attention efficiency relative to other types of input tokens. In experiments detailed in lines (d) and (f) of the study, we specifically pruned tokens that were not related to images, such as system prompts and instruction tokens. This selective pruning resulted in significant performance declines, even when only a minimal number of non-image tokens were removed. We also compare randomly drop visual tokens instead of dropping by attention rank, as shown in line (c). It resulted in declined results compared with origin FastV (b). These findings underscore the distinct roles that visual and textual tokens play within LVLMs. It highlights FastV\u2019s effectiveness in precisely targeting image tokens for reduction, thereby optimizing performance without compromising the model\u2019s overall functionality.\nIn our previous observation about attention efficiency, we find out that the system prompt takes up of most attention even if they carry the least semantic information in the context. We conduct another experiment by directly prune the first half tokens of the system prompt. Comparing line (d) and (e), we can find that the head tokens in the system prompt have dominant effect on the model performance. Our findings also align with StreamingLLM Xiao et al. (2023  ###reference_b37###) where they find that the first 4 tokens in LLM play the most important role during inference.\n###figure_6### As we find that LVLM and LLM bear similar attention patterns that the head tokens play an important role during inference, we wonder that whether the same attention optimization technique curated for LLM can be transfered to LVLM. Following the methodology outlined in StreamingLLM Xiao et al. (2023  ###reference_b37###), we implemented an attention optimization scheme that incorporates an attention sink for the first four tokens and keep all local attention (context size=4) for the preceding four tokens throughout the decoding process. The experiment\u2019s outcome, presented in line (g) of Table 5  ###reference_###, indicates a substantial degradation in LVLM\u2019s performance when this specific attention pattern is applied. This suggests a fundamental difference in how image tokens, as opposed to text tokens, contribute to the information processing within LLMs. This observation highlights the necessity for further investigation into the unique behaviors of image tokens within the LVLM framework, suggesting that the direct application of LLM attention/KV-Cache optimization mechanisms to LVLMs requires adaptation to accommodate the distinct characteristics of visual information."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Evaluation Tasks",
            "text": "We conduct a wide range of evaluation including image and video understanding tasks to examine the influence of FastV on the performance of LVLMs. We use greedy search for all experiments and provide details on the prompts used for each task in the section A of supplement material.\nImage caption requires the model to generate a description for a given image. We choose Nocaps Agrawal et al. (2019  ###reference_b1###  ###reference_b1###) and Flickr30k Plummer et al. (2015  ###reference_b30###  ###reference_b30###) as benchmarks and report CIDEr score Vedantam et al. (2015  ###reference_b35###  ###reference_b35###) as metric.\nVQA requires the model to generate an answer for a given image-question pair. We select the development set of A-OKVQA Schwenk et al. (2022  ###reference_b32###  ###reference_b32###) and the test set of OCR-VQA  Mishra et al. (2019  ###reference_b28###  ###reference_b28###) as the benchmark and the report the multiple choice (MC) score of AOKVQA and Rouge-L score of OCR-VQA.\nCompared with VQA, multimodal reasoning requires more advanced perception, knowledge and reasoning skills of the model, which are more suitable benchmarks to evaluate the integrated abilities of LVLMs. We choose MMMU and PCA-Bench Chen et al. (2024  ###reference_b5###  ###reference_b5###) as benchmarks. MMMU is a multimodal benchmark featuring multi-discipline tasks\ndemanding college-level subject knowledge and reasoning skills. PCA-Bench is a complex embodied reasoning benchmark with error localization, which features three different domains including autonomous driving, robot and game. We report the multiple choice accuracy for the development set of MMMU and Perception, Cognition, Action, Genuine PCA scores for both the open and closed test set of PCA-Bench.\nSimlar to VQA for single image, Video Question Answering requires the model to generate answer given a video-question pair. Current LVLMs usually deal with video question answering tasks by sampling multiple frames as input, resulting in longer image token sequences. We choose TGIF-QA Jang et al. (2017  ###reference_b10###  ###reference_b10###), MSVD-QA Xu et al. (2017b  ###reference_b39###  ###reference_b39###) and MSRVTT-QA Xu et al. (2017a  ###reference_b38###  ###reference_b38###) as benchmarks following the evaluation pipeline of Video-ChatGPT Maaz et al. (2023  ###reference_b27###  ###reference_b27###) and report the accuracy and chatgpt-score as metrics. We use the first 1K examples in each benchmark in our experiments due to the limited commercial API usage in evaluation."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Model Settings",
            "text": "We test FastV with various open source models. For image understanding tasks, we conduct experiments on LLaVA1.5-7B, 13B111https://github.com/haotian-liu/LLaVA  ###reference_### Liu et al. (2023b  ###reference_b23###), and Qwen-VL222https://github.com/QwenLM/Qwen-VL  ###reference_github.com/QwenLM/Qwen-VL### Bai et al. (2023  ###reference_b2###). When it comes to video understanding tasks, our baseline model is VideoLLaVA333https://github.com/PKU-YuanGroup/Video-LLaVA  ###reference_VA### Lin et al. (2023a  ###reference_b19###). We adopt the settings as reported in their paper for the baseline models."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Main Results",
            "text": "The performance on tasks under different FastV settings are shown in Table 1  ###reference_###  ###reference_### (Nocaps, Flickr30k, A-OKVQA, MMMU) and Table 3  ###reference_###  ###reference_### (PCA-Bench, OCR-VQA). The result of latency test is shown in Table 2  ###reference_###  ###reference_###.\nIn Table 1  ###reference_###  ###reference_###, we present the performance trend with FLOPs ratio ranging from 19% to 100% by FastV, for different type and size of models. We also plot the relation between FLOPs Reduction ratio (1-FLOPs Ratio) and average performance in Figure 1  ###reference_###  ###reference_###. The results indicate that FastV (K=2, R=50%) could achieve about 45% FLOPs reduction for different LVLMs without sacrificing the performance. The FLOPs-Performance trade-off is is also highly adjustable by lowering  and increasing  if we want to pursue an ultimate speed up. As shown in the latency test (Table 2  ###reference_###  ###reference_###), an 13B model with FastV could inference as fast as a 7B model with superior performance for A-OKVQA.\nIn PCA-Bench and OCR-VQA, (Table 3  ###reference_###  ###reference_###), which runs finegrained analysis on perception, cognition, action and OCR abilities, we find that FastV (K=2, R=50%) could maintain the sub-scores while significantly decreasing the FLOPs.\nThe results of FastV on different video question answering tasks in shown in table 4  ###reference_###  ###reference_### (TGIF, MSVD, MSRVTT). To our surprise, we find FastV could generally improves the Video-QA tasks performance while saving 40%+ computations especially for the TGIF task. We think the main reason is that the redundancy information problem is more severe for video understanding as multiple images from the video are transformed to tokens when sending to the LLM. For example, an image costs 576 tokens in LLaVA1.5 model, while a video costs 2048 tokens in Video-LLaVA. As shown in the case from Figure 5  ###reference_###  ###reference_###, setting suitable FastV parameters could lead to much FLOPs reduction for Video-LLaVA while the outputs are nearly identical.\nConsidering the parameters of FastV, a lower  and higher  would generally result in a more significant FLOPs reduction with relative performance drop. Above from the decreased inference budget, it is interesting to note that sometimes FastV could lead to better performance than vanilla decoding. This\nbolsters our theory that FastV can be seen as a redundancy reduction operation."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Ablation Studies",
            "text": "We conduct ablation studies to evaluate the impact of various components within FastV, with the findings detailed in Figure 7  ###reference_### and Table 5  ###reference_###.\nWe conduct an ablation experiment on how the parameters (K and R) influence the acceleration and downstream task\u2019s performance. We select OCR-VQA as the task, which necessitates a through understanding of the image. The result is shown in Figure 7  ###reference_###  ###reference_###. When K is small, lowering R would improve the performance with a smaller FLOPs reduction ratio. In contrast, when K is large, adjusting R has minimal impact on the overall performance. This observation further proves that in deep layers, there is high redundancy in image tokens.\nFastV reduces computational requirements (FLOPs) by pruning tokens during the inference stage. An alternative approach for token reduction involves training the LVLM at a lower resolution. To facilitate a fair comparison, we retrained two LLaVA1.5-7B models, adhering to the original pretraining and supervised finetuning protocols. The sole modification in the second model\u2019s training process was the incorporation of an average pooling layer (with a stride of 2) following the Clip encoder, leading to a 50% reduction in image tokens during training. A comparison between lines (a) and (b) in Table 5  ###reference_###  ###reference_### reveals that reducing the input resolution directly during training results in diminished performance. Conversely, FastV manages to decrease the number of image tokens without compromising performance, showcasing its efficiency in balancing computational savings with model efficacy.\nFastV strategically reduces the number of image tokens during the inference phase of LVLMs, motivated by our observation that image tokens exhibit the lowest attention efficiency relative to other types of input tokens. In experiments detailed in lines (d) and (f) of the study, we specifically pruned tokens that were not related to images, such as system prompts and instruction tokens. This selective pruning resulted in significant performance declines, even when only a minimal number of non-image tokens were removed. We also compare randomly drop visual tokens instead of dropping by attention rank, as shown in line (c). It resulted in declined results compared with origin FastV (b). These findings underscore the distinct roles that visual and textual tokens play within LVLMs. It highlights FastV\u2019s effectiveness in precisely targeting image tokens for reduction, thereby optimizing performance without compromising the model\u2019s overall functionality.\nIn our previous observation about attention efficiency, we find out that the system prompt takes up of most attention even if they carry the least semantic information in the context. We conduct another experiment by directly prune the first half tokens of the system prompt. Comparing line (d) and (e), we can find that the head tokens in the system prompt have dominant effect on the model performance. Our findings also align with StreamingLLM Xiao et al. (2023  ###reference_b37###  ###reference_b37###) where they find that the first 4 tokens in LLM play the most important role during inference.\n###figure_7### As we find that LVLM and LLM bear similar attention patterns that the head tokens play an important role during inference, we wonder that whether the same attention optimization technique curated for LLM can be transfered to LVLM. Following the methodology outlined in StreamingLLM Xiao et al. (2023  ###reference_b37###  ###reference_b37###), we implemented an attention optimization scheme that incorporates an attention sink for the first four tokens and keep all local attention (context size=4) for the preceding four tokens throughout the decoding process. The experiment\u2019s outcome, presented in line (g) of Table 5  ###reference_###  ###reference_###, indicates a substantial degradation in LVLM\u2019s performance when this specific attention pattern is applied. This suggests a fundamental difference in how image tokens, as opposed to text tokens, contribute to the information processing within LLMs. This observation highlights the necessity for further investigation into the unique behaviors of image tokens within the LVLM framework, suggesting that the direct application of LLM attention/KV-Cache optimization mechanisms to LVLMs requires adaptation to accommodate the distinct characteristics of visual information."
        },
        {
            "section_id": "5.5",
            "parent_section_id": "5",
            "section_name": "Limitations",
            "text": "The FLOPs reduction ratio is based on the theoretical calculation considering the removal of image tokens, while actual inference budget can be influenced by a variety of factors such as inference framework optimization, specific CUDA kernels and hardwares. We are working on integrating FastV into mainstream LLM inference frameworks such as vLLM Kwon et al. (2023  ###reference_b12###) for broader application."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we propose FastV, a plug-and-play inference budget optimization method for Large Vision-Language Models. Our insight for FastV arises from our observation that the attention computation over visual tokens is of extreme inefficiency in the deep layers of popular LVLMs though they take up a large portion of input tokens. FastV prunes out the unnecessary visual tokens according to the attention score ranking, which results in significant inference budget reduction without sacrificing performance."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A Prompts for Different Tasks",
            "text": ""
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T1\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Performance/Computation Balance of FastV under different configurations (K for filtering layer, R for filtering ratio). <span class=\"ltx_text\" id=\"S5.T1.3.1\" style=\"color:#FF0000;\">Highest score</span> for each model is in red while the <span class=\"ltx_text\" id=\"S5.T1.4.2\" style=\"color:#0000FF;\">second highest</span> is in blue.</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S5.T1.5\" style=\"width:433.6pt;height:593.8pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(26.2pt,-35.9pt) scale(1.13752713702237,1.13752713702237) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T1.5.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T1.5.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\" id=\"S5.T1.5.1.1.1.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S5.T1.5.1.1.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\" colspan=\"4\" id=\"S5.T1.5.1.1.1.2\">FastV Settings</th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T1.5.1.1.1.3\">Nocaps</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T1.5.1.1.1.4\">Flickr30k</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T1.5.1.1.1.5\">A-OKVQA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T1.5.1.1.1.6\">MMMU</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T1.5.1.1.1.7\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S5.T1.5.1.1.1.7.1\">Avg</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.5.1.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S5.T1.5.1.2.2.1\">K</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.2.2.2\">R</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.2.2.3\">Flops(B)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T1.5.1.2.2.4\">Flops Ratio</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.2.2.5\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T1.5.1.2.2.5.1\">CIDEr</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.2.2.6\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T1.5.1.2.2.6.1\">CIDEr</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.2.2.7\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T1.5.1.2.2.7.1\">Accuracy</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.2.2.8\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T1.5.1.2.2.8.1\">Accuracy</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.5.1.3.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S5.T1.5.1.3.3.1\" rowspan=\"13\"><span class=\"ltx_text\" id=\"S5.T1.5.1.3.3.1.1\">LLaVA-1.5-7B</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"2\" id=\"S5.T1.5.1.3.3.2\">Baseline</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.5.1.3.3.3\">99.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T1.5.1.3.3.4\">100%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.5.1.3.3.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.5.1.3.3.5.1\" style=\"color:#FF0000;\">99.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.5.1.3.3.6\"><span class=\"ltx_text\" id=\"S5.T1.5.1.3.3.6.1\" style=\"color:#0000FF;\">67.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.5.1.3.3.7\">76.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.5.1.3.3.8\"><span class=\"ltx_text\" id=\"S5.T1.5.1.3.3.8.1\" style=\"color:#0000FF;\">34.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.5.1.3.3.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.5.1.3.3.9.1\" style=\"color:#FF0000;\">69.8</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.5.1.4.4\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S5.T1.5.1.4.4.1\">2</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.4.4.2\">90%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.4.4.3\">19.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T1.5.1.4.4.4\">20%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.4.4.5\">72.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.4.4.6\">43.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.4.4.7\">70.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.4.4.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.5.1.4.4.8.1\" style=\"color:#FF0000;\">35</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.4.4.9\">55.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.5.1.5.5\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S5.T1.5.1.5.5.1\">2</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.5.5.2\">75%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.5.5.3\">32.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T1.5.1.5.5.4\">33%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.5.5.5\">94.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.5.5.6\">63.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.5.5.7\">75.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.5.5.8\"><span class=\"ltx_text\" id=\"S5.T1.5.1.5.5.8.1\" style=\"color:#0000FF;\">34.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.5.5.9\">67.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.5.1.6.6\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S5.T1.5.1.6.6.1\">2</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.6.6.2\">50%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.6.6.3\">54.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T1.5.1.6.6.4\">55%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.6.6.5\">99.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.6.6.6\">67.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.6.6.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.5.1.6.6.7.1\" style=\"color:#FF0000;\">77</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.6.6.8\">34.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.6.6.9\"><span class=\"ltx_text\" id=\"S5.T1.5.1.6.6.9.1\" style=\"color:#0000FF;\">69.7</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.5.1.7.7\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S5.T1.5.1.7.7.1\">3</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.7.7.2\">90%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.7.7.3\">22.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T1.5.1.7.7.4\">23%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.7.7.5\">87.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.7.7.6\">55.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.7.7.7\">71.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.7.7.8\"><span class=\"ltx_text\" id=\"S5.T1.5.1.7.7.8.1\" style=\"color:#0000FF;\">34.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.7.7.9\">62.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.5.1.8.8\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S5.T1.5.1.8.8.1\">3</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.8.8.2\">75%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.8.8.3\">34.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T1.5.1.8.8.4\">35%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.8.8.5\">98</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.8.8.6\">65</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.8.8.7\">74.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.8.8.8\">34.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.8.8.9\">68.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.5.1.9.9\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S5.T1.5.1.9.9.1\">3</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.9.9.2\">50%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.9.9.3\">56.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T1.5.1.9.9.4\">57%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.9.9.5\"><span class=\"ltx_text\" id=\"S5.T1.5.1.9.9.5.1\" style=\"color:#0000FF;\">99.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.9.9.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.5.1.9.9.6.1\" style=\"color:#FF0000;\">68.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.9.9.7\">76.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.9.9.8\">34.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.9.9.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.5.1.9.9.9.1\" style=\"color:#FF0000;\">69.8</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.5.1.10.10\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S5.T1.5.1.10.10.1\">5</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.10.10.2\">90%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.10.10.3\">27.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T1.5.1.10.10.4\">28%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.10.10.5\">88.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.10.10.6\">59.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.10.10.7\">70.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.10.10.8\">33.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.10.10.9\">63.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.5.1.11.11\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S5.T1.5.1.11.11.1\">5</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.11.11.2\">75%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.11.11.3\">39.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T1.5.1.11.11.4\">40%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.11.11.5\">98.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.11.11.6\">66.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.11.11.7\">74.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.11.11.8\">34.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.11.11.9\">68.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.5.1.12.12\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S5.T1.5.1.12.12.1\">5</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.12.12.2\">50%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.12.12.3\">59.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T1.5.1.12.12.4\">60%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.12.12.5\">99.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.12.12.6\"><span class=\"ltx_text\" id=\"S5.T1.5.1.12.12.6.1\" style=\"color:#0000FF;\">67.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.12.12.7\"><span class=\"ltx_text\" id=\"S5.T1.5.1.12.12.7.1\" style=\"color:#0000FF;\">76.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.12.12.8\">34.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.12.12.9\">69.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.5.1.13.13\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S5.T1.5.1.13.13.1\">0</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.13.13.2\">90%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.13.13.3\">18.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T1.5.1.13.13.4\">19%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.13.13.5\">7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.13.13.6\">53.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.13.13.7\">66.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.13.13.8\">34.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.13.13.9\">40.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.5.1.14.14\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S5.T1.5.1.14.14.1\">0</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.14.14.2\">75%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.14.14.3\">28.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T1.5.1.14.14.4\">29%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.14.14.5\">27.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.14.14.6\">61.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.14.14.7\">72.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.14.14.8\">35.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.14.14.9\">49.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.5.1.15.15\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S5.T1.5.1.15.15.1\">0</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.15.15.2\">50%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.15.15.3\">51.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T1.5.1.15.15.4\">52%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.15.15.5\">100.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.15.15.6\">65.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.15.15.7\">75.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.15.15.8\">34.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.15.15.9\">69.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.5.1.16.16\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S5.T1.5.1.16.16.1\" rowspan=\"10\"><span class=\"ltx_text\" id=\"S5.T1.5.1.16.16.1.1\">LLaVA-1.5-13B</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"2\" id=\"S5.T1.5.1.16.16.2\">Baseline</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.5.1.16.16.3\">154.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T1.5.1.16.16.4\">100%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.5.1.16.16.5\"><span class=\"ltx_text\" id=\"S5.T1.5.1.16.16.5.1\" style=\"color:#0000FF;\">102.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.5.1.16.16.6\">73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.5.1.16.16.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.5.1.16.16.7.1\" style=\"color:#FF0000;\">82</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.5.1.16.16.8\">36.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.5.1.16.16.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.5.1.16.16.9.1\" style=\"color:#FF0000;\">73.6</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.5.1.17.17\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S5.T1.5.1.17.17.1\">2</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.17.17.2\">90%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.17.17.3\">29.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T1.5.1.17.17.4\">19%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.17.17.5\">87.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.17.17.6\">62</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.17.17.7\">75</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.17.17.8\">36.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.17.17.9\">65.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.5.1.18.18\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S5.T1.5.1.18.18.1\">2</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.18.18.2\">75%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.18.18.3\">50.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T1.5.1.18.18.4\">32%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.18.18.5\">100.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.18.18.6\">72.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.18.18.7\">80.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.18.18.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.5.1.18.18.8.1\" style=\"color:#FF0000;\">38.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.18.18.9\">73.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.5.1.19.19\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S5.T1.5.1.19.19.1\">2</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.19.19.2\">50%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.19.19.3\">84.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T1.5.1.19.19.4\">55%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.19.19.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.5.1.19.19.5.1\" style=\"color:#FF0000;\">103.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.19.19.6\"><span class=\"ltx_text\" id=\"S5.T1.5.1.19.19.6.1\" style=\"color:#0000FF;\">73.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.19.19.7\">81</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.19.19.8\"><span class=\"ltx_text\" id=\"S5.T1.5.1.19.19.8.1\" style=\"color:#0000FF;\">36.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.19.19.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.5.1.19.19.9.1\" style=\"color:#FF0000;\">73.6</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.5.1.20.20\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S5.T1.5.1.20.20.1\">3</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.20.20.2\">90%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.20.20.3\">33.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T1.5.1.20.20.4\">21%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.20.20.5\">90.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.20.20.6\">63.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.20.20.7\">75.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.20.20.8\">34.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.20.20.9\">66.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.5.1.21.21\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S5.T1.5.1.21.21.1\">3</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.21.21.2\">75%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.21.21.3\">52.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T1.5.1.21.21.4\">34%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.21.21.5\">100.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.21.21.6\">72.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.21.21.7\">79.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.21.21.8\">36.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.21.21.9\">72.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.5.1.22.22\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S5.T1.5.1.22.22.1\">3</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.22.22.2\">50%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.22.22.3\">86.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T1.5.1.22.22.4\">56%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.22.22.5\">102.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.22.22.6\"><span class=\"ltx_text\" id=\"S5.T1.5.1.22.22.6.1\" style=\"color:#0000FF;\">73.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.22.22.7\"><span class=\"ltx_text\" id=\"S5.T1.5.1.22.22.7.1\" style=\"color:#0000FF;\">81.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.22.22.8\">36.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.22.22.9\"><span class=\"ltx_text\" id=\"S5.T1.5.1.22.22.9.1\" style=\"color:#0000FF;\">73.5</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.5.1.23.23\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S5.T1.5.1.23.23.1\">5</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.23.23.2\">90%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.23.23.3\">39.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T1.5.1.23.23.4\">26%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.23.23.5\">93.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.23.23.6\">67.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.23.23.7\">75.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.23.23.8\">35.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.23.23.9\">68.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.5.1.24.24\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S5.T1.5.1.24.24.1\">5</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.24.24.2\">75%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.24.24.3\">58.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T1.5.1.24.24.4\">38%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.24.24.5\">101.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.24.24.6\">72.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.24.24.7\">80</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.24.24.8\">36.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.24.24.9\">72.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.5.1.25.25\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S5.T1.5.1.25.25.1\">5</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.25.25.2\">50%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.25.25.3\">90.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T1.5.1.25.25.4\">58%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.25.25.5\">102.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.25.25.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.5.1.25.25.6.1\" style=\"color:#FF0000;\">73.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.25.25.7\">81.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.25.25.8\">36.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.25.25.9\"><span class=\"ltx_text\" id=\"S5.T1.5.1.25.25.9.1\" style=\"color:#0000FF;\">73.5</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.5.1.26.26\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" id=\"S5.T1.5.1.26.26.1\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S5.T1.5.1.26.26.1.1\">QwenVL-Chat-7B</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"2\" id=\"S5.T1.5.1.26.26.2\">Baseline</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.5.1.26.26.3\">71.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T1.5.1.26.26.4\">100%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.5.1.26.26.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.5.1.26.26.5.1\" style=\"color:#FF0000;\">94.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.5.1.26.26.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.5.1.26.26.6.1\" style=\"color:#FF0000;\">72.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.5.1.26.26.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.5.1.26.26.7.1\" style=\"color:#FF0000;\">75.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.5.1.26.26.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.5.1.26.26.8.1\" style=\"color:#FF0000;\">35.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.5.1.26.26.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.5.1.26.26.9.1\" style=\"color:#FF0000;\">69.7</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.5.1.27.27\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S5.T1.5.1.27.27.1\">2</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.27.27.2\">90%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.27.27.3\">15.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T1.5.1.27.27.4\">22%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.27.27.5\">81.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.27.27.6\">61.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.27.27.7\">68.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.27.27.8\">35.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.27.27.9\">61.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.5.1.28.28\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S5.T1.5.1.28.28.1\">2</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.28.28.2\">75%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.28.28.3\">24.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T1.5.1.28.28.4\">34%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.28.28.5\">90.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.28.28.6\">67.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.28.28.7\">75.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.28.28.8\">35.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.1.28.28.9\">67.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.5.1.29.29\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\" id=\"S5.T1.5.1.29.29.1\">2</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T1.5.1.29.29.2\">50%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T1.5.1.29.29.3\">39.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S5.T1.5.1.29.29.4\">55%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T1.5.1.29.29.5\"><span class=\"ltx_text\" id=\"S5.T1.5.1.29.29.5.1\" style=\"color:#0000FF;\">94.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T1.5.1.29.29.6\"><span class=\"ltx_text\" id=\"S5.T1.5.1.29.29.6.1\" style=\"color:#0000FF;\">71.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T1.5.1.29.29.7\"><span class=\"ltx_text\" id=\"S5.T1.5.1.29.29.7.1\" style=\"color:#0000FF;\">75.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T1.5.1.29.29.8\"><span class=\"ltx_text\" id=\"S5.T1.5.1.29.29.8.1\" style=\"color:#0000FF;\">35.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T1.5.1.29.29.9\"><span class=\"ltx_text\" id=\"S5.T1.5.1.29.29.9.1\" style=\"color:#0000FF;\">69.2</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 1: Performance/Computation Balance of FastV under different configurations (K for filtering layer, R for filtering ratio). Highest score for each model is in red while the second highest is in blue."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T2\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Real inference budget comparison between FastV and vanilla decoding. To get rid of the influence of output sequence length on decoding time, we report the result on A-OKVQA dataset where the model only needs to output an option. With FastV, an 13B model could inference as fast as a 7B model while maintaining its superior performance. The latency experiments are conducted on single A40 GPU. </figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S5.T2.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T2.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S5.T2.1.1.1.1\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T2.1.1.1.2\">Total-Time</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T2.1.1.1.3\">GPU-Memory</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T2.1.1.1.4\">Score</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T2.1.1.1.5\">Latency/Example</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T2.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T2.1.2.1.1\">LLaVA-1.5-7B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.2.1.2\">6:34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.2.1.3\">19G</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.2.1.4\">76.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.2.1.5\">0.344s</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T2.1.3.2.1\">\u2003w/ FastV (K=0, R=50%)</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.3.2.2\">4:23</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.3.2.3\">16G</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.3.2.4\">75.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.3.2.5\">0.230s</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T2.1.4.3.1\">LLaVA-1.5-13B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.4.3.2\">10:17</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.4.3.3\">38G</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.4.3.4\">82.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.4.3.5\">0.539s</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S5.T2.1.5.4.1\">\u2003w/ FastV (K=0, R=50%)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T2.1.5.4.2\">6:30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T2.1.5.4.3\">30G</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T2.1.5.4.4\">80.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T2.1.5.4.5\">0.341s</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 2: Real inference budget comparison between FastV and vanilla decoding. To get rid of the influence of output sequence length on decoding time, we report the result on A-OKVQA dataset where the model only needs to output an option. With FastV, an 13B model could inference as fast as a 7B model while maintaining its superior performance. The latency experiments are conducted on single A40 GPU. "
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T3\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Finegrained Results on PCA-Bench and OCR-VQA. P, C, and A each denotes Perception, Cognition and Action score. G-PCA denotes Genuine PCA score where the model must make correct perception, cognition and action for one test example to gain 1 score. The scores are averaged among all three domains including Auto-Driving, Domestic Robot and Open-World Game.</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S5.T3.1\" style=\"width:433.6pt;height:135.5pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(15.2pt,-4.7pt) scale(1.07516253963325,1.07516253963325) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T3.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T3.1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S5.T3.1.1.1.1.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S5.T3.1.1.1.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S5.T3.1.1.1.1.2\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S5.T3.1.1.1.1.2.1\">FLOPs</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"4\" id=\"S5.T3.1.1.1.1.3\">PCA-Bench Open Test</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"4\" id=\"S5.T3.1.1.1.1.4\">PCA-Bench Closed Test</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T3.1.1.1.1.5\">OCRVQA</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.1.1.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T3.1.1.2.2.1\">P</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T3.1.1.2.2.2\">C</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T3.1.1.2.2.3\">A</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\" id=\"S5.T3.1.1.2.2.4\">G-PCA</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T3.1.1.2.2.5\">P</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T3.1.1.2.2.6\">C</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T3.1.1.2.2.7\">A</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\" id=\"S5.T3.1.1.2.2.8\">G-PCA</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T3.1.1.2.2.9\">Rouge-L</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T3.1.1.3.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T3.1.1.3.1.1\">LLaVA-1.5-7B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T3.1.1.3.1.2\">99.3B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.1.1.3.1.3\">0.493</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.1.1.3.1.4\">0.353</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.1.1.3.1.5\">0.433</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T3.1.1.3.1.6\">0.263</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.1.1.3.1.7\">0.513</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.1.1.3.1.8\">0.387</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.1.1.3.1.9\">0.450</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T3.1.1.3.1.10\">0.277</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.1.1.3.1.11\">0.51</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.1.1.4.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T3.1.1.4.2.1\">LLaVA-1.5-13B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T3.1.1.4.2.2\">154.6B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.1.4.2.3\">0.530</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.1.4.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.4.2.4.1\">0.460</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.1.4.2.5\">0.503</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T3.1.1.4.2.6\">0.333</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.1.4.2.7\">0.563</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.1.4.2.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.4.2.8.1\">0.550</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.1.4.2.9\">0.573</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T3.1.1.4.2.10\">0.353</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.1.4.2.11\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.4.2.11.1\">0.55</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.1.1.5.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T3.1.1.5.3.1\">\u2003w/ FastV (K=0, R=50%)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T3.1.1.5.3.2\">78.9B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.1.5.3.3\">0.490</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.1.5.3.4\">0.395</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.1.5.3.5\">0.443</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T3.1.1.5.3.6\">0.292</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.1.5.3.7\">0.519</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.1.5.3.8\">0.450</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.1.5.3.9\">0.512</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T3.1.1.5.3.10\">0.283</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.1.5.3.11\">0.49</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.1.1.6.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T3.1.1.6.4.1\">\u2003w/ FastV (K=2, R=50%)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T3.1.1.6.4.2\">84.6B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.1.6.4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.6.4.3.1\">0.533</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.1.6.4.4\">0.423</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.1.6.4.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.6.4.5.1\">0.513</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T3.1.1.6.4.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.6.4.6.1\">0.340</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.1.6.4.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.6.4.7.1\">0.581</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.1.6.4.8\">0.545</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.1.6.4.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.6.4.9.1\">0.580</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T3.1.1.6.4.10\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.6.4.10.1\">0.368</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.1.6.4.11\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.6.4.11.1\">0.55</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.1.1.7.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" id=\"S5.T3.1.1.7.5.1\">\u2003w/ FastV (K=2, R=75%)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S5.T3.1.1.7.5.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.7.5.2.1\">50.2B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T3.1.1.7.5.3\">0.513</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T3.1.1.7.5.4\">0.417</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T3.1.1.7.5.5\">0.483</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S5.T3.1.1.7.5.6\">0.320</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T3.1.1.7.5.7\">0.523</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T3.1.1.7.5.8\">0.510</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T3.1.1.7.5.9\">0.533</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S5.T3.1.1.7.5.10\">0.323</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T3.1.1.7.5.11\">0.54</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 3: Finegrained Results on PCA-Bench and OCR-VQA. P, C, and A each denotes Perception, Cognition and Action score. G-PCA denotes Genuine PCA score where the model must make correct perception, cognition and action for one test example to gain 1 score. The scores are averaged among all three domains including Auto-Driving, Domestic Robot and Open-World Game."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T4\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>GPT-Evaluation Results on Video Question Answering Tasks.</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S5.T4.1\" style=\"width:433.6pt;height:107.8pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(35.8pt,-8.9pt) scale(1.1977507790641,1.1977507790641) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T4.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S5.T4.1.1.1.1.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S5.T4.1.1.1.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"2\" id=\"S5.T4.1.1.1.1.2\">TGIF</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"2\" id=\"S5.T4.1.1.1.1.3\">MSVD</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"2\" id=\"S5.T4.1.1.1.1.4\">MSRVTT</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S5.T4.1.1.1.1.5\">Avg</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T4.1.1.2.2.1\">Acc</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\" id=\"S5.T4.1.1.2.2.2\">Score</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T4.1.1.2.2.3\">Acc</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\" id=\"S5.T4.1.1.2.2.4\">Score</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T4.1.1.2.2.5\">Acc</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\" id=\"S5.T4.1.1.2.2.6\">Score</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T4.1.1.2.2.7\">Acc</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T4.1.1.2.2.8\">Score</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.3.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T4.1.1.3.1.1\">Video-LLaVA (FLOPs=100%)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.1.3.1.2\">0.18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T4.1.1.3.1.3\">2.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.1.3.1.4\">0.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T4.1.1.3.1.5\">3.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.1.3.1.6\">0.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T4.1.1.3.1.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.3.1.7.1\">3.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.1.3.1.8\">0.48</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.1.3.1.9\">3.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.4.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T4.1.1.4.2.1\">\u2003w/ FastV (K=2, R=50%, FLOPs=52.3%)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.1.4.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.4.2.2.1\">0.21</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T4.1.1.4.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.4.2.3.1\">2.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.1.4.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.4.2.4.1\">0.71</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T4.1.1.4.2.5\">3.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.1.4.2.6\">0.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T4.1.1.4.2.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.4.2.7.1\">3.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.1.4.2.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.4.2.8.1\">0.49</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.1.4.2.9\">3.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.5.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" id=\"S5.T4.1.1.5.3.1\">\u2003w/ FastV (K=5, R=50%, FLOPs=57.1%)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.1.1.5.3.2\">0.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S5.T4.1.1.5.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.5.3.3.1\">2.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.1.1.5.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.5.3.4.1\">0.71</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S5.T4.1.1.5.3.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.5.3.5.1\">4.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.1.1.5.3.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.5.3.6.1\">0.57</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S5.T4.1.1.5.3.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.5.3.7.1\">3.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.1.1.5.3.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.5.3.8.1\">0.49</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.1.1.5.3.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.5.3.9.1\">3.4</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 4: GPT-Evaluation Results on Video Question Answering Tasks."
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T5\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>Ablation studies results. Scores labelled as \u201cFailed\u201d denotes the model could not follow instructions to generates valid results for evaluation.</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S5.T5.1\" style=\"width:433.6pt;height:138.6pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-36.7pt,11.7pt) scale(0.855270149360147,0.855270149360147) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T5.1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S5.T5.1.1.1.1.1\">Model</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S5.T5.1.1.1.1.2\">Nocaps</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S5.T5.1.1.1.1.3\">Flickr30k</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S5.T5.1.1.1.1.4\">A-OKVQA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T5.1.1.1.1.5\">MMMU</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.2.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T5.1.1.2.2.1\">LLaVA1.5-7B (Retrained)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T5.1.1.2.2.2\">100.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T5.1.1.2.2.3\">70.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T5.1.1.2.2.4\">78.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.2.2.5\">34.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.3.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T5.1.1.3.3.1\">(a) w/ Train with 50% image tokens</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T5.1.1.3.3.2\">98.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T5.1.1.3.3.3\">68.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T5.1.1.3.3.4\">76.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.3.3.5\">33.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.4.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T5.1.1.4.4.1\">(b) w/ FastV (K=2, R=50%)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T5.1.1.4.4.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.4.4.2.1\">100.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T5.1.1.4.4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.4.4.3.1\">70</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T5.1.1.4.4.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.4.4.4.1\">78.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.4.4.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.4.4.5.1\">34.6</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.5.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T5.1.1.5.5.1\">(c) w/ FastV (K=2, R=50%, Random)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T5.1.1.5.5.2\">99.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T5.1.1.5.5.3\">68.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T5.1.1.5.5.4\">78.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.5.5.5\">34.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.6.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T5.1.1.6.6.1\">(d) w/ FastV (K=2, R=50%, on system prompt)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T5.1.1.6.6.2\">89.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T5.1.1.6.6.3\">64.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T5.1.1.6.6.4\">69.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.6.6.5\">33.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.7.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T5.1.1.7.7.1\">(e) w/ FastV (K=2, Prune first half system prompt)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T5.1.1.7.7.2\">17.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T5.1.1.7.7.3\">27.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T5.1.1.7.7.4\">Failed</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.7.7.5\">Failed</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.8.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T5.1.1.8.8.1\">(f) w/ FastV (K=2, R=50%, on instruction)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T5.1.1.8.8.2\">77.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T5.1.1.8.8.3\">50.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T5.1.1.8.8.4\">56.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.8.8.5\">29.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.9.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" id=\"S5.T5.1.1.9.9.1\">(g) w/ StreamingLLM\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Xiao et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.06764v2#bib.bib37\" title=\"\">2023</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S5.T5.1.1.9.9.2\">13.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S5.T5.1.1.9.9.3\">21.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S5.T5.1.1.9.9.4\">Failed</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T5.1.1.9.9.5\">Failed</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 5: Ablation studies results. Scores labelled as \u201cFailed\u201d denotes the model could not follow instructions to generates valid results for evaluation."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.06764v2_figure_1.png",
            "caption": "Figure 1: The Efficiency/Performance trade-off curve of FastV. The x-axis stands for the theoretical FLOPs reduction ratio under different FastV configurations. The y-axis stands for performance under different settings, we report the average scores of {Nocaps (Cider), Flickr30k (Cider), A-OKVQA (Acc), MMMU (Acc)}. We can see that FastV can achieve 45% FLOPs reduction with nearly no performance loss for different models."
        },
        "2": {
            "figure_path": "2403.06764v2_figure_2.png",
            "caption": "Figure 2: Classic network architecture of LVLM. Image tokens and different types of text tokens are sent to the LLM as input. LLM generates output tokens conditioned on the input tokens and preceding output in an auto-regressive manner."
        },
        "3": {
            "figure_path": "2403.06764v2_figure_3.png",
            "caption": "Figure 3: Illustration of inefficient visual attention phenomena. The left part shows the relative position and average number of different type of input tokens, tokens could only attend to preceding tokens in the self-attention module. In average, image tokens take up most of the input tokens (64%). The middle and right part show the average attention allocation \u03bb\ud835\udf06\\lambdaitalic_\u03bb and attention efficiency \u03f5italic-\u03f5\\epsilonitalic_\u03f5 in shallow and deep layers. Image tokens receive far less attention relative to their number in the deep layers."
        },
        "4": {
            "figure_path": "2403.06764v2_figure_4.png",
            "caption": "Figure 4: The attention maps during the decoding process of one model response for LLaVA1.5-7B. We can see that in the bottom layer, attention distributes relatively smooth across different type of tokens. In the the deep layers, above from local attention, the attention scores are aggregated to system prompt, instruction and output tokens. The attention over image tokens is rather sparse."
        },
        "5": {
            "figure_path": "2403.06764v2_figure_5.png",
            "caption": "Figure 5: Illustration of FastV. For image or video input (multiple image frames), they are first transformed to visual tokens with a pretrained image encoder like CLIP-VIT and then processed by the LLM decoder. FastV dynamically prunes R%percent\ud835\udc45R\\%italic_R % image tokens after layer K\ud835\udc3eKitalic_K in the forward process of input tokens. We can tell from the output that FastV does not influence the correctness while reducing significant FLOPs. The correct facts in the outputs are marked green. The first three outputs are completely identical."
        },
        "6": {
            "figure_path": "2403.06764v2_figure_6.png",
            "caption": "Figure 6: The heat map of theoretical FLOPs reduction ratio. The color in the figure represents the reduction ratio in different K\ud835\udc3eKitalic_K and R\ud835\udc45Ritalic_R in FastV."
        },
        "7": {
            "figure_path": "2403.06764v2_figure_7.png",
            "caption": "Figure 7: Ablation study on filtering layer K\ud835\udc3eKitalic_K and filtering ratio R\ud835\udc45Ritalic_R in FastV. Experiments are conducted with LLaVA1.5-13B on OCR-VQA task. When K is small, lowering R would improve the performance with a smaller FLOPs reduction ratio. In contrast, when K is large, changing R has minimal impact on the overall performance."
        },
        "8": {
            "figure_path": "2403.06764v2_figure_8.png",
            "caption": "Figure 8: Attention maps for each layer of LLaVA during one whole decoding process."
        }
    },
    "references": [
        {
            "1": {
                "title": "nocaps: novel object captioning at scale.",
                "author": "Harsh Agrawal, Peter Anderson, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, and Stefan Lee.",
                "venue": "In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pp.  8947\u20138956, 2019.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Qwen-vl: A frontier large vision-language model with versatile abilities.",
                "author": "Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou.",
                "venue": "ArXiv preprint, abs/2308.12966, 2023.",
                "url": null
            }
        },
        {
            "3": {
                "title": "Introducing our multimodal models, 2023.",
                "author": "Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sa\u011fnak Ta\u015f\u0131rlar.",
                "venue": "URL https://www.adept.ai/blog/fuyu-8b.",
                "url": null
            }
        },
        {
            "4": {
                "title": "Towards end-to-end embodied decision making via multi-modal large language model: Explorations with gpt4-vision and beyond.",
                "author": "Liang Chen, Yichi Zhang, Shuhuai Ren, Haozhe Zhao, Zefan Cai, Yuchi Wang, Peiyi Wang, Tianyu Liu, and Baobao Chang.",
                "venue": "ArXiv, 2023.",
                "url": null
            }
        },
        {
            "5": {
                "title": "Pca-bench: Evaluating multimodal large language models in perception-cognition-action chain.",
                "author": "Liang Chen, Yichi Zhang, Shuhuai Ren, Haozhe Zhao, Zefan Cai, Yuchi Wang, Peiyi Wang, Xiangdi Meng, Tianyu Liu, and Baobao Chang.",
                "venue": "2024.",
                "url": null
            }
        },
        {
            "6": {
                "title": "Flashattention-2: Faster attention with better parallelism and work partitioning, 2023.",
                "author": "Tri Dao.",
                "venue": null,
                "url": null
            }
        },
        {
            "7": {
                "title": "Flashattention: Fast and memory-efficient exact attention with io-awareness, 2022.",
                "author": "Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9.",
                "venue": null,
                "url": null
            }
        },
        {
            "8": {
                "title": "Palm-e: An embodied multimodal language model.",
                "author": "Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence.",
                "venue": "volume abs/2303.03378, 2023.",
                "url": null
            }
        },
        {
            "9": {
                "title": "Model tells you what to discard: Adaptive kv cache compression for llms, 2024.",
                "author": "Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao.",
                "venue": null,
                "url": null
            }
        },
        {
            "10": {
                "title": "Tgif-qa: Toward spatio-temporal reasoning in visual question answering, 2017.",
                "author": "Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim.",
                "venue": null,
                "url": null
            }
        },
        {
            "11": {
                "title": "Videopoet: A large language model for zero-shot video generation, 2023.",
                "author": "Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jos\u00e9 Lezama, Jonathan Huang, Rachel Hornung, Hartwig Adam, Hassan Akbari, Yair Alon, Vighnesh Birodkar, Yong Cheng, Ming-Chang Chiu, Josh Dillon, Irfan Essa, Agrim Gupta, Meera Hahn, Anja Hauth, David Hendon, Alonso Martinez, David Minnen, David Ross, Grant Schindler, Mikhail Sirotenko, Kihyuk Sohn, Krishna Somandepalli, Huisheng Wang, Jimmy Yan, Ming-Hsuan Yang, Xuan Yang, Bryan Seybold, and Lu Jiang.",
                "venue": null,
                "url": null
            }
        },
        {
            "12": {
                "title": "Efficient memory management for large language model serving with pagedattention, 2023.",
                "author": "Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica.",
                "venue": null,
                "url": null
            }
        },
        {
            "13": {
                "title": "Empowering vision-language models to follow interleaved vision-language instructions.",
                "author": "Juncheng Li, Kaihang Pan, Zhiqi Ge, Minghe Gao, Hanwang Zhang, Wei Ji, Wenqiao Zhang, Tat-Seng Chua, Siliang Tang, and Yueting Zhuang.",
                "venue": "arXiv preprint arXiv:2308.04152, 2023a.",
                "url": null
            }
        },
        {
            "14": {
                "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.",
                "author": "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.",
                "venue": "ArXiv preprint, abs/2301.12597, 2023b.",
                "url": null
            }
        },
        {
            "15": {
                "title": "Silkie: Preference distillation for large visual language models, 2023c.",
                "author": "Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, and Lingpeng Kong.",
                "venue": null,
                "url": null
            }
        },
        {
            "16": {
                "title": "MIT: A large-scale dataset towards multi-modal multilingual instruction tuning.",
                "author": "Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu Sun, Lingpeng Kong, and Qi Liu.",
                "venue": "ArXiv preprint, abs/2306.04387, 2023d.",
                "url": null
            }
        },
        {
            "17": {
                "title": "Llama-vid: An image is worth 2 tokens in large language models, 2023e.",
                "author": "Yanwei Li, Chengyao Wang, and Jiaya Jia.",
                "venue": null,
                "url": null
            }
        },
        {
            "18": {
                "title": "Monkey: Image resolution and text label are important things for large multi-modal models.",
                "author": "Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai.",
                "venue": "arXiv preprint arXiv:2311.06607, 2023f.",
                "url": null
            }
        },
        {
            "19": {
                "title": "Video-llava: Learning united visual representation by alignment before projection.",
                "author": "Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan.",
                "venue": "arXiv preprint arXiv:2311.10122, 2023a.",
                "url": null
            }
        },
        {
            "20": {
                "title": "Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models, 2023b.",
                "author": "Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, Jiaming Han, Siyuan Huang, Yichi Zhang, Xuming He, Hongsheng Li, and Yu Qiao.",
                "venue": null,
                "url": null
            }
        },
        {
            "21": {
                "title": "Ring attention with blockwise transformers for near-infinite context, 2023a.",
                "author": "Hao Liu, Matei Zaharia, and Pieter Abbeel.",
                "venue": null,
                "url": null
            }
        },
        {
            "22": {
                "title": "World model on million-length video and language with ringattention, 2024a.",
                "author": "Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel.",
                "venue": null,
                "url": null
            }
        },
        {
            "23": {
                "title": "Improved baselines with visual instruction tuning, 2023b.",
                "author": "Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.",
                "venue": null,
                "url": null
            }
        },
        {
            "24": {
                "title": "Visual instruction tuning.",
                "author": "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.",
                "venue": "ArXiv preprint, abs/2304.08485, 2023c.",
                "url": null
            }
        },
        {
            "25": {
                "title": "Llava-next: Improved reasoning, ocr, and world knowledge, January 2024b.",
                "author": "Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee.",
                "venue": "URL https://llava-vl.github.io/blog/2024-01-30-llava-next/.",
                "url": null
            }
        },
        {
            "26": {
                "title": "Unified-io 2: Scaling autoregressive multimodal models with vision, language, audio, and action, 2023.",
                "author": "Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi.",
                "venue": null,
                "url": null
            }
        },
        {
            "27": {
                "title": "Video-chatgpt: Towards detailed video understanding via large vision and language models.",
                "author": "Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan.",
                "venue": "arXiv:2306.05424, 2023.",
                "url": null
            }
        },
        {
            "28": {
                "title": "Ocr-vqa: Visual question answering by reading text in images.",
                "author": "Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty.",
                "venue": "In 2019 international conference on document analysis and recognition (ICDAR), pp.  947\u2013952. IEEE, 2019.",
                "url": null
            }
        },
        {
            "29": {
                "title": "Gpt-4v(ision) system card.",
                "author": "OpenAI.",
                "venue": "2023.",
                "url": null
            }
        },
        {
            "30": {
                "title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models.",
                "author": "Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik.",
                "venue": "In Proceedings of the IEEE international conference on computer vision, pp.  2641\u20132649, 2015.",
                "url": null
            }
        },
        {
            "31": {
                "title": "Learning transferable visual models from natural language supervision.",
                "author": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.",
                "venue": "In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp.  8748\u20138763, 2021.",
                "url": null
            }
        },
        {
            "32": {
                "title": "A-okvqa: A benchmark for visual question answering using world knowledge.",
                "author": "Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi.",
                "venue": "In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part VIII, pp.  146\u2013162. Springer, 2022.",
                "url": null
            }
        },
        {
            "33": {
                "title": "Gemini: A family of highly capable multimodal models, 2023.",
                "author": "Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul R. Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Ana\u00efs White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, Alexandre Frechette, Charlotte Smith, Laura Culp, Lev Proleev, Yi Luan, Xi Chen, James Lottes, Nathan Schucher, Federico Lebron, Alban Rrustemi, Natalie Clay, Phil Crone, Tomas Kocisky, Jeffrey Zhao, Bartek Perz, Dian Yu,\nHeidi Howard, Adam Bloniarz, Jack W. Rae, Han Lu, Laurent Sifre, Marcello Maggioni, Fred Alcober, Dan Garrette, Megan Barnes, Shantanu Thakoor, Jacob Austin, Gabriel Barth-Maron, William Wong, Rishabh Joshi, Rahma Chaabouni, Deeni Fatiha, Arun Ahuja, Ruibo Liu, Yunxuan Li, Sarah Cogan, Jeremy Chen, Chao Jia, Chenjie Gu, Qiao Zhang, Jordan Grimstad, Ale Jakse Hartman, Martin Chadwick, Gaurav Singh Tomar, Xavier Garcia, Evan Senter, Emanuel Taropa, Thanumalayan Sankaranarayana Pillai, Jacob Devlin, Michael Laskin, Diego de Las Casas, Dasha Valter, Connie Tao, Lorenzo Blanco, Adri\u00e0 Puigdom\u00e8nech Badia, David Reitter, Mianna Chen, Jenny Brennan, Clara Rivera, Sergey Brin, Shariq Iqbal, Gabriela Surita, Jane Labanowski, Abhi Rao, Stephanie Winkler, Emilio Parisotto, Yiming Gu, Kate Olszewska, Yujing Zhang, Ravi Addanki, Antoine Miech, Annie Louis, Laurent El Shafey, Denis Teplyashin, Geoff Brown, Elliot Catt, Nithya Attaluri, Jan Balaguer, Jackie Xiang, Pidong Wang, Zoe Ashwood, Anton Briukhov, Albert Webson,\nSanjay Ganapathy, Smit Sanghavi, Ajay Kannan, Ming-Wei Chang, Axel Stjerngren, Josip Djolonga, Yuting Sun, Ankur Bapna, Matthew Aitchison, Pedram Pejman, Henryk Michalewski, Tianhe Yu, Cindy Wang, Juliette Love, Junwhan Ahn, Dawn Bloxwich, Kehang Han, Peter Humphreys, Thibault Sellam, James Bradbury, Varun Godbole, Sina Samangooei, Bogdan Damoc, Alex Kaskasoli, S\u00e9bastien M. R. Arnold, Vijay Vasudevan, Shubham Agrawal, Jason Riesa, Dmitry Lepikhin, Richard Tanburn, Srivatsan Srinivasan, Hyeontaek Lim, Sarah Hodkinson, Pranav Shyam, Johan Ferret, Steven Hand, Ankush Garg, Tom Le Paine, Jian Li, Yujia Li, Minh Giang, Alexander Neitz, Zaheer Abbas, Sarah York, Machel Reid, Elizabeth Cole, Aakanksha Chowdhery, Dipanjan Das, Dominika Rogozi\u0144ska, Vitaly Nikolaev, Pablo Sprechmann, Zachary Nado, Lukas Zilka, Flavien Prost, Luheng He, Marianne Monteiro, Gaurav Mishra, Chris Welty, Josh Newlan, Dawei Jia, Miltiadis Allamanis, Clara Huiyi Hu, Raoul de Liedekerke, Justin Gilmer, Carl Saroufim, Shruti Rijhwani, Shaobo\nHou, Disha Shrivastava, Anirudh Baddepudi, Alex Goldin, Adnan Ozturel, Albin Cassirer, Yunhan Xu, Daniel Sohn, Devendra Sachan, Reinald Kim Amplayo, Craig Swanson, Dessie Petrova, Shashi Narayan, Arthur Guez, Siddhartha Brahma, Jessica Landon, Miteyan Patel, Ruizhe Zhao, Kevin Villela, Luyu Wang, Wenhao Jia, Matthew Rahtz, Mai Gim\u00e9nez, Legg Yeung, Hanzhao Lin, James Keeling, Petko Georgiev, Diana Mincu, Boxi Wu, Salem Haykal, Rachel Saputro, Kiran Vodrahalli, James Qin, Zeynep Cankara, Abhanshu Sharma, Nick Fernando, Will Hawkins, Behnam Neyshabur, Solomon Kim, Adrian Hutter, Priyanka Agrawal, Alex Castro-Ros, George van den Driessche, Tao Wang, Fan Yang, Shuo yiin Chang, Paul Komarek, Ross McIlroy, Mario Lu\u010di\u0107, Guodong Zhang, Wael Farhan, Michael Sharman, Paul Natsev, Paul Michel, Yong Cheng, Yamini Bansal, Siyuan Qiao, Kris Cao, Siamak Shakeri, Christina Butterfield, Justin Chung, Paul Kishan Rubenstein, Shivani Agrawal, Arthur Mensch, Kedar Soparkar, Karel Lenc, Timothy Chung, Aedan Pope, Loren\nMaggiore, Jackie Kay, Priya Jhakra, Shibo Wang, Joshua Maynez, Mary Phuong, Taylor Tobin, Andrea Tacchetti, Maja Trebacz, Kevin Robinson, Yash Katariya, Sebastian Riedel, Paige Bailey, Kefan Xiao, Nimesh Ghelani, Lora Aroyo, Ambrose Slone, Neil Houlsby, Xuehan Xiong, Zhen Yang, Elena Gribovskaya, Jonas Adler, Mateo Wirth, Lisa Lee, Music Li, Thais Kagohara, Jay Pavagadhi, Sophie Bridgers, Anna Bortsova, Sanjay Ghemawat, Zafarali Ahmed, Tianqi Liu, Richard Powell, Vijay Bolina, Mariko Iinuma, Polina Zablotskaia, James Besley, Da-Woon Chung, Timothy Dozat, Ramona Comanescu, Xiance Si, Jeremy Greer, Guolong Su, Martin Polacek, Rapha\u00ebl Lopez Kaufman, Simon Tokumine, Hexiang Hu, Elena Buchatskaya, Yingjie Miao, Mohamed Elhawaty, Aditya Siddhant, Nenad Tomasev, Jinwei Xing, Christina Greer, Helen Miller, Shereen Ashraf, Aurko Roy, Zizhao Zhang, Ada Ma, Angelos Filos, Milos Besta, Rory Blevins, Ted Klimenko, Chih-Kuan Yeh, Soravit Changpinyo, Jiaqi Mu, Oscar Chang, Mantas Pajarskas, Carrie Muir, Vered Cohen,\nCharline Le Lan, Krishna Haridasan, Amit Marathe, Steven Hansen, Sholto Douglas, Rajkumar Samuel, Mingqiu Wang, Sophia Austin, Chang Lan, Jiepu Jiang, Justin Chiu, Jaime Alonso Lorenzo, Lars Lowe Sj\u00f6sund, S\u00e9bastien Cevey, Zach Gleicher, Thi Avrahami, Anudhyan Boral, Hansa Srinivasan, Vittorio Selo, Rhys May, Konstantinos Aisopos, L\u00e9onard Hussenot, Livio Baldini Soares, Kate Baumli, Michael B. Chang, Adri\u00e0 Recasens, Ben Caine, Alexander Pritzel, Filip Pavetic, Fabio Pardo, Anita Gergely, Justin Frye, Vinay Ramasesh, Dan Horgan, Kartikeya Badola, Nora Kassner, Subhrajit Roy, Ethan Dyer, V\u00edctor Campos, Alex Tomala, Yunhao Tang, Dalia El Badawy, Elspeth White, Basil Mustafa, Oran Lang, Abhishek Jindal, Sharad Vikram, Zhitao Gong, Sergi Caelles, Ross Hemsley, Gregory Thornton, Fangxiaoyu Feng, Wojciech Stokowiec, Ce Zheng, Phoebe Thacker, \u00c7a\u011flar \u00dcnl\u00fc, Zhishuai Zhang, Mohammad Saleh, James Svensson, Max Bileschi, Piyush Patil, Ankesh Anand, Roman Ring, Katerina Tsihlas, Arpi Vezer, Marco Selvi, Toby\nShevlane, Mikel Rodriguez, Tom Kwiatkowski, Samira Daruki, Keran Rong, Allan Dafoe, Nicholas FitzGerald, Keren Gu-Lemberg, Mina Khan, Lisa Anne Hendricks, Marie Pellat, Vladimir Feinberg, James Cobon-Kerr, Tara Sainath, Maribeth Rauh, Sayed Hadi Hashemi, Richard Ives, Yana Hasson, YaGuang Li, Eric Noland, Yuan Cao, Nathan Byrd, Le Hou, Qingze Wang, Thibault Sottiaux, Michela Paganini, Jean-Baptiste Lespiau, Alexandre Moufarek, Samer Hassan, Kaushik Shivakumar, Joost van Amersfoort, Amol Mandhane, Pratik Joshi, Anirudh Goyal, Matthew Tung, Andrew Brock, Hannah Sheahan, Vedant Misra, Cheng Li, Nemanja Raki\u0107evi\u0107, Mostafa Dehghani, Fangyu Liu, Sid Mittal, Junhyuk Oh, Seb Noury, Eren Sezener, Fantine Huot, Matthew Lamm, Nicola De Cao, Charlie Chen, Gamaleldin Elsayed, Ed Chi, Mahdis Mahdieh, Ian Tenney, Nan Hua, Ivan Petrychenko, Patrick Kane, Dylan Scandinaro, Rishub Jain, Jonathan Uesato, Romina Datta, Adam Sadovsky, Oskar Bunyan, Dominik Rabiej, Shimu Wu, John Zhang, Gautam Vasudevan, Edouard Leurent,\nMahmoud Alnahlawi, Ionut Georgescu, Nan Wei, Ivy Zheng, Betty Chan, Pam G Rabinovitch, Piotr Stanczyk, Ye Zhang, David Steiner, Subhajit Naskar, Michael Azzam, Matthew Johnson, Adam Paszke, Chung-Cheng Chiu, Jaume Sanchez Elias, Afroz Mohiuddin, Faizan Muhammad, Jin Miao, Andrew Lee, Nino Vieillard, Sahitya Potluri, Jane Park, Elnaz Davoodi, Jiageng Zhang, Jeff Stanway, Drew Garmon, Abhijit Karmarkar, Zhe Dong, Jong Lee, Aviral Kumar, Luowei Zhou, Jonathan Evens, William Isaac, Zhe Chen, Johnson Jia, Anselm Levskaya, Zhenkai Zhu, Chris Gorgolewski, Peter Grabowski, Yu Mao, Alberto Magni, Kaisheng Yao, Javier Snaider, Norman Casagrande, Paul Suganthan, Evan Palmer, Geoffrey Irving, Edward Loper, Manaal Faruqui, Isha Arkatkar, Nanxin Chen, Izhak Shafran, Michael Fink, Alfonso Casta\u00f1o, Irene Giannoumis, Wooyeol Kim, Miko\u0142aj Rybi\u0144ski, Ashwin Sreevatsa, Jennifer Prendki, David Soergel, Adrian Goedeckemeyer, Willi Gierke, Mohsen Jafari, Meenu Gaba, Jeremy Wiesner, Diana Gage Wright, Yawen Wei, Harsha Vashisht,\nYana Kulizhskaya, Jay Hoover, Maigo Le, Lu Li, Chimezie Iwuanyanwu, Lu Liu, Kevin Ramirez, Andrey Khorlin, Albert Cui, Tian LIN, Marin Georgiev, Marcus Wu, Ricardo Aguilar, Keith Pallo, Abhishek Chakladar, Alena Repina, Xihui Wu, Tom van der Weide, Priya Ponnapalli, Caroline Kaplan, Jiri Simsa, Shuangfeng Li, Olivier Dousse, Fan Yang, Jeff Piper, Nathan Ie, Minnie Lui, Rama Pasumarthi, Nathan Lintz, Anitha Vijayakumar, Lam Nguyen Thiet, Daniel Andor, Pedro Valenzuela, Cosmin Paduraru, Daiyi Peng, Katherine Lee, Shuyuan Zhang, Somer Greene, Duc Dung Nguyen, Paula Kurylowicz, Sarmishta Velury, Sebastian Krause, Cassidy Hardin, Lucas Dixon, Lili Janzer, Kiam Choo, Ziqiang Feng, Biao Zhang, Achintya Singhal, Tejasi Latkar, Mingyang Zhang, Quoc Le, Elena Allica Abellan, Dayou Du, Dan McKinnon, Natasha Antropova, Tolga Bolukbasi, Orgad Keller, David Reid, Daniel Finchelstein, Maria Abi Raad, Remi Crocker, Peter Hawkins, Robert Dadashi, Colin Gaffney, Sid Lall, Ken Franko, Egor Filonov, Anna Bulanova, R\u00e9mi\nLeblond, Vikas Yadav, Shirley Chung, Harry Askham, Luis C. Cobo, Kelvin Xu, Felix Fischer, Jun Xu, Christina Sorokin, Chris Alberti, Chu-Cheng Lin, Colin Evans, Hao Zhou, Alek Dimitriev, Hannah Forbes, Dylan Banarse, Zora Tung, Jeremiah Liu, Mark Omernick, Colton Bishop, Chintu Kumar, Rachel Sterneck, Ryan Foley, Rohan Jain, Swaroop Mishra, Jiawei Xia, Taylor Bos, Geoffrey Cideron, Ehsan Amid, Francesco Piccinno, Xingyu Wang, Praseem Banzal, Petru Gurita, Hila Noga, Premal Shah, Daniel J. Mankowitz, Alex Polozov, Nate Kushman, Victoria Krakovna, Sasha Brown, MohammadHossein Bateni, Dennis Duan, Vlad Firoiu, Meghana Thotakuri, Tom Natan, Anhad Mohananey, Matthieu Geist, Sidharth Mudgal, Sertan Girgin, Hui Li, Jiayu Ye, Ofir Roval, Reiko Tojo, Michael Kwong, James Lee-Thorp, Christopher Yew, Quan Yuan, Sumit Bagri, Danila Sinopalnikov, Sabela Ramos, John Mellor, Abhishek Sharma, Aliaksei Severyn, Jonathan Lai, Kathy Wu, Heng-Tze Cheng, David Miller, Nicolas Sonnerat, Denis Vnukov, Rory Greig, Jennifer\nBeattie, Emily Caveness, Libin Bai, Julian Eisenschlos, Alex Korchemniy, Tomy Tsai, Mimi Jasarevic, Weize Kong, Phuong Dao, Zeyu Zheng, Frederick Liu, Fan Yang, Rui Zhu, Mark Geller, Tian Huey Teh, Jason Sanmiya, Evgeny Gladchenko, Nejc Trdin, Andrei Sozanschi, Daniel Toyama, Evan Rosen, Sasan Tavakkol, Linting Xue, Chen Elkind, Oliver Woodman, John Carpenter, George Papamakarios, Rupert Kemp, Sushant Kafle, Tanya Grunina, Rishika Sinha, Alice Talbert, Abhimanyu Goyal, Diane Wu, Denese Owusu-Afriyie, Cosmo Du, Chloe Thornton, Jordi Pont-Tuset, Pradyumna Narayana, Jing Li, Sabaer Fatehi, John Wieting, Omar Ajmeri, Benigno Uria, Tao Zhu, Yeongil Ko, Laura Knight, Am\u00e9lie H\u00e9liou, Ning Niu, Shane Gu, Chenxi Pang, Dustin Tran, Yeqing Li, Nir Levine, Ariel Stolovich, Norbert Kalb, Rebeca Santamaria-Fernandez, Sonam Goenka, Wenny Yustalim, Robin Strudel, Ali Elqursh, Balaji Lakshminarayanan, Charlie Deck, Shyam Upadhyay, Hyo Lee, Mike Dusenberry, Zonglin Li, Xuezhi Wang, Kyle Levin, Raphael Hoffmann, Dan\nHoltmann-Rice, Olivier Bachem, Summer Yue, Sho Arora, Eric Malmi, Daniil Mirylenka, Qijun Tan, Christy Koh, Soheil Hassas Yeganeh, Siim P\u00f5der, Steven Zheng, Francesco Pongetti, Mukarram Tariq, Yanhua Sun, Lucian Ionita, Mojtaba Seyedhosseini, Pouya Tafti, Ragha Kotikalapudi, Zhiyu Liu, Anmol Gulati, Jasmine Liu, Xinyu Ye, Bart Chrzaszcz, Lily Wang, Nikhil Sethi, Tianrun Li, Ben Brown, Shreya Singh, Wei Fan, Aaron Parisi, Joe Stanton, Chenkai Kuang, Vinod Koverkathu, Christopher A. Choquette-Choo, Yunjie Li, TJ Lu, Abe Ittycheriah, Prakash Shroff, Pei Sun, Mani Varadarajan, Sanaz Bahargam, Rob Willoughby, David Gaddy, Ishita Dasgupta, Guillaume Desjardins, Marco Cornero, Brona Robenek, Bhavishya Mittal, Ben Albrecht, Ashish Shenoy, Fedor Moiseev, Henrik Jacobsson, Alireza Ghaffarkhah, Morgane Rivi\u00e8re, Alanna Walton, Cl\u00e9ment Crepy, Alicia Parrish, Yuan Liu, Zongwei Zhou, Clement Farabet, Carey Radebaugh, Praveen Srinivasan, Claudia van der Salm, Andreas Fidjeland, Salvatore Scellato, Eri Latorre-Chimoto,\nHanna Klimczak-Pluci\u0144ska, David Bridson, Dario de Cesare, Tom Hudson, Piermaria Mendolicchio, Lexi Walker, Alex Morris, Ivo Penchev, Matthew Mauger, Alexey Guseynov, Alison Reid, Seth Odoom, Lucia Loher, Victor Cotruta, Madhavi Yenugula, Dominik Grewe, Anastasia Petrushkina, Tom Duerig, Antonio Sanchez, Steve Yadlowsky, Amy Shen, Amir Globerson, Adam Kurzrok, Lynette Webb, Sahil Dua, Dong Li, Preethi Lahoti, Surya Bhupatiraju, Dan Hurt, Haroon Qureshi, Ananth Agarwal, Tomer Shani, Matan Eyal, Anuj Khare, Shreyas Rammohan Belle, Lei Wang, Chetan Tekur, Mihir Sanjay Kale, Jinliang Wei, Ruoxin Sang, Brennan Saeta, Tyler Liechty, Yi Sun, Yao Zhao, Stephan Lee, Pandu Nayak, Doug Fritz, Manish Reddy Vuyyuru, John Aslanides, Nidhi Vyas, Martin Wicke, Xiao Ma, Taylan Bilal, Evgenii Eltyshev, Daniel Balle, Nina Martin, Hardie Cate, James Manyika, Keyvan Amiri, Yelin Kim, Xi Xiong, Kai Kang, Florian Luisier, Nilesh Tripuraneni, David Madras, Mandy Guo, Austin Waters, Oliver Wang, Joshua Ainslie, Jason Baldridge, Han\nZhang, Garima Pruthi, Jakob Bauer, Feng Yang, Riham Mansour, Jason Gelman, Yang Xu, George Polovets, Ji Liu, Honglong Cai, Warren Chen, XiangHai Sheng, Emily Xue, Sherjil Ozair, Adams Yu, Christof Angermueller, Xiaowei Li, Weiren Wang, Julia Wiesinger, Emmanouil Koukoumidis, Yuan Tian, Anand Iyer, Madhu Gurumurthy, Mark Goldenson, Parashar Shah, MK Blake, Hongkun Yu, Anthony Urbanowicz, Jennimaria Palomaki, Chrisantha Fernando, Kevin Brooks, Ken Durden, Harsh Mehta, Nikola Momchev, Elahe Rahimtoroghi, Maria Georgaki, Amit Raul, Sebastian Ruder, Morgan Redshaw, Jinhyuk Lee, Komal Jalan, Dinghua Li, Ginger Perng, Blake Hechtman, Parker Schuh, Milad Nasr, Mia Chen, Kieran Milan, Vladimir Mikulik, Trevor Strohman, Juliana Franco, Tim Green, Demis Hassabis, Koray Kavukcuoglu, Jeffrey Dean, and Oriol Vinyals.",
                "venue": null,
                "url": null
            }
        },
        {
            "34": {
                "title": "Attention is all you need.",
                "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin.",
                "venue": "In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp.  5998\u20136008, 2017.",
                "url": null
            }
        },
        {
            "35": {
                "title": "Cider: Consensus-based image description evaluation, 2015.",
                "author": "Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh.",
                "venue": null,
                "url": null
            }
        },
        {
            "36": {
                "title": "Mobile-agent: Autonomous multi-modal mobile device agent with visual perception, 2024.",
                "author": "Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang.",
                "venue": null,
                "url": null
            }
        },
        {
            "37": {
                "title": "Efficient streaming language models with attention sinks.",
                "author": "Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis.",
                "venue": "arXiv, 2023.",
                "url": null
            }
        },
        {
            "38": {
                "title": "Video question answering via gradually refined attention over appearance and motion.",
                "author": "Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang.",
                "venue": "In Proceedings of the 2017 ACM on Multimedia Conference, MM 2017, Mountain View, CA, USA, October 23-27, 2017, pp.  1645\u20131653, 2017a.",
                "url": null
            }
        },
        {
            "39": {
                "title": "Video question answering via gradually refined attention over appearance and motion.",
                "author": "Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang.",
                "venue": "In ACM Multimedia, 2017b.",
                "url": null
            }
        },
        {
            "40": {
                "title": "Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi.",
                "author": "Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen.",
                "venue": "arXiv preprint arXiv:2311.16502, 2023.",
                "url": null
            }
        },
        {
            "41": {
                "title": "Mmicl: Empowering vision-language model with multi-modal in-context learning.",
                "author": "Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, and Baobao Chang.",
                "venue": "ArXiv preprint, abs/2309.07915, 2023.",
                "url": null
            }
        },
        {
            "42": {
                "title": "Gpt-4v(ision) is a generalist web agent, if grounded, 2024.",
                "author": "Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su.",
                "venue": null,
                "url": null
            }
        },
        {
            "43": {
                "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models.",
                "author": "Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.",
                "venue": "ArXiv preprint, abs/2304.10592, 2023.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.06764v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.1",
            "5.2",
            "5.3",
            "5.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "1",
            "4.3",
            "5.4"
        ]
    },
    "research_context": {
        "paper_id": "2403.06764v2",
        "paper_title": "An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models",
        "research_background": "### Motivation\n\nThe motivation of this paper stems from the computational inefficiencies observed in current Large Vision-Language Models (LVLMs). Despite their success in merging visual and linguistic capabilities for various tasks, these models exhibit an imbalance in attention scores between image tokens and textual tokens, particularly in deeper layers. This inefficiency presents an opportunity for optimization by pruning image tokens that contribute minimally to the final output, thereby reducing computational complexity without sacrificing performance.\n\n### Research Problem\n\nThe primary research problem addressed in this paper is the inefficient utilization of visual information in LVLMs, where image tokens receive significantly lower attention compared to textual tokens in deeper layers. The authors highlight two under-explored areas in this context:\n1. Understanding how language models process and interpret images.\n2. Addressing the gap in efficient training and inference specific to LVLMs.\n\nThey specifically focus on optimizing the inference process by exploring the potential of dynamically pruning image tokens that have low impact on the model's performance in deeper layers.\n\n### Relevant Prior Work\n\nThe paper builds on and contrasts with several strands of prior research, which can be summarized as follows:\n\n1. **Sequential Visual Representation in LVLMs**:\n   - Notable works such as those by OpenAI (2023), Zhu et al. (2023), Liu et al. (2023c, b, 2024b), Zhao et al. (2023), Bai et al. (2023), and Li et al. (2023d, c) have focused on converting images into hundreds or thousands of tokens that are processed sequentially with language prompts.\n\n2. **Efficiency in Large Language Models (LLMs)**:\n   - While significant attention has been given to making LLMs more efficient in terms of training and inference, these aspects have not been thoroughly explored within the context of LVLMs.\n\n3. **Enhancing Image Resolution for LVLMs**:\n   - Research by Liu et al. (2023c) and Li et al. (2023f) has emphasized the importance of image resolution in improving LVLM performance. However, higher resolution also increases computational costs, something that this paper seeks to mitigate.\n\n### Proposed Solution\n\nThe authors introduce **FastV**, a plug-and-play dynamic image token pruning method. FastV prunes image tokens at a specific layer by focusing on the most impactful tokens, determined by their attention scores. This approach significantly reduces the inference budget by eliminating the need to process less impactful tokens in deeper layers.\n\n### Validation\n\nThe effectiveness of FastV is validated through comprehensive experiments on various vision-language tasks (e.g., captioning, multiple choice, complex embodied reasoning, OCR, and video understanding). The results demonstrate a notable reduction in computational complexity while maintaining high performance, highlighting FastV's potential to balance speed and accuracy in LVLM inferences.",
        "methodology": "The proposed method, FastV, addresses the challenge of reducing inference budgets for large vision-language models (LVLMs) while maintaining performance. Key components and innovations of FastV are as follows:\n\n1. **Inference Acceleration**: FastV focuses on optimizing the inference process of LVLMs, particularly by targeting inefficiencies that occur early in the processing layers. This allows the models to rely less on certain computationally expensive operations without compromising the overall output quality.\n\n2. **Layer-Wise Optimization**: A critical insight of FastV is that significant performance gains can be achieved by making adjustments to the model after the second layer. This suggests that from the third layer onward, certain tokens or intermediate representations can be reduced by half, effectively cutting down the number of computations required.\n\n3. **Plug-and-Play Nature**: FastV is designed to be easily integrated into existing LVLMs. This \"plug-and-play\" characteristic ensures that the method can be applied to a broad range of models with minimal alterations to their original architecture or training process.\n\n4. **Retaining Performance**: Despite the reduction in computational demands, FastV is validated to retain the performance of LVLMs. The method leverages insights from the initial layers to optimize the subsequent processes, ensuring that the accuracy and effectiveness of the models are not compromised.\n\nWith these components, FastV presents an innovative approach to enhancing the efficiency of vision-language models, offering significant reductions in computational load while maintaining high performance.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\n#### Image Captioning:\n- **Datasets**: Nocaps and Flickr30k\n- **Evaluation Metric**: CIDEr score\n\n#### Visual Question Answering (VQA):\n- **Datasets**: A-OKVQA (development set) and OCR-VQA (test set)\n- **Evaluation Metrics**: Multiple Choice (MC) score for A-OKVQA and Rouge-L score for OCR-VQA\n\n#### Multimodal Reasoning:\n- **Datasets**: MMMU (development set) and PCA-Bench (open and closed test set)\n- **Evaluation Metrics**: Multiple choice accuracy for MMMU and Perception, Cognition, Action, Genuine PCA scores for PCA-Bench\n\n#### Video Question Answering:\n- **Datasets**: TGIF-QA, MSVD-QA, MSRVTT-QA (first 1K examples each)\n- **Evaluation Metrics**: Accuracy and chatgpt-score\n\n### Results:\n\n### Key Findings:\n- **FLOPs Reduction**: FastV can significantly reduce computational requirements by strategically pruning image tokens, achieving up to 45% reduction without sacrificing performance.\n- **Performance Consistency**: Across various datasets and evaluation metrics, FastV maintains or even improves performance while also reducing computational costs.\n- **Video QA Impact**: Evident improvement in video-QA tasks suggests that FastV effectively curtails redundancy in video processing, thereby optimizing performance and resource usage.\n\nIn summary, FastV demonstrates considerable potential in enhancing the computational efficiency of Large Vision-Language Models (LVLMs) without compromising their performance, particularly excelling in tasks involving high token redundancy, such as video question answering."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the impact of various components within FastV, with particular focus on how parameters (K and R) influence acceleration and downstream task performance.",
            "experiment_process": "We conducted experiments on the OCR-VQA task, which requires thorough understanding of images. We adjusted the parameters K and R to observe their effects on FLOPs reduction and task performance. Additionally, we retrained two LLaVA1.5-7B models: one following the original protocols, and the other incorporating an average pooling layer with a stride of 2 after the Clip encoder, reducing image tokens by 50% during training. We compared the results between these models and tested random visual token drops versus attention-ranked drops.",
            "result_discussion": "When K is small, lowering R improves performance but reduces the FLOPs reduction ratio; when K is large, R has minimal impact. Training with reduced visual tokens resulted in diminished performance, while FastV maintained performance by pruning fewer tokens during inference. Image tokens are least attention-efficient relative to other input tokens, and randomly dropping tokens declined performance compared to attention-ranked pruned tokens. Additionally, pruning the first half of system prompt tokens degraded performance, showing the importance of head tokens. Finally, attention optimization techniques designed for LLMs significantly degraded LVLM performance, indicating distinct processing requirements for visual and textual tokens.",
            "ablation_id": "2403.06764v2.No1"
        },
        {
            "research_objective": "To evaluate the effectiveness of training LVLMs with fewer visual tokens and compare this method with FastV's pruning strategy during inference.",
            "experiment_process": "We introduced pooling on the output of the visual encoder during training to directly reduce visual tokens. Then, we compared the results of this method with FastV's approach, which involves pruning image tokens during the inference stage. Specifically, we retrained two LLaVA1.5-7B models: one with standard training protocols and the other with an average pooling layer to reduce visual tokens during training.",
            "result_discussion": "Training with fewer visual tokens by pooling resulted in lower performance compared to the original model. In contrast, FastV managed to reduce image tokens without compromising performance, highlighting its efficiency in balancing computational savings with model efficacy. This study underscores the importance of strategic token pruning during inference rather than reducing resolution during training.",
            "ablation_id": "2403.06764v2.No2"
        }
    ]
}