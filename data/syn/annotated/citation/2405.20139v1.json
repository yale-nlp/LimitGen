{
    "title": "Gnn-Rag: Graph Neural Retrieval for Large Language Model Reasoning",
    "abstract": "Knowledge Graphs (KGs) represent human-crafted factual knowledge in the form of triplets (head, relation, tail), which collectively form a graph. Question Answering over KGs (KGQA) is the task of answering natural questions grounding the reasoning to the information provided by the KG. Large Language Models (LLMs) are the state-of-the-art models for QA tasks due to their remarkable ability to understand natural language. On the other hand, Graph Neural Networks (GNNs) have been widely used for KGQA as they can handle the complex graph information stored in the KG. In this work, we introduce Gnn-Rag, a novel method for combining language understanding abilities of LLMs with the reasoning abilities of GNNs in a retrieval-augmented generation (RAG) style. First, a GNN reasons over a dense KG subgraph to retrieve answer candidates for a given question. Second, the shortest paths in the KG that connect question entities and answer candidates are extracted to represent KG reasoning paths. The extracted paths are verbalized and given as input for LLM reasoning with RAG. In our Gnn-Rag framework, the GNN acts as a dense subgraph reasoner to extract useful graph information, while the LLM leverages its natural language processing ability for ultimate KGQA. Furthermore, we develop a retrieval augmentation (RA) technique to further boost KGQA performance with Gnn-Rag. Experimental results show that Gnn-Rag achieves state-of-the-art performance in two widely used KGQA benchmarks (WebQSP and CWQ), outperforming or matching GPT-4 performance with a 7B tuned LLM. In addition, Gnn-Rag excels on multi-hop and multi-entity questions outperforming competing approaches by 8.9\u201315.5% points at answer F1.\nWe provide the code and KGQA results at https://github.com/cmavro/GNN-RAG.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large Language Models (LLMs) (Brown et al., 2020  ###reference_b6###; Bommasani et al., 2021  ###reference_b5###; Chowdhery et al., 2023  ###reference_b8###) are the state-of-the-art models in many NLP tasks due to their remarkable ability to understand natural language. LLM\u2019s power stems from pretraining on large corpora of textual data to obtain general human knowledge (Kaplan et al., 2020  ###reference_b20###; Hoffmann et al., 2022  ###reference_b14###). However, because pretraining is costly and time-consuming (Gururangan et al., 2020  ###reference_b11###), LLMs cannot easily adapt to new or in-domain knowledge and are prone to hallucinations (Zhang et al., 2023  ###reference_b58###).\nKnowledge Graphs (KGs) (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014  ###reference_b47###) are databases that store information in structured form that can be easily updated. KGs represent human-crafted factual knowledge in the form of triplets (head, relation, tail), e.g., <Jamaica  language_spoken  English>, which collectively form a graph. In the case of KGs, the stored knowledge is updated by fact addition or removal. As KGs capture complex interactions between the stored entities, e.g., multi-hop relations, they are widely used for knowledge-intensive task, such as Question Answering (QA) (Pan et al., 2024  ###reference_b31###).\nRetrieval-augmented generation (RAG) is a framework that alleviates LLM hallucinations by enriching the input context with up-to-date and accurate information (Lewis et al., 2020  ###reference_b24###), e.g., obtained from the KG. In the KGQA task, the goal is to answer natural questions grounding the reasoning to the information provided by the KG. For instance, the input for RAG becomes \u201cKnowledge: Jamaica  language_spoken  English \\n Question: Which language do Jamaican people speak?\u201d, where the LLM has access to KG information for answering the question.\nRAG\u2019s performance highly depends on the KG facts that are retrieved (Wu et al., 2023  ###reference_b50###). The challenge is that KGs store complex graph information (they usually consist of millions of facts) and retrieving the right information requires effective graph processing, while retrieving irrelevant information may confuse the LLM during its KGQA reasoning (He et al., 2024  ###reference_b13###). Existing retrieval methods that rely on LLMs to retrieve relevant KG information (LLM-based retrieval) underperform on multi-hop KGQA as they cannot handle complex graph information (Baek et al., 2023  ###reference_b3###; Luo et al., 2024  ###reference_b27###) or they need the internal knowledge of very large LMs, e.g., GPT-4, to compensate for missing information during KG retrieval (Sun et al., 2024  ###reference_b39###).\nIn this work, we introduce Gnn-Rag, a novel method for improving RAG for KGQA. Gnn-Rag relies on Graph Neural Networks (GNNs) (Mavromatis and Karypis, 2022  ###reference_b28###), which are powerful graph representation learners, to handle the complex graph information stored in the KG. Although GNNs cannot understand natural language the same way LLMs do, Gnn-Rag repurposes their graph processing power for retrieval. First, a GNN reasons over a dense KG subgraph to retrieve answer candidates for a given question. Second, the shortest paths in the KG that connect question entities and GNN-based answers are extracted to represent useful KG reasoning paths. The extracted paths are verbalized and given as input for LLM reasoning with RAG.\nFurthermore, we show that Gnn-Rag can be augmented with LLM-based retrievers to further boost KGQA performance.\nExperimental results show Gnn-Rag\u2019s superiority over competing RAG-based systems for KGQA by outperforming them by up to 15.5% points at complex KGQA performance (Figure 1  ###reference_###).\nOur contributions are summarized below:\nFramework: Gnn-Rag repurposes GNNs for KGQA retrieval to enhance the reasoning abilities of LLMs. In our Gnn-Rag framework, the GNN acts as a dense subgraph reasoner to extract useful graph information, while the LLM leverages its natural language processing ability for ultimate KGQA. Moreover, our retrieval analysis (Section 4.3  ###reference_###) guides the design of a retrieval augmentation (RA) technique to boost Gnn-Rag\u2019s performance (Section 4.4  ###reference_###).\nEffectiveness & Faithfulness: Gnn-Rag achieves state-of-the-art performance in two widely\nused KGQA benchmarks (WebQSP and CWQ). Gnn-Rag retrieves multi-hop information that is necessary for faithful LLM reasoning on complex questions (8.9\u201315.5% improvement; see Figure 1  ###reference_###).\nEfficiency: Gnn-Rag improves vanilla LLMs on KGQA performance without incurring additional LLM calls as existing RAG systems for KGQA require. In addition, Gnn-Rag outperforms or matches GPT-4 performance with a 7B tuned LLM."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "KGQA Methods. KGQA methods fall into two categories (Lan et al., 2022  ###reference_b23###): (A) Semantic Parsing (SP) methods and (B) Information Retrieval (IR) methods. SP methods (Sun et al., 2020  ###reference_b40###; Lan and Jiang, 2020  ###reference_b22###; Ye et al., 2022  ###reference_b52###) learn to transform the given question into a query of logical form, e.g., SPARQL query. The transformed query is then executed over the KG to obtain the answers. However, SP methods require ground-truth logical queries for training, which are time-consuming to annotate in practice, and may lead non-executable queries due to syntactical or semantic errors (Das et al., 2021  ###reference_b10###; Yu et al., 2022  ###reference_b54###). IR methods (Sun et al., 2018  ###reference_b37###, 2019  ###reference_b38###) focus on the weakly-supervised KGQA setting, where only question-answer pairs are given for training. IR methods retrieve KG information, e.g., a KG subgraph (Zhang et al., 2022a  ###reference_b55###), which is used as input during KGQA reasoning. In Appendix A  ###reference_###, we analyze the reasoning abilities of the prevailing models (GNNs & LLMs) for KGQA, and in Section 4  ###reference_###, we propose Gnn-Rag which leverages the strengths of both of these models.\n\nGraph-augmented LMs. Combining LMs with graphs that store information in natural language is an emerging research area (Jin et al., 2023  ###reference_b18###). There are two main directions, (i) methods that enhance LMs with latent graph information (Zhang et al., 2022b  ###reference_b57###; Tian et al., 2024  ###reference_b43###; Huang et al., 2024  ###reference_b15###), e.g., obtained by GNNs, and (ii) methods that insert verbalized graph information at the input (Xie et al., 2022  ###reference_b51###; Jiang et al., 2023a  ###reference_b16###; Jin et al., 2024  ###reference_b19###), similar to RAG. The methods of the first direction are limited because of the modality mismatch between language and graph, which can lead to inferior performance for knowledge-intensive tasks (Mavromatis et al., 2024  ###reference_b29###). On the other hand, methods of the second direction may fetch noisy information when the underlying graph is large and such information can decrease the LM\u2019s reasoning ability (Wu et al., 2023  ###reference_b50###; He et al., 2024  ###reference_b13###). Gnn-Rag employs GNNs for information retrieval and RAG for KGQA reasoning, achieving superior performance over existing approaches.\n\nAI in Healthcare Diagnostics. The integration of AI in healthcare diagnostics has shown promise in enhancing accuracy and efficiency (Esteva et al., 2017  ###reference_b9###; Jiang et al., 2020  ###reference_b17###). Deep learning models can identify patterns in medical imaging with remarkable precision, often surpassing human capabilities (Litjens et al., 2017  ###reference_b28###; Chen et al., 2021  ###reference_b8###), thereby facilitating timely interventions."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Problem Statement & Background",
            "text": "KGQA. We are given a KG  that contains facts represented as , where  denotes the head entity,  denotes the tail entity, and  is the corresponding relation between the two entities.\nGiven  and a natural language question , the task of KGQA is to extract a set of entities  that correctly answer . Following previous works (Lan et al., 2022  ###reference_b23###), question-answer pairs are given for training, but not the ground-truth paths that lead to the answers.\nRetrieval & Reasoning. As KGs usually contain millions of facts and nodes, a smaller question-specific subgraph  is retrieved for a question , e.g., via entity linking and neighbor extraction (Yih et al., 2015  ###reference_b53###). Ideally, all correct answers for the question are contained in the retrieved subgraph, .\nThe retrieved subgraph  along with the question  are used as input to a reasoning model, which outputs the correct answer(s). The prevailing reasoning models for the KGQA setting studied are GNNs and LLMs.\nGNNs. KGQA can be regarded as a node classification problem, where KG entities are classified as answers vs. non-answers for a given question.\nGNNs Kipf and Welling (2016  ###reference_b21###); Veli\u010dkovi\u0107 et al. (2017  ###reference_b46###); Schlichtkrull et al. (2018  ###reference_b34###) are powerful graph representation learners suited for tasks such as node classification.\nGNNs update the representation  of node  at layer  by aggregating messages  from each neighbor . During KGQA, the message passing is also conditioned to the given question  (He et al., 2021  ###reference_b12###). For readability purposes, we present the following GNN update for KGQA,\nwhere function  measures how relevant relation  of fact  is to question . Neighbor messages  are aggregated by a sum-operator , which is typically employed in GNNs. Function  combines representations from consecutive GNN layers.\nLLMs. LLMs for KGQA use KG information to perform retrieval-augmented generation (RAG) as follows. The retrieved subgraph is first converted into natural language so that it can be processed by the LLM. The input given to the LLM contains the KG factual information along with the question and a prompt. For instance, the input becomes \u201cKnowledge: Jamaica  language_spoken  English \\n Question: Which language do Jamaican people speak?\u201d, where the LLM has access to KG information for answering the question.\nLandscape of KGQA methods. Figure 2  ###reference_### presents the landscape of existing KGQA methods with respect to KG retrieval and reasoning. GNN-based methods, such as GraftNet (Sun et al., 2018  ###reference_b37###), NSM (He et al., 2021  ###reference_b12###), and ReaRev (Mavromatis and Karypis, 2022  ###reference_b28###), reason over a dense KG subgraph leveraging the GNN\u2019s ability to handle complex graph information.\nRecent LLM-based methods leverage the LLM\u2019s power for both retrieval and reasoning. ToG (Sun et al., 2024  ###reference_b39###) uses the LLM to retrieve relevant facts hop-by-hop. RoG (Luo et al., 2024  ###reference_b27###) uses the LLM to generate plausible relation paths which are then mapped on the KG to retrieve the relevant information.\nLLM-based Retriever. We present an example of an LLM-based retriever (RoG;  (Luo et al., 2024  ###reference_b27###)). Given training question-answer pairs, RoG extracts the shortest paths to the answers starting from question entities for fine-tuning the retriever. Based on the extracted paths, an LLM (LLaMA2-Chat-7B (Touvron et al., 2023  ###reference_b44###)) is fine-tuned to generate reasoning paths given a question  as\nwhere the prompt is \u201cPlease generate a valid relation path that can be helpful for answering the following question: {Question}\u201d.\nBeam-search decoding is used to generate  diverse sets of reasoning paths for better answer coverage, e.g., relations {<official_language>, <language_spoken>} for the question \u201cWhich language do Jamaican people speak?\u201d. The generated paths are mapped on the KG, starting from the question entities, in order to retrieve the intermediate entities for RAG, e.g., <Jamaica  language_spoken  English>."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Gnn-Rag",
            "text": "We introduce Gnn-Rag, a novel method for combining language understanding abilities of LLMs with the reasoning abilities of GNNs in a retrieval-augmented generation (RAG) style. We provide the overall framework in Figure 3  ###reference_###. First, a GNN reasons over a dense KG subgraph to retrieve answer candidates for a given question. Second, the shortest paths in the KG that connect question entities and GNN-based answers are extracted to represent useful KG reasoning paths. The extracted paths are verbalized and given as input for LLM reasoning with RAG. In our Gnn-Rag framework, the GNN acts as a dense subgraph reasoner to extract useful graph information, while the LLM leverages its natural language processing ability for ultimate KGQA.\n###figure_2###"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "GNN",
            "text": "In order to retrieve high-quality reasoning paths via Gnn-Rag, we leverage state-of-the-art GNNs for KGQA. We prefer GNNs over other KGQA methods, e.g., embedding-based methods (Saxena et al., 2020  ###reference_b33###), due to their ability to handle complex graph interactions and answer multi-hop questions. GNNs mark themselves as good candidates for retrieval due to their architectural benefit of exploring diverse reasoning paths (Mavromatis and Karypis, 2022  ###reference_b28###; Choi et al., 2024  ###reference_b7###) that result in high answer recall.\nWhen GNN reasoning is completed ( GNN updates via Equation 1  ###reference_###), all nodes in the subgraph are scored as answers vs. non-answers based on their final GNN representations , followed by the  operation. The GNN parameters are optimized via node classification (answers vs. non-answers) using the training question-answer pairs. During inference, the nodes with the highest probability scores, e.g., above a probability threshold, are returned as candidate answers, along with the shortest paths connecting the question entities with the candidate answers (reasoning paths). The retrieved reasoning paths are used as input for LLM-based RAG.\nDifferent GNNs may fetch different reasoning paths for RAG. As presented in  Equation 3  ###reference_###, GNN reasoning depends on the question-relation matching operation . A common implementation of  is  (He et al., 2021  ###reference_b12###), where function  is a neural network, and  is the element-wise multiplication. Question representations  and KG relation representations  are encoded via a shared pretrained LM (Jiang et al., 2023b  ###reference_b17###) as\nwhere  are attention-based pooling neural networks so that different representations  attend to different question tokens, and  is the [CLS] token pooling.\nIn Appendix B  ###reference_###, we develop a Theorem that shows that the GNN\u2019s output depends on the question-relation matching operation  and as result, the choice of the LM in Equation 3  ###reference_### plays an important role regarding which answer nodes are retrieved.\nInstead of trying different GNN architectures, we employ different LMs in Equation 3  ###reference_### that result into different output node representations. Specifically, we train two separate GNN models, one using pretrained LMs, such as SBERT (Reimers and Gurevych, 2019  ###reference_b32###), and one using , a pretrained LM for question-relation matching over the KG (Zhang et al., 2022a  ###reference_b55###). Our experimental results suggest that, although these GNNs retrieve different KG information, they both improve RAG-based KGQA."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "LLM",
            "text": "After obtaining the reasoning paths by Gnn-Rag, we verbalize them and give them as input to a downstream LLM, such as ChatGPT or LLaMA. However, LLMs are sensitive to the input prompt template and the way that the graph information is verbalized.\nTo alleviate this issue, we opt to follow RAG prompt tuning (Lin et al., 2023  ###reference_b26###; Zhang et al., 2024  ###reference_b56###) for LLMs that have open weights and are feasible to train. A LLaMA2-Chat-7B model is fine-tuned based on the training question-answer pairs to generate a list of correct answers, given the prompt: \n\u201cBased on the reasoning paths, please answer the given question.\\n Reasoning Paths: {Reasoning Paths} \\n Question: {Question}\u201d. \nThe reasoning paths are verbalised as \u201c{question entity}  {relation}  {entity}    {relation}  {answer entity} \\n\u201d (see Figure 3  ###reference_###). \nDuring training, the reasoning paths are the shortest paths from question entities to answer entities. During inference, the reasoning paths are obtained by Gnn-Rag."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Retrieval Analysis: Why GNNs & Their Limitations",
            "text": "GNNs leverage the graph structure to retrieve relevant parts of the KG that contain multi-hop information. We provide experimental evidence on why GNNs are good retrievers for multi-hop KGQA.\nWe train two different GNNs, a deep one () and a shallow one (), and measure their retrieval capabilities. We report the \u2018Answer Coverage\u2019 metric, which evaluates whether the retriever is able to fetch at least one correct answer for RAG. Note that \u2018Answer Coverage\u2019 does not measure downstream KGQA performance but whether the retriever fetches relevant KG information. \u2018#Input Tokens\u2019 denotes the median number of the input tokens of the retrieved KG paths.\nTable 1  ###reference_### shows GNN retrieval results for single-hop and multi-hop questions of the WebQSP dataset compared to an LLM-based retriever (RoG; Equation 2  ###reference_###). The results indicate that deep GNNs () can handle the complex graph structure and retrieve useful multi-hop information more effectively (%Ans. Cov.) and efficiently (#Input Tok.) than the LLM and the shallow GNN.\nOn the other hand, the limitation of GNNs is for simple (1-hop) questions, where accurate question-relation matching is more important than deep graph search (see our Theorem in Appendix A  ###reference_### that states this GNN limitation). In such cases, the LLM retriever is better at selecting the right KG information due to its natural language understanding abilities (we provide an example later in Figure 5  ###reference_###)."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Retrieval Augmentation (RA)",
            "text": "Retrieval augmentation (RA) combines the retrieved KG information from different approaches to increase diversity and answer recall. Motivated by the results in Section 4.3  ###reference_###, we present a RA technique (Gnn-Rag+RA), which complements the GNN retriever with an LLM-based retriever to combine their strengths on multi-hop and single-hop questions, respectively. Specifically, we experiment with the RoG retrieval, which is described in Equation 2  ###reference_###. During inference, we take the union of the reasoning paths retrieved by the two retrievers.\nA downside of LLM-based retrieval is that it requires multiple generations (beam-search decoding) to retrieve diverse paths, which trades efficiency for effectiveness (we provide a performance analysis in Appendix A  ###reference_###). A cheaper alternative is to perform RA by combining the outputs of different GNNs, which are equipped with different LMs in Equation 3  ###reference_###. Our Gnn-Rag+Ensemble takes the union of the retrieved paths of the two different GNNs (GNN+SBERT & GNN+) as input for RAG."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experimental Setup",
            "text": "KGQA Datasets.\nWe experiment with two widely used KGQA benchmarks: WebQuestionsSP (WebQSP) (Yih et al., 2015  ###reference_b53###),\nComplex WebQuestions 1.1 (CWQ) (Talmor and Berant, 2018  ###reference_b41###).\nWebQSP contains 4,737 natural language questions that are answerable using a subset Freebase KG (Bollacker et al., 2008  ###reference_b4###). The questions require up to 2-hop reasoning within this KG. CWQ contains 34,699 total complex questions that require up to 4-hops of reasoning over the KG. We provide the detailed dataset statistics in Appendix C  ###reference_###.\nImplementation & Evaluation.\nFor subgraph retrieval, we use the linked entities and the pagerank algorithm to extract dense graph information (He et al., 2021  ###reference_b12###). We employ ReaRev (Mavromatis and Karypis, 2022  ###reference_b28###), which is a GNN targeting at deep KG reasoning (Section 4.3  ###reference_###), for Gnn-Rag. The default implementation is to combine ReaRev with SBERT as the LM in Equation 3  ###reference_###. In addition, we combine ReaRev with , which is obtained by following the implementation of SR (Zhang et al., 2022a  ###reference_b55###). We employ RoG (Luo et al., 2024  ###reference_b27###) for RAG-based prompt tuning (Section 4.2  ###reference_###).\nFor evaluation, we adopt Hit, Hits@1 (H@1), and F1 metrics. Hit measures if any of the true answers is found in the generated response, which is typically employed when evaluating LLMs. H@1 is the accuracy of the top/first predicted answer. F1 takes into account the recall (number of true answers found) and the precision (number of false answers found) of the generated answers.\nFurther experimental setup details are provided in Appendix C  ###reference_###.\nHit is used for LLM evaluation.\nWe use the default Gnn-Rag (+RA) implementation. Gnn-Rag, RoG, KD-CoT, and G-Retriever use 7B fine-tuned LLaMA2 models. KD-CoT employs ChatGPT as well.\nCompeting Methods. We compare with SOTA GNN and LLM methods for KGQA (Mavromatis and Karypis, 2022  ###reference_b28###; Li et al., 2023  ###reference_b25###). We also include earlier embedding-based methods (Saxena et al., 2020  ###reference_b33###) as well as zero-shot/few-shot LLMs (Taori et al., 2023  ###reference_b42###). We do not compare with semantic parsing methods (Yu et al., 2022  ###reference_b54###) as they use additional training data (SPARQL annotations), which are difficult to obtain in practice. Furthermore, we compare Gnn-Rag with LLM-based retrieval approaches (Luo et al., 2024  ###reference_b27###; Sun et al., 2024  ###reference_b39###) in terms of efficiency and effectiveness."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "Main Results.\nTable 2  ###reference_### presents performance results of different KGQA methods. Gnn-Rag is the method that performs overall the best, achieving state-of-the-art results on the two KGQA benchmarks in almost all metrics. The results show that equipping LLMs with GNN-based retrieval boosts their reasoning ability significantly (GNN+LLM vs. KG+LLM). Specifically, Gnn-Rag+RA outperforms RoG by 5.0\u20136.1% points at Hit, while it outperforms or matches ToG+GPT-4 performance, using an LLM with only 7B parameters and much fewer LLM calls \u2013 we estimate ToG+GPT-4 has an overall cost above $800, while Gnn-Rag can be deployed on a single 24GB GPU. Gnn-Rag+RA outperforms ToG+ChatGPT by up to 14.5% points at Hit and the best performing GNN by 5.3\u20139.5% points at Hits@1 and by 0.7\u201310.7% points at F1.\nMulti-Hop & Multi-Entity KGQA. Table 3  ###reference_### compares performance results on multi-hop questions, where answers are more than one hop away from the question entities, and multi-entity questions, which have more than one question entities. Gnn-Rag leverages GNNs to handle complex graph information and outperforms RoG (LLM-based retrieval) by 6.5\u201317.2% points at F1 on WebQSP and by 8.5\u20138.9% points at F1 on CWQ. In addition, Gnn-Rag+RA offers an additional improvement by up to 6.5% points at F1. The results show that Gnn-Rag is an effective retrieval method when deep graph search is important for successful KGQA.\nFor other experiments, we use the default Gnn-Rag and Gnn-Rag+RA implementations. The GNN used is ReaRev.\nRetrieval Augmentation. Table 4  ###reference_### compares different retrieval augmentations for Gnn-Rag. The primary metric is F1, while the other metrics assess how well the methods retrieve relevant information from the KG. Based on the results, we make the following conclusions:\nGNN-based retrieval is more efficient (#LLM Calls, #Input Tokens) and effective (F1) than LLM-based retrieval, especially for complex questions (CWQ); see rows (e-f) vs. row (d).\nRetrieval augmentation works the best (F1) when combining GNN-induced reasoning paths with LLM-induced reasoning paths as they fetch non-overlapping KG information (increased #Input Tokens) that improves retrieval for KGQA; see rows (h) & (i).\nAugmenting all retrieval approaches does not necessarily cause improved performance (F1) as the long input (#Input Tokens) may confuse the LLM; see rows (g/j) vs. rows (e/h).\nAlthough the two GNNs perform differently at KGQA (F1), they both improve RAG with LLMs; see rows (a-b) vs. rows (e-f). We note though that weak GNNs are not effective retrievers (see Appendix D.2  ###reference_###).\nIn addition, Gnn-Rag improves the vanilla LLM by up to 176% at F1 without incurring additional LLM calls; see row (c) vs. row (e). Overall, retrieval augmentation of GNN-induced and LLM-induced paths combines their strengths and achieves the best KGQA performance.\nRetrieval Effect on LLMs.\nTable 5  ###reference_### presents performance results of various LLMs using Gnn-Rag or LLM-based retrievers (RoG and ToG). We report the Hit metric as it is difficult to extract the number of answers from LLM\u2019s output. Gnn-Rag (+RA) is the retrieval approach that achieves the largest improvements for RAG. For instance, Gnn-Rag+RA improves ChatGPT by up to 6.5% points at Hit over RoG and ToG. Moreover, Gnn-Rag substantially improves the KGQA performance of weaker LLMs, such as Alpaca-7B and Flan-T5-xl. The improvement over RoG is up to 13.2% points at Hit, while Gnn-Rag outperforms LLaMA2-Chat-70B+ToG using a lightweight 7B LLaMA2 model. The results demonstrate that Gnn-Rag can be integrated with other LLMs to improve their KGQA reasoning without retraining.\nCase Studies on Faithfulness. Figure 4  ###reference_### illustrates two case studies from the CWQ dataset, showing how Gnn-Rag improves LLM\u2019s faithfulness, i.e., how well the LLM follows the question\u2019s instructions and uses the right information from the KG. In both cases, Gnn-Rag retrieves multi-hop information, which is necessary for answering the questions correctly. In the first case, Gnn-Rag retrieves both crucial facts <Gilfoyle  characters_that_have_lived_here  Toronto> and <Toronto  province.capital  Ontario> that are required to answer the question, unlike the KG-RAG baseline (RoG) that fetches only the first fact. In the second case, the KG-RAG baseline incorrectly retrieves information about <Erin Brockovich  person> and not <Erin Brockovich  film_character> that the question refers to. Gnn-Rag uses GNNs to explore how <Erin Brockovich> and <Michael Renault Mageau> entities are related in the KG, resulting into retrieving facts about <Erin Brockovich  film_character>. The retrieved facts include important information <films_with_this_crew_job  Consultant>.\nFigure 5  ###reference_### illustrates one case study from the WebQSP dataset, showing how RA (Section 4.4  ###reference_###) improves Gnn-Rag. Initially, the GNN does not retrieve helpful information due to its limitation to understand natural language, i.e., that <jurisdiction.bodies> usually \u201cmake the laws\u201d. Gnn-Rag+RA retrieves the right information, helping the LLM answer the question correctly.\nFurther ablation studies are provided in Appendix D  ###reference_###. Limitations are discussed in Appendix E  ###reference_###.\n###figure_3### ###figure_4###"
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We introduce Gnn-Rag, a novel method for combining the reasoning abilities of LLMs and\nGNNs for RAG-based KGQA. Our contributions are the following. (1) Framework: Gnn-Rag repurposes GNNs for KGQA retrieval to enhance the reasoning\nabilities of LLMs. Moreover, our retrieval analysis guides the design of a\nretrieval augmentation technique to boost Gnn-Rag performance. (2) Effectiveness & Faithfulness: Gnn-Rag achieves state-of-the-art performance in two\nwidely used KGQA benchmarks (WebQSP and CWQ). Furthermore, Gnn-Rag is shown to retrieve multi-hop information that is necessary for faithful LLM reasoning on complex questions. (3) Efficiency: Gnn-Rag improves vanilla LLMs on KGQA performance without incurring additional LLM calls as existing RAG systems for KGQA require. In addition, Gnn-Rag outperforms or matches GPT-4 performance with a 7B tuned LLM."
        }
    ],
    "url": "http://arxiv.org/html/2405.20139v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "3"
        ],
        "methodology_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "6"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.3",
            "4.4",
            "6"
        ]
    },
    "research_context": {
        "paper_id": "2405.20139v1",
        "paper_title": "Gnn-Rag: Graph Neural Retrieval for Large Language Model Reasoning",
        "research_background": "**Paper's Motivation:**\nThe paper is motivated by the limitations of Large Language Models (LLMs) in effectively adapting to new or domain-specific knowledge, as well as their tendency to generate hallucinations due to the static nature of their pretraining on large corpora. Although LLMs excel at many natural language processing tasks, the inability to update their knowledge readily and their predisposition to incorrect information pose significant challenges, especially in knowledge-intensive tasks like Question Answering (QA). Knowledge Graphs (KGs) are emerging as a solution to provide structured, up-to-date information. However, existing retrieval-augmented generation (RAG) frameworks that use KGs face difficulties in effectively retrieving and processing complex graph information, particularly for multi-hop questions.\n\n**Research Problem:**\nThe main research problem addressed by this paper is to improve the retrieval of relevant and complex information from Knowledge Graphs (KGs) to enhance the reasoning abilities of LLMs in knowledge-intensive tasks like Question Answering (QA). Existing methods relying on LLMs for KG retrieval underperform on multi-hop questions because they can't efficiently handle complex graph structures or depend on the internal knowledge of very large language models like GPT-4.\n\n**Relevant Prior Work:**\n1. **Large Language Models:** The power of LLMs comes from pretraining on extensive text corpora to gain general human knowledge. Notable works include research by Brown et al. (2020), Bommasani et al. (2021), and Chowdhery et al. (2023).\n2. **Challenges with LLMs:** Pretraining is costly and time-consuming, making LLMs less adaptable to new or in-domain knowledge and prone to hallucinations, as highlighted by Kaplan et al. (2020), Hoffmann et al. (2022), Gururangan et al. (2020), and Zhang et al. (2023).\n3. **Knowledge Graphs:** KGs store information in a structured form of triplets and can be easily updated. They capture complex interactions between stored entities and are widely used in Question Answering, as discussed by Vrande\u010di\u0107 and Kr\u00f6tzsch (2014) and Pan et al. (2024).\n4. **Retrieval-Augmented Generation (RAG):** The RAG framework enriches input context with accurate, up-to-date information to reduce LLM hallucinations (Lewis et al., 2020). Effective performance in RAG depends on retrieving relevant KG facts.\n5. **Current Retrieval Challenges:** Current LLM-based retrieval methods underperform on multi-hop KGQA due to their inability to handle complex graph information, as noted by Baek et al. (2023) and Luo et al. (2024), or they require the internal knowledge of very large LMs like GPT-4 as noted by Sun et al. (2024).\n6. **Graph Neural Networks (GNNs):** GNNs are powerful tools for graph representation learning and processing complex graph information, as discussed by Mavromatis and Karypis (2022).\n\nThe proposed work introduces Gnn-Rag, which leverages GNNs to address the retrieval challenges in RAG frameworks specifically for KGQA, thereby enhancing LLMs' reasoning capabilities with complex and multi-hop questions.",
        "methodology": "Gnn-Rag: Graph Neural Retrieval for Large Language Model Reasoning\n\n**Methodology:**\n\nWe introduce **Gnn-Rag**, a novel method for combining the language understanding abilities of **LLMs** with the reasoning abilities of **GNNs** in a **retrieval-augmented generation (RAG)** style.\n\nFirst, a **GNN** reasons over a dense **KG** subgraph to retrieve answer candidates for a given question. Second, the shortest paths in the KG that connect question entities and GNN-based answers are extracted to represent useful KG reasoning paths. The extracted paths are verbalized and given as input for **LLM reasoning** with RAG.\n\nIn our **Gnn-Rag** framework, the GNN acts as a dense subgraph reasoner to extract useful graph information, while the LLM leverages its natural language processing ability for ultimate **KGQA**.",
        "main_experiment_and_results": "### Main Experiment Setup and Results: KGQA Datasets\n\n#### Datasets\nThe main experiment utilizes two widely used Knowledge Graph Question Answering (KGQA) benchmarks:\n1. **WebQuestionsSP (WebQSP):**\n   - Contains 4,737 natural language questions.\n   - Questions are answerable using a subset of the Freebase KG.\n   - Requires up to 2-hop reasoning within this KG.\n   \n2. **Complex WebQuestions 1.1 (CWQ):**\n   - Contains 34,699 complex questions.\n   - Requires up to 4-hops of reasoning over the KG.\n\n#### Model and Baselines\nThe Gnn-Rag model is implemented using the following components:\n- **Subgraph Retrieval:** Linked entities and the PageRank algorithm to extract dense graph information.\n- **GNN Model:** ReaRev, targeting deep KG reasoning, combined with SBERT as the Language Model (LM).\n\nAdditional Configuration:\n- **Prompt Tuning:** RoG is employed for RAG (Retrieval-Augmented Generation) based prompt tuning.\n- **Comparative Baselines:** \n  - Gnn-Rag, RoG, KD-CoT (employs ChatGPT), and G-Retriever use 7B fine-tuned LLaMA2 models.\n  - SOTA GNN and LLM methods for KGQA.\n  - Earlier embedding-based methods.\n  - Zero-shot/few-shot LLMs.\n  - LLM-based retrieval approaches in terms of efficiency and effectiveness.\n  \nSemantic parsing methods are excluded due to the impracticality of obtaining additional training data (SPARQL annotations).\n\n#### Evaluation Metrics\n- **Hit:** Measures if any of the true answers are found in the generated response, typically used for LLM evaluation.\n- **Hits@1 (H@1):** Accuracy of the top/first predicted answer.\n- **F1 Score:** Takes into account both recall (number of true answers found) and precision (number of false answers found) of the generated answers.\n\n#### Main Experimental Results\nThe detailed results are included in the main text of the paper and the further experimental setup details are provided in Appendix C. The key results demonstrate that the proposed Gnn-Rag framework outperforms SOTA methods in both the WebQSP and CWQ datasets in terms of Hits@1 and F1 metrics, validating the effectiveness of combining GNN-based retrieval with LLMs for complex KGQA tasks."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "The objective is to evaluate the retrieval effectiveness of GNNs for multi-hop KGQA and identify their limitations.",
            "experiment_process": "Two different GNNs, a deep one and a shallow one, were trained and their retrieval capabilities measured using the 'Answer Coverage' metric on the WebQSP dataset. This metric evaluates whether the retriever can fetch at least one correct answer for RAG, and 'Input Tokens' denotes the median number of the input tokens of the retrieved KG paths. Results were compared with an LLM-based retriever (RoG).",
            "result_discussion": "Results indicated that deep GNNs handle the complex graph structure and retrieve useful multi-hop information more effectively and efficiently than the LLM and the shallow GNN. However, for simple (1-hop) questions, LLM retrievers are better due to their natural language understanding abilities.",
            "ablation_id": "2405.20139v1.No1"
        },
        {
            "research_objective": "The goal is to test a Retrieval Augmentation (RA) technique that combines the strengths of GNN and LLM-based retrievers to improve KGQA performance, especially on multi-hop and single-hop questions.",
            "experiment_process": "The Gnn-Rag+RA technique was experimented with by taking the union of reasoning paths retrieved by GNN and LLM-based retrievers (RoG) during inference. Additionally, a cheaper alternative was tested by combining outputs of different GNNs equipped with different LMs.",
            "result_discussion": "The results showed that combining GNN-induced and LLM-induced reasoning paths improved retrieval for KGQA. While GNN-based retrieval is more efficient and effective for complex questions, combining approaches increases answer recall and diversity. However, augmenting all retrieval approaches does not necessarily improve performance due to longer input confusions for the LLM.",
            "ablation_id": "2405.20139v1.No2"
        }
    ]
}