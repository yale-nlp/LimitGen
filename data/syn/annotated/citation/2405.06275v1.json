{
    "title": "Pruning as a Domain-specific LLM Extractor",
    "abstract": "Large Language Models (LLMs) have exhibited remarkable proficiency across a wide array of NLP tasks. However, the escalation in model size also engenders substantial deployment costs. While few efforts have explored model pruning techniques to reduce the size of LLMs, they mainly center on general or task-specific weights.\nThis leads to suboptimal performance due to lacking specificity on the target domain or generality on different tasks when applied to domain-specific challenges.\nThis work introduces an innovative unstructured dual-pruning methodology, D-Pruner, for domain-specific compression on LLM. It extracts a compressed, domain-specific, and task-agnostic LLM by identifying LLM weights that are pivotal for general capabilities, like linguistic capability and multi-task solving, and domain-specific knowledge.\nMore specifically, we first assess general weight importance by quantifying the error incurred upon their removal with the help of an open-domain calibration dataset.\nThen, we utilize this general weight importance\nto refine the training loss, so that it preserves generality when fitting into a specific domain. Moreover, by efficiently approximating weight importance with the refined training loss on a domain-specific calibration dataset, we obtain a pruned model emphasizing generality and specificity. Our comprehensive experiments across various tasks in healthcare and legal domains show the effectiveness of D-Pruner in domain-specific compression. Our code is available at https://github.com/psunlpgroup/D-Pruner.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large Language Models (LLMs) such as the GPT family (Brown et al., 2020  ###reference_b5###) and the LLaMA family (Touvron et al., 2023  ###reference_b38###) have exhibited remarkable advancements across a diverse spectrum of NLP tasks.\nHowever, the substantial size of LLMs engenders cost-intensive deployment in real-world applications and renders them unsuitable for scenarios necessitating efficient inference and low latency (Bai et al., 2024  ###reference_b1###).\nRecently, model pruning techniques have been successfully applied to language models (Han et al., 2015  ###reference_b13###; Xia et al., 2022  ###reference_b40###; Frantar and Alistarh, 2023  ###reference_b11###). These methods aim to yield a compact language model characterized by a significantly reduced parameter count, which is cost-efficient for deployment. However, most of them target relatively small language models, and only a few focus on LLMs (Frantar and Alistarh, 2023  ###reference_b11###; Ma et al., 2023  ###reference_b26###; Sun et al., 2023  ###reference_b34###; Xia et al., 2023  ###reference_b39###).\nMoreover, the existing strategies mainly center on general or task-specific weights, leading to suboptimal performance due to lacking specificity on the target domain or generality on different tasks when applied to domain-specific challenges.\nHere generality refers to the general capabilities of an LLM such as language understanding and generation, and multi-task solving, and specificity refers to the capability of an LLM to understand domain-specific knowledge.\nAs shown in Figure 1  ###reference_###, the weights in an LLM work together to support its general capabilities and to store various domain knowledge. The domain-shared weights (or general weights) empower the LLM with linguistic and multi-task solving prowess akin to human language usage and thinking. The domain-specific weights (or domain weights) are pivotal for endowing the LLM with domain-specific expertise mirroring that of domain experts.\nHowever, the current pruning methods mainly focus on preserving general or task-specific weights, which may not be enough to deal with domain-specific problems.\nFor example, post-training pruning methods (Frantar and Alistarh, 2023  ###reference_b11###) assume the model is optimized and prune unimportant weights based on an open-domain calibration dataset. This leads to a pruned model that focuses on model generality with domain-specific weights not considered. On the other hand, pruning with fine-tuning methods (Ma et al., 2023  ###reference_b26###) utilizes gradients during fine-tuning on a specific task to estimate the importance of parameters.\nAs a result, the pruned model focuses on the model specificity while decreasing the linguistic and multi-task solving capabilities, compromising the LLM\u2019s capacity as a versatile task-agnostic solver.\n###figure_1### To this end, this study introduces a novel dual-pruning approach, D-Pruner, for domain-specific unstructured pruning on LLMs, which aims to extract a domain-specific LLM from the foundation LLM. This extracted model is able to solve different tasks in the target domain and facilitates further domain-specific fine-tuning.\nD-Pruner is designed to harness calibration data for guiding LLM pruning processes while preserving generality and specificity for multi-task solving and domain challenges. The resulting compressed LLM can be seamlessly adapted to the target domain, enabling deployment with limited computing resources.\nSpecifically, D-Pruner adeptly captures and retains both general and domain parameters while selectively eliminating insignificant model parameters. This mechanism comprises the following steps: firstly, a general weight importance module operates to assess the significance of model parameters for general capabilities. Subsequently, we propose an updated training loss function based on the autoregressive training objective for the next token prediction by integrating the general importance as a regularization term. This way, we identify weights contributing to both generality and domain specificity when training on a domain calibration dataset. Then, with the updated loss function, we compute the weight importance leveraging gradients without updating the model. Moreover, an approximation algorithm, empirical Fisher (Martens, 2020  ###reference_b27###; Sung et al., 2021  ###reference_b35###), is utilized to compute the weight importance efficiently for pruning.\nWe evaluate the performance of D-Pruner on LLaMA2 (Touvron et al., 2023  ###reference_b38###), a widely adopted open-source LLM.\nOur experimental findings demonstrate that D-Pruner exhibits remarkable efficiency in the extraction of sparse domain networks from pre-trained LLMs, with a limited amount of calibration data provided. Remarkably, D-Pruner achieves comparable results to the full dense model while achieving 50% sparsity, surpassing the performance of alternative pruning techniques across diverse domain-specific datasets in healthcare and legal domains encompassing language comprehension, question answering, and summarization tasks."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Model compression involves transforming a large, resource-intensive model into a compact version suitable for low-resource deployment Deng et al. (2020  ###reference_b6###); Zhu et al. (2023  ###reference_b44###). There are mainly three techniques for model compression, which are pruning, knowledge distillation, and quantization. Pruning techniques in neural networks can be broadly classified into structured pruning and unstructured pruning (Xia et al., 2022  ###reference_b40###; Sanh et al., 2020  ###reference_b33###; Du et al., 2021  ###reference_b10###). Structured pruning entails the removal of entire network components, such as channels or layers, guided by specific criteria, while maintaining the overall network architecture. In contrast, unstructured pruning targets individual weights, leading to an irregular sparse structure. While numerous attempts have been made to prune language models of relatively small scales, such as BERT (Kenton and Toutanova, 2019  ###reference_b19###), scant attention has been devoted to pruning LLMs containing billions of parameters. These larger models possess 100-1000 times more weights, rendering the pruning task significantly more challenging. SparseGPT (Frantar and Alistarh, 2023  ###reference_b11###), a post-training method for Large Language Models (LLMs), lacks the capability to identify crucial weights tailored to specific domains or tasks as it refrains from fine-tuning. On the other hand, LLM-Pruner (Ma et al., 2023  ###reference_b26###) employs gradient-based techniques for pruning. However, it falls short in identifying pivotal weights essential for domain-shared knowledge, resulting in pruned models that lack the desired level of generality. The existing pruning methods either focus on general or domain-specific weights, yet none of them consider preserving both at the same time. To the best of our knowledge, we are the first to work on pruning LLMs while preserving weights important to both generality and specificity. Knowledge Distillation (KD) has emerged as a powerful technique, drawing considerable interest for its ability to augment model performance and enhance generalization capacities (Hinton et al., 2015  ###reference_b16###; Zhu et al., 2023  ###reference_b44###). At its core, KD revolves around the transfer of expertise from a complex model, referred to as the \u201cteacher model\u201d, to a simplified counterpart known as the \u201cstudent model\u201d. This intricate process of knowledge transfer aims to distill the profound insights encapsulated within the teacher models, condensing them into a more concise and efficient representation within the student models. While KD has been proven a powerful tool for model compression, it needs specific downstream tasks and a large amount of data for the student models to learn from the teacher models. Thus, the output that student models produce mainly focuses on a specific task and loses the generality capability. KD generally sets higher requirements on data availability and computation budgets (e.g., GPU memory) than pruning. In the realm of model compression, quantization has emerged as a widely embraced technique to alleviate the storage and computational challenges inherent in deep learning models (Guo et al., 2020  ###reference_b12###; Dettmers et al., 2021  ###reference_b8###, 2022  ###reference_b7###, 2023  ###reference_b9###). Conventional model representations rely on floating-point numbers, but quantization converts them into integers or discrete forms. This transformation leads to substantial reductions in storage requirements and computational complexities. While a certain degree of precision loss is inevitable, carefully designed quantization methods can achieve significant model compression with minimal accuracy degradation. Although challenges remain, such as maintaining model interpretability and addressing task-specific intricacies, the current body of research establishes a robust groundwork for ongoing advancements in LLM quantization, which could be complementary to LLM pruning.\n\nArtificial intelligence in autonomous vehicles continues to evolve, focusing on enhanced decision-making and safety (Bojarski et al., 2016 ###reference_b50###; Grigorescu et al., 2020 ###reference_b51###). Advanced AI systems integrate machine learning, sensor fusion, and computer vision to navigate complex environments. Ongoing research emphasizes robustness and real-time processing (Kiran et al., 2021 ###reference_b52###), ensuring autonomous vehicles can adapt to dynamic driving scenarios effectively."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "To preserve both generality and specificity on the pruned model, our dual-pruning method D-Pruner considers weights important to both generality and specificity during training on a calibration dataset. Note we only use the weight gradient generated from the training process but do not update the model weights. Our model is pruned in a task-agnostic fashion (e.g., we adopted a pre-training objective, next token prediction, as a part of training loss) so that the pruned model can solve different tasks in the target domain.\nD-Pruner comprises the following steps: firstly, a general weight locating module operates to assess the significance of model parameters for general understanding (Section 3.1  ###reference_###). Subsequently, an updated loss function for the training process is proposed by integrating the general weight importance as a regularization term. This way, we identify weights contributing to both general and domain knowledge (Section 3.2  ###reference_###). Finally, with the updated loss function, we compute the weight gradients on a small domain calibration dataset without updating the model and approximate our dual-pruning weight importance by utilizing the empirical Fisher index (Sung et al., 2021  ###reference_b35###) for pruning (Section 3.3  ###reference_###).\nOur method concentrates on unstructured pruning in a layer-by-layer manner for the Transformers model. We consider query, key, value, and output projections of all self-attention layers and gate (Liu et al., 2021  ###reference_b24###), down, and up projections of all MLP (multilayer perceptron) layers for pruning."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "General Weight Importance",
            "text": "The first step of our method involves locating important weights in terms of general knowledge. Following the same hypothesis as Frantar and Alistarh (2023  ###reference_b11###), we assume that an important weight will cause a larger increase in loss value than those less important ones if it is pruned (set to ) during training. Formally, if a dataset of the open-domain calibration  with size \nis used for training and  stands for weight matrices of a model, the importance of each weight at index , denoted as , can be approximated using Taylor series as shown\nby LeCun et al. (1989  ###reference_b22###):\nwhere  denotes the Hessian matrix, and  is the cross-entropy loss. For a model that is sufficiently trained to a local minimum on its loss curvature (e.g., pretrained foundational language models such as LLaMA), the classic Optimal Brain Surgeon (Hassibi et al., 1993  ###reference_b14###) further approximates the importance of  as:\ncan also be viewed as the error caused by removing the weight . We compute  for all the weights subject to pruning and construct a matrix of importance scores  with respect to general domains that have the same dimension as ."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Updated Loss with Regularization",
            "text": "To identify the weights that are important in both general and domain-specific knowledge, we modify the original loss function of LLM training. In LLM training, cross-entropy loss is used in the next token prediction task (Radford et al., 2018  ###reference_b29###). Similar to Thompson et al. (2019  ###reference_b37###), we add a regularization term to constrain the change of important general weights found in the first step. Suppose that there are  number of prunable weights in total. To train on a domain-specific calibration dataset , we add the proposed regularization term on top of the next token prediction loss  to obtain our final training objective:\nwhere  is the general weight importance,  denotes the updated weight value of ,  is a hyperparameter, and the second term on the right is .\nIn practice, the direct calculation of this regularization term in the forward pass is computationally expensive for two reasons: (1) it involves both  and  which are very large, and (2) gathering updated model parameters () in a partitioned (Rasley et al., 2020  ###reference_b31###) or sharded (Zhao et al., 2023  ###reference_b42###) system is inefficient. Based on the recent success of applying gradient descent on full fine-tuning of LLMs (Lv et al., 2023  ###reference_b25###), we choose to use gradient descent to optimize parameters. Therefore, at a learning rate , denoting the gradient of each parameter with respect to  as , we reduce the regularization term to:\nDuring the backward pass, optimizing this regularization term requires second-order derivatives, which indicates that\nHessian matrices () are needed. Directly computing the Hessian matrices is infeasible for such a large number of parameters. Therefore, we use the Fisher information matrix to approximate the diagonal of the Hessian (Sung et al., 2021  ###reference_b35###). And the Fisher information matrix can be further approximated by the\naverage of the squared gradient of the model\u2019s prediction over . We write the gradient of the regularization with respect to every parameter matrix in a finer granularity:\nWe directly compute  via Equation 5  ###reference_### above\ninstead of relying on PyTorch backward pass to maximize computing efficiency. The final gradient computation of our regularized loss function is shown below:"
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Dual-pruning Importance Score",
            "text": "Finally, we calculate the dual-pruning importance score of each weight, and unimportant weights can be pruned according to their importance.\nWe use Equation LABEL:eq1 for importance estimation instead of Equation 2  ###reference_###, because our model has not converged to an optimum on the target domain. However, direct computation of the Hessian matrix in\nEquation 2  ###reference_### is infeasible since it involves  complexity for each weight update. Therefore, we also\nleverage Sung et al. (2021  ###reference_b35###) to approximate the diagonal of the Hessian, and the final importance score  can be defined as:\nHere  can be neglected according to the quadratic approximation LeCun et al. (1989  ###reference_b22###).\nNote the calculation of  considers both general and domain-specific knowledge via our regularized training objective.\nCombining both regularization and importance estimation via empirical Fisher approximation, our method expects to conduct pruning that maintains weights important to both general and domain-specific knowledge, thus preserving generality and specificity.\nAnd these importance scores are used to guide our pruning decisions. For example, if we set the sparsity level to be 50%, weights that have the smallest 50% of importance scores in each layer will be pruned."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiment Setup",
            "text": "We evaluate D-Pruner on two knowledge-intensive domains, which are healthcare and legal. For model generality under domain-specific challenges, we evaluate the linguistic capability using domain text generation, and evaluate the multi-task solving capability on different domain tasks, i.e., natural language inference (NLI), question answering (QA), and summarization. Since we use domain datasets, the model specificity on domains can also be evaluated. In addition, we fine-tune the pruned model on domain datasets to further evaluate the generality and specificity.\nWe evaluate D-Pruner on the LLaMA2 model family, which is the most used open-source LLM. We mainly apply our pruning method and baseline methods to LLaMA2-7B and LLaMA2-13B to show our results. Our method can also be easily applied to other LLMs with different sizes and architectures. For instance, Appendix B  ###reference_### shows further experiment on BLOOM model (Le Scao et al., 2022  ###reference_b21###)."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Iterative blocking",
            "text": "Motivated by Frantar and Alistarh (2023  ###reference_b11###), we perform experiments (in Table 2  ###reference_###) on D-Pruner with and without iterative blocking. Iterative blocking means to make pruning decisions for every fixed number () of columns within a weight matrix. In other words, instead of selecting a single pruning mask for an entire weight matrix, a pruning sub-mask is selected for every  columns to reach overall sparsity level. We set  for weight matrices with the smallest number of columns and increase  for those with more columns. Except Table 2  ###reference_###, D-Pruner in other tables does not adopt iterative blocking."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Datasets and Evaluations",
            "text": "Datasets.\nTable 1  ###reference_### shows the details of each dataset that we used.\nSpecifically, for healthcare, we select a medical textbook InternalMed_Harrison (Bigby, 1988  ###reference_b4###), MedNLI (Romanov and Shivade, 2018  ###reference_b32###), PubMedQA (Jin et al., 2019  ###reference_b18###), and Health Question Summarization (HQS) from the MEDIQA 2021 shared task 1 (Ben Abacha et al., 2021  ###reference_b3###; Ben Abacha and Demner-Fushman, 2019  ###reference_b2###) as domain datasets. For legal domain, we select MultiLegalPile (Niklaus et al., 2023  ###reference_b28###), CaseHOLD (Zheng et al., 2021  ###reference_b43###), and BillSum (Kornilova and Eidelman, 2019  ###reference_b20###). As for open-domain calibration data, we extract text from C4 dataset (Raffel et al., 2019  ###reference_b30###).\nTo construct our domain-specific calibration data, we select training instances from MedNLI, PubMedQA, and HQS at a ratio of 20%/60%/20% and from CaseHOLD and BillSum at a ratio of 50%/50%. These ratios are determined based on the difficulties and training sizes of these benchmarks. Both NLI and QA tasks that we adopt are asking models to perform classification. We experiment with different sizes of the domain-specific calibration dataset and find a size of 1000 achieves the best trade-off in terms of pruning efficiency and effectiveness for both domains. For model evaluation, besides using the test instances of those benchmarks, we leverage InternalMed_Harrison and MultiLegalPile for perplexity evaluation. 300 paragraphs are selected from each data source to form the test set of perplexity. Note that we use a subset of all the test examples of CaseHOLD and BillSum, since these two benchmarks are significantly larger in size and their individual instance tends to be longer.\nEvaluation Metrics. We first evaluate the linguistic capability of pruned models on InternalMed_Harrison and MultiLegalPile using perplexity. We then evaluate the multi-task solving capability and domain specificity on different domain tasks. Specifically, we choose accuracy metric for NLI task (MedNLI), macro-F1 for QA tasks (PubMedQA and CaseHOLD), and ROUGE scores (Lin, 2004  ###reference_b23###) for summarization tasks (HQS and BillSum)."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Baselines",
            "text": "We compare our method with a variety of LLM pruning baselines. All methods are applied to the same foundation model (either 7B of 13B of LLaMA2) for fair comparisons. As an ablation study, we also evaluate an unstructured pruning method using weight gradient by removing the regularization term in the training loss of D-Pruner.\nMagnitude pruning prunes weights based on their magnitudes (Han et al., 2015  ###reference_b13###). We follow the standard practice of magnitude pruning on language models, where weights are compared layer-wise. Magnitude pruning is a simple and robust baseline that has been demonstrated to outperform many other pruning methods.\nLLM-Pruner is a structured pruning method using weight gradient to evaluate weight importance (Ma et al., 2023  ###reference_b26###). A calibration dataset is used for its gradient calculation, so we combine both open-domain (C4) and domain-specific calibration data when we use LLM-Pruner.\nSparseGPT is an unstructured post-training pruning method (Frantar and Alistarh, 2023  ###reference_b11###). It uses an efficient weight update procedure that iterates between weight removal and weight update at each layer. It also uses a calibration dataset for approximation. Thus, similarly to D-Pruner and LLM-Pruner, we use open-domain and domain-specific calibration data for fair comparisons.\nMoreover, for all the baseline methods, we continue to fine-tune their pruned models using LoRA (Hu et al., 2021  ###reference_b17###) on all the datasets together (NLI, QA, and summarization data combined) in each domain and then test the fine-tuned model on the datasets in Table 1  ###reference_###. We only use the default open-domain calibration dataset for the pruned models of LLM-Pruner and SparseGPT at this step, because these models will eventually undergo LoRA fine-tuning. Data instances of our fine-tuning dataset follow the Alpaca (Taori et al., 2023  ###reference_b36###) template so that models are trained to predict the responses. Specifically, for healthcare, we have 7000, 7000, and 1000 training instances from MedNLI, PubMedQA, and HQS, respectively. For legal domain, we have 13000 training instances from CaseHOLD and 2000 from BillSum."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Implementation Details",
            "text": "We perform prompt engineering in a zero-shot setting before prompting a series of models. The finalized prompt is kept the same across all candidate models on one task to ensure fairness. The hyperparameters used by different models are in Appendix C  ###reference_###."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Results and Analysis",
            "text": "Our results and analysis aim to answer the following research questions:\nRQ 1: How does D-Pruner compare against other pruning baselines (5.1  ###reference_###)?\nRQ 2: What are the performance of all candidate models after LoRA fine-tuning (5.2  ###reference_###)?\nRQ 3: As an important contribution of D-Pruner, is dual-pruning an effective method of compressing LLM (5.1  ###reference_###, 5.3  ###reference_###, and 5.5  ###reference_###)?\nRQ 4: How does D-Pruner perform under different sparsity levels or different sizes of domain-specific calibration data (5.4  ###reference_###)?\nD-Pruner delivers consistent score improvement on NLI and QA tasks when it is compared against baselines based on LLaMA2-7B and LLaMA2-13B. With two exceptions, variants of D-Pruner based on the inclusion and exclusion of iterative blocking outperform baselines on 4 out of 6 cases when classification is performed (MedNLI, PubMedQA, and CaseHOLD on both 7B and 13B LLaMA2) in Table 2  ###reference_###. It is clear to see that magnitude pruning and SparseGPT are generally stronger models than LLM-Pruner. The dense model sometimes has worse scores than others across 7B and 13B LLaMA2, which indicates that scaling parameters of a pre-trained language model does not necessarily increase the performance on a single benchmark on NLI and QA. We can see that iterative blocking generally yields better scores on these classification tasks such as reaching 30.56 F1 score on CaseHOLD based on LLaMA2-7B, which is a significant improvement over baselines and D-Pruner without it. Thus, we recommend to adopt iterative blocking on the classification tasks when strong domain knowledge is required.\nD-Pruner presents the strongest summarization performance. The most exciting thing is that its ROUGE scores are mostly higher than the dense ones. We notice the top summarization performance of LLaMA2-13B-based models on HQS is lower than that of LLaMA2-7B-based models, which is counterintuitive. According to the state-of-the-art of HQS Zhang et al. (2023  ###reference_b41###); He et al. (2021  ###reference_b15###), we find that D-Pruner is close to the best ROUGE scores produced by single systems, so we consider that this dataset is relatively simple. Thus, our LLaMA2-7B-based models seem to find an upper limit of ROUGE given the existing reference summaries, so going from 7B to 13B incurs a small performance degradation on dense model, SparseGPT, and D-Pruner. The strong summarization performance of D-Pruner on both domains demonstrates its usability as an efficient and domain-specific language model. As for iterative blocking, D-Pruner without it generally has better perplexity and summarization performance. However, considering the exception in the legal domain based on LLaMA2-7B, we recommend to check perplexity scores on the validation data when deciding whether to use iterative blocking for perplexity and summarization assessment.\nD-Pruner has the second best perplexity scores on healthcare and legal domains across 7B and 13B LLaMA2. These scores reflect the strong linguistic capabilities of SparseGPT and D-Pruner when they encounter knowledge-intensive domains. D-Pruner does not surpass SparseGPT on perplexity metric, and the reason might come from the fine-tuning pipeline (Lv et al., 2023  ###reference_b25###) we use. Lv et al. (2023  ###reference_b25###) is a full-parameter fine-tuning pipeline that aims towards GPU memory efficiency, so its effectiveness on a specific metric might be compromised. Moreover, we suspect that the data we use from InternalMed_Harrison and MultiLegalPile may be closer to the general domain both semantically and syntactically. Since SparseGPT prunes LLM mainly based on generality, it has better perplexity scores than ours."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Overall Results",
            "text": "Our overall results for the two domains are presented in Table 2  ###reference_###. All models are pruned to 50% sparsity level except the dense one.\nD-Pruner delivers consistent score improvement on NLI and QA tasks when it is compared against baselines based on LLaMA2-7B and LLaMA2-13B. With two exceptions, variants of D-Pruner based on the inclusion and exclusion of iterative blocking outperform baselines on 4 out of 6 cases when classification is performed (MedNLI, PubMedQA, and CaseHOLD on both 7B and 13B LLaMA2) in Table 2  ###reference_###  ###reference_###. It is clear to see that magnitude pruning and SparseGPT are generally stronger models than LLM-Pruner. The dense model sometimes has worse scores than others across 7B and 13B LLaMA2, which indicates that scaling parameters of a pre-trained language model does not necessarily increase the performance on a single benchmark on NLI and QA. We can see that iterative blocking generally yields better scores on these classification tasks such as reaching 30.56 F1 score on CaseHOLD based on LLaMA2-7B, which is a significant improvement over baselines and D-Pruner without it. Thus, we recommend to adopt iterative blocking on the classification tasks when strong domain knowledge is required.\nD-Pruner presents the strongest summarization performance. The most exciting thing is that its ROUGE scores are mostly higher than the dense ones. We notice the top summarization performance of LLaMA2-13B-based models on HQS is lower than that of LLaMA2-7B-based models, which is counterintuitive. According to the state-of-the-art of HQS Zhang et al. (2023  ###reference_b41###  ###reference_b41###); He et al. (2021  ###reference_b15###  ###reference_b15###), we find that D-Pruner is close to the best ROUGE scores produced by single systems, so we consider that this dataset is relatively simple. Thus, our LLaMA2-7B-based models seem to find an upper limit of ROUGE given the existing reference summaries, so going from 7B to 13B incurs a small performance degradation on dense model, SparseGPT, and D-Pruner. The strong summarization performance of D-Pruner on both domains demonstrates its usability as an efficient and domain-specific language model. As for iterative blocking, D-Pruner without it generally has better perplexity and summarization performance. However, considering the exception in the legal domain based on LLaMA2-7B, we recommend to check perplexity scores on the validation data when deciding whether to use iterative blocking for perplexity and summarization assessment.\nD-Pruner has the second best perplexity scores on healthcare and legal domains across 7B and 13B LLaMA2. These scores reflect the strong linguistic capabilities of SparseGPT and D-Pruner when they encounter knowledge-intensive domains. D-Pruner does not surpass SparseGPT on perplexity metric, and the reason might come from the fine-tuning pipeline (Lv et al., 2023  ###reference_b25###  ###reference_b25###) we use. Lv et al. (2023  ###reference_b25###  ###reference_b25###) is a full-parameter fine-tuning pipeline that aims towards GPU memory efficiency, so its effectiveness on a specific metric might be compromised. Moreover, we suspect that the data we use from InternalMed_Harrison and MultiLegalPile may be closer to the general domain both semantically and syntactically. Since SparseGPT prunes LLM mainly based on generality, it has better perplexity scores than ours."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Performance After Fine-tuning",
            "text": "Table 3  ###reference_### shows the results of fine-tuned candidate models at 50% sparsity. Similar to the performance discussed above, D-Pruner always delivers the best summarization scores and mostly presents the best classification results after fine-tuning, which demonstrates that fine-tuning can further improve the pruning performance of our method. For most models, macro-F1 on PubMedQA decreases after fine-tuning, because this test set is imbalanced and models mostly learn to predict the majority class labels. In fact, the accuracies of most models on PubMedQA increase after fine-tuning as shown in Appendix A  ###reference_###, so this fine-tuning method still makes a difference. We also do not see too much score improvement for many models on CaseHOLD, since it is a quite challenging task for our experiment setting (e.g., we combine only a small subset of original training data for each task and perform multi-task fine-tuning as discussed in Section 4  ###reference_###)."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Ablation Study",
            "text": "In Table 4  ###reference_###, we show that pruning without integrating general domain importance as a regularization term yields suboptimal performance. In other words, this means to remove the consideration of generality. We find perplexities in both domains are higher than pruning with regularization. This demonstrates that our dual pruning mechanism that considers both generality and specificity is able to improve model performance."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Effect of Sparsity and Domain Calibration Data",
            "text": "In Table 5  ###reference_###, it is clear that perplexity keeps increasing when D-Pruner becomes more sparse, which is expected. Since 50% sparsity is a good balance between sparsity and performance, we select it to report our performance in Table 2  ###reference_### and 3  ###reference_###.\nBased on Table 6  ###reference_###, we believe setting the size of domain-specific calibration data to 1000 is reasonable. As the last row shows, increasing its size does not always guarantee a performance improvement.\n###figure_2### ###figure_3###"
        },
        {
            "section_id": "5.5",
            "parent_section_id": "5",
            "section_name": "Mask Similarity",
            "text": "To better understand the pruned model on different domains, we compare the similarity of the pruning masks.\nIn our study on LLaMA2-7B, each generated mask contains 7*32 matrices for 32 layers and 7 projection matrices in the self-attention module (q, k, v, o) and MLP module (down, up, gate) in each layer. For each matrix, we calculate the similarity as the number of shared \u201c1\u201d elements (\u201c1\u201d means weights not pruned) in the two masks divided by the matrix size. Note all the masks are generated in 50% sparsity.\nFigure 2  ###reference_### (a) shows the mask similarity between the open-domain and healthcare domain, and 2  ###reference_### (b) shows the mask similarity between the healthcare domain and legal domain.\nThe results show that the masks are quite different, with shared elements as low as 35%.\nGenerally, the self-attention modules share fewer elements than the MLP modules. This means self-attention modules contribute more to specificity, and MLP modules store knowledge that is shared by different domains."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We introduce D-Pruner, an innovative unstructured dual-pruning method for domain-specific compression on LLM. It is able to extract a compressed, domain-specific, and task-agnostic LLM by identifying weights that are pivotal for both generality and specificity.\nMore specifically, the general weight importance is first assessed by quantifying the error incurred upon their removal with the help of open-domain calibration data.\nThen, we utilize this general weight importance to refine our training loss, so that it considers generality when fitting into a specific domain. Moreover, by efficiently approximating weight importance with the refined training loss on a domain-specific calibration dataset, we obtain a pruned model emphasizing general capabilities and domain-specific knowledge. Our comprehensive experiments across various tasks in different domains show the effectiveness of D-Pruner in domain-specific pruning."
        }
    ],
    "url": "http://arxiv.org/html/2405.06275v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4",
            "5",
            "5.1",
            "5.2",
            "5.3",
            "5.4",
            "5.5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.1",
            "5.3"
        ]
    },
    "research_context": {
        "paper_id": "2405.06275v1",
        "paper_title": "Pruning as a Domain-specific LLM Extractor",
        "research_background": "### Motivation\n\nThe motivation for this paper stems from the observed advancements and challenges associated with deploying Large Language Models (LLMs) such as GPT and LLaMA families. While these LLMs showcase exceptional performance across various Natural Language Processing (NLP) tasks, their large size incurs significant computational costs and makes them inefficient for real-world applications that require fast inference and low latency. Current model pruning techniques can reduce LLM size but typically focus on smaller models or task-specific applications, which may compromise either general linguistic capabilities or domain-specific expertise.\n\n### Research Problem\n\nThe primary research problem identified in this paper is how to develop a pruning method that can effectively reduce the size of LLMs while maintaining both their general capabilities (multi-task solving and language understanding) and their domain-specific expertise (such as knowledge pertinent to specialized fields like healthcare or law). Existing pruning techniques either focus on preserving general weights based on open-domain data or optimize for task-specific weights, neither of which adequately address the need for balancing generality and domain specificity when dealing with domain-specific challenges.\n\n### Relevant Prior Work\n\n1. **Large Language Models (LLMs):** The paper builds on the success of LLMs like GPT (Brown et al., 2020) and LLaMA (Touvron et al., 2023), which have become benchmarks in NLP for their strong performance across a variety of tasks.\n   \n2. **Model Pruning Techniques:** The paper references various successful applications of model pruning to language models, starting with Han et al.'s work in 2015 and extending to more recent efforts by Xia et al. (2022), Frantar and Alistarh (2023), and others. While these methods reduce parameters and deployment costs, they mainly target smaller models rather than large-scale LLMs.\n\n3. **Domain-Specific Challenges in Pruning:**\n   - **General Weights Focus:** Techniques like those proposed by Frantar and Alistarh (2023) focus on pruning weights based on open-domain calibration data, preserving general capabilities but overlooking domain-specific needs.\n   - **Task-Specific Weights Focus:** Approaches like those of Ma et al. (2023) estimate parameter importance during task-specific fine-tuning, which enhances domain specificity but can reduce the model's overall generality and versatility.\n\nBy summarizing the known limitations and combining insights from these previous efforts, the paper underscores the necessity for a novel approach that harmonizes general and domain-specific capabilities in pruned LLMs.",
        "methodology": "### Methodology: Pruning as a Domain-specific LLM Extractor\n\nTo preserve both generality and specificity in the pruned model, our dual-pruning method, D-Pruner, takes into account weights important to both aspects during training on a calibration dataset. Notably, we utilize the weight gradients generated from the training process but do not update the model weights. Our approach ensures the model is pruned in a task-agnostic manner. For this, we incorporate a pre-training objective, specifically next-token prediction, into the training loss. Consequently, the pruned model retains the capability to address various tasks within the target domain.\n\nD-Pruner involves several key steps:\n\n1. **General Weight Locating Module**:\n    - This module assesses the significance of model parameters in achieving a general understanding of the data (detailed in Section 3.1 ###reference_###).\n\n2. **Updated Loss Function**:\n    - An updated loss function is formulated by integrating the importance of general weights as a regularization term. This step helps us identify weights that contribute to both general knowledge and domain-specific knowledge (detailed in Section 3.2 ###reference_###).\n\n3. **Weight Gradients and Dual-Pruning Weight Importance**:\n    - Using the updated loss function, we compute the weight gradients on a small, domain-specific calibration dataset. Without updating the model weights, we approximate dual-pruning weight importance using the empirical Fisher index (as referenced in Sung et al., 2021 ###reference_b35###) for the pruning process (detailed in Section 3.3 ###reference_###).\n\nOur method specifically targets unstructured pruning in a layer-by-layer manner for Transformer models. We focus on the following components for pruning:\n- **Self-Attention Layers**: Includes query, key, value, and output projections.\n- **MLP Layers**: Includes gate (as referenced in Liu et al., 2021 ###reference_b24###), down, and up projections.\n\nBy systematically pruning these components, D-Pruner ensures that the pruned model maintains a balanced performance across both general and domain-specific tasks.",
        "main_experiment_and_results": "### Pruning as a Domain-specific LLM Extractor: Main Experiment Setup and Results\n\n#### Experiment Setup\nWe evaluate D-Pruner on two knowledge-intensive domains, healthcare and legal. To assess model generality under domain-specific challenges, we focus on two main metrics:\n1. **Linguistic Capability**: Evaluated using domain text generation.\n2. **Multi-task Solving Capability**: Evaluated across various domain-specific tasks such as natural language inference (NLI), question answering (QA), and summarization.\n\nWe utilize domain datasets for these evaluations, which allows us to also assess the model's specificity to these domains. Additionally, we fine-tune the pruned model on domain datasets to further understand its generality and specificity.\n\n#### Datasets and Models\nWe conduct our experiments on the LLaMA2 model family, focusing on the LLaMA2-7B and LLaMA2-13B models. These models provide a robust foundation as they are widely used open-source large language models (LLMs). Appendix B references further experiments on the BLOOM model (Le Scao et al., 2022).\n\n#### Baselines\nTo establish comparative benchmarks, we implement baseline methods alongside our pruning technique. These baselines help in highlighting the gains achieved by D-Pruner in terms of domain-specific performance.\n\n#### Evaluation Metrics\nThe evaluation criteria include:\n- **Linguistic Capability**: Assessed through domain text generation tasks.\n- **Task-Specific Performance**: Measured on NLI, QA, and summarization tasks within each domain.\n- **Fine-tuning Effects**: Comparison of pre- and post-fine-tuning performance to evaluate improvements in generality and specificity.\n\n#### Main Experimental Results\nThe results showcase the efficacy of D-Pruner in tailoring LLaMA2 models to specific domains:\n- **Domain Text Generation**: Significant improvements in linguistic capability within healthcare and legal domains.\n- **Task Performance**: Enhanced performance across NLI, QA, and summarization tasks, demonstrating the model's strengthened multi-task solving capabilities.\n- **Fine-Tuning Outcomes**: Fine-tuning the pruned models on domain-specific datasets leads to notable enhancements in both generality and specificity, affirming the adaptability and robustness of D-Pruner.\n\nThese results collectively underscore the effectiveness of D-Pruner in extracting domain-specific knowledge, improving both general and task-focused performance in the evaluated domains."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To investigate the impact of iterative blocking within the D-Pruner methodology on model pruning decisions and overall sparsity level.",
            "experiment_process": "Experiments were conducted with D-Pruner, both with and without iterative blocking. Iterative blocking entails making pruning decisions for every fixed number (c) of columns within a weight matrix. For weight matrices with fewer columns, a smaller c is set, and for matrices with more columns, a larger c is used. The outcomes were compared as shown in Table 2, with other tables not using iterative blocking.",
            "result_discussion": "The detailed outcomes and comparisons are presented in Table 2, implying specific performance impacts of iterative blocking on D-Pruner. However, the results suggest that iterative blocking\u2014adjusting the pruning decisions for different column counts\u2014can influence the final sparsity and possibly the model\u2019s efficiency.",
            "ablation_id": "2405.06275v1.No1"
        },
        {
            "research_objective": "To assess the necessity of integrating general domain importance as a regularization term in the pruning process and its effect on model performance within D-Pruner.",
            "experiment_process": "Experiments were performed to showcase the difference in performance when pruning without the incorporation of a general domain importance regularization term. The focus was to observe differences in perplexities across different domains without this regularization.",
            "result_discussion": "The results, as shown in Table 4, indicate that pruning without the general domain importance regularization term results in suboptimal performance. Specifically, the perplexities in both domains were higher compared to pruning with the regularization term, demonstrating that the dual pruning mechanism, considering both generality and specificity, enhances model performance.",
            "ablation_id": "2405.06275v1.No2"
        }
    ]
}