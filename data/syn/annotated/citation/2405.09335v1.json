{
    "title": "Prompting-based Synthetic Data Generation for Few-Shot Question Answering",
    "abstract": "Although language models (LMs) have boosted the performance of Question Answering, they still need plenty of data.\nData annotation, in contrast, is a time-consuming process.\nThis especially applies to Question Answering, where possibly large documents have to be parsed and annotated with questions and their corresponding answers.\nFurthermore, Question Answering models often only work well for the domain they were trained on.\nSince annotation is costly, we argue that domain-agnostic knowledge from LMs, such as linguistic understanding, is sufficient to create a well-curated dataset.\nWith this motivation, we show that using large language models can improve Question Answering performance on various datasets in the few-shot setting compared to state-of-the-art approaches.\nFor this, we perform data generation leveraging the Prompting framework, suggesting that language models contain valuable task-agnostic knowledge that can be used beyond the common pre-training/fine-tuning scheme.\nAs a result, we consistently outperform previous approaches on few-shot Question Answering.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "Machine Reading Question Answering (MRQA) is an important task in Natural Language Processing and allows to easily access information by providing answers to specific questions.\nWhile there are several subtasks related to MRQA such as open-domain, binary/multiple choice, conversational or generative QA, we focus on extractive QA in this work.\nIn extractive QA, the goal is to find the answer to a question by extracting it from a given context.\nMRQA has also raised attention in the community as a surrogate where other tasks are cast as question answering problems, thereby enabling a broad range of applications.\nThis includes, for example, Named Entity Recognition (NER, Li et al., 2020  ###reference_b20###; Arora and Park, 2023  ###reference_b3###), entity relation extraction (Levy et al., 2017  ###reference_b17###; Li et al., 2019  ###reference_b21###; Zhang et al., 2022  ###reference_b49###) and slot filling (Gao et al., 2019  ###reference_b10###).\nPre-training language models (LMs) on Natural Language Understanding (NLU) objectives such as Masked Language Modeling (MLM, Devlin et al., 2019  ###reference_b8###) led to strong MRQA models (Rajpurkar et al., 2016  ###reference_b33###) and even surpasses human level111e.g., on the SQuAD (Rajpurkar et al., 2016  ###reference_b33###) benchmark: https://rajpurkar.github.io/SQuAD-explorer/  ###reference_/###.\nSince the downstream task uses a different objective function, fine-tuning pre-trained LMs (PLMs) is necessary to adapt to the task.\nArguably, this misalignment leads to poor results if labeled data for the downstream task is scarce.\nHowever, annotating data for MRQA is time-consuming and expensive.\nAdditionally, few-shot MRQA poses an interesting challenge, especially for specific domains, where high effort is needed to annotate data or domain experts are missing.\nAlso, there is still a gap in the performance of few-shot models when compared to the high-resource setting.\nFor example, the best model on TextbookQA (Kembhavi et al., 2017  ###reference_b15###) using 16 labeled samples is currently reported in literature to reach at most 49.9% F1 (Castel et al., 2022  ###reference_b5###).\n###figure_1### To deal with the low-resource MRQA setting, previous work has proposed to generate synthetic data to augment the training set (e.g., Alberti et al., 2019  ###reference_b1###; Puri et al., 2020  ###reference_b30###; Shakeri et al., 2020  ###reference_b39###, 2021  ###reference_b38###).\nWith a similar objective, PLMs have been employed for other tasks in Natural Language Processing (e.g., Anaby-Tavor et al., 2020  ###reference_b2###; Schick and Sch\u00fctze, 2021  ###reference_b35###).\nHowever, MRQA is more challenging when generating synthetic data:\nWe cannot simply generate text given a label, but have to come up with a sample\u2019s input as well as its label in the form of a question and its answer.\nAdditionally, both the answer and the question are mutually dependent.\nIn this work, we explore an approach that enables this; a high-level overview is given in figure 1  ###reference_###.\nMore precisely, we aim to answer the following research questions:\nHow can we use LMs to generate synthetic data for improving the few-shot MRQA task?\nTo what extend can synthetic data improve performance?\nHow does the answer selection affect the performance?\nDo we need labeled data at all or can LMs generate helpful data out of the box?\nHow does the proposed approach generalize to other domains?\nTo achieve this, we believe that there is a more effective way to employ LMs:\nWe propose to use the linguistic knowledge encoded in these models to generate synthetic data for the target domain to counteract the effect of data scarcity.\nFor this, we use the LM\u2019s ability to generate questions conditioned on the input, and we argue that this can easily be carried out in any target domain since we build on unsupervised PLMs.\nIn summary, our contributions are as follows:\n1) We propose an approach generating valuable labeled data for the target domain by using the linguistic knowledge encoded in LMs222Our source code is available publicly here: https://github.com/mxschmdt/mrqa-prompting-gen  ###reference_-gen###;\n2) we improve the performance of few-shot QA for many dataset sizes across various domains to further bridge the performance gap between the few-shot and the full data setting; 3) we demonstrate the high quality of questions generated by our approach in a user study.\nWhile introducing a new, strong approach outperforming many state-of-the-art approaches in few-shot MRQA, our model even outperforms the full data setting of TextbookQA with 64% F1 with only 64 labeled samples."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Related Work",
            "text": "In this section, we review existing work related to our setting, i.e., few-shot, as well as applications for Prompting. In recent years, AI-driven personalized learning systems have emerged as a transformative tool in education, adapting to individual learning styles and needs. These systems utilize machine learning algorithms to tailor content and assessments, improving engagement and efficacy. Advances in natural language processing and data analytics have facilitated the development of adaptive learning paths, offering real-time feedback and adjustments to optimize educational outcomes."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   Method",
            "text": "Here, we give an overview of the problem and describe our approach in detail.\nFormally, MRQA is defined as given context  and question , the goal is to predict the answer .\nWe further focus on extractive MRQA, that is,  is a single contiguous span within .\nNext, we introduce our approach leveraging the Prompting framework.\nThe high-level idea of our approach is composed of two steps:\nFirst, we sample answer candidates from a document.\nIn a second step, we then query a pre-trained LM for generating questions using the document and the previously sampled answers.\nAn overview of our data generation pipeline is given in figure 2  ###reference_###.\nIn the following we describe each step of o ur method in detail."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   Answer Sampling",
            "text": "For sampling answer candidates, we apply NER333In fact, we also select values similar in style to names, e.g., periods. to the context  and the resulting entities are used as textual answers  with spans  (a tuple of character start and end indices).\nWe chose this technique because it is a simple resource-sparse approach and does not need to have knowledge on the domain\u2019s topic (i.e., any English NER model is sufficient for our domains).\nFurthermore, NER is feasible in many languages and the datasets on which we evaluate our method, the few-shot MRQA benchmark, work with such style.\nAs a result, it can be applied to any domain for which a NER model exists in the given language.\nNote that NER does not necessarily rely on many labeled samples or labeled data at all (e.g., rule-based approaches, using weak supervision (Lison et al., 2020  ###reference_b22###) or Prompting (Liu et al., 2022  ###reference_b23###; Ma et al., 2022  ###reference_b26###))."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   Question Generation",
            "text": "In Prompting, a LM takes a text input and, depending on the training objective, predicts the next token (as in the case of language modeling) or one or multiple masked tokens.\nFor example, T5 (Raffel et al., 2020  ###reference_b32###) is an encoder-decoder model which is fed a text input possibly containing multiple masked tokens.\nFor each masked token, one or more tokens can appear in the output prefixed by a sentinel token, marking the masked token in the input to which the following tokens belong.\nFor our purpose we only use a single mask in the input.\nFor generating questions, we transform the sample inputs into prompts for the LM.\nFor this purpose we apply a template, thus replacing placeholders (marked starting with < and ending with >) with the actual values from the sample.\nSince we aim at generating a question given a context and a sampled answer candidate, the template is formulated to include the context and the answer, and the expected output is the question.\nTherefore the question is formally defined as\nAt training time, we use the original objective used for pre-training the underlying language model in order to model the question\u2019s probability by only computing the loss on the question  in the output.\nIn our preliminary experiments, we have found that it is crucial to rely on sequence-to-sequence models, as these allow to condition the output not only on previous tokens but on the whole sequence.\nWe believe this is due to the more natural formulation where a question occurs before its answer in a sentence.\nIn contrast, using a left-to-right decoding model only, the input would have to be formulated such that the generated question can be answered by the previously (i.e., left to the question) given answer.\nObviously, this not only yields a longer prompt but also increases its complexity.\nFurthermore, we make use of soft tokens in the input.\nThat is, all textual tokens from the template are trained in addition to the remaining model weights and we initialize them with the corresponding weights from the pre-trained word embedding.\nFor generation, we decode the question  token-by-token and filtering is applied.\nWe do this for the following reasons:\n1) the generated question could be noisy, e.g., not a valid question, and\n2) the generated question may not be helpful for the Question Answering downstream task, possibly being underspecified.\nFor example, the generated question may have \u2013 in addition to the provided answer \u2013 several other correct answers in a given context.\nAs filtering technique we apply a two-step process.\nFirst, we discard generated samples based on rule-based filtering.\nWe then apply consistency filtering (Alberti et al., 2019  ###reference_b1###; Anaby-Tavor et al., 2020  ###reference_b2###) for which a model similar to the final MRQA model is employed.\nGenerated samples are discarded depending on the F1 score of the predicted answer (where the reference is the generated answer) using the MRQA model.\nWe do not use iterative consistency filtering (Wang et al., 2022b  ###reference_b45###) as this involves MRQA model re-training in each iteration, resulting in increased hyperparameter tuning complexity and resource drain."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Experimental Setup",
            "text": "Here, we first introduce the few-shot SQuAD dataset followed by implementational details of our approach and the description of baselines we consider.\nThis is the model proposed by Ram et al. (2021  ###reference_b34###) using an RSS pre-training phase which is fine-tuned on the few-shot MRQA datasets.\nWe show the results as reported by the authors for the base model.\nFewshotBARTL is the best performing model reported in FewshotQA (Chada and Natarajan, 2021  ###reference_b6###).\nThis is a Prompting-based MRQA model using BART (Lewis et al., 2019  ###reference_b18###).\nCastel et al. (2022  ###reference_b5###) reported results of a Prompting-based MRQA model similar to FewshotBARTL but using T5 v1.1.\nThis model is similar to the Prompting model above, but with additional pre-training using RSS.\nSince Castel et al. (2022  ###reference_b5###) only report results on SQuAD, we consider this model only for RQ1.\nFor RQ2, we evaluate a re-implemented version of this model (see next model).\nTo account for implementational differences and to be able to evaluate the Prompting model with RSS on the full few-shot MRQA benchmark, we also consider a reimplementation of the Prompting+RSS model where we also directly perform MRQA via Prompting.\nAdditionally, we fine-tune soft tokens in the input initialized with weights from the embedding using the template.\nTo this end, we transform the sample into a prompt such that the pre-trained model answers the question using the given context.\nThis model is equal to the MRQA model we use in our approach, i.e. using the same template and the same hyperparameters.\nThis model proposed by Chen et al. (2023  ###reference_b7###) is similar to Prompting with additional pre-training on entity-aware masks.\nPMR (Xu et al., 2022  ###reference_b46###) employs pre-training on automatically generated data in MRQA style and has a dedicated MRQA fine-tuning stage where the structure of inputs and outputs are similar.\nWe also show results reported by Ram et al. (2021  ###reference_b34###) for a model following the standard pre-training/fine-tuning paradigm using Roberta (base) (Liu et al., 2019  ###reference_b25###) with a span extraction head."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1.   Few-shot Setting",
            "text": "We perform experiments on several datasets in order to compare with existing approaches.\nFor this, we rely on the subsampled train and test splits from the Few-Shot MRQA benchmark444https://github.com/oriram/splinter#downloading-few-shot-mrqa-splits  ###reference_ding-few-shot-mrqa-splits### from Ram et al. (2021  ###reference_b34###) which are based on the preprocessed versions from the MRQA Shared Task 2019555https://github.com/mrqa/MRQA-Shared-Task-2019  ###reference_019### (Fisch et al., 2019  ###reference_b9###).\nMore specific, this includes as domains SQuAD (Rajpurkar et al., 2016  ###reference_b33###), TriviaQA (Joshi et al., 2017  ###reference_b14###), NaturalQuestionsShort (NQ) (Kwiatkowski et al., 2019  ###reference_b16###), NewsQA (Trischler et al., 2017  ###reference_b41###), HotpotQA (Yang et al., 2018  ###reference_b47###), BioASQ (Tsatsaronis et al., 2015  ###reference_b42###) and TextbookQA (Kembhavi et al., 2017  ###reference_b15###) and we evaluate our approach using the splits with 16, 32, 64 and 128 training samples.\nWe perform our experiments for RQ1 on SQuAD while the remaining datasets are used to test the generalization capability of our approach (RQ2)."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2.   Data Generation",
            "text": "For generating questions, we have tested various models, templates and pre-processing strategies in preliminary experiments.\nWe found T5 (Raffel et al., 2020  ###reference_b32###) to perform best.\nInstead of the original v1 model, which makes use of labeled data during pre-training thus violating our few-shot setting, we employ the v1.1 model in its large variant (800M parameters).\nDecoder-only models like GPT-2 (Radford et al., 2019  ###reference_b31###) performed worse as mentioned in section 3.2  ###reference_###.\nIn the templates, we considered case sensitivity as well as different wordings.\nAs a result of manual investigation, context: <context> question: <mask> answer: <answer>.666<mask>is replaced by the model-specific mask token. turned out to work well for our purpose and is similar to the findings of Castel et al. (2022  ###reference_b5###)."
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1.   Training of Question Generation Model",
            "text": "For training the data generation model, similar to Castel et al. (2022  ###reference_b5###) we create an academic development dataset to cater to the few-shot setting where having a separate development split leads to bad generalizability due to its small size.\nTherefore, we tune the learning rate and the number of training steps on a validation set of 2048 samples from SQuAD\u2019s training data, and choose the set of hyperparameters that has the best normalized performance across all few-shot sizes as described in Castel et al. (2022  ###reference_b5###).\nAs a result, the question generation model was trained for 130 training steps with a batch size of 32 using a linear learning rate of 1e-4 with the Adafactor optimizer.\nFurthermore, the soft tokens add 8192 weights for the question generation model.\nWe additionally chunk the provided contexts using a stride of 100 tokens so that at most 450 tokens of the context are included in a single input to allow for sufficient space for the question.\nSince chunking can create instances where the answer is not part of the context, we drop these as we cannot expect them to yield a semantically correct question."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2.   Synthetic Data Generation",
            "text": "For generating synthetic data, in case of SQuAD, TriviaQA, NQ, NewsQA, SearchQA and HotpotQA, we use the documents from the training corpus.\nSince BioASQ and TextbookQA both comprise rather few documents, we collect abstracts from PubMed777https://pubmed.ncbi.nlm.nih.gov/  ###reference_pubmed.ncbi.nlm.nih.gov/### and lessons from CK-12888https://www.ck12.org  ###reference_www.ck12.org###, respectively, for the purpose of generating data.\nWe then apply stanza\u2019s NER999https://stanfordnlp.github.io/stanza/ner.html  ###reference_tml### using all its entity types101010https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0.pdf  ###reference_T19/OntoNotes-Release-5.0.pdf### p.21f in order to sample answers from these documents.\nAfterwards, similar to training time, we apply chunking with a stride of 100 tokens to feed the documents into the model (by realizing the template) whereby we again only keep instances where the answer is contained in the context for the same reason as above.\nIn a subsequent step, the generated question is greedily decoded with a beam size of 5, top-k sampling with k equal to 20 and nucleus sampling (Holtzman et al., 2020  ###reference_b13###) keeping tokens comprising 95% of probability mass in each step.\nThe language models\u2019 special tokens are stripped in a subsequent step.\nSince we only allow one mask in the input (for the question), we make sure that only the output tokens corresponding to this mask are used.\nFor the rule-based filtering in a subsequent step, we randomly select 1,000,000 samples and discard generated samples where the answer is contained in the question, or the question is only containing meaningless words or is empty.\nAfterwards, we apply consistency filtering discarding generated samples with an F1 score of less than 80% using a Prompting-based MRQA model which is trained similar to the one described in the next subsection."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "4.3.   MRQA Model",
            "text": "For the final step of our approach, we train an MRQA model using synthetic and available labeled data as shown in figure 2  ###reference_###.\nSince a Prompting-based approach turned out to perform better than a span extraction head on top of a Transformer-based (Vaswani et al., 2017  ###reference_b43###) encoder model, we also use T5 v1.1 (large) with the Prompting framework as our MRQA model.\nAdditionally, we compared T5 v1.1 pre-trained with and without recurring span selection (RSS)111111https://huggingface.co/tau/t5-v1_1-large-rss  ###reference_ss### (Castel et al., 2022  ###reference_b5###) on the few-shot MRQA benchmark (see below in section \u00a74.4  ###reference_###) and found that MRQA performance is generally improved if RSS is used.\nTherefore we use this model as basis for our MRQA models.\nAs template, we use context: <context> question: <question> answer: <mask>., and again use soft tokens (accounting for 9216 weights) which are optimized in addition to the full model during training.\nThe MRQA model is first trained on synthetic data for 1 epoch or at least 500 steps.\nIn a subsequent step, we further train on the annotated data from the few-shot splits.\nFor this, we use the hyperparameters reported in Castel et al. (2022  ###reference_b5###), that is a constant learning rate of 5e-5 for 512 training steps using the Adafactor optimizer (Shazeer and Stern, 2018  ###reference_b40###) with a batch size of 32 and a dropout of 0.1.\nWe report the mean and standard deviation over 5 MRQA model runs while training the data generation model only once to save computation resources.\nModel\n0\n16\n32\n64\n128\n\n\n\nOur approach\n85.5\n86.40.6\n88.30.4\n87.70.4\n89.30.6\n\nPrompting+RSS Re-Impl\n71.5\n84.0\n86.8\n86.8\n88.8\n\nPrompting+RSS (Castel et al., 2022  ###reference_b5###)\n71.4\n85.6\n86.7\n87.9\n89.4\n\nPrompting (Castel et al., 2022  ###reference_b5###)\n60.0\n82.6\n85.2\n86.7\n89.0\n\nGotta (Chen et al., 2023  ###reference_b7###)\n-\n74.61.9\n76.02.0\n78.910.5\n80.81.7\n\nPMR (large) (Xu et al., 2022  ###reference_b46###)\n17.2\n60.34.0\n70.03.2\n76.61.9\n81.71.2\n\nFewshotBARTL (Chada and Natarajan, 2021  ###reference_b6###)\n-\n68.92.7\n72.31.0\n73.61.9\n79.41.5\n\nSplinter (base) (Ram et al., 2021  ###reference_b34###)\n-\n54.66.4\n59.22.1\n65.21.4\n72.71.0\n\nRoberta (base) (Ram et al., 2021  ###reference_b34###)\n-\n7.74.3\n18.25.1\n28.41.7\n43.07.1"
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "4.4.   Comparison Models",
            "text": "We compare our approach to several recent and well performing MRQA models which we describe in the following.\nThis is the model proposed by Ram et al. (2021  ###reference_b34###  ###reference_b34###) using an RSS pre-training phase which is fine-tuned on the few-shot MRQA datasets.\nWe show the results as reported by the authors for the base model.\nFewshotBARTL is the best performing model reported in FewshotQA (Chada and Natarajan, 2021  ###reference_b6###  ###reference_b6###).\nThis is a Prompting-based MRQA model using BART (Lewis et al., 2019  ###reference_b18###  ###reference_b18###).\nCastel et al. (2022  ###reference_b5###  ###reference_b5###) reported results of a Prompting-based MRQA model similar to FewshotBARTL but using T5 v1.1.\nThis model is similar to the Prompting model above, but with additional pre-training using RSS.\nSince Castel et al. (2022  ###reference_b5###  ###reference_b5###) only report results on SQuAD, we consider this model only for RQ1.\nFor RQ2, we evaluate a re-implemented version of this model (see next model).\nTo account for implementational differences and to be able to evaluate the Prompting model with RSS on the full few-shot MRQA benchmark, we also consider a reimplementation of the Prompting+RSS model where we also directly perform MRQA via Prompting.\nAdditionally, we fine-tune soft tokens in the input initialized with weights from the embedding using the template.\nTo this end, we transform the sample into a prompt such that the pre-trained model answers the question using the given context.\nThis model is equal to the MRQA model we use in our approach, i.e. using the same template and the same hyperparameters.\nThis model proposed by Chen et al. (2023  ###reference_b7###  ###reference_b7###) is similar to Prompting with additional pre-training on entity-aware masks.\nPMR (Xu et al., 2022  ###reference_b46###  ###reference_b46###) employs pre-training on automatically generated data in MRQA style and has a dedicated MRQA fine-tuning stage where the structure of inputs and outputs are similar.\nWe also show results reported by Ram et al. (2021  ###reference_b34###  ###reference_b34###) for a model following the standard pre-training/fine-tuning paradigm using Roberta (base) (Liu et al., 2019  ###reference_b25###  ###reference_b25###) with a span extraction head."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Results and Discussion",
            "text": "In order to judge the performance of our approach, we report the F1 score on the tested datasets for our approach as well as for the models we compare with.\nWe now examine our research questions using the reported results.\nModel\n0\n16\n32\n64\n128\n\n\n\nsampled answer\n85.5\n86.4\n88.3\n87.7\n89.3\n\ngold answers\n87.3\n87.6\n89.5\n88.3\n90.3\n\n  \u2003synthetic data only\n87.3\n90.0\n91.0\n90.7\n91.3\nIn general, as reported in table 1  ###reference_###, our proposed method outperforms many existing approaches on SQuAD on all sizes although it does not perform best with 64 and 128 samples but is very close.\nAlso, there is a trend that more data improves our data generation approach although there is a fluctuation which we trace back to difficulties in training LMs for the MRQA task on little data in the final step as observed.\nFor this research question, we compare the performance of generated data by our approach when using answers sampled using NER and when using the gold answers for SQuAD in table 2  ###reference_###.\nWe can observe that generated data with sampled answers by NER performs in terms of F1 score on the MRQA model on average only 1.3% worse than data generated using the gold answers.\nTherefore the chosen answer sampling strategy can be a good replacement for answers as in the case of SQuAD.\nSince we observed suboptimal training performance of the MRQA model on labeled data in the final step, we additionally report MRQA performance on synthetic data only (i.e., before fine-tuning on labeled data in a final step).\nThis shows that generated data can even perform better if care is taken when integrating the labeled samples into the eventual MRQA model.\nModel\n\nTriviaQA\nNQ\nNewsQA\nSearchQA\nHotpotQA\nBioASQ\nTextbookQA\nMean\n\n16 Samples\n\n\n\n\n\n\n\n\n\n\nOur approach\n\n76.40.5\n\n68.50.6\n\n51.40.8\n\n71.11.3\n\n72.40.6\n\n72.31.5\n\n60.12.4\n\n67.5\n\nPrompting+RSS Re-Impl\n\n76.1\n\n67.0\n\n48.3\n\n71.9\n\n71.3\n\n73.0\n\n60.4\n\n66.9\n\nPrompting (Castel et al., 2022  ###reference_b5###)\n\n74.8\n\n64.4\n\n44.7\n\n64.1\n\n66.3\n\n74.7\n\n49.9\n\n62.7\n\nGotta (Chen et al., 2023  ###reference_b7###)\n\n63.38.0\n\n58.91.9\n\n47.32.5\n\n56.83.9\n\n59.82.1\n\n66.13.1\n\n38.55.3\n\n55.8\n\nPMR (large) (Xu et al., 2022  ###reference_b46###)\n\n56.23.1\n\n43.61.7\n\n30.13.7\n\n58.25.0\n\n46.14.7\n\n54.23.4\n\n31.01.8\n\n45.6\n\nFewshotBARTL (Chada and Natarajan, 2021  ###reference_b6###)\n\n65.21.8\n\n60.42.0\n\n48.42.2\n\n47.85.4\n\n58.01.8\n\n63.01.1\n\n37.73.7\n\n54.4\n\nSplinter (base) (Ram et al., 2021  ###reference_b34###)\n\n18.94.1\n\n27.44.6\n\n20.82.7\n\n26.33.9\n\n24.05.0\n\n28.24.9\n\n19.44.6\n\n23.6\n\nRoberta (base) (Ram et al., 2021  ###reference_b34###)\n\n7.54.4\n\n17.33.3\n\n1.40.8\n\n6.92.7\n\n10.52.5\n\n16.77.1\n\n3.32.1\n\n9.1\n\n32 Samples\n\n\n\n\n\n\n\n\n\n\nOur approach\n\n76.80.5\n\n68.50.8\n\n50.60.8\n\n72.90.8\n\n73.40.6\n\n74.50.8\n\n61.01.8\n\n68.2\n\nPrompting+RSS Re-Impl\n\n75.6\n\n64.0\n\n49.0\n\n71.1\n\n71.7\n\n73.1\n\n61.0\n\n66.5\n\nPrompting (Castel et al., 2022  ###reference_b5###)\n\n74.8\n\n66.7\n\n48.8\n\n66.2\n\n70.3\n\n76.8\n\n51.2\n\n65.0\n\nGotta (Chen et al., 2023  ###reference_b7###)\n\n61.94.8\n\n59.82.4\n\n51.21.5\n\n63.13.1\n\n62.71.2\n\n69.51.0\n\n46.33.7\n\n59.2\n\nPMR (large) (Xu et al., 2022  ###reference_b46###)\n\n66.32.5\n\n48.53.5\n\n36.62.1\n\n64.82.2\n\n52.92.5\n\n62.92.4\n\n36.43.2\n\n52.6\n\nFewshotBARTL (Chada and Natarajan, 2021  ###reference_b6###)\n\n65.11.2\n\n61.51.7\n\n51.71.7\n\n58.31.5\n\n60.40.2\n\n67.81.0\n\n37.79.8\n\n57.5\n\nSplinter (base) (Ram et al., 2021  ###reference_b34###)\n\n28.93.1\n\n33.62.4\n\n27.53.2\n\n34.81.8\n\n34.73.9\n\n36.53.2\n\n27.64.3\n\n31.9\n\nRoberta (base) (Ram et al., 2021  ###reference_b34###)\n\n10.51.8\n\n22.90.7\n\n3.21.7\n\n13.51.8\n\n10.41.9\n\n23.36.6\n\n4.30.9\n\n12.6\n\n64 Samples\n\n\n\n\n\n\n\n\n\n\nOur approach\n\n76.20.5\n\n70.40.4\n\n56.40.7\n\n75.01.4\n\n74.70.2\n\n76.80.5\n\n64.01.0\n\n70.5\n\nPrompting+RSS Re-Impl\n\n76.3\n\n68.7\n\n52.9\n\n72.5\n\n73.4\n\n78.5\n\n61.8\n\n69.2\n\nPrompting (Castel et al., 2022  ###reference_b5###)\n\n75.3\n\n68.5\n\n49.9\n\n71.7\n\n73.1\n\n80.4\n\n55.6\n\n67.8\n\nGotta (Chen et al., 2023  ###reference_b7###)\n\n59.611.9\n\n63.611.0\n\n54.313.0\n\n66.312.5\n\n64.311.7\n\n73.211.5\n\n51.212.8\n\n61.8\n\nPMR (large) (Xu et al., 2022  ###reference_b46###)\n\n67.51.7\n\n53.42.3\n\n46.82.6\n\n69.32.4\n\n61.72.1\n\n71.51.8\n\n43.43.6\n\n59.1\n\nFewshotBARTL (Chada and Natarajan, 2021  ###reference_b6###)\n\n64.61.4\n\n63.02.1\n\n53.50.9\n\n65.52.4\n\n62.91.6\n\n73.90.8\n\n45.01.7\n\n61.2\n\nSplinter (base) (Ram et al., 2021  ###reference_b34###)\n\n35.53.7\n\n38.22.3\n\n37.41.2\n\n39.83.6\n\n45.42.3\n\n49.53.6\n\n35.93.1\n\n40.2\n\nRoberta (base) (Ram et al., 2021  ###reference_b34###)\n\n12.51.4\n\n24.21.0\n\n4.62.8\n\n19.82.4\n\n15.03.9\n\n34.01.8\n\n5.41.1\n\n16.5\n\n128 Samples\n\n\n\n\n\n\n\n\n\n\nOur approach\n\n77.40.3\n\n73.00.4\n\n57.40.7\n\n78.60.5\n\n76.50.2\n\n82.40.3\n\n63.51.6\n\n72.7\n\nPrompting+RSS Re-Impl\n\n77.1\n\n71.3\n\n55.1\n\n75.9\n\n75.9\n\n81.8\n\n62.4\n\n71.4\n\nPrompting (Castel et al., 2022  ###reference_b5###)\n\n76.7\n\n69.9\n\n51.8\n\n73.4\n\n74.6\n\n85.2\n\n58.0\n\n69.9\n\nGotta (Chen et al., 2023  ###reference_b7###)\n\n60.03.6\n\n64.91.2\n\n57.41.2\n\n69.81.5\n\n66.71.8\n\n78.62.1\n\n53.31.7\n\n64.4\n\nPMR (large) (Xu et al., 2022  ###reference_b46###)\n\n70.30.5\n\n57.42.6\n\n52.31.4\n\n70.01.1\n\n65.91.0\n\n78.80.5\n\n45.11.2\n\n62.8\n\nFewshotBARTL (Chada and Natarajan, 2021  ###reference_b6###)\n\n65.80.9\n\n64.31.3\n\n57.00.9\n\n67.71.0\n\n75.11.5\n\n75.01.5\n\n48.42.7\n\n64.8\n\nSplinter (base) (Ram et al., 2021  ###reference_b34###)\n\n44.73.9\n\n46.30.8\n\n43.51.3\n\n47.23.5\n\n54.71.4\n\n63.24.1\n\n42.62.5\n\n48.9\n\nRoberta (base) (Ram et al., 2021  ###reference_b34###)\n\n19.12.9\n\n30.11.9\n\n16.73.8\n\n27.82.5\n\n27.33.9\n\n46.11.4\n\n8.21.1\n\n25.0\n\nFull Dataset\n\n\n\n\n\n\n\n\n\n\nSplinter (base) (Ram et al., 2021  ###reference_b34###)\n\n76.5\n\n81.0\n\n71.3\n\n83.0\n\n80.7\n\n91.011footnotemark: 1\n\n54.511footnotemark: 1\n\n76.9\n\nRoberta (base) (Ram et al., 2021  ###reference_b34###)\n\n74.0\n\n79.6\n\n69.8\n\n81.5\n\n78.7\n\n84.111footnotemark: 1\n\n35.811footnotemark: 1\n\n71.9\nAs reported in table 1  ###reference_###, without any labeled data our approach gets an F1 score of 85.5%.\nAlthough the performance increases if labeled data is added, this clearly outlines that there are strong zero-shot capabilities by employing data generation for MRQA.\nTherefore, we can conclude that the LM has learned during pre-training the relationship between questions and answers to an extent that is useful for SQuAD.\nIn summary, as an answer to RQ1, our Prompting-based data generation approach efficiently employs LMs for MRQA increasing performance compared to existing work.\nThe proposed method proves to be competitive on SQuAD and performs well especially without training data, establishing a new state of the art."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "5.1.   RQ1: Synthetic Data Generation using LMs",
            "text": "First, we answer the nether research questions in order to answer RQ1.\nFor this, we only evaluate on SQuAD.\nIn general, as reported in table 1  ###reference_###  ###reference_###, our proposed method outperforms many existing approaches on SQuAD on all sizes although it does not perform best with 64 and 128 samples but is very close.\nAlso, there is a trend that more data improves our data generation approach although there is a fluctuation which we trace back to difficulties in training LMs for the MRQA task on little data in the final step as observed.\nFor this research question, we compare the performance of generated data by our approach when using answers sampled using NER and when using the gold answers for SQuAD in table 2  ###reference_###  ###reference_###.\nWe can observe that generated data with sampled answers by NER performs in terms of F1 score on the MRQA model on average only 1.3% worse than data generated using the gold answers.\nTherefore the chosen answer sampling strategy can be a good replacement for answers as in the case of SQuAD.\nSince we observed suboptimal training performance of the MRQA model on labeled data in the final step, we additionally report MRQA performance on synthetic data only (i.e., before fine-tuning on labeled data in a final step).\nThis shows that generated data can even perform better if care is taken when integrating the labeled samples into the eventual MRQA model.\nModel\n\nTriviaQA\nNQ\nNewsQA\nSearchQA\nHotpotQA\nBioASQ\nTextbookQA\nMean\n\n16 Samples\n\n\n\n\n\n\n\n\n\n\nOur approach\n\n76.40.5\n\n68.50.6\n\n51.40.8\n\n71.11.3\n\n72.40.6\n\n72.31.5\n\n60.12.4\n\n67.5\n\nPrompting+RSS Re-Impl\n\n76.1\n\n67.0\n\n48.3\n\n71.9\n\n71.3\n\n73.0\n\n60.4\n\n66.9\n\nPrompting (Castel et al., 2022  ###reference_b5###  ###reference_b5###)\n\n74.8\n\n64.4\n\n44.7\n\n64.1\n\n66.3\n\n74.7\n\n49.9\n\n62.7\n\nGotta (Chen et al., 2023  ###reference_b7###  ###reference_b7###)\n\n63.38.0\n\n58.91.9\n\n47.32.5\n\n56.83.9\n\n59.82.1\n\n66.13.1\n\n38.55.3\n\n55.8\n\nPMR (large) (Xu et al., 2022  ###reference_b46###  ###reference_b46###)\n\n56.23.1\n\n43.61.7\n\n30.13.7\n\n58.25.0\n\n46.14.7\n\n54.23.4\n\n31.01.8\n\n45.6\n\nFewshotBARTL (Chada and Natarajan, 2021  ###reference_b6###  ###reference_b6###)\n\n65.21.8\n\n60.42.0\n\n48.42.2\n\n47.85.4\n\n58.01.8\n\n63.01.1\n\n37.73.7\n\n54.4\n\nSplinter (base) (Ram et al., 2021  ###reference_b34###  ###reference_b34###)\n\n18.94.1\n\n27.44.6\n\n20.82.7\n\n26.33.9\n\n24.05.0\n\n28.24.9\n\n19.44.6\n\n23.6\n\nRoberta (base) (Ram et al., 2021  ###reference_b34###  ###reference_b34###)\n\n7.54.4\n\n17.33.3\n\n1.40.8\n\n6.92.7\n\n10.52.5\n\n16.77.1\n\n3.32.1\n\n9.1\n\n32 Samples\n\n\n\n\n\n\n\n\n\n\nOur approach\n\n76.80.5\n\n68.50.8\n\n50.60.8\n\n72.90.8\n\n73.40.6\n\n74.50.8\n\n61.01.8\n\n68.2\n\nPrompting+RSS Re-Impl\n\n75.6\n\n64.0\n\n49.0\n\n71.1\n\n71.7\n\n73.1\n\n61.0\n\n66.5\n\nPrompting (Castel et al., 2022  ###reference_b5###  ###reference_b5###)\n\n74.8\n\n66.7\n\n48.8\n\n66.2\n\n70.3\n\n76.8\n\n51.2\n\n65.0\n\nGotta (Chen et al., 2023  ###reference_b7###  ###reference_b7###)\n\n61.94.8\n\n59.82.4\n\n51.21.5\n\n63.13.1\n\n62.71.2\n\n69.51.0\n\n46.33.7\n\n59.2\n\nPMR (large) (Xu et al., 2022  ###reference_b46###  ###reference_b46###)\n\n66.32.5\n\n48.53.5\n\n36.62.1\n\n64.82.2\n\n52.92.5\n\n62.92.4\n\n36.43.2\n\n52.6\n\nFewshotBARTL (Chada and Natarajan, 2021  ###reference_b6###  ###reference_b6###)\n\n65.11.2\n\n61.51.7\n\n51.71.7\n\n58.31.5\n\n60.40.2\n\n67.81.0\n\n37.79.8\n\n57.5\n\nSplinter (base) (Ram et al., 2021  ###reference_b34###  ###reference_b34###)\n\n28.93.1\n\n33.62.4\n\n27.53.2\n\n34.81.8\n\n34.73.9\n\n36.53.2\n\n27.64.3\n\n31.9\n\nRoberta (base) (Ram et al., 2021  ###reference_b34###  ###reference_b34###)\n\n10.51.8\n\n22.90.7\n\n3.21.7\n\n13.51.8\n\n10.41.9\n\n23.36.6\n\n4.30.9\n\n12.6\n\n64 Samples\n\n\n\n\n\n\n\n\n\n\nOur approach\n\n76.20.5\n\n70.40.4\n\n56.40.7\n\n75.01.4\n\n74.70.2\n\n76.80.5\n\n64.01.0\n\n70.5\n\nPrompting+RSS Re-Impl\n\n76.3\n\n68.7\n\n52.9\n\n72.5\n\n73.4\n\n78.5\n\n61.8\n\n69.2\n\nPrompting (Castel et al., 2022  ###reference_b5###  ###reference_b5###)\n\n75.3\n\n68.5\n\n49.9\n\n71.7\n\n73.1\n\n80.4\n\n55.6\n\n67.8\n\nGotta (Chen et al., 2023  ###reference_b7###  ###reference_b7###)\n\n59.611.9\n\n63.611.0\n\n54.313.0\n\n66.312.5\n\n64.311.7\n\n73.211.5\n\n51.212.8\n\n61.8\n\nPMR (large) (Xu et al., 2022  ###reference_b46###  ###reference_b46###)\n\n67.51.7\n\n53.42.3\n\n46.82.6\n\n69.32.4\n\n61.72.1\n\n71.51.8\n\n43.43.6\n\n59.1\n\nFewshotBARTL (Chada and Natarajan, 2021  ###reference_b6###  ###reference_b6###)\n\n64.61.4\n\n63.02.1\n\n53.50.9\n\n65.52.4\n\n62.91.6\n\n73.90.8\n\n45.01.7\n\n61.2\n\nSplinter (base) (Ram et al., 2021  ###reference_b34###  ###reference_b34###)\n\n35.53.7\n\n38.22.3\n\n37.41.2\n\n39.83.6\n\n45.42.3\n\n49.53.6\n\n35.93.1\n\n40.2\n\nRoberta (base) (Ram et al., 2021  ###reference_b34###  ###reference_b34###)\n\n12.51.4\n\n24.21.0\n\n4.62.8\n\n19.82.4\n\n15.03.9\n\n34.01.8\n\n5.41.1\n\n16.5\n\n128 Samples\n\n\n\n\n\n\n\n\n\n\nOur approach\n\n77.40.3\n\n73.00.4\n\n57.40.7\n\n78.60.5\n\n76.50.2\n\n82.40.3\n\n63.51.6\n\n72.7\n\nPrompting+RSS Re-Impl\n\n77.1\n\n71.3\n\n55.1\n\n75.9\n\n75.9\n\n81.8\n\n62.4\n\n71.4\n\nPrompting (Castel et al., 2022  ###reference_b5###  ###reference_b5###)\n\n76.7\n\n69.9\n\n51.8\n\n73.4\n\n74.6\n\n85.2\n\n58.0\n\n69.9\n\nGotta (Chen et al., 2023  ###reference_b7###  ###reference_b7###)\n\n60.03.6\n\n64.91.2\n\n57.41.2\n\n69.81.5\n\n66.71.8\n\n78.62.1\n\n53.31.7\n\n64.4\n\nPMR (large) (Xu et al., 2022  ###reference_b46###  ###reference_b46###)\n\n70.30.5\n\n57.42.6\n\n52.31.4\n\n70.01.1\n\n65.91.0\n\n78.80.5\n\n45.11.2\n\n62.8\n\nFewshotBARTL (Chada and Natarajan, 2021  ###reference_b6###  ###reference_b6###)\n\n65.80.9\n\n64.31.3\n\n57.00.9\n\n67.71.0\n\n75.11.5\n\n75.01.5\n\n48.42.7\n\n64.8\n\nSplinter (base) (Ram et al., 2021  ###reference_b34###  ###reference_b34###)\n\n44.73.9\n\n46.30.8\n\n43.51.3\n\n47.23.5\n\n54.71.4\n\n63.24.1\n\n42.62.5\n\n48.9\n\nRoberta (base) (Ram et al., 2021  ###reference_b34###  ###reference_b34###)\n\n19.12.9\n\n30.11.9\n\n16.73.8\n\n27.82.5\n\n27.33.9\n\n46.11.4\n\n8.21.1\n\n25.0\n\nFull Dataset\n\n\n\n\n\n\n\n\n\n\nSplinter (base) (Ram et al., 2021  ###reference_b34###  ###reference_b34###)\n\n76.5\n\n81.0\n\n71.3\n\n83.0\n\n80.7\n\n91.011footnotemark: 1\n\n54.511footnotemark: 1\n\n76.9\n\nRoberta (base) (Ram et al., 2021  ###reference_b34###  ###reference_b34###)\n\n74.0\n\n79.6\n\n69.8\n\n81.5\n\n78.7\n\n84.111footnotemark: 1\n\n35.811footnotemark: 1\n\n71.9\nAs reported in table 1  ###reference_###  ###reference_###, without any labeled data our approach gets an F1 score of 85.5%.\nAlthough the performance increases if labeled data is added, this clearly outlines that there are strong zero-shot capabilities by employing data generation for MRQA.\nTherefore, we can conclude that the LM has learned during pre-training the relationship between questions and answers to an extent that is useful for SQuAD.\nIn summary, as an answer to RQ1, our Prompting-based data generation approach efficiently employs LMs for MRQA increasing performance compared to existing work.\nThe proposed method proves to be competitive on SQuAD and performs well especially without training data, establishing a new state of the art."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "5.2.   RQ2: Domain Generalization",
            "text": "###figure_2### To answer the second research question, we report results on the few-shot MRQA benchmark excluding SQuAD in table 3  ###reference_### (we additionally show the results of the best performing approaches for the mean of all datasets including SQuAD and all dataset sizes in figure 3  ###reference_###).\nFor NQ, HotpotQA and TextbookQA we consistently rank first across all dataset sizes.\nLargest absolute increases of F1 score can be observed on NQ with 32 samples (1.8%), SearchQA with 32, 64 and 128 samples (1.8%, 2.5% and 2.7%, respectively), HotpotQA with 32 samples (1.7%), and TextbookQA with 64 samples (2.2%).\nAlso, we see that in general the performance increases with more labeled data, although this behavior is not consistent in the case of TriviaQA, NewsQA and TextbookQA.\nInterestingly, on all sizes of BioASQ, our approach performs worse than directly using a Prompting model.\nSince the same applies to the Prompting approach using RSS, we assume that the MRQA model in our approach also suffers from the RSS pre-training although we cannot find reasons for RSS performing worse in this domain.\nFinally, we note that we saw quite high fluctuations between model training runs in the few-shot setting.\nWe assume this is owed to suboptimal hyperparameters which do not generalize well across domains and to too few samples.\nTraining on 128 samples or less can easily lead to overfitting resulting in loss of generalization.\nTherefore, with a better incorporation of few labeled data into the models, we believe that the MRQA performance of a Prompting-based data generation approach can further be increased.\nFor answering research question RQ2, we can conclude that our data generation approach also generalizes to other domains as demonstrated by the few-shot MRQA benchmark.\nTo analyze the benefit of our approach, we further investigate the quality of the generated question-answer pairs."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Analysis",
            "text": "###figure_3### In order to assess the data quality of the generated questions and answers a user study was taken.\nA total of 30 people, which were recruited via the Prolific platform, took part in the study.\nIn order to achieve a high significance, the selection of participants was restricted by the following screening:\nThe participants must have a bachelor\u2019s degree or higher, speak English as their primary language, and have a 100% approval rate with Prolific.\nThe aim of the study is to find out whether the generated data from our approach provide a comparable data quality as labeled data, are correct question-answer pairs with respect to the context, and can be improved quality-wise with more labeled samples.\nIn order to carry out the analysis, the study participants were randomly given 10 samples per participant.\nIn these, the context, the question candidate as well as the answer candidate was provided.\nIn total 300 question-answer pairs were individually assessed by humans with regard to their quality.\nFor this purpose, one question was asked per shown sample about the data quality121212The participants were asked to answer the following question: \u201dIs the question candidate correctly answered by the answer candidate?\u201d, which made it possible to distinguish between correct answers, partially correct answers (in the case of several answers possible from the context) as well as incorrect answers.\nIn the introduction it was explicitly pointed out that only the given context may be considered for answering the question candidate.\nThe dataset used was NewsQA, which has the lowest F1 compared to the other datasets used (cf. table 3  ###reference_###) and the quality of the generated question-answer pairs is thus tested on a comparatively difficult domain.\nTo achieve the above study objectives, 3 different settings were chosen, each given to 10 participants: generated data using 16 and 128 samples as well as gold data.\nThe results of the study are shown in figure 4  ###reference_###.\nFirst, we can observe that with 128 labeled samples, generated data is comparable to the quality of labeled data.\nAlthough with only 16 samples the majority of generated question-answer pairs is of high quality (59%) despite the extremely small effort needed by humans, this lacks behind 10 points in absolute percentage when compared to the data quality generated using 128 samples.\nTherefore, labeling 128 samples can be sufficient for our approach for the NewsQA dataset to get a similar quality of question-answer pairs compared to human annotated data that is more complex and costly."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7.   Conclusion",
            "text": "In summary, we introduced a new approach for MRQA that makes use of the linguistic knowledge encoded in LMs.\nTo this end, we proposed to generate synthetic question-answer pairs for MRQA and run several experiments to test the performance of our approach in the zero- and few-shot setting, thereby also showing its generalizability.\nAs a result, we have shown that LMs can be more effectively used, and find that our approach outperforms many state-of-the-art approaches for the MRQA task.\nFurthermore, in some settings, synthetic data is even on par with human annotated data.\nHowever, the performance heavily depends on the domain under consideration, with the highest absolute increase of performance for the most difficult domain.\nFinally, we demonstrated in a user study that it is possible with only taking into account 128 human annotated samples to generate question-answer pairs which are comparable to human annotated data in terms of quality.\nWe believe that there are many more ways to effectively use LMs and hope that our work will be an incentive to explore other possibilities."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "8.   Future Work",
            "text": "Although we have shown that our approach using NER sampled answers performs comparably well, other methods are worth to be explored too.\nFor example, leveraging LLMs to also generate the answer is interesting, but poses additional challenges for extractive MRQA.\nThey struggle in providing the start and end indices of answers if they are also used for selecting the answers, which renders some model architectures invalid.\nWe therefore believe that more investigation is necessary to enable more effective and more efficient use of LLMs.\nFor the generation, feedback (for example provided by humans) could be included to continuously improve the quality of synthetic data.\nRegarding the MRQA model, other methods to incorporate synthetic data should be taken into account too.\nFor example, adopting in-context learning for extractive MRQA is a highly interesting direction but out of scope of this work.\nFurthermore, since our approach performs well for SQuAD in the zero-shot setting as well, it should further be investigated, also for other domains."
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "9.   Ethical Considerations",
            "text": "Regarding our user study, participants were acquired using the Prolific platform.\nPrior to obtaining consent, we provided detailed instructions and descriptions of how their answers are processed, and that their participation is voluntary.\nWe also ensured that payment was not below Prolific\u2019s recommendation for the participants.\nRegarding privacy, we did not collect any personal or identifying data, or data other than for the question mentioned in this work."
        }
    ],
    "url": "http://arxiv.org/html/2405.09335v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.2.1",
            "4.2.2",
            "4.3",
            "4.4",
            "5",
            "5.1",
            "5.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "4.1",
            "4.2",
            "4.2.1",
            "4.2.2",
            "4.3"
        ]
    },
    "research_context": {
        "paper_id": "2405.09335v1",
        "paper_title": "Prompting-based Synthetic Data Generation for Few-Shot Question Answering",
        "research_background": "**Motivation:**\nThe motivation behind this paper is rooted in the significant challenge associated with Machine Reading Question Answering (MRQA), particularly in scenarios where labeled data is scarce (few-shot MRQA). The process of annotating data for MRQA is often time-consuming and expensive, and there is a noticeable performance gap between models trained with abundant data and those trained in low-resource settings. This discrepancy becomes even more pronounced in specific domains that lack extensive data or domain experts. Thus, the need for an effective method to generate high-quality synthetic data is evident to mitigate the issue of data scarcity and improve the performance of few-shot models.\n\n**Research Problem:**\nThe main research problem addressed in this paper is the effective use of language models (LMs) to generate synthetic data for improving the performance of MRQA under few-shot conditions. Specifically, the paper explores:\n1. How LMs can be utilized to generate synthetic data for the few-shot MRQA task.\n2. The extent to which synthetic data can improve performance.\n3. The impact of answer selection on performance.\n4. The necessity of labeled data versus the utility of LMs generating helpful data autonomously.\n5. The generalizability of the proposed approach across different domains.\n\n**Relevant Prior Work:**\nThe paper builds on various preceding works on MRQA and related tasks. Significant references include:\n\n1. **Surrogate MRQA Applications:**\n   - Named Entity Recognition (NER): Li et al. (2020), Arora and Park (2023)\n   - Entity Relation Extraction: Levy et al. (2017), Li et al. (2019), Zhang et al. (2022)\n   - Slot Filling: Gao et al. (2019)\n\n2. **Pre-training Language Models:**\n   - Success in applying pre-trained models to MRQA has been demonstrated, with notable works such as Masked Language Modeling (Devlin et al., 2019) contributing to this success. Rajpurkar et al. (2016) shows significant achievements surpassing even human-level performance on benchmarks like SQuAD.\n\n3. **Synthetic Data Generation:**\n   - Prior attempts to address low-resource MRQA settings through synthetic data have been proposed by Alberti et al. (2019), Puri et al. (2020), and Shakeri et al. (2020, 2021). \n   - Using PLMs for synthetic data generation in other NLP tasks has been showcased by Anaby-Tavor et al. (2020) and Schick and Sch\u00fctze (2021).\n\nThe paper emphasizes the unique challenge in MRQA of not just generating text given a label but creating both the question and the answer, where these elements are interdependent. The contribution from this work is presented as a novel method leveraging the linguistic knowledge in LMs to produce high-quality synthetic data for few-shot MRQA and demonstrating significant performance improvements, even outperforming high-resource settings in some cases.",
        "methodology": "**Prompting-based Synthetic Data Generation for Few-Shot Question Answering Methodology**:\n\nThe proposed methodology aims at tackling the task of Machine Reading Question Answering (MRQA), where given a context \\( C \\) and a question \\( Q \\), the goal is to predict the answer \\( A \\). In the specific focus of this work, the problem is further constrained to extractive MRQA, implying that the answer \\( A \\) is a single contiguous span within the context \\( C \\).\n\n**Approach Summary**:\nThe method leverages the Prompting framework and consists of two primary steps:\n\n1. **Sampling Answer Candidates**:\n   First, the approach involves sampling answer candidates directly from a given document. This step identifies potential correct answers within the text which will be used in the subsequent step.\n\n2. **Generating Questions**:\n   Secondly, the sampled answers, along with the document, are used to query a pre-trained language model (LM). The LM generates questions corresponding to the sampled answers. This is the question generation step, which aligns the sampled answers with suitable questions derived from the content of the document.\n\n**Data Generation Pipeline**:\nThe described methodology forms a data generation pipeline. The detailed process involves:\n- Sampling answers: Identifies answer candidates from the document.\n- Utilizing a pre-trained LM: Generates corresponding questions based on the sampled answers and the document context.\n\nThis innovative approach, centered around leveraging pre-trained language models for generating synthetic questions from sampled answer spans within the text, aims to improve the dataset quality for few-shot question answering tasks.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Datasets\n\nThe primary dataset used in the main experiment is the few-shot SQuAD dataset, which is a subset of the Stanford Question Answering Dataset (SQuAD) tailored for few-shot learning scenarios.\n\n#### Implementational Details and Approach\n\nThe experiment involves a model using a Prompting-based approach with fine-tuning on few-shot MRQA datasets. This approach incorporates soft token fine-tuning initialized with weights from embeddings and employs a template to convert the sample into a prompt, enabling the pre-trained model to answer the question using the provided context.\n\n#### Baselines\n\nThe following baselines are considered for comparison:\n1. **Base Model by Ram et al. (2021)**: Utilizes an RSS pre-training phase followed by fine-tuning on few-shot MRQA datasets.\n2. **FewshotBARTL (Chada and Natarajan, 2021)**: A prompting-based model utilizing BART.\n3. **Castel et al. (2022)**: A prompting-based MRQA model using T5 v1.1, with additional RSS pre-training.\n4. **Chen et al. (2023)**: A prompting model with additional pre-training on entity-aware masks.\n5. **PMR (Xu et al., 2022)**: Includes pre-training on automatically generated data in an MRQA style, followed by MRQA-specific fine-tuning.\n6. **Ram et al. (2021)**: A standard pre-training/fine-tuning model using Roberta (base) with a span extraction head.\n\n#### Evaluation Metrics\n\nThe evaluation metrics used in the experiments are not explicitly mentioned in the provided text. However, common metrics for QA tasks typically include Exact Match (EM) and F1 score.\n\n#### Main Experimental Results\n\nThe main experimental results focus on comparing the performance of the proposed prompting-based model with various baselines. Notably:\n- **FewshotBARTL** is identified as the best-performing model reported in FewshotQA.\n- The model by **Castel et al. (2022)** was only included for results on SQuAD.\n- A re-implemented version of the **Prompting+RSS** model is evaluated on the full few-shot MRQA benchmark.\n- The **Chen et al. (2023)** model with entity-aware masks and **PMR (Xu et al., 2022)** with MRQA style pre-training show competitive results.\n- **Ram et al. (2021)**'s Roberta-based model is considered a strong baseline following the standard pre-training/fine-tuning paradigm.\n\nThe results highlight the efficacy of the prompting-based approach in comparison to other state-of-the-art methods and underline the importance of soft token fine-tuning and entity-aware masks in enhancing few-shot question answering performance."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To investigate the performance of different prompting-based models, including various pre-training techniques and generated synthetic data, for few-shot question answering (QA) tasks.",
            "experiment_process": "Experiments were performed on several datasets from the Few-Shot MRQA benchmark, including SQuAD, TriviaQA, NaturalQuestionsShort, NewsQA, HotpotQA, BioASQ, and TextbookQA, using subsampled train and test splits with 16, 32, 64, and 128 training samples. The models compared include the original Prompting model with RSS pre-training, FewshotBARTL, a re-implemented Prompting+RSS model, and Chen et al. (2023)'s entity-aware Prompting model. Various hyperparameters were tuned using a validation set of SQuAD dataset.",
            "result_discussion": "The results showed that the proposed approach outperforms several baselines, including the Prompting+RSS model. Our approach achieved a mean and standard deviation over 5 runs, consistently outperforming previous models across different training sample sizes, thus demonstrating improved generalizability and robustness in few-shot QA settings.",
            "ablation_id": "2405.09335v1.No1"
        },
        {
            "research_objective": "To analyze the effectiveness of synthetic data generation using different models and the impact of various pre-processing strategies on QA performance.",
            "experiment_process": "Multiple models, templates, and pre-processing strategies were tested for generating questions, with the T5 model (v1.1 large) performing best. The model was trained using SQuAD's training data with specific hyperparameters set after tuning on a validation set. Different datasets, including PubMed for BioASQ and CK-12 for TextbookQA, were used to generate synthetic data. The generated questions were filtered using rule-based and consistency checks to ensure quality.",
            "result_discussion": "Using the T5 model with a specific template and pre-processing strategies resulted in high-quality synthetic data that significantly improved the performance of the MRQA model. The filtering steps ensured the generated samples were meaningful, contributing to the overall success of the QA model.",
            "ablation_id": "2405.09335v1.No2"
        },
        {
            "research_objective": "To validate the final MRQA model trained on both synthetic and few-shot annotated data and its effectiveness for QA tasks.",
            "experiment_process": "The MRQA model used T5 v1.1 (large) with the Prompting framework, trained initially on synthetic data for one epoch. Subsequent training was performed using annotated data from few-shot splits with specific hyperparameters. The final MRQA model's performance was assessed based on mean and standard deviation over 5 runs.",
            "result_discussion": "The final MRQA model, especially when incorporating RSS pre-training, demonstrated superior performance compared to several baselines, confirming its efficacy in few-shot QA tasks. The model trained on both synthetic and annotated data showed robust and improved results, further supporting the utility of synthetic data generation in enhancing QA performance.",
            "ablation_id": "2405.09335v1.No3"
        }
    ]
}