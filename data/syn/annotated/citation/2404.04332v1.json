{
    "title": "Scope Ambiguities in Large Language Models",
    "abstract": "Sentences containing multiple semantic operators with overlapping scope often create ambiguities in interpretation, known as scope ambiguities.\nThese ambiguities offer rich insights into the interaction between semantic structure and world knowledge in language processing.\nDespite this, there has been little research into how modern large language models treat them.\nIn this paper, we investigate how different versions of certain autoregressive language models\u2014GPT-2, GPT-3/3.5, Llama 2 and GPT-4\u2014treat scope ambiguous sentences, and compare this with human judgments.\nWe introduce novel datasets that contain a joint total of almost 1,000 unique scope-ambiguous sentences, containing interactions between a range of semantic operators, and annotated for human judgments.\nUsing these datasets, we find evidence that several models (i) are sensitive to the meaning ambiguity in these sentences, in a way that patterns well with human judgments, and (ii) can successfully identify human-preferred readings at a high level of accuracy (over 90% in some cases).111Data and code are available at: https://github.com/McGill-NLP/scope-ambiguity",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "###figure_1### Sentences like \u2018every farmer owns a donkey\u2019 are systematically ambiguous between two readings: one in which the embedded noun phrase (NP) (e.g. \u2018a donkey\u2019) is interpreted within the scope of the quantifier that precedes it (\u2018every\u2019), and another in which the embedded NP is interpreted outside its scope. As shown in Figure 1  ###reference_###, \u2018every farmer owns a donkey\u2019, for example, could either mean (i) that each farmer simply owns their own (possibly unique) donkey, or (ii) that there is a specific donkey in question that all farmers jointly own.\nSuch constructions are examples of what are known as scope ambiguities. They are called so because the standard account of these ambiguities is that they arise when the respective scope of multiple semantic operators in the expression is ambiguous, yielding more than one possible semantic structure.\nConsider the following example:\nEvery farmer owns a donkey.\n\\ex Surface Scope: \n\\ex Inverse Scope:\n(1  ###reference_###), in logical form, involves a universal quantifier (introduced by \u2018every\u2019), and an existential quantifier (introduced by \u2018a\u2019).\nThe ambiguity lies in the order of application (and thereby scopes) of these two operators.\nThe surface scope reading of the sentence, (1  ###reference_###), involves the universal quantifier outscoping the existential quantifier.\nThe inverse scope reading, (1  ###reference_###), involves the reverse.\nImportantly for this present work, English speakers (i) have access to both kinds of readings, and (ii) generally disambiguate between them to arrive at a preferred reading (see Kurtzman and MacDonald, 1993  ###reference_b22###).\nFor example, although (1  ###reference_###) has two possible interpretations, without further context, most people would prefer the surface reading, due to at least the surface positions of \u2018a\u2019 and \u2018every\u2019 in the sentence, as well as background world knowledge about farmers and donkeys (see Kurtzman and MacDonald, 1993  ###reference_b22###; Saba and Corriveau, 2001  ###reference_b37###; Anderson, 2004  ###reference_b3###, for insights into how such factors affect reading preferences).\nThe focus of this paper is how large language models (LLMs) treat such ambiguities.\nAssessing how they do so offers important insights into interactions between semantic structure and world knowledge, as well as the representation of scope in LLMs:\nScope disambiguation lies at the interface between natural language semantics and background world knowledge.\nScope ambiguous sentences like (1  ###reference_###) are ambiguous between two semantic structures; disambiguating between these two possible structures (and the different readings they yield), however, often requires background world knowledge Saba and Corriveau (2001  ###reference_b37###).\nTake the following two sentences:\nEvery conference attendee ate a Big Mac.\n\\exEvery conference attendee attended a networking event.\nBoth examples in (1  ###reference_.SSS0.Px1###) are scope-ambiguous in a similar way to (1  ###reference_###)\u2014each offers two possible semantic structures yielding different readings.\nHowever, choosing the preferred reading is easy in both cases: in (1  ###reference_.SSS0.Px1###), the surface scope reading (every attendee ate a potentially different Big Mac) is preferred, while in (1  ###reference_.SSS0.Px1###), the inverse scope reading (there was a single networking event that all attendees attended) is preferred.\nThese preferences are a result of the general knowledge we have about conference attendees, networking events and Big Macs.\nLLMs have been shown to be able to capture aspects of world knowledge (e.g., Roberts et al., 2020  ###reference_b36###; Heinzerling and Inui, 2021  ###reference_b15###; AlKhamissi et al., 2022  ###reference_b1###), and, separately, to capture some properties of natural language semantics (e.g., Jawahar et al., 2019  ###reference_b20###; Ettinger, 2020  ###reference_b10###; Pavlick, 2022  ###reference_b32###).\nScope ambiguities present an opportunity to assess how they might integrate the two.\nModel weights are largely uninterpretable, so despite generally high performance on language-based tasks, many questions remain about the abstract linguistic structures they capture Belinkov and Glass (2019  ###reference_b6###); Hewitt and Manning (2019  ###reference_b16###); Baroni (2022  ###reference_b5###).\nThe ambiguities discussed here arise out of a crucial component of linguistic structure: scope.\nAnalyzing how LLMs treat them helps us gain insight into how well they capture this component of structure.\nThis is particularly interesting because while formal logic, as in (1  ###reference_###), allows for a straightforward, symbolic representation of scope ambiguities, it remains an open question whether vector-based LLM representations can adequately capture the multiple readings of such constructions.\nThis paper therefore attempts to answer two questions:\nDo LLMs exhibit similar preferences to humans in the interpretation of scope ambiguous sentences?\nAre LLMs sensitive to the presence of more than one reading of scope ambiguous sentences?\nWe conduct two experiments to investigate these questions.\nFrom these experiments, we present evidence that the answer to these questions\u2014at least for the more powerful models\u2014is \u2018yes\u2019."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Scope ambiguities have been the focus in research within computational linguistics and natural language processing (NLP) primarily through the task of quantifier scope disambiguation, which involves the proper selection of a preferred scope reading given a scope-ambiguous sentence. Early examples of NLP research on this task, such as Higgins and Sadock (2003  ###reference_b17###) and Andrew and MacCartney (2004  ###reference_b4###), frame it as a classification task, and find models that outperform naive heuristics. Such work predates modern neural language models; Rasmussen (2022  ###reference_b34###), however, builds on this approach, framing quantifier scope disambiguation as a span-pair classification task. They test RoBERTa Liu et al. (2019  ###reference_b27###) on this task, and find that the model achieves higher accuracy on it than a majority-prediction baseline. This work, however, does not directly test the model\u2019s underlying linguistic capabilities; it tests the model on the classification task only after it is trained on examples from the dataset used. As a result, it is unclear to what degree the model\u2019s performance on the test set is due to linguistic capabilities that emerged from its pretraining. Manshadi and Allen (2011  ###reference_b28###) and Tsiolis (2020  ###reference_b42###) approach the problem differently, as neither frames it as a classification task. Manshadi and Allen (2011  ###reference_b28###) represent scope relations as graphs, and frame the task as one of graph construction; they present a support vector machine that beats a naive heuristic baseline. Tsiolis (2020  ###reference_b42###), on the other hand, attempts to reframe the task as a natural language inference task, Q&A task, or one in which probabilities of continuations are compared. They use a large language model\u2014GPT-2 Radford et al. (2019  ###reference_b33###) \u2014but present mixed results. Other research focuses on the linguistic factors that determine scope reading preferences in a corpus. AnderBois et al. (2012  ###reference_b2###) find that linear order, grammatical roles and lexical effects determine these preferences; Leczkowski et al. (2022  ###reference_b23###) build on this work and find that prepositions and preposition senses also affect scope reading preference. The only two instances of work assessing how LLMs treat scope ambiguities in zero-shot contexts are, to our knowledge, recent works by Liu et al. (2023  ###reference_b26###), and Stengel-Eskin et al. (2023  ###reference_b40###). The latter assesses how LLMs treat ambiguous inputs in terms of semantic parsing. The authors use templates to generate ambiguous sentences\u2014including scope-ambiguous sentences\u2014along with logical parses of them, and assess the abilities of LLMs to properly produce the two logical parses of each ambiguous sentence, in both few-shot and zero-shot contexts. They find that models are poor at generating both parses of ambiguous sentences in zero-shot contexts, but can more accurately generate both parses in few-shot contexts. Liu et al. (2023  ###reference_b26###), on the other hand, assess how LLMs treat linguistic ambiguity in terms of entailment relations. Using prompting approaches to the task, as well as observing probabilities assigned to continuations of ambiguous sentences, they present evidence suggesting LLMs struggle to model ambiguity. Both works, though they do not primarily focus on it, do include scope ambiguity data, and are thus relevant to our work. Where we diverge from these works, however, is in our data and experimental methods. While the templates Stengel-Eskin et al. (2023  ###reference_b40###) use allow for the generation of hundreds of sentences, they do limit the diversity of these stimuli; moreover, the scope ambiguities in their datasets are limited to instances of quantifier-quantifier interactions. Similarly, Liu et al. (2023  ###reference_b26###) estimate from a random sample that roughly 7.6% of their data involves scope ambiguity; manually inspecting all 579 ambiguous sentences in their dataset, however, we find that the dataset contains a total of around 20 instances of scope ambiguity. We also employ different experimental set-ups (see Sections 4.1  ###reference_### and 5.1  ###reference_###) than those used in the aforementioned works. Crucially, these experimental methods may be what provide us opposite findings from both of them; we discuss this difference in Section 8  ###reference_###. More broadly, our work belongs to a growing body of literature evaluating how well neural language models capture a range of semantic phenomena (see Pavlick, 2022  ###reference_b"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Background",
            "text": "We focus on scope ambiguities involving quantifiers such as \u2018some\u2019, \u2018every\u2019 and \u2018most\u2019, as well as quantifier-like determiners, like indefinites and numbers.\n(1  ###reference_###) is an example of scope ambiguity arising out of quantifier-quantifier interactions.\nBut scope ambiguities can also arise out of quantifier-negation and quantifier-adverb interactions, as shown below:\nQuantifier + Negation:\n{exe}\n\\exSita doesn\u2019t like a classmate of hers.\n\n\\ex{xlist}\n\\exSurface Reading: There is no classmate that Sita likes.\n\n\\exInverse Reading: There is a specific classmate that Sita does not like.\nQuantifier + Adverb:\n{exe}\n\\exBachi usually meets two professors.\n\n\\ex{xlist}\n\\exSurface Reading: Usually, Bachi meets any two professors, who are possibly different each time.\n\n\\exInverse Reading: There are two professors who Bachi meets regularly.\nIn all three cases, the different readings have different truth conditions, and each is therefore logically compatible with a different set of propositions.\nAs an illustration, consider our original example, reproduced here as (1  ###reference_###):\nEvery farmer owns a donkey.\n\n\\ex{xlist}\n\\exEach farmer has a different donkey.\n\n\\exAll farmers have the same donkey.\n(1  ###reference_###) is logically compatible with (1  ###reference_###) only given the surface scope reading of the sentence, which states that each farmer has a potentially unique donkey.\nIt is not logically compatible with the inverse scope reading of the sentence, which states that there is an individual donkey that all farmers (jointly) have.\n(1  ###reference_###), however, is also logically compatible with the inverse scope reading of the sentence.\nIn Experiments 1A and 1B, we use these differences in logical compatibility to assess whether LLMs exhibit similar preferences to humans in the interpretation of scope ambiguous sentences.\nSimilarly, different scope readings often yield different effects in a discourse setting.\nConsider (1  ###reference_###): under the inverse scope reading, two professors are introduced as constant across the instances of Bachi\u2019s meetings.\nConsequently, they can therefore be further referred to in the discourse, as in (1  ###reference_###).\nHe likes those two professors.\n\n\\exIt\u2019s a different pair each time.\nBut under the surface scope reading of (1  ###reference_###), there aren\u2019t necessarily two professors that are constant across instances, and who can therefore be further referred to in the discourse.\nAs a result, (1  ###reference_###) is not an acceptable follow-up.\nThe possible variability of the professors across multiple instances, however, does mean that (1  ###reference_###) is an acceptable follow-up (which it is not given the inverse scope reading).\nIn Experiments 2A and 2B, we use such patterns of acceptable and unacceptable followups to assess whether LLMs are sensitive to the presence of multiple readings of scope ambiguous sentences."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiment 1A",
            "text": "###table_1###"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Method",
            "text": "In our first experiment, we assess whether LLMs show similar preferences to humans in how scope ambiguous sentences are interpreted.\nWe frame this as a Q&A task.\nWe present the LLM with sentences that are technically scope ambiguous, but have one strongly preferred scope reading (in some cases, this is a surface scope reading, and in others, it is an inverse scope reading).\nWe then present the model with two possible statements based on this ambiguous sentence.\nOne statement is compatible with only the surface scope reading of the ambiguous sentence, while the other statement is compatible with the inverse scope reading.\nIn the case of chat-optimized models, we then ask the model which option is more likely; in the case of models not optimized for chat, we then obtain this answer through next token prediction.\nFinally, we observe whether these responses align with the reading preferred by most humans.\nFigure 2  ###reference_### shows an example of how we conduct this method, using the dataset we develop for this experiment (details in Section 4.2  ###reference_###).\nWe concatenate, with newlines as shown in Figure 2  ###reference_###, (i) a test sentence that is technically scope-ambiguous; (ii) an explanation that there are two options; (iii) two statements, labelled Option A and Option B, where one is compatible only with the surface scope reading, and the other is compatible with the inverse scope reading; and (iv) a prompt that elicits the model\u2019s preferred choice.\nIn the case of chat-optimized models, we observe the model\u2019s response to a question asking it to choose between the options.\nFor other models, we observe the next token predicted by the model after the text \u2018the most likely option among these two is option\u2019: this is either \u2018A\u2019 or \u2018B\u2019, corresponding to Option A and Option B respectively.\nWe treat these as the model\u2019s \u2018answer\u2019, and evaluate the model based on whether it aligns with preferred human readings."
        },
        {
            "section_id": "4.1.x",
            "parent_section_id": "4.1",
            "section_name": "Control: No Sentence in Prompt",
            "text": "One possible issue with this approach is that answers may depend more on the likeliness of the two options as general statements than on their likeliness given the ambiguous sentence.\nWe therefore add a control: we remove the ambiguous sentence altogether from the stimulus, and present the rest of it to the model, conducting the same task as before.\nIn this setting, the model should do significantly worse, as it is not exposed to the sentence that is being evaluated with respect to the two options.\nFor instance, in the example in Figure 2  ###reference_###, both options appear plausible in the absence of any context; following the scope-ambiguous sentence, however, only Option A is plausible.\nIf model performance does not drop significantly when the ambiguous sentence is dropped, the model\u2019s performance in the original setting is likely unrelated to its processing of the ambiguous sentence, and instead reflects the background likeliness of each option."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Dataset",
            "text": "We build upon the quantifier scope dataset presented by AnderBois et al. (2012  ###reference_b2###).\nWe chose this dataset as a starting point because among the few existing scope ambiguity datasets (see Section 2  ###reference_###), it was the dataset that had datapoints most appropriate to the focus of this study.\nThis dataset contains around 1,700 sentences and phrases scraped from LSAT (Law School Admission Test) logic puzzles, and marked for quantifier scope where present.\nWe filtered the dataset for instances of two interacting \u2018quantifiers\u2019.222These included constructions such as \u2018per\u2019 in \u2018one person per appointment\u2019\u2014constructions that have some quantificational force, even if they aren\u2019t quantifiers in the strict sense\u2014as well as instances of negation such as \u2018none\u2019 in \u2018none of the cities\u2019.\nThis narrowed it down to around 400 datapoints.\nNext, we manually constructed contrasting \u2018options\u2019 based on surface and inverse scope readings for whichever of these roughly 400 datapoints allowed this approach, giving us 186 sentences with accompanying contrasting statements.\nTo further ensure that these datapoints had strong scope reading preferences, we then conducted two rounds of human validation.\nIn both rounds, we recruited participants via Prolific.\nParticipants (38 in each round) were presented the filtered scope-ambiguous sentences along with two accompanying options, and were asked to pick the most likely option.\nIn the first round, we reworded datapoints with low subject agreement; in the second round, we dropped any datapoints with less than 75% agreement (all datapoints received at least 4 evaluations).\nFor the datapoints that remained, gold labels (i.e. the correct option for each datapoint, and consequently, the preferred scope reading) were taken as the majority vote from study participants.\nThis process ultimately yielded 153 scope ambiguous sentences, each with a pair of options.\nOf these, 41 had an inverse scope reading preferred, while the remaining 112 had a surface scope reading preferred.\nAlmost all were examples of scope ambiguities arising from quantifier-quantifier interactions, with a handful involving quantifier-negation interactions, and even fewer involving other types of interactions (see Table 1  ###reference_### for a breakdown by interaction type).\nAs a final step, we duplicated each datapoint, but with a flipped order of options (i.e. \u2018Option A\u2019 was labelled \u2018Option B\u2019, and vice versa).\nThis meant that while the distribution of preferred scope-readings remained skewed, the final dataset\u2014which contains 306 datapoints covering 153 unique sentences\u2014had an even distribution of correct answers (50% \u2018A\u2019 and 50% \u2018B\u2019)."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Models",
            "text": "For all experiments, we choose to use autoregressive language models, due to their growing prevalence in practical applications using prompting.\nSpecifically, for this experiment, we use chat and vanilla versions of Llama 2 Touvron et al. (2023  ###reference_b41###) at 7B, 13B and 70B sizes333For Llama 2 at 70B, we use a version of the model loaded in 8-bit (see Dettmers et al. (2022  ###reference_b9###))., three variants of GPT-3/3.5 Brown et al. (2020  ###reference_b7###); Ouyang et al. (2022  ###reference_b31###): davinci, text-davinci-002, and text-davinci-003, and GPT-4 OpenAI (2023  ###reference_b30###).\nSee Table 2  ###reference_### for a summary of key differences between these models.\nGPT-3-davinci\n175B\nGPT-3.5-text-davinci-002\nUnclear\nGPT-3.5-text-davinci-003\nUnclear\nGPT-3.5-turbo\nUnclear\nGPT-4\nUnclear; possibly ensemble model\nLlama 2\n7B, 13B, 70B\nLlama 2 Chat\n7B, 13B, 70B\nGPT-2\n117M, 345M, 774M, 1.5B"
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Human Baselines",
            "text": "After the human feedback-based filtering mentioned above, we conducted another round of the same experiment with humans to get human baselines on this dataset.\nWe are testing models on their ability to choose the scope readings preferred by most people\u2014but how good are human themselves at choosing the scope readings preferred by most other people?\nOur human baselines should provide a sense of the answer to this question.\n68 native speakers of English were recruited via Prolific for a repeat of the experimental set-up described in Section 4.2  ###reference_###, but this time with the final dataset.\nEach participant was presented with 18 datapoints and evaluated on their answers.\nWe then calculated overall accuracy as the total proportion of correct responses out of all responses."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Results",
            "text": "The results of Experiment 1A are shown in Table 3  ###reference_###.\nHuman responses yield an average accuracy of around 90%, suggesting that English speakers can, with a high degree of accuracy, arrive at scope reading preferences shared by most other people.\nWhen it comes to model responses, although models like davinci and Llama2-7b do not perform far above chance (50%, since correct answers in the dataset were balanced through duplication), several other models do achieve high performance\u2014both versions of Llama2-70b, as well as all the GPT-3.5 models achieve an accuracy of 80% or more in the test setting, while GPT-4 achieves 98%, close to the ceiling.\nThe control setting adds further insights to these results.\nNeither davinci nor the versions of Llama 2 at 7B see their accuracy scores drop when prompts are provided without the actual scope ambiguous sentence (Llama2-7b actually produces a higher accuracy in this setting), suggesting that performance in the test setting is not a result of the models\u2019 processing of the ambiguous input, but primarily driven by the background likeliness of the two options. The models with higher accuracy scores, however, see more severe drop-offs in the control setting, most notably with GPT-4, which sees its accuracy drop to 75% in the control setting."
        },
        {
            "section_id": "4.6",
            "parent_section_id": "4",
            "section_name": "Discussion",
            "text": "These results suggest that the more advanced LLMs evaluated\u2014GPT-3.5, Llama 2 at 70B, and most notably GPT-4\u2014are able to exhibit similar scope reading preferences as humans, with a high level of accuracy.\nSmaller or less advanced models, however, such as Llama 2 at 7B, appear to fail.\nAlso worth noting is the fact that, for almost all models, performance on sentences that had a preferred inverse scope reading was lower than on those that had a preferred surface scope reading.\nThis aligns with literature suggesting that inverse scope readings are generally harder to access than surface readings (see e.g., Kurtzman and MacDonald, 1993  ###reference_b22###; AnderBois et al., 2012  ###reference_b2###), but curiously, does not align with the behaviour of humans in this experiment, who showed no such dispreference.\nThe deeper implication of some of the models\u2019 high performance, however, is that LLMs can not only capture different types of readings\u2014surface and inverse, which correspond to different semantic structures\u2014but also integrate background world knowledge in their behavioural preferences when confronted with scope ambiguous constructions."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiment 2A",
            "text": ""
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Method",
            "text": "###figure_2### Our first experiment showed us that, where there is a clear preferred reading, LLMs can mimic human preferences in the interpretation of scope ambiguous sentences.\nIt did not, however, indicate whether or not LLMs were sensitive to the fact that each such scope ambiguous sentence had more than one reading.\nThis question is the focus of our second experiment.\nHere, we assess whether models exhibit different behaviour for scope ambiguous sentences than they do for similar, non scope ambiguous sentences, in a manner that indicates a sensitivity to the meaning ambiguity in the former but not the latter.\nWe do not frame this as a conventional goal-oriented task such as Q&A.444This was mainly due to our focus on examples without one strongly preferred reading (see Section 5.2  ###reference_###). Without one strongly preferred reading, tasks like Q&A, which require a \u2018right\u2019 and \u2018wrong\u2019 answer, are difficult to implement.\nInstead, following work that brings psycholinguistic methods to language model analysis (see Linzen et al., 2016  ###reference_b25###; Futrell et al., 2019  ###reference_b13###; Ettinger, 2020  ###reference_b10###; Baroni, 2022  ###reference_b5###; Schuster and Linzen, 2022  ###reference_b38###), we investigate the question by observing the probabilities a model assigns to different types of continuations given a scope-ambiguous sentence.\nFigures 1  ###reference_### and 3  ###reference_### illustrate the general set-up we employ:\nwe begin by presenting the LLM with a scope ambiguous sentence .\nWe then observe the probabilities the model assigns to two followups to , labelled  and .\n is an acceptable continuation to  only given the inverse scope reading of , while  is an acceptable continuation to  only given the surface scope reading of .\nWe then compare these probabilities with those the model assigns to  and  given a control sentence, .\n is highly similar in both syntax and semantics to , but differs in that it is not scope ambiguous;  remains an acceptable continuation to , though  does not.\nIf an LLM successfully captures the meaning ambiguity of a sentence like , we would expect the ratio of probabilities it assigns to  and  as continuations to  to be smaller than the ratio of probabilities it assigns to the same followups as continuations to .\nIn other words, we expect the following inequality to hold:\n\nThis is because while  is ambiguous between two readings, and therefore allows for both  and  as continuations,  has only one reading, and allows only for  as a continuation (see Figure 3  ###reference_###).\nIf an LLM upholds (5.1  ###reference_###), it thus provides evidence of capturing the fact that  is ambiguous between two readings, in a way that non scope-ambiguous sentences, even if syntactically and semantically similar, are not.\nFor a more thorough analysis, we also observe the degree to which  is greater than .\nWe measure the difference in the log ratios of  and  given , and  and  given , and use it to calculate what we call the model\u2019s ambiguity recognition score, or \u2018-score\u2019:\n\nIf the inequality in (5.1  ###reference_###) holds, the -score will be positive; the larger its value, the greater the difference in ratios."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Dataset",
            "text": "Existing scope ambiguity datasets (see Section 2  ###reference_###) are (i) few in number, and (ii) generally involve examples where one scope reading is strongly preferred over the other.\nWhile we made use of this second observation in our first experiment, where we assessed whether LLMs exhibited similar scope reading preferences as humans, it is a significant problem for the current experiment, due to its aims.\nIn this experiment we aim to test LLMs for their sensitivity to the presence of multiple readings of a scope ambiguous sentence.\nBut in such cases, if humans themselves find one of these readings very hard to access without further context, it would be unfair to expect models to do so.\nFor a fair evaluation, therefore, it is crucial that we use sentences which do not have one reading strongly preferred over another.\nWe therefore construct a small-scale dataset, consisting of 38 manually handcrafted datapoints, where each datapoint includes a scope ambiguous sentence (), a matching non scope-ambiguous control sentence (), and two follow-up phrases ( and ), yielding a total of 152 sentence-continuation pairs.\nFor further validation, these datapoints were then filtered through our human baselines: any datapoints that yielded negative human-derived scores (details in Section 5.3  ###reference_###) were dropped, as such scores indicated that these were datapoints for which the inequality in (5.1  ###reference_###) did not align with human judgments.\nThis left us with 29 unique datapoints, yielding 116 unique sentence-continuation pairs (see Table 1  ###reference_### for a breakdown by interaction type)."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Human Baselines",
            "text": "Since the current experiment involves the analysis of probabilities assigned to text sequences\u2014something not directly replicable with humans\u2014we use proxy scores derived from a crowdsourced judgment task as our human baselines.\nWe conducted a crowdsourced study via Prolific, involving 140 native speakers of English; each was presented random sentence-continuation pairs from the dataset, and asked to provide ratings from 1 to 7 on how \u2018natural-sounding\u2019 the continuation was to the sentence.\nFrom these ratings, we computed the mean score for each sentence-continuation pair, and normalized them to be in an interval between 0 and 1.\nWe then treat these normalized scores as we treat model-assigned probabilities when calculating -scores and label the negative difference of log ratios our proxy -score.\nThis proxy score gives us an indirect means by which to compare human judgments of scope-ambiguous sentences and continuations with LLM-assigned probabilities of the latter given the former.\nJust as in the case of -scores for models, we would expect human proxy scores to be positive."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Models",
            "text": "As in Experiment 1A, we work with autoregressive LLMs.\nUnlike in Experiment 1A, however, the current experimental set-up allows us to also work with models ill-suited to zero-shot contexts.\nWe therefore ran this experiment not only on the models tested before, but also on several smaller variants of GPT-2 Radford et al. (2019  ###reference_b33###): small (117M params), medium (345M params), large (774M params) and XL (1.5B params).555Probabilities from GPT-2 and Llama 2 models were extracted using the minicons library Misra (2022  ###reference_b29###).\nThe reliance on probabilities, however, forces us to omit GPT-3.5-turbo and GPT-4, for which sequence log-probabilities are not accessible."
        },
        {
            "section_id": "5.5",
            "parent_section_id": "5",
            "section_name": "Results",
            "text": ""
        },
        {
            "section_id": "5.5.1",
            "parent_section_id": "5.5",
            "section_name": "5.5.1 Mean Scores",
            "text": "We first compute mean  and proxy scores, along with -values derived from paired -tests.666We choose this statistical measure as both  and proxy scores are calculated as paired differences of differences.\nPositive, statistically significant mean scores point to an overall sensitivity to the meaning ambiguity of the sentences in the dataset; comparing between models, higher mean scores also suggest stronger overall sensitivity.777While the latter is true between models, this relationship is less clear when comparing model scores and human scores. The main reason is that while human scores are derived from a bounded set of ratings between 1 and 7, model log-probabilities practically have no negative bound, allowing for more extreme differences between them. Since -scores are computed as differences of log differences (see (5.1  ###reference_###)), it is thus possible for them to be much higher than derived human scores, purely on account of being unbounded below zero.\nTable 4  ###reference_### shows our results.\nAll models yield positive mean -scores; and barring the case of GPT-2-small, all are statistically significant at a threshold of ."
        },
        {
            "section_id": "5.5.2",
            "parent_section_id": "5.5",
            "section_name": "5.5.2 Correlations Between Model and Human Scores",
            "text": "We also compute the correlation between model-derived -scores and human-derived proxy scores, to see how model behavior aligns with human judgments; if the two do align well, we expect to see a strong, positive correlation between them.\nTable 4  ###reference_### shows model-wise Pearson correlation coefficients between -scores and human proxy scores, along with corresponding -values.\nMany models fail to produce correlations that are significant at ; text-davinci-003 and Llama 2 at 13B, however, both produce highly significant correlation scores.\nThe former also produces the highest correlation score, at around 0.62.\nSee Figure 4  ###reference_### for a scatterplot of its -scores against corresponding human proxy scores; the plot lends further evidence to this correlation.\n###figure_3###"
        },
        {
            "section_id": "5.5.3",
            "parent_section_id": "5.5",
            "section_name": "5.5.3 Proportion of Positive -Scores",
            "text": "Lastly, we compute the proportion of datapoints for which models produced positive -scores, which allows us to assess whether the models behave consistently across datapoints. Like with correlations, we observe an effect of model size: larger models perform well, with several yielding positive -scores for over 90% of the data, and once again, text-davinci-003 performs the best."
        },
        {
            "section_id": "5.6",
            "parent_section_id": "5",
            "section_name": "Discussion",
            "text": "These results suggest that a wide range of LLMs may be sensitive to the meaning ambiguity in scope ambiguous sentences.\nThe positive mean -scores provide evidence that larger or more powerful models (i.e. those besides GPT-2 small) distinguish between scope ambiguous and non-scope ambiguous sentences in a manner consistent with their meanings.\nSimilarly, the statistically significant correlations we see between some models\u2019 -scores and human proxy scores suggest that, at least for certain models, this behaviour correlates well with human judgments.\nThese models also produce positive -scores for a high proportion of the data, indicating a high level consistency in this behaviour.\nComparing chat and vanilla versions of Llama 2 also reveals an interesting pattern.\nAs Table 4  ###reference_### shows, chat versions of Llama 2 produce slightly higher mean -scores and correlations than their non-chat equivalents, but also lower proportions of positive -scores\u2014indicating increased alignment with human judgments on several sentences, but lower overall consistency.\nThe broader takeaway, however, is that several LLMs appear sensitive to a meaning ambiguity that arises from the presence of different possible semantic structures, which vary vis-\u00e0-vis scope relations.\nConsequently, although the current work does not investigate how or where models represent scope, these results suggest that LLMs capture scope-related phenomena."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Re-evaluation on Expanded Datasets",
            "text": "The results from Experiments 1A and 2A are promising, but rely on relatively small datasets that contain 153 and 29 unique datapoints respectively\u2014raising questions of how generalizable our results are.\nAs a followup, we therefore considerably expand these two datasets, and rerun the same experiments described in Sections 4  ###reference_### and 5  ###reference_### on the expanded datasets.\nTo expand our Experiment 1A dataset, we begin by annotating it for both semantic operator and quantifier types: whether an ambiguity arose out of a negation-quantifier or quantifier-quantifier interaction, as well as whether the quantifier was an existential quantifier, universal quantifier, number or indefinite.\nCombined with scope reading preference labels (see Section 4.2  ###reference_###), this gave us 13 categories of scope reading and operator combinations (e.g. negation-indefinite_surface, or number-universal_inverse).\nFollowing this categorization, we add manually handcrafted examples to any sparse categories, so each contains at least 10 unique datapoints.\nWe then use GPT-4 to expand the dataset.\nFor each category in our annotated dataset, we randomly sample 5 datapoints, and instruct GPT-4 to produce 10 novel datapoints based on them.\nWe repeat this process ten times, such that we have 100 datapoints generated from each of our annotated categories.\nWe then manually inspect the combined 1,300 generated datapoints, removing duplicates, and dropping or editing low-quality datapoints; this left us with 1,062 datapoints.\nFinally, we run a crowd-sourced study via Prolific (278 participants, 5 ratings per datapoint), similar to those described in Sections 4.2  ###reference_### and 4.4  ###reference_###, to obtain our gold labels for preferred scope readings, and filter out any datapoints that received low inter-subject agreement.\nThis process eventually yields 837 unique scope-ambiguous sentences (with accompanying \u2018options\u2019, and human preference labels); 534 receive a preferred surface scope reading, and 303 an inverse scope reading.\nWe use a process similar to the one for Experiment 1A.\nWe first split our Experiment 2A dataset into categories based on whether the datapoints involve negation-quantifier, adverb-quantifier or quantifier-quantifier interactions.\nWe then run the same sampling, generation and manual filtering process as with the Experiment 1A dataset, giving us 126 datapoints from 300 GPT-4 generated datapoints.\nFinally, we run another study via Prolific (223 participants, 8 ratings per sentence-followup pair), and use this human judgement data to further filter the dataset.\nOur final dataset consists of 110 unique datapoints, where each datapoint consists of a scope-ambiguous sentence, control sentence, follow-up supporting an inverse scope reading, and follow-up supporting a surface scope reading."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Dataset Expansion Process",
            "text": "To expand our Experiment 1A dataset, we begin by annotating it for both semantic operator and quantifier types: whether an ambiguity arose out of a negation-quantifier or quantifier-quantifier interaction, as well as whether the quantifier was an existential quantifier, universal quantifier, number or indefinite.\nCombined with scope reading preference labels (see Section 4.2  ###reference_###  ###reference_###), this gave us 13 categories of scope reading and operator combinations (e.g. negation-indefinite_surface, or number-universal_inverse).\nFollowing this categorization, we add manually handcrafted examples to any sparse categories, so each contains at least 10 unique datapoints.\nWe then use GPT-4 to expand the dataset.\nFor each category in our annotated dataset, we randomly sample 5 datapoints, and instruct GPT-4 to produce 10 novel datapoints based on them.\nWe repeat this process ten times, such that we have 100 datapoints generated from each of our annotated categories.\nWe then manually inspect the combined 1,300 generated datapoints, removing duplicates, and dropping or editing low-quality datapoints; this left us with 1,062 datapoints.\nFinally, we run a crowd-sourced study via Prolific (278 participants, 5 ratings per datapoint), similar to those described in Sections 4.2  ###reference_###  ###reference_### and 4.4  ###reference_###  ###reference_###, to obtain our gold labels for preferred scope readings, and filter out any datapoints that received low inter-subject agreement.\nThis process eventually yields 837 unique scope-ambiguous sentences (with accompanying \u2018options\u2019, and human preference labels); 534 receive a preferred surface scope reading, and 303 an inverse scope reading.\nWe use a process similar to the one for Experiment 1A.\nWe first split our Experiment 2A dataset into categories based on whether the datapoints involve negation-quantifier, adverb-quantifier or quantifier-quantifier interactions.\nWe then run the same sampling, generation and manual filtering process as with the Experiment 1A dataset, giving us 126 datapoints from 300 GPT-4 generated datapoints.\nFinally, we run another study via Prolific (223 participants, 8 ratings per sentence-followup pair), and use this human judgement data to further filter the dataset.\nOur final dataset consists of 110 unique datapoints, where each datapoint consists of a scope-ambiguous sentence, control sentence, follow-up supporting an inverse scope reading, and follow-up supporting a surface scope reading."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Experiment 1B",
            "text": "We re-run Experiment 1A on the expanded dataset; Table 5  ###reference_### shows our results.\nAs can be seen, the general patterns observed in Experiment 1A (see Section 4.5  ###reference_###) continue to hold true even when the models are evaluated on a much larger dataset.\nWhile some models either perform around chance or do not show a major accuracy drop in the control setting, models like GPT-4, text-davinci-003 and Llama2-70b show both high performance (all above 85% accuracy in the test setting, with GPT-4 achieving 96% accuracy, albeit on data it produced in a separate context), as well as a drop-off in performance in the control setting."
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "Experiment 2B",
            "text": "Similarly, we re-run Experiment 2A on the expanded dataset; Table 6  ###reference_### shows our results.\nAs with Experiments 1A and 1B, the general patterns observed continue to hold for the expanded dataset.\nAll models still produce positive mean -scores.\nThough not as high as on the original dataset, text-davinci-003 once again produces the highest correlation with human data, with a R-value of roughly 0.48.\nSimilarly, most models show a high level of consistency in their behaviour, producing positive -scores from, in the case of text-davinci-003 and Llama2-13b, over 90% of the data.888GPT-2 models also do much better on the expanding dataset. This may be because in the expansion process, we ensured that the contrast between acceptable and unacceptable sentence-followup pairs (see Figure 3  ###reference_###) was more clear cut than in the original dataset, often coming from grammatical cues, rather than world knowledge cues. GPT-2 may recognize the former more than the latter, and thus perform better here.\nOnce again, Llama 2 chat models produce higher correlations and mean scores than their vanilla counterparts, but in two out of three cases, lower proportions of positive -scores."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "Our results, which indicate that LLMs are both proficient at choosing the scope readings preferred by most humans, and sensitive to the meaning ambiguity in scope ambiguous constructions, offer further evidence of the capacity of large language models to induce semantic structure (see Pavlick, 2022  ###reference_b32###), and linguistic structure more generally (see Linzen and Baroni, 2021  ###reference_b24###; Baroni, 2022  ###reference_b5###).\nOn the other hand, these results contrast with closely related work by Liu et al. (2023  ###reference_b26###) and Stengel-Eskin et al. (2023  ###reference_b40###), who both find that LLMs struggle to model ambiguity in zero-shot contexts.\nWhat explains this contrast?\nOne possible explanation is the difference in methodologies used.\nWe assess models using Q&A- and probability-based approaches (see Sections 4.1  ###reference_### and 5.1  ###reference_###) that implicitly test models\u2019 access to different scope readings.\nLiu et al. (2023  ###reference_b26###), on the other hand, mostly use prompting-based approaches that elicit model responses on what an ambiguous sentence may mean or entail, and Stengel-Eskin et al. (2023  ###reference_b40###) assess models in terms of their abilities to logically parse ambiguous inputs.\nIt is possible that LLMs implicitly capture meaning ambiguities and human-preferred interpretations, but cannot reliably produce meta-linguistic judgments or logical translations consistent with this information.\nThis is would be in line with findings from Hu and Levy (2023  ###reference_b18###), which suggest meta-linguistic prompting-based approaches may underestimate LLMs\u2019 linguistic abilities.\nTo test this theory, we adapt a random sample of our Experiment 2B dataset to the format Liu et al. (2023  ###reference_b26###) use in their True/False evaluation of models (see Section 4.2 of Liu et al., 2023  ###reference_b26###).\nIn this format, models are prompted to answer whether it is true or false that, given one of its disambiguations, an ambiguous input may, may not, cannot, or can only mean the disambiguation.\nWe rerun their experiment on this subset of our data; our results, shown in Table 7  ###reference_###, are similar to the authors\u2019 findings on their own data.\nMost models do poorly on this task, performing around chance (50%), with GPT-4 and GPT-3.5-turbo only achieving 64% accuracy.\nAs shown in Section 6.3  ###reference_###, however, using the dataset in our experimental format yields positive results that contrast this poor performance.\nThis divergence highlights the importance of diverse approaches to investigating the linguistic capacities of language models; our results suggest that probability- and prompting-based methods may yield differing conclusions."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we investigated how different autoregressive language models treat scope ambiguities.\nIn doing so, we introduced novel datasets that contain a joint total of roughly 1,000 unique and diverse scope-ambiguous sentences, annotated for human judgments\u2014the largest of this kind.\nOur results indicate that LLMs are able to exhibit behaviour in line with human preferences of interpretation\u2014informed at least in part by background knowledge\u2014as well as compatible with different types of semantic structures.\nFinally, the contrast between our findings and those of other recent works emphasizes the need for diverse approaches in assessing the linguistic capacities of large language models."
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "Aside from its focus only on English, one constraint of this work is that it does not assess how context affects scope reading preferences.\nAda often studies with a few of her friends.\n{xlist}\n\\exContext: Ada finds it hard to study alone, so she generally invites others for joint study sessions.\n\\exContext: Ada, Rohan and Jo are good friends in the same program, and prepare for exams together.\n(9  ###reference_###) is ambiguous between a surface scope reading ((9  ###reference_###) refers to no friends in particular) and an inverse scope reading ((9  ###reference_###) refers to some specific friends).\nDifferent background contexts can prompt different readings: (9  ###reference_###) prompts the surface scope reading, while (9  ###reference_###) prompts the inverse scope reading.\nOur work does not address such effects.\nAt a higher level, while this work shows how LLMs treat scope-ambiguous inputs, it also does not reveal how or where models represent scope.\nParallel work on model interpretability (such as causal mediation analysis, e.g., Vig et al., 2020  ###reference_b43###; Finlayson et al., 2021  ###reference_b12###; Geiger et al., 2022  ###reference_b14###) could provide exciting insights to this question."
        }
    ],
    "url": "http://arxiv.org/html/2404.04332v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "4.1",
            "4.1.x",
            "4.2",
            "4.3",
            "4.4",
            "5.1",
            "5.2",
            "5.3",
            "5.4"
        ],
        "main_experiment_and_results_sections": [
            "4.5",
            "4.6",
            "5.5",
            "5.5.1",
            "5.5.2",
            "5.5.3",
            "5.6"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.1",
            "4.1.x",
            "4.2",
            "4.3",
            "4.4",
            "4.5",
            "5.1",
            "5.2",
            "5.3",
            "5.4",
            "5.5",
            "5.5.1",
            "5.5.2",
            "5.5.3",
            "6.1",
            "6.2",
            "6.3"
        ]
    },
    "research_context": {
        "paper_id": "2404.04332v1",
        "paper_title": "Scope Ambiguities in Large Language Models",
        "research_background": "### Paper's Motivation:\nThe motivation of this paper is to investigate how large language models (LLMs) handle scope ambiguities in natural language sentences. Sentences like \"every farmer owns a donkey\" have more than one possible interpretation due to the different possible scopes of their quantifiers. Understanding how LLMs manage these ambiguities can reveal important insights about the interactions between semantic structure and world knowledge in these models. This is critical, as LLMs perform well on language tasks but their ability to capture abstract linguistic structures, such as scope, remains underexplored.\n\n### Research Problem:\nThere are two main research questions that this paper aims to address:\n1. Do LLMs exhibit similar preferences to humans in the interpretation of scope ambiguous sentences?\n2. Are LLMs sensitive to the presence of more than one reading of scope ambiguous sentences?\n\n### Relevant Prior Work:\n1. **Human Interpretation of Scope Ambiguities**:\n   - **Kurtzman and MacDonald (1993)**: Offers insights into how English speakers access and disambiguate between different readings of scope ambiguous sentences.\n   - **Saba and Corriveau (2001)**; **Anderson (2004)**: Discuss factors such as sentence structure and world knowledge that influence reading preferences.\n\n2. **LLMs and World Knowledge**:\n   - **Roberts et al. (2020)**; **Heinzerling and Inui (2021)**; **AlKhamissi et al. (2022)**: Demonstrate that LLMs can capture aspects of world knowledge.\n\n3. **LLMs and Natural Language Semantics**:\n   - **Jawahar et al. (2019)**; **Ettinger (2020)**; **Pavlick (2022)**: Show that LLMs can capture some properties of natural language semantics.\n\n4. **LLM Performance and Interpretability**:\n   - **Belinkov and Glass (2019)**; **Hewitt and Manning (2019)**; **Baroni (2022)**: Point out that while LLMs achieve high performance, their model weights are mostly uninterpretable, leaving questions about the abstract linguistic structures they capture.\n  \nBy exploring how LLMs handle scope ambiguities, this paper sheds light on whether these models adequately represent and differentiate multiple interpretations of these linguistic constructs.",
        "methodology": "The proposed methodology aims to evaluate if large language models (LLMs) exhibit human-like preferences when interpreting scope-ambiguous sentences. This is framed as a Q&A task, structured as follows:\n\n1. **Presentation of Ambiguous Sentences**: LLMs are presented with sentences that are technically scope-ambiguous but have a strongly preferred scope reading according to human preferences. These sentences can favor either a surface scope reading or an inverse scope reading.\n\n2. **Preparation of Two Statements**: Following the ambiguous sentence, the LLM is presented with two potential interpretations or statements:\n   - **Option A**: Compatible only with the surface scope reading.\n   - **Option B**: Compatible only with the inverse scope reading.\n\n3. **Query for Preference**: Depending on the type of model:\n   - **Chat-Optimized Models**: The model is directly asked which option is more likely.\n   - **Non-Chat-Optimized Models**: The model's preference is inferred through the next token prediction method. The text ends with 'the most likely option among these two is option:', and the LLM is expected to predict either 'A' or 'B'.\n\n4. **Example Format**: The procedure is meticulously structured. It includes:\n   - A technically scope-ambiguous test sentence.\n   - An explanation of the two options.\n   - The two statements (Option A and Option B).\n   - A prompt that elicits the model\u2019s preferred choice.\n\n5. **Response Collection and Evaluation**: Responses are collected based on the model's choice between Option A and Option B. These responses are then compared to human-preferred readings to assess alignment.\n\nIn summary, the methodology aims to systematically determine if LLMs align with human scope-reading preferences in ambiguous sentences by comparing model predictions with human-like interpretations, using configurable prompts tailored to the model\u2019s interaction capabilities (chat optimization versus next token prediction).",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n**Experiment Setup:**\n1. **Datasets:** The dataset used in the main experiment consisted of sentences with balanced correct answers to ensure a 50% chance performance level by random guessing.\n2. **Baselines:** The human performance average accuracy of around 90% served as the baseline for the model's accuracy comparison.\n3. **Models Evaluated:** \n- davinci\n- Llama2-7b\n- Both versions of Llama2-70b\n- All versions of GPT-3.5\n- GPT-4\n4. **Evaluation Metrics:** Accuracy in determining scope reading preferences was the primary evaluation metric.\n\n**Main Experimental Results:**\n- **Human Performance:** Averaged around 90%, indicating strong consistency among English speakers in scope reading preferences.\n- **Model Performance in Test Setting:**\n- **davinci and Llama2-7b:** Did not exceed chance performance (50%).\n- **Llama2-70b (both versions):** Achieved an accuracy of 80% or above.\n- **All GPT-3.5 models:** Achieved an accuracy of 80% or above.\n- **GPT-4:** Achieved an accuracy of 98%, closely matching the human accuracy baseline.\n\nThese results indicate that while some models do not effectively process scope ambiguities, others like Llama2-70b, GPT-3.5 models, and especially GPT-4, show strong performance, suggesting their responses are driven by genuine understanding rather than background likelihoods."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Assess whether LLMs show similar preferences to humans in interpreting scope ambiguous sentences.",
            "experiment_process": "The experiment was framed as a Q&A task. The LLM was presented with scope ambiguous sentences having a strongly preferred scope reading, followed by two statements: one compatible with the surface scope reading, and the other with the inverse scope reading. The model was then asked to choose the more likely option. For chat-optimized models, the response was directly obtained through chat; for other models, it was obtained via next token prediction after the phrase 'the most likely option among these two is option'. The model's responses were then compared with human preferred readings to evaluate alignment.",
            "result_discussion": "Human responses averaged around 90% accuracy, indicating high agreement among English speakers. While some models like davinci and Llama2-7b performed close to chance, others like Llama2-70b and GPT-3.5 variants achieved over 80% accuracy. GPT-4 achieved a near-ceiling accuracy of 98%. However, in the control setting (where the ambiguous sentence was omitted), models like davinci and Llama2-7b did not show performance drops, suggesting their original performance was not due to processing the ambiguous input. In contrast, models with higher accuracy, particularly GPT-4, saw significant drop-offs in the control setting.",
            "ablation_id": "2404.04332v1.No1"
        },
        {
            "research_objective": "Determine if LLMs are sensitive to the multiple readings of scope ambiguous sentences rather than similar, non-ambiguous sentences.",
            "experiment_process": "The experiment observed the probabilities models assigned to different continuations after presenting a scope ambiguous sentence versus a non-ambiguous control sentence. A model should assign higher probabilities to different kinds of continuations given the ambiguous sentence compared to the control sentence. These probabilities were used to calculate the ambiguity recognition score (P-score), with positive P-scores indicating the model's recognition of ambiguity.",
            "result_discussion": "All models showed positive mean P-scores, indicating sensitivity to meaning ambiguity. Most models, except GPT-2-small, were statistically significant. Correlation between model-derived P-scores and human proxy scores was significant in models like text-davinci-003 and Llama 2 at 13B, suggesting strong alignment with human judgments. Larger models consistently yielded higher proportions of positive P-scores, evidencing consistent behavior across data points, with text-davinci-003 performing exceptionally well.",
            "ablation_id": "2404.04332v1.No2"
        },
        {
            "research_objective": "Expand Experiment 1A's dataset and re-evaluate model performance on a larger dataset.",
            "experiment_process": "The dataset was expanded by adding manually crafted examples to sparse categories of the original dataset, followed by generating novel datapoints using GPT-4 and filtering through human validation. The resulting dataset contained 837 unique scope-ambiguous sentences. The re-evaluation followed the same procedure as Experiment 1A.",
            "result_discussion": "Patterns observed in Experiment 1A held for the expanded dataset. High-performing models like GPT-4, text-davinci-003, and Llama2-70b continued to achieve high accuracy, with GPT-4 reaching 96%. These models also showed a performance drop in the control setting, reinforcing the validity of the original experiment's outcomes.",
            "ablation_id": "2404.04332v1.No3"
        },
        {
            "research_objective": "Expand Experiment 2A's dataset and re-assess model sensitivity to scope ambiguity.",
            "experiment_process": "Similar to Experiment 1B, the dataset was expanded and re-evaluated using the same methodology as Experiment 2A. The expanded dataset consisted of 110 unique datapoints, each including a scope-ambiguous sentence, control sentence, and two follow-ups supporting different scope readings.",
            "result_discussion": "Positive mean P-scores persisted across all models. Although text-davinci-003's correlation with human data decreased slightly (R-value of approx. 0.48), it remained the highest. Most models showed consistent behavior, with text-davinci-003 and Llama2-13b yielding over 90% positive P-scores. GPT-2 models performed better on the expanded dataset, possibly due to clearer grammatical contrasts. Llama 2 chat models outperformed their vanilla counterparts in correlation and mean scores.",
            "ablation_id": "2404.04332v1.No4"
        }
    ]
}