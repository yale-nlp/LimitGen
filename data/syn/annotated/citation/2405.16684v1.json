{
    "title": "gzip Predicts Data-dependent Scaling Laws",
    "abstract": "Past work has established scaling laws that predict the performance of a neural language model (LM) as a function of its parameter count and the number of tokens it\u2019s trained on, enabling optimal allocation of a fixed compute budget. Are these scaling laws agnostic to training data as some prior work suggests?\nWe generate training datasets of varying complexities by modulating the syntactic properties of a PCFG, finding that 1) scaling laws are sensitive to differences in data complexity and that 2) gzip, a compression algorithm, is an effective predictor of how data complexity impacts scaling properties. We propose a new data-dependent scaling law for LM\u2019s that accounts for the training data\u2019s gzip-compressibility; its compute-optimal frontier increases in dataset size preference (over parameter count preference) as training data becomes harder to compress.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "A neural network\u2019s performance generally increases as more compute is allocated for training. When scaling compute, one must decide whether to increase a model\u2019s parameter count or increase the dataset\u2019s size\u2014these must trade-off within a fixed compute budget. Scaling laws can tell us what specific allocation (i.e. parameters v.s. data) will maximize performance given a compute budget. Much work has explored scaling laws for neural LM\u2019s [Kaplan et al., 2020  ###reference_b26###], generally concluding that parameter & training token count should be scaled 1-to-1 [Hoffmann et al., 2022  ###reference_b20###].\nHowever, most prior work on scaling laws for LM\u2019s have been estimated from transformers trained on scraped web text. Of course, this is quite a specific data distribution, so we may naturally ask whether the scaling laws extrapolated from such web text datasets generalize to other distributions. Furthermore, it is generally understood that the art of training data mixture is the \u2018secret sauce\u2019 that enables frontier industry labs to continually deliver state-of-the-art LLMs [Penedo et al., 2024  ###reference_b34###, 2023  ###reference_b33###, Xie et al., 2024  ###reference_b42###].\nConsidering that improving data quality can significantly raise LM performance [Gunasekar et al., 2023  ###reference_b17###] and that scaling laws in reinforcement learning have been shown to scale with game difficulty [Jones, 2021  ###reference_b24###], we may hypothesize that current LM scaling laws (e.g. Chinchilla [Hoffmann et al., 2022  ###reference_b20###]) are individual web-text-specific cases of a more general scaling law conditioned on properties of the training data.\nThen, what properties of a training dataset of token sequences are neural scaling laws sensitive to? In other words, what can we measure about our data to more accurately predict the optimal compute allocation for training? Furthermore, is data-dependence of scaling laws only of theoretical interest, or can laws be considerably different for real-world datasets?\nIn order to study these questions, we seek a textual data setting where we can intuitively control its complexity as well as open avenues for information-theoretic understandings of why scaling laws are data-dependent. We therefore settle on probabilistic context-free grammars (PCFG) [Chomsky, 1956  ###reference_b9###] which are relatively naturalistic (can model natural language, code, etc.), controllable in syntactic complexity, and follow some well-understood information-theoretic principles [Chi, 1999  ###reference_b8###].\nWe generate 6 datasets of varying complexity by modulating syntactic properties of a PCFG. For each of these datasets, we train LMs of 6 different sizes (4.4M to 1.4B parameters) and record results at 6 different train step counts (100K to 100M tokens). We then fit a scaling law for each dataset [Muennighoff et al., 2023  ###reference_b31###], finding meaningful shifts in the law\u2019s parameters as syntactic complexity increases. Following prior work on entropy of formal grammars [Arora et al., 2022  ###reference_b2###], we use median compressibility of each token sequence in a dataset as a measure of complexity that is straightforward to compute with gzip [Gailly and Adler, 1992  ###reference_b15###].\nWe find that as the training data becomes less compressible (more complex), the scaling law\u2019s compute-optimal frontier gradually increases its preference for dataset size over parameter count. We then measure the compressibility of real-world code & natural language datasets, showing that the former is considerably more compressible and thus subject to a predictably different scaling law. With some napkin math, we estimate that we could reach the performance of StarCoder [Li et al., 2023  ###reference_b27###] with 24% fewer FLOPs ($278,000 in H100 hours) since our compute-optimal scaling adjusts for data complexity."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Scaling Laws",
            "text": "Early work established that a neural network\u2019s test error is a power law of training dataset size [Cortes et al., 1993  ###reference_b11###], model parameter count [Rosenfeld et al., 2019  ###reference_b37###], and that the relationship holds over many orders of magnitude [Hestness et al., 2017  ###reference_b19###]. Kaplan et al. [2020  ###reference_b26###] applied scaling laws to transformer-based language models and identified a compute-optimal frontier along which parameter & dataset size should be scaled. Hoffmann et al. [2022  ###reference_b20###] propose the Chinchilla scaling laws, finding that Kaplan et al. [2020  ###reference_b26###] and Rae et al. [2021  ###reference_b36###] overparameterized their models and that the compute-optimal frontier requires parameter & dataset size to be scaled equally (rather than parameters scaling at 3x the rate of data). Sorscher et al. [2022  ###reference_b41###] find that we can reach exponential (rather than power law) scaling on dataset size by pruning out redundant examples that do not provide much information to learn from. Liu and Tegmark [2023  ###reference_b28###] identify a mechanistic explanation of why scaling follows a power law on model width. Aghajanyan et al. [2023  ###reference_b1###] extend the Chinchilla scaling laws to a variety of modalities such as speech, image-text, and code. Caballero et al. [2023  ###reference_b6###] propose a novel functional form for scaling laws that better models complex non-monotonic behavior and also apply it to several modalities including code. Both these works and Hoffmann et al. [2022  ###reference_b20###] (in Appendix C) find that scaling behavior for code is different than for natural language\u2014code is easier to learn and its compute-optimal frontier prefers parameters slightly over data. Bi et al. [2024  ###reference_b3###] cursorily investigate scaling laws across datasets of different qualities, finding that cleaner & higher quality data results in the \u201cmodel scaling exponent increasing\u201d because \u201chigh-quality data usually implies logical clarity and less predictive difficulty after sufficient training\u201d. However, none of these works identify an underlying general principle that explains why scaling behavior varies across data modalities & complexities (or even just between code and natural language). Meanwhile, Jones [2021  ###reference_b24###] explores scaling laws in the context of board games, finding that \u201cthe compute required for a desired level of performance can be calculated directly from the board size\u201d. If scaling laws smoothly scale with data complexity in the board game setting, do they also smoothly scale with textual data complexity? And how might we measure a text dataset\u2019s complexity?\n\nIn the healthcare domain, artificial intelligence has shown remarkable potential in enhancing diagnostics. Recent studies reveal AI's ability to outperform traditional methods across various medical imaging tasks, achieving higher accuracy and efficiency [Esteva et al., 2017  ###reference_b13###; Rajpurkar et al., 2018  ###reference_b34###]. Moreover, integration of machine learning facilitates early disease detection and personalized treatment strategies [Topol, 2019  ###reference_b42###]."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Syntax & Information Theory",
            "text": "A longstanding body of research has aimed to apply information-theoretic measures to natural language [Shannon, 1951 ###reference_b39###, Cherry et al., 1953 ###reference_b7###, Harris, 1991 ###reference_b18###, Piantadosi et al., 2011 ###reference_b35###]. Specifically, entropy (a fairly abstract information-theoretic measure) can be operationalized in a number of concrete ways to measure the informational complexity of a linguistic distribution [Arora et al., 2022 ###reference_b2###]. One way to determine the \u2018goodness\u2019 of an entropy measure over sequences is whether it grows as the syntactic complexity of a language increases (e.g. more production rules, more non-terminals, longer right-hand sides). While entropy is straightforward to compute in closed-form on simple formal languages (i.e. Type 3 Chomskyan grammars, generated by Finite State Automata) [Grenander, 1967 ###reference_b16###, S\u00e1nchez et al., 2018 ###reference_b38###], for Context-free (Type 2) grammars, we must estimate entropy from a set of generated samples [Chi, 1999 ###reference_b8###, Corazza and Satta, 2007 ###reference_b10###]. Leveraging the widely-recognized relationship between entropy and compression [Shannon, 1948 ###reference_b40###, Huffman, 1952 ###reference_b22###, Ziv and Lempel, 1977 ###reference_b44###], we can use gzip [Gailly and Adler, 1992 ###reference_b15###], a lossless compression utility that implements DEFLATE [Deutsch, 1996 ###reference_b14###] (a combination of Huffman coding & the Lempel-Ziv algorithm) to estimate the entropy of sampled linguistic sequences. Del\u00e9tang et al. [2023 ###reference_b13###] adapt language models as lossless compressors of token sequences and provide new insights on scaling laws from this compression perspective, finding that beyond a critical model size, the compression rate (accounting for parameter count) reverses its improvement. In a somewhat similar vein, Jiang et al. [2023 ###reference_b23###] use gzip with k-Nearest-Neighbors to beat neural text embeddings. Importantly, they both brought to attention the effectiveness of compression algorithms for heuristically measuring the structure learned by language models [Huang et al., 2024 ###reference_b21###]. Another line of recent work investigates the learnability of different languages by LMs, finding that natural languages vary in their learnability [Cotterell et al., 2018 ###reference_b12###] and that LMs struggle to model syntactically \u2018impossible\u2019 languages [Kallini et al., 2024 ###reference_b25###].\n\nAI in healthcare diagnostics has made significant strides, utilizing machine learning algorithms to enhance diagnostic precision and efficiency. Studies have shown AI's potential in identifying patterns in medical imaging and electronic health records, contributing to early disease detection and personalized treatment plans [Jiang et al., 2022 ###reference_b24###, Smith et al., 2023 ###reference_b30###]."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Modulating Data Complexity via Syntactic Properties of a PCFG",
            "text": "Probabilistic Context-Free Grammars (PCFGs) are a fundamental tool in computational linguistics for modeling the syntax of natural languages. A PCFG extends the concept of a standard Context-Free Grammar (CFG) by associating probabilities with its production rules, enabling the representation of language ambiguity and variability in a quantifiable manner. These grammars generate trees where each node represents a syntactic category, and the edges represent production rules applied to generate sentences. When generating sentences from a PCFG, we probabilistically sample sequences of production rules to apply until all leaves of the tree are terminals (actual vocab tokens).\nWe can control the syntactic properties of a PCFG to naturalistically modulate the complexity of a textual dataset that we sample from its generated sentences. Specifically, our PCFG creation function accepts arguments for the number of terminals, number of non-terminals, the maximum length of the right-hand side of a production rule, and the maximum number of production rules allowed for any non-terminal (i.e. if this is 1, a given non-terminal will always lead to the same right-hand side). Intuitively, as each of these values increase, the language\u2019s syntactic complexity also increases.\nTo create a PCFG from the above arguments, for each non-terminal, we randomly choose its number of productions (RHS options), each of those production\u2019s length, instantiate a production rule by randomly sampling from the terminals & non-terminals, and assign it a probability normalized by the total RHS options for the non-terminal. We then collect all the generated production rules for all the non-terminals and instantiate a grammar using the PCFG package [Breydo, 2021  ###reference_b5###] built on NLTK [Bird and Loper, 2004  ###reference_b4###].\nWe can then use the grammar that we have randomly (within given constraints) created to probabilistically sample sentences to construct a dataset of token sequences. To make it easier later to compare training across grammars that produce sentences of different average lengths, we decide to sample sentences into documents of the same token count (our LM\u2019s context length). We sample sentences from our grammar until we\u2019ve filled up the context length, and if we overflow, we simply truncate the sequence.\nSentences are composed of terminals that are just integers and can thus be treated as token IDs for our LM; we concatenate sentences with the unused integer 0, effectively corresponding to a period in natural language. To clarify, we do not generate strings that \u2018look\u2019 like natural language and must be tokenized\u2014the PCFG generates sequences of token IDs themselves. Now, we can generate 6 datasets of token sequences with varying complexities from just 6 sets of initial grammatical constraints."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "gzip-compressibility Measures Syntactic Complexity",
            "text": "Now we must choose a metric to estimate the complexity of our datasets. While for our PCFG-generated data, we could use a trivial function of its syntactic properties (e.g. number of non-terminals plus median production rule length), we would only be able to apply this metric to datasets where we precisely know the grammar that generated the data. Instead, we need a measure of complexity that we can apply to real-world token sequence datasets so that we can see whether our data-sensitive scaling results on PCFGs generalize to real-world datasets. While one might try to use grammar induction to identify the underlying syntax of a real-world dataset and then compute complexity from its syntactic properties, this is computationally intensive and difficult with noisy web-text data.\nTherefore, as explained in Sec. 2.2  ###reference_###, we choose to use a compression algorithm, gzip, to estimate the complexity of a dataset by virtue of considerable theoretical work establishing  entropy [Shannon, 1948  ###reference_b40###] and entropy  syntactic complexity [Chi, 1999  ###reference_b8###]. Specifically, for each token sequence in a sample of 1000 from the dataset, we apply gzip and compute the ratio of the size (in bytes) of the compressed data to the original data. We then compute the median and standard deviation in compressibility, confirming that grammars with higher syntactic complexity result in datasets that are more difficult to compress.\n###figure_1### In Tab. 1  ###reference_###, we list the syntactic parameters for each grammar and the compression ratio we measured of token sequences sampled from the grammar. Observe that as non-terminals (grammatical categories), terminals (tokens), right-hand side options, and right-hand side length increase, the gzip-compressibility also increases (i.e. it becomes harder to compress). We plot these datasets alongside natural language & code in Fig. 1  ###reference_###, showing how some PCFG datasets are more similar in complexity to code (the ones that are easier to compress) while others are more similar to natural language."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Are Scaling Laws Sensitive to Data Complexity?",
            "text": "To identify the scaling law for a dataset, we train a set of models of varying sizes (4.2M, 8.8M, 20.3M, 59.0M, 275.3M, 1.4B parameters; architectural specifics in Tab. 6  ###reference_###) on varying size subsets of the data (100K, 1M, 5M, 20M, 50M, 100M tokens) and fit a power law on the resulting final losses of all training runs. Most experiments were run on a cluster of 4 Nvidia A100\u2019s with 80 GB VRAM each using PyTorch FSDP [Paszke et al., 2019  ###reference_b32###, Zhao et al., 2023  ###reference_b43###]. We use a batch size of 32, single epoch training, the  optimizer [Loshchilov and Hutter, 2017  ###reference_b30###], and a learning rate starting from  and cosine decayed over the number of train steps [Loshchilov and Hutter, 2016  ###reference_b29###].\nAs intuitively expected, the more compressible a dataset is (lower compressibility ratio), the faster that models regardless of size will converge (Fig. 2  ###reference_###). While this shows that we need more compute to model more complex datasets (which is in and of itself notable), we need more evidence to determine if the compute-optimal frontier directionally shifts based on data complexity.\nTo establish such a non-trivial sensitivity of scaling laws to data complexity, we need to compute the law for each dataset and examine its fitted parameters.\n###figure_2### ###figure_3### ###figure_4### ###figure_5### ###figure_6### ###figure_7###"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Computing Data-sensitive Scaling Laws from gzip-compressibility",
            "text": "The scaling law functional form proposed by Hoffmann et al. [2022  ###reference_b20###] predicts training loss as a function of model & dataset size:\nwhere  is the model\u2019s parameter count and  is the training dataset\u2019s token count. They claim that  captures the \u201centropy of natural text\u201d (their Sec. 3.3) and that scaling laws are \u201cindependent of dataset\u201d (their App. C). However, when we fit this function on the training results for each of our PCFG datasets (leveraging the helpful open-source implementation of Muennighoff et al. [2023  ###reference_b31###]), we find considerably different laws for each dataset (Tab. 2  ###reference_###).\nThe scaling law induces a compute-optimal frontier for parameter count derived by Kaplan et al. [2020  ###reference_b26###] & Hoffmann et al. [2022  ###reference_b20###] and simplifiable as:\nwhere  is a compute budget in FLOPs. We plot this compute-optimal frontier for Chinchilla as well as our fitted law for each PCFG dataset in Fig. 3  ###reference_###. From a cursory glance, we can see that the frontier of our fitted law progressively becomes more data-preferent as data becomes harder to compress, crossing over Chinchilla\u2019s 1-to-1 frontier at some point .\n###figure_8### ###figure_9### ###figure_10### ###figure_11### In order to predict scaling law parameters from a dataset\u2019s compressibility, let us fit a simple linear regression on the fitted scaling law parameters for each dataset. Recall from Sec. 3.1  ###reference_### that we compute the compressibility  of a dataset  by taking the average ratio of compressed bits to original bits for each element :\nSo once we\u2019ve fit lines to predict each parameter (E, A, B, , ) from , we can re-define each parameter as a function of compressibility:\nwhere  and  are just our fitted linear regressions\u2019 parameters. These fitted values (along with the regressions\u2019 -values) are presented in Tab. 3  ###reference_### and the linear regressions are visualized in Fig. 4  ###reference_###. They\u2019re all pretty much monotonically decreasing at different rates with an interesting - intercept at . It\u2019s also interesting to note that , originally set as the constant \u2018entropy of natural language\u2019, is the only parameter that loosely increases with  (though not significantly)\u2014our proxy for complexity or entropy.\n###figure_12### ###figure_13### Now, we can reparameterize Eq. 1  ###reference_### as a function of compressibility :\nHowever, since our experiments are considerably smaller scale and primarily on PCFG data, we also present a form of our data-dependent scaling law as an adjustment of Chinchilla, where  is the weightage of our adjustment for the gzip-compressibility of the training data (and prime (\u2032) parameters are the Chinchilla constants)."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Eliminating Syntactic Parameters as a Confounder of Compressibility",
            "text": "Given just the above experiments, we have not addressed the possibility that our compressibility measure is confounded by some underlying syntactic property (e.g. vocab size). To address this concern, in Fig. 5  ###reference_### we provide results showing that when holding vocab size steady and changing other syntactic properties (Tab. 4  ###reference_###), gzip-compressibility still predicts scaling law parameter shifts (with an even stronger correlation than in the increasing vocab size setting).\n###figure_14### ###figure_15### We also empirically show the contrapositive in Fig. 6  ###reference_###, demonstrating that when we widely vary syntactic properties (Tab. 5  ###reference_###) but such that the datasets\u2019 final gzip-compressibility are all the same, there is no significant shift in scaling law parameters.\n###figure_16### ###figure_17### Although the intersection behavior observed in Fig. 4  ###reference_### is not observed in the iso-vocab case (Fig. 5  ###reference_###), the steeper negative slope of  v.s.  (and  v.s. ) implies the same phenomenon of increased data-preference as gzip-compressibility grows.\nThus we have shown that scaling laws are dependent on training data and gzip-compressibility is a strong predictor of how data complexity will impact scaling properties. We provide fitted coefficients (Tab. 3  ###reference_###) for predicting scaling parameters as a function of  (gzip-compressibility), and offer an adjustment-based formulation (Eq. 6  ###reference_###) of the data-dependent scaling law to make our small scale synthetic experiments more adaptable to training frontier models."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "We have shown that the Chinchilla scaling law [Hoffmann et al., 2022  ###reference_b20###] and similar laws that claim to be data-agnostic are in fact web-text-specific instances of a broader family of scaling laws sensitive to information-theoretic measures of training data. Not only do datasets that are difficult to compress (regardless of syntactic specifics) require more compute, they also require a different trade-off between model size & data.\nWhy does a dataset with a certain level of compressibility  result in the specific scaling parameters that it does? And why does increased gzip-compressibility result in increased preference for data over parameter count? These questions warrant further exploration from a theoretical perspective\u2014we hope to leverage connections between syntax & information theory to answer this question in future work.\nOn the empirical side, an especially promising avenue is in LM\u2019s for code since code datasets have significantly lower gzip-compressibility than natural language. Therefore, we can expect the compute-optimal scaling law for code to have stronger parameter-preference than Chinchilla. We are currently running experiments at 1.92e19 FLOPs (some gzip-compressibility analysis in Fig. 8  ###reference_### and preliminary runs in Fig. 9  ###reference_###) to compare performance based on Chinchilla\u2019s allocation and our data-adjusted scaling law\u2019s (Eq. 6  ###reference_###) allocation. As highlighted in Sec. 1  ###reference_###, scaling up this finding could save $278,000 in H100 hours when training a single relatively small (6B parameter) code-generation model.\nBeyond a deeper theoretical understanding of scaling laws & more optimal compute allocation for code-generation LMs, our findings also open possibilities for using compressibility as a metric in training data filtering, curriculum learning ordering, and fine-tuning data requirement estimation. Anecdotally, we have heard from researchers at frontier AI labs that compression algorithms are already used in production for filtering LLM training data.\nThe call-to-action here is not per se to use our data-dependent scaling law to decide how to spend your next million dollars of compute, but rather to begin investigating whether performance discrepancies between models trained on different datasets are explainable by simple information-theoretic measures (e.g. gzip-compressibility) of training data. If so (as experienced by Bi et al. [2024  ###reference_b3###]), it may be worth building systems to dynamically determine compute allocation for each training dataset, with the general rule of thumb that easy-to-compress datasets prefer parameters and hard-to-compress datasets prefer data."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "The work presented here is restricted to the synthetic PCFG setting and does not yet explicitly show generalization to predicting different scaling properties of real-world datasets. Our method of modulating training data complexity by varying syntactic properties of a PCFG, though naturalistic, is fairly specific and may still be prone to confounders. In addition, there are features of training data that affect scaling but appear to not be modeled well by gzip-compressibility. Alternative compression algorithms & information-theoretic measures of training data were not tested though they likely all broadly correlate with gzip-compressibility."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Acknowledgements",
            "text": "We are grateful to Alon Albalak, Rylan Schaeffer, Charlie Snell, Ethan Caballero, Martin Ziqiao Ma, Sankeerth Rao Karingula, Guillaume Lample, Vibhu Sapra, Aryaman Arora, Tristan Thrush, and several anonymous Twitter users for their thorough reviews & feedback on drafts of this paper.\nThis work was also sponsored by generous compute grants from Luke Piette (RunPod), Kye Gomez (Swarms), Vincent Weisser & Johannes Hagemann (Prime Intellect), and of course the rest of my team at Reworkd\u2014Asim Shrestha & Adam Watkins."
        }
    ],
    "url": "http://arxiv.org/html/2405.16684v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "3",
            "3.1"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "1",
            "4",
            "4.1",
            "4.2"
        ]
    },
    "research_context": {
        "paper_id": "2405.16684v1",
        "paper_title": "gzip Predicts Data-dependent Scaling Laws",
        "research_background": "The paper focuses on the interplay between dataset complexity and scaling laws for training large language models (LMs). Here is a structured summary based on the provided sections:\n\n### Motivation\n1. **Performance Scaling**: The performance of neural networks usually improves as more compute is allocated for training. Given a fixed compute budget, deciding whether to increase the model's size (parameters) or the dataset's size (tokens) is crucial for maximizing performance.\n2. **Data Quality**: Improving the quality of training data can significantly enhance LM performance. Prior work on scaling laws has primarily been based on transformers trained on scraped web text, which might not generalize well to other data distributions.\n3. **Data Mixture**: The careful selection and mixture of training data are key factors for achieving state-of-the-art performance in industry settings. Understanding how scaling laws change with data complexity could lead to optimizing compute allocation more effectively.\n\n### Research Problems\n1. **Generalization of Scaling Laws**: Do scaling laws derived from web text generalize to other types of datasets?\n2. **Data Properties and Scaling Laws**: What measurable properties of a training dataset affect neural scaling laws? Specifically, can characteristics of the data inform us about the optimal allocation of compute resources between model parameters and dataset size?\n3. **Practical Relevance**: Is the dependence of scaling laws on data properties merely of theoretical interest, or do they have practical implications for real-world datasets and training performance?\n\n### Relevant Prior Work\n1. **Compute Allocation**: Previous studies (Kaplan et al., 2020; Hoffmann et al., 2022) have explored scaling laws, typically recommending a 1-to-1 scaling of parameters and training tokens.\n2. **Data Mixture and Quality**: The importance of high-quality data and its impact on LM performance have been highlighted in various studies (Gunasekar et al., 2023; Penedo et al., 2024; Xie et al., 2024).\n3. **Control and Complexity**: Formal grammatical frameworks like probabilistic context-free grammars (PCFG) allow for controlling syntactic complexity and have known information-theoretic properties (Chomsky, 1956; Chi, 1999).\n4. **Scaling Laws and Entropy**: Research indicates that the difficulty and entropy of a task-specific dataset can impact scaling laws in reinforcement learning (Jones, 2021), and the entropy of formal grammars has been analyzed in other contexts (Arora et al., 2022).\n5. **Compressibility as Complexity Measure**: The paper uses median compressibility (measured with gzip) as a proxy for dataset complexity (Gailly and Adler, 1992).\n\n### Methodology\n1. **Dataset Generation**: Six datasets of varying complexity were generated by modulating syntactic properties of a PCFG.\n2. **Experimental Setup**: LMs of six different sizes were trained on these datasets, with performance recorded over multiple training steps.\n3. **Complexity Measurement**: Used median compressibility to assess dataset complexity.\n4. **Results**: Demonstrated that less compressible (more complex) data shifted the scaling law's preference towards larger datasets over more parameters.\n\n### Practical Implications\n1. **Real-world Data**: Using the developed methodology, compressibility was measured for real-world datasets, leading to insights that could optimize compute allocation.\n2. **Cost Efficiency**: Estimated significant cost savings and efficiency gains (e.g., reaching the performance of state-of-the-art models with fewer resources) through data complexity-aware scaling.\n\nBy combining theoretical insights with practical experiments, the study aims to refine our understanding of how data properties influence the efficacy of compute resource allocation in training large language models.",
        "methodology": "### Methodology: Gzip Predicts Data-dependent Scaling Laws\n\n#### Overview\nThe methodology revolves around the use of Probabilistic Context-Free Grammars (PCFGs) to predict data-dependent scaling laws for language models. PCFGs are extended versions of standard Context-Free Grammars (CFGs) that incorporate probabilities into production rules, thus enabling the modeling of ambiguities and variabilities in natural languages.\n\n#### Key Components and Innovations\n\n1. **Probabilistic Context-Free Grammars (PCFGs):**\n   - **Extension of CFGs:** PCFGs add probabilistic elements to CFGs by associating probabilistic weights with production rules. This helps in quantifying the syntactic variability in a dataset.\n   - **Tree Generation:** The grammar generates syntactic trees where nodes represent syntactic categories while edges represent production rules. Sentences are probabilistically generated by sampling sequences of production rules from these trees.\n   \n2. **PCFG Creation:**\n   - **Controlled Complexity:** The PCFG creation function is designed to take into account several parameters like the number of terminals, number of non-terminals, the maximum length of the production rule's right-hand side (RHS), and the maximum number of production rules for any non-terminal. By manipulating these parameters, one can control the syntactic complexity of the dataset.\n   - **Random Generation:** For each non-terminal, production rules are randomly created by determining the number of RHS options, RHS length, and the corresponding productions using a random sampling of terminals and non-terminals. Probabilities are normalized for each non-terminal to ensure the sum of all generative possibilities equals one.\n   - **Instantiation:** All generated production rules are collected and instantiated into a grammar using the PCFG package built on top of the NLTK framework.\n\n3. **Dataset Generation:**\n   - **Sentence Sampling:** From the created grammar, sentences are probabilistically sampled to form a dataset of token sequences.\n   - **Context Length Management:** Sentences are sampled until a predefined token count (context length) is reached. If the token count exceeds the context length, truncation is applied to the sequence. This method ensures a uniform context length across different datasets.\n   \n4. **Token Representation:**\n   - **Integer Tokens:** Terminals in the PCFG are represented as integers (token IDs), streamlining the process for language model training. The integers are concatenated with the unused number 0, functioning as a period in natural language, to separate sentences.\n   - **Direct Generation:** Instead of generating strings that mimic natural language, the PCFG directly generates token ID sequences. This bypasses the need for additional tokenization steps.\n\nBy utilizing PCFGs to synthetically generate datasets with varied complexities through a controlled parameter set, the methodology provides a robust framework to study the impact of data complexity on language model performance.\n\n5. **Experimental Setup:**\n   - **Varied Complexity:** Generate six distinct datasets with varying levels of complexity by altering grammatical constraints.\n   - **Subsequent Analysis:** Use these datasets to train and evaluate models, examining how different complexities affect performance and scaling behaviors.\n\n6. **Implementation:** The approach leverages tools like the PCFG package and NLTK to simplify grammar instantiation and manipulation, ensuring the pipeline remains efficient and reproducible.\n\nBy leveraging the inherent flexibility of PCFGs to simulate complex language data, this method provides a robust foundation for studying and predicting the scaling laws pertinent to language model training.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n**Model Sizes and Training:**\n- Models of varying sizes: 4.2M, 8.8M, 20.3M, 59.0M, 275.3M, 1.4B parameters.\n- Training on varying size subsets of the data: 100K, 1M, 5M, 20M, 50M, 100M tokens.\n- Single epoch training with a batch size of 32.\n- Optimizer: [Loshchilov and Hutter, 2017].\n- Learning rate: Starting from a specified value and cosine decayed over the number of train steps [Loshchilov and Hutter, 2016].\n\n**Evaluation Metrics:**\n- The main evaluation metric used is the final loss after training.\n\n**Training Infrastructure:**\n- Experiments primarily conducted on a cluster of 4 Nvidia A100\u2019s with 80 GB VRAM each.\n- Framework used: PyTorch FSDP [Paszke et al., 2019; Zhao et al., 2023].\n\n### Main Experimental Results\n\n- **Compressibility and Convergence:** \n  - More compressible datasets (lower compressibility ratio) lead to faster model convergence regardless of model size. This implies that complex datasets require more compute to model, which is a significant insight.\n- **Scaling Law Sensitivity:** \n  - The sensitivity of scaling laws to data complexity was investigated by calculating the power law for each dataset and examining the fitted parameters, though additional evidence is needed to determine the directional shift of the compute-optimal frontier based on data complexity.\n\nConclusively, compressibility plays a crucial role in determining the rate of convergence, suggesting a correlation between data complexity and computational requirements."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "The objective is to determine if neural scaling laws for language models are sensitive to the complexity of the training data and whether the compression algorithm gzip can effectively predict the impact of data complexity on these scaling laws.",
            "experiment_process": "We generated six datasets of varying complexities by altering the syntactic properties of a probabilistic context-free grammar (PCFG). Each dataset was used to train language models of six different sizes (4.4M to 1.4B parameters) over six different training step counts (100K to 100M tokens). The experimental setup included measuring the final losses for all training runs and fitting a scaling law for each dataset. The median compressibility of each dataset using gzip was calculated to serve as a complexity measure. Training was performed on a cluster of four Nvidia A100 GPUs with 80 GB VRAM each, using PyTorch FSDP, a batch size of 32, single epoch training, the optimizer, and a cosine-decayed learning rate.",
            "result_discussion": "The study found that as training data becomes less compressible (i.e., more complex), the scaling law's compute-optimal frontier increasingly favors larger datasets over more parameters. Data complexity significantly impacts the scaling laws, with real-world datasets showing predictable differences based on their compressibility. For example, real-world code is more compressible compared to natural language, thus subject to different scaling laws. The study provides a formulation for a data-dependent scaling law that adjusts for data complexity, potentially reducing compute costs for training large models.",
            "ablation_id": "2405.16684v1.No1"
        },
        {
            "research_objective": "To determine whether the compressibility measure is confounded by underlying syntactic properties of the datasets, such as vocabulary size.",
            "experiment_process": "Experiments were conducted by holding the vocabulary size constant while changing other syntactic properties and observing the impact on gzip compressibility and scaling law parameters. Additionally, experiments were conducted with varying syntactic properties while keeping the datasets' final gzip-compressibility identical to investigate if scaling law parameters shift without changes in compressibility.",
            "result_discussion": "The results showed that gzip-compressibility still predicts scaling law parameter shifts even when vocabulary size is held constant, implying a stronger correlation in these settings. Additionally, when varying syntactic properties to achieve identical gzip-compressibility across datasets, there was no significant shift in scaling law parameters. This demonstrates that gzip-compressibility is a strong predictor of data complexity's impact on scaling laws, independent of specific syntactic attributes.",
            "ablation_id": "2405.16684v1.No2"
        }
    ]
}