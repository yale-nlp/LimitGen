{
    "title": "Mixture-of-Prompt-Experts for Multi-modal Semantic Understanding",
    "abstract": "Deep multi-modal semantic understanding that goes beyond the mere superficial content relation mining has received increasing attention in the realm of artificial intelligence. The challenges of collecting and annotating high-quality multi-modal data have underscored the significance of few-shot learning. In this paper, we focus on two critical tasks under this context: few-shot multi-modal sarcasm detection (MSD) and multi-modal sentiment analysis (MSA). To address them, we propose Mixture-of-Prompt-Experts with Block-Aware Prompt Fusion (MoPE-BAF), a novel multi-modal soft prompt framework based on the unified vision-language model (VLM). Specifically, we design three soft prompt experts: a text prompt and an image prompt that extract modality-specific features to enrich the single-modal representation and a unified prompt to assist multi-modal interaction.\nAdditionally, we reorganize Transformer layers into several blocks and introduce cross-modal prompt attention between adjacent blocks, which smoothens the transition from single-modal representation to multi-modal fusion. On both MSD and MSA datasets in few-shot settings, our proposed model not only surpasses the 8.2B model InstructBLIP with merely 2% parameters (150M), but also significantly outperforms other widely-used prompt methods on VLMs or task-specific methods.\n\n\n\nKeywords:\u2009multi-modal sarcasm detection, multi-modal sentiment analysis, prompt learning",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "Multi-modal semantic understanding (MSU) is crucial for the development of machines capable of interpreting the complex interplay of textual and visual information. In social media platforms, where the combination of text and imagery can often present conflicting messages or nuanced sentiments that are not immediately apparent from a single modality alone, such understanding is vital for accurately interpreting the intent and sentiment. Among the fields of MSU, Multi-modal Sarcasm Detection (MSD) and Multi-modal Sentiment Analysis (MSA) emerge as two representing tasks. These tasks exemplify the intricate process of aligning and comprehending the relations from different modalities to discern the intended meaning or sentiment. As highlighted by the examples in Table 1, solely reading the text or image of MSD is prone to misinterpreting it as a positive comment while ignoring the sarcasm about the discount being too small.\nRecent approaches for MSU normally exploit a dual-encoder architecture, i.e., use separate pre-trained encoders to extract features for different modalities (e.g., BERT (Devlin et al., 2019  ###reference_b9###) for text and ResNet (He et al., 2016  ###reference_b15###) for image). The features then interact with each other to capture the incongruity and fed into a classification head for the prediction.\nUnder this framework, researchers are dedicated to designing effective methods of interaction, including attention mechanisms Pan et al. (2020  ###reference_b35###); Xu et al. (2020  ###reference_b48###); Han et al. (2021a  ###reference_b13###), graph structures Liang et al. (2021  ###reference_b22###, 2022  ###reference_b23###); Liu et al. (2022a  ###reference_b24###), optimal transport Pramanick et al. (2022  ###reference_b36###) and dynamic routing Tian et al. (2023  ###reference_b42###).\n###figure_1### ###figure_2### Wish you a good Valentines Day.\nAlthough the previous studies have achieved good performance in semantic understanding tasks like MSD or MSA, they mostly rely on sufficient training data. However, collecting a large amount of high-quality multi-modal data, sarcasm especially, is a non-trivial task. According to Misra and Arora (2023  ###reference_b33###), sarcasm expressions require a high level of cognitive ability and rarely appear on social media, making it difficult to collect and annotate. Moreover, most existing models use separate pre-trained encoders to process text and image, which might lead to the misalignment of different modalities and thus hurt modal fusion.\nNowadays, pre-trained on large-scale image-text pairs, the vision-language models (VLMs) achieve good image-text correspondences and can perform well on cross-modal reasoning. Given these two aspects, we propose to address the few-shot MSU tasks by using VLMs.\nTo adapt the pre-trained VLMs to downstream multi-modal tasks on few-shot settings, prompt-based learning is widely applied and has demonstrated promising performance Liu et al. (2023a  ###reference_b25###). Compared to manually designed prompts, continuous prompts (also referred to as soft prompts) are preferred due to their flexibility and scalability  Liu et al. (2021a  ###reference_b26###, b  ###reference_b28###); Han et al. (2021b  ###reference_b14###). However, most previous work only coped with text data.\nIn this paper, we explore different methods of utilizing soft prompts\nto address the few-shot MSU task.\nTwo primary architectures are prevalent in VLMs.\nOne line of work\nencodes images and texts respectively and performs modal fusion by simply computing the similarities between them, like CLIP Radford et al. (2021  ###reference_b37###) and ALIGN Jia et al. (2021  ###reference_b17###). Another line of work adopts a unified network for both single-modal representation and multi-modal fusion, like VilBert Lu et al. (2019  ###reference_b32###) and VLMo Bao et al. (2022  ###reference_b1###). In the first line of work, the soft prompt method has been studied, like CoOp Zhou et al. (2022b  ###reference_b56###), CoCoOp Zhou et al. (2022a  ###reference_b55###), UPT Zang et al. (2022  ###reference_b52###) and MaPLe Khattak et al. (2023  ###reference_b18###).\nNonetheless, there has been little work applying soft prompts in the second line of VLMs yet. Thus, exploring multi-modal prompts in a unified Transformer network remains an open issue.\nIn this paper, towards the deep MSU,\nwe propose a novel multi-modal soft prompt framework MoPE-BAF, Mixture-of-Prompt-Experts with Block-Aware prompt Fusion. Specifically, we devise a set of soft prompts corresponding with different roles, an image prompt expert, a text prompt expert and a unified prompt expert. The first two extract semantic features within a single modality, while the third assists in capturing inter-modality information. Furthermore, we introduce a block-aware prompt fusion mechanism to enhance the connection between different prompt experts. We re-organize the transformer layers into several blocks and apply cross-attention to enable the exchange of prompt expert information between two adjacent blocks. It facilitates deep interactions between modalities and enables smoother transitions from single-modal representation to multi-modal fusion.\nWe conduct a series of experiments on the MSDT dataset (Cai et al., 2019  ###reference_ib1###) in the few-shot setting. Our proposed model significantly outperforms the classical CLIP and the base VLMo.\nMore importantly, our model with only 150M parameters obtains a better performance than InstructBLIP Dai et al. (2023  ###reference_b7###), the most advanced model with 8.2B parameters.\nBesides, experimental results demonstrate a stable performance gain compared to conventional soft prompts, no matter using a LM head or a classification head. Furthermore, we apply our model to the MSA task on the MVSA-S data (Niu et al., 2016  ###reference_ib2###), outperforming the previous state-of-the-art UP-MPF (Yu et al., 2022  ###reference_b51###) by 3.91 F1 points.\nTo sum up, our contributions are:\nWe propose a novel mixture-of-prompt-experts method on the unified VLMs, which drives the pre-trained model to get refined in both single-modal representation and multi-modal fusion.\nWe present a block-aware prompt fusion mechanism, which activates deep interactions between prompt experts and\nbalances the twin objectives of single-modal specialization and multi-modal fusion in VLMs.\nWe conduct experiments on the few-shot multi-modal sarcasm detection and sentiment analysis, outperforming the previous state-of-the-art methods and the advanced large language models."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1.   Multi-modal Sarcasm Detection",
            "text": "Researchers have been exploring different methods to model the incongruity within the image-text pair for the MSD task. At the outset, feature-based approaches are adopted Schifanella et al. (2016  ###reference_b40###); Castro et al. (2019  ###reference_b4###). Subsequently, researchers pay more attention to modality interaction. Cai et al. (2019  ###reference_b3###) leverages a hierarchical strategy to fuse the three modalities of image, attribute and text by attention weights. Sangwan et al. (2020  ###reference_b38###) exploits the interaction among the input modalities using the recurrent neural network. Pan et al. (2020  ###reference_b35###) tries to capture the incongruity by inter-modal attention and co-attention within the text. Xu et al. (2020  ###reference_b48###); Han et al. (2021a  ###reference_b13###) decompose the model into a separation network for discrepancy increment and a relation network for relevance increment. Liang et al. (2021  ###reference_b22###, 2022  ###reference_b23###); Liu et al. (2022a  ###reference_b24###) introduce graph structures for depicting incongruity relations. More recently, Tian et al. (2023  ###reference_b42###) utilizes dynamic paths to activate different routing Transformers. However, these works rely on a large amount of training data to finetune the pre-trained models.\n\nAI Ethics in Autonomous Systems has garnered significant attention with researchers like Awad et al. (2018) examining moral dilemmas in machine decision-making, while Raji et al. (2020) emphasize transparency and accountability in AI systems. Rahwan et al. (2019) discuss the social implications, highlighting the necessity for ethical AI frameworks to govern autonomous system behavior."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2.   Multi-modal Sentiment Analysis",
            "text": "In recent years, MSA has become a popular research topic. Xu et al. (2018  ###reference_b47###) designs a co-memory network based on attention to predict the whole sentiment of text-image pairs. Yang et al. (2021a  ###reference_b49###) proposes a multi-view attention network, which utilizes an attention memory network to extract text and image features, and then fuses multi-modal features through a stacking-pooling module. After that, they incorporated graph neural network in MSA (2021b  ###reference_b50###). Yu et al. (2022  ###reference_b51###) proposes a unified pre-training stage to narrow the semantic gap between different image and text pre-trained models.\n\nAI Ethics in Autonomous Systems has garnered significant attention as it addresses the moral implications of AI decision-making. Researchers like Awad et al. (2020) have explored ethical frameworks for autonomous vehicles, while Etzioni and Etzioni (2017) highlight the necessity for ethical guidelines in AI deployment. The growing discourse emphasizes transparency, accountability, and fairness within AI systems (Binns, 2018)."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "2.3.   Multi-modal Prompt Learning",
            "text": "Recently, researchers adapt VLMs to fit downstream tasks via prompt learning Gu et al. (2023  ###reference_b12###), which can be generally categorized into single-modal prompt and multi-modal prompt methods. In single-modal prompt methods, learnable continuous prompts are appended in front of the text data. For example, CoOp Zhou et al. (2022b  ###reference_b56###) and CoCoOp Zhou et al. (2022a  ###reference_b55###) replace the manual-crafted text prompts used in CLIP with continuous vectors for image classification. Multi-modal prompt learning aims to optimize the text and image inputs simultaneously. UPT Zang et al. (2022  ###reference_b52###) designs a shared initial prompt for CLIP text and visual encoders. MaPLe Khattak et al. (2023  ###reference_b18###) leverages prefix tuning in both modality encoders and designs a coupling function to enable a mutual promotion between prompts. CMPA Liu et al. (2023b  ###reference_b29###) designs prompts for both encoders and employ a cross-modal prompt attention at each layer. These prompt-based methods are mainly applied to the CLIP architecture, which encodes the text and image inputs separately. To the best of our knowledge, there has been little work that applies soft prompts to a unified vision-language pre-trained model.\n\nIn the realm of AI ethics for autonomous systems, scholars focus on embedding ethical guidelines within AI frameworks to ensure safety and fairness. Binns (2023) emphasizes the importance of accountability in autonomous decision-making, while Zhao et al. (2023) explore bias mitigation strategies. These studies highlight the growing need for ethical frameworks in AI technology development."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   Preliminary",
            "text": "Since our work adopts the vision-language pre-trained model VLMo as the backbone, to better illustrate our proposed method,\nwe provide an overview of VLMo as well as the basic knowledge of applying it to MSU tasks.\nThe most straightforward approach is to register a classification head on top of VLMo. After encoding, the vector of the text-start token ([CLS]) is used as the final representation of the image-text pair, and the prediction result is obtained:\nwhere  denote the parameters of the pre-trained model and the classification head respectively.\nAnother way to utilize VLMo is to reformulate the task into a mask language modeling task using a hard template containing [MASK]:\nwhere  denotes the prompt template and  denotes the verbalizer that maps the [MASK] token to the probabilities on label words.\nThe soft prompt method utilizes a set of trainable virtual tokens, which eliminates the need to manually design templates:\nwhere  is\nthe virtual token."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   The Vision-language Pre-trained Model: VLMo",
            "text": "VLMo presents\na Transformer architecture with Mixture-of-Modality-Experts\nwhere the feed-forward neural (FFN) network switches based on the input modality and fusion requirements. As shown in Figure 1  ###reference_###, given an image-text pair, VLMo performs\nthe unified encoding\nin two stages: (1) Employing vision FFN (V-FFN) and language FFN (L-FFN) to encode the respective modality representations at the bottom Transformer layers, and (2) Using vision-language FFN (VL-FFN) to perform multi-modal interaction at the top layers.\nConcretely, in Stage 1, denoting the hidden vision and language representations of the previous layer as  respectively, VLMo first employs shared self-attention across modalities to align their contents, and then pass them to a uni-modal FFN to obtain the output of this layer:\nIn Stage 2, after the self-attention operation, the intermediate outputs are combined and forwarded to a vision-language FFN:"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   Tuning on Multi-modal Tasks",
            "text": "In MSD or MSA task, the input consists of an image  and associated text , with the goal of predicting its corresponding category.\nThe most straightforward approach is to register a classification head on top of VLMo. After encoding, the vector of the text-start token ([CLS]) is used as the final representation of the image-text pair, and the prediction result is obtained:\nwhere  denote the parameters of the pre-trained model and the classification head respectively.\nAnother way to utilize VLMo is to reformulate the task into a mask language modeling task using a hard template containing [MASK]:\nwhere  denotes the prompt template and  denotes the verbalizer that maps the [MASK] token to the probabilities on label words.\nThe soft prompt method utilizes a set of trainable virtual tokens, which eliminates the need to manually design templates:\nwhere  is\nthe virtual token."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   The Proposed Model",
            "text": "###figure_4### ###figure_5### We prepend a single-modal prompt to each modality input and pass them together through\nVLMo Transformer layers.\nwhere  denote vision representation, language representation, V-Prompt and L-prompt, respectively.\nTo ensure the specialization of prompt experts within their corresponding modalities, we restrict their receptive field in the self-attention module, as illustrated in Figure 3  ###reference_###. V-Prompt is dedicated to the image input only, while the image can attend to both V-Prompt and the text input, enjoying the cross-modal alignment and uni-modal enhancement simultaneously. L-Prompt performs in the same way.\nA multi-modal unified prompt expert is\nintroduced to enhance the interaction between modalities:\nwhere  is the representation of VL-Prompt."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1.   Mixture-of-Prompt-Experts",
            "text": "One of the primary challenges in applying soft prompts to multi-modal tasks\nis the specialization of prompt properties across different modalities.\nIn the traditional soft prompt method, all prompts and input tokens are treated equally within Transformer layers, which aligns with the VLMo\u2019s second stage. However, VLMo differentiates image and text inputs with distinct FFNs in the first stage. Thus, only the second stage can be activated by prompts, which inhibits the full exploitation of the multi-modal encoder in extracting modality-specific features.\nIn view of this, we propose a novel multi-modal soft prompt approach, Mixture-of-Prompt-Experts (MoPE), to serve the two encoding stages in VLMo.\nIt contains a set of soft prompts corresponding to the three functional FFNs of VLMo: image expert (V-Prompt), text expert (L-Prompt) and unified cross-modal expert (VL-Prompt), where V-Prompt and L-Prompt assist in extracting the semantic features from the respective modality in Stage 1, and VL-Prompt assists in enhancing inter-modal interaction. Each prompt expert is initialized as a set of trainable vectors with dimension matching the embeddings of the pretrained model. The overall structure of our proposed model is illustrated in Figure 2  ###reference_###.\nWe prepend a single-modal prompt to each modality input and pass them together through\nVLMo Transformer layers.\nwhere  denote vision representation, language representation, V-Prompt and L-prompt, respectively.\nTo ensure the specialization of prompt experts within their corresponding modalities, we restrict their receptive field in the self-attention module, as illustrated in Figure 3  ###reference_###  ###reference_###. V-Prompt is dedicated to the image input only, while the image can attend to both V-Prompt and the text input, enjoying the cross-modal alignment and uni-modal enhancement simultaneously. L-Prompt performs in the same way.\nA multi-modal unified prompt expert is\nintroduced to enhance the interaction between modalities:\nwhere  is the representation of VL-Prompt."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2.   Block-Aware Prompt Fusion",
            "text": "Recall that deep MSU\nnecessitates the ability to discern the complex relationships across image and text modalities. These tasks demand a profound understanding of content relations beyond simple fusion.\nDespite VLMs being pre-trained on large image-text corpora and demonstrating strong performance on traditional multi-modal tasks (e.g., image classification),\ntheir performance on the complex multi-modal task remains subpar due to the superficial fusion process.\nIn VLMo, the allocation of fusion layers is limited (e.g., VLMo-Base-plus assigns only 3 layers),\nconstraining the model\u2019s fusion capacity.\nAccordingly, we propose a new block-aware prompt fusion (BAF) mechanism to make different modality prompts interact deeply and meet the fusion requirements in deep MSU.\nTo be specific, we re-organize the transformer layers\ninto several blocks, and\nintroduce a cross-attention fusion layer between two adjacent blocks. Assuming that each block contains  layers, for the first layer of block  (Layer ), the input prompt\nis reconstructed from the output of the last layer of block  (Layer ):\nwhere  and  are the input for Layer .\nIntuitively, an efficient prompt fusion should introduce knowledge from another modality while retaining the original specialization. We manage it through controlling the number of blocks in BAF. With an appropriate block number, the representations of different modality get fused gradually as the blocks progress, facilitating a seamless transition between two stages\nand striking a balance between single-modal specialization and multi-modal fusion."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Experimental Setup",
            "text": "For image-text MSD, the MSDT dataset (Cai et al., 2019  ###reference_ib1###) stands as the sole benchmark dataset currently available. MSDT has 29k/2.4k/2.4k sample pairs for train/validation/test, each of which contains an image-text pair with a binary label {sarcasm, nonsarcasm}. We keep the test set unchanged and randomly select 32 samples from the train/validation set to construct our few-shot dataset. To balance the label distribution, we control the samples for each label to account for half of the total number. Following the previous work, we adopt Accuracy and F1 as our evaluation metrics. To improve measuring robustness, we sample three disjoint datasets and report the mean result on them. The statistics of MSDT dataset is shown in Table 2  ###reference_###.\nOur experiments for MSA are based on the MVSA-S (Niu et al., 2016  ###reference_ib2###) dataset. Each sample in MVSA-S contains an image-text pair annotated with one of the labels: {positive, neutral, negative}.\nFor few-shot MSA, Yu et al. (2022  ###reference_b51###) performed random sampling on both training and development sets from MVSA-S, constituting 1% of the total. We keep the same setting with them for fair comparison and use Accuracy, Weighted-F1, and Macro-F1 as metrics."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "5.1.   Dataset and Evaluation Metrics",
            "text": "We conduct experiments on two representative MSU\ntasks: multi-modal sarcasm detection and multi-modal sentiment analysis.\nFor image-text MSD, the MSDT dataset (Cai et al., 2019  ###reference_ib1###  ###reference_ib1###) stands as the sole benchmark dataset currently available. MSDT has 29k/2.4k/2.4k sample pairs for train/validation/test, each of which contains an image-text pair with a binary label {sarcasm, nonsarcasm}. We keep the test set unchanged and randomly select 32 samples from the train/validation set to construct our few-shot dataset. To balance the label distribution, we control the samples for each label to account for half of the total number. Following the previous work, we adopt Accuracy and F1 as our evaluation metrics. To improve measuring robustness, we sample three disjoint datasets and report the mean result on them. The statistics of MSDT dataset is shown in Table 2  ###reference_###  ###reference_###.\nOur experiments for MSA are based on the MVSA-S (Niu et al., 2016  ###reference_ib2###  ###reference_ib2###) dataset. Each sample in MVSA-S contains an image-text pair annotated with one of the labels: {positive, neutral, negative}.\nFor few-shot MSA, Yu et al. (2022  ###reference_b51###  ###reference_b51###) performed random sampling on both training and development sets from MVSA-S, constituting 1% of the total. We keep the same setting with them for fair comparison and use Accuracy, Weighted-F1, and Macro-F1 as metrics."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "5.2.   Implementation Details",
            "text": "We choose VLMo-Base-plus as our baseline model. For data pre-processing, we follow the same steps as VLMo. The images are resized to  resolution and segmented into  patches with RandAugment (Cubuk et al., 2020  ###reference_b6###). We utilize the tokenizer from the uncased version of BERT, limiting the input text to a length of 40 and truncating any exceeding portions. For hyper-parameters, the block number is set to 2, and the prompt length is set to 10 by default. All experiments adhere to the full parameter training paradigm. During training, the model is optimized by AdamW (Loshchilov and Hutter, 2019  ###reference_b31###) with . The peak learning rate is . Weight decay is 0.01. In the 32-shot setting, the batch size is 8, and we use linear warmup over the first 10% of the whole 200 steps.\nWe use 1 Nvidia GeForce 3080Ti card for experiments. One training needs around 8GB GPU memory and takes about 2 hours.\n1.Text:<text> Answer the question: Is this image-text pair sarcastic or nonsarcastic? Answer:\n2.Text:<text> Based on the image and text, answer the question: Is this image-text pair sarcastic or nonsarcastic? Answer: \n3.Text: <text> Combining the text, is this sarcastic or nonsarcastic? Answer:\n1.Text:<text> Answer the question: Which sentiment does this image-text pair contain, negative, neutral or positive? Answer:\n2.Text:<text> Based on the image and text, answer the question: Which sentiment does this image-text pair contain, negative, neutral or positive? Answer:\n3.Text:<text> Combining the text, which sentiment does this contain, negative, neutral or positive? Answer:"
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "5.3.   Comparing Methods",
            "text": "We conduct a comprehensive evaluation by comparing with the vanilla multi-modal pre-trained methods, the methods specialized for MSD or MSA tasks, and the prompt methods applied to VLMs."
        },
        {
            "section_id": "5.3.x",
            "parent_section_id": "5.3",
            "section_name": "Multi-modal Pre-trained methods",
            "text": "CLIP (Radford et al., 2021  ###reference_b37###) holds a dual-encoder architecture, designed to encode texts and images separately.\nWe further add trainable projection layers at the output of both encoders and perform classification on their concatenated encoding results. InstructBLIP (Dai et al., 2023  ###reference_b7###) is the latest and most advanced general-purpose VLM that introduces instruction tuning techniques to extract informative features tailored to the given instruction. We adopt the Vicuna7B as the base and design three textual prompts for MSD and MSA as the instruction (shown in Table 3  ###reference_###) and report their average performance. We use 1 Nvidia A40 card for training InstructBLIP and it takes around 24GB GPU memory."
        },
        {
            "section_id": "5.3.x",
            "parent_section_id": "5.3",
            "section_name": "Multi-modal Prompt Learning Methods",
            "text": "We also consider the multi-modal prompt methods based on CLIP as our baselines. CoOp (Zhou et al., 2022b  ###reference_b56###) replaces the manual-crafted text prompts in CLIP with continuous vectors. MaPLe (Khattak et al., 2023  ###reference_b18###) leverages prefix tuning in both modality encoders and designs a coupling function to enable prompt interactions. CMPA (Liu et al., 2023b  ###reference_b29###) designs prompts for both encoders and employs a cross-modal prompt attention at each layer."
        },
        {
            "section_id": "5.3.x",
            "parent_section_id": "5.3",
            "section_name": "Multi-modal Sarcasm Detection methods",
            "text": "For the MSD task, previous work has only considered a full-shot experimental setup. Based on our few-shot dataset, we replicate some of the studies for which the source code is available.\nHFM (Cai et al., 2019  ###reference_b3###) leverages a hierarchical strategy to fuse the image, attribute and text modalities by attention weights. ResBERT (Pan et al., 2020  ###reference_b35###) captures the cross-modal incongruity by inter-modal attention and contradiction within the text by co-attention. HKE (Liu et al., 2022a  ###reference_b24###) learns composition-level congruity based on graph neural networks and introduces auto-generated captions as external knowledge."
        },
        {
            "section_id": "5.3.x",
            "parent_section_id": "5.3",
            "section_name": "Multi-modal Sentiment Analysis methods",
            "text": "MVAN Yang et al. (2021a  ###reference_b49###) utilizes a multi-view memory network to extract single-modal emotion features and interactively capture the cross-view dependencies between the image and text. MGNNS Yang et al. (2021b  ###reference_b50###) learns multi-modal representations by a multi-channel graph neural network based on the global characteristics of MVSA.\nUP-MPF Yu et al. (2022  ###reference_b51###) adopts the multi-modal prompt-based finetuning paradigm and proposes a pre-training stage to narrow the semantic gap between image and text encoders."
        },
        {
            "section_id": "5.3.x",
            "parent_section_id": "5.3",
            "section_name": "Different Prompting Learning Strategies",
            "text": "Manual Prompt defines a hard template combined with input. The representation of [MASK] is then fed into the verbalizer for prediction. Soft Prompt prepends several virtual tokens before the input and utilizes [CLS] token for classification.\nP-Tuning Liu et al. (2021b  ###reference_b28###) combines soft prompt with manual prompt. Table 3  ###reference_### lists the templates used in these methods. P-Tuning v2 Liu et al. (2021a  ###reference_b26###) adopts the prefix-tuning idea and expands the prompt parameter space to each transformer layer, using [CLS] token for classification."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Results and Analysis",
            "text": "Generally, a longer prompt correlates with an increase in learnable parameters, while in the few-shot setting, it may exacerbate the over-fitting problem. We vary the prompt length from 5 to 50 in MoPE, and the results concerning different lengths are visualized in Figure 4  ###reference_### (a). Overall, the impact of prompt length on model performance shows a trend of initially increasing, subsequently diminishing, and then stabilizing. Our explanation is that prompts too short are insufficient to bring about qualitative changes to the model, while too long may pose challenges for searching for the optimal solution due to the complexity of the expanded search space.\nWe investigate the impact of the block number in the BAF module. Specifically, in the first 21 layers of VLMo that use V-Prompt and L-Prompt experts, we divide them into 2-7 blocks. A configuration with 1 block implies no prompt fusion, and is therefore not displayed here. If the number of model layers cannot be evenly divided by the block number,\nthe excess layers are allocated to the bottom blocks. This ensures that the maximum layer difference\nbetween any two blocks does not exceed 1. The results are shown in Figure 4  ###reference_### (b).\nWhen the block number is between 3 and 5, the overall performance remains\nalmost similar. However, a noticeable decline can be observed after 6 blocks. We speculate that too many cross-modal cross-attention operations between blocks cause their original specialization to be discarded, thereby contradicting the function of MoPE. An appropriate number of blocks can strike a balance between\nthe specialization and different modalities interaction.\nThe number of training shots plays a crucial role in model performance, particularly in the few-shot setting. We conduct experiments to train the VLMo base and our full model with different numbers of samples. The results are presented in Figure 4  ###reference_### (c). We find that the performance of both models improves as training shots increase. Besides, applying MoPE-BAF consistently improves the performance of VLMo with the number of training examples from 4 to 64 in the f1 score, which demonstrates the robustness and efficiency of our method."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "6.1.   Evaluation on Multi-modal Sarcasm Detection",
            "text": "We conduct extensive experiments on the dataset MSDT, and report the results in Table 4  ###reference_###. We implement two settings for our method. The first employs a classification head using [CLS] for prediction, termed MoPE-BAF. The second uses an additional manual template containing [MASK], termed MoPE-BAF + Manual Prompt (our full model).\nCompared to previous methods that target full-shot MSD, our model MoPE-BAF achieves significant improvements over HFM and resBERT, and surpasses HKE by a margin of 4.56 points on Accuracy, which leverages external caption knowledge.\nAmong the current advanced multi-modal models, our method achieves promising results. Compared with the backbone model VLMo, we realize 5.14 and 7.36 points improvement on Accuracy and F1, respectively. Compared with CLIP, our methods attain consistent improvements across all four metrics. In addition, our model with only 150m parameters surpasses the large model InstructBLIP with 8.2B parameters, by 4.76 points increment on Accuracy. This efficiency highlights the superiority of our methods and suggests our potential for practical resource-saving applications.\nIn comparison to the multi-modal prompt learning methods CoOp, MaPLE and CMPA that implemented on CLIP, MoPE-BAF showcases outstanding performance despite being built on a less powerful backbone model VLMo. Besides, under the scope of prompt methods on VLMo, our full model achieves the best results, whether combined with a classification head or a LM head with the verbalizer. We speculate that our method distinguishes prompts for different modalities and facilitates the interaction between them as the layers deepen, which enhances the specialization of single-modal representation and guarantees a thorough modality fusion. Thus, we obtain significant improvements compared with the non-specific prompt methods P-Tuning and P-Tuning v2, even though P-Tuning v2 requires a larger prompt parameter space for prepending soft prompts in each Transformer layer."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "6.2.   Evaluation on Multi-modal Sentiment Analysis",
            "text": "###figure_6### For the MSA task, the experimental results on MVSA-S dataset are shown in Table 5  ###reference_###.\nThere is a huge performance gap between the non-pretrained methods (MVAN, MGNNS) and pre-trained methods.\nAmong the pre-trained multi-modal methods, InstructBLIP demonstrates impressive efficacy, outperforming the current state-of-the-art UP-MPF. VLMo series perform better than CLIP series in this task, perhaps because the unified structure of VLMo, which effectively leverages both text and image input, is well-suited to this type of task, and the abundant pre-training tasks (especially mask language modeling), endows it with the ability to infer sentiment.\nVLMo performs well in the zero-shot setting, establishing a robust baseline. When integrated with prompts, the performance of VLMs is significantly boosted. VLMo adopting prompt-based finetuning (+MP) delivers the most superior results compared to those without VLMo, even outperforming the large-scale language model InstructBLIP. After combining with our MoPE-BAF method, the performance is further elevated, surpassing UP-MPF significantly by 3.91 F1 points. It demonstrates the generality and validity of our proposed model in different MSU tasks."
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "6.3.   Ablation Study",
            "text": "We conduct ablation experiments to investigate the effects of MoPE and BAF,\nand the results are presented in Table 6  ###reference_###.\nIn both settings (with or without using Manual Prompt), MoPE significantly improves the performance of VLMo, and further adding the prompt fusion operation achieves better performance.\nPlease note that we cannot validate the effect of BAF separately since it cannot exist independently of MoPE."
        },
        {
            "section_id": "6.4",
            "parent_section_id": "6",
            "section_name": "6.4.   Model Analysis",
            "text": "We also conduct experiments to analyze some controllable factors in the proposed MoPE-BAF model.\nGenerally, a longer prompt correlates with an increase in learnable parameters, while in the few-shot setting, it may exacerbate the over-fitting problem. We vary the prompt length from 5 to 50 in MoPE, and the results concerning different lengths are visualized in Figure 4  ###reference_###  ###reference_### (a). Overall, the impact of prompt length on model performance shows a trend of initially increasing, subsequently diminishing, and then stabilizing. Our explanation is that prompts too short are insufficient to bring about qualitative changes to the model, while too long may pose challenges for searching for the optimal solution due to the complexity of the expanded search space.\nWe investigate the impact of the block number in the BAF module. Specifically, in the first 21 layers of VLMo that use V-Prompt and L-Prompt experts, we divide them into 2-7 blocks. A configuration with 1 block implies no prompt fusion, and is therefore not displayed here. If the number of model layers cannot be evenly divided by the block number,\nthe excess layers are allocated to the bottom blocks. This ensures that the maximum layer difference\nbetween any two blocks does not exceed 1. The results are shown in Figure 4  ###reference_###  ###reference_### (b).\nWhen the block number is between 3 and 5, the overall performance remains\nalmost similar. However, a noticeable decline can be observed after 6 blocks. We speculate that too many cross-modal cross-attention operations between blocks cause their original specialization to be discarded, thereby contradicting the function of MoPE. An appropriate number of blocks can strike a balance between\nthe specialization and different modalities interaction.\nThe number of training shots plays a crucial role in model performance, particularly in the few-shot setting. We conduct experiments to train the VLMo base and our full model with different numbers of samples. The results are presented in Figure 4  ###reference_###  ###reference_### (c). We find that the performance of both models improves as training shots increase. Besides, applying MoPE-BAF consistently improves the performance of VLMo with the number of training examples from 4 to 64 in the f1 score, which demonstrates the robustness and efficiency of our method."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7.   Conclusion",
            "text": "In this paper, we present MoPE-BAF, a new multi-modal soft prompt framework catering to unified VLMs for few-shot multi-modal tasks. Specifically, we devise two\nprompt experts to serve the text and image modality separately\nwith a better specialization ability, and further activate the interactions of prompt experts by inserting cross-modal prompt attention between adjacent Transformer blocks.\nIn this way, we reach a harmonious balance in modality specialization and fusion, thus fulfilling the requirement of modeling deep relations between modalities.\nExperiments on multi-modal sarcasm detection and multi-modal sentiment analysis show that our MoPE-BAF model\nnot only surpasses other widely-used prompt methods, but also outperforms the advanced large language models.\nFurther analysis confirms the effectiveness of MoPE and BAF.\nIn the future, we intend to incorporate task-related external knowledge into our prompt design, and broaden the scope of our method to include other tasks, such as multi-modal content generation and multi-modal reasoning."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "8.   Limitations",
            "text": "While our study provides insights into the soft prompt technique on VLMs, it has to be acknowledged that it has some limitations. First, we observe a sensitivity of MoPE during training. The performance of MoPE relies on appropriate hyperparameter selection and the optimal hyperparameters differ across downstream tasks. While performing perform a grid search on hyperparameters may mitigate the issue, we believe that how to effectively control the sensitivity is worth further exploration in future work.\nBesides, although MoPE-BAF can theoretically be applied to any unified VLMs without disrupting the architecture or encoding process of the base architecture, our study was conducted exclusively on VLMo, and we have not yet extended MoPE-BAF to other pre-trained VLMs. This restricts the generalizability analysis of our methods. In the future, we would apply MoPE-BAF on more VLMs, which we think would provide a more comprehensive understanding of the role and impact of our methods."
        }
    ],
    "appendix": [],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S1.T1\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S1.T1.2\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S1.T1.2.3.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S1.T1.2.3.1.1\"><span class=\"ltx_text\" id=\"S1.T1.2.3.1.1.1\" style=\"font-size:70%;\">Task</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S1.T1.2.3.1.2\"><span class=\"ltx_text\" id=\"S1.T1.2.3.1.2.1\" style=\"font-size:70%;\">MSD</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S1.T1.2.3.1.3\"><span class=\"ltx_text\" id=\"S1.T1.2.3.1.3.1\" style=\"font-size:70%;\">MSA</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S1.T1.2.2.3\"><span class=\"ltx_text\" id=\"S1.T1.2.2.3.1\" style=\"font-size:70%;\">Image</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S1.T1.1.1.1\"><span class=\"ltx_inline-para ltx_minipage ltx_align_middle\" id=\"S1.T1.1.1.1.1\" style=\"width:54.6pt;\">\n<span class=\"ltx_para\" id=\"S1.T1.1.1.1.1.p1\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_square\" height=\"538\" id=\"S1.T1.1.1.1.1.p1.g1\" src=\"extracted/5491506/figures/msd-example-remake.jpg\" width=\"538\"/>\n</span></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S1.T1.2.2.2\"><span class=\"ltx_inline-para ltx_minipage ltx_align_middle\" id=\"S1.T1.2.2.2.1\" style=\"width:54.6pt;\">\n<span class=\"ltx_para\" id=\"S1.T1.2.2.2.1.p1\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_square\" height=\"538\" id=\"S1.T1.2.2.2.1.p1.g1\" src=\"extracted/5491506/figures/mvsa-example.png\" width=\"538\"/>\n</span></span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S1.T1.2.4.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" id=\"S1.T1.2.4.1.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S1.T1.2.4.1.1.1\" style=\"font-size:70%;\">Text</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" id=\"S1.T1.2.4.1.2\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S1.T1.2.4.1.2.1\" style=\"font-size:70%;\">Great Sale!</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S1.T1.2.4.1.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S1.T1.2.4.1.3.1\"><span class=\"ltx_text\" id=\"S1.T1.2.4.1.3.1.1\" style=\"font-size:70%;\">Wish you a good Valentines Day.</span></p>\n<span class=\"ltx_text\" id=\"S1.T1.2.4.1.3.2\" style=\"font-size:70%;\"></span>\n</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:70%;\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S1.T1.10.1.1\" style=\"font-size:129%;\">Table 1</span>: </span><span class=\"ltx_text\" id=\"S1.T1.11.2\" style=\"font-size:129%;\">Examples of MSD and MSA task. The left example displays <span class=\"ltx_text ltx_font_italic\" id=\"S1.T1.11.2.1\">sarcasm</span>, as the text \u201cgreat sale\u201d contradicts with the image depicting a mere 1-dollar discount. The right example conveys a positive wishing attitude, thus categorized as <span class=\"ltx_text ltx_font_italic\" id=\"S1.T1.11.2.2\">positive</span>.</span></figcaption>\n</figure>",
            "capture": "Table 1: Examples of MSD and MSA task. The left example displays sarcasm, as the text \u201cgreat sale\u201d contradicts with the image depicting a mere 1-dollar discount. The right example conveys a positive wishing attitude, thus categorized as positive."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T2\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S5.T2.2\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T2.2.1.1\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S5.T2.2.1.1.1\" rowspan=\"2\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"2\" id=\"S5.T2.2.1.1.2\">Full Split</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"2\" id=\"S5.T2.2.1.1.3\">Few-shot Split</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T2.2.1.1.4\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S5.T2.2.1.1.4.1\">Avg. length</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.2.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T2.2.2.2.1\">#Sarcasm</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\" id=\"S5.T2.2.2.2.2\">#Nonsarcasm</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T2.2.2.2.3\">#Sarcasm</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\" id=\"S5.T2.2.2.2.4\">#Nonsarcasm</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T2.2.3.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T2.2.3.1.1\">Train</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.2.3.1.2\">8642</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T2.2.3.1.3\">11174</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.2.3.1.4\">16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T2.2.3.1.5\">16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.2.3.1.6\">21.85</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.2.4.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T2.2.4.2.1\">Dev</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.2.4.2.2\">959</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T2.2.4.2.3\">1451</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.2.4.2.4\">16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T2.2.4.2.5\">16</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.2.4.2.6\">21.79</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.2.5.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S5.T2.2.5.3.1\">Test</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T2.2.5.3.2\">959</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S5.T2.2.5.3.3\">1450</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T2.2.5.3.4\">959</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S5.T2.2.5.3.5\">1450</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T2.2.5.3.6\">22.22</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S5.T2.3.1.1\" style=\"font-size:90%;\">Table 2</span>: </span><span class=\"ltx_text\" id=\"S5.T2.4.2\" style=\"font-size:90%;\">Statistics of MSDT dataset, with Avg. length calculated using Bert Tokenizer.</span></figcaption>\n</figure>",
            "capture": "Table 2: Statistics of MSDT dataset, with Avg. length calculated using Bert Tokenizer."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T3\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S5.T3.2\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T3.2.1.1\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt\" id=\"S5.T3.2.1.1.1\"></th>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S5.T3.2.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.2.1.1.2.1\" style=\"font-size:70%;\">MSD</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S5.T3.2.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.2.1.1.3.1\" style=\"font-size:70%;\">MSA</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.2.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S5.T3.2.2.2.1\"><span class=\"ltx_text\" id=\"S5.T3.2.2.2.1.1\" style=\"font-size:70%;\">Manual Prompt</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T3.2.2.2.2\"><span class=\"ltx_text\" id=\"S5.T3.2.2.2.2.1\" style=\"font-size:70%;\">The image-text pair is [MASK]. &lt;text&gt;</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.2.2.2.3\"><span class=\"ltx_text\" id=\"S5.T3.2.2.2.3.1\" style=\"font-size:70%;\">Sentiment of the text: [MASK]. &lt;text&gt;</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.2.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T3.2.3.3.1\"><span class=\"ltx_text\" id=\"S5.T3.2.3.3.1.1\" style=\"font-size:70%;\">Soft Prompt</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T3.2.3.3.2\"><span class=\"ltx_text\" id=\"S5.T3.2.3.3.2.1\" style=\"font-size:70%;\">[V1] [V2] \u2026 [Vn] &lt;text&gt;</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.2.3.3.3\"><span class=\"ltx_text\" id=\"S5.T3.2.3.3.3.1\" style=\"font-size:70%;\">[V1] [V2] \u2026 [Vn] &lt;text&gt;</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.2.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T3.2.4.4.1\"><span class=\"ltx_text\" id=\"S5.T3.2.4.4.1.1\" style=\"font-size:70%;\">P-Tuning</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T3.2.4.4.2\"><span class=\"ltx_text\" id=\"S5.T3.2.4.4.2.1\" style=\"font-size:70%;\">[V1] [V2] \u2026 [Vn] The image-text pair is [MASK]. &lt;text&gt;</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.2.4.4.3\"><span class=\"ltx_text\" id=\"S5.T3.2.4.4.3.1\" style=\"font-size:70%;\">[V1] [V2] \u2026 [Vn] Sentiment of the text: [MASK]. &lt;text&gt;</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.2.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" id=\"S5.T3.2.5.5.1\" rowspan=\"8\"><span class=\"ltx_text\" id=\"S5.T3.2.5.5.1.1\" style=\"font-size:70%;\">Instruction Prompt</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\" id=\"S5.T3.2.5.5.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T3.2.5.5.2.1\"><span class=\"ltx_text\" id=\"S5.T3.2.5.5.2.1.1\" style=\"font-size:70%;\">1.Text:&lt;text&gt; Answer the question: Is this image-text pair sarcastic or nonsarcastic? Answer:</span>\n<br class=\"ltx_break\"/><span class=\"ltx_text\" id=\"S5.T3.2.5.5.2.1.2\" style=\"font-size:70%;\">2.Text:&lt;text&gt; Based on the image and text, answer the question: Is this image-text pair sarcastic or nonsarcastic? Answer: </span>\n<br class=\"ltx_break\"/><span class=\"ltx_text\" id=\"S5.T3.2.5.5.2.1.3\" style=\"font-size:70%;\">3.Text: &lt;text&gt; Combining the text, is this sarcastic or nonsarcastic? Answer:</span></p>\n<span class=\"ltx_text\" id=\"S5.T3.2.5.5.2.2\" style=\"font-size:70%;\"></span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S5.T3.2.5.5.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T3.2.5.5.3.1\"><span class=\"ltx_text\" id=\"S5.T3.2.5.5.3.1.1\" style=\"font-size:70%;\">1.Text:&lt;text&gt; Answer the question: Which sentiment does this image-text pair contain, negative, neutral or positive? Answer:</span>\n<br class=\"ltx_break\"/><span class=\"ltx_text\" id=\"S5.T3.2.5.5.3.1.2\" style=\"font-size:70%;\">2.Text:&lt;text&gt; Based on the image and text, answer the question: Which sentiment does this image-text pair contain, negative, neutral or positive? Answer:</span>\n<br class=\"ltx_break\"/><span class=\"ltx_text\" id=\"S5.T3.2.5.5.3.1.3\" style=\"font-size:70%;\">3.Text:&lt;text&gt; Combining the text, which sentiment does this contain, negative, neutral or positive? Answer:</span></p>\n<span class=\"ltx_text\" id=\"S5.T3.2.5.5.3.2\" style=\"font-size:70%;\"></span>\n</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:70%;\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S5.T3.5.1.1\" style=\"font-size:129%;\">Table 3</span>: </span><span class=\"ltx_text\" id=\"S5.T3.6.2\" style=\"font-size:129%;\">The text templates used in different prompt methods for MSD and MSA tasks. The instruction prompt is for InstructBLIP. We do not include P-Tuning v2 here as it does not require a template for input texts.</span></figcaption>\n</figure>",
            "capture": "Table 3: The text templates used in different prompt methods for MSD and MSA tasks. The instruction prompt is for InstructBLIP. We do not include P-Tuning v2 here as it does not require a template for input texts."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T4\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S5.T4.2\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T4.2.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" id=\"S5.T4.2.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.2.1.1.1.1\">Methods</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.2.1.1.2\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.2.1.1.2.1\">w/ CH</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.2.1.1.3\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.2.1.1.3.1\">w/ LMH</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.2.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.2.1.1.4.1\">Accuracy</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.2.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.2.1.1.5.1\">Precision</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.2.1.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.2.1.1.6.1\">Recall</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.2.1.1.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.2.1.1.7.1\">F1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.2.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T4.2.2.2.1\">HFM <cite class=\"ltx_cite ltx_citemacro_citep\">(Cai et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.11311v2#bib.bib3\" title=\"\">2019</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.2.2.2\">\u2713</td>\n<td class=\"ltx_td ltx_border_t\" id=\"S5.T4.2.2.2.3\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.2.2.4\">58.88 (2.93)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.2.2.5\">48.93 (2.00)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.2.2.6\">65.83 (12.28)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.2.2.7\">55.73 (4.51)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.2.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T4.2.3.3.1\">resBERT <cite class=\"ltx_cite ltx_citemacro_citep\">(Pan et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.11311v2#bib.bib35\" title=\"\">2020</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.3.3.2\">\u2713</td>\n<td class=\"ltx_td\" id=\"S5.T4.2.3.3.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.3.3.4\">58.85 (2.04)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.3.3.5\">48.95 (1.62)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.3.3.6\">70.87 (4.61)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.3.3.7\">57.82 (0.45)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.2.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T4.2.4.4.1\">HKE <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.11311v2#bib.bib24\" title=\"\">2022a</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.4.4.2\">\u2713</td>\n<td class=\"ltx_td\" id=\"S5.T4.2.4.4.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.4.4.4\">60.76 (4.00)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.4.4.5\">52.56 (3.41)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.4.4.6\">74.43 (14.13)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.4.4.7\">61.02 (3.80)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.2.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T4.2.5.5.1\">VLMo <cite class=\"ltx_cite ltx_citemacro_citep\">(Bao et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.11311v2#bib.bib1\" title=\"\">2022</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.5.5.2\">\u2713</td>\n<td class=\"ltx_td ltx_border_t\" id=\"S5.T4.2.5.5.3\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.5.5.4\">60.18 (3.62)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.5.5.5\">50.97 (4.26)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.5.5.6\">60.31 (5.28)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.5.5.7\">53.96 (3.65)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.2.6.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T4.2.6.6.1\">CLIP <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.11311v2#bib.bib37\" title=\"\">2021</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.6.6.2\">\u2713</td>\n<td class=\"ltx_td\" id=\"S5.T4.2.6.6.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.6.6.4\">61.31 (1.28)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.6.6.5\">51.24 (1.30)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.6.6.6\">68.33 (8.26)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.6.6.7\">58.26 (2.22)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.2.7.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T4.2.7.7.1\">InstructBLIP (zero-shot)</th>\n<td class=\"ltx_td\" id=\"S5.T4.2.7.7.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.7.7.3\">\u2713</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.7.7.4\">57.59 (2.87)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.7.7.5\">47.22 (3.12)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.7.7.6\">50.33 (13.25)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.7.7.7\">47.89 (5.49)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.2.8.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T4.2.8.8.1\">InstructBLIP <cite class=\"ltx_cite ltx_citemacro_citep\">(Dai et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.11311v2#bib.bib7\" title=\"\">2023</a>)</cite>\n</th>\n<td class=\"ltx_td\" id=\"S5.T4.2.8.8.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.8.8.3\">\u2713</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.8.8.4\">60.56 (4.02)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.8.8.5\">50.57 (3.29)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.8.8.6\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.2.8.8.6.1\">78.13</span> (5.94)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.8.8.7\">61.05 (2.11)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.2.9.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T4.2.9.9.1\">CoOp <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.11311v2#bib.bib56\" title=\"\">2022b</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.9.9.2\">\u2713</td>\n<td class=\"ltx_td ltx_border_t\" id=\"S5.T4.2.9.9.3\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.9.9.4\">63.00 (5.99)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.9.9.5\">53.06 (6.42)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.9.9.6\">67.71 (4.69)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.9.9.7\">59.41 (5.23)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.2.10.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T4.2.10.10.1\">CMPA <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.11311v2#bib.bib29\" title=\"\">2023b</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.10.10.2\">\u2713</td>\n<td class=\"ltx_td\" id=\"S5.T4.2.10.10.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.10.10.4\">59.94 (2.34)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.10.10.5\">49.75 (2.35)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.10.10.6\">63.43 (4.08)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.10.10.7\">55.75 (2.86)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.2.11.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T4.2.11.11.1\">MaPLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Khattak et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.11311v2#bib.bib18\" title=\"\">2023</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.11.11.2\">\u2713</td>\n<td class=\"ltx_td\" id=\"S5.T4.2.11.11.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.11.11.4\">61.28 (2.43)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.11.11.5\">50.87 (1.95)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.11.11.6\">78.00 (6.42)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.11.11.7\">61.26 (2.80)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.2.12.12\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T4.2.12.12.1\">VLMo + Manual Prompt</th>\n<td class=\"ltx_td ltx_border_t\" id=\"S5.T4.2.12.12.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.12.12.3\">\u2713</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.12.12.4\">59.85 (2.77)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.12.12.5\">50.12 (2.64)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.12.12.6\">73.20 (8.25)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.12.12.7\">59.10 (1.15)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.2.13.13\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T4.2.13.13.1\">\u2003+ Soft Prompt</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.13.13.2\">\u2713</td>\n<td class=\"ltx_td\" id=\"S5.T4.2.13.13.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.13.13.4\">62.74 (0.65)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.13.13.5\">53.26 (1.56)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.13.13.6\">57.84 (11.09)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.13.13.7\">54.74 (4.71)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.2.14.14\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T4.2.14.14.1\">\u2003+ P-Tuning <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.11311v2#bib.bib28\" title=\"\">2021b</a>)</cite>\n</th>\n<td class=\"ltx_td\" id=\"S5.T4.2.14.14.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.14.14.3\">\u2713</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.14.14.4\">60.34 (1.61)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.14.14.5\">50.11 (1.31)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.14.14.6\">75.46 (4.03)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.14.14.7\">60.21 (2.06)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.2.15.15\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T4.2.15.15.1\">\u2003+ P-Tuning v2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.11311v2#bib.bib26\" title=\"\">2021a</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.15.15.2\">\u2713</td>\n<td class=\"ltx_td\" id=\"S5.T4.2.15.15.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.15.15.4\">61.78 (3.02)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.15.15.5\">52.23 (2.94)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.15.15.6\">63.99 (15.12)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.15.15.7\">56.36 (5.98)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.2.16.16\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T4.2.16.16.1\">VLMo + MoPE-BAF</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.16.16.2\">\u2713</td>\n<td class=\"ltx_td ltx_border_t\" id=\"S5.T4.2.16.16.3\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.16.16.4\">64.06 (0.71)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.16.16.5\">53.69 (4.87)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.16.16.6\">71.60 (2.78)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.16.16.7\">61.32 (2.57)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.2.17.17\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S5.T4.2.17.17.1\">\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0+ MoPE-BAF + MP</th>\n<td class=\"ltx_td ltx_border_bb\" id=\"S5.T4.2.17.17.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.2.17.17.3\">\u2713</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.2.17.17.4\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.2.17.17.4.1\">65.32</span> (3.30)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.2.17.17.5\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.2.17.17.5.1\">55.92</span> (3.91)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.2.17.17.6\">68.64 (7.52)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.2.17.17.7\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.2.17.17.7.1\">61.32</span> (1.15)</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S5.T4.3.1.1\" style=\"font-size:90%;\">Table 4</span>: </span><span class=\"ltx_text\" id=\"S5.T4.4.2\" style=\"font-size:90%;\">Experimental results on\nthe MSDT dataset with 32 training samples, where the standard deviations are shown in parentheses. CH refers to the classification head and LMH refers to the language modeling head. MP means Manual Prompt. </span></figcaption>\n</figure>",
            "capture": "Table 4: Experimental results on\nthe MSDT dataset with 32 training samples, where the standard deviations are shown in parentheses. CH refers to the classification head and LMH refers to the language modeling head. MP means Manual Prompt. "
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"S6.T5\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S6.T5.2\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S6.T5.2.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" id=\"S6.T5.2.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T5.2.1.1.1.1\">Methods</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S6.T5.2.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T5.2.1.1.2.1\">Acc</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S6.T5.2.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T5.2.1.1.3.1\">Mac-F1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S6.T5.2.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T5.2.1.1.4.1\">Wtd-F1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T5.2.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S6.T5.2.2.2.1\">MVAN</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T5.2.2.2.2\">42.77</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T5.2.2.2.3\">36.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T5.2.2.2.4\">44.14</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T5.2.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T5.2.3.3.1\">MGNNS</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.2.3.3.2\">34.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.2.3.3.3\">32.05</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.2.3.3.4\">36.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T5.2.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T5.2.4.4.1\">UP-MPF</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.2.4.4.2\">58.21</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.2.4.4.3\">51.08</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.2.4.4.4\">58.49</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T5.2.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S6.T5.2.5.5.1\">CLIP</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T5.2.5.5.2\">49.51</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T5.2.5.5.3\">45.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T5.2.5.5.4\">51.63</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T5.2.6.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T5.2.6.6.1\">CoOp</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.2.6.6.2\">51.47</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.2.6.6.3\">40.58</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.2.6.6.4\">48.52</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T5.2.7.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T5.2.7.7.1\">MaPLe</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.2.7.7.2\">50.49</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.2.7.7.3\">43.06</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.2.7.7.4\">51.74</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T5.2.8.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T5.2.8.8.1\">CMPA</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.2.8.8.2\">56.74</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.2.8.8.3\">42.75</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.2.8.8.4\">53.86</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T5.2.9.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T5.2.9.9.1\">InstructBLIP</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.2.9.9.2\">59.80</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.2.9.9.3\">48.59</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.2.9.9.4\">59.73</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T5.2.10.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T5.2.10.10.1\">VLMo + MP (zero-shot)</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.2.10.10.2\">59.07</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.2.10.10.3\">38.53</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.2.10.10.4\">51.94</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T5.2.11.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T5.2.11.11.1\">VLMo + MP</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.2.11.11.2\">60.79</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.2.11.11.3\">52.62</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.2.11.11.4\">61.27</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T5.2.12.12\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T5.2.12.12.1\">VLMo + P-Tuning</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.2.12.12.2\">61.03</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.2.12.12.3\">51.28</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.2.12.12.4\">60.75</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T5.2.13.13\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S6.T5.2.13.13.1\">VLMo + MoPE-BAF + MP</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T5.2.13.13.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T5.2.13.13.2.1\">63.48</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T5.2.13.13.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T5.2.13.13.3.1\">52.92</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T5.2.13.13.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T5.2.13.13.4.1\">62.40</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S6.T5.3.1.1\" style=\"font-size:90%;\">Table 5</span>: </span><span class=\"ltx_text\" id=\"S6.T5.4.2\" style=\"font-size:90%;\">Comparison of results between our approach and previous methods on the few-shot MVSA-S dataset.\nMac-F1 and Wtd-F1 denote Macro-F1 and Weighted-F1 respectively. MP denotes manual prompt. The results of the first group are from the work <cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.11311v2#bib.bib51\" title=\"\">2022</a>)</cite>, and we implemented the models in the second group.</span></figcaption>\n</figure>",
            "capture": "Table 5: Comparison of results between our approach and previous methods on the few-shot MVSA-S dataset.\nMac-F1 and Wtd-F1 denote Macro-F1 and Weighted-F1 respectively. MP denotes manual prompt. The results of the first group are from the work (Yu et\u00a0al., 2022), and we implemented the models in the second group."
        },
        "6": {
            "table_html": "<figure class=\"ltx_table\" id=\"S6.T6\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S6.T6.2\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S6.T6.2.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" id=\"S6.T6.2.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.2.1.1.1.1\">Methods</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S6.T6.2.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.2.1.1.2.1\">A</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S6.T6.2.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.2.1.1.3.1\">P</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S6.T6.2.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.2.1.1.4.1\">R</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S6.T6.2.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.2.1.1.5.1\">F1</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S6.T6.2.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S6.T6.2.2.1.1\">VLMo</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.2.2.1.2\">60.18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.2.2.1.3\">50.97</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.2.2.1.4\">60.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.2.2.1.5\">53.96</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.2.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S6.T6.2.3.2.1\">\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0+ MoPE</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.2.3.2.2\">61.73</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.2.3.2.3\">51.35</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.2.3.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.2.3.2.4.1\">75.13</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.2.3.2.5\">60.94</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.2.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S6.T6.2.4.3.1\">\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0+ MoPE + BAF</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.2.4.3.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.2.4.3.2.1\">64.06</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.2.4.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.2.4.3.3.1\">53.69</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.2.4.3.4\">71.60</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.2.4.3.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.2.4.3.5.1\">61.32</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.2.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S6.T6.2.5.4.1\">VLMo + MP</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.2.5.4.2\">59.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.2.5.4.3\">50.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.2.5.4.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.2.5.4.4.1\">73.20</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.2.5.4.5\">59.10</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.2.6.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S6.T6.2.6.5.1\">\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u2003+ MoPE</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.2.6.5.2\">64.06</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.2.6.5.3\">53.98</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.2.6.5.4\">70.73</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.2.6.5.5\">60.93</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.2.7.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S6.T6.2.7.6.1\">\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u2003+ MoPE + BAF</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T6.2.7.6.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.2.7.6.2.1\">65.32</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T6.2.7.6.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.2.7.6.3.1\">55.92</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T6.2.7.6.4\">68.64</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T6.2.7.6.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.2.7.6.5.1\">61.32</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S6.T6.3.1.1\" style=\"font-size:90%;\">Table 6</span>: </span><span class=\"ltx_text\" id=\"S6.T6.4.2\" style=\"font-size:90%;\">Ablative analysis of the MoPE and BAF modules.</span></figcaption>\n</figure>",
            "capture": "Table 6: Ablative analysis of the MoPE and BAF modules."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.11311v2_figure_1.png",
            "caption": "Table 1: Examples of MSD and MSA task. The left example displays sarcasm, as the text \u201cgreat sale\u201d contradicts with the image depicting a mere 1-dollar discount. The right example conveys a positive wishing attitude, thus categorized as positive."
        },
        "2": {
            "figure_path": "2403.11311v2_figure_2.png",
            "caption": "Table 1: Examples of MSD and MSA task. The left example displays sarcasm, as the text \u201cgreat sale\u201d contradicts with the image depicting a mere 1-dollar discount. The right example conveys a positive wishing attitude, thus categorized as positive."
        },
        "3": {
            "figure_path": "2403.11311v2_figure_3.png",
            "caption": "Figure 1: The framework of VLMo for image-text detection task. We demonstrate two methods.\nIn finetuning, the [CLS] representation is fed to a classification head,\nwhile in manual prompt, the representation of [MASK] is fed to a verbalizer."
        },
        "4": {
            "figure_path": "2403.11311v2_figure_4.png",
            "caption": "Figure 2: Our proposed MoPE-BAF model for multi-modal semantic understanding ."
        },
        "5": {
            "figure_path": "2403.11311v2_figure_5.png",
            "caption": "Figure 3: Receptive fields of different prompts, image patches, text tokens in the self-attention module when using MoPE. VP and LP are shorthand for V-Prompt, L-Prompt."
        },
        "6": {
            "figure_path": "2403.11311v2_figure_6.png",
            "caption": "Figure 4: (a) F1 performance training\nMoPE with different prompt lengths. (b) F1 scores training MoPE-BAF with different block numbers. (c) Comparison between VLMo and VLMo + MoPE-BAF under different training shots."
        }
    },
    "references": [
        {
            "1": {
                "title": "Vlmo: Unified vision-language pre-training with mixture-of-modality-experts.",
                "author": "Hangbo Bao, Wenhui Wang, Li Dong, Qiang Liu, Owais Khan Mohammed, Kriti Aggarwal, Subhojit Som, Songhao Piao, and Furu Wei. 2022.",
                "venue": "In Advances in Neural Information Processing Systems, volume 35, pages 32897\u201332912. Curran Associates, Inc.",
                "url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/d46662aa53e78a62afd980a29e0c37ed-Paper-Conference.pdf"
            }
        },
        {
            "2": {
                "title": "Automatic sarcasm detection: Systematic literature review.",
                "author": "Alexandru-Costin B\u0103roiu and \\textcommabelowStefan Tr\u0103u\\textcommabelowsan-Matu. 2022.",
                "venue": "Information, 13(8).",
                "url": "https://doi.org/10.3390/info13080399"
            }
        },
        {
            "3": {
                "title": "Multi-modal sarcasm detection in Twitter with hierarchical fusion model.",
                "author": "Yitao Cai, Huiyu Cai, and Xiaojun Wan. 2019.",
                "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2506\u20132515, Florence, Italy. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/P19-1239"
            }
        },
        {
            "4": {
                "title": "Towards multimodal sarcasm detection (an _Obviously_ perfect paper).",
                "author": "Santiago Castro, Devamanyu Hazarika, Ver\u00f3nica P\u00e9rez-Rosas, Roger Zimmermann, Rada Mihalcea, and Soujanya Poria. 2019.",
                "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4619\u20134629, Florence, Italy. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/P19-1455"
            }
        },
        {
            "5": {
                "title": "Sentiment and emotion help sarcasm? a multi-task learning framework for multi-modal sarcasm, sentiment and emotion analysis.",
                "author": "Dushyant Singh Chauhan, Dhanush S R, Asif Ekbal, and Pushpak Bhattacharyya. 2020.",
                "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4351\u20134360, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2020.acl-main.401"
            }
        },
        {
            "6": {
                "title": "Randaugment: Practical automated data augmentation with a reduced search space.",
                "author": "Ekin Dogus Cubuk, Barret Zoph, Jon Shlens, and Quoc Le. 2020.",
                "venue": "In Advances in Neural Information Processing Systems, volume 33, pages 18613\u201318624. Curran Associates, Inc.",
                "url": "https://proceedings.neurips.cc/paper_files/paper/2020/file/d85b63ef0ccb114d0a3bb7b7d808028f-Paper.pdf"
            }
        },
        {
            "7": {
                "title": "Instructblip: Towards general-purpose vision-language models with instruction tuning.",
                "author": "Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2305.06500"
            }
        },
        {
            "8": {
                "title": "Nice perfume. how long did you marinate in it? multimodal sarcasm explanation.",
                "author": "Poorav Desai, Tanmoy Chakraborty, and Md Shad Akhtar. 2022.",
                "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 36(10):10563\u201310571.",
                "url": "https://doi.org/10.1609/aaai.v36i10.21300"
            }
        },
        {
            "9": {
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding.",
                "author": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.",
                "venue": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/N19-1423"
            }
        },
        {
            "10": {
                "title": "An image is worth 16x16 words: Transformers for image recognition at scale.",
                "author": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021.",
                "venue": "In International Conference on Learning Representations.",
                "url": "https://openreview.net/forum?id=YicbFdNTTy"
            }
        },
        {
            "11": {
                "title": "Making pre-trained language models better few-shot learners.",
                "author": "Tianyu Gao, Adam Fisch, and Danqi Chen. 2021.",
                "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3816\u20133830, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.acl-long.295"
            }
        },
        {
            "12": {
                "title": "A systematic survey of prompt engineering on vision-language foundation models.",
                "author": "Jindong Gu, Zhen Han, Shuo Chen, Ahmad Beirami, Bailan He, Gengyuan Zhang, Ruotong Liao, Yao Qin, Volker Tresp, and Philip Torr. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2307.12980"
            }
        },
        {
            "13": {
                "title": "Bi-bimodal modality fusion for correlation-controlled multimodal sentiment analysis.",
                "author": "Wei Han, Hui Chen, Alexander Gelbukh, Amir Zadeh, Louis-philippe Morency, and Soujanya Poria. 2021a.",
                "venue": "In Proceedings of the 2021 International Conference on Multimodal Interaction, ICMI \u201921, page 6\u201315, New York, NY, USA. Association for Computing Machinery.",
                "url": "https://doi.org/10.1145/3462244.3479919"
            }
        },
        {
            "14": {
                "title": "Ptr: Prompt tuning with rules for text classification.",
                "author": "Xu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, and Maosong Sun. 2021b.",
                "venue": "arXiv preprint arXiv:2105.11259.",
                "url": null
            }
        },
        {
            "15": {
                "title": "Deep residual learning for image recognition.",
                "author": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.",
                "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
                "url": null
            }
        },
        {
            "16": {
                "title": "Unsupervised prompt learning for vision-language models.",
                "author": "Tony Huang, Jack Chu, and Fangyun Wei. 2022.",
                "venue": "arXiv preprint arXiv:2204.03649.",
                "url": null
            }
        },
        {
            "17": {
                "title": "Scaling up visual and vision-language representation learning with noisy text supervision.",
                "author": "Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 2021.",
                "venue": "In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 4904\u20134916. PMLR.",
                "url": "https://proceedings.mlr.press/v139/jia21b.html"
            }
        },
        {
            "18": {
                "title": "Maple: Multi-modal prompt learning.",
                "author": "Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad Shahbaz Khan. 2023.",
                "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 19113\u201319122.",
                "url": null
            }
        },
        {
            "19": {
                "title": "Vilt: Vision-and-language transformer without convolution or region supervision.",
                "author": "Wonjae Kim, Bokyung Son, and Ildoo Kim. 2021.",
                "venue": "In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 5583\u20135594. PMLR.",
                "url": "https://proceedings.mlr.press/v139/kim21k.html"
            }
        },
        {
            "20": {
                "title": "The power of scale for parameter-efficient prompt tuning.",
                "author": "Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.",
                "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045\u20133059, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.emnlp-main.243"
            }
        },
        {
            "21": {
                "title": "Prefix-tuning: Optimizing continuous prompts for generation.",
                "author": "Xiang Lisa Li and Percy Liang. 2021.",
                "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582\u20134597, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.acl-long.353"
            }
        },
        {
            "22": {
                "title": "Multi-modal sarcasm detection with interactive in-modal and cross-modal graphs.",
                "author": "Bin Liang, Chenwei Lou, Xiang Li, Lin Gui, Min Yang, and Ruifeng Xu. 2021.",
                "venue": "In Proceedings of the 29th ACM International Conference on Multimedia, MM \u201921, page 4707\u20134715, New York, NY, USA. Association for Computing Machinery.",
                "url": "https://doi.org/10.1145/3474085.3475190"
            }
        },
        {
            "23": {
                "title": "Multi-modal sarcasm detection via cross-modal graph convolutional network.",
                "author": "Bin Liang, Chenwei Lou, Xiang Li, Min Yang, Lin Gui, Yulan He, Wenjie Pei, and Ruifeng Xu. 2022.",
                "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1767\u20131777, Dublin, Ireland. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2022.acl-long.124"
            }
        },
        {
            "24": {
                "title": "Towards multi-modal sarcasm detection via hierarchical congruity modeling with knowledge enhancement.",
                "author": "Hui Liu, Wenya Wang, and Haoliang Li. 2022a.",
                "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 4995\u20135006, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.",
                "url": "https://aclanthology.org/2022.emnlp-main.333"
            }
        },
        {
            "25": {
                "title": "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing.",
                "author": "Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023a.",
                "venue": "ACM Comput. Surv., 55(9).",
                "url": "https://doi.org/10.1145/3560815"
            }
        },
        {
            "26": {
                "title": "P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks.",
                "author": "Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2021a.",
                "venue": "CoRR, abs/2110.07602.",
                "url": "http://arxiv.org/abs/2110.07602"
            }
        },
        {
            "27": {
                "title": "P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks.",
                "author": "Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2022b.",
                "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 61\u201368, Dublin, Ireland. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2022.acl-short.8"
            }
        },
        {
            "28": {
                "title": "Gpt understands, too.",
                "author": "Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2021b.",
                "venue": null,
                "url": "http://arxiv.org/abs/2103.10385"
            }
        },
        {
            "29": {
                "title": "Deeply coupled cross-modal prompt learning.",
                "author": "Xuejing Liu, Wei Tang, Jinghui Lu, Rui Zhao, Zhaojun Guo, and Fei Tan. 2023b.",
                "venue": "In Findings of the Association for Computational Linguistics: ACL 2023, pages 7957\u20137970, Toronto, Canada. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.findings-acl.504"
            }
        },
        {
            "30": {
                "title": "Vision-and-language pretrained models: A survey.",
                "author": "Siqu Long, Feiqi Cao, Soyeon Caren Han, and Haiqin Yang. 2022.",
                "venue": "In Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22, pages 5530\u20135537. International Joint Conferences on Artificial Intelligence Organization.",
                "url": "https://doi.org/10.24963/ijcai.2022/773"
            }
        },
        {
            "31": {
                "title": "Decoupled weight decay regularization.",
                "author": "Ilya Loshchilov and Frank Hutter. 2019.",
                "venue": "In International Conference on Learning Representations.",
                "url": "https://openreview.net/forum?id=Bkg6RiCqY7"
            }
        },
        {
            "32": {
                "title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks.",
                "author": "Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019.",
                "venue": "In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.",
                "url": "https://proceedings.neurips.cc/paper_files/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf"
            }
        },
        {
            "33": {
                "title": "Sarcasm detection using news headlines dataset.",
                "author": "Rishabh Misra and Prahal Arora. 2023.",
                "venue": "AI Open, 4:13\u201318.",
                "url": "https://doi.org/https://doi.org/10.1016/j.aiopen.2023.01.001"
            }
        },
        {
            "34": {
                "title": "Sentiment analysis on multi-view social data.",
                "author": "Teng Niu, Shiai Zhu, Lei Pang, and Abdulmotaleb El Saddik. 2016.",
                "venue": "In MultiMedia Modeling, pages 15\u201327, Cham. Springer International Publishing.",
                "url": null
            }
        },
        {
            "35": {
                "title": "Modeling intra and inter-modality incongruity for multi-modal sarcasm detection.",
                "author": "Hongliang Pan, Zheng Lin, Peng Fu, Yatao Qi, and Weiping Wang. 2020.",
                "venue": "In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1383\u20131392, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2020.findings-emnlp.124"
            }
        },
        {
            "36": {
                "title": "Multimodal learning using optimal transport for sarcasm and humor detection.",
                "author": "Shraman Pramanick, Aniket Roy, and Vishal M. Patel. 2022.",
                "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 3930\u20133940.",
                "url": null
            }
        },
        {
            "37": {
                "title": "Learning transferable visual models from natural language supervision.",
                "author": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021.",
                "venue": "In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 8748\u20138763. PMLR.",
                "url": "https://proceedings.mlr.press/v139/radford21a.html"
            }
        },
        {
            "38": {
                "title": "I didn\u2019t mean what i wrote! exploring multimodality for sarcasm detection.",
                "author": "Suyash Sangwan, Md Shad Akhtar, Pranati Behera, and Asif Ekbal. 2020.",
                "venue": "In 2020 International Joint Conference on Neural Networks (IJCNN), pages 1\u20138.",
                "url": "https://doi.org/10.1109/IJCNN48605.2020.9206905"
            }
        },
        {
            "39": {
                "title": "It\u2019s not just size that matters: Small language models are also few-shot learners.",
                "author": "Timo Schick and Hinrich Sch\u00fctze. 2021.",
                "venue": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2339\u20132352, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.naacl-main.185"
            }
        },
        {
            "40": {
                "title": "Detecting sarcasm in multimodal social platforms.",
                "author": "Rossano Schifanella, Paloma de Juan, Joel Tetreault, and LiangLiang Cao. 2016.",
                "venue": "In Proceedings of the 24th ACM International Conference on Multimedia, MM \u201916, page 1136\u20131145, New York, NY, USA. Association for Computing Machinery.",
                "url": "https://doi.org/10.1145/2964284.2964321"
            }
        },
        {
            "41": {
                "title": "LXMERT: Learning cross-modality encoder representations from transformers.",
                "author": "Hao Tan and Mohit Bansal. 2019.",
                "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5100\u20135111, Hong Kong, China. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/D19-1514"
            }
        },
        {
            "42": {
                "title": "Dynamic routing transformer network for multimodal sarcasm detection.",
                "author": "Yuan Tian, Nan Xu, Ruike Zhang, and Wenji Mao. 2023.",
                "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2468\u20132480, Toronto, Canada. Association for Computational Linguistics.",
                "url": "https://aclanthology.org/2023.acl-long.139"
            }
        },
        {
            "43": {
                "title": "Multimodal sarcasm target identification in tweets.",
                "author": "Jiquan Wang, Lin Sun, Yi Liu, Meizhi Shao, and Zengwei Zheng. 2022.",
                "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8164\u20138175, Dublin, Ireland. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2022.acl-long.562"
            }
        },
        {
            "44": {
                "title": "Why do pretrained language models help in downstream tasks? an analysis of head and prompt tuning.",
                "author": "Colin Wei, Sang Michael Xie, and Tengyu Ma. 2021.",
                "venue": "In Advances in Neural Information Processing Systems, volume 34, pages 16158\u201316170. Curran Associates, Inc.",
                "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/86b3e165b8154656a71ffe8a327ded7d-Paper.pdf"
            }
        },
        {
            "45": {
                "title": "Dip: Dual incongruity perceiving network for sarcasm detection.",
                "author": "Changsong Wen, Guoli Jia, and Jufeng Yang. 2023.",
                "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2540\u20132550.",
                "url": null
            }
        },
        {
            "46": {
                "title": "Modeling incongruity between modalities for multimodal sarcasm detection.",
                "author": "Yang Wu, Yanyan Zhao, Xin Lu, Bing Qin, Yin Wu, Jian Sheng, and Jinlong Li. 2021.",
                "venue": "IEEE MultiMedia, 28(2):86\u201395.",
                "url": "https://doi.org/10.1109/MMUL.2021.3069097"
            }
        },
        {
            "47": {
                "title": "A co-memory network for multimodal sentiment analysis.",
                "author": "Nan Xu, Wenji Mao, and Guandan Chen. 2018.",
                "venue": "In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, SIGIR \u201918, page 929\u2013932, New York, NY, USA. Association for Computing Machinery.",
                "url": "https://doi.org/10.1145/3209978.3210093"
            }
        },
        {
            "48": {
                "title": "Reasoning with multimodal sarcastic tweets via modeling cross-modality contrast and semantic association.",
                "author": "Nan Xu, Zhixiong Zeng, and Wenji Mao. 2020.",
                "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3777\u20133786, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2020.acl-main.349"
            }
        },
        {
            "49": {
                "title": "Image-text multimodal emotion classification via multi-view attentional network.",
                "author": "Xiaocui Yang, Shi Feng, Daling Wang, and Yifei Zhang. 2021a.",
                "venue": "IEEE Transactions on Multimedia, 23:4014\u20134026.",
                "url": "https://doi.org/10.1109/TMM.2020.3035277"
            }
        },
        {
            "50": {
                "title": "Multimodal sentiment detection based on multi-channel graph neural networks.",
                "author": "Xiaocui Yang, Shi Feng, Yifei Zhang, and Daling Wang. 2021b.",
                "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 328\u2013339, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.acl-long.28"
            }
        },
        {
            "51": {
                "title": "Unified multi-modal pre-training for few-shot sentiment analysis with prompt-based learning.",
                "author": "Yang Yu, Dong Zhang, and Shoushan Li. 2022.",
                "venue": "In Proceedings of the 30th ACM International Conference on Multimedia, MM \u201922, page 189\u2013198, New York, NY, USA. Association for Computing Machinery.",
                "url": "https://doi.org/10.1145/3503161.3548306"
            }
        },
        {
            "52": {
                "title": "Unified vision and language prompt learning.",
                "author": "Yuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, and Chen Change Loy. 2022.",
                "venue": null,
                "url": "http://arxiv.org/abs/2210.07225"
            }
        },
        {
            "53": {
                "title": "Vision-language models for vision tasks: A survey.",
                "author": "Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu. 2023a.",
                "venue": null,
                "url": "http://arxiv.org/abs/2304.00685"
            }
        },
        {
            "54": {
                "title": "Stance-level sarcasm detection with bert and stance-centered graph attention networks.",
                "author": "Yazhou Zhang, Dan Ma, Prayag Tiwari, Chen Zhang, Mehedi Masud, Mohammad Shorfuzzaman, and Dawei Song. 2023b.",
                "venue": "ACM Trans. Internet Technol., 23(2).",
                "url": "https://doi.org/10.1145/3533430"
            }
        },
        {
            "55": {
                "title": "Conditional prompt learning for vision-language models.",
                "author": "Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. 2022a.",
                "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 16816\u201316825.",
                "url": null
            }
        },
        {
            "56": {
                "title": "Learning to prompt for vision-language models.",
                "author": "Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. 2022b.",
                "venue": "Int. J. Comput. Vision, 130(9):2337\u20132348.",
                "url": "https://doi.org/10.1007/s11263-022-01653-1"
            }
        },
        {
            "57": {
                "title": "Multi-Modal Sarcasm Detection in Twitter with Hierarchical Fusion Model.",
                "author": "Cai, Yitao and Cai, Huiyu and Wan, Xiaojun. 2019.",
                "venue": "Institute of Computer Science and Technology, Peking University.",
                "url": "https://github.com/headacheboy/data-of-multimodal-sarcasm-detection"
            }
        },
        {
            "58": {
                "title": "MVSA: Sentiment Analysis on Multi-view Social Data.",
                "author": "Teng Niu and Shiai Zhu and Lei Pang and Abdulmotaleb El-Saddik. 2016.",
                "venue": "MCRLab, University of Ottawa.",
                "url": "https://mcrlab.net/research/mvsa-sentiment-analysis-on-multi-view-social-data/"
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.11311v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2",
            "2.3"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "4",
            "4.1",
            "4.2"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.1",
            "5.2",
            "5.3",
            "6",
            "6.1",
            "6.2",
            "6.3",
            "6.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "6.3",
            "6.4"
        ]
    },
    "research_context": {
        "paper_id": "2403.11311v2",
        "paper_title": "Mixture-of-Prompt-Experts for Multi-modal Semantic Understanding",
        "research_background": "### Paper Motivation\nThe motivation behind this paper arises from the challenges in multi-modal semantic understanding (MSU), especially in tasks like multi-modal sarcasm detection (MSD) and multi-modal sentiment analysis (MSA) on social media platforms. Accurate MSU is critical due to the complex and often conflicting information presented through text and imagery. The existing approaches often demand large amounts of high-quality training data, which is challenging to collect, especially for nuanced information like sarcasm. Furthermore, the prevalent use of separate pre-trained encoders for different modalities can result in misalignment and suboptimal fusion of the modalities. Given these challenges, there is a need for more efficient and effective methods, particularly in few-shot learning scenarios.\n\n### Research Problem\nThe core research problem addressed in this paper is how to effectively leverage vision-language models (VLMs) and prompt-based learning for MSU in few-shot scenarios. Specifically, the paper aims to:\n1. Develop a robust multi-modal model that can handle limited training data while achieving high performance in tasks like MSD and MSA.\n2. Identify how to utilize soft prompts in a unified Transformer network to enhance both single-modal representation and multi-modal fusion.\n3. Design a mechanism to facilitate deep interactions between modalities and enable smooth transitions between them.\n\n### Relevant Prior Work\nThe paper builds on several advancements in the fields of MSU and VLMs:\n1. **Pre-trained Encoders**: Previous methods commonly employ dual-encoder architectures, utilizing models like BERT for text and ResNet for images to extract features (Devlin et al., 2019; He et al., 2016).\n2. **Interaction Mechanisms**: Researchers have designed various methods for feature interaction, such as:\n   - Attention mechanisms (Pan et al., 2020; Xu et al., 2020; Han et al., 2021a)\n   - Graph structures (Liang et al., 2021, 2022; Liu et al., 2022a)\n   - Optimal transport (Pramanick et al., 2022)\n   - Dynamic routing (Tian et al., 2023)\n3. **Vision-Language Models (VLMs)**: \n   - For the first category, models like CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021) encode images and texts separately and fuse them based on similarities.\n   - For the second category, unified models like VilBert (Lu et al., 2019) and VLMo (Bao et al., 2022) handle both single-modal and multi-modal representations within the same network.\n4. **Prompt-based Learning**: Prompt-based learning, particularly with continuous prompts or soft prompts, has shown promise in adapting pre-trained models to downstream tasks (Liu et al., 2021a, b; Han et al., 2021b; Liu et al., 2023a). However, their use has been mostly limited to text data and separate modality models.\n5. **Application to Few-Shot MSU**: Previous models making use of soft prompts like CoOp, CoCoOp, UPT, and MaPLe have predominantly focused on dual-path VLMs and have not fully explored unified VLMs for multi-modal prompts.\n\n### Novel Contributions\nTo address these gaps, the paper proposes:\n1. **Mixture-of-Prompt-Experts with Block-Aware Prompt Fusion (MoPE-BAF)**: A framework using soft prompt experts specialized for text, image, and unified inter-modality features.\n2. **Block-Aware Prompt Fusion Mechanism**: To enhance connection and facilitate deep interaction between different prompt experts by reorganizing Transformer layers into blocks and applying cross-attention.\n\n### Impact\nThe proposed model aims to provide significant improvements in few-shot MSU tasks, showing superior performance compared to conventional methods and even large-sized language models. The experimental results on MSD and MSA tasks support this ambition, indicating notable advancements over previous state-of-the-art methods.",
        "methodology": "### Mixture-of-Prompt-Experts for Multi-modal Semantic Understanding\n\n**Methodology:**\n\nIn our study, we incorporate the vision-language pre-trained model (VLMo) as the foundational structure (backbone) of our proposed approach. To clarify our method, we provide an overview of VLMo along with essential details on how it can be applied to multi-modal semantic understanding (MSU) tasks.\n\n1. **Classical Approach**:\n   - The most direct method involves attaching a classification head on top of the VLMo model. After the text and image inputs are encoded, the vector corresponding to the text-start token ([CLS]) serves as the composite representation of the image-text pair. The final prediction is derived from this representation.\n   \n   \\[\n   \\text{Prediction} = \\text{ClassHead}(\\text{VLMo}_{\\text{[CLS]}})\n   \\]\n   \n   In this equation, \\(\\theta\\) denotes the parameters of the pre-trained VLMo model, and \\(\\phi\\) represents the parameters of the classification head.\n\n2. **Prompt-based Methods**:\n   - **Hard Prompt Method**: This strategy involves transforming the task into a masked language modeling task by employing a rigid template that includes a [MASK] token:\n   \n     \\[\n     \\text{Prediction} = \\text{Verbalizer}(\\text{VLMo}_{\\text{[MASK]}})\n     \\]\n     \n     Here, \\(T\\) represents the prompt template and \\(V\\) signifies the verbalizer that translates the [MASK] token into probabilities for various label words.\n   \n   - **Soft Prompt Method**: This technique makes use of a flexible set of trainable virtual tokens. As a result, it removes the necessity for manually designing templates:\n   \n     \\[\n     \\text{Prediction} = \\text{VLMo}_{\\text{Virtual Token}}\n     \\]\n     \n     In this form, \\(\\mathcal{V}\\) represents the virtual tokens.\n\n**Innovations**:\n- Our approach leverages the established capabilities of the VLMo model and extends it across different applications of multi-modal tasks.\n- By using a combination of both hard and soft prompts, we aim for the model to not only perform well with manual guidelines (templates) but also effectively adapt through trainable prompts.\n- This mixture of prompt methods allows for greater flexibility and performance robustness in semantic understanding tasks.\n\n### Summary\n\nThe methodology utilizes the VLMo model, a crossover between vision and language pre-training models, in two primary ways: through classic classification head attachment and prompt-based methods (hard and soft prompts). The innovation lies in using a mixture of prompt strategies to improve task flexibility and performance.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Datasets\n- **Image-Text MSU (Multi-modal Sarcasm Detection):**\n  - **Dataset:** MSDT (Cai et al., 2019)\n  - **Statistics:** 29k/2.4k/2.4k train/validation/test samples\n  - **Sample Characteristics:** Each consists of an image-text pair with a binary label {sarcasm, nonsarcasm}\n  - **Few-Shot Dataset:** 32 samples from train/validation, balanced with equal labels\n\n- **Multimodal Sentiment Analysis (MSA):**\n  - **Dataset:** MVSA-S (Niu et al., 2016)\n  - **Sample Characteristics:** Image-text pairs labeled as {positive, neutral, negative}\n  - **Few-Shot Setting:** 1% random sampling from training and development sets (as per Yu et al., 2022)\n\n#### Evaluation Metrics\n- **MSD:**\n  - Accuracy\n  - F1 score\n  - Results reported as the mean of three disjoint sampled datasets for robustness\n  \n- **MSA:**\n  - Accuracy\n  - Weighted-F1 score\n  - Macro-F1 score\n\n#### Baselines\nThe document didn't provide specific algorithms or models defined as baselines. Typically, these would include state-of-the-art algorithms in the respective tasks, but they need to be checked in the referenced papers or prior works in the field.\n\n#### Main Experimental Results\nThe document doesn't include explicit results within the specified sections. Typically, these results would entail the performance (in terms of Accuracy, F1 scores) of the Mixture-of-Prompt-Experts approach on the mentioned few-shot datasets along with comparison to baseline models. \n\nIn summary, the experiment setup emphasizes few-shot learning scenarios on standardized multi-modal datasets for sarcasm detection and sentiment analysis, utilizing established evaluation metrics to benchmark performance."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To investigate the effects of the Mixture-of-Prompt-Experts (MoPE) and Block-Aware Prompt Fusion (BAF) on the performance of the unified vision-language model (VLMo).",
            "experiment_process": "Ablation experiments were conducted to measure the performance difference when using or not using the Manual Prompt, MoPE, and the combined MoPE+BAF setup. The results were analyzed to identify the contribution of each component. Given the dependencies, BAF could not be independently validated from MoPE.",
            "result_discussion": "MoPE significantly improved the performance of VLMo in both settings (with or without Manual Prompt), and further combining it with prompt fusion (MoPE+BAF) achieved even better performance. BAF's independent effect couldn't be validated since it relies on MoPE.",
            "ablation_id": "2403.11311v2.No1"
        },
        {
            "research_objective": "To analyze the impact of controllable factors within the MoPE-BAF model, including prompt length, block number in the BAF module, and number of training shots.",
            "experiment_process": "Experiments were conducted by varying different factors: 1) Prompt length from 5 to 50, evaluated for its influence on model performance; 2) Number of blocks in the BAF module, dividing the first 21 layers of VLMo into 2-7 blocks, ensuring no prompt fusion if configured with 1 block; 3) Number of training shots, comparing performance across different sample sizes. Results were visualized in Figure 4 for prompt length, block number, and training shots respectively.",
            "result_discussion": "The optimal performance was achieved with a moderate prompt length, showing an initial increase followed by a stabilization phase. Too many blocks in the BAF module led to specialization loss and degraded performance, with 3-5 blocks being optimal. Increasing training shots improved performance across models, with MoPE-BAF consistently outperforming VLMo.",
            "ablation_id": "2403.11311v2.No2"
        }
    ]
}