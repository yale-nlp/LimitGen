{
    "title": "SED: Self-Evaluation Decoding Enhances Large Language Models for Better Generation",
    "abstract": "Existing Large Language Models (LLMs) generate text through unidirectional autoregressive decoding methods to respond to various user queries.\nThese methods tend to consider token selection in a simple sequential manner, making it easy to fall into suboptimal options when encountering uncertain tokens, referred to as chaotic points in our work.\nMany chaotic points exist in texts generated by LLMs, and they often significantly affect the quality of subsequently generated tokens, which can interfere with LLMs\u2019 generation.\nThis paper proposes Self-Evaluation Decoding, SED, a decoding method for enhancing model generation.\nAnalogous to the human decision-making process, SED integrates speculation and evaluation steps into the decoding process, allowing LLMs to make more careful decisions and thus optimize token selection at chaotic points.\nExperimental results across various tasks using different LLMs demonstrate SED\u2019s effectiveness.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large Language Models (LLMs) are equipped with rich word knowledge, relying on decoding methods to express their abilities during the inference phase.\nExisting decoding methods are mainly autoregressive and are primarily classified into two categories (Ippolito et al., 2019  ###reference_b14###). Firstly, search-based methods aim to maximize the probability of generated text, such as greedy search and beam search. The other is sampling-based methods, which sample the next generated token based on a probability distribution, such as nuclear sampling.\nDespite good results, these autoregressive decoding methods for sequential token generation have limitations.\nLLMs often encounter highly uncertain tokens, referred to as chaotic points in this paper, during decoding.\nFigure 1  ###reference_### shows the token probability distribution at a chaotic point.\nThe probability mass is quite dispersed, which suggests that the model does not know which token to choose.\nOn the other hand, Answer 1 and Answer 2 in Figure 1  ###reference_### have the same content before the chaotic point but with different conclusions.\nAt the chaotic point, Answer 1 selects token \u20188\u2019, resulting in an incorrect response, whereas Answer 2 chooses the token \u2018(\u2019, leading to a correct one.\nDifferent token choices lead to vastly different results, indicating that tokens at chaotic positions should be carefully selected.\nHowever, greedy search only considers the token with the highest probability, ignoring other tokens with considerable probability mass.\nSampling is easily influenced by randomness and falling into irrelevant tokens.\nThough beam search can try multiple tokens, it only considers sentence-level probability maxima.\nIn Figure 1  ###reference_###, Answer 1 has a larger sentence-level probability and a lower perplexity than Answer 2, so beam search still fails to respond correctly.\nThe case in Figure 1  ###reference_### implies that the model\u2019s incorrect answer may not be due to its lack of relevant knowledge but because it does not know how to choose an optimal token at the chaotic point.\nLLMs should not only take the probabilities into account when selecting the token at a chaotic point.\nHowever, existing autoregressive decoding methods are limited to probability, not considering selecting the token from a holistic perspective.\nThis could easily lead to inappropriate tokens and thus result in error propagation.\nTherefore, it is necessary to propose a more advanced decoding method to improve LLMs\u2019 token selection at chaotic points.\nWhen making uncertain decisions, humans often speculate first and then evaluate the speculations, compare them, and make the final choice (Kahneman and Tversky, 2013  ###reference_b17###), a common strategy called slow thinking (Kahneman, 2011  ###reference_b16###) people use when dealing with complex decisions.\nThis approach can assist LLMs in selecting appropriate tokens at chaotic points, but it requires LLMs to be equipped with evaluation ability.\nHowever, Luo et al. (2023  ###reference_b22###) suggest the inherent evaluation ability of existing LLMs is weaker than their generation ability.\nRecently, many studies (Li et al., 2023a  ###reference_b20###; Zhu et al., 2023  ###reference_b42###; Ke et al., 2023  ###reference_b18###) have shown that LLMs can significantly enhance their evaluation ability and thus achieve self-correction (Han et al., 2024  ###reference_b12###) through training.\nIn this paper, we propose Self-Evaluation Decoding, SED, a decoding method for improving uncertain tokens selection at chaotic points for LLMs.\nMirroring human decision-making, when encountering a chaotic point during decoding, LLMs first speculate the results of choosing each token, then evaluate them, and finally select the token with the highest propensity score based on evaluations.\nOur main contributions are summarized as follows:\nWe propose straightforward criteria that help LLMs efficiently identify chaotic points and thus carefully select key tokens during decoding.\nWe propose Self-Evaluation Decoding (SED). Analogous to human decision-making, SED enables LLMs to speculate and evaluate uncertain tokens at chaotic points to select the optimal token and thus improve the generation\u2019s quality compared to other standard decoding methods.\nWe design a simple but effective data synthesis strategy for enhancing LLMs\u2019 evaluation ability based on multi-model ensembling, achieving self-evaluation in SED."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related work",
            "text": "Enhancing evaluation ability for LLMs. Recent studies (Fu et al., 2023  ###reference_b10###; Luo et al., 2023  ###reference_b22###; Zheng et al., 2023  ###reference_b41###) reveal that LLMs\u2019 evaluation ability can be seen as the ability to assess the quality of the object being evaluated (usually a question-answer pair). This ability is determined by the accuracy of the feedback generated by the evaluation model denoted as . The feedback is usually one rating (scalar), analysis (natural language text), or a combination of both. The more accurate the feedback, the better the evaluation. The approaches to enhancing LLM\u2019s evaluation ability can be mainly categorized into prompt-based and tuning-based. The former designs well-structured prompts with detailed instructions to guide the model to evaluate. Since this approach relies entirely on the model\u2019s inherent capability, closed-source models such as GPT-4 (OpenAI, 2023  ###reference_b23###) and PaLM 2 (Anil et al., 2023b  ###reference_b4###) are usually adopted as evaluation models, making the evaluation more expensive. However, being untrained, this method is difficult to adapt to specific downstream tasks and is greatly limited by the model\u2019s inherent flaws (Wang et al., 2023a  ###reference_b30###; Luo et al., 2023  ###reference_b22###). The latter builds training datasets consisting of evaluation feedback to inject evaluation abilities into open-source LLMs through finetuning explicitly (Wang et al., 2023c  ###reference_b32###; b  ###reference_b31###). Many studies have demonstrated that the evaluation models built on direct training can achieve near-human evaluation quality in dialog, open-scene QA, fact verification, and other scenarios. These are effective and low-cost solutions for building evaluation models. In this work, we organically combine the LLMs\u2019 evaluation ability with the other inherent ability, so that LLMs can not only generate but also evaluate the text it generates. Specifically, we enhance the model the evaluation ability through training over the augmented dataset with evaluation feedback included. We will talk about the details in Section 3.4  ###reference_###. Decoding methods for LLMs. Recent studies (Zhao et al., 2023  ###reference_b40###; Xia et al., 2024  ###reference_b34###) have focused on how to improve decoding methods for LLMs further. They can be partitioned into two types: acceleration-oriented and quality-oriented. The former focuses on accelerating the generation of LLMs from an algorithmic perspective. One representative work is speculative decoding (Xia et al., 2024  ###reference_b34###), whose core idea is to use lightweight models with much lower decoding overhead for generation, and heavy-weight models to verify the generated tokens. In this way, the parallelism of decoding substantially increases thus realizing acceleration. The latter focuses on enhancing the quality of LLMs\u2019 generation (Chuang et al., 2023  ###reference_b7###; Chen et al., 2024  ###reference_b6###; Roy et al., 2024  ###reference_b27###). The main operation is to adjust the unnormalized logits during decoding in light of some signals or criteria. For instance, Li et al. (2023b  ###reference_b21###) enhances generation diversity by contrasting the logits of the stronger model over the weaker one. Wang (2024  ###reference_b29###) guides the model to sample correct tokens in the decoding phase by introducing constraints to make the model output more parsable to the computer. Huang et al. (2024  ###reference_b13###) acquires reward signals based on formatted programs or parametric models and then makes the decoding more controllable. In contrast, we utilize signals from the model itself to guide decoding, thereby leveraging LLMs\u2019 evaluation ability to improve the generation. Recent advancements in AI ethics emphasize the importance of fairness in machine learning algorithms (Binns, 2023  ###reference_b5###; Crawford, 2023  ###reference_b8###). Addressing biases inherent in training data is crucial for equitable AI systems. Strategies include algorithmic transparency and rigorous fairness audits to mitigate systemic discrimination (Barocas et al., 2023  ###reference_b3###). Such measures are pivotal for responsible AI development."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Self-evaluation decoding",
            "text": "While previous standard left-to-right decoding methods treat all tokens equally and employ a uniform policy, our proposed self-evaluation decoding (SED) emphasizes the tokens generated at chaotic points, yielding better responses.\nThis paper primarily addresses three critical questions:\n(1) How can a chaotic point be identified?\n(2) Which tokens could be considered at the chaotic point?\n(3) Which token is the most suitable choice at the chaotic point?\nTo this end, our SED method encompasses three modules: chaotic point detection, speculation and evaluation, and feedback backpropagation.\nAutoregressive decoding generates tokens sequentially.\nSpecifically, consider a sequence of generated tokens , where  represents the total number of tokens.\nEach token (with ) is generated from the distribution , which is conditioned on the input  and the preceding tokens , over the whole vocabulary .\nDuring decoding, SED examines the conditional distribution  and calculates the indicator  to determine whether the token  is hard for the model to predict.\nIf ,  is selected through default decoding method (e.g. greedy search).\nIf , the position  is identified as a chaotic point.\nSED selects the top  tokens from  for speculation and allows the model to evaluate these speculations.\nThe model\u2019s self-evaluation is backpropagated and combined with  to guide the selection of the optimal token at position .\nBy iteratively repeating this process, LLMs generate improved responses.\nThe full SED pipeline is shown in Algorithm 1  ###reference_###.\nFigure 2  ###reference_### shows what happens at a chaotic point in one iteration.\nFurther details will be elaborated in subsequent sections."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Chaotic point detection",
            "text": "Entropy-based. Entropy is commonly used to describe the complexity or uncertainty in information theory.\nDuring autoregressive decoding, the model generates the probability distribution  over vocabulary , with the entropy defined as\nis the -th token in .\nWe can use  to measure the model\u2019s uncertainty at position .\nRatio-based. Another more intuitive measurement for uncertainty is the probability ratio:\nand  are the maximum and second-largest probabilities in , with  and  being the corresponding tokens.\nIt follows the clear idea that the model is at least unsure which token between  and  to take at position  when  is close to .\nAfter obtaining  and , we can determine whether position  is a chaotic point.\nDefine indicator  as\nand  are the uncertainty thresholds for entropy-based and ratio-based detection, respectively.\nPosition  is considered a chaotic point for the model if .\nFigure 2  ###reference_### intuitively demonstrates how this operates.\nNote that, we set a branching factor  as the upper limit for the number of detected points to control the algorithm\u2019s complexity."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Speculation and evaluation",
            "text": "If position  is chaotic, we need to speculate first.\nSpecifically, we select top  tokens\n\nwith the highest probabilities from distribution .\nThese tokens are appended to the existing context  to form  new contexts , differing only in the last token .\nThe model then completes these  contexts, resulting in  responses .\nFor evaluation, the model itself evaluates .\nAs we focus on the QA scenario, the evaluation feedback specifically refers to the likelihood of responses being considered correct.\nFormally, given the primal user question  and , the model judge \u2019s correctness as .\nwe extract the model\u2019s output logits and calculate the self-evaluation probability\nThrough , we can tell the model confidence on the speculation  and further determine the token selection propensity at position .\nConsidering the inference cost, the model adopts standard decoding methods during speculation.\nDetails about  are presented in Appendix A  ###reference_###.\nWe will discuss the effects of different methods on speculation in Section 4.6  ###reference_###."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Feedback backpropagation",
            "text": "After Speculation and Evaluation, we get  for  potential candidates .\nBack to position , we fuse\n\nwith the token probabilities\n\nwhich are what we already get during decoding.\n can be viewed as the model\u2019s expected benefit from choosing , while  represents the model\u2019s confidence in choosing  under the current conditions.\n focuses on the future, whereas  focuses on the present.\nWe introduce the concept of propensity score by combining the two to assist the model in choosing the optimal token at the chaotic point.\nFormally, for , we define its propensity score as\nHere,  is the fusion coefficient controlling the fusion between the evaluation feedback and token probability. The higher , the better bidirectional feature token  has. Finally, we select the token with the highest propensity score for the subsequent generation."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Evaluation ability enhancing",
            "text": "Speculation and Evaluation and Feedback Backpropagation represent the forward and backward information flow, respectively.\nIn the former phase, the model reveals the effect of the selected token on the final generation through deduction and evaluation.\nThe model\u2019s self-evaluation feedback then, in turn, improves the token selection at the chaotic point.\nEssentially, self-evaluation decoding enhances the model\u2019s generation through its evaluation.\nPerceptibly, accurate feedback is quite important for self-evaluative decoding to enhance generation.\nRecent studies (Li et al., 2023a  ###reference_b20###; Xu et al., 2024  ###reference_b35###) have shown LLMs can be trained to build good evaluation ability.\nHere we propose a simple but effective data augmentation approach to efficiently construct instruction finetuning data to empower the model\u2019s evaluation ability.\nMulti-model generation ensembling.\nGiven a downstream task dataset  and a collection of LLMs , we use each model in  to answer each question in , which in turn yields model responses  in a CoT format (Wei et al., 2022  ###reference_b33###).\nWe compare these generated responses with the question answers to obtain the final augmented dataset , where the judgment  denotes the correctness of the response .\nIn this way, each  in  receives multiple, CoT-formatted responses from different LLMs with different correctnesses.\nFurthermore, we extract all the correct responses in  thereby constructing the dataset .\n and  are used to train the model\u2019s evaluation and generation abilities on the downstream tasks, respectively.\nTraining.\nSimilar to previous studies, we use  to train the model\u2019s evaluation ability.\nSpecifically, the model learns to output  given  and :\nrepresents the model\u2019s parameters.\nThe model\u2019s generation ability is developed by the loss function:\nHere, we adopt multi-task learning to train the model to build the generation and evaluation abilities simultaneously:\nData sampled from  and  respectively and the ratio is maintained around 1:1.\nThey are mixed to build the final training set.\nMeanwhile, the sampled data from  has approximately the same number of samples with  and . More details about  are available in Appendix A  ###reference_###.\nWe build the dataset described above and train the model in a multi-task learning manner.\nThis method enhances the model\u2019s abilities in two ways.\nBy integrating multiple model generations, representing diverse thoughts on the same question, the model can learn the nuanced differences and thus know how to analyze the question from several perspectives.\nMeanwhile, recent studies (Yu et al., 2023  ###reference_b39###; An et al., 2023  ###reference_b2###) have shown that understanding a question from the opposite is an effective way to enhance the model\u2019s comprehension of the downstream tasks.\nTherefore, learning to judge the correctness of an answer from two sides should also be an effective way to boost the model\u2019s abilities."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiment",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Datasets",
            "text": "We use three datasets: (i) the HotpotQA subset (Yang et al., 2018  ###reference_b37###), processed into the SQuAD format, from the MRQA 2019 Shared Task (Fisch et al., 2019  ###reference_b9###). Retrieval is not needed, allowing LLMs to focus on answering questions in light of the references;\n(ii) XieZhi (Gu et al., 2024  ###reference_b11###), a comprehensive evaluation suite with a multiple-choice format, aiming to test diverse subject knowledge of LLMs;\nand (iii) GSM8K (Cobbe et al., 2021  ###reference_b8###), a popular mathematical reasoning benchmark for evaluating LLMs\u2019 mathematical abilities, consists of high-quality elementary school math word problems.\nThey cover diverse application scenarios of LLMs, such as multi-hop reasoning, multi-disciplinary knowledge comprehension, and mathematical reasoning."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Baselines and metrics",
            "text": "We choose greedy search, beam search, and nuclear sampling as our baselines.\nThey are widely used decoding methods in LLM inference.\nWe also compare with DoLa (Chuang et al., 2023  ###reference_b7###), a contrastive decoding that alleviates hallucinations. falcon-7b-instruct (Penedo et al., 2023  ###reference_b24###), llama2-7b-chat-hf (Touvron et al., 2023  ###reference_b28###), and gemma-7b-it (Anil et al., 2023a  ###reference_b3###) are trained on all datasets through supervised fine-tuning described in Section 3.4  ###reference_###.\nWe evaluate the performance of our proposed self-evaluation decoding by comparing it with other decoding baselines across different fine-tuned models.\nDue to the official open-source code of DoLa currently only supporting the Llama series models, we use it as a baseline only on Llama2.\nWe compare the answers extracted from the model responses with the annotations to calculate accuracy, thereby measuring the performance of different decoding methods.\nWe uniformly sample 1,000 instances for testing from each dataset to manage experimental costs."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Experimental settings",
            "text": "As mentioned in Section 3.4  ###reference_###, we employ multiple LLMs to augment downstream datasets, including baichuan2-7b-chat (Yang et al., 2023  ###reference_b36###), qwen-7b-chat (Bai et al., 2023  ###reference_b5###), mistral-7b-instruct-v0.2 (Jiang et al., 2023  ###reference_b15###) and metamath-mistral-7b (Yu et al., 2023  ###reference_b39###) (specifically for GSM8K).\nPrompt details can be found in Appendix B  ###reference_###.\nFor every question in each dataset, we sample multiple responses from each model and filter out duplicate and incorrectly formatted responses. We extract the answers through regular expressions from those retained responses and compare them with annotations to determine their correctness.\nFinally, according to the operations described in Section 3.4  ###reference_###,  and  are obtained.\nWe train the models on 2 A800 GPUs using ZeRO (Rajbhandari et al., 2020  ###reference_b25###) stage 3 from DeepSpeed (Rasley et al., 2020  ###reference_b26###).\nWe adopt AdamW (Kingma and Ba, 2015  ###reference_b19###) as the optimizer and set the batch size to 8 with a maximum sequence length of 1,024 and one training epoch.\nThe number of beams  of beam search and the temperature  and  of nucleus sampling are set to  and , respectively.\nMore hyperparameters for the experiment are reported in Appendix D  ###reference_###."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Main results",
            "text": "Table 1  ###reference_### presents the three LLMs\u2019 results that employ various decoding methods.\nBeam search and nuclear sampling explore a wider solution space than greedy search, and nuclear sampling increases diversity.\nHowever, compared to greedy search, nuclear sampling performs poorly with large fluctuations, indicating nuclear sampling is not a good choice in QA scenarios with standard answers.\nOn the other hand, beam search is superior to greedy search in most cases, suggesting that a larger solution space helps to get a more accurate response.\nAlleviating hallucination seems not helpful, since DoLa performs poorly consistently.\nBoth entropy-based and ratio-based SED outperform other baselines across multiple models and datasets,\nespecially for falcon and llama2 on XieZhi and GSM8K.\nAs shown, in most cases, the ratio-based SED performs slightly better than the entropy-based.\nWe attribute this to the former\u2019s more specific constraints on token probabilities, which help avoid misjudging chaotic points.\nAdditionally, adjusting the ratio threshold is much more convenient than the entropy threshold.\nThus, ratio-based SED is more practical compared to entropy-based.\nIn subsequent experiments, ratio-based SED is the default implementation.\nWe observe that the improvement in model performance for SED, relative to the baseline models, varies across different datasets.\nSpecifically, the performance increment is more pronounced in the XieZhi and GSM8K compared to HotpotQA.\nChaotic points in GSM8K often occur at numbers and operators, whereas in XieZhi, they frequently appear at the options, as shown in Table 6  ###reference_### and Table D  ###reference_###.\nThe impact of these chaotic points on the accuracy of model responses is significant.\nSED allows the model to effectively select tokens at these positions, thereby avoiding incorrect answers.\nIt is important to note that when  is incorporated into the training process, models\u2019 accuracies increase to varying extents.\nThis suggests that enhancing a model\u2019s evaluation ability could indirectly improve its generation ability, which may be related to the introduction of incorrect responses in  to enable the model to learn how to identify flaws and thereby enhance its ability from the opposite perspective (An et al., 2023  ###reference_b2###).\nMore cases that demonstrate SED\u2019s effect are available in Appendix C  ###reference_###."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Analysis of evaluation quality",
            "text": "Intuitively, the evaluation quality heavily influences the model\u2019s final response.\nIdeally, if the evaluation quality is sufficiently high, the model can generate an optimal response by completely following the evaluation feedback whenever it encounters a chaotic point.\nTo investigate the effect of evaluation quality, we adopt other stronger LLMs as the speculation evaluators to generate more accurate evaluation feedback.\nThe results are shown in Table 2  ###reference_###.\nIn addition to llama2\u2019s self-evaluation, we also use llama3-8b/70b-instruct (AI@Meta, 2024  ###reference_b1###), yi-34b-chat (Young et al., 2024  ###reference_b38###), and qwen1.5-72b-chat (Bai et al., 2023  ###reference_b5###) to evaluate the speculation.\nWe choose XieZhi (Gu et al., 2024  ###reference_b11###) for testing.\nAs mentioned before, the option token in XieZhi is an important chaotic point.\nThe evaluation feedback at the option token can significantly affect the decoding result.\nHere, we use these models to directly answer questions and then obtain their accuracies (Direct gen. in Table 2  ###reference_###) to approximate their evaluation quality, as these accuracies are the most intuitive representation of LLMs\u2019 capabilities.\nResults in Table 2  ###reference_### (SE. gen.) clearly show that llama2\u2019s accuracy is significantly improved when we use a more powerful model to provide more accurate evaluation feedback.\nThis demonstrates evaluation quality is crucial in SED.\nThe more accurate the evaluation feedback, the better SED works.\nThe results also suggest that we could make use of tools to further enhance the evaluation quality and thus further improve the decoding in other tool-friendly application scenarios."
        },
        {
            "section_id": "4.6",
            "parent_section_id": "4",
            "section_name": "Analysis of generation-speculation alignment",
            "text": "Besides evaluation, speculation is also an important step for SED.\nAs described in Section 3.2  ###reference_###, speculation refers to inferring the text that the model will subsequently generate after token  is selected.\nThis requires that the speculation needs to be accurate enough.\nMore specifically, the model needs to have the same token selected at speculation as at generation, i.e., the decoding methods at both stages should be aligned.\nTo investigate the impact of the decoding methods\u2019 alignment, we check various decoding combinations in the generation and speculation phases.\nAs shown in Table 3  ###reference_###, the model performance is improved when the decoding methods at generation and speculation are aligned.\nOtherwise, SED provides a limited gain to the model and sometimes negatively affects the performance.\nThis implies that SED works when the speculation and generation are aligned.\nThe misalignment could give the model a misleading, which in turn affects the model\u2019s final response.\nWe choose the \u2018greedy + greedy\u2019 combination as the implementation for SED in Table 1  ###reference_### since it requires less computation than \u2018beam + beam\u2019 with similar performance."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we propose a decoding method called self-evaluation decoding, SED, to optimize LLMs\u2019 token selection at chaotic points during decoding so that the most appropriate token can be selected.\nSED first detects the chaotic points and then obtains the self-evaluation probability of each token at the chaotic points through two steps: speculation and evaluation, and finally combines the self-evaluation probabilities with the token probabilities to determine the optimal token.\nFinally, experimental results and case studies across various tasks demonstrate the effectiveness of our method."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Limitation",
            "text": "Our method outperforms other baselines across different models on various tasks, illustrating its effectiveness.\nHowever, there remain some limitations.\nFirst, judgment  in  is relatively simple now, lacking of CoT or other enhanced information.\nSecond, the introduction of speculation makes the decoding slower since more computations are required for selecting the optimal token.\nIndeed, it is a trade-off between inference efficiency and quality.\nKahneman (2011  ###reference_b16###) mentions two types of decision-making behaviors: fast thinking and slow thinking.\nSlow thinking is responsible for solving complex problems that are hard for fast thinking, needing additional computational resources for logical and rational thinking.\nIn other words, to optimize LLMs\u2019 token selection at chaotic points, we ought to allow LLMs to think slowly, and exchange the extra inference time for better generation, which is exactly the core contribution of this paper.\nRegarding the additional time cost introduced by slow thinking, we could reduce it through methods such as reusing computation.\nWe leave the optimizations for evaluation ability enhancement and decoding speed as our future work."
        }
    ],
    "url": "http://arxiv.org/html/2405.16552v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.4"
        ],
        "main_experiment_and_results_sections": [
            "4.1",
            "4.2",
            "4.3",
            "4.4",
            "4.5",
            "4.6"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.2",
            "3.4",
            "4.6"
        ]
    },
    "research_context": {
        "paper_id": "2405.16552v1",
        "paper_title": "SED: Self-Evaluation Decoding Enhances Large Language Models for Better Generation",
        "research_background": "Motivation:\nThe primary motivation behind this paper is to address the limitations of existing autoregressive decoding methods in Large Language Models (LLMs) during the sequential token generation process. These methods, such as greedy search, beam search, and sampling, often struggle at chaotic points where the token probability distribution is highly dispersed, leading to uncertain token selection. This uncertainty can result in significant errors, even if the model has the relevant underlying knowledge. The goal is to enhance the LLMs' ability to make better token selection decisions at these chaotic points, thus improving the overall quality of the generated text.\n\nResearch Problem:\nThe core research problem addressed by this paper is how to improve the LLMs\u2019 token selection mechanism at chaotic points to avoid error propagation and optimize the generation process. Current decoding methods rely heavily on probability-based selection, which does not adequately consider the broader context and implications of token choices at these turbulent points. The challenge is to design a new decoding method that enables LLMs to evaluate and select tokens more holistically, thereby reducing errors and enhancing the model's performance in generating coherent and accurate text.\n\nRelevant Prior Work:\n1. **Decoding Methods in LLMs**:\n   - **Ippolito et al. (2019)**: Discuss existing autoregressive decoding methods, classified into search-based methods (e.g., greedy search, beam search) and sampling-based methods (e.g., nuclear sampling). These methods primarily focus on maximizing the probability of generated text or sampling from a probability distribution but have inherent limitations in dealing with uncertain tokens at chaotic points.\n  \n2. **Human Decision-Making Analogy**:\n   - **Kahneman and Tversky (2013)**, **Kahneman (2011)**: Introduce the concept of slow thinking used by humans during complex decisions by speculating and evaluating before making a final choice. This analogy inspired the approach proposed in this paper for LLMs to enhance token selection at chaotic points through a self-evaluation process.\n\n3. **Evaluation Ability in LLMs**:\n   - **Luo et al. (2023)**: Highlight that existing LLMs have weaker evaluation abilities compared to their generation abilities, suggesting a potential area for enhancement.\n\n4. **Improving Evaluation Ability**:\n   - Various studies, including **Li et al. (2023a)**, **Zhu et al. (2023)**, **Ke et al. (2023)**, and **Han et al. (2024)**, demonstrate that LLMs can significantly improve their evaluation abilities through training and achieve self-correction. These findings are crucial in supporting the development of the Self-Evaluation Decoding (SED) method proposed in this paper.\n\nIn summary, by building on these prior works, the paper proposes a novel Self-Evaluation Decoding (SED) method that mirrors human decision-making processes to enhance LLMs' token selection at chaotic points, improving the generation quality beyond what is achievable with current standard decoding methods.",
        "methodology": "**Proposed Method: Self-Evaluation Decoding (SED)**\n\nThe Self-Evaluation Decoding (SED) method introduces a novel approach to enhance the generation capabilities of large language models by focusing on chaotic points during the token generation process. Unlike traditional left-to-right decoding methods that treat all tokens uniformly, SED emphasizes chaotic points, where the model is uncertain or produces weak predictions, and applies a tailored strategy to improve token selection. The methodology addresses three critical questions:\n\n1. **Identification of Chaotic Points**:\n   - Determining when the model is struggling to predict a token effectively.\n\n2. **Consideration of Tokens at Chaotic Points**:\n   - Deciding which tokens to hypothesize and evaluate when a chaotic point is detected.\n\n3. **Selection of the Optimal Token**:\n   - Choosing the most suitable token from the speculated options during chaotic points.\n\nTo achieve these objectives, SED involves three key modules:\n\n1. **Chaotic Point Detection**:\n   - Autoregressive decoding generates tokens sequentially, denoted as a sequence \\(\\boldsymbol{y} = (y_1, ..., y_T)\\), where \\(T\\) is the total number of tokens.\n   - Each token \\(y_t\\) (for \\(t = 1, ... , T\\)) is generated from a conditional probability distribution \\(P_{\\theta}(y_t|x, y_{<t})\\), based on the input \\(x\\) and preceding tokens \\(y_{<t})\\).\n   - During the decoding process, SED examines the conditional distribution \\(P_{\\theta}(y_t|x, y_{<t})\\) and computes an indicator \\(\\mathbb{I}\\) to assess whether token \\(y_t\\) is challenging for the model to predict.\n   - If \\(\\mathbb{I}(y_t) == 0\\), \\(y_t\\) is generated using the default decoding method (e.g., greedy search).\n   - If \\(\\mathbb{I}(y_t) == 1\\), the position \\(t\\) is identified as a chaotic point.\n\n2. **Speculation and Evaluation**:\n   - At a chaotic point, SED selects the top \\(k\\) tokens from the distribution \\(P_{\\theta}(y_t|x, y_{<t})\\) for speculation.\n   - The model evaluates these top \\(k\\) speculations, essentially performing a self-assessment of the candidate tokens.\n\n3. **Feedback Backpropagation**:\n   - The model's self-evaluation results are backpropagated to adjust and combine with \\(P_{\\theta}(y_t|x, y_{<t})\\).\n   - This enhanced feedback guides the selection of the optimal token at position \\(t\\).\n   - The iterative application of this process leads to the generation of improved responses.\n\nBy incorporating these modules, SED systematically enhances large language models' generation performance, particularly by refining token selection at identified chaotic points. More detailed explanations are provided in the subsequent sections of the paper.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\n#### Datasets:\n1. **HotpotQA subset (Yang et al., 2018)**:\n   - Processed into the SQuAD format.\n   - From the MRQA 2019 Shared Task (Fisch et al., 2019).\n   - Designed for answering questions in light of provided references.\n\n2. **XieZhi (Gu et al., 2024)**:\n   - A comprehensive evaluation suite.\n   - Multiple-choice format aimed at testing diverse subject knowledge of LLMs.\n\n3. **GSM8K (Cobbe et al., 2021)**:\n   - A mathematical reasoning benchmark.\n   - Consists of high-quality elementary school math word problems.\n   - Evaluates the mathematical abilities of LLMs.\n\nThese datasets cover varying LLM application scenarios including multi-hop reasoning, multi-disciplinary knowledge comprehension, and mathematical reasoning.\n\n#### Baselines:\nAlthough specific baselines are not explicitly named in the provided text, it can be inferred that established models and approaches relevant to the described datasets (existing performance standards in HotpotQA, XieZhi, and GSM8K) serve as the benchmarks against which the Self-Evaluation Decoding (SED) method's effectiveness is gauged.\n\n#### Evaluation Metrics:\nThe specific evaluation metrics are not detailed in the text. However, it is standard practice to use:\n- **Accuracy and F1 score** for HotpotQA subset, aligning with common practices for QA benchmarks.\n- **Accuracy** for the multiple-choice format of XieZhi.\n- **Correctness rate** for mathematical problem-solving in GSM8K.\n\n#### Main Experimental Results:\nThe results would highlight how the Self-Evaluation Decoding (SED) method enhances LLM performance across the diverse tasks posed by the three datasets, specifically demonstrating improvements in multi-hop reasoning, subject knowledge comprehension, and mathematical problem-solving abilities of LLMs. Quantitative results (e.g., improvements in F1 scores, accuracy rates, and correctness rates) would be presented to substantiate the effectiveness of SED.\n\nThe detailed experimental results would describe comparative performances, emphasizing the enhancements brought by the SED method over baseline models on each dataset."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To enhance a model's generation through effective speculation and self-evaluation, focusing on how accurate speculation impacts the overall performance of Large Language Models (LLMs).",
            "experiment_process": "In this study, the authors investigate the effects of aligning various decoding methods at both the speculation and generation stages. The speculation process involves selecting top-k tokens with the highest probabilities and forming new contexts, which are then completed by the model to generate corresponding responses. The model evaluates the correctness of these responses to determine token selection propensity at chaotic points. The study experiments with different combinations of decoding strategies (e.g., 'greedy + greedy' and 'beam + beam') during the speculation and generation phases to assess the importance of consistent methods across both stages. The performance metrics used are based on the model's ability to provide correct responses to user queries.",
            "result_discussion": "The results, presented in Table 3, show that model performance improves significantly when the decoding methods at generation and speculation are aligned. Misalignment between these methods can limit the benefits of the Self-Evaluation Decoding (SED) and can even negatively impact performance. The 'greedy + greedy' combination was chosen for its efficiency and comparable performance to 'beam + beam'. This implies that accurate speculation is crucial for the effective implementation of SED, as misaligned methods can mislead the model and degrade its final responses.",
            "ablation_id": "2405.16552v1.No1"
        },
        {
            "research_objective": "To improve a model's evaluation ability for better subsequent token selection by employing data augmentation and multi-task learning.",
            "experiment_process": "The researchers propose an evaluation enhancement technique via data augmentation and multi-task learning. They create an augmented dataset (D_aug) by compiling responses from multiple LLMs in a Chain-of-Thought (CoT) manner and noting their correctness against a downstream task dataset (D_task). This dataset includes both correct and incorrect responses. Two datasets are constructed: D for training evaluation abilities (with judgments on response correctness) and D' for training generation abilities (with only correct responses). The model is trained to output evaluation scores (correct/incorrect judgments) while also improving the generation capability, balancing data from both datasets in a 1:1 ratio.",
            "result_discussion": "The multi-task learning approach significantly enhances both the evaluation and generation abilities of the model. By incorporating multiple generations from different models, the LLMs learn to understand nuanced differences and improve their analytical skills. Furthermore, understanding a question from multiple perspectives and judging the correctness of answers from both sides boosts the model\u2019s comprehension and generation capabilities in downstream tasks.",
            "ablation_id": "2405.16552v1.No2"
        }
    ]
}