{
    "title": "FastSAG: Towards Fast Non-Autoregressive Singing Accompaniment Generation",
    "abstract": "Singing Accompaniment Generation (SAG), which generates instrumental music to accompany input vocals, is crucial to developing human-AI symbiotic art creation systems. The state-of-the-art method, SingSong, utilizes a multi-stage autoregressive (AR) model for SAG, however, this method is extremely slow as it generates semantic and acoustic tokens recursively, and this makes it impossible for real-time applications. In this paper, we aim to develop a Fast SAG method that can create high-quality and coherent accompaniments. A non-AR diffusion-based framework is developed, which by carefully designing the conditions inferred from the vocal signals, generates the Mel spectrogram of the target accompaniment directly. With diffusion and Mel spectrogram modeling, the proposed method significantly simplifies the AR token-based SingSong framework, and largely accelerates the generation. We also design semantic projection, prior projection blocks as well as a set of loss functions, to ensure the generated accompaniment has semantic and rhythm coherence with the vocal signal. By intensive experimental studies, we demonstrate that the proposed method can generate better samples than SingSong, and accelerate the generation by at least 30 times. Audio samples and code are available at https://fastsag.github.io/.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Singing Accompaniment Generation (SAG) aims to create instrumental audio tracks that harmonize with vocal performances. This technique empowers individuals to compose complete songs by merely recording their singing. Since the human voice is often regarded as the most intuitive musical instrument, SAG allows people to express their musicality without the need for additional instrumental skills.\nEarly approaches for SAG are based on retrieval, for instance, the Microsoft Songsmith Simon et al. (2008  ###reference_b33###). The Songsmith extracts pitches of input vocals, then predicts the symbolic chord label sequences that complement the melody, and finally retrieves suitable symbolic instrumental accompaniments from datasets given chord label sequences. An intrinsic limitation of retrieval-based methods is that they actually could not generate new music pieces creatively, therefore, the resulting pieces are not optimal for the vocal inputs.\nLearning-driven approaches are also developed, which generally perform Audio2Audio generation. As a related work, in Wu et al. (2022  ###reference_b38###), the Jukedrummer is proposed to generate a drum audio track based on drumless audio tracks, however, this method cannot perform SAG directly. In Donahue et al. (2023  ###reference_b9###), the learning-driven SAG is for the first time developed, which, similar to MusicLM Agostinelli et al. (2023  ###reference_b1###) and AudioLM Borsos et al. (2023  ###reference_b2###), is basically based on autoregressive (AR) language models (LMs) to learn the token associations between the vocals and accompaniments. Multiple LMs are involved in the AR generation, which includes a) semantic LM b) coarse acoustic LM c) fine acoustic LM. The resulting tokens are finally decoded to the audios through the Soundstream Zeghidour et al. (2021  ###reference_b41###) decoder. The main drawback of SingSong is the extremely slow generation speed. Since many AR-based LMs are adopted, the whole generation pipeline becomes complicated and in practice, one second of accompaniment needs dozens of seconds for generation on the Nvidia A100 GPU.\nIn this paper, we propose FastSAG, a diffusion-based method for fast, coherent, and high-quality SAG. Rather than using AR-based LMs, we design a non-AR diffusion model that directly creates the Mel spectrogram of the accompaniment given specially designed input conditions. In this way, the generation pipeline is substantially simplified and the generation is largely accelerated. To ensure the semantic and rhythm coherence between vocals and accompaniments, when generating the conditions for diffusion, we propose a semantic projection block for semantic alignment, and a prior projection block to enhance the frame-level alignment and control. A set of loss functions are also designed to further improve the semantic and rhythm alignment. Experimental results show that the proposed FastSAG could produce better accompaniments than the SingSong, while accelerating the generation speed by more than 30 times, making the generation to the level of real-time factor smaller than 1. The key contributions are briefly summarized as:\nWe design a diffusion-based non-AR framework for SAG, which largely simplifies the SAG pipeline as compared with SingSong;\nWe propose semantic projection block, prior projection block, and a set of loss functions to ensure the rhythmic coherence between vocals and generated accompaniments;\nThe experimental results demonstrate that the proposed FastSAG significantly accelerates the generation and produces better samples as compared with the baseline.\n\n###figure_1###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Works",
            "text": "Besides the SAG which we have reviewed in the introduction, here, we further discuss the accompaniment generation given instrumental inputs, and the audio generation methods, which would facilitate the discussions in subsequent sections.\n\nArtificial intelligence in healthcare has emerged as a powerful tool for predicting patient outcomes. Recent studies highlight the use of machine learning algorithms to analyze complex datasets, improving the accuracy of forecasts related to patient recovery and treatment efficacy. AI tools optimize healthcare delivery by providing personalized insights, enhancing decision-making processes and patient management."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Accompaniment Generation Given Instrumental Inputs",
            "text": "For symbolic music, the melody track and the remained accompaniment tracks could be separated easily, and there are some works for symbolic accompaniment generation that take the melody track as input. PopMAG Ren et al. (2020  ###reference_b29###) is proposed to generate the accompaniment track which consists of drum, piano, string, guitar, and bass track based on MuMIDI representation, by using Transformer-XL Dai et al. (2019  ###reference_b5###) as the backbone. MuseFlow Ding and Cui (2023  ###reference_b8###) uses the flow model to generate accompaniment based on the revised piano-roll representation. In Wang et al. (2022  ###reference_b36###), the SongDriver is proposed for real-time accompaniment generation, which consists of two phases: arrangement phase and prediction phase. The above methods generally aim to generate symbolic music. In Mariani et al. (2023  ###reference_b23###), by still relying on the symbolic MIDI dataset, the multi-track accompaniment generation is performed in the audio domain based on the rendered multi-track audio waveforms.\n\nIn healthcare, AI predictive models have gained traction in forecasting patient outcomes by leveraging vast medical datasets. Initiatives using machine learning techniques, such as neural networks and decision trees, have demonstrated improved accuracy in predicting disease progression, patient survival rates, and potential complications, offering enhanced personalized treatment strategies."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Audio Generation",
            "text": "SAG is a type of audio generation task. Significant progress has been made in generating general audio, music, and speech with the advancement of generative models. Now we discuss the methods in terms of representations used: a) raw audio waveform, b) hand-crafted representation, and c) neural representation. The raw audio waveform captures the fine-grained details of audio and serves as the direct representation of audio data. Wavenet Oord et al. (2016  ###reference_b24###) employs dilated convolutions to capture long-range dependencies in audio signals, enabling the generation of high-quality and realistic audio. WaveGlow Prenger et al. (2019  ###reference_b26###) achieves high-quality audio synthesis using a flow-based approach, which allows for efficient sampling and parallel processing. Working directly with raw waveforms can be challenging due to their high dimensionality and complex temporal dependencies. The most common hand-crafted representation for audio generation is the Mel spectrogram. The audio is generally produced by first relying on an acoustic model to produce the Mel spectrogram given input controls (e.g., text for speech synthesis, and prompts for general audio and music generation), and then a vocoder to convert the Mel spectrogram to the audio domain. For speech synthesis, typical acoustic models include FastSpeech Ren et al. (2019  ###reference_b28###), GradTTS Popov et al. (2021  ###reference_b25###), and CoMoSpeech Ye et al. (2023  ###reference_b40###), and for general audio and music generation, acoustic models such as Riffusion111https://github.com/riffusion/riffusion  ###reference_###, Mousai Schneider et al. (2023  ###reference_b31###), Noise2Music Huang et al. (2023a  ###reference_b12###), Make-An-Audio Huang et al. (2023b  ###reference_b13###), and AudioLDM Liu et al. (2023  ###reference_b22###) are developed. HiFi-GAN Kong et al. (2020  ###reference_b16###), BigvGAN Lee et al. (2023  ###reference_b19###), VOCOS Siuzdak (2023  ###reference_b34###) are popular vocoders to recover audio waveform. Another type of representation is the neural tokens (such as SoundStream Zeghidour et al. (2021  ###reference_b41###), Encodec D\u00e9fossez et al. (2022  ###reference_b7###), DAC Kumar et al. (2023  ###reference_b18###)), which learn a discrete representation from the audio waveform. Further, AR model and LMs could be used to model the evolutions of these discrete tokens in chain rule, which leads to a series of token-based audio generation methods, including unconditional audio generation (audioLM Borsos et al. (2023  ###reference_b2###)), text-to-music generation (MusicLM Agostinelli et al. (2023  ###reference_b1###), MusicGen Copet et al. (2023  ###reference_b4###)), text-to-audio (AudioGen Kreuk et al. (2022  ###reference_b17###)), accompaniment generation (SingSong Donahue et al. (2023  ###reference_b9###)), text-to-speech TTS (Wang et al. (2023  ###reference_b37###)). In addition, the continuous representation from residual VQ (RVQ) codebooks could also be modeled using the diffusion model, for instance, in NaturalSpeech2 Shen et al. (2023  ###reference_b32###).\n\nAI in healthcare has rapidly advanced, particularly in predicting patient outcomes. Recent studies leverage machine learning and deep learning algorithms to analyze complex datasets, such as electronic health records, to forecast disease progression, treatment responses, and patient survival rates, showing enhanced accuracy and clinical applicability."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "EDM Formulation",
            "text": "In this section, we introduce the EDM diffusion model Karras et al. (2022  ###reference_b15###), which will be used as a conditional probability model illustrated in Figure 1  ###reference_### (c).\nSupposing the data distribution is  and considering the family of mollified distributions  which is obtained by adding Gaussian noise  to the data, the idea of EDM diffusion model is to randomly sample a noisy sample , and further sequentially denoise it into sample  with noise levels . The number of sampling steps is denoted as , and the final outcome of this process, , exhibits a distribution that aligns with the original data. This diffusion process belongs to the variance exploding family Song et al. (2020  ###reference_b35###).\nThe stochastic differential equation (SDE) of diffusion is expressed as Song et al. (2020  ###reference_b35###):\nwhere  and  mean drift and diffusion coefficients respectively, and  is the standard Wiener process.\nThe above process corresponds to a probability flow ordinary differential equation (ODE), and in EDM Karras et al. (2022  ###reference_b15###), a schedule of  is chosen to specify the desired noise level at time . Then the ODE is expressed as\nwhich defines the process evolving a sample  from time  to  yields a sample , and  to  can be either forward or reverse in time. The  denotes a time derivative, and  is the score function Song et al. (2020  ###reference_b35###). As long as the score function is known, the probability flow ODE in (2  ###reference_###) can be used for sampling.\nOne natural way for  scheduling is , which corresponds to the constant-speed heat diffusion. However, Karras et al. (2022  ###reference_b15###) shows that it is not convenient practically. EDM adopts another schedule, which uses\nfor training, and\nfor sampling. In our setting of EDM, , , , , and  which controls how much the steps near  are shortened at the expense of longer steps near .\nSupposing  is the denoising function that minimizes the expected L2 denoising error for samples drawn from  separately for every , i.e.,\nthe score function can be written as,\nwhere  if the training sample and  is the noise.\nIn the diffusion model, the denoiser  can be implemented as a neural network  and  for unconditional and conditional diffusion respectively, where  is the condition. Similar to the EDM setting,\nwhere  can be any well-designed neural network, for example, Wavenet Oord et al. (2016  ###reference_b24###) is used in Liu et al. (2022  ###reference_b21###) and U-net Ronneberger et al. (2015  ###reference_b30###) is used in Popov et al. (2021  ###reference_b25###).  and  are used to control the skip connect and the magnitudes of , respectively. The  and  can be expressed as\nwhere  and  denoting the smallest time interval during sampling."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Proposed FastSAG",
            "text": "In this section, we introduce our proposed method FastSAG for singing accompaniment generation. As shown in Figure. 1  ###reference_###, it contains three main parts: source separation for data processing, condition block for computing condition of the diffusion model, and EDM-based SDE solver for generating the Mel spectrogram of accompaniment."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Overview",
            "text": "Most public songs on the internet are audio mixes of vocals and accompaniments, and we denote the mixture signal as , the vocal as , and the accompaniment as  respectively. To obtain the paired vocal and accompaniment data, source separation, such as Demucs D\u00e9fossez et al. (2019  ###reference_b6###), is applied to the mixture signals, and resulting pseudo vocal-accompaniment pairs  are acquired. Similar to SingSong Donahue et al. (2023  ###reference_b9###), slight white Gaussian noise  is added to the vocal input to mitigate source separation artifacts. The core of SAG is building the conditional probabilistic model .\nDifferent from SingSong which uses the LM for discrete audio tokens and is time-consuming, we design a diffusion-based non-AR framework here. In the continuous Mel spectrogram space, the EDM introduced in Section 3 takes the conditions containing the semantic and rhythmic information to generate the Mel spectrogram of the accompaniment, denoted as . The resulting Mel spectrogram is transformed to the audio domain by using Bigvgan Lee et al. (2023  ###reference_b19###) as a vocoder."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Condition Block",
            "text": "###figure_2### The condition block takes the vocal signal as input, and incorporates two cascading blocks: a) the Semantic Projection Block, which is responsible for mapping high-level semantic features, and b) the Prior Projection Block, which generates frame-level aligned conditions. The rationale behind this cascading design is that directly modeling the frame-level acoustic relationship is challenging, while modeling the high-level relationship is comparatively easier. Hence, by first capturing the high-level semantic features and then generating frame-level conditions based on them, we can effectively address the complexity of modeling the frame-level acoustic relationship."
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1 Semantic Projection Block",
            "text": "Denoting the input vocal audio signal as  with  as the number of frames in waveform format, the high-level semantic feature  is extracted using MERT Li et al. (2023  ###reference_b20###), where  is the frame number of MERT feature and  is the dimension. The Semantic Projection Block consists of a neural network (for example, Wavenet Oord et al. (2016  ###reference_b24###)) to obtain the predicted semantic feature of the accompaniment ."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2 Prior Projection Block",
            "text": "A high-level semantic feature is utilized for generating frame-level roughly aligned prior through the Prior Projection Block further. Inspired by Grad-TTS Popov et al. (2021  ###reference_b25###), they used the frame-aligned prior as the condition to diffusion model by using an aligner to transform phoneme-level feature to frame-level prior. Illustrated as Figure 2  ###reference_###, our Prior Projection Block consists of a resampling module and an encoder module.\nConsidering that the shape (time resolution and feature dimension) of the semantic feature may differ from that of the desired frame-level prior, we design two kinds of resampling modules. The first one is using bi-linear interpolation directly. The second one is that we utilize Perceiver-IO Jaegle et al. (2022  ###reference_b14###) as the resampling module to obtain the resampled semantic feature , where we mixed the vocal semantic feature  and predicted accompaniment semantic  by just adding to acquire better performance.  and  denote the length and number bin of Mel spectrogram, respectively.\nThe computation of PerceiverIO is described as follows. The input of Perceiver-IO  is then encoded into a latent space by cross attention, yielding , where  is a learnable variable. Multiple self-attention layers further convert  to another hidden embedding , and then  is decoded to output array  with cross attention, where  is a learnable output query. The output array  is the desired resampled semantic feature  introduced. In our experiments, we set .\nFurther, the resampled semantic feature  is processed to obtain a Mel spectrogram-like prior  through encoder module, which serves as the condition for diffusion model latter. We use Wavenet Oord et al. (2016  ###reference_b24###) as the encoder module again."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Conditional Denoiser",
            "text": "Similar to Grad-TTS Popov et al. (2021  ###reference_b25###), we use Unet2d Ronneberger et al. (2015  ###reference_b30###) as denoiser, but with a slight modification. As introduced in Section 3, different level of noise is sampled from , we have no explicit timesteps  controlling noise level. Therefore, we replace  with  and take it as one of the inputs to denoiser , where  is the noised Mel spectrogram of accompaniment,  is the condition computed in last subsection, and  is the denoised output."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Loss Function",
            "text": "To make the model generate coherent and harmonious accompaniments, by taking the separated accompaniment  as ground truths, we design three different loss functions, including the semantic loss, prior loss, and diffusion loss. We extract semantic ground truths  and Mel spectrogram ground truths  of accompaniment music through MERT Li et al. (2023  ###reference_b20###) and Bigvgan Lee et al. (2023  ###reference_b19###), respectively, the loss functions are computed as below.\nSemantic Loss. The goal of the semantic loss is to establish a high-level semantic relationship between the vocal and accompaniment tracks. We believe it is much easier than constructing the acoustic mapping directly, and it is defined as:\nPrior Loss. The purpose of the prior loss is to establish a rough frame-level alignment between the Mel spectrogram of the accompaniment track. It is defined as,\nDiffusion Loss. While the condition block alone can generate a rough accompaniment, the diffusion model is employed to produce a refined accompaniment by conditioning it on the prior. The diffusion loss is defined as,\nThe final loss function is a combination of semantic loss, prior loss, and diffusion loss, as\nwhere ,  and  are the weights of corresponding loss terms. In our experiments, we set ,  and ."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": ""
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Dataset",
            "text": "Here, we discuss the data collection and processing, as well as the composition of the training and evaluation datasets.\nData Collection and Processing. We collected over 300k songs from public sources on the internet, most of which are Chinese and English songs. By using Demucs D\u00e9fossez et al. (2019  ###reference_b6###), we obtain mono vocal-accompaniment pairs with a sampling rate of 44.1 kHz. Then, filtering is applied to keep sample pairs with clear vocals and accompaniments, which is done by first obtaining 10-second paired clips and then keeping pairs where both the vocal and accompaniment have a peak RMS amplitude over -25dB. As a result, we obtain a collection of over 1.2 million pairs of 10-second clips, totaling more than 3000 hours.\nTraining Dataset. We divide the 1.2 million pairs of 10-second clips into training and (in-domain) evaluation datasets, with 2,000 samples for in-domain evaluation. To ensure that the evaluation samples are not seen during training, samples of each song are exclusively allocated to either the training dataset or the evaluation dataset.\nEvaluation Dataset. We construct two kinds of evaluation datasets: the in-domain evaluation dataset, which has been introduced, and the zero-shot evaluation dataset. Although the in-domain evaluation dataset is not used for training, they follow a similar distribution. An out-of-domain zero-shot evaluation dataset is additionally built from the MUSDB18 dataset Rafii et al. (2017  ###reference_b27###), which is also the evaluation dataset of SingSong Donahue et al. (2023  ###reference_b9###). Following the same procedures for training data processing on the MUSDB18 test dataset, we obtain 348 paired clips. The MUSDB18 training dataset is not used for training so the MUSDB18 test dataset is zero-shot."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Baselines and Implementation Details",
            "text": "We use two baseline methods, SingSong and RandSong, for comparison. As we could not find source codes for baselines, we provided implementation details for SingSong, RandSong, and the proposed FastSAG. Both SingSong and FastSAG are trained on the same dataset to ensure a fair comparison.\nSingSong. We implemented SingSong based on the codebase of open-musiclm 222https://github.com/zhvng/open-musiclm.git  ###reference_###. Same with open-musiclm, we utilize Encodec D\u00e9fossez et al. (2022  ###reference_b7###) as a replacement for SoundStream Zeghidour et al. (2021  ###reference_b41###), and MERT Li et al. (2023  ###reference_b20###) as a replacement for w2v-BERT Chung et al. (2021  ###reference_b3###). The main reason for replacement is that we can not find open source code and pre-trained model of SoundStream and w2v-BERT when we re-implement SingSong. We trained the models of the semantic stage, coarse acoustic stage, and fine acoustic stage separately for 240k, 130k, and 240k steps respectively on a single NVIDIA A100 GPU with 80GB memory. The batch size and grad accumulation steps are (24, 1) for the semantic stage model, (12, 4) for the coarse acoustic model, and (4, 4) for the fine acoustic model. The Encodec D\u00e9fossez et al. (2022  ###reference_b7###) employs an RVQ scheme that generates 8-dimensional acoustic codes at a rate of 75 Hz. The first 3 dimensions correspond to the coarse acoustic codes, while the remaining 5 dimensions represent the fine acoustic codes.\nRandSong. The RandSong is a weak baseline to examine the importance and sensitiveness of harmony and coherence between vocal voice and instrumental accompaniment audio. Firstly, we construct a big candidate set of instrumental accompaniment audio (20,000 pieces) from human-composed music separated by Demucs D\u00e9fossez et al. (2019  ###reference_b6###). Then for a given vocal voice, we randomly choose one accompaniment from the candidate set. The samples from RandSong are only used for subjective evaluation.\nFastSAG. We trained our FastSAG on a single NVIDIA A100 GPU with 80GB memory for 0.5M steps using Adam optimizer with a constant learning rate of 0.0001 and batch size of 28. For the vocoder, we utilize Bigvgan Lee et al. (2023  ###reference_b19###), which operates at a sampling rate of 24kHz. The Mel spectrogram used by Bigvgan consists of 100 bins and is computed at a rate of 93.75Hz. We normalize the logarithmic Mel spectrogram to -1 to 1, instead of using the original one, because the original logarithmic Mel spectrogram ranges from -12 to 2, which is more difficult to model. The normalization and de-normalization are described as:\nwhere ,  and  denote the original spectrogram, normalized spectrogram and de-normalized spectrogram, respectively, ,  are constants. During the inference process, we employ a first-order ODE solver with a total of 50 sampling steps."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Evaluation Metrics",
            "text": "We examine the proposed model and baselines through objective evaluation and subjective evaluations.\nObjective evaluation. SingSong employed FAD-vggish as an objective evaluation metric, utilizing embeddings from the VGGish audio classifier Hershey et al. (2017  ###reference_b11###). However, relying solely on FAD-vggish may not accurately reflect the true quality of the generated music.\nTo address this, we utilize the FADTK Gui et al. (2023  ###reference_b10###), an extended FAD toolkit that includes various embedding extractors specifically designed for evaluating generative music. We incorporate three types of embedding extractors: VGGish Hershey et al. (2017  ###reference_b11###), MERT Li et al. (2023  ###reference_b20###), and clap-laion-music Wu et al. (2023  ###reference_b39###). The latter two are trained on large music datasets, resulting in the evaluation metrics , , and . For MERT embeddings, we conduct experiments using different layers, specifically the 4th, 7th, and 11th layers, denoted as , , and , respectively.\nWhen calculating FADs, ground truth mixtures may be degraded using the corresponding vocoder or codec, to eliminate the impact of sampling rate, vocoders, and codecs on the data distribution.\nIn addition to evaluating the quality of the generated music, we also assess the speed of the music generation process using the real-time factor (RTF), which is calculated as the ratio between the total time taken for audio generation and the duration of the generated audio.\nSubjective evaluation. To assess the quality of the generated music, we employ Mean Opinion Score (MOS) through human evaluation. We invited professional 15 listeners to rate the harmony and coherence of the testing samples on a scale ranging from 1 to 5. The testing samples include mixtures composed by humans, mixtures generated by SingSong, mixtures with randomly selected accompaniments (referred to as RandSong), and mixtures generated by FastSAG."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Experiment Results",
            "text": ""
        },
        {
            "section_id": "5.4.1",
            "parent_section_id": "5.4",
            "section_name": "5.4.1 A. Objective Evaluation",
            "text": "Table 1  ###reference_### shows the objective evaluation results on the zero-shot MUSDB18 test dataset, including comparison with baseline SingSong and some ablation studies.\nZero-shot Evaluation. For baseline SingSong, we analyze two-stage (semantic + coarse acoustic) and three-stage (semantic + coarse acoustic + fine acoustic) inference results. We find two-stage results are better than three-stage ones both in FAD and RTF metrics in our re-implementation, and the best  0.8632 is better than the original paper reported (0.96). Our method FastSAG with bi-linear interpolation as a resampling module achieves better results both in all FAD metrics and over 30 times faster than two-stage SingSong and over 140 times faster than three-stage SingSong. Our method FastSAG with bi-linear interpolation as the resampling module results in improved performance across all FAD metrics. Additionally, it achieves a significant acceleration, being over 30 times faster than the two-stage SingSong method and over 140 times faster than the three-stage SingSong method.\nAblation Study. We also conduct several ablation studies to check the function of different settings, consisting of three aspects:\nNormalization is important. Using a normalized Mel spectrogram achieves better performance than an unnormalized one in all FAD metrics.\nSemantic as the condition is better. We conduct experiments with different condition types: only semantic feature, only Mel spectrogram feature, and mixed feature of semantic and Mel spectrogram feature. We can see using only the semantic feature as the condition achieves better performance in all FAD metrics, which is consistent with the findings of SingSong.\nDiscussion of resampling module. Our findings indicate that the utilization of bi-linear interpolation leads to improved FAD scores in  and . However, the FAD scores are comparatively lower when using MERT as the feature extractor. To further assess their performance, we will conduct subjective evaluations.\nIn-Domain Evaluation. We additionally assess the performance of the in-domain test dataset. As shown in Table 2  ###reference_###, SingSong achieves the highest FADVGGish score, but it performs relatively worse in terms of FAD score when using other feature extractors. While our method utilizing interpolation as the resampling module achieves improved FADVGGish and  compared to the approach employing Perveiver-IO as the resampling module, it exhibits poorer performance on the in-domain test dataset."
        },
        {
            "section_id": "5.4.2",
            "parent_section_id": "5.4",
            "section_name": "5.4.2 B. Subjective Evaluation",
            "text": "Table 3  ###reference_### presents the subjective evaluation results. For this evaluation, 100 testing samples were randomly chosen from both the in-domain test dataset and the zero-shot MUSDB18 test dataset, maintaining a 2:1 ratio respectively. We engaged 15 participants in the evaluation process, with each individual assessing 20 samples randomly picked from the 100 selected samples. The results indicate that RandSong received the lowest MOS score, highlighting the importance of harmony and coherence in human music perception. Furthermore, our FastSAG method, whether employing interpolation or the Perceiver-IO as the resampling module, consistently outperforms the baseline SingSong. However, using the Perceiver-IO as the resampling module results in the closest approximation to human-composed music.\n\n###figure_3###"
        },
        {
            "section_id": "5.4.3",
            "parent_section_id": "5.4",
            "section_name": "5.4.3 C. Discussion on Prior Loss and Diffusion Loss",
            "text": "Figure 3  ###reference_### (a) and (b) display the Mel spectrograms of the singing voice and corresponding accompaniment, respectively. There is a significant gap between their Mel spectrograms. (c) represents the predicted prior, which is a first step towards being closely aligned with the accompaniment. As seen in the figure, it is roughly aligned. (d) is generated by the diffusion model, conditioned on (c), and presents a more detailed approximation of the accompaniment. In summary, our framework consists of two stages. The first stage involves a single encoder-decoder, which is not sufficient for generating accompaniment. The second stage refines the output using a diffusion model. The complete framework serves as a conditional probabilistic model for the complex mapping between singing voice and accompaniment."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion and Future Work",
            "text": "In this paper, we introduce FastSAG, a diffusion-based non-autoregressive method for singing accompaniment generation. This approach not only achieves higher generation speeds but also maintains more harmonious and coherent accompaniments, as demonstrated by both objective and subjective evaluations.\nThere are, however, areas for improvement. First, the generated audio quality could be better. The low audio quality may be due to several factors: a) the low sampling rate of the training data; b) the degradation of audio quality caused by source separation; and c) the potential for vocoders to degrade audio quality. Second, the generated accompaniment consists of numerous instrumental components, so it may be beneficial to generate each track with more fine-grained control. Lastly, both SingSong and our FastSAG are offline algorithms, meaning that the accompaniment is generated for an entire piece of singing voice. In the future, we could explore designing a framework for online accompaniment generation that adapts as the singing voice progresses."
        }
    ],
    "url": "http://arxiv.org/html/2405.07682v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "3",
            "4",
            "4.1",
            "4.2",
            "4.2.1",
            "4.2.2",
            "4.3",
            "4.4"
        ],
        "main_experiment_and_results_sections": [
            "5.1",
            "5.2",
            "5.3",
            "5.4",
            "5.4.1",
            "5.4.2",
            "5.4.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "5.4.1",
            "5.4.2",
            "5.4.3"
        ]
    },
    "research_context": {
        "paper_id": "2405.07682v1",
        "paper_title": "FastSAG: Towards Fast Non-Autoregressive Singing Accompaniment Generation",
        "research_background": "### FastSAG: Towards Fast Non-Autoregressive Singing Accompaniment Generation\n\n**Motivation:** The motivation behind the paper lies in the desire to simplify and accelerate the Singing Accompaniment Generation (SAG) process while improving the quality of the generated instrumental accompaniments that harmonize with vocal performances. Existing approaches, particularly those based on autoregressive (AR) language models, tend to be slow and complex, limiting their practical applicability.\n\n**Research Problem:** The primary research problem the paper addresses is the slow and complicated nature of current SAG processes, specifically those relying on autoregressive methods. These methods require significant computational resources and time, which can impede real-time application and user experience. Therefore, the paper aims to develop a faster and more efficient method without compromising the quality and coherence of the generated accompaniments.\n\n**Relevant Prior Work:**\n1. **Retrieval-Based Methods:** Early methods like Microsoft Songsmith (Simon et al., 2008) retrieve instrumental accompaniments based on symbolic chord label sequences extracted from input vocals. However, these methods lack the ability to generate novel and creative music pieces tailored to the specific vocal inputs.\n   \n2. **Learning-Driven Approaches:** \n   - **Audio2Audio Generation:** Techniques such as in Wu et al. (2022) generate specific instrumental tracks like drums but are not directly applicable to SAG.\n   - **Autoregressive Models:** Donahue et al. (2023) introduced a learning-driven approach for SAG, using multiple AR-based language models (semantic, coarse acoustic, fine acoustic LMs) similar to MusicLM (Agostinelli et al., 2023) and AudioLM (Borsos et al., 2023). These approaches, while effective in generating high-quality musical accompaniments, are extremely slow, with generation times well over real-time standards (e.g., one second of accompaniment taking dozens of seconds on high-end GPUs).\n\nThe proposed FastSAG aims to overcome these limitations by using a non-AR diffusion model, significantly simplifying the SAG pipeline, enhancing the speed, and improving the coherence and quality of the final accompaniment.\n\n**Key Contributions:**\n1. **Diffusion-Based Non-AR Framework:** The design of a diffusion-based, non-autoregressive framework for SAG simplifies the generation pipeline compared to AR-based methods like SingSong.\n2. **Alignment Techniques:** Introduction of semantic projection block and prior projection block, along with a set of tailored loss functions, to ensure that the generated accompaniments are rhythmically and semantically coherent with the vocals.\n3. **Performance:** Experimental results showcase that FastSAG achieves better quality accompaniments while accelerating the generation speed by more than 30 times, making real-time generation feasible.",
        "methodology": "**Methodology:**\n\nIn this section, we introduce the **EDM diffusion model** based on the work of Karras et al. (2022). This model will be utilized as a conditional probability model.\n\nThe model operates with the foundational assumption of a given data distribution and employs a set of mollified distributions obtained by adding Gaussian noise to the data. Key to the model is the process of sampling a noisy instance and progressively denoising it into a clean instance across noise levels. The entire procedure requires a specified number of sampling steps, ultimately producing an outcome mirroring the original data distribution. This methodology is classified within the variance exploding family, as noted by Song et al. (2020).\n\nThe **stochastic differential equation (SDE)** governing the diffusion is given by:\n\n\\[ d\\mathbf{x}_t = f(\\mathbf{x}_t, t) dt + g(t) d\\mathbf{w}_t \\]\n\nwhere and represent the drift and diffusion coefficients respectively, and is the standard Wiener process.\n\nThis SDE can be transformed into a **probability flow ordinary differential equation (ODE)**. Specifically, in the EDM framework by Karras et al. (2022), a designated noise level over time determines the dynamics. This ODE is written as:\n\n\\[ \\frac{d\\mathbf{x}_t}{dt} = g(t)^2 \\nabla_{\\mathbf{x}_t} \\log p_t(\\mathbf{x}_t) \\]\n\nwhich describes the evolution of a sample from time to , yielding a sample , where and signify a time derivative and the score function, respectively. The probability flow ODE can be leveraged for sampling, provided the score function is known.\n\nFor scheduling , a natural choice could be , linked to constant-speed heat diffusion, but Karras et al. (2022) demonstrated practical inconveniences. Consequently, EDM adopts an alternative schedule:\n\n\\[ t_{\\text{train}} = T_0^2, \\qquad t_{\\text{sample}} = T_1^2 \\]\n\nFor our EDM configuration:\n\n\\[ T_0 = 1, \\quad T_1 = \\infty, \\quad \\sigma_\\text{min} = \\sigma_0, \\quad \\sigma_\\text{max} = \\sigma_1, \\quad \\rho \\]\n\ncontrols the step lengths, emphasizing shorter steps near and longer ones near .\n\nSuppose represents the denoising function that minimizes the expected L2 denoising error across samples from for each , given as:\n\n\\[ D_\\theta(\\mathbf{x}_t, t) \\]\n\nwhere is the original training sample and the noise, the score function translates to:\n\n\\[ \\nabla_{\\mathbf{x}_t} \\log p_t(\\mathbf{x}_t) \\]\n\nIn the diffusion framework, the denoising function can be realized as a neural network, cast as and for unconditional and conditional diffusion, respectively, where denotes the conditioning variable. Following the EDM setup:\n\n\\[ \\theta(\\mathbf{x}_t, t) = \\theta_u(\\mathbf{x}_t, t) = U(\\mathbf{x}_t, t) \\]\n\nThe neural network can be any sophisticated model, such as Wavenet (Oord et al., 2016) employed in Liu et al. (2022) or U-net (Ronneberger et al., 2015) utilized in Popov et al. (2021). The parameters and manage skip connections and the magnitudes of , with:\n\n\\[ \\gamma_\\sigma = \\sigma_s \\exp(\\alpha \\sqrt{T}) \\]\n\nwhere and denote the smallest time interval during sampling.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n**Data Collection and Processing.** \n- Songs: 300k songs from public sources (Chinese and English).\n- Segmentation: Used Demucs to obtain mono vocal-accompaniment pairs.\n- Sampling Rate: 44.1 kHz.\n- Filtering: Preserve pairs with clear vocals and accompaniments, using a peak RMS amplitude threshold of -25dB over 10-second clips.\n- Final Dataset: 1.2 million pairs of 10-second clips, totaling more than 3000 hours.\n\n**Training Dataset.**\n- Divided dataset: 1.2 million pairs split into training and in-domain evaluation.\n- In-Domain Evaluation: 2,000 samples exclusively from either training or evaluation datasets.\n\n**Evaluation Dataset.**\n- In-Domain Evaluation: 2,000 samples not seen in training.\n- Zero-Shot Evaluation: Constructed from the MUSDB18 dataset, split similarly and obtaining 348 paired clips. The MUSDB18 training dataset was unused to ensure zero-shot learning conditions.\n\n**Baselines:**\n- SingSong (Donahue et al., 2023) is used for comparison, employing the same MUSDB18 zero-shot evaluation dataset.\n\n**Evaluation Metrics:**\n- The paper does not explicitly state the evaluation metrics in the provided sections, so it is presumed standard metrics for audio generation tasks might be employed. These could include metrics like Signal-to-Distortion Ratio (SDR), Signal-to-Interference Ratio (SIR), and Signal-to-Artifact Ratio (SAR), which are commonly used in similar works.\n\n**Main Experimental Results:**\n- The main experimental results are not detailed in the provided sections. However, it is implied that the results are compared against the SingSong baseline, likely focusing on the fidelity and quality of the generated accompaniments in both in-domain and zero-shot settings."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the function of different settings in the FastSAG method by conducting various ablation studies, aiming to identify which elements contribute most significantly to the system's performance.",
            "experiment_process": "The ablation study involved testing different configurations and settings of the FastSAG method. Three main aspects were examined: (1) Normalization of the Mel spectrogram, comparing the performance of normalized versus unnormalized spectrograms; (2) The impact of different condition types, such as using only the semantic feature, only the Mel spectrogram feature, or a combination of both; (3) The performance of different resampling modules, specifically bi-linear interpolation versus MERT as the feature extractor. Performance was measured using FAD metrics.",
            "result_discussion": "The study found that using a normalized Mel spectrogram leads to better performance in all FAD metrics. Furthermore, using only the semantic feature as the condition resulted in superior performance compared to other condition types, aligning with findings from SingSong. The utilization of bi-linear interpolation improved FAD scores compared to MERT as a feature extractor.",
            "ablation_id": "2405.07682v1.No1"
        },
        {
            "research_objective": "To evaluate the performance of FastSAG in terms of subjective quality as perceived by human listeners.",
            "experiment_process": "The subjective evaluation involved selecting 100 samples from both the in-domain test dataset and the zero-shot MUSDB18 test dataset, maintaining a 2:1 ratio respectively. Fifteen participants were engaged in the evaluation, each assessing 20 samples randomly selected from the 100 samples. The evaluation employed MOS (Mean Opinion Score) to gauge the perceived quality of the generated music.",
            "result_discussion": "FastSAG consistently outperformed the baseline SingSong in terms of MOS scores, regardless of whether interpolation or Perceiver-IO was used as the resampling module. The Perceiver-IO setup resulted in music that was closest to human composition, underscoring the importance of harmony and coherence in music generation.",
            "ablation_id": "2405.07682v1.No2"
        },
        {
            "research_objective": "To investigate the roles of the prior loss and diffusion loss within the FastSAG framework and how these components impact the generation process.",
            "experiment_process": "The Mel spectrograms of the singing voice and corresponding accompaniment were analyzed. The study examined the predicted prior (first-step alignment towards the accompaniment) and the output generated by the diffusion model, which is conditioned on the predicted prior. The experiment provided a visual and quantitative assessment of the framework's two-stage process, involving an encoder-decoder and subsequent refinement through the diffusion model.",
            "result_discussion": "The analysis revealed a significant gap between the Mel spectrograms of the singing voice and the initial accompaniment. The prior alignment step showed rough alignment, and the diffusion model conditioned on this prior provided a more detailed approximation of the accompaniment. These findings highlight the necessity of the two-stage process, illustrating that the encoder-decoder alone is insufficient without the refinement offered by the diffusion model.",
            "ablation_id": "2405.07682v1.No3"
        }
    ]
}