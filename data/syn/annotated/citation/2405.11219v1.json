{
    "title": "Identifying and Aligning Medical Claims Made on Social Media with Medical Evidence",
    "abstract": "Evidence-based medicine is the practise of making medical decisions that adhere to the latest, and best known evidence at that time. Currently, the best evidence is often found in the form of documents, such as randomized control trials, meta-analyses and systematic reviews. This research focuses on aligning medical claims made on social media platforms with this medical evidence. By doing so, individuals without medical expertise can more effectively assess the veracity of such medical claims. We study three core tasks: identifying medical claims, extracting medical vocabulary from these claims, and retrieving evidence relevant to those identified medical claims. We propose a novel system that can generate synthetic medical claims to aid each of these core tasks. We additionally introduce a novel dataset produced by our synthetic generator that, when applied to these tasks, demonstrates not only a more flexible and holistic approach, but also an improvement in all comparable metrics. We make our dataset, the Expansive Medical Claim Corpus (EMCC), available at https://zenodo.org/records/8321460.\n\n\n\nKeywords:\u2009Evidenced-based Medicine, PICO, Synthetic Generators, Information Retrieval",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "The proliferation of social media has made it easier for individuals to access and produce health information online. Patients often turn to social media for support on their conditions (Berry et al., 2017  ###reference_b7###), however it can be challenging for both medical and non-medical persons to discern whether information is reliable and evidence-based (Bastian et al., 2010  ###reference_b5###). This study aims to investigate the intersection between health information on social media and evidence-based medicine (EBM). By understanding more about how we can support individuals in navigating the vast amount of health information available on social media, and in scientific texts (Bastian et al., 2010  ###reference_b5###), we hope to enable them to make more informed decisions about their health.\nEvidence-based medicine is the practise of making medical decisions, where those decisions are informed by the entirety of the current and best evidence available. This evidence often comes in the form of randomised control trials (RCT). RCT\u2019s can be further aggregated into meta-analyses and systematic reviews, allowing these documents to provide a total view of the evidence for a given medical question. The basis of these documents is a research question, and often this question is constructed via the PICO framework. PICO (Santos et al., 2007  ###reference_b39###), a methodology widely applied in EBM, consists of four components: population, intervention, comparator, and outcome. An example PICO question is:\n\u201cCan a population of people with a broken bone: using an intervening pain medication ibuprofen, compared with a placebo, reduce a given outcome measure, feeling of pain?\u201d.\nIn this example, broken bone is the population, ibuprofen the intervention, placebo as comparator and outcome measure of pain. As shown by Wadhwa et al. (2023  ###reference_b46###), when annotating medical claims from social media with PICO elements, it can serve as the basis for retrieving relevant evidence.\nOur key contributions of this research will focus on four aspects of this task: 1) a synthetic data generator to produce a corpus of medical claims, 2) identifying medical claims in social media texts, 3) identifying PICO spans within medical claims, and 4) the retrieval of medical evidence relevant to the claims.\nGiven the previously discussed, the following research questions are addressed in this study:\nRQ1: Can a synthetic dataset of medical claims, PICO elements, and real medical evidence be built using generative language models?\nRQ2: To what extent, can an automated system trained with a synthetic dataset improve medical claims classification in social media posts?\nRQ3: To what extent, can an automated text analysis system trained with synthetic dataset improve PICO elements classification in medical claims made within social media posts?\nRQ4: Given a medical claim and PICO elements, how reliable does a system yield medical evidence that is relevant to this statement?"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Related Work",
            "text": "Identifying health information on social media has gained significant attention in recent years due to the abundance of user-generated content (UGC) on platforms such as Twitter and Reddit. These platforms offer a valuable source of information for various health-related applications, including monitoring adverse drug reactions (ADR) (Zhang et al., 2020  ###reference_b49###; Dirkson et al., 2021  ###reference_b15###; Saha et al., 2020  ###reference_b37###; Sarker et al., 2018  ###reference_b40###), tracking the spread of infectious diseases (Charles-Smith et al., 2015  ###reference_b12###) and identifying health-related trends and needs (Correia et al., 2020  ###reference_b13###). It is clear that the task of extracting useful health information from social media data is not straightforward, due to several challenges that must be addressed. One of the main challenges in social media mining is the presence of noise within the users' use of language: misspellings, poor punctuation, grammatical errors, informal language, emoticons, and slang. This noise can make it difficult to accurately classify social media posts also containing medical vernacular. To address this challenge, researchers have employed deep learning models with linguistic features, part-of-speech tags, as feature inputs to improve accuracy in ADR classification (Zhang et al., 2020  ###reference_b49###). Another challenge is the lack of access to expert-annotated datasets or expert-validated datasets, that cover a wide range of medical topics, for token and span classification tasks. To overcome this, some studies have focused on document-level classification of social media posts (Zhang et al., 2020  ###reference_b49###; Weissenbacher et al., 2019  ###reference_b47###; Sarker et al., 2018  ###reference_b40###) as document annotations require less development overhead. These studies have shown that document-level classification can be used to identify potentially relevant texts that can then be further analyzed for specific span classification tasks (Saha et al., 2020  ###reference_b37###; Portelli et al., 2022  ###reference_b30###). This ensemble approach can be seen many times in the literature but is often focused on a single medical domain, such as diseases. Given the limited availability of high-quality annotated datasets, the use of large language models (LLM) has been instrumental in improving the accuracy of information extraction tasks applied to social media health data. These models are able to build complex feature representations unsupervised, as shown in BertTweet (Nguyen et al., 2020  ###reference_b27###). Nguyen et al. (2020  ###reference_b27###) present a BERT-based (Devlin et al., 2019  ###reference_b14###) LLM trained on a large dataset of tweets, this has demonstrated success in classifying tweets related to specific health conditions, such as COVID-19, and detecting spans of ADRs. In addition, Aji et al. (2021  ###reference_b2###) used BertTweet in conjunction with data augmentation techniques, including downsampling and back-translation, to further improve their results in the identification of ADR spans. Other researchers have used transfer learning to improve word embeddings for health-specific tasks. Basaldella et al. (2020  ###reference_b4###) trained existing embeddings, including GloVe (Pennington et al., 2014  ###reference_b29###) and Flair Akbik et al. (2018  ###reference_b3###), on a corpus of health-based UGC from websites, such as Health Unlocked111www.healthunlocked.com  ###reference_thunlocked.com### and Reddit. They achieved improvements on two health-based named entity recognition benchmark datasets, PsyTar (Zolnoori et al., 2019  ###reference_b50###) and CADEC (Karimi et al., 2015  ###reference_b21###). Batbaatar and Ryu (2019  ###reference_b6###) utilize Bidirectional Long Short Term Memory and congruence with a CRF output layer for PICO annotation of medical abstracts. The authors use both character and word embeddings along with linguistic features. This model was trained and applied on tweets only, given tweets are restricted in length, the use of different embedding layers could be useful here as shown by Joshi et al. (2020  ###reference_b20###). Weissenbacher et al. (2019  ###reference_b47###) demonstrate how deep learning models vastly outperform lexical models with character and word embeddings.\n\nIn the realm of AI in autonomous vehicles, substantial advancements have been made in adaptive learning algorithms and sensory data processing to enable self-driving capabilities (Bojarski et al"
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1.   PICOs, Claims and Evidence",
            "text": "PICO being utilised in a social media related task is an emerging idea in the literature Wadhwa et al. (2023 ###reference_b46###); Ramachandran et al. (2023 ###reference_b33###). Wadhwa et al. (2023 ###reference_b46###) have a made available corpora containing PICO annotations, an example annotation can be found in Table 1 ###reference_###. The corpus contains Reddit posts, where spans of text within the posts are annotated with PICO categories, and identifies spans where medical claims exist. Although the first of its kind, it is limited in the amount of health conditions targeted, the authors opted for static templating mechanism for generating synthetic medical claims and the existing annotations are noisy due to being crowd-sourced. This is similar to Sarrouti et al. (2021 ###reference_b41###) who sourced their claims from search engine results. Due to successes in generative models for retrieval tasks Izacard and Grave (2021 ###reference_b18###), this work focuses on navigating current limitations with synthetic data generators to improve PICO extraction, claim identification, and evidence retrieval. We will utilise the corpora offered by Wadhwa et al. (2023 ###reference_b46###) to fine-tune a generative model that can produce the necessary synthetic data to improve all of the discussed tasks of this study.\n\nThe application of AI in autonomous vehicles continues to advance with significant strides made in navigation and safety systems (Chen et al., 2023 ###reference_b52###). State-of-the-art machine learning models enable decision-making processes that improve driving efficiency and ensure passenger safety (Zhang and Liu, 2023 ###reference_b68###). Such advancements are critical as the automotive industry moves towards full autonomy."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2.   Datasets",
            "text": "The only corpora that has investigated similar research questions is the Reddit Health Online Talk (RedHOT) corpus (Wadhwa et al., 2023  ###reference_b46###). This is a corpus comprising of more than crowd annotated social media posts sourced from Reddit, covering 24 distinct health conditions. The annotations encompass various aspects, such as identifying spans related to medical claims. Notably, the dataset includes annotations for medically significant PIO elements associated with a claim. Using these naturally occurring claims, we aim to train a generative model that encodes PIO elements that then decodes a medical claim. Once a fine-tuned generative model is availabl a medical evidence dataset is required to generate synthetic claims. Trialstreamer (Marshall et al., 2020  ###reference_b26###) offers a dataset of all randomised control trials (RCTs) along with the PIO elements, extracted using machine learning methodologies (Marshall et al., 2017  ###reference_b25###), related to those publications. Once a generative model is fine-tuned using RedHOT to generate medical claims for a given PIO set, this means for every available piece of medical evidence, we can use the PIO span from that evidence as input to generate a synthetic medical claim resulting in an aligned claim-abstract pair. In this study, we drop the use of the element, a comparator is always an intervention, and it is unnecessary to create a distinction in our generators and classifiers in the context of this task (Brockmeier et al., 2019  ###reference_b10###; Chabou and Iglewski, 2018  ###reference_b11###).\n\nAI in autonomous vehicles focuses on developing machine learning algorithms capable of navigating complex driving environments. Core technologies include computer vision and sensor fusion, which combine data from cameras and LiDAR to perceive the road (Bojarski et al., 2016 ###reference_b47###; Chen et al., 2020 ###reference_b48###). Research emphasizes safety, decision-making, and real-time processing to enhance self-driving systems\u2019 efficiency and reliability."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   Expansive Medical Claim Corpus",
            "text": "The methodology described in this section is for creating the Expansive Medical Claim Corpus (EMCC) where that methodology can be used to synthesise medical claims for any unseen PIO sequence. The corpus is generated with three steps: (1) curation of RedHot dataset, (2) using the selected dataset as the foundation for fine-tuning a pretrained generative model, and (3) observing the quality produced by the fine-tuned generative model through qualitative analysis. See Figure 1  ###reference_### for an overview of the synthetic generator methodology.\nThe basis of the training data was curated by taking all the of the medical claims in the RedHOT corpora, and the PIO elements that they are annotated with. The PIO elements are fed to the encoder and the medical claim fed to the decoder of a given generative model. The aim with this methodology is that a model will learn to generate naturally sounding claims given an unseen set of PIO elements.\n###figure_1### Three models were selected in this work for generating the synthetic medical claims: Falcon-7B (Penedo et al., 2023  ###reference_b28###), T5 (Raffel et al., 2019  ###reference_b32###) and ByT5 (Xue et al., 2022  ###reference_b48###). Due to computational power constraints, the majority of our experiments utilise t5-base and byt5-base.\nFalcon is a seven billion parameter autoregressive decoder-only model trained on trillions of tokens, and has demonstrated high performance in a wide range of language tasks. We utilise this model because it will allow us to draw comparisons between a large scale model versus small models, such as the T5 family. T5 is capable of performing any text generation task, and this capability makes T5 flexible in a wide variety of tasks without task-specific architectures. For example, T5 has shown promise in the generation of synthetic corpora for QA tasks (Agarwal et al., 2021  ###reference_b1###). We conduct further experiments with ByT5, a byte-level encoder-decoder model, and variant of T5. By altering the tokenisation technique to operate on only the bytes within a string, as opposed to the words or subwords, the tokenisation is potentially more robustness against noise and more capable in multilingual settings. This token free approach has shown promising results in tasks with noisy data (Samuel and Straka, 2021  ###reference_b38###), and further results shown in its ability to generate synthetic data (Stankevi\u010dius et al., 2022  ###reference_b42###).\nThe selected models are then further fine-tuned with the curated dataset. The curation process takes all of the medical claims in the RedHOT corpora that are annotated with the PIO elements, as per the example shown in Table 1  ###reference_###. The data is to be split into input (PIO elements) and target text (medical claim) for the encoder and decoder respectively. We split the curated dataset into training (90%) and validation (10%) for qualitative analysis. A specific test set was not isolated at this stage as generation quality was monitored qualitatively post training. The aim of this methodology is that the model will generate naturally sounding claims given an unseen set of PIO elements from the RedHOT corpus.\nA qualitative analysis takes place at the end of each training cycle to observe the synthetic claims being generated by the model. This process requires randomly selecting PIO elements from abstracts within Trialstreamer, unseen from within the RedHOT corpus, and then using the generative model to generate medical claims. These tests are all PIO sets that were previously unseen to the model to understand model adaptability to unseen questions.\nFinally, we focus on two decoding strategies: contrastive search (Su et al., 2022  ###reference_b44###) and multinomial sampling (Keskar et al., 2019  ###reference_b23###). We utilise these strategies due to their results in handling repetition and tasks regarding fluency."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Implementation Detail",
            "text": "In this section, we describe the details of the EMCC\u2019s generation, including the qualitative-only analysis and hyperparameters."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1.   Hyperparameters",
            "text": "The following hyperparameter values were manually selected: batch size , maximum epochs  with an early stopping mechanism, AdamW optimizer and learning rate  for both classification tasks - we use identical hyperparameters for our two selected models, however the sequence lengths differ. These are important to the selected models and the task of generating medical claims, because claims are generally limited to single sentences and short utterances. We, therefore, alter the sequence lengths to reflect this idiosyncrasy. The minimum and maximum sequence lengths differ substantially between the character-level model, ByT5, and the token-level model - T5. Given that ByT5 operates at the byte-level it means that a single byte is generated for each element of a sequence, where as token models generate a token which is made up of many bytes. ByT5 was given a minimum length of  and a maximum length of , in contrast to the the token-level model where we selected  and .\nOnce training had begun with the prepared data, the training loss was observed at each epoch until it had stabilised and then training was halted."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2.   Qualitative-Only Analysis",
            "text": "For this study, we performed a qualitative-only analysis of our synthetic medical claims. With this we took an iterative approach to analyse a test set of PIO claims unseen in the training data.\nDifferent prompts containing PIO elements, taken at random from the medical evidence-base, Trialstreamer, were used to observe the synthetic medical claims being generated. First observations show that models have potential for generating naturally sounding medical claims. We looked to identify the following criteria: claims appear to be coherent, claims appear natural, and if were being read by a patient could benefit from being aligned with medical evidence. See Table 2  ###reference_### for an example.\nIn Table 3  ###reference_###, we observe a problem in the synthetic claims being generated by the fine-tuned models. Although these sequences appear natural, they are either vague or not a claim. The response by T5, in Table 3  ###reference_###, identifies a personal experience rather than a specific claim. Using the same prompt, ByT5 generates a natural sounding claim, however due to its vagueness it becomes difficult to assess how relevant it is to the PIO used in the prompt.\nIn the example Table 4  ###reference_###, we see naturally sounding synthetic claims, however the constrastive search is producing more claims that adhere to the quality criteria we specified.\nExperiments using Falcon were dropped at this stage, in order to have high speed inference it requires significant compute resources, and it\u2019s qualitative analysis did not suggest that it was generating claims that were any more naturally\nsounding than that of the smaller models.\nAfter repeating this process with other sets of PIO elements, we identified at this point that the constrastive model was generating more natural sounding claims, particularly when used with T5. We next went on to experiment with other hyperparameters, such as temperature, to understand the impacts on the sequence being generated by both models. ByT5 was often repeating itself, therefore we iteratively altered our decoding parameters. In the appendixes we attach additional information; see Table 15  ###reference_5### for the final selected decoding parameters , and Table 16  ###reference_6### for\nmore details on decoding experiments in the appendix.\nAdditionally, we also conducted small scale one- and few-shot experiments using the GPT-3 model series, we have attached these results in the appendices. See subsection 11.2  ###reference_2### for additional information.\nIn the appendix, subsection 11.1  ###reference_1### and Table 11  ###reference_1###, we share an overview of the final available corpora generated using T5 and the discussed decoding parameters."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Downstream Task Evaluation",
            "text": "We evaluate the quality of our generated corpus (EMCC) using two downstream tasks Evidence retrieval (Section 5.2  ###reference_###) and Claim and PIO Identification (Section 5.1  ###reference_###)"
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "5.1.   Claim and PICO Identification",
            "text": "Given a social media post as a sequence of text, the task is to annotate that post with the necessary labels in order to identify the medical claims within it and further identify any PIO elements. As proposed by Brockmeier et al. (2019  ###reference_b10###), we use a Beginning-Inside-Out (BIO) style format to label Beginning of the PICO mention, Inside of the PICO mention, and Out side of the PICO mention. See Table 5  ###reference_### for a full example. We perform a similar labeling approach to that of PICO identification. In this task, we utilise each of the the BIO classes in conjunction with each of the PIO categories:  represents Population,  represents Intervention and  represents Outcome. This can be seen in the example in Table 6  ###reference_###.\nTwo models were selected for this task; a statistical machine learning model and a deep learning model. Conditional Random Fields (CRF) was chosen for building a baseline, as is often reflected in the literature (Chabou and Iglewski, 2018  ###reference_b11###; Kim et al., 2011  ###reference_b24###), and an encoder-only transformer-based model was used for comparison with current state-of-the-art models (Wadhwa et al., 2023  ###reference_b46###).\nStatistical Model We extract and use similar features to that of Wadhwa et al. (2023  ###reference_b46###) to train our model, each token is labelled with the following feature: part-of-speech (POS) tag, it is alphanumeric, it is an uppercase word, it is numeric, it is title-case, the previous three words and their POS tags, the following three words and their POS tags.\nTransformer Model We look to use BERT-based uncased, and also adapt this model on the new synthetic claims. We hypothesise that by adapting our encoder-only model on our new in-domain synthetic corpora and further fine-tuning on the task specific dataset, when testing the model on that existing RedHOT corpora, we can address our original research question.\nGiven that our synthetic dataset is not an annotated corpus of social media like that of RedHOT, we require a different approach to training as we can\u2019t fine-tune specifically for span classification within posts. We look to reuse the BERT masked language modelling strategy for in-domain adaptation, further fine-tune and then evaluate using the RedHOT corpora.\nThe following hyperparameter values were manually selected: batch size , maximum epochs  with an early stopping mechanism, AdamW optimizer and learning rate  for both classification tasks.\nFor both experiments we used an uncased base BERT model and also the domain adapted BERT trained using masked language modelling on the synthetically generated medical claims. We selected  whole word token masking approach to our corpus. Once the corpus was sufficiently masked, the following hyperparameter values were manually selected: batch size , an optimizer with a learning rate  and 1000 warm-up steps."
        },
        {
            "section_id": "5.1.1",
            "parent_section_id": "5.1",
            "section_name": "5.1.1.   Results",
            "text": "In Table 7  ###reference_###, we display the results of our claim classification performance on a test set derived from the original RedHOT corpus (Wadhwa et al., 2023  ###reference_b46###).\nIn Table 8  ###reference_###, we display the results of our claim classification performance on a test set derived from the original RedHOT corpus (Wadhwa et al., 2023  ###reference_b46###).\nWe find in our results that BERT adapted to our synthetic dataset, then tested on the RedHOT corpus is the highest performing system by 7%, where CRF is the second most performant. This beats all current state-of-the-art methods Wadhwa et al. (2023  ###reference_b46###); Ramachandran et al. (2023  ###reference_b33###).\nWe find in our results that BERT adapted to our synthetic dataset, fine-tuned and tested on the RedHOT corpus is the highest performing system by 3%, where BERT without any further training on the synthetic data is the second most performant. This beats all current state-of-the-art methods Wadhwa et al. (2023  ###reference_b46###); Ramachandran et al. (2023  ###reference_b33###)."
        },
        {
            "section_id": "5.1.2",
            "parent_section_id": "5.1",
            "section_name": "5.1.2.   Discussion",
            "text": "Our results for claim classification suggest that a statistical model is far more performant than a BERT model that is not domain adapted. CRF has a 16 point improvement over BERT, however when BERT is adapted to the domain there was an increase of 7 points over the statistical model. We note that our CRF system has a 25 point improvement over the original works (Wadhwa et al., 2023  ###reference_b46###), this is likely due to our methodology containing more features. This heavy feature engineering process, however, is not required by deep learning models making them more adaptable to change.\nWe find for PIO classification that utilising BERT is more performant than the statistical model. We also find that model BERT-Synth has a 2% increase in F1 score than the existing works (Wadhwa et al., 2023  ###reference_b46###). This is suggestive that domain-specific synthetic data can have a positive impact on PIO classification, furthermore this was performed on a smaller model than that of the state-of-the-art.\nWe have been able to demonstrate that deep learning mechanisms vastly out perform statistical models for the tasks within this study. Given the expansive body of literature in deep learning models we expected an improvement using an encoder-only architecture, however a unique improvement we also see is the introduction of PIO elements into the query. The addition of this into the query elements offers a significant performance improvement across all model architectures.\nGiven that our results are centered around synthetic medical claims and therefore synthetically aligned documents, we felt that this bias could be alleviated using a medical expert to perform manual evaluation against real medical claims retrieved from social media. In the results, we found it was clear that the system is capable of returning medical abstracts that are relevant to the claim. Moreover, in some instances, the system is capable of returning highly relevant documents. These abstracts offer tangible advice that could utilised by a patient of professional in order to resolve or investigate the claim further."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "5.2.   Evidence retrieval",
            "text": "Evidence retrieval can be framed as a question-answering task, whereby we use the PIO elements and the medical claim as the question and the answer is a set of appropriate medical abstracts.\nThe aim of the evidence retrieval component is to retrieve the most pertinent evidence in relation to a given medical claim and it\u2019s associated PIO data. This requires being able to evaluate the accuracy of the system in regards to its pertinence, therefore we look to use the metric precision@ (Jirvelin and Kekiiliinen, 2017  ###reference_b19###) in order to do this.  represents the quantity of documents being returned by the system, and we look to count how many relevant documents are found within that specified quantity of results.\nWe utilise two machine learning methodologies for this task: a statistical model and a deep learning model. BM25 (Robertson and Zaragoza, 2009  ###reference_b35###) is utilised for deriving a baseline as seen in the literature (Wadhwa et al., 2023  ###reference_b46###) and dense passage retrieval (DPR), a deep learning technique is used to observe performance against the existing state of the art (Wadhwa et al., 2023  ###reference_b46###).\nBM25 and DPR requires separating queries from answers, therefore we prepare our dataset, as such that our synthetic claims and PIO elements are separate from the medical abstracts. We propose the use of two experiments: (1) is where only the synthetic claim is used as a query to fetch medical abstracts, and (2) is where the concatenation of claim, and PIO elements are used as a query to fetch medical abstracts.\nThe literature (Karpukhin et al., 2020  ###reference_b22###) notes that DPR is highly resource intensive, therefore we will only experiment with the query construction that is the highest performing from the BM25 experiments. Due to the time and resources required to train a resource intensive model, we leave more intensive experiments for future work.\nPerformance in DPR is greater when negative samples (Gillick et al., 2019  ###reference_b16###) are provided along with the positive samples. We construct a negative sample set of abstracts for every claim-evidence pair where those PIO elements are not used. This would mean for the PIO elements \"lupus, methotrexate, plaquenil nauseous\", we would take all other aligned PIO claim abstracts for our negative set."
        },
        {
            "section_id": "5.2.1",
            "parent_section_id": "5.2",
            "section_name": "5.2.1.   Results",
            "text": "We believe it to be necessary to conduct evidence retrieval experiments where the query is only the synthetic medical claim, and a second set of experiments where the PIO elements and the synthetic claim are concatenated.\nWe report the results of the evidence retrieval task in Table 9  ###reference_###. We conducted three experiments over two models, where both models were tested with PIO elements concatenated with claims as queries, also a claim only query experiment was conducted.\n###table_1### As seen in Table 9  ###reference_###, we first find that where the evidence query contains PIO and claim elements, BM25-PIO and DPR-PIO, there is a significant improvement in precision. DPR-PIO gives an 8 times improvement in results when  is set to 1, and at least double in all other  scoring experiments."
        },
        {
            "section_id": "5.2.2",
            "parent_section_id": "5.2",
            "section_name": "5.2.2.   Expert Evaluation",
            "text": "To further validate the evidence retrieval system, we look to utilise the naturally occurring claims from the RedHOT corpora and query our trained evidence retrieval system with those claims. An expert, in the form of a qualified doctor currently practising as a first year general practitioner (GP), is utilised to perform the analysis.\nWe first randomly select 15 medical claims from the RedHOT corpus. These are then presented to the Doctor with the first 5 medical abstracts returned from our highest performing system (DPR-PICO) in ranked order. We then asked the expert to allocate a score to each of the 5 abstracts. The score was based on the works of Wadhwa et al. (2023  ###reference_b46###), where the expert was asked to allocate a relevant, somewhat relevant or irrelevant label to each abstract. We expand on this evaluation, on advice from the medical expert, with an additional scoring category - highly relevant. We introduce a highly relevant category to identify abstracts that provide direct and tangible advice that an expert or non-expert could utilise in a real-world setting. These scores can be used within a precision@ style scoring to understand the relevancy of abstracts at different precision counts\n.\nIn Table 10  ###reference_###, we display our results in regards to the expert evaluation. We find that when the precision@ is higher more irrelevant results are introduced, however the total sum of each relevant category always outweighs that of the irrelevant category."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Limitations",
            "text": "A limitation of this work is that we were vastly limited on how many data points we could review from our synthetic corpus. It is, therefore, possible that noise or poorly generated claims are stored within the dataset. An alternative to the qualitative approach approach may be to build a classifier that dictates whether or not a generated claim is acceptable or not. This acceptability is decided by a classifier trained upon a dataset of naturally occurring claims, i.e the data from Wadhwa et al. (2023  ###reference_b46###). A further improvement for this would be to domain-adapt the encoders in the evidence retrieval architecture as we did for the classification results.\nWe felt we were limited in our ability to perform a like-for-like comparison as previous works did not share the claims and evidence they used to perform their expert evaluation. Due to limited resources too, we were unable to a test a similar quantity of claims and abstracts. We leave these issues for future works."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7.   Conclusions",
            "text": "RQ1 addressed whether a methodology could be constructed to produce a synthetic corpus of medical claims, and whether that corpus could improve downstream tasks. A requirement of this corpus is that it be aligned with PIO elements and medical abstracts. We believe we constructed a working methodology that is capable of generating a synthetic claim for any given PIO structure from the medical literature. This corpora was qualitatively-only evaluated, and in future we look for automated quantitative measures of quality.\nRQ2 addressed to what extent a classification system could identify medical claims in social media posts. We conducted experiments to look at how well system perform on existing corpora on both statistical and deep learning architectures. We found that deep learning architectures gave us a  increase in performance, when utilising EMCC in an in-domain adaptation technique using the new synthetic corpora on a base large language model.\nRQ3 addressed to what extent a classification system could identify medical claims in social media posts. We conducted experiments to observe the performance on existing corpora and synthetic corpora. We found that deep learning architectures adapted on the EMCC gave us a  increase over statistical methodologies.\nRQ4 addressed how precisely we can retrieve medical evidence that relates to medical claims made in social media posts. We conducted our experiments using the synthetic dataset, where each claim in that set is paired with a medical abstract.\nIn our results, we saw that a deep learning architecture vastly outperforms that of the statistical models. Furthermore, the introduction of additional context in the query, in this task it was the PIO elements, improves the precision by 3 times when precision@ is set to one. In our expert analysis, we find that for almost every query we get a least  relevant document when precision@ is set to five."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "8.   Acknowledgements",
            "text": "This work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation (Grant number EP/S023062/1)."
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "9.   Bibliographical References",
            "text": ""
        }
    ],
    "url": "http://arxiv.org/html/2405.11219v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1"
        ],
        "methodology_sections": [
            "3"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.1",
            "5.1.1",
            "5.1.2",
            "5.2",
            "5.2.1",
            "5.2.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "4.1",
            "4.2"
        ]
    },
    "research_context": {
        "paper_id": "2405.11219v1",
        "paper_title": "Identifying and Aligning Medical Claims Made on Social Media with Medical Evidence",
        "research_background": "**Motivation:**\nThe motivation for this paper stems from the increasing difficulty for both medical and non-medical individuals to discern reliable and evidence-based health information on social media. While social media platforms serve as accessible and supportive tools for patients seeking health-related information, they often disseminate unreliable or non-evidence-based content (Berry et al., 2017; Bastian et al., 2010). By bridging the gap between health information found on social media and credible medical evidence derived from evidence-based medicine (EBM), the study aims to support individuals in making informed health decisions.\n\n**Research Problem:**\nThe primary research problem addressed in this paper is the challenge of identifying and aligning medical claims made on social media with credible medical evidence. Specifically, the study focuses on synthesizing data, classifying medical claims, identifying PICO (Population, Intervention, Comparator, Outcome) elements within these claims, and retrieving relevant medical evidence based on these elements.\n\n**Relevant Prior Work:**\n1. **Wadhwa et al. (2023):** Demonstrated that annotating medical claims from social media with PICO elements can aid in retrieving relevant evidence.\n2. **Berry et al. (2017):** Highlighted the reliance of patients on social media for support on their conditions.\n3. **Bastian et al. (2010):** Emphasized the challenge of discerning reliable health information on social media and scientific texts.\n\nThis study builds on these works by proposing a comprehensive system that involves generating synthetic data, identifying medical claims and PICO spans in social media posts, and retrieving corresponding medical evidence.",
        "methodology": "The methodology section describes the creation of the Expansive Medical Claim Corpus (EMCC) to synthesize medical claims for any unseen PIO (Population, Intervention, Outcome) sequence through three primary steps:\n\n1. **Curation of RedHot Dataset**:\n   - Initially, the methodology centers on curating the RedHOT corpora, which contains medical claims annotated with PIO elements.\n   - These annotations are used as the foundation for the dataset used in the subsequent steps.\n\n2. **Fine-tuning Pretrained Generative Models**:\n   - The PIO elements from the RedHOT corpus are fed into the encoder, while the medical claims serve as inputs to the decoder of the generative model.\n   - Three models are selected for generating synthetic medical claims:\n     - **Falcon-7B**: An autoregressive, decoder-only model with seven billion parameters, known for high performance in various language tasks.\n     - **T5**: A versatile model capable of performing a range of text generation tasks. It has previously shown effectiveness in generating synthetic corpora for QA tasks.\n     - **ByT5**: A variant of T5 that operates on byte-level tokenization, making it potentially more robust against noise and effective in multilingual settings.\n   - Experiments primarily utilize the base versions of T5 and ByT5 due to computational constraints.\n\n3. **Qualitative Analysis of Generated Claims**:\n   - After fine-tuning, the quality of generated medical claims is observed through qualitative analysis.\n   - This involves randomly selecting unseen PIO elements from abstracts within Trialstreamer and using the generative models to create synthetic claims.\n   - This qualitative analysis is carried out using PIO sets that the model has not seen before to assess the adaptability of the model to new data.\n\nThe final step focuses on evaluating two decoding strategies to improve generation quality:\n\n- **Contrastive Search**: Known for handling repeated phrases and ensuring fluency in generated text.\n- **Multinomial Sampling**: Employed for generating diverse and fluent outputs.\n\nOverall, this methodology aims to produce naturally sounding medical claims given new PIO elements by leveraging and fine-tuning large language models with curated medical data.",
        "main_experiment_and_results": "In the main experiment setup for the paper *Identifying and Aligning Medical Claims Made on Social Media with Medical Evidence*, the primary focus is on two downstream tasks: Evidence Retrieval and Claim and PIO Identification. Here, the experimental details are provided for each task:\n\n### Evidence Retrieval\n**Dataset:**\n- Evidence-based Medical Corpus of Claims (EMCC).\n\n**Baseline:**\n- The baseline models used for comparison are not explicitly mentioned.\n\n**Evaluation Metrics:**\n- Standard information retrieval metrics such as Precision, Recall, and F1-score are typically used to evaluate the performance of evidence retrieval systems.\n\n### Claim and PIO Identification\n**Dataset:**\n- Evidence-based Medical Corpus of Claims (EMCC).\n\n**Baseline:**\n- The baseline models used for this task are also not specified explicitly.\n\n**Evaluation Metrics:**\n- Commonly used evaluation metrics for claim and PIO identification tasks include Precision, Recall, and F1-score.\n\n### Main Experimental Results\nThe paper reports on the performance of the proposed system across the two tasks:\n\n1. **For Evidence Retrieval:**\n   - The proposed system demonstrates an improvement over the baseline models in retrieving relevant medical evidence.\n   \n2. **For Claim and PIO Identification:**\n   - The system also shows superior performance in accurately identifying claims and related Patient/Intervention/Outcome (PIO) elements compared to baselines.\n\nThese results suggest the efficacy of the EMCC corpus and the proposed methods in aligning medical claims on social media with relevant medical evidence, contributing positively to these downstream tasks."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Assess the effectiveness of different models (T5, ByT5, and Falcon) to generate naturally sounding medical claims based on PIO elements.",
            "experiment_process": "The experiment involved three steps: curating the RedHot dataset, fine-tuning pretrained models (T5, ByT5, and Falcon) using this dataset, and conducting qualitative analysis. The curated dataset comprised medical claims from RedHOT annotated with PIO elements fed to the encoder and decoder of the models. The curated dataset was split into 90% training and 10% validation. After training, a qualitative analysis was performed using unseen PIO elements from Trialstreamer to generate claims and evaluate their naturalness and coherence. Decoding strategies included contrastive search and multinomial sampling.",
            "result_discussion": "The models demonstrated potential for generating natural-sounding claims. However, T5 sometimes produced personal experiences rather than specific claims while ByT5's claims were coherent but often vague. Falcon was dropped due to high computational requirements and no significant improvement in quality. The contrastive model generated more natural-sounding claims, especially with T5. ByT5 faced issues of repetition, leading to iterative adjustments of decoding parameters. Consequently, further hyperparameter adjustments improved the naturalness of generated claims.",
            "ablation_id": "2405.11219v1.No1"
        },
        {
            "research_objective": "Evaluate the impact of different hyperparameters on the quality of synthetic medical claims generated using T5 and ByT5 models.",
            "experiment_process": "The experiment used identical hyperparameters for both models, tailored to account for single-sentence medical claims. Sequence lengths were adapted due to the differences in token generation between ByT5 (character-level) and T5 (token-level). ByT5 had minimum and maximum lengths set differently compared to T5. Training was monitored through training loss until stabilization. The iterative analysis used test PIO claims from Trialstreamer to evaluate naturalness and coherence.",
            "result_discussion": "The qualitative evaluation indicated that while both models generated coherent claims, T5 sometimes produced personal experiences, and ByT5\u2019s claims were often vague. By iteratively adjusting hyperparameters, the researchers identified optimal settings that improved claim quality. Falcon was excluded due to computational constraints and insignificant quality benefits. T5 using contrastive search and adjusted hyperparameters generated the highest quality claims. The repetitive issues in ByT5 were mitigated through iterative hyperparameter tuning.",
            "ablation_id": "2405.11219v1.No2"
        }
    ]
}