{
    "title": "Octopus: On-device language model for function calling of software APIs",
    "abstract": "In the rapidly evolving domain of artificial intelligence, Large Language Models (LLMs) play a crucial role due to their advanced text processing and generation abilities. This study introduces a new strategy aimed at harnessing on-device LLMs in invoking software APIs. We meticulously compile a dataset derived from software API documentation and apply fine-tuning to LLMs with capacities of 2B, 3B and 7B parameters, specifically to enhance their proficiency in software API interactions. Our approach concentrates on refining the models\u2019 grasp of API structures and syntax, significantly enhancing the accuracy of API function calls. Additionally, we propose conditional masking techniques to ensure outputs in the desired formats and reduce error rates while maintaining inference speeds. We also propose a novel benchmark designed to evaluate the effectiveness of LLMs in API interactions, establishing a foundation for subsequent research. Octopus, the fine-tuned model, is proved to have better performance than GPT-4 for the software APIs calling. This research aims to advance automated software development and API integration, representing substantial progress in aligning LLM capabilities with the demands of practical software engineering applications.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The advent of Large Language Models (LLMs) has revolutionized the field of artificial intelligence, bringing forth a wide array of capabilities in natural language processing, alongside applications in specialized domains such as mathematics (Imani et al. (2023  ###reference_b16###); He-Yueya et al. (2023  ###reference_b14###))), healthcare(Imani et al. (2023  ###reference_b16###); Jo et al. (2023  ###reference_b19###); Thirunavukarasu et al. (2023  ###reference_b37###)), and legal analysis (Cui et al. (2023  ###reference_b7###); Fei et al. (2023  ###reference_b9###); Luppi et al. (2022  ###reference_b25###)). Despite these advancements, LLMs face challenges in assimilating real-time updates and executing specific tasks like image/video editing (Fu et al. (2023  ###reference_b10###)) or intricate tax filings. The integration of Large Language Models (LLMs) with external APIs emerges as a pivotal improvement. This synthesis, leveraging APIs, not only augments the LLMs\u2019 capabilities by facilitating access to up-to-date information and specialized functionalities but also sparks the creation of novel applications such as code interpreters (Bairi et al. (2023  ###reference_b4###); Vaithilingam et al. (2022  ###reference_b39###); Chen et al. (2021  ###reference_b6###)). Research like ToolAlpaca (Tang et al. (2023  ###reference_b35###)) and NexusRaven (Srinivasan et al. (2023  ###reference_b34###)) also proves the function calling capability of open source language model. Consequently, this integration signifies a crucial step toward overcoming the inherent limitations of LLMs, thereby extending their utility and potential for innovation in the field.\nEnhancing the integration of Large Language Models (LLMs) with external APIs necessitates addressing the challenge of balancing large-scale model dependency against efficiency and cost. Focusing on specific tasks that utilize only a fraction of available APIs indicates the inefficiency of merely replying on large models like GPT-4 (Radford et al. (2018  ###reference_b28###; 2019  ###reference_b29###); Brown et al. (2020  ###reference_b5###); Achiam et al. (2023  ###reference_b2###); Wu et al. (2023  ###reference_b44###)), which need a lot of computational resources. This scenario advocates for the development of smaller, task-oriented LLMs that preserve essential functionality while minimizing operational costs (Shen et al. (2024b  ###reference_b32###); Pallagani et al. (2024  ###reference_b26###)). However, this shift towards smaller models introduces new challenges, including an increased risk of errors or \u201dhallucinations\u201d (Yao et al. (2023  ###reference_b46###); Zhang et al. (2023  ###reference_b47###); Ji et al. (2023  ###reference_b17###)) which causes issues in precise output formatting (Jiang et al. (2023  ###reference_b18###)). And the correct output formatting is critical for robust software application.\nIn response to the limitations of oversized Large Language Models (LLMs), which entail unnecessary inference costs and exhibit a lack of focus in training data, we propose a new framework to do LLM training and inference. Grounded in an expansive dataset of over 30,000 widely-utilized APIs from Rapid API Hub (rap (2024  ###reference_b1###)), this framework spans a diverse array of functionalities from Google searches to Amazon product lookups. By leveraging curriculum learning (Liu et al. (2024  ###reference_b24###)) strategies, we significantly refine the LLMs\u2019 proficiency in selecting the appropriate API functions from a pool of similar options. This strategic dataset engineering, combined with our choice of base models, including Codellama7b (Roziere et al. (2023  ###reference_b30###); Touvron et al. (2023  ###reference_b38###)), Google\u2019s Gemma 7B & 2B (Gemma Team, Google DeepMind (2023  ###reference_b11###)), Stable Code 3B(Pinnaparaju et al. (2023  ###reference_b27###)), underscores the effectiveness of our approach, outperforming GPT-4\u2019s benchmarks. Moreover, this ensures the practicality of our solution across various platforms, including mobile devices since these models can already been deployed on mobile (team (2023  ###reference_b36###)).\nTo ensure the consistency of our model\u2019s output formatting, we introduce a conditional masking technique during inference. This innovative approach guarantees that our LLMs generate outputs in the desired formats, markedly improving accuracy and minimize validation loss without sacrificing inference speed. We also prove that the technique of conditional mask will only increase the accuracy mathematically.\nThis advancement, validated across our selected base models, not only showcases the potential of compact LLMs in external API integration but also sets a new efficiency benchmark for scalable AI applications. Through detailed exposition of our model selection and training process, we present a holistic solution that effectively addresses the prevailing challenges in LLM API utility. The dataset used for the LLM training and fine-tuned models will be open sourced soon."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Enhancing LLMs with Tools\u2003The integration of external computational tools within Large Language Models (LLMs) like GPT-4, Alpaca, and Llama signifies a substantial advancement in augmenting their capabilities. Initially, integration efforts were centered around model-specific fine-tuning methods (Lin et al. (2024  ###reference_b23###); Hu et al. (2023  ###reference_b15###)), which, despite their effectiveness, encountered challenges in widespread and flexible application. A notable shift occurred with the adoption of prompts containing exemplary demonstrations, broadening the scope of tool accessibility. This range includes specialized code interpreters and extensive retrieval frameworks, significantly enhancing the models\u2019 ability to interpret and execute complex instructions(Zhou et al. (2023  ###reference_b51###)). Developments in simulated environments for tool interaction (Shen et al. (2024a  ###reference_b31###); Du et al. (2024  ###reference_b8###); Xi et al. (2023  ###reference_b45###)) and frameworks for API engagement (Li et al. (2023  ###reference_b22###)) have been observed as well. Furthermore, the incorporation of advanced reasoning (Valmeekam et al. (2022  ###reference_b40###); Hao et al. (2023  ###reference_b12###); Lewkowycz et al. (2022  ###reference_b21###)) strategies has significantly improved the models\u2019 efficiency in interpreting and solving complex tasks.\n\nDataset format\u2003The optimization of datasets (Zhuang et al. (2024  ###reference_b52###); Kong et al. (2023  ###reference_b20###)) for model fine-tuning is critical for enhancing LLM performance. This process involves a multi-stage refinement utilizing models such as GPT-4 and Alpaca. By iteratively enhancing the dataset, this methodology not only refines prompts but also improves response quality and develops advanced chain-of-thought (Wang et al. (2023  ###reference_b42###); Zhang et al. (2022  ###reference_b48###); Shridhar et al. (2023  ###reference_b33###); Zheng et al. (2023a  ###reference_b49###); Wei et al. (2022  ###reference_b43###)) processes. Such advancements lead to a significant increase in the accuracy of function calling within LLMs, setting new benchmarks in dataset optimization and model training. This iterative refinement represents a strategic shift towards enhancing LLM output precision and quality.\n\nRobustness in LLM Generation\u2003Contrary to article generation, which may accommodate flexible output formats, software applications necessitate strict adherence to specific output structures, such as JSON formatting in Zheng et al. (2023b  ###reference_b50###). And many format consistency problem have been observed in the LLM generation (Vaswani et al. (2017  ###reference_b41###); Ackerman & Cybenko (2023  ###reference_b3###)). Some research has been done to enforce these rigid output formats to maintain consistency and reliability in LLM-generated content. For example, in langchain framework Harrison (2022  ###reference_b13###), there are many output parsers to enforce the formats like YAML, JSON, CSV and so on. However, there are still many cases that can\u2019t be resolved by output parser, especially for the function call response.\n\nAI for Autonomous Vehicle Navigation\u2002Recent advancements in AI have significantly enhanced autonomous vehicle navigation, deploying sophisticated machine learning algorithms to process real-time data for dynamic decision-making. Methods such as deep reinforcement learning (Aradi et al. (2021  ###reference_b1###); Kiran et al. (2022  ###reference_b14###)) and sensor fusion techniques (Chen et al. (2022  ###reference_b4###)) are pivotal in improving path planning and environmental perception."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "In this section, we detail our approach to dataset collection and preparation, introducing the workflow we designed to format the dataset for effective training. We then describe the development of our model, Octopus, highlighting the training techniques and inference strategies we employed. One of the key innovation in our model is the use of a conditional mask for inference enhancement, which represents a novel approach to improving model performance. This methodology combines comprehensive data preparation with advanced modeling techniques to address the challenges of training and inference for function call model development."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Dataset collection and refinement",
            "text": "Our initial dataset comprises API documentation sourced from RapidAPI Hub, one of the world\u2019s largest API repositories. This selection was made based on the website\u2019s claim of millions developers\u2019 engagement. To facilitate the large language model\u2019s comprehension of API usage patterns, we compiled a comprehensive collection of API documentation, focusing on approximately 30,000 of the most frequently utilized APIs. This dataset acquisition was structured in two primary stages: the initial collection and processing of individual API documentation pieces, followed by a meticulous refinement process to optimize the dataset for training purposes.\nSingle API data preprocess\u2003From the detailed exploration of the document, we gather a comprehensive understanding of how RapidHub\u2019s API usage examples are structured and utilized. The approach involves meticulously extracting API usage examples, which detail the API\u2019s name, description, argument names, and their respective descriptions, and formatting this information in JSON. This data is then reorganized using OPENAI GPT-3.5 and CodeLlama 70B models to align with desired organizational standards. Then, we refine the function names based on their descriptions to ensure they are concise and informative. Subsequently, arguments\u2019 names and descriptions are captured. To counteract potential inaccuracies (\u201dhallucinations\u201d) inherent in smaller LLM model, the Python coding format is employed. This decision is strategic, leveraging the models\u2019 inherent code reasoning capabilities from their training on extensive code datasets like in CodeLlama7b and StableCode3B model. This process not only streamlines the API information for enhanced usability, but also leverages advanced AI models to ensure the information is presented in a structured, accessible manner. By prioritizing the function description as a guide for renaming and carefully detailing argument names and descriptions, the approach ensures that the essential elements of API usage are conveyed effectively, supporting developers in integrating these APIs into their projects seamlessly. One example of the converted function can be found below.\nIn our methodology, we deliberately excluded function body for the final dataset compilation. Through a meticulous selection process, we aggregated approximately 20,000 APIs, employing OPENAI GPT-4 for a comprehensive examination and removal of APIs with deficiencies, such as missing arguments or inconsistencies between function descriptions and their parameters. This stringent selection criterion was pivotal in assuring the dataset\u2019s quality. Each API underwent this rigorous scrutiny, culminating in the compilation of dataset A. The dataset A will be the basis for the subsequent data processing.\nDataset refinement\u2003To enhance decision-making in Large Language Models (LLMs) for real-world API usage, we present a sophisticated dataset construction approach, crucial to our study. We begin by integrating various functions, intentionally incorporating some irrelevant functions to create a complex environment for the LLM. Inspired by curriculum learning, we design our dataset to include hard negative samples gradually. This involves introducing similar functions to incrementally raise the challenge of selecting the most relevant function. Our approach is depicted in Figure (1  ###reference_###), illustrating the detailed process of compiling the dataset. Below, we describe the techniques employed.\nNegative samples\u2003To enhance the model\u2019s reasoning capabilities and practical applicability, our methodology involves sampling both positive and negative examples. The ratio of these datasets is represented by the variable  in Figure (1  ###reference_###), serving as an important parameter in our experimental setup. Specifically, in our framework, we select  and  to be equal, setting both values at 1.\nSimilar functions cluster\u2003In our practical implementation, the model selects functions from a diverse pool in response to user queries. To intensify the training challenge, we deliberately complicate the selection process. Specifically, we construct training data by associating a given data point with three semantically similar ones. This process involves calculating vector embeddings from function descriptions, with Milvus facilitating the search. The sampling of three similar functions is determined by their similarity scores, focusing on ranks 5 to 10, to deliberately exclude overly similar functions and avoid redundancy in individual queries. This approach guarantees a challenging training setting, cultivating a model capable of differentiating between closely related functions in practical use cases.\nGPT 4 generated query The creation of a high-quality dataset is crucially dependent on the formulation of qualified queries. In this context, we opt to generate positive queries solvable by a single API. Moreover, for such positive instances, we also generate and incorporate a Chain of Thought (CoT), which is utilized during model training. Recent studies have demonstrated that the addition of CoT not only enhances model performance but also significantly improves its reasoning abilities (Srinivasan et al. (2023  ###reference_b34###)). Note worthily, the creation of qualified queries and auxiliary information is crucial in developing effective training datasets.\nGPT 4 verification Our dataset\u2019s development included an observation regarding the potential error of GPT-4 generated responses, despite its advanced capabilities. Thus, we designed a workflow to let GPT-4 conduct the self-verification, effectively identifying and rectifying inaccuracies in its outputs. After getting dataset B, we employed GPT-4 to meticulously verify and eliminate any data points that failed to meet our stringent quality criteria. This rigorous validation process led to the exclusion of approximately 1000 data points, significantly contributing to our model\u2019s enhanced performance.\n###figure_1### Adhering to the outlined methodology, we are poised to meticulously compile the training dataset, achieving an impressive collection of approximately 150,000 data points. Each individual API is associated with 5 positive queries, which it can resolve. To provide a comprehensive understanding, a sample of the complete dataset has been included in the Appendix (B.1  ###reference_###), showcasing the detailed structure and composition of our training data."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Octopus",
            "text": "To validate the efficacy of our framework, we fine-tuned it on four renowned open-source models: CodeLlama7b, Google Gemma 2B & 7B, and Stable Code LM 3B. A standardized training template was employed across all models, detailed in the Appendix (B.1  ###reference_###). We utilized LoRA and 8-bit quantization techniques, allocating A100 80GB GPU hours as follows: 90h for CodeLlama7b and Google Gemma 7B, 30h for Google Gemma 2B, and 60h for Stable Code LM 3B. The learning rate was set at 510-5, with a linear scheduler optimizing outcomes. In the inference stage, user queries trigger function retrieval and execution, mapping the generated function and its arguments to corresponding APIs for final responses, thus ensuring accurate results upon correct function and argument name generation.\nWe have experimented different lora setup, and we found that the best setup is to choose lora rank as 16, and apply the method to the modulus of \"q_proj\", \"v_proj\", \"o_proj\", \"up_proj\", \"down_proj\". We also attach the training and validation loss for selected models in Figure (2  ###reference_###). During training, we progressively trained on dataset point with more similar examples to do the curriculum learning.\n###figure_2###"
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Inference using conditional mask",
            "text": "The utilization of smaller-parameter Large Language Models (LLMs) has a pivotal challenge: a noticeable decrement in robustness when generating outputs. This challenge is also observed in our model, which necessitates the need to enforce the response with precise function names along with their corresponding arguments. The expected output format demands that arguments be encapsulated within parentheses, function names align with a pre-defined repository, and argument values conform to their designated types. Discrepancies, such as typographical errors in function names or misalignment in argument types, critically undermine the integrity of the output, rendering it susceptible to errors. For instance, both in GPT-4 and our model, deviations in the function name\u2014whether through misspelling or elongated expressions\u2014can lead to unintended corrections that fail to map back to the original function names, thereby distorting the intended output. The original LLM generation process to sample the next token is\nwhere  is all the current tokens, with the sequence length as , and  is the next token to be sampled. What we do here is to introduce another mask dependent on  so that\nIn constructing the mask, we designate all tokens, which are not aligned with correct format, to be masked by assigning a value of 0 to their respective positions, and a value of 1 to all other positions. For example, if we already know the next token represents integers, we will only unmask the tokens that are used for integers. Therefore, the formulation of an accurate mask is paramount for achieving the desired outcome. In this context, we delineate several methodologies that were investigated for the derivation of the mask.\nenum data type\u2003Function names are usually already known, and will not change during inference. We can treat them as enumerable data variables. To efficiently manage these names, a Trie tree can be constructed, facilitating the retrieval of the mask with a time complexity of , where  denotes the Trie tree\u2019s depth, equivalent to the maximum length of a function name, which in our case is approximately 20. This result in the constant time complexity. As an alternative approach, storing all prefixes of potential function names within a dictionary could further reduce the complexity to . The implementation of the Trie class is provided in the Appendix (B.2  ###reference_###).\nstring, float, dict, int type\u2003Regular expressions can be employed to analyze subsequent tokens and generate the conditional mask.\nTherefore, we can confirm that the output result is free from formatting errors.\nOur experimental findings indicate that the application of the conditional mask significantly enhances the robustness of the Large Language Model (LLM) in the context of function calls."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "LLM Evaluation for Function Calling",
            "text": "We conducted a comprehensive series of tests on our dataset, evaluating the Octopus model against other leading models. This evaluation focused on Octopus\u2019s capability to understand API calls, specifically those on RapidAPI. Additionally, we explored the impact of incorporating various retrieval techniques during the training phase on the model\u2019s ultimate effectiveness.\nIn terms of baselines, our primary comparison was with cutting-edge language models in a zero-shot framework. The models we analyzed include GPT-4 by OpenAI, utilizing the gpt-4-0314 checkpoint, and GPT-3.5-turbo, employing the gpt-3.5-turbo-0301 checkpoint. Both models have been refined through RLHF (Reinforcement Learning from Human Feedback) for enhanced conversational abilities."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Evaluation dataset and benchmark",
            "text": "To benchmark function calls within commonly used software APIs, we specifically constructed a dataset. This dataset was generated by randomly selecting four different function APIs and sampling queries that could be addressed by these APIs. The sampling utilized the same prompt template employed during training, details of which are provided in Appendix B.1  ###reference_###. Additionally, we included queries that these APIs could not resolve, maintaining a balanced ratio of solvable to unsolvable queries at 1:1. Ground truth for the dataset was established through human annotation. We applied rigorous standards for benchmarking, focusing on real-world application requirements, including the precise matching of function names and arguments. For models not trained on this dataset, issues related to format correctness were overlooked to provide a more fair comparison. Consequently, in our analysis, GPT 3.5 was not marked incorrect for format discrepancies.\n###figure_3###"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Performance without the conditional mask",
            "text": "In the task of inferring function calls, we initially employed both GPT-3.5 and GPT-4 models to generate responses. For these pretrained models, the greedy search technique was utilized to select responses. This decision was based on the higher precision required for accurately identifying both function names and their corresponding parameters, where the model\u2019s ability to choose the correct function name and parameters is crucial. Therefore, alternative methods such as sampling and beam search were not adopted for this task. The resulting accuracy metrics from this approach are presented in Figure (3  ###reference_###).\nThe results highlight that GPT-4 consistently achieves the highest accuracy in producing correct outcomes. A notable issue leading to inaccuracies with GPT-4 involves \u201dhallucinations,\u201d such as its ability to autocorrect misspelled function names, exemplified by transforming send_emil into send_email. It is critical that the function name provided in the initial prompt remains unaltered, regardless of spelling errors. This correction issue extends to parameters as well; for instance, GPT-4 might generate Australian as a parameter when the query explicitly requires a country name. The primary source of incorrect outputs is attributed to the generation of inaccurate parameters. However, all pretrained models demonstrate near-perfect performance in identifying the correct function name.\n###figure_4###"
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Performance with the conditional mask",
            "text": "In contrast to the inference approach described in the preceding subsection, we implemented a conditional mask during inference for this scenario. This modification has been effective in enhancing outcomes, particularly in the generation of parameters. Utilizing a conditional mask, especially when an input is of an enum type like a country name, helps prevent the model from generating unexpected parameters. The improvements facilitated by this approach are illustrated in Figure (4  ###reference_###). However, since the APIs for GPT-3.5 and GPT-4 do not provide logits, the conditional mask technique could not be applied to these models, and thus, no improvement in their metrics was observed. Nevertheless, it\u2019s noteworthy that two 7B models were able to achieve performance over GPT-4, highlighting the efficacy of the conditional masking technique in certain contexts."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this study, we present a novel framework designed to train large language models on practical software APIs, with a subsequent evaluation of their performance in making API calls, specifically in comparison to the GPT-4 model. Our approach includes a methodology for refining the dataset and the associated prompt template, which incorporates negative sampling and curriculum learning strategies. Additionally, we introduce an innovative technique known as the conditional mask, aimed at addressing the challenge of mismatched output formats."
        }
    ],
    "url": "http://arxiv.org/html/2404.01549v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.2",
            "3.3",
            "4.2",
            "4.3"
        ]
    },
    "research_context": {
        "paper_id": "2404.01549v1",
        "paper_title": "Octopus: On-device language model for function calling of software APIs",
        "research_background": "### Introduction\n\n**Motivation:** The paper is driven by the significant impact and revolutionary capabilities of Large Language Models (LLMs) in fields such as natural language processing, mathematics, healthcare, and legal analysis. Despite these advances, LLMs face persistent challenges, such as adapting to real-time updates and executing specific tasks like image or video editing and complex tax filings. Integrating LLMs with external APIs is posited as a key solution, enhancing the models' capabilities by providing access to up-to-date information and specialized functionalities.\n\n**Research Problem:** The primary problem addressed in this paper is the inefficiency and high operational costs associated with relying exclusively on large-scale LLMs for tasks that utilize only a fraction of available APIs. Additionally, the shift towards smaller, task-specific models introduces risks of errors or \"hallucinations\" in outputs, which are problematic for applications requiring precise output formatting, such as software development.\n\n### Relevant Prior Work\n\n1. **LLMs in Specialized Domains:** \n   - Mathematics: (Imani et al., 2023; He-Yueya et al., 2023)\n   - Healthcare: (Imani et al., 2023; Jo et al., 2023; Thirunavukarasu et al., 2023)\n   - Legal Analysis: (Cui et al., 2023; Fei et al., 2023; Luppi et al., 2022)\n \n2. **Integration with APIs as Solutions:**\n   - The introduction of external APIs to LLMs has led to the creation of novel applications such as code interpreters (Bairi et al., 2023; Vaithilingam et al., 2022; Chen et al., 2021).\n   - Research on function-calling capabilities of open-source language models: (ToolAlpaca by Tang et al., 2023; NexusRaven by Srinivasan et al., 2023).\n\n3. **Efficient Small-Scale Models:**\n   - Studies highlighting the inefficiency of using large models like GPT-4 for specific tasks due to computational resource requirements, advocating for smaller, task-oriented LLMs (Radford et al., 2018; 2019; Brown et al., 2020; Achiam et al., 2023; Wu et al., 2023).\n   - Challenges associated with smaller models, including increased risks of errors and formatting issues (Yao et al., 2023; Zhang et al., 2023; Ji et al., 2023; Jiang et al., 2023).\n\n4. **Model Training Techniques and Base Models:**\n   - Utilization of a large dataset of over 30,000 APIs from Rapid API Hub.\n   - Incorporation of curriculum learning strategies for model training (Liu et al., 2024).\n   - Effective base models: Codellama7b (Roziere et al., 2023; Touvron et al., 2023), Google\u2019s Gemma 7B & 2B (Google DeepMind, 2023), Stable Code 3B (Pinnaparaju et al., 2023).\n\n### Proposed Solutions\n\n1. **Framework Design:** A novel framework designed to enhance LLM training and inference for better integration with external APIs. The framework targets efficiency by emphasizing task-specific models grounded in an extensive and diverse dataset.\n   \n2. **Output Formatting Accuracy:** Introduction of a conditional masking technique during inference to ensure that the smaller LLMs generate correctly formatted outputs, significantly boosting accuracy and minimizing validation loss without sacrificing speed.\n\n3. **Efficiency and Scalability:** Demonstrating the feasibility of compact LLMs on various platforms, including mobile devices, and setting new efficiency benchmarks for scalable AI applications.\n\n**Conclusion:** The proposed framework and techniques not only address the inefficiencies and cost challenges of using large-scale LLMs but also mitigate the risk of errors in smaller models. This holistic solution makes significant strides in the integration of LLMs with external APIs, showcasing practical and efficient AI applications. The open-sourcing of the dataset and fine-tuned models will further contribute to the research and development community.",
        "methodology": "To address the challenges of training and inference for function call model development, we propose Octopus, an on-device language model specifically designed for handling software API function calls. Our methodology can be broken down into several key components and innovations:\n\n1. **Dataset Collection and Preparation:** \n   - We initiated our approach by meticulously collecting and preparing a dataset tailored for effective training. The workflow we designed ensured that the data was formatted appropriately to meet the demands of our model, thereby enhancing the quality and relevance of the training data.\n\n2. **Development of Octopus Model:** \n   - The core of our methodology involved the development of the Octopus model. This included leveraging state-of-the-art training techniques to optimize the model's performance in recognizing and generating function calls for various software APIs.\n\n3. **Inference Strategies:**\n   - A notable innovation in our methodology is the use of a **conditional mask for inference enhancement**. This novel approach significantly boosts the model's performance by guiding it during the inference phase to make more accurate and contextually appropriate function calls.\n\n4. **Comprehensive Data Preparation:**\n   - Our method underscores the importance of thorough and detailed data preparation. By ensuring that the dataset is not only comprehensive but also precisely formatted, we lay a strong foundation for the subsequent model training phase.\n\n5. **Advanced Modeling Techniques:**\n   - The combination of these advanced techniques ensures that the model is not only robust but also proficient in tackling the complexities involved in function call generation and recognition within different software environments.\n\nBy integrating these components, our methodology offers a robust framework for developing a highly effective on-device language model that excel",
        "main_experiment_and_results": "### Main Experiment Setup\n\n1. **Datasets**:\n   - The principal dataset comprises API calls sourced from RapidAPI, which were utilized to evaluate the model's comprehension and generation capabilities.\n\n2. **Baselines**:\n   - **GPT-4 (OpenAI)**: Using the gpt-4-0314 checkpoint.\n   - **GPT-3.5-turbo (OpenAI)**: Using the gpt-3.5-turbo-0301 checkpoint.\n   - Both baseline models are fine-tuned with RLHF to enhance their conversational skills.\n\n3. **Experimental Framework**:\n   - **Zero-Shot Evaluation**: The models, including Octopus, were assessed in a zero-shot setup without further specific fine-tuning on the task data.\n   - **Model Comparison**: Octopus's performance was gauged against the leading models GPT-4 and GPT-3.5-turbo, focusing on their ability to accurately understand and generate API calls.\n\n### Evaluation Metrics\n\n- **Accuracy**: Measurement of correct API calls interpreted and generated by the model.\n- **Effectiveness**: Assessed through the accurate application and understanding of API calls.\n- **Comparative Performance**: Relative performance of Octopus compared to GPT-4 and GPT-3.5-turbo in the same zero-shot context.\n\n### Main Experimental Results\n\n- **Octopus Performance**: Demonstrated significant capabilities in understanding and generating API calls from RapidAPI.\n- **Comparison**: Outperformed state-of-the-art language models GPT-4 and GPT-3.5-turbo in specific aspects of API call comprehension and generation in zero-shot scenarios.\n\n### Summary\nThe main experiment showcases Octopus as a robust model for on-device language understanding and generation of software API calls, highlighting its competitive edge over existing state-of-the-art models (GPT-4 and GPT-3.5-turbo) in a zero-shot framework."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To validate the efficacy of the Octopus framework by fine-tuning it on multiple open-source models and determining the best LoRA setup for enhancing software API interactions.",
            "experiment_process": "The researchers fine-tuned four open-source models: CodeLlama7b, Google Gemma 2B & 7B, and Stable Code LM 3B. LoRA and 8-bit quantization techniques were used, with GPU hours allocated as 90h for CodeLlama7b and Google Gemma 7B, 30h for Google Gemma 2B, and 60h for Stable Code LM 3B on an A100 80GB GPU. A learning rate of 510-5 was set with a linear scheduler for optimization. Different LoRA setups were tested, and the optimal setup was using a LoRA rank of 16 applied to specific modules. Training also incorporated curriculum learning by progressively training on more similar examples.",
            "result_discussion": "The optimal LoRA setup was found to be a rank of 16 applied to modules q_proj, v_proj, o_proj, up_proj, and down_proj. The training and validation loss for the selected models indicated effective training progression.",
            "ablation_id": "2404.01549v1.No1"
        },
        {
            "research_objective": "To improve the output robustness of smaller-parameter LLMs during function calls by enforcing conditional masks ensuring correct function names and arguments.",
            "experiment_process": "The study explored enforcing a conditional mask to ensure responses included precise function names and corresponding arguments. The mask construction involved masking tokens not aligned with the correct format. For function names, a Trie tree was used for efficient mask retrieval, and for data types (string, float, dict, int), regular expressions were employed. The mask ensured tokens representing integers were unmasked when needed, thus preventing formatting errors.",
            "result_discussion": "Applying the conditional mask significantly enhanced the robustness of the LLM in generating accurate function calls. This technique improved the precision of outputs by ensuring correct function names and argument types.",
            "ablation_id": "2404.01549v1.No2"
        },
        {
            "research_objective": "To analyze the performance of function calls using pretrained GPT-3.5 and GPT-4 models without the application of conditional masks.",
            "experiment_process": "GPT-3.5 and GPT-4 models were initially used to generate infer function calls, utilizing the greedy search technique to select responses given the necessity for precise identification of function names and parameters. The accuracy metrics were compiled and results analyzed.",
            "result_discussion": "GPT-4 achieved the highest accuracy, though a significant issue was the autocorrection of misspelled function names, which distorted intended outputs. GPT-4 generated occasional inaccuracies in parameters, highlighting the issue of \u201challucinations.\u201d However, all pretrained models identified the correct function name with near-perfect accuracy.",
            "ablation_id": "2404.01549v1.No3"
        },
        {
            "research_objective": "To assess the impact of implementing conditional masks during inference on function call performance.",
            "experiment_process": "A conditional mask was employed during inference to enhance the accuracy of parameter generation, especially for enum types like country names. The approach was evaluated and compared against metrics from previous experiments without conditional masks.",
            "result_discussion": "The implementation of conditional masking showed significant improvement in parameter generation and overall performance, enabling two 7B models to surpass GPT-4. However, due to the lack of logit outputs in GPT-3.5 and GPT-4 APIs, the conditional mask could not be applied to these models, resulting in no improvement in their metrics.",
            "ablation_id": "2404.01549v1.No4"
        }
    ]
}