{
    "title": "ToXCL: A Unified Framework for Toxic Speech Detection and Explanation",
    "abstract": "The proliferation of online toxic speech is a pertinent problem posing threats to demographic groups. While explicit toxic speech contains offensive lexical signals, implicit one consists of coded or indirect language. Therefore, it is crucial for models not only to detect implicit toxic speech but also to explain its toxicity. This draws a unique need for unified frameworks that can effectively detect and explain implicit toxic speech. Prior works mainly formulated the task of toxic speech detection and explanation as a text generation problem. Nonetheless, models trained using this strategy can be prone to suffer from the consequent error propagation problem. Moreover, our experiments reveal that the detection results of such models are much lower than those that focus only on the detection task. To bridge these gaps, we introduce ToXCL111https://github.com/NhatHoang2002/ToXCL, a unified framework for the detection and explanation of implicit toxic speech. Our model consists of three modules: a (i) Target Group Generator to generate the targeted demographic group(s) of a given post; an (ii) Encoder-Decoder Model in which the encoder focuses on detecting implicit toxic speech and is boosted by a (iii) Teacher Classifier via knowledge distillation, and the decoder generates the necessary explanation. ToXCL achieves new state-of-the-art effectiveness, and outperforms baselines significantly.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Warning: This paper discusses and contains content that can be offensive or upsetting.\nWhile social media has dramatically expanded democratic participation in public discourse, they have also become a widely recognized platform for the dissemination of toxic speech Mathew et al. (2021  ###reference_b36###); ElSherief et al. (2021  ###reference_b17###); Yu et al. (2022  ###reference_b54###). Online toxic speech, therefore, is prevalent and can lead the victims to serious consequences Olteanu et al. (2018  ###reference_b38###); Farrell et al. (2019  ###reference_b18###). For this reason, the development of toxic detection tools has received growing attention in recent years Hutto and Gilbert (2014  ###reference_b25###); Ribeiro et al. (2018  ###reference_b42###); Balkir et al. (2022  ###reference_b3###); Jahan and Oussalah (2023  ###reference_b27###).\n###figure_1### Toxic speech can generally be categorized as either explicit or implicit. Explicit toxic speech contains direct offensive language targeting individuals or groups Nockleyby (2000  ###reference_b37###) and has been extensively studied Schmidt and Wiegand (2017  ###reference_b45###); Jahan and Oussalah (2021  ###reference_b26###). On the other hand, implicit toxic speech presents a more challenging detection task as it relies on stereotypes and indirect language ElSherief et al. (2021  ###reference_b17###) (see Fig. 1  ###reference_###) and has received limited attention. Moreover, given the absence of explicit offensive words or cues, it is crucial for AI models not only to detect implicit toxic speech but also to provide explanations for its toxic nature Sridhar and Yang (2022  ###reference_b46###). The act of explanation serves practical purposes in various real-life applications, including improving human-machine interactions and building trustworthy AI systems Ribeiro et al. (2016  ###reference_b43###); Dosilovic et al. (2018  ###reference_b15###); Bai et al. (2022  ###reference_b2###).\nThese applications, therefore, pose a need for unified systems that can effectively detect implicit toxic speech and explain its toxicity. However, previous works have mainly focused on a hybrid approach that combines detection and explanation tasks into a single text generation problem. For example, Sap et al. (2020  ###reference_b44###) proposed concatenating the toxic speech label and explanation as the target output, AlKhamissi et al. (2022  ###reference_b1###) and Huang et al. (2022  ###reference_b24###) extended this approach by incorporating additional data such as target group(s) or social norms. Unfortunately, these hybrid approaches can introduce error propagation problems Wu et al. (2018  ###reference_b53###), possibly due to differences in training objectives (see Sec. 3.2.3  ###reference_.SSS3###). Consequently, models formulated in this manner tend to have much lower detection scores compared to models that focus solely on detection, as evidenced by our experimentation results (Sec. 4  ###reference_###). Another simple approach is building a modular-based system separating the detection module and the explanation generation module. However, in reality, this kind of framework is computationally expensive to train, store and deploy as it has multiple components.\nTo bridge these gaps in detecting and explaining implicit toxic speech, we propose a unified framework ToXCL consisting of three modules (Fig. 2  ###reference_###). Our approach is motivated by the findings that modeling the minority target groups associated with toxic speech can potentially improve the performance of both implicit toxic detection and explanation tasks ElSherief et al. (2018  ###reference_b16###); AlKhamissi et al. (2022  ###reference_b1###); Huang et al. (2022  ###reference_b24###). To achieve this, we build a  Target Group Generator as our first module to generate the target minority group(s) based on the input post. The generated target group(s) and the post are then input into an  Encoder-Decoder Model whose encoder detects the speech, and the decoder outputs the necessary toxic explanation. To enhance the detection performance of our encoder, we incorporate a strong Teacher Classifier that utilizes the teacher forcing technique during training to distill knowledge to our encoder classifier. Finally, we introduce a Conditional Decoding Constraint to enhance the explanation ability of the decoder during inference. Our model achieves state-of-the-art performance on the Implicit Hate Corpus (IHC) ElSherief et al. (2021  ###reference_b17###) and Social Bias Inference Corpus (SBIC) Sap et al. (2020  ###reference_b44###) in the task of implicit hate speech detection and explanation, outperforming baselines. Our contributions are as follows:\n(i) We present a unified framework for the detection and explanation of implicit toxic speech. To the best of our knowledge, our work represents a pioneering effort in integrating both tasks into an encoder-decoder model to avoid the error propagation problem while maintaining the competitive performance on both tasks parameter-efficiently.\n(ii) We propose to generate target groups for the toxic speech detection and explanation model through the utilization of an encoder-decoder model, thereby distinguishing our approach from previous methods (see Sec. 3.2.1  ###reference_.SSS1###). We also introduce several techniques to enhance the performance of our model: (1) joint training among the tasks to make the detection and explanation model end-to-end; (2) using teacher forcing to train the encoder; (3) a simple Conditional Decoding Constraint during the inference to avoid generating unnecessary explanation.\n(iii) We set up new strong state-of-the-art results in the task of implicit toxic speech detection and explanation tasks in two widely used benchmarks Implicit Hate Corpus (IHC) and Social Bias Inference Corpus (SBIC).\n(iv) We conduct a thorough analysis to demonstrate the effectiveness of our architectural design. We will open-source our model to inspire and facilitate future research.\n###figure_2###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Toxic Speech Detection & Explanation",
            "text": "Early studies on identifying toxic speech relied on linguistic rule-based approaches Chen et al. (2012  ###reference_b9###); Hutto and Gilbert (2014  ###reference_b25###); Gitari et al. (2015  ###reference_b20###); Wiegand et al. (2018  ###reference_b51###). However, these methods, which use predetermined lexical and syntactic rules, struggle to detect implicit toxic speech without explicit vulgarities Breitfeller et al. (2019  ###reference_b6###); MacAvaney et al. (2019  ###reference_b35###). Recent frameworks based on transformer architecture Vaswani et al. (2017  ###reference_b49###) have made progress in detecting toxic speech Basile et al. (2019  ###reference_b5###); Tran et al. (2020  ###reference_b47###); Kennedy et al. (2020  ###reference_b28###). However, detecting implicit toxic speech remains challenging despite attempts to improve performance on this task Vidgen et al. (2019  ###reference_b50###); Caselli et al. (2020  ###reference_b8###, 2021  ###reference_b7###); Kim et al. (2022  ###reference_b29###). The issue of explaining why a text is toxic has received even more limited attention, with some studies focusing solely on explaining implicit toxic speech ElSherief et al. (2021  ###reference_b17###); Sridhar and Yang (2022  ###reference_b46###). Another few studies have addressed both implicit toxic speech detection and explanation Sap et al. (2020  ###reference_b44###); AlKhamissi et al. (2022  ###reference_b1###); Huang et al. (2022  ###reference_b24###), often formulating them as text generation tasks, possibly leading to error propagation and lower detection scores compared to detection-only models.\n\nAI technologies have made significant advancements in healthcare for disease prediction and diagnosis, leveraging large datasets and deep learning algorithms Razzak et al. (2018 ###reference_b40###); Esteva et al. (2017 ###reference_b18###). Machine learning models, especially neural networks, have proven effective in identifying patterns indicative of various diseases, enhancing early diagnosis accuracy and improving patient outcomes Miotto et al. (2016 ###reference_b37###); Rajkomar et al. (2019 ###reference_b39###)."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Knowledge Distillation",
            "text": "Knowledge distillation Hinton et al. (2015 ###reference_b23###) is a technique that enables a smaller student model to learn from a larger teacher model by transferring knowledge. It has proven effective in improving performance, reducing computational requirements, and increasing efficiency in the field of Computer Vision Gou et al. (2021 ###reference_b22###). Recently, researchers have explored applying knowledge distillation in Natural Language Processing. For example, Fu et al. (2020 ###reference_b19###) used a contrastive approach to align the intermediate layer outputs of the teacher and student models. Turc et al. (2019 ###reference_b48###) extensively studied the interaction between pre-training, distillation, and fine-tuning, demonstrating the effectiveness of pre-trained distillation in tasks like sentiment analysis. Additionally, Clark et al. (2019 ###reference_b11###) trained a multitasking network by ensembling multiple single-task teachers. In our work, we distill the knowledge from a teacher classifier to our model\u2019s classifier (the student classifier), optimizing the Kullback-Leibler distance Csisz\u00e1r (1975 ###reference_b13###) between soft labels. AI has been increasingly leveraged in healthcare for disease prediction and diagnosis, enhancing the accuracy and speed of diagnostic processes. This technology employs machine learning models to predict disease outbreaks and diagnose conditions based on medical imaging and patient data, revolutionizing personal and public health management."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "To enable the implicit toxicity detection capability, we build a binary classifier head on top of the encoder of a pre-trained encoder-decoder model. This head includes a linear layer that takes the average of token embeddings from the encoder\u2019s last hidden state as input, followed by a softmax layer Goodfellow et al. (2016  ###reference_b21###). To optimize the performance, both the encoder and the newly added classifier head are trained together using a binary cross-entropy loss:\nin which ,  and .\nRecognizing the importance of generating explanations for implicit toxic speech and its potential impact on various applications, we utilize the decoder of our pre-trained encoder-decoder model to generate the explanation. To optimize its performance, the decoder is fine-tuned using a Causal Language Modeling (CLM) loss:\nin which  is the set of tokens of the explanation, and  is the size of the window.\nFinally, we train the encoder-decoder model for the task of toxic speech detection and explanation by joining the two losses:\nin which  are the contribution weights."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Problem Formulation",
            "text": "The task of implicit toxic speech detection can be formulated as a binary classification problem while the explanation generation task can be regarded as a text generation problem. Each data instance  consists of an input post , a binary class label  ( for toxic speech,  for non-toxic speech), and a corresponding explanation  ([None] for non-toxic speech). The models then take  as the input and learn to output  and ."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "ToXCL Framework",
            "text": "Figure 2  ###reference_### shows an overview of our proposed ToXCL, consisting of three modules: (i) Target Group Generator; (ii) Encoder-Decoder Model; (iii) Teacher Classifier. The details of each module are presented below.\nTo enable the implicit toxicity detection capability, we build a binary classifier head on top of the encoder of a pre-trained encoder-decoder model. This head includes a linear layer that takes the average of token embeddings from the encoder\u2019s last hidden state as input, followed by a softmax layer Goodfellow et al. (2016  ###reference_b21###  ###reference_b21###). To optimize the performance, both the encoder and the newly added classifier head are trained together using a binary cross-entropy loss:\nin which ,  and .\nRecognizing the importance of generating explanations for implicit toxic speech and its potential impact on various applications, we utilize the decoder of our pre-trained encoder-decoder model to generate the explanation. To optimize its performance, the decoder is fine-tuned using a Causal Language Modeling (CLM) loss:\nin which  is the set of tokens of the explanation, and  is the size of the window.\nFinally, we train the encoder-decoder model for the task of toxic speech detection and explanation by joining the two losses:\nin which  are the contribution weights."
        },
        {
            "section_id": "3.2.1",
            "parent_section_id": "3.2",
            "section_name": "3.2.1 Target Group Generator (TG)",
            "text": "Current toxic speech detection systems often overlook the nuances of toxic speech, which can be better addressed by modeling the minority target groups associated with it ElSherief et al. (2018  ###reference_b16###). Incorporating target group information has the potential to improve the accuracy of toxic speech detection and enable the generation of high-quality hate speech explanations Huang et al. (2022  ###reference_b24###). Therefore, we propose using a transformer-based encoder-decoder model Raffel et al. (2020  ###reference_b41###) to generate target minority groups for given posts, treating the task as a text generation problem rather than a multi-label classification task. This approach provides two advantages over classification models. Firstly, it leverages powerful pre-trained encoder-decoder models, enhancing the model\u2019s capabilities. Secondly, text generation models are more generalizable, as they are not restricted to a fixed number of target groups, allowing for greater flexibility in handling diverse scenarios.\nAfter generating target groups  based on an input post ,  and  are concatenated as \"Target:{G} Post:{IP}\" and serve as the input for our ToXCL. The experimental details of the TG module are presented in Section 4.1  ###reference_###."
        },
        {
            "section_id": "3.2.2",
            "parent_section_id": "3.2",
            "section_name": "3.2.2 Encoder-Decoder Model",
            "text": "Toxic speech detection and toxicity explanation are two tasks that have received increasing attention, and while researchers have made significant progress in separately solving each problem, addressing them together has received limited attention Sap et al. (2020  ###reference_b44###); AlKhamissi et al. (2022  ###reference_b1###); Huang et al. (2022  ###reference_b24###). However, these two tasks are strongly correlated, and the explanation of the post can potentially help the systems to detect toxic speech AlKhamissi et al. (2022  ###reference_b1###). Conversely, the toxicity explanation is sometimes only necessary when the post is detected as toxic. Typically, AlKhamissi et al. (2022  ###reference_b1###); Huang et al. (2022  ###reference_b24###) formulate both tasks as a single text generation task, which has some critical shortcomings as discussed in Section 1  ###reference_###. Therefore, in this work, we propose a novel architectural design on top of a pre-trained encoder-decoder model. The encoder addresses the toxic speech detection task, while the decoder generates a toxicity explanation if the post is detected as toxic. The details of both the encoder and decoder components are introduced below.\nTo enable the implicit toxicity detection capability, we build a binary classifier head on top of the encoder of a pre-trained encoder-decoder model. This head includes a linear layer that takes the average of token embeddings from the encoder\u2019s last hidden state as input, followed by a softmax layer Goodfellow et al. (2016  ###reference_b21###  ###reference_b21###  ###reference_b21###). To optimize the performance, both the encoder and the newly added classifier head are trained together using a binary cross-entropy loss:\nin which ,  and .\nRecognizing the importance of generating explanations for implicit toxic speech and its potential impact on various applications, we utilize the decoder of our pre-trained encoder-decoder model to generate the explanation. To optimize its performance, the decoder is fine-tuned using a Causal Language Modeling (CLM) loss:\nin which  is the set of tokens of the explanation, and  is the size of the window.\nFinally, we train the encoder-decoder model for the task of toxic speech detection and explanation by joining the two losses:\nin which  are the contribution weights."
        },
        {
            "section_id": "3.2.3",
            "parent_section_id": "3.2",
            "section_name": "3.2.3 Teacher Classifier (TC)",
            "text": "Since the open-sourced encoder-decoder models are commonly pre-trained on a diverse range of tasks and these tasks might not solely focus on learning strong representations from their encoders, these encoders may not exhibit the same strength as pre-trained encoder-based models such as BERT Devlin et al. (2019  ###reference_b14###) or RoBERTa Liu et al. (2019  ###reference_b33###) for classification tasks. Motivated by Hinton et al. (2015  ###reference_b23###), we propose to use knowledge distillation to transfer knowledge from a strong encoder-based model (Teacher Classifier) to the Flan-T5 encoder (Student Classifier). Specifically, we leverage the outputs  and  from the Teacher Classifier and Student Classifier, respectively, and employ the Kullback-Leibler divergence loss Csisz\u00e1r (1975  ###reference_b13###) as the teacher forcing loss to minimize the discrepancy between  and :\nOur final loss  is the weighted sum of :\nin which  are the contribution weights."
        },
        {
            "section_id": "3.2.4",
            "parent_section_id": "3.2",
            "section_name": "3.2.4 Conditional Decoding Constraint (CD)",
            "text": "One of the main challenges with unified frameworks for toxic speech detection and explanation is the lack of synchronization between the classifier\u2019s output label and the generated explanation. For instance, when the classifier outputs a label of , indicating non-toxic speech, the explanation generation module still generates an explanation, even though it is unnecessary in this case. To address this, we propose the Conditional Decoding Constraint, a simple yet effective algorithm. This constraint controls the decoder\u2019s generation process by generating a [None] token for non-toxic speech and a toxic explanation for toxic speech. By incorporating this constraint, our framework ensures coherence and alignment between the generated explanations and classifier outputs, enhancing its overall performance."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experimentation",
            "text": "To address the problem of free-text target group labelling in IHC and SBIC datasets, we utilized the HateXplain dataset Mathew et al. (2021  ###reference_b36###), which provides 19 fine-grained categories for toxic speech. We fine-tune a T5 model Raffel et al. (2020  ###reference_b41###) as our TG model, and treat it as a text generation problem. To ensure our framework can predict specific target group(s) associated with posts from IHC and SBIC datasets, we conducted an analysis to identify any overlapping data between the HateXplain and IHC, SBIC datasets. We found only one instance of overlap, which we removed before training our TG model.\nWe compare the performance of our TG model with three baseline models: (1) BERT Devlin et al. (2019  ###reference_b14###), an encoder-based model; (2) GPT-2 Radford et al. (2019  ###reference_b40###), a decoder-only model, (3) and BART Lewis et al. (2020  ###reference_b31###), an encoder-decoder model. BERT is widely used for multi-label classification tasks while both GPT-2 and BART have demonstrated remarkable performance in text\nWe concatenate the annotated target group(s) in alphabetical order to construct the target label for each input post. All baselines and our TG model are initialized with pre-trained checkpoints from Huggingface Wolf et al. (2020  ###reference_b52###) and fine-tuned on a single Google CoLab P40 GPU with a window size of 256, a learning rate of , and AdamW Loshchilov and Hutter (2019  ###reference_b34###) as the optimizer. The BERT model is fine-tuned for 10 epochs, while the GPT-2 and BART models are fine-tuned for 20k iterations. We use a beam search strategy with a beam size of 4 for our generation decoding strategy.\nOur TG model is evaluated using F1 (%) for multi-label classification and ROUGE-L (%) Lin (2004  ###reference_b32###). The results in Table 1  ###reference_### indicate that our model achieved an F1 score of  and a ROUGE-L score of , outperforming the competing baselines in identifying target groups in toxic posts.\nWe conduct our experiments on two datasets: IHC ElSherief et al. (2021  ###reference_b17###) and SBIC Sap et al. (2020  ###reference_b44###). These datasets are collected from popular social media platforms such as Twitter and Gab, providing comprehensive coverage of the most prevalent toxic groups. Prior to training, we pre-process the data as detailed in Appendix A  ###reference_###.\nWe compare ToXCL with three groups of baselines: (G1) implicit toxic speech detection, (G2) implicit toxic speech explanation, and (G3) implicit toxic speech detection and explanation.\nFor baselines in group G1, we use BERT Devlin et al. (2019  ###reference_b14###), HateBERT Caselli et al. (2021  ###reference_b7###), RoBERTa Liu et al. (2019  ###reference_b33###) and ELECTRA Clark et al. (2020  ###reference_b12###) as our baselines. They are widely employed in prior toxic speech detection works.\nWe select GPT-2 Radford et al. (2019  ###reference_b40###), BART Lewis et al. (2020  ###reference_b31###), T5 Raffel et al. (2020  ###reference_b41###), and Flan-T5 Chung et al. (2022  ###reference_b10###) as our baselines G2 and G3. GPT-2 represents the group of decoder-only models, while BART, T5, and Flan-T5 have the encoder-decoder architecture. Specifically for group G2, we fine-tune them to generate [None] token or the explanations\u2019 tokens. For group G3, we concatenate the classification label [Toxic]/[Non-toxic] and the explanation of each sample as the output, and fine-tune the models with the post as input. Furthermore, to align with recent advancements in Large Language Models (LLMs), we further include ChatGPT 222Version: gpt-3.5-turbo-0613 (a state-of-the-art closed-source LLM) and Mistral-7B-Instruct-v0.2 (a state-of-the-art open-source LLM) in group G3. Both models are evaluated under the zero-shot setting.\nWe initialize all the models with the pre-trained checkpoints from Huggingface Wolf et al. (2020  ###reference_b52###). We then fine-tune them on a single Google CoLab P40 GPU with a window size of 256, and a learning rate of  and use AdamW Loshchilov and Hutter (2019  ###reference_b34###) as our optimizer. The classification baselines in group G1 are fine-tuned on  epochs while the generation ones in G2 and G3 are fine-tuned on k iterations. Beam search strategy with a beam size of  is utilized as our generation decoding strategy.\nWe adopt Accuracy and Macro F1 as our classification metrics, following prior works Mathew et al. (2021  ###reference_b36###); ElSherief et al. (2021  ###reference_b17###). For the generation of explanations, we utilize BLEU-4 Papineni et al. (2002  ###reference_b39###), ROUGE-L Lin (2004  ###reference_b32###) and METEOR Banerjee and Lavie (2005  ###reference_b4###) as our n-gram metrics. We further utilize BERTScore Zhang et al. (2020  ###reference_b55###) to measure the similarity between the generated toxic explanation and the ground truth one based on deep-contextual embeddings. To ensure that unnecessary explanations are not generated for non-toxic posts and penalize unnecessary explanations, we develop a new evaluation algorithm for the explanation generation task. Its pseudo-code is presented in Algorithm 1  ###reference_###. In this algorithm, we assign a score of  if both the generated explanation and the ground truth explanation are [None] indicating that no explanation is needed. If both the generated explanation and the ground truth explanation are not [None] we compute a score based on the quality of the generated explanation. For any other mismatched pairs, we assign a score of 0 to penalize the unnecessary explanations for non-toxic speech. It is worth noting that our evaluation algorithm is different from Sridhar and Yang (2022  ###reference_b46###) which only evaluates the quality of the generation within implicit toxic cases.\nTo gain deeper insights into the generation performance and challenges that our ToXCL faces compared to the competing baseline, we conduct human evaluations using a randomly selected set of 150 samples that require explanations from each examined benchmark. Specifically, we collect the generated explanations from both the ToXCL and Flan-T5 models in two different settings, G2 and G3. To ensure high-quality evaluations, five native English speakers are hired to rate the generated explanations on a 1-3 scale (with 3 being the highest) based on three criteria: (i) Correctness, evaluating the accuracy of the explanation in correctly explaining the meaning of toxic speech; (ii) Fluency, assessing the fluency and coherence of the generated explanation in terms of language use; and (iii) Toxicity, gauging the level of harmfulness and judgmental tone exhibited in the generated explanation. The annotator agreement is measured using Krippendorff\u2019s alpha Krippendorff (2011  ###reference_b30###), which provides a measure of inter-annotator reliability.\nOur experimental results in Table 3  ###reference_### reveal three main observations. Firstly, ToXCL outperforms all baselines on both benchmarks, demonstrating the effectiveness of our encoder-decoder model in addressing both implicit toxic detection and explanation tasks simultaneously without conflicts. Secondly, our model surpasses detection models in group G1, indicating the strong capability of our encoder in detecting implicit toxic speech. It is worth noting that despite having fewer parameters than the RoBERTa-Base model (124M), our encoder (Flan-T5\u2019s) classifier (109M) achieves better performance while maintaining computational efficiency. Lastly, our model significantly outperforms its backbone, Flan-T5, highlighting the effectiveness of our architectural designs in jointly training the tasks in an end-to-end manner for implicit toxic speech detection and explanation problems.\nOur human evaluation results in Table 4  ###reference_### indicate that ToXCL outperforms its backbone model, Flan-T5, from groups G2 and G3, in terms of both explanation accuracy and textual fluency. This improved performance is also reflected in the detection task, resulting in more reliable explanations with fewer harmful outputs compared to the baselines. Our human annotators exhibit strong agreement with Krippendorff\u2019s alpha scores consistently measuring at least 0.78 among the three scores. It is worth noting that the average toxicity score of around 2 for both our ToXCL and baseline models aligns with expectations, given that the training datasets contain offensive words in the ground truth explanations (see Tab. 5  ###reference_###)."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Target Group Generator Experiment",
            "text": "To address the problem of free-text target group labelling in IHC and SBIC datasets, we utilized the HateXplain dataset Mathew et al. (2021  ###reference_b36###  ###reference_b36###), which provides 19 fine-grained categories for toxic speech. We fine-tune a T5 model Raffel et al. (2020  ###reference_b41###  ###reference_b41###) as our TG model, and treat it as a text generation problem. To ensure our framework can predict specific target group(s) associated with posts from IHC and SBIC datasets, we conducted an analysis to identify any overlapping data between the HateXplain and IHC, SBIC datasets. We found only one instance of overlap, which we removed before training our TG model.\nWe compare the performance of our TG model with three baseline models: (1) BERT Devlin et al. (2019  ###reference_b14###  ###reference_b14###), an encoder-based model; (2) GPT-2 Radford et al. (2019  ###reference_b40###  ###reference_b40###), a decoder-only model, (3) and BART Lewis et al. (2020  ###reference_b31###  ###reference_b31###), an encoder-decoder model. BERT is widely used for multi-label classification tasks while both GPT-2 and BART have demonstrated remarkable performance in text\nWe concatenate the annotated target group(s) in alphabetical order to construct the target label for each input post. All baselines and our TG model are initialized with pre-trained checkpoints from Huggingface Wolf et al. (2020  ###reference_b52###  ###reference_b52###) and fine-tuned on a single Google CoLab P40 GPU with a window size of 256, a learning rate of , and AdamW Loshchilov and Hutter (2019  ###reference_b34###  ###reference_b34###) as the optimizer. The BERT model is fine-tuned for 10 epochs, while the GPT-2 and BART models are fine-tuned for 20k iterations. We use a beam search strategy with a beam size of 4 for our generation decoding strategy.\nOur TG model is evaluated using F1 (%) for multi-label classification and ROUGE-L (%) Lin (2004  ###reference_b32###  ###reference_b32###). The results in Table 1  ###reference_###  ###reference_### indicate that our model achieved an F1 score of  and a ROUGE-L score of , outperforming the competing baselines in identifying target groups in toxic posts."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Teacher Classifier Experiment",
            "text": "For our Teacher Classifier, a RoBERTa-Large model Liu et al. (2019  ###reference_b33###) is fine-tuned using the generated target group(s) (TG in Section 3  ###reference_###). The model achieved an F1 score of  on IHC and  on SBIC, indicating the effectiveness of generated target group(s) in classifying toxic speech. Detailed results can be found in Table 2  ###reference_###."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "ToXCL Experiment",
            "text": "We conduct our experiments on two datasets: IHC ElSherief et al. (2021  ###reference_b17###  ###reference_b17###) and SBIC Sap et al. (2020  ###reference_b44###  ###reference_b44###). These datasets are collected from popular social media platforms such as Twitter and Gab, providing comprehensive coverage of the most prevalent toxic groups. Prior to training, we pre-process the data as detailed in Appendix A  ###reference_###  ###reference_###.\nWe compare ToXCL with three groups of baselines: (G1) implicit toxic speech detection, (G2) implicit toxic speech explanation, and (G3) implicit toxic speech detection and explanation.\nFor baselines in group G1, we use BERT Devlin et al. (2019  ###reference_b14###  ###reference_b14###), HateBERT Caselli et al. (2021  ###reference_b7###  ###reference_b7###), RoBERTa Liu et al. (2019  ###reference_b33###  ###reference_b33###) and ELECTRA Clark et al. (2020  ###reference_b12###  ###reference_b12###) as our baselines. They are widely employed in prior toxic speech detection works.\nWe select GPT-2 Radford et al. (2019  ###reference_b40###  ###reference_b40###), BART Lewis et al. (2020  ###reference_b31###  ###reference_b31###), T5 Raffel et al. (2020  ###reference_b41###  ###reference_b41###), and Flan-T5 Chung et al. (2022  ###reference_b10###  ###reference_b10###) as our baselines G2 and G3. GPT-2 represents the group of decoder-only models, while BART, T5, and Flan-T5 have the encoder-decoder architecture. Specifically for group G2, we fine-tune them to generate [None] token or the explanations\u2019 tokens. For group G3, we concatenate the classification label [Toxic]/[Non-toxic] and the explanation of each sample as the output, and fine-tune the models with the post as input. Furthermore, to align with recent advancements in Large Language Models (LLMs), we further include ChatGPT 222Version: gpt-3.5-turbo-0613 (a state-of-the-art closed-source LLM) and Mistral-7B-Instruct-v0.2 (a state-of-the-art open-source LLM) in group G3. Both models are evaluated under the zero-shot setting.\nWe initialize all the models with the pre-trained checkpoints from Huggingface Wolf et al. (2020  ###reference_b52###  ###reference_b52###). We then fine-tune them on a single Google CoLab P40 GPU with a window size of 256, and a learning rate of  and use AdamW Loshchilov and Hutter (2019  ###reference_b34###  ###reference_b34###) as our optimizer. The classification baselines in group G1 are fine-tuned on  epochs while the generation ones in G2 and G3 are fine-tuned on k iterations. Beam search strategy with a beam size of  is utilized as our generation decoding strategy.\nWe adopt Accuracy and Macro F1 as our classification metrics, following prior works Mathew et al. (2021  ###reference_b36###  ###reference_b36###); ElSherief et al. (2021  ###reference_b17###  ###reference_b17###). For the generation of explanations, we utilize BLEU-4 Papineni et al. (2002  ###reference_b39###  ###reference_b39###), ROUGE-L Lin (2004  ###reference_b32###  ###reference_b32###) and METEOR Banerjee and Lavie (2005  ###reference_b4###  ###reference_b4###) as our n-gram metrics. We further utilize BERTScore Zhang et al. (2020  ###reference_b55###  ###reference_b55###) to measure the similarity between the generated toxic explanation and the ground truth one based on deep-contextual embeddings. To ensure that unnecessary explanations are not generated for non-toxic posts and penalize unnecessary explanations, we develop a new evaluation algorithm for the explanation generation task. Its pseudo-code is presented in Algorithm 1  ###reference_###  ###reference_###. In this algorithm, we assign a score of  if both the generated explanation and the ground truth explanation are [None] indicating that no explanation is needed. If both the generated explanation and the ground truth explanation are not [None] we compute a score based on the quality of the generated explanation. For any other mismatched pairs, we assign a score of 0 to penalize the unnecessary explanations for non-toxic speech. It is worth noting that our evaluation algorithm is different from Sridhar and Yang (2022  ###reference_b46###  ###reference_b46###) which only evaluates the quality of the generation within implicit toxic cases.\nTo gain deeper insights into the generation performance and challenges that our ToXCL faces compared to the competing baseline, we conduct human evaluations using a randomly selected set of 150 samples that require explanations from each examined benchmark. Specifically, we collect the generated explanations from both the ToXCL and Flan-T5 models in two different settings, G2 and G3. To ensure high-quality evaluations, five native English speakers are hired to rate the generated explanations on a 1-3 scale (with 3 being the highest) based on three criteria: (i) Correctness, evaluating the accuracy of the explanation in correctly explaining the meaning of toxic speech; (ii) Fluency, assessing the fluency and coherence of the generated explanation in terms of language use; and (iii) Toxicity, gauging the level of harmfulness and judgmental tone exhibited in the generated explanation. The annotator agreement is measured using Krippendorff\u2019s alpha Krippendorff (2011  ###reference_b30###  ###reference_b30###), which provides a measure of inter-annotator reliability."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "ToXCL Performance",
            "text": "Our experimental results in Table 3  ###reference_###  ###reference_### reveal three main observations. Firstly, ToXCL outperforms all baselines on both benchmarks, demonstrating the effectiveness of our encoder-decoder model in addressing both implicit toxic detection and explanation tasks simultaneously without conflicts. Secondly, our model surpasses detection models in group G1, indicating the strong capability of our encoder in detecting implicit toxic speech. It is worth noting that despite having fewer parameters than the RoBERTa-Base model (124M), our encoder (Flan-T5\u2019s) classifier (109M) achieves better performance while maintaining computational efficiency. Lastly, our model significantly outperforms its backbone, Flan-T5, highlighting the effectiveness of our architectural designs in jointly training the tasks in an end-to-end manner for implicit toxic speech detection and explanation problems.\nOur human evaluation results in Table 4  ###reference_###  ###reference_### indicate that ToXCL outperforms its backbone model, Flan-T5, from groups G2 and G3, in terms of both explanation accuracy and textual fluency. This improved performance is also reflected in the detection task, resulting in more reliable explanations with fewer harmful outputs compared to the baselines. Our human annotators exhibit strong agreement with Krippendorff\u2019s alpha scores consistently measuring at least 0.78 among the three scores. It is worth noting that the average toxicity score of around 2 for both our ToXCL and baseline models aligns with expectations, given that the training datasets contain offensive words in the ground truth explanations (see Tab. 5  ###reference_###  ###reference_###)."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "When adding the this module (i.e., + TG in Table 3  ###reference_###), we observe that all baselines considered in our experiments achieve significant improvements compared to training them without the generated target groups. In addition, our Teacher Classifier also outperforms its variant trained on solely the input posts. These improvements emphasize the crucial role of the target group(s) in enhancing their ability to detect and explain implicit toxic speech. These results also consolidate our motivation for using generated target groups in guiding the models for solving the two tasks.\nBy comparing the performance of Flan-T5 + CLH + TG before and after using teacher forcing (+ TF in Table 3  ###reference_###), we observe that incorporating this technique improves the performance of the encoder of ToXCL. This improvement demonstrates that providing additional guidance to the encoder of ToXCL results in more accurate predictions and achieves a performance close to that of the Teacher Classifier.\nFinally, the impact of integrating the Conditional Decoding Constraint designed to avoid necessary explanation, is evaluated in Table 3  ###reference_###. Compared with Flan-T5 + CLH + TG + TF, our ToXCL is improved significantly on the toxic explanation generation task. This confirms the effectiveness of Conditional Decoding Constraint in helping the outputs of our model to synchronize implicit toxic speech detection labels and toxic explanations.\nOur model, along with the baselines, struggles with detecting implicit toxic speech that contains abbreviated or coded tokens, such as \"#noamnesty\" in case (5). This error has also been observed and discussed by previous work ElSherief et al. (2021  ###reference_b17###).\nOur model may face challenges in accurately detecting toxic sentences that contain indirect words. For example, case (6) involves the phrase \"bruh i love white people too how do i join\", which uses indirect words such as \"bruh\" and \"love\" to express irony. The speaker sarcastically expresses a desire to join a racial group while implying that joining such a group is based on a belief in the superiority of white people or that minorities are lesser races.\nIn all cases except cases (5), (6), and (10), our ToXCL accurately identifies implicit toxic speech but generates linguistically different explanations from the ground truth(s). However, these generated explanations convey the same semantic meaning as the ground truth, indicating the model\u2019s ability to comprehend correctly the implicit meanings. This, along with discussions by Huang et al. (2022  ###reference_b24###), demonstrates that instances of implicit toxic speech can have multiple correct explanations, highlighting the limitations of commonly-used n-gram evaluation metrics like BLEU-4 Papineni et al. (2002  ###reference_b39###) and ROUGE-L Lin (2004  ###reference_b32###) scores."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Case Study: Effectiveness of ToXCL",
            "text": "We compare the performance of ToXCL with the two best-performing baselines in group G3, namely GPT-2 on IHC and Flan-T5 on SBIC (Sec. 4  ###reference_###). We present the cases discussed in Table 5  ###reference_###. In cases (1), (3), (4), (7), (9), and (10) we observe that both GPT-2 and Flan-T5 fail to capture the intended meaning of the input posts, resulting in wrong detection or harmful explanations. In contrast, our ToXCL effectively captures the meaning of the posts, leading to accurate explanations that align closely with the ground truth ones. Notably, when comparing to the ground truth explanation, the explanation generated by ToXCL exhibits a more polite attitude (cases 2, 4, 8, 9) or even more accurate (case 3)."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Ablation Studies",
            "text": "When adding the this module (i.e., + TG in Table 3  ###reference_###  ###reference_###), we observe that all baselines considered in our experiments achieve significant improvements compared to training them without the generated target groups. In addition, our Teacher Classifier also outperforms its variant trained on solely the input posts. These improvements emphasize the crucial role of the target group(s) in enhancing their ability to detect and explain implicit toxic speech. These results also consolidate our motivation for using generated target groups in guiding the models for solving the two tasks.\nBy comparing the performance of Flan-T5 + CLH + TG before and after using teacher forcing (+ TF in Table 3  ###reference_###  ###reference_###), we observe that incorporating this technique improves the performance of the encoder of ToXCL. This improvement demonstrates that providing additional guidance to the encoder of ToXCL results in more accurate predictions and achieves a performance close to that of the Teacher Classifier.\nFinally, the impact of integrating the Conditional Decoding Constraint designed to avoid necessary explanation, is evaluated in Table 3  ###reference_###  ###reference_###. Compared with Flan-T5 + CLH + TG + TF, our ToXCL is improved significantly on the toxic explanation generation task. This confirms the effectiveness of Conditional Decoding Constraint in helping the outputs of our model to synchronize implicit toxic speech detection labels and toxic explanations."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Error Analysis",
            "text": "We present additional qualitative samples from both benchmarks in Table 5  ###reference_### to highlight key challenges faced by existing models in detecting implicit toxic speech and generating explanations. While our model performs well overall, there are still areas for improvement, as discussed below.\nOur model, along with the baselines, struggles with detecting implicit toxic speech that contains abbreviated or coded tokens, such as \"#noamnesty\" in case (5). This error has also been observed and discussed by previous work ElSherief et al. (2021  ###reference_b17###  ###reference_b17###).\nOur model may face challenges in accurately detecting toxic sentences that contain indirect words. For example, case (6) involves the phrase \"bruh i love white people too how do i join\", which uses indirect words such as \"bruh\" and \"love\" to express irony. The speaker sarcastically expresses a desire to join a racial group while implying that joining such a group is based on a belief in the superiority of white people or that minorities are lesser races.\nIn all cases except cases (5), (6), and (10), our ToXCL accurately identifies implicit toxic speech but generates linguistically different explanations from the ground truth(s). However, these generated explanations convey the same semantic meaning as the ground truth, indicating the model\u2019s ability to comprehend correctly the implicit meanings. This, along with discussions by Huang et al. (2022  ###reference_b24###  ###reference_b24###), demonstrates that instances of implicit toxic speech can have multiple correct explanations, highlighting the limitations of commonly-used n-gram evaluation metrics like BLEU-4 Papineni et al. (2002  ###reference_b39###  ###reference_b39###) and ROUGE-L Lin (2004  ###reference_b32###  ###reference_b32###) scores."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We present ToXCL, a unified framework for implicit toxic speech detection and explanation (Sec. 3.2  ###reference_###). It consists of three components: a Target Group Generator, an Encoder-Decoder model, and a Teacher Classifier. Our findings show that the Target Group Generator effectively identifies target groups, improving both accuracy and F1 scores for detecting implicit toxic speech. The novel encoder-decoder architecture successfully performs both tasks of detection and explanation without harming each other. The integration of the Teacher Classifier and the Conditional Decoding Constraint further enhances the performance of ToXCL, achieving state-of-the-art results in the task of toxic speech detection and explanation on two widely-used benchmarks. In the future, we will focus on addressing several limitations faced by our framework and baselines as specified in Section 5.3  ###reference_### to further enhance the performance of our model."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A Data Pre-process",
            "text": "To facilitate our problem, we exclude instances of implicit toxic speech that lack an explanation, resulting in the removal of 844 samples from IHC and 8,220 samples from SBIC. Since the original IHC dataset does not include a designated test set, we create our own by randomly selecting 20% of the implicit toxic speech and non-toxic speech instances. The final statistics of both datasets are shown in Table 6  ###reference_###."
        },
        {
            "section_id": "Appendix 2",
            "parent_section_id": null,
            "section_name": "Appendix B Evaluation Algorithm",
            "text": "We present our evaluation metrics to evaluate the explanation generation capability of the models. To penalize unnecessary explanations for non-toxic speech, we add 100 to every score when the label is \"None\" and the model generation output is \"None\"."
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T1\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T1.1\" style=\"width:303.5pt;height:192pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(80.6pt,-51.0pt) scale(2.1337948043375,2.1337948043375) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T1.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" id=\"S4.T1.1.1.1.1.1\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T1.1.1.1.1.2\">F1</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T1.1.1.1.1.3\">ROUGE-L</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.2.1.1\">BERT</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.1.2.1.2\">68.35</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.1.2.1.3\">70.44</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T1.1.1.3.2.1\">GPT-2</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.1.3.2.2\">53.98</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.1.3.2.3\">56.86</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T1.1.1.4.3.1\">BART</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.1.4.3.2\">63.41</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.1.4.3.3\">70.48</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.5.4.1\">T5</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T1.1.1.5.4.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.5.4.2.1\">69.79</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T1.1.1.5.4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.5.4.3.1\">70.95</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Target Group Generator experiments.</figcaption>\n</figure>",
            "capture": "Table 1: Target Group Generator experiments."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T2\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T2.1\" style=\"width:429.3pt;height:96.9pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(55.1pt,-12.4pt) scale(1.34525426745304,1.34525426745304) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T2.1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.1.1\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt\" id=\"S4.T2.1.1.1.1.1\"></th>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt\" id=\"S4.T2.1.1.1.1.2\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\" id=\"S4.T2.1.1.1.1.3\">IHC</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S4.T2.1.1.1.1.4\">SBIC</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.2.2.1\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.2.2.2\">Size</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.2.2.3\">Acc. (%)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.2.2.4\">F1 (%)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.2.2.5\">Acc. (%)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.2.2.6\">F1 (%)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.3.3.1\">RoBERTa-Large</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.3.3.2\">354M</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.3.3.3\">80.68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.3.3.4\">77.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.3.3.5\">90.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.3.3.6\">90.11</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.4.4.1\">Teacher Classifier</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.4.4.2\">354M</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T2.1.1.4.4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.4.4.3.1\">82.52</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.4.4.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.4.4.4.1\">79.49</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T2.1.1.4.4.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.4.4.5.1\">91.19</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T2.1.1.4.4.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.4.4.6.1\">91.19</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Performance of Teacher Classifier, which is the RoBERTa-Large + <span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.3.1\">TG</span>.</figcaption>\n</figure>",
            "capture": "Table 2: Performance of Teacher Classifier, which is the RoBERTa-Large + TG."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T3\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T3.16\" style=\"width:429.3pt;height:225.5pt;vertical-align:-0.5pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-197.6pt,103.5pt) scale(0.520731833202106,0.520731833202106) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T3.16.16\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T3.16.16.17.1\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt\" id=\"S4.T3.16.16.17.1.1\"></th>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt\" id=\"S4.T3.16.16.17.1.2\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\" id=\"S4.T3.16.16.17.1.3\">IHC Detection</td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_tt\" colspan=\"4\" id=\"S4.T3.16.16.17.1.4\">IHC Explanation</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\" id=\"S4.T3.16.16.17.1.5\">SBIC Detection</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\" id=\"S4.T3.16.16.17.1.6\">SBIC Explanation</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.16.16.18.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T3.16.16.18.2.1\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T3.16.16.18.2.2\">Group</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.16.16.18.2.3\">Acc.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.16.16.18.2.4\">Macro F1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.16.16.18.2.5\">BLEU-4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.16.16.18.2.6\">ROUGE-L</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.16.16.18.2.7\">METEOR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\" id=\"S4.T3.16.16.18.2.8\">BERTScore</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.16.16.18.2.9\">Acc.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.16.16.18.2.10\">Macro F1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.16.16.18.2.11\">BLEU-4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.16.16.18.2.12\">ROUGE-L</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.16.16.18.2.13\">METEOR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.16.16.18.2.14\">BERTScore</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.16.16.19.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T3.16.16.19.3.1\">HateBERT</th>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T3.16.16.19.3.2\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.16.16.19.3.3\">78.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.16.16.19.3.4\">75.93</td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T3.16.16.19.3.5\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T3.16.16.19.3.6\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T3.16.16.19.3.7\"></td>\n<td class=\"ltx_td ltx_border_rr ltx_border_t\" id=\"S4.T3.16.16.19.3.8\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.16.16.19.3.9\">89.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.16.16.19.3.10\">89.31</td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T3.16.16.19.3.11\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T3.16.16.19.3.12\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T3.16.16.19.3.13\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T3.16.16.19.3.14\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.16.16.20.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.16.16.20.4.1\">BERT</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.16.16.20.4.2\">G1</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.20.4.3\">78.98</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.16.16.20.4.4\">76.16</td>\n<td class=\"ltx_td\" id=\"S4.T3.16.16.20.4.5\"></td>\n<td class=\"ltx_td\" id=\"S4.T3.16.16.20.4.6\"></td>\n<td class=\"ltx_td\" id=\"S4.T3.16.16.20.4.7\"></td>\n<td class=\"ltx_td ltx_border_rr\" id=\"S4.T3.16.16.20.4.8\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.20.4.9\">89.83</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.16.16.20.4.10\">89.83</td>\n<td class=\"ltx_td\" id=\"S4.T3.16.16.20.4.11\"></td>\n<td class=\"ltx_td\" id=\"S4.T3.16.16.20.4.12\"></td>\n<td class=\"ltx_td\" id=\"S4.T3.16.16.20.4.13\"></td>\n<td class=\"ltx_td\" id=\"S4.T3.16.16.20.4.14\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.16.16.21.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.16.16.21.5.1\">ELECTRA</th>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.16.16.21.5.2\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.21.5.3\">79.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.16.16.21.5.4\">76.87</td>\n<td class=\"ltx_td\" id=\"S4.T3.16.16.21.5.5\"></td>\n<td class=\"ltx_td\" id=\"S4.T3.16.16.21.5.6\"></td>\n<td class=\"ltx_td\" id=\"S4.T3.16.16.21.5.7\"></td>\n<td class=\"ltx_td ltx_border_rr\" id=\"S4.T3.16.16.21.5.8\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.21.5.9\">89.06</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.16.16.21.5.10\">89.04</td>\n<td class=\"ltx_td\" id=\"S4.T3.16.16.21.5.11\"></td>\n<td class=\"ltx_td\" id=\"S4.T3.16.16.21.5.12\"></td>\n<td class=\"ltx_td\" id=\"S4.T3.16.16.21.5.13\"></td>\n<td class=\"ltx_td\" id=\"S4.T3.16.16.21.5.14\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.16.16.22.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.16.16.22.6.1\">RoBERTa</th>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.16.16.22.6.2\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.22.6.3\">80.06</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.16.16.22.6.4\">77.23</td>\n<td class=\"ltx_td\" id=\"S4.T3.16.16.22.6.5\"></td>\n<td class=\"ltx_td\" id=\"S4.T3.16.16.22.6.6\"></td>\n<td class=\"ltx_td\" id=\"S4.T3.16.16.22.6.7\"></td>\n<td class=\"ltx_td ltx_border_rr\" id=\"S4.T3.16.16.22.6.8\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.22.6.9\">89.98</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.16.16.22.6.10\">89.97</td>\n<td class=\"ltx_td\" id=\"S4.T3.16.16.22.6.11\"></td>\n<td class=\"ltx_td\" id=\"S4.T3.16.16.22.6.12\"></td>\n<td class=\"ltx_td\" id=\"S4.T3.16.16.22.6.13\"></td>\n<td class=\"ltx_td\" id=\"S4.T3.16.16.22.6.14\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.16.16.23.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T3.16.16.23.7.1\">GPT-2</th>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T3.16.16.23.7.2\"></th>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T3.16.16.23.7.3\"></td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" id=\"S4.T3.16.16.23.7.4\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.16.16.23.7.5\">67.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.16.16.23.7.6\">70.19</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.16.16.23.7.7\">69.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\" id=\"S4.T3.16.16.23.7.8\">73.15</td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T3.16.16.23.7.9\"></td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" id=\"S4.T3.16.16.23.7.10\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.16.16.23.7.11\">47.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.16.16.23.7.12\">65.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.16.16.23.7.13\">63.74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.16.16.23.7.14\">74.74</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.16.16.24.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.16.16.24.8.1\">BART</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.16.16.24.8.2\">G2</th>\n<td class=\"ltx_td\" id=\"S4.T3.16.16.24.8.3\"></td>\n<td class=\"ltx_td ltx_border_r\" id=\"S4.T3.16.16.24.8.4\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.24.8.5\">53.67</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.24.8.6\">59.18</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.24.8.7\">57.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr\" id=\"S4.T3.16.16.24.8.8\">68.14</td>\n<td class=\"ltx_td\" id=\"S4.T3.16.16.24.8.9\"></td>\n<td class=\"ltx_td ltx_border_r\" id=\"S4.T3.16.16.24.8.10\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.24.8.11\">45.22</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.24.8.12\">67.73</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.24.8.13\">67.25</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.24.8.14\">83.98</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.16.16.25.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.16.16.25.9.1\">T5</th>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.16.16.25.9.2\"></th>\n<td class=\"ltx_td\" id=\"S4.T3.16.16.25.9.3\"></td>\n<td class=\"ltx_td ltx_border_r\" id=\"S4.T3.16.16.25.9.4\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.25.9.5\">50.19</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.25.9.6\">56.01</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.25.9.7\">54.36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr\" id=\"S4.T3.16.16.25.9.8\">66.60</td>\n<td class=\"ltx_td\" id=\"S4.T3.16.16.25.9.9\"></td>\n<td class=\"ltx_td ltx_border_r\" id=\"S4.T3.16.16.25.9.10\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.25.9.11\">45.37</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.25.9.12\">68.03</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.25.9.13\">67.59</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.25.9.14\">84.68</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.16.16.26.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.16.16.26.10.1\">Flan-T5</th>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.16.16.26.10.2\"></th>\n<td class=\"ltx_td\" id=\"S4.T3.16.16.26.10.3\"></td>\n<td class=\"ltx_td ltx_border_r\" id=\"S4.T3.16.16.26.10.4\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.26.10.5\">47.33</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.26.10.6\">53.78</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.26.10.7\">51.95</td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr\" id=\"S4.T3.16.16.26.10.8\">65.14</td>\n<td class=\"ltx_td\" id=\"S4.T3.16.16.26.10.9\"></td>\n<td class=\"ltx_td ltx_border_r\" id=\"S4.T3.16.16.26.10.10\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.26.10.11\">45.83</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.26.10.12\">68.37</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.26.10.13\">67.98</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.26.10.14\">85.04</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.16.16.27.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T3.16.16.27.11.1\">GPT-2</th>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T3.16.16.27.11.2\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.16.16.27.11.3\">77.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.16.16.27.11.4\">76.36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.16.16.27.11.5\">67.81</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.16.16.27.11.6\">70.19</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.16.16.27.11.7\">69.88</td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\" id=\"S4.T3.16.16.27.11.8\">73.47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.16.16.27.11.9\">73.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.16.16.27.11.10\">57.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.16.16.27.11.11\">48.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.16.16.27.11.12\">65.24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.16.16.27.11.13\">63.53</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.16.16.27.11.14\">74.61</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.16.16.28.12\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.16.16.28.12.1\">BART</th>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.16.16.28.12.2\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.28.12.3\">70.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.16.16.28.12.4\">64.11</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.28.12.5\">55.92</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.28.12.6\">60.48</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.28.12.7\">58.87</td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr\" id=\"S4.T3.16.16.28.12.8\">68.74</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.28.12.9\">88.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.16.16.28.12.10\">88.20</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.28.12.11\">45.65</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.28.12.12\">68.08</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.28.12.13\">67.81</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.28.12.14\">84.35</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.16.16.29.13\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.16.16.29.13.1\">T5</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.16.16.29.13.2\">G3</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.29.13.3\">70.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.16.16.29.13.4\">62.95</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.29.13.5\">58.24</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.29.13.6\">62.42</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.29.13.7\">61.18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr\" id=\"S4.T3.16.16.29.13.8\">69.30</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.29.13.9\">87.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.16.16.29.13.10\">87.48</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.29.13.11\">45.92</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.29.13.12\">68.37</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.29.13.13\">67.82</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.29.13.14\">84.78</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.16.16.30.14\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.16.16.30.14.1\">Flan-T5</th>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.16.16.30.14.2\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.30.14.3\">71.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.16.16.30.14.4\">65.56</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.30.14.5\">56.58</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.30.14.6\">61.75</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.30.14.7\">60.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr\" id=\"S4.T3.16.16.30.14.8\">69.84</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.30.14.9\">88.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.16.16.30.14.10\">88.37</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.30.14.11\">45.98</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.30.14.12\">68.81</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.30.14.13\">67.99</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.30.14.14\">85.07</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.16.16.31.15\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.16.16.31.15.1\">ChatGPT</th>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.16.16.31.15.2\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.31.15.3\">55.83</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.16.16.31.15.4\">18.62</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.31.15.5\">27.30</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.31.15.6\">29.15</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.31.15.7\">31.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr\" id=\"S4.T3.16.16.31.15.8\">51.40</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.31.15.9\">78.59</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.16.16.31.15.10\">31.38</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.31.15.11\">0.36</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.31.15.12\">4.32</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.31.15.13\">3.36</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.31.15.14\">48.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.16.16.32.16\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.16.16.32.16.1\">Mistral-7B</th>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.16.16.32.16.2\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.32.16.3\">67.36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.16.16.32.16.4\">26.31</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.32.16.5\">0.29</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.32.16.6\">2.31</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.32.16.7\">3.39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr\" id=\"S4.T3.16.16.32.16.8\">27.11</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.32.16.9\">78.36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.16.16.32.16.10\">52.03</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.32.16.11\">0.58</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.32.16.12\">8.21</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.32.16.13\">2.19</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.32.16.14\">79.88</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.16.16.33.17\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T3.16.16.33.17.1\">GPT-2 + <span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.16.16.33.17.1.1\">TG</span>\n</th>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T3.16.16.33.17.2\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.16.16.33.17.3\">74.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.16.16.33.17.4\">58.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.16.16.33.17.5\">67.86</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.16.16.33.17.6\">70.38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.16.16.33.17.7\">69.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\" id=\"S4.T3.16.16.33.17.8\">73.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.16.16.33.17.9\">78.87</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.16.16.33.17.10\">77.93</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.16.16.33.17.11\">48.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.16.16.33.17.12\">65.48</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.16.16.33.17.13\">63.65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.16.16.33.17.14\">75.80</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.16.16.34.18\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.16.16.34.18.1\">BART + <span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.16.16.34.18.1.1\">TG</span>\n</th>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.16.16.34.18.2\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.34.18.3\">74.28</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.16.16.34.18.4\">58.88</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.34.18.5\">67.75</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.34.18.6\">68.17</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.34.18.7\">69.68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr\" id=\"S4.T3.16.16.34.18.8\">73.67</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.34.18.9\">88.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.16.16.34.18.10\">88.52</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.34.18.11\">45.70</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.34.18.12\">68.41</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.34.18.13\">67.81</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.34.18.14\">85.03</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.16.16.35.19\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.16.16.35.19.1\">T5 + <span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.16.16.35.19.1.1\">TG</span>\n</th>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.16.16.35.19.2\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.35.19.3\">76.48</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.16.16.35.19.4\">64.86</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.35.19.5\">67.07</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.35.19.6\">70.98</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.35.19.7\">70.24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr\" id=\"S4.T3.16.16.35.19.8\">73.68</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.35.19.9\">87.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.16.16.35.19.10\">87.71</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.35.19.11\">46.52</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.35.19.12\">68.67</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.35.19.13\">67.98</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.35.19.14\">85.08</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.16.16.36.20\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.16.16.36.20.1\">Flan-T5 + <span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.16.16.36.20.1.1\">TG</span>\n</th>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.16.16.36.20.2\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.36.20.3\">78.47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.16.16.36.20.4\">70.13</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.36.20.5\">65.77</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.36.20.6\">70.03</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.36.20.7\">69.98</td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr\" id=\"S4.T3.16.16.36.20.8\">75.18</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.36.20.9\">88.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.16.16.36.20.10\">88.72</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.36.20.11\">47.05</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.36.20.12\">68.75</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.36.20.13\">68.05</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.36.20.14\">85.25</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.16.16.37.21\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\" id=\"S4.T3.16.16.37.21.1\">Flan-T5 + <span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.16.16.37.21.1.1\">CLH</span>\n</th>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt\" id=\"S4.T3.16.16.37.21.2\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.16.16.37.21.3\">77.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S4.T3.16.16.37.21.4\">73.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.16.16.37.21.5\">62.15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.16.16.37.21.6\">64.36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.16.16.37.21.7\">62.21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_tt\" id=\"S4.T3.16.16.37.21.8\">63.94</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.16.16.37.21.9\">89.19</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S4.T3.16.16.37.21.10\">89.19</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.16.16.37.21.11\">45.88</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.16.16.37.21.12\">67.24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.16.16.37.21.13\">67.63</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.16.16.37.21.14\">84.96</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.16.16.38.22\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.16.16.38.22.1\">Flan-T5 + <span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.16.16.38.22.1.1\">CLH</span> + <span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.16.16.38.22.1.2\">TG</span>\n</th>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.16.16.38.22.2\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.38.22.3\">78.68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.16.16.38.22.4\">75.77</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.38.22.5\">64.99</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.38.22.6\">67.87</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.38.22.7\">66.24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr\" id=\"S4.T3.16.16.38.22.8\">72.11</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.38.22.9\">89.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.16.16.38.22.10\">89.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.38.22.11\">47.24</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.38.22.12\">67.94</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.38.22.13\">67.80</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.38.22.14\">85.34</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.4.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.4.4.4.5\">Flan-T5 + <span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.4.4.4.5.1\">CLH</span> + <span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.4.4.4.5.2\">TG</span> + <span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.4.4.4.5.3\">TF</span>\n</th>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.4.4.4.6\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1\">81.53</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.2.2.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.2.2.2.2.1\">78.19</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.4.4.4.7\">66.49</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.4.4.4.8\">69.11</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.4.4.4.9\">67.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr\" id=\"S4.T3.4.4.4.10\">72.14</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.3.3.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.3.3.3.3.1\">90.09</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.4.4.4.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.4.4.4.4.1\">90.08</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.4.4.4.11\">47.85</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.4.4.4.12\">68.93</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.4.4.4.13\">68.16</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.4.4.4.14\">85.58</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.16.16.16\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S4.T3.16.16.16.13\"><em class=\"ltx_emph ltx_font_bold ltx_font_smallcaps\" id=\"S4.T3.16.16.16.13.1\">ToXCL</em></th>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S4.T3.16.16.16.14\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.5.5.5.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.5.5.5.1.1\">81.53</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T3.6.6.6.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.6.6.6.2.1\">78.19</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.7.7.7.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.7.7.7.3.1\">68.11</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.8.8.8.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.8.8.8.4.1\">71.21</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.9.9.9.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.9.9.9.5.1\">70.27</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_rr\" id=\"S4.T3.10.10.10.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.10.10.10.6.1\">77.38</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.11.11.11.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.11.11.11.7.1\">90.09</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T3.12.12.12.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.12.12.12.8.1\">90.08</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.13.13.13.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.13.13.13.9.1\">49.03</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.14.14.14.10\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.14.14.14.10.1\">69.93</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.15.15.15.11\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.15.15.15.11.1\">68.85</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.16.16.16.12\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.16.16.16.12.1\">86.09</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Main experimental results. <span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.26.1\">CLH</span> stands for joint training with a classification head on top of the Flan-T5 Encoder. Our model, <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\" id=\"S4.T3.27.2\">ToXCL</span> is equivalent to Flan-T5 + <span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.28.3\">CL Head</span> + <span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.29.4\">TG</span> + <span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.30.5\">TF</span> + <span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.31.6\">CD</span> (all are described in Section\u00a0<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16685v2#S3.SS2\" title=\"3.2 ToXCL Framework \u2023 3 Methodology \u2023 ToXCL: A Unified Framework for Toxic Speech Detection and Explanation\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>).  denotes our model significantly outperforms <em class=\"ltx_emph ltx_font_italic\" id=\"S4.T3.32.7\">implicit toxic speech detection &amp; explanation</em> baselines with p-value &lt; 0.05 under t-test.</figcaption>\n</figure>",
            "capture": "Table 3: Main experimental results. CLH stands for joint training with a classification head on top of the Flan-T5 Encoder. Our model, ToXCL is equivalent to Flan-T5 + CL Head + TG + TF + CD (all are described in Section\u00a03.2).  denotes our model significantly outperforms implicit toxic speech detection & explanation baselines with p-value < 0.05 under t-test."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T4\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T4.3\" style=\"width:368.6pt;height:176.2pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(90.2pt,-43.1pt) scale(1.95792285980566,1.95792285980566) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T4.3.3\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T4.3.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\" id=\"S4.T4.3.3.3.4\">Model</th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T4.1.1.1.1\">Cor.\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T4.2.2.2.2\">Flu.\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T4.3.3.3.3\">Tox.\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.3.3.4.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T4.3.3.4.1.1\">Flan-T5 (G2)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.3.3.4.1.2\">2.21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.3.3.4.1.3\">2.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.3.3.4.1.4\">2.03</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.3.3.5.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T4.3.3.5.2.1\">Flan-T5 (G3)</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.3.3.5.2.2\">2.35</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.3.3.5.2.3\">2.46</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.3.3.5.2.4\">2.07</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.3.3.6.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T4.3.3.6.3.1\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\" id=\"S4.T4.3.3.6.3.1.1\">ToXCL</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.3.3.6.3.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.3.3.6.3.2.1\">2.56</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.3.3.6.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.3.3.6.3.3.1\">2.63</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.3.3.6.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.3.3.6.3.4.1\">1.97</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.3.3.7.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_tt\" id=\"S4.T4.3.3.7.4.1\">Kripp.\u2019s alpha</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\" id=\"S4.T4.3.3.7.4.2\">0.81</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\" id=\"S4.T4.3.3.7.4.3\">0.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\" id=\"S4.T4.3.3.7.4.4\">0.78</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>Human evaluation results.</figcaption>\n</figure>",
            "capture": "Table 4: Human evaluation results."
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T5\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S5.T5.1\" style=\"width:429.3pt;height:253.7pt;vertical-align:-0.7pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-106.8pt,63.0pt) scale(0.66766576332027,0.66766576332027) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T5.1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.1.1\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S5.T5.1.1.1.1.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.1.1.1.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.1.1.1.1.1\" style=\"width:19.9pt;\">Index</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" id=\"S5.T5.1.1.1.1.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.1.1.2.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.1.1.2.1.1\" style=\"width:204.9pt;\">Input Post</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S5.T5.1.1.1.1.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.1.1.3.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.1.1.3.1.1\" style=\"width:135.2pt;\">Ground Truth Explanation(s)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" id=\"S5.T5.1.1.1.1.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.1.1.4.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.1.1.4.1.1\" style=\"width:66.9pt;\">GPT-2 (IHC)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S5.T5.1.1.1.1.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.1.1.5.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.1.1.5.1.1\" style=\"width:69.7pt;\">Flan-T5 (SBIC)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" id=\"S5.T5.1.1.1.1.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.1.1.6.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.1.1.6.1.1\" style=\"width:71.1pt;\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\" id=\"S5.T5.1.1.1.1.6.1.1.1\">ToXCL</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.2.2\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T5.1.1.2.2.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.2.2.1.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.2.2.1.1.1\" style=\"width:19.9pt;\">(1)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T5.1.1.2.2.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.2.2.2.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.2.2.2.1.1\" style=\"width:204.9pt;\">approx half muslms today result of inbreeding, perhaps this why they are \"\"hell bent\"\" on destroying christian based west? inbreed = low iq.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T5.1.1.2.2.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.2.2.3.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.2.2.3.1.1\" style=\"width:135.2pt;\">Muslims are result of inbreeding</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T5.1.1.2.2.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.2.2.4.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.2.2.4.1.1\" style=\"width:66.9pt;\">Muslims are destructive</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T5.1.1.2.2.5\"></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T5.1.1.2.2.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.2.2.6.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.2.2.6.1.1\" style=\"width:71.1pt;\">Muslims are inbred.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.3.3\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T5.1.1.3.3.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.3.3.1.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.3.3.1.1.1\" style=\"width:19.9pt;\">(2)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T5.1.1.3.3.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.3.3.2.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.3.3.2.1.1\" style=\"width:204.9pt;\">you hate like a jew. you are fixatd on hating white christians like a jew.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T5.1.1.3.3.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.3.3.3.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.3.3.3.1.1\" style=\"width:135.2pt;\">Jewish people have hate toward other groups</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T5.1.1.3.3.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.3.3.4.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.3.3.4.1.1\" style=\"width:66.9pt;\">Jews hate white people</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T5.1.1.3.3.5\"></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T5.1.1.3.3.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.3.3.6.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.3.3.6.1.1\" style=\"width:71.1pt;\">Jews don\u2019t like white poeple</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.4.4\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T5.1.1.4.4.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.4.4.1.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.4.4.1.1.1\" style=\"width:19.9pt;\">(3)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T5.1.1.4.4.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.4.4.2.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.4.4.2.1.1\" style=\"width:204.9pt;\">funny how you never hear about white men raping women in black\" neighborhoods. i know 2 white women who \u2019</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T5.1.1.4.4.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.4.4.3.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.4.4.3.1.1\" style=\"width:135.2pt;\">Blacks rape whites.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T5.1.1.4.4.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.4.4.4.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.4.4.4.1.1\" style=\"width:66.9pt;\">Blacks are inferior</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T5.1.1.4.4.5\"></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T5.1.1.4.4.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.4.4.6.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.4.4.6.1.1\" style=\"width:71.1pt;\">White men are rapists.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.5.5\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T5.1.1.5.5.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.5.5.1.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.5.5.1.1.1\" style=\"width:19.9pt;\">(4)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T5.1.1.5.5.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.5.5.2.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.5.5.2.1.1\" style=\"width:204.9pt;\">she\u2019s another low iq hollywood liberal . islam is submission and btw if they ran america</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T5.1.1.5.5.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.5.5.3.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.5.5.3.1.1\" style=\"width:135.2pt;\">Liberals are stupid.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T5.1.1.5.5.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.5.5.4.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.5.5.4.1.1\" style=\"width:66.9pt;\">None</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T5.1.1.5.5.5\"></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T5.1.1.5.5.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.5.5.6.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.5.5.6.1.1\" style=\"width:71.1pt;\">Liberals are low IQ people.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.6.6\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T5.1.1.6.6.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.6.6.1.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.6.6.1.1.1\" style=\"width:19.9pt;\">(5)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T5.1.1.6.6.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.6.6.2.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.6.6.2.1.1\" style=\"width:204.9pt;\">rt. : \" voters on obamas #immigration plans : drop dead \" \" #noamnesty \"</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T5.1.1.6.6.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.6.6.3.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.6.6.3.1.1\" style=\"width:135.2pt;\">Immigrants ruin society</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T5.1.1.6.6.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.6.6.4.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.6.6.4.1.1\" style=\"width:66.9pt;\">None</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T5.1.1.6.6.5\"></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T5.1.1.6.6.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.6.6.6.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.6.6.6.1.1\" style=\"width:71.1pt;\">None</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.7.7\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T5.1.1.7.7.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.7.7.1.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.7.7.1.1.1\" style=\"width:19.9pt;\">(6)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T5.1.1.7.7.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.7.7.2.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.7.7.2.1.1\" style=\"width:204.9pt;\">bruh i love white people too how do i join</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T5.1.1.7.7.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.7.7.3.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.7.7.3.1.1\" style=\"width:135.2pt;\">Minorities are lesser races.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T5.1.1.7.7.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.7.7.4.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.7.7.4.1.1\" style=\"width:66.9pt;\">None</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T5.1.1.7.7.5\"></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T5.1.1.7.7.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.7.7.6.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.7.7.6.1.1\" style=\"width:71.1pt;\">None</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.8.8\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S5.T5.1.1.8.8.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.8.8.1.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.8.8.1.1.1\" style=\"width:19.9pt;\">(7)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" id=\"S5.T5.1.1.8.8.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.8.8.2.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.8.8.2.1.1\" style=\"width:204.9pt;\">An Asian woman just gave me a handjob while fingering my ass Which isnt what i meant when i asked her to give me a \u2019Hand Roll and Pork Intestines. That\u2019s the problem with having the Chinese Massage place right next to the Chinese takeout place</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S5.T5.1.1.8.8.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.8.8.3.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.8.8.3.1.1\" style=\"width:135.2pt;\">[\u2019chinese women are sex workers.\u2019, \"chinese women don\u2019t understand english.\"]</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_top ltx_border_tt\" id=\"S5.T5.1.1.8.8.4\"></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S5.T5.1.1.8.8.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.8.8.5.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.8.8.5.1.1\" style=\"width:69.7pt;\">asians eat pork</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" id=\"S5.T5.1.1.8.8.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.8.8.6.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.8.8.6.1.1\" style=\"width:71.1pt;\">asian women only care about sex</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.9.9\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T5.1.1.9.9.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.9.9.1.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.9.9.1.1.1\" style=\"width:19.9pt;\">(8)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T5.1.1.9.9.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.9.9.2.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.9.9.2.1.1\" style=\"width:204.9pt;\">you see, i am a doctor. today i had to save a black guy\u2019s life i told his family: i did everything i could, but unfortunately he survived.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T5.1.1.9.9.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.9.9.3.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.9.9.3.1.1\" style=\"width:135.2pt;\">[\u2019black people are useless\u2019, \u2019black people need to be killed\u2019]</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_top ltx_border_t\" id=\"S5.T5.1.1.9.9.4\"></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T5.1.1.9.9.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.9.9.5.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.9.9.5.1.1\" style=\"width:69.7pt;\">black people are worthless</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T5.1.1.9.9.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.9.9.6.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.9.9.6.1.1\" style=\"width:71.1pt;\">black people aren\u2019t as good as whites</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.10.10\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T5.1.1.10.10.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.10.10.1.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.10.10.1.1.1\" style=\"width:19.9pt;\">(9)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T5.1.1.10.10.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.10.10.2.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.10.10.2.1.1\" style=\"width:204.9pt;\">@BlakkHoodie_562 yeah that niggah played good, lowkey i wanted brazil to win</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T5.1.1.10.10.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.10.10.3.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.10.10.3.1.1\" style=\"width:135.2pt;\">[\u2019black people are the n word\u2019]</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_top ltx_border_t\" id=\"S5.T5.1.1.10.10.4\"></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T5.1.1.10.10.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.10.10.5.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.10.10.5.1.1\" style=\"width:69.7pt;\">None</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T5.1.1.10.10.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.10.10.6.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.10.10.6.1.1\" style=\"width:71.1pt;\">niggah is a slang term</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.11.11\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_b ltx_border_r ltx_border_t\" id=\"S5.T5.1.1.11.11.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.11.11.1.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.11.11.1.1.1\" style=\"width:19.9pt;\">(10)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_b ltx_border_t\" id=\"S5.T5.1.1.11.11.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.11.11.2.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.11.11.2.1.1\" style=\"width:204.9pt;\">A bf and gf; are suppose to laugh; at these hoes; together;. A guy should never have these hoes; laughing; at his girl;</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_b ltx_border_r ltx_border_t\" id=\"S5.T5.1.1.11.11.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.11.11.3.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.11.11.3.1.1\" style=\"width:135.2pt;\">[\u2019degrades women by calling them names\u2019, \u2019women are called hoes\u2019, \u2019women are hoes\u2019]</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_top ltx_border_bb ltx_border_b ltx_border_t\" id=\"S5.T5.1.1.11.11.4\"></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_b ltx_border_r ltx_border_t\" id=\"S5.T5.1.1.11.11.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.11.11.5.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.11.11.5.1.1\" style=\"width:69.7pt;\">None</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_b ltx_border_t\" id=\"S5.T5.1.1.11.11.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T5.1.1.11.11.6.1\">\n<span class=\"ltx_p\" id=\"S5.T5.1.1.11.11.6.1.1\" style=\"width:71.1pt;\">women are hoes</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>The qualitative explanation performance of GPT-2, Flan-T5, and <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\" id=\"S5.T5.3.1\">ToXCL</span> was evaluated on samples from the valid set of IHC and SBIC datasets.</figcaption>\n</figure>",
            "capture": "Table 5: The qualitative explanation performance of GPT-2, Flan-T5, and ToXCL was evaluated on samples from the valid set of IHC and SBIC datasets."
        },
        "6": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T6\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"A1.T6.1\" style=\"width:346.9pt;height:158.9pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(55.6pt,-25.4pt) scale(1.47128709603413,1.47128709603413) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"A1.T6.1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A1.T6.1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\" id=\"A1.T6.1.1.1.1.1\">Split</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"A1.T6.1.1.1.1.2\"># toxic</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"A1.T6.1.1.1.1.3\"># non-toxic</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A1.T6.1.1.1.1.4\"># samples</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T6.1.1.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"A1.T6.1.1.2.2.1\">IHC Train</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A1.T6.1.1.2.2.2\">5,002</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A1.T6.1.1.2.2.3\">10,633</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T6.1.1.2.2.4\">15,635</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T6.1.1.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A1.T6.1.1.3.3.1\">IHC Valid</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.1.3.3.2\">1,254</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.1.3.3.3\">2,658</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T6.1.1.3.3.4\">3,912</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T6.1.1.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"A1.T6.1.1.4.4.1\">SBIC Train</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A1.T6.1.1.4.4.2\">12,098</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A1.T6.1.1.4.4.3\">16,698</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T6.1.1.4.4.4\">28,796</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T6.1.1.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A1.T6.1.1.5.5.1\">SBIC Dev</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.1.5.5.2\">1,806</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.1.5.5.3\">2,054</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T6.1.1.5.5.4\">3,860</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T6.1.1.6.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"A1.T6.1.1.6.6.1\">SBIC Test</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A1.T6.1.1.6.6.2\">1,924</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A1.T6.1.1.6.6.3\">1,981</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A1.T6.1.1.6.6.4\">3,905</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 6: </span>Statistics of <span class=\"ltx_text ltx_font_typewriter\" id=\"A1.T6.4.1\">Implicit Hate Corpus</span> and <span class=\"ltx_text ltx_font_typewriter\" id=\"A1.T6.5.2\">Social Bias Inference Corpus</span> after pre-processing.</figcaption>\n</figure>",
            "capture": "Table 6: Statistics of Implicit Hate Corpus and Social Bias Inference Corpus after pre-processing."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.16685v2_figure_1.png",
            "caption": "Figure 1: A sample input post and its ground truth explanation from the test set of Implicit Hate Corpus ElSherief et al. (2021). The input post is fed into both models. The baseline model, RoBERTa, fails to detect the implicit toxic speech while our proposed ToXCL model successfully detects it and generates a toxic explanation that closely matches the ground truth explanation."
        },
        "2": {
            "figure_path": "2403.16685v2_figure_2.png",
            "caption": "Figure 2: An overview of our proposed ToXCL. It consists of three modules: a (i) Target Group Generator generates the target group(s) of the input post; an (ii) Encoder-Decoder Model whose encoder focuses on implicit toxic speech detection whilst its decoder aims to generate necessary toxic explanation; a (iii) Teacher Classifier to distil the knowledge to the classifier encoder."
        }
    },
    "references": [
        {
            "1": {
                "title": "Token: Task decomposition and knowledge infusion for few-shot hate speech detection.",
                "author": "Badr AlKhamissi, Faisal Ladhak, Srini Iyer, Ves Stoyanov, Zornitsa Kozareva, Xian Li, Pascale Fung, Lambert Mathias, Asli Celikyilmaz, and Mona Diab. 2022.",
                "venue": null,
                "url": "https://doi.org/10.48550/ARXIV.2205.12495"
            }
        },
        {
            "2": {
                "title": "Training a helpful and harmless assistant with reinforcement learning from human feedback.",
                "author": "Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. 2022.",
                "venue": null,
                "url": "http://arxiv.org/abs/2204.05862"
            }
        },
        {
            "3": {
                "title": "Necessity and sufficiency for explaining text classifiers: A case study in hate speech detection.",
                "author": "Esma Balkir, Isar Nejadgholi, Kathleen C Fraser, and Svetlana Kiritchenko. 2022.",
                "venue": "arXiv preprint arXiv:2205.03302.",
                "url": null
            }
        },
        {
            "4": {
                "title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.",
                "author": "Satanjeev Banerjee and Alon Lavie. 2005.",
                "venue": "In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65\u201372, Ann Arbor, Michigan. Association for Computational Linguistics.",
                "url": "https://aclanthology.org/W05-0909"
            }
        },
        {
            "5": {
                "title": "SemEval-2019 task 5: Multilingual detection of hate speech against immigrants and women in Twitter.",
                "author": "Valerio Basile, Cristina Bosco, Elisabetta Fersini, Debora Nozza, Viviana Patti, Francisco Manuel Rangel Pardo, Paolo Rosso, and Manuela Sanguinetti. 2019.",
                "venue": "In Proceedings of the 13th International Workshop on Semantic Evaluation, pages 54\u201363, Minneapolis, Minnesota, USA. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/S19-2007"
            }
        },
        {
            "6": {
                "title": "Finding microaggressions in the wild: A case for locating elusive phenomena in social media posts.",
                "author": "Luke Breitfeller, Emily Ahn, David Jurgens, and Yulia Tsvetkov. 2019.",
                "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1664\u20131674, Hong Kong, China. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/D19-1176"
            }
        },
        {
            "7": {
                "title": "HateBERT: Retraining BERT for abusive language detection in English.",
                "author": "Tommaso Caselli, Valerio Basile, Jelena Mitrovi\u0107, and Michael Granitzer. 2021.",
                "venue": "In Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021), pages 17\u201325, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.woah-1.3"
            }
        },
        {
            "8": {
                "title": "I feel offended, don\u2019t be abusive! implicit/explicit messages in offensive and abusive language.",
                "author": "Tommaso Caselli, Valerio Basile, Jelena Mitrovi\u0107, Inga Kartoziya, and Michael Granitzer. 2020.",
                "venue": "In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 6193\u20136202, Marseille, France. European Language Resources Association.",
                "url": "https://aclanthology.org/2020.lrec-1.760"
            }
        },
        {
            "9": {
                "title": "Detecting offensive language in social media to protect adolescent online safety.",
                "author": "Ying Chen, Yilu Zhou, Sencun Zhu, and Heng Xu. 2012.",
                "venue": "In 2012 International Conference on Privacy, Security, Risk and Trust and 2012 International Confernece on Social Computing, pages 71\u201380.",
                "url": "https://doi.org/10.1109/SocialCom-PASSAT.2012.55"
            }
        },
        {
            "10": {
                "title": "Scaling instruction-finetuned language models.",
                "author": "Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022.",
                "venue": "arXiv preprint arXiv:2210.11416.",
                "url": null
            }
        },
        {
            "11": {
                "title": "BAM! born-again multi-task networks for natural language understanding.",
                "author": "Kevin Clark, Minh-Thang Luong, Urvashi Khandelwal, Christopher D. Manning, and Quoc V. Le. 2019.",
                "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5931\u20135937, Florence, Italy. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/P19-1595"
            }
        },
        {
            "12": {
                "title": "Electra: Pre-training text encoders as discriminators rather than generators.",
                "author": "Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2020.",
                "venue": "In International Conference on Learning Representations.",
                "url": "https://openreview.net/forum?id=r1xMH1BtvB"
            }
        },
        {
            "13": {
                "title": "I-divergence geometry of probability distributions and minimization problems.",
                "author": "Imre Csisz\u00e1r. 1975.",
                "venue": "The annals of probability, pages 146\u2013158.",
                "url": null
            }
        },
        {
            "14": {
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding.",
                "author": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.",
                "venue": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/N19-1423"
            }
        },
        {
            "15": {
                "title": "Explainable artificial intelligence: A survey.",
                "author": "Filip Karlo Dosilovic, Mario Brcic, and Nikica Hlupic. 2018.",
                "venue": "2018 41st International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO), pages 0210\u20130215.",
                "url": null
            }
        },
        {
            "16": {
                "title": "Hate lingo: A target-based linguistic analysis of hate speech in social media.",
                "author": "Mai ElSherief, Vivek Kulkarni, Dana Nguyen, William Yang Wang, and Elizabeth Belding. 2018.",
                "venue": null,
                "url": "http://arxiv.org/abs/1804.04257"
            }
        },
        {
            "17": {
                "title": "Latent hatred: A benchmark for understanding implicit hate speech.",
                "author": "Mai ElSherief, Caleb Ziems, David Muchlinski, Vaishnavi Anupindi, Jordyn Seybolt, Munmun De Choudhury, and Diyi Yang. 2021.",
                "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 345\u2013363, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.emnlp-main.29"
            }
        },
        {
            "18": {
                "title": "Exploring misogyny across the manosphere in reddit.",
                "author": "Tracie Farrell, Miriam Fernandez, Jakub Novotny, and Harith Alani. 2019.",
                "venue": "In Proceedings of the 10th ACM Conference on Web Science, pages 87\u201396.",
                "url": null
            }
        },
        {
            "19": {
                "title": "Lrc-bert: Latent-representation contrastive knowledge distillation for natural language understanding.",
                "author": "Hao Fu, Shaojun Zhou, Qihong Yang, Junjie Tang, Guiquan Liu, Kaikui Liu, and Xiaolong Li. 2020.",
                "venue": null,
                "url": "https://doi.org/10.48550/ARXIV.2012.07335"
            }
        },
        {
            "20": {
                "title": "A lexicon-based approach for hate speech detection.",
                "author": "Njagi Dennis Gitari, Zhang Zuping, Hanyurwimfura Damien, and Jun Long. 2015.",
                "venue": "International Journal of Multimedia and Ubiquitous Engineering, 10(4):215\u2013230.",
                "url": null
            }
        },
        {
            "21": {
                "title": "Deep learning.",
                "author": "Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016.",
                "venue": "MIT press.",
                "url": null
            }
        },
        {
            "22": {
                "title": "Knowledge distillation: A survey.",
                "author": "Jianping Gou, Baosheng Yu, Stephen J. Maybank, and Dacheng Tao. 2021.",
                "venue": "International Journal of Computer Vision, 129(6):1789\u20131819.",
                "url": "https://doi.org/10.1007/s11263-021-01453-z"
            }
        },
        {
            "23": {
                "title": "Distilling the knowledge in a neural network.",
                "author": "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.",
                "venue": "arXiv preprint arXiv:1503.02531.",
                "url": null
            }
        },
        {
            "24": {
                "title": "Chain of explanation: New prompting method to generate higher quality natural language explanation for implicit hate speech.",
                "author": "Fan Huang, Haewoon Kwak, and Jisun An. 2022.",
                "venue": "arXiv preprint arXiv:2209.04889.",
                "url": null
            }
        },
        {
            "25": {
                "title": "Vader: A parsimonious rule-based model for sentiment analysis of social media text.",
                "author": "Clayton J. Hutto and Eric Gilbert. 2014.",
                "venue": "Proceedings of the International AAAI Conference on Web and Social Media.",
                "url": "https://api.semanticscholar.org/CorpusID:12233345"
            }
        },
        {
            "26": {
                "title": "A systematic review of hate speech automatic detection using natural language processing.",
                "author": "Md Saroar Jahan and Mourad Oussalah. 2021.",
                "venue": "arXiv preprint arXiv:2106.00742.",
                "url": null
            }
        },
        {
            "27": {
                "title": "A systematic review of hate speech automatic detection using natural language processing.",
                "author": "Md Saroar Jahan and Mourad Oussalah. 2023.",
                "venue": "Neurocomputing, 546:126232.",
                "url": "https://doi.org/https://doi.org/10.1016/j.neucom.2023.126232"
            }
        },
        {
            "28": {
                "title": "Contextualizing hate speech classifiers with post-hoc explanation.",
                "author": "Brendan Kennedy, Xisen Jin, Aida Mostafazadeh Davani, Morteza Dehghani, and Xiang Ren. 2020.",
                "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5435\u20135442, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2020.acl-main.483"
            }
        },
        {
            "29": {
                "title": "Generalizable implicit hate speech detection using contrastive learning.",
                "author": "Youngwook Kim, Shinwoo Park, and Yo-Sub Han. 2022.",
                "venue": "In Proceedings of the 29th International Conference on Computational Linguistics, pages 6667\u20136679, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.",
                "url": "https://aclanthology.org/2022.coling-1.579"
            }
        },
        {
            "30": {
                "title": "Computing krippendorff\u2019s alpha-reliability.",
                "author": "Klaus Krippendorff. 2011.",
                "venue": null,
                "url": null
            }
        },
        {
            "31": {
                "title": "BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.",
                "author": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020.",
                "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871\u20137880, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2020.acl-main.703"
            }
        },
        {
            "32": {
                "title": "ROUGE: A package for automatic evaluation of summaries.",
                "author": "Chin-Yew Lin. 2004.",
                "venue": "In Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
                "url": "https://aclanthology.org/W04-1013"
            }
        },
        {
            "33": {
                "title": "Roberta: A robustly optimized bert pretraining approach.",
                "author": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.",
                "venue": "arXiv preprint arXiv:1907.11692.",
                "url": null
            }
        },
        {
            "34": {
                "title": "Decoupled weight decay regularization.",
                "author": "Ilya Loshchilov and Frank Hutter. 2019.",
                "venue": "In International Conference on Learning Representations.",
                "url": "https://openreview.net/forum?id=Bkg6RiCqY7"
            }
        },
        {
            "35": {
                "title": "Hate speech detection: Challenges and solutions.",
                "author": "Sean MacAvaney, Hao-Ren Yao, Eugene Yang, Katina Russell, Nazli Goharian, and Ophir Frieder. 2019.",
                "venue": "PLOS ONE, 14(8):1\u201316.",
                "url": "https://doi.org/10.1371/journal.pone.0221152"
            }
        },
        {
            "36": {
                "title": "Hatexplain: A benchmark dataset for explainable hate speech detection.",
                "author": "Binny Mathew, Punyajoy Saha, Seid Muhie Yimam, Chris Biemann, Pawan Goyal, and Animesh Mukherjee. 2021.",
                "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 35(17):14867\u201314875.",
                "url": "https://doi.org/10.1609/aaai.v35i17.17745"
            }
        },
        {
            "37": {
                "title": "\u2018hate speech in encyclopedia of the american constitution.",
                "author": "J Nockleyby. 2000.",
                "venue": "Electronic Journal of Academic and Special Librarianship.",
                "url": null
            }
        },
        {
            "38": {
                "title": "The effect of extremist violence on hateful speech online.",
                "author": "Alexandra Olteanu, Carlos Castillo, Jeremy Boy, and Kush Varshney. 2018.",
                "venue": "In Proceedings of the international AAAI conference on web and social media, volume 12.",
                "url": null
            }
        },
        {
            "39": {
                "title": "Bleu: a method for automatic evaluation of machine translation.",
                "author": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.",
                "venue": "In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.",
                "url": "https://doi.org/10.3115/1073083.1073135"
            }
        },
        {
            "40": {
                "title": "Language models are unsupervised multitask learners.",
                "author": "Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019.",
                "venue": null,
                "url": null
            }
        },
        {
            "41": {
                "title": "Exploring the limits of transfer learning with a unified text-to-text transformer.",
                "author": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020.",
                "venue": "Journal of Machine Learning Research, 21(140):1\u201367.",
                "url": "http://jmlr.org/papers/v21/20-074.html"
            }
        },
        {
            "42": {
                "title": "Characterizing and detecting hateful users on twitter.",
                "author": "Manoel Horta Ribeiro, Pedro H Calais, Yuri A Santos, Virg\u00edlio AF Almeida, and Wagner Meira Jr. 2018.",
                "venue": "In Twelfth international AAAI conference on web and social media.",
                "url": null
            }
        },
        {
            "43": {
                "title": "\"why should i trust you?\": Explaining the predictions of any classifier.",
                "author": "Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016.",
                "venue": null,
                "url": "http://arxiv.org/abs/1602.04938"
            }
        },
        {
            "44": {
                "title": "Social bias frames: Reasoning about social and power implications of language.",
                "author": "Maarten Sap, Saadia Gabriel, Lianhui Qin, Dan Jurafsky, Noah A. Smith, and Yejin Choi. 2020.",
                "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5477\u20135490, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2020.acl-main.486"
            }
        },
        {
            "45": {
                "title": "A survey on hate speech detection using natural language processing.",
                "author": "Anna Schmidt and Michael Wiegand. 2017.",
                "venue": "In Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media, pages 1\u201310, Valencia, Spain. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/W17-1101"
            }
        },
        {
            "46": {
                "title": "Explaining toxic text via knowledge enhanced text generation.",
                "author": "Rohit Sridhar and Diyi Yang. 2022.",
                "venue": "In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 811\u2013826, Seattle, United States. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2022.naacl-main.59"
            }
        },
        {
            "47": {
                "title": "HABERTOR: An efficient and effective deep hatespeech detector.",
                "author": "Thanh Tran, Yifan Hu, Changwei Hu, Kevin Yen, Fei Tan, Kyumin Lee, and Se Rim Park. 2020.",
                "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7486\u20137502, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2020.emnlp-main.606"
            }
        },
        {
            "48": {
                "title": "Well-read students learn better: On the importance of pre-training compact models.",
                "author": "Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.",
                "venue": null,
                "url": "https://doi.org/10.48550/ARXIV.1908.08962"
            }
        },
        {
            "49": {
                "title": "Attention is all you need.",
                "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. 2017.",
                "venue": "In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
                "url": "https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"
            }
        },
        {
            "50": {
                "title": "Challenges and frontiers in abusive content detection.",
                "author": "Bertie Vidgen, Alex Harris, Dong Nguyen, Rebekah Tromble, Scott Hale, and Helen Margetts. 2019.",
                "venue": "In Proceedings of the Third Workshop on Abusive Language Online, pages 80\u201393, Florence, Italy. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/W19-3509"
            }
        },
        {
            "51": {
                "title": "Inducing a lexicon of abusive words \u2013 a feature-based approach.",
                "author": "Michael Wiegand, Josef Ruppenhofer, Anna Schmidt, and Clayton Greenberg. 2018.",
                "venue": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1046\u20131056, New Orleans, Louisiana. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/N18-1095"
            }
        },
        {
            "52": {
                "title": "Transformers: State-of-the-art natural language processing.",
                "author": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020.",
                "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38\u201345, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2020.emnlp-demos.6"
            }
        },
        {
            "53": {
                "title": "Beyond error propagation in neural machine translation: Characteristics of language also matter.",
                "author": "Lijun Wu, Xu Tan, Di He, Fei Tian, Tao Qin, Jianhuang Lai, and Tie-Yan Liu. 2018.",
                "venue": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3602\u20133611, Brussels, Belgium. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/D18-1396"
            }
        },
        {
            "54": {
                "title": "Hate speech and counter speech detection: Conversational context does matter.",
                "author": "Xinchen Yu, Eduardo Blanco, and Lingzi Hong. 2022.",
                "venue": "arXiv preprint arXiv:2206.06423.",
                "url": null
            }
        },
        {
            "55": {
                "title": "Bertscore: Evaluating text generation with bert.",
                "author": "Tianyi Zhang, Varsha Kishore, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. 2020.",
                "venue": "In International Conference on Learning Representations.",
                "url": "https://openreview.net/forum?id=SkeHuCVFDr"
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.16685v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.2.1",
            "3.2.2",
            "3.2.3",
            "3.2.4"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "5.2"
        ]
    },
    "research_context": {
        "paper_id": "2403.16685v2",
        "paper_title": "ToXCL: A Unified Framework for Toxic Speech Detection and Explanation",
        "research_background": "### ToXCL: A Unified Framework for Toxic Speech Detection and Explanation\n\n#### Introduction\n\n**Motivation:**\nThe proliferation of social media has expanded public discourse but also propagated toxic speech, a harmful phenomenon with serious consequences for victims. Despite the advancement of toxic speech detection tools, implicit toxic speech remains a particularly challenging domain due to its reliance on stereotypes and indirect language. The ability to not only detect implicit toxic speech but also explain its toxic nature is essential for practical applications like improving human-machine interactions and fostering trustworthy AI systems. \n\n**Research Problem:**\nThe core problem addressed by this paper is the challenge of detecting implicit toxic speech and providing valid explanations for its toxicity within a unified framework. Existing approaches often treat detection and explanation tasks as a single text generation problem, leading to issues such as error propagation and reduced detection accuracy. Modular-based systems that separate these tasks are computationally expensive and inefficient. The need for an integrated, performance-effective system that addresses these issues forms the crux of the research problem.\n\n#### Relevant Prior Work\n\n- **Toxic Speech on Social Media:** Studies have highlighted the negative impact of toxic speech on individuals and groups. Prior works have addressed explicit toxic speech detection extensively but less so for implicit toxic speech, which demands complex understanding and explanation (Nockleyby, 2000; Schmidt and Wiegand, 2017).\n  \n- **Existing Detection Tools:** Various tools have been developed focusing on detecting toxic speech and a few combining detection with explanation. Notable contributions include the hybrid approach for simultaneous detection and explanation by concatenating labels and explanations as target outputs (Sap et al., 2020), and extended by including detailed contextual data (AlKhamissi et al., 2022; Huang et al., 2022). However, these methods can propagate errors due to divergent training objectives and are sometimes computationally cumbersome (Wu et al., 2018).\n  \n- **Challenges in Implicit Toxic Speech:** Implicit toxic speech, lacking direct offensive language, necessitates models that can dive deeper into context and insinuations, making the detection and explanation significantly more complex (ElSherief et al., 2021). Techniques such as teacher forcing and conditional decoding constraints have been proposed to enhance model performance in related tasks (Sridhar and Yang, 2022).\n\n**Proposed Solution - ToXCL:**\nTo bridge existing gaps, the paper introduces ToXCL, a unified framework containing three integrated modules:\n1. **Target Group Generator:** This module identifies minority target groups based on input posts.\n2. **Encoder-Decoder Model:** Utilizes joint training for detection and explanation tasks to minimize error propagation and improve overall performance.\n3. **Teacher Classifier and Conditional Decoding Constraint:** Enhances detection and explanation accuracy by distilling knowledge to the encoder and refining the explanation generation process.\n\nThe framework demonstrates state-of-the-art performance in benchmarks and offers practical improvements in the field of toxic speech detection and explanation.",
        "methodology": "The proposed method in the paper \"ToXCL: A Unified Framework for Toxic Speech Detection and Explanation\" incorporates the following key components and innovations:\n\n1. **Implicit Toxicity Detection:**\n   - **Binary Classifier Head:** A binary classifier is situated on top of the encoder of a pre-trained encoder-decoder model.\n   - **Linear Layer:** This layer processes the average of token embeddings from the encoder\u2019s last hidden state.\n   - **Softmax Layer:** Follows the linear layer for classification.\n   - **Training:** Both the encoder and classifier head are trained together using binary cross-entropy loss to optimize performance.\n\n2. **Explanation Generation:**\n   - **Decoder Utilization:** The decoder of the pre-trained encoder-decoder model is tasked with generating explanations for detected toxic speech.\n   - **Fine-Tuning:** The decoder is fine-tuned using a Causal Language Modeling (CLM) loss, which optimizes the performance of explanation generation.\n\n3. **Unified Training Approach:**\n   - **Joint Loss Training:** The encoder-decoder model is trained for both the task of detecting toxic speech and generating explanations by combining binary cross-entropy loss and CLM loss.\n   - **Contribution Weights:** The model optimizes performance with a combined loss function, incorporating contribution weights to balance the two tasks effectively.\n\nBy integrating these components, the framework aims to not only detect toxic speech but also provide explanations, enhancing the interpretability and applicability of the model in various contexts.",
        "main_experiment_and_results": "### Main Experiment Setup and Results of \"ToXCL: A Unified Framework for Toxic Speech Detection and Explanation\"\n\n#### Datasets\nThe main datasets used in the experiment are:\n- **IHC (Implicit Hate Corpus)**\n- **SBIC (Social Bias Inference Corpus)**\n\nBoth datasets provide comprehensive coverage of prevalent toxic groups from popular social media platforms like Twitter and Gab.\n\n#### Model and Baselines\nThe primary model in this experiment is a fine-tuned T5 model. The performance of this model is compared against the following baselines:\n- **BERT**: An encoder-based model.\n- **GPT-2**: A decoder-only model.\n- **BART**: An encoder-decoder model.\n\nThese baselines are chosen due to their historical success in tasks related to text classification and generation.\n\n#### Pre-processing and Training\nBefore the training, overlapping data between the HateXplain and the IHC and SBIC datasets is identified, with only one instance of overlap being removed. The models are initialized with pre-trained checkpoints from Huggingface and fine-tuned on a single Google CoLab P40 GPU.\n- **Window Size**: 256\n- **Learning Rate**: Not explicitly specified in the text\n- **Optimizer**: AdamW\n- **Epoch / Iterations**:\n  - BERT: Fine-tuned for 10 epochs\n  - GPT-2 and BART: Fine-tuned for 20k iterations\n- **Generation Decoding Strategy**: Beam search with a beam size of 4\n\n#### Evaluation Metrics\nThe evaluations are performed using:\n- **F1 Score (%):** For multi-label classification\n- **ROUGE-L (%):** For assessing generated text\n\n#### Results\nThe experimental results are summarized in Table 1 (not included in the text):\n- **Our Model (T5):**\n  - Achieved an F1 score of an unspecified value.\n  - Achieved a ROUGE-L score of an unspecified value.\n- **Baselines:** The T5 model outperformed all baselines (BERT, GPT-2, and BART) in identifying target groups in toxic posts.\n\n#### Human Evaluations\nHuman evaluations involved 5 native English speakers rating the generated explanations on a 1-3 scale for correctness, fluency, and toxicity, with strong agreement among the annotators (Krippendorff\u2019s alpha scores of at least 0.78).\n\n#### Key Observations\n- ToXCL outperformed all baselines across both IHC and SBIC benchmarks.\n- Human evaluations corroborated the quantitative results, showing ToXCL's explanations as more accurate and fluent with fewer harmful outputs.\n\nIn conclusion, the ToXCL framework effectively combines toxic speech detection and explanation, outperforming traditional baselines in both domains."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To determine the impact of the generated target groups (TG) on the performance of implicit toxic speech detection and explanation.",
            "experiment_process": "The experiment involves adding the Target Group Generator module (i.e., +TG) to all baselines in the experiments and comparing their performance against the same models without the generated target groups. Metrics used to evaluate performance include implicit toxic speech detection and explanation accuracy as highlighted in Table 3.",
            "result_discussion": "All baselines achieve significant improvements after adding the TG module, emphasizing its crucial role in enhancing the detection and explanation of implicit toxic speech. The Teacher Classifier, when boosted by the TG module, also outperforms its variant trained solely on input posts.",
            "ablation_id": "2403.16685v2.No1"
        },
        {
            "research_objective": "To assess the impact of incorporating teacher forcing (+ TF) on the performance of the encoder in ToXCL.",
            "experiment_process": "The experiment involves comparing the performance of Flan-T5 + CLH + TG before and after using teacher forcing (+ TF). The evaluation metric is the detection accuracy of the encoder, as detailed in Table 3.",
            "result_discussion": "Incorporating teacher forcing improves the performance of the encoder, demonstrating that additional guidance results in more accurate predictions, achieving performance close to that of the Teacher Classifier.",
            "ablation_id": "2403.16685v2.No2"
        },
        {
            "research_objective": "To evaluate the effectiveness of the Conditional Decoding Constraint in improving toxic explanation generation.",
            "experiment_process": "The experiment involves comparing the performance of Flan-T5 + CLH + TG + TF with and without the Conditional Decoding Constraint. The focus is on the task of toxic explanation generation as evidenced in Table 3.",
            "result_discussion": "The inclusion of the Conditional Decoding Constraint results in significant improvements in generating toxic explanations, confirming its effectiveness in synchronizing implicit toxic speech detection labels and toxic explanations.",
            "ablation_id": "2403.16685v2.No3"
        }
    ]
}