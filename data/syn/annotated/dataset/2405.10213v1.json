{
    "title": "Words as Trigger Points in Social Media Discussions",
    "abstract": "Trigger points are a concept introduced by Mau, Lux, and Westheuser (2023) to study qualitative focus group interviews and understand polarisation in Germany. When people communicate, trigger points represent moments when individuals feel that their understanding of what is fair, normal, or appropriate in society is questioned. In the original studies, individuals react affectively to such triggers and show strong and negative emotional responses. In this paper, we introduce the first systematic study of the large-scale effect of individual words as trigger points by analysing a large amount of social media posts. We examine online deliberations on Reddit between 2020 and 2022 and collect 100 million posts from subreddits related to a set of words identified as trigger points in UK politics. We find that such trigger words affect user engagement and have noticeable consequences on animosity in online discussions. We share empirical evidence of trigger words causing animosity, and how they provide incentives for hate speech, adversarial debates, and disagreements. Our work is the first to introduce trigger points to computational studies of online communication. Our findings are relevant to researchers interested in online harms and who examine how citizens debate politics and society in light of affective polarisation.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Trigger points, introduced by Mau, Lux, and Westheuser (2023), are rooted in theories of affective political identity and relate to deeply lying beliefs about moral expectations and social dispositions. Examining trigger points in online discussions helps understand why and when social media users engage in disagreements or affective political deliberations. This opens the door to modelling social media user engagement more effectively and studying the conditions and causal mechanisms that lead to negative reactions, hate speech, and abusive language in online debates.\n\nTrigger points are related to how individuals perceive themselves in society (Mau, Lux, and Westheuser 2023). People have subjective expectations about acceptable behaviour such as explicit knowledge of law-like rules and, more importantly, implicit understanding of appropriate actions. These deeply running beliefs of what is fair and unfair constitute a moral compass fundamental to navigating the social world. \n\nPeople feel triggered if these normative understandings are questioned. More formally, Mau, Lux, and Westheuser (2023, 246) define trigger points as \u201cthose moments that question what is acceptable in individuals understanding of the social contract. Trigger points jeopardise what individuals understand as the fabric of society and their own role in it.\u201d \n\nMau, Lux, and Westheuser (2023) propose four different mechanisms why trigger points affect individuals.  \n- Inequality. When expectations about equality might not be met. Either similar people are treated unequally, or, to the contrary, people who should be treated differently are treated the same. For example, social benefits might be at eye level with minimal wage earners.  \n- Norm transgression. It is the transgression of what is considered \u201cnormal\u201d behaviour. A typical example relates to the lavish lifestyle of the privileged.  \n- Slippery slope. There could be a fear of society developing norms about appropriate behaviour that goes in subjectively undesired directions and the impression that the individual might not have the power to do something against it. \u201cOpening the borders so that a country could be flooded with immigrants\u201d is an example in this context.  \n- Behavioural demands. When society may be perceived as demanding unreasonable behaviour. The role of pronouns is a point in case where the generic masculine pronouns \u201che/him\u201d might trigger some, and the neutral pronoun \u201cthey/them\u201d might trigger others.  \n\nGoing against deeply held beliefs about individuals and their societal role causes discomfort. People respond to such triggers in an affective behavioural mode and increase a debate\u2019s intensity and emotionality (Mau, Lux, and Westheuser 2023). In the social media context, we expect that this has two direct consequences. Trigger points lead to:  \n- Hypothesis 1 more user engagement, which incites a higher number of messages;\n- Hypothesis 2 higher levels of animosity, which may lead to polarisation, negativity, anger, and hate.  \n\nIn our study, we focus on how individual words can already act as trigger points in online debates. We look into five trigger words (Rwanda, Brexit, NHS, Vaccine, Feminism) in the context of UK politics and collect 100 million related Reddit posts. We then leverage textual and Natural Language Processing (NLP) tools to look into the extent to which these words lead to potentially negative, emotional, and harmful responses, and analyse the results based on a systematic comparison between three treatment and control groups:  \n- Space: specific subreddits we expect to be triggered by the selected words.\n- Semantics: words similar in meaning to the target words but without the triggering component.\n- Time: time periods identified as relevant for each specific trigger word.  \n\nOur results show that these trigger words cause higher levels of engagement and more animosity, measured as an increase in the controversiality, negative sentiment, anger, and hate speech in a thread."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Trigger points (Mau, Lux, and Westheuser 2023  ###reference_b28###) bear great potential to understand polarisation and division in online discussions. However, the existence or validity of the concept has not been tested on a large scale. Trigger points can be particularly relevant to work on social media. They help understand the causes of disagreements in online conversations (De Kock and Vlachos 2021  ###reference_b16###) and reasons for their escalation (De Kock and Vlachos 2022  ###reference_b17###; van der Meer et al. 2023  ###reference_b46###).\nFurther to triggering ad hominem attacks, escalation can lead to hate speech. There is, however, no unanimous agreement on how it differs from abusive, and offensive language (Basile et al. 2019  ###reference_b6###; Davidson et al. 2017  ###reference_b15###; Talat and Hovy 2016  ###reference_b41###).\nDespite the rising interest in hate speech and harmful language in social media platforms, including its implicit forms (Ousidhoum et al. 2019  ###reference_b34###; Sap et al. 2019  ###reference_b40###; ElSherief et al. 2021  ###reference_b18###; Ocampo et al. 2023  ###reference_b32###), and dogwhistles, which are coded expressions that carry two meanings with a hateful or provocative coded one (Breitholtz and Cooper 2021  ###reference_b10###), we know little about the sources of these phenomena. Examining what can trigger harmful reactions may help us make the detection process more robust and avoid the well-known problem of selection bias (Wiegand, Ruppenhofer, and Kleinbauer 2019  ###reference_b49###; Ousidhoum, Song, and Yeung 2020  ###reference_b35###) caused by the common data collection strategies based on hashtags (Talat and Hovy 2016  ###reference_b41###) and keywords (Davidson et al. 2017  ###reference_b15###).\nSimilarly, considering the broad array of harmful online behaviours, the wide range of elements they involve, and how they can be included in social media data (Olteanu et al. 2019  ###reference_b33###), Blodgett et al. (2020  ###reference_b8###) report a missing paradigm that helps assess the initial causes without mainly focusing on the performance of the detection model. Consequently, studying trigger points can help us track the origins of bias and assess whether a social media post can lead to hateful comments (Dahiya et al. 2021  ###reference_b14###)."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "Mau, Lux, and Westheuser (2023) developed the concept of trigger points based on qualitative interviews with focus groups to analyze the perceived polarization of German society. We use this concept to study online communication on social media at a large scale. To critically assess its reliability, we test whether individual words can trigger the reactions outlined in the two hypotheses, i.e., (1) more engagement and (2) animosity.\n\nRwanda plays a particular role in the UK\u2019s immigration policy. Until late 2021, Rwanda was just the name of any other African country. However, with the then government, the term\u2019s meaning changed in early 2022 and became emblematic of how the British government suggested handling asylum seekers, and the home secretary struck a deal with Rwanda: the Migration and Economic Development Partnership. Instead of handling asylum cases directly in the UK, migrants would be flown to Kigali, and their claims were to be processed there, in turn for U$300m, (2.3% of Rwandan GDP) and expenses for every migrant sent (The Economist 2023). With this, Rwanda became a trigger word for discussions on immigration policy in the UK from 2022 onwards, particularly relating to concerns about \u201cunfair favors towards migrants and unregulated immigration.\u201d\n\nThe UK\u2019s departure from the European Union has been controversial since its inception. Even eight years post-exit, debates persist regarding the soundness of the decision. The division among the population was already evident during the tightly contested 2016 referendum, which revealed distinct trends among various demographic groups based on location, age, and income (Arnorsson and Zoega 2018). Since the referendum, further divisions have emerged concerning the future relationship between the UK and EU (Vasilopoulou and Talving 2019). Additionally, misinformation and disinformation related to Brexit have intensified these debates (H\u00f6ller 2021; Bruno, Lambiotte, and Saracco 2022). Here, considerations about treating EU migrants differently than the national population (inequality), norm transgressions of \u201ccriminal\u201d migrants, particularly from the East of Europe, or opening the gate to \u201cunlimited\u201d immigration make Brexit a potential trigger word for analysis.\n\nDue to recent financial austerity measures, the UK government was accused of neglecting the health care system and reducing its budget, which leads to deteriorating services (The Guardian 2022; The Independent 2024). As a result, the NHS has become a hot topic of discussion among the UK population. Public opinion is divided, with some supporting that it receives adequate funding, while others believe it wastes resources or that its services are in decline (Gershlick, Charlesworth, and Taylor 2015). This trigger word is related to going against considerations about unequal treatment (i.e., inequality as shown in Table 1).\n\nPublic vaccination programs, particularly those against COVID-19, have caused significant debate in recent years. Even after their widespread roll-out, discussions over their effectiveness and safety are ongoing. Initial reluctance was observed across various demographic segments, influenced by factors such as education level, political affiliation, and socioeconomic status (Troiano and Nardi 2021). Post-roll-out, debates have shifted to focus on issues such as the fairness of vaccine distribution and unreasonable demands for vaccination (Nichol and Mermin-Bunnell 2021). Moreover, the proliferation of misinformation and conspiracy theories regarding vaccines has further fueled public division (Hayawi et al. 2022; Garett and Young 2021), particularly in social media, making Vaccine an ideal trigger word for our study.\n\nMisogyny can be observed on social media, where anonymity often allows users to express offensive and hateful sentiments (Barker and Jurasz 2019). Previous work has shown that women politicians and journalists tend to be harassed online every few seconds (Posetti et al. 2022). Some social media influencers have built a large audience by sharing and promoting harmful content against women. This has led to the formation of online echo chambers (Farrell et al. 2019) where such views are amplified and propagated. Our research includes Feminism as a trigger word based on mechanisms related to norm transgressions, fears of a slippery slope and unreasonable behavioral demands.\n\nDiscussions on Reddit are structured in different fora, also called subreddits. Aiming to investigate the different effects a trigger word might have on specific subsets of users, we define the treatment group as a set of popular subreddits related to the trigger word (Table 3). We compare them with discussions in a control group, i.e., the rest of Reddit with many non-domain specific subreddits. We also compare trigger words with a term that shares a similar meaning without going against the deeper underlying transgressions that incite trigger responses. To members of the subreddits in Table 3, using an explicit trigger word or its"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Term selection",
            "text": "We selected terms likely to provoke reactions from individuals in the United Kingdom. Specifically, we include National Health Service (NHS), Brexit, Rwanda, Vaccine, and Feminist/Feminism in our study. These trigger words were chosen because they are associated with high-profile public debates, policy changes, and social movements, which have been known to provoke significant public engagement and emotional responses. Table 1 indicates the respective mechanism through which the terms may trigger individuals.\n\nRwanda plays a particular role in the UK\u2019s immigration policy. Until late 2021, Rwanda was just the name of any other African country. However, with the then government, the term\u2019s meaning changed in early 2022 and became emblematic of how the British government suggested handling asylum seekers, and the home secretary struck a deal with Rwanda: the Migration and Economic Development Partnership. Instead of handling asylum cases directly in the UK, migrants would be flown to Kigali, and their claims were to be processed there, in turn for U$300m, (2.3% of Rwandan GDP) and expenses for every migrant sent (The Economist 2023). With this, Rwanda became a trigger word for discussions on immigration policy in the UK from 2022 onwards, particularly relating to concerns about \u201cunfair favours towards migrants and unregulated immigration\u201d.\n\nThe UK\u2019s departure from the European Union has been controversial since its inception. Even eight years post-exit, debates persist regarding the soundness of the decision. The division among the population was already evident during the tightly contested 2016 referendum, which revealed distinct trends among various demographic groups based on location, age, and income (Arnorsson and Zoega 2018). Since the referendum, further divisions have emerged concerning the future relationship between the UK and EU (Vasilopoulou and Talving 2019). Additionally, misinformation and disinformation related to Brexit have intensified these debates (H\u00f6ller 2021; Bruno, Lambiotte, and Saracco 2022). Here, considerations about treating EU migrants differently than the national population (inequality), norm transgressions of \u201ccriminal\u201d migrants, particularly from the East of Europe, or opening the gate to \u201cunlimited\u201d immigration make Brexit a potential trigger word for analysis.\n\nDue to recent financial austerity measures, the UK government was accused of neglecting the health care system and reducing its budget, which leads to deteriorating services (The Guardian 2022; The Independent 2024). As a result, the NHS has become a hot topic of discussion among the UK population. Public opinion is divided, with some supporting that it receives adequate funding, while others believe it wastes resources or that its services are in decline (Gershlick, Charlesworth, and Taylor 2015). This trigger word is related to going against considerations about unequal treatment (i.e., inequality as shown in Table 1).\n\nPublic vaccination programs, particularly those against COVID-19, have caused significant debate in recent years. Even after their widespread roll-out, discussions over their effectiveness and safety are ongoing. Initial reluctance was observed across various demographic segments, influenced by factors such as education level, political affiliation, and socioeconomic status (Troiano and Nardi 2021). Post-roll-out, debates have shifted to focus on issues such as the fairness of vaccine distribution and unreasonable demands for vaccination (Nichol and Mermin-Bunnell 2021). Moreover, the proliferation of misinformation and conspiracy theories regarding vaccines has further fuelled public division (Hayawi et al. 2022; Garett and Young 2021), particularly in social media, making Vaccine an ideal trigger word for our study.\n\nMisogyny can be observed on social media, where anonymity often allows users to express offensive and hateful sentiments (Barker and Jurasz 2019). Previous work has shown that women politicians and journalists tend to be harassed online every few seconds (Posetti et al. 2022).\n\nSome social media influencers have built a large audience by sharing and promoting harmful content against women. This has led to the formation of online echo chambers (Farrell et al. 2019) where such views are amplified and propagated. Our research includes Feminism as a trigger word based on mechanisms related to norm transgressions, fears of a slippery slope and unreasonable behavioural demands."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Data collection",
            "text": "Our study uses data from Reddit comments in English, collected via the Pushshift API (Baumgartner et al. 2020). We searched for exact matches of specific keywords in comments, ignoring the respective case, and gathered all corresponding posts from 2019 to 2022. Subsequently, the entire discussion thread for each comment extracted was collected. When extracting comments for the term woman, we capped the maximum number of comments at 20 million due to the large volume.\n\nWe excluded posts in languages other than English, as identified with fastText-based language identifier (Bojanowski et al. 2017). Posts marked as \u201c[deleted]\u201d by the API were also removed. Following this, only threads that contained posts before and after the appearance of the target keywords were retained, as it is crucial to analyse the potential differences before and after the appearance of a given trigger point. As a final filtering step, and considering the fast-paced nature of Reddit, we limited our analysis to posts occurring 30 minutes before and 30 minutes after the first mention of the triggering word. Table 2 shows the total number of comments, threads, and comments with triggers collected for each trigger point and the time period they were gathered."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Research design",
            "text": "We identify the causal effect of trigger points with the difference-in-difference estimator (Angrist and Pischke 2009  ###reference_b1###; Card and Krueger 1994  ###reference_b13###). This estimator is based on systematically comparing a treatment and a control group. It calculates the causal effect of an intervention as the difference before and after the intervention in the treatment group minus the difference before and after the intervention in the control group. In our context, we expect users in the treatment group to be highly likely to be triggered by the respective terms and those in the control group to be much less triggered by the respective words.\n\nDiscussions on Reddit are structured in different fora, also called subreddits. Aiming to investigate the different effects a trigger word might have on specific subsets of users, we define the treatment group as a set of popular subreddits related to the trigger word (Table 3  ###reference_###  ###reference_###). We compare them with discussions in a control group, i.e., the rest of Reddit with many non-domain specific subreddits. \n\nWe also compare trigger words with a term that shares a similar meaning without going against the deeper underlying transgressions that incite trigger responses. To members of the subreddits in Table 3  ###reference_###  ###reference_###, using an explicit trigger word or its more harmless control term should make a real difference. We define related control group words and contrast woman to Feminism/Feminist, Red Cross to NHS, EU/European Union to Brexit, surgery to vaccine, and names of Rwanda\u2019s neighbouring countries444Burundi, Tanzania, Congo, Democratic Republic of Congo, Uganda, Kenya, and Zambia. to Rwanda.\n\nIn the third experiment, we make use of the fact that two of our terms\u2014Vaccine and Rwanda\u2014only became explicit trigger words at specific points in time. For \u201cRwanda\u201d we compare discussions in subreddits likely to incite reactions in 2020 to discussions in 2022. For Vaccine we compare threads from 2019 (before the COVID-19 pandemic) to 2022\u2019s (post COVID-19 pandemic). Again, we expect that users from the subreddits in Table 3  ###reference_###  ###reference_### are triggered by those only in 2022 but not in the respective earlier years."
        },
        {
            "section_id": "3.3.x",
            "parent_section_id": "3.3",
            "section_name": "Control groups",
            "text": "We systematically compare treatment and control groups along three different dimensions. Discussions on Reddit are structured in different fora, also called subreddits. Aiming to investigate the different effects a trigger word might have on specific subsets of users, we define the treatment group as a set of popular subreddits related to the trigger word. We compare them with discussions in a control group, i.e., the rest of Reddit with many non-domain specific subreddits.\n\nWe also compare trigger words with a term that shares a similar meaning without going against the deeper underlying transgressions that incite trigger responses. To members of the subreddits we analyze, using an explicit trigger word or its more harmless control term should make a real difference. We define related control group words and contrast woman to Feminism/Feminist, Red Cross to NHS, EU/European Union to Brexit, surgery to vaccine, and names of Rwanda\u2019s neighbouring countries (Burundi, Tanzania, Congo, Democratic Republic of Congo, Uganda, Kenya, and Zambia) to Rwanda.\n\nIn the third experiment, we make use of the fact that two of our terms\u2014Vaccine and Rwanda\u2014only became explicit trigger words at specific points in time. For \u201cRwanda\u201d we compare discussions in subreddits likely to incite reactions in 2020 to discussions in 2022. For Vaccine we compare threads from 2019 (before the COVID-19 pandemic) to 2022\u2019s (post COVID-19 pandemic). Again, we expect that users from the subreddits we study are triggered by those only in 2022 but not in the respective earlier years."
        },
        {
            "section_id": "3.3.x",
            "parent_section_id": "3.3",
            "section_name": "Statistical analysis",
            "text": "We use the difference-in-difference (DiD) estimator to analyse the causal effect of the trigger word on different online behaviours. The idea behind the DiD estimator is to compare the outcome change between the treatment and control groups before and after the trigger word. Specifically, we calculate the difference in the average outcome change for the treatment group before and after the trigger word and then subtract the difference in the average outcome change for the control group. If the treatment has a causal effect, we expect a larger change for the treatment group than for the control group. For each thread, we normalise by the actual number of messages, and calculate the share of messages before and after the appearance of the trigger word. To estimate the causal effect, we specify the DiD as an OLS regression:\nwhere proportion is the per-thread-normalised target feature and the interaction term is given by"
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Features",
            "text": "We annotate several features to examine our initial hypotheses and study the impact of trigger words in online discussions. We simply use post counts for the first hypothesis (H1) related to user engagement. The second hypothesis (H2) requires a measure of animosity. To identify relevant features that can operationalise animosity, we build on various language models that follow the RoBERTa architecture (Liu et al. 2019) and are trained on social media corpora, specifically X (Twitter). Even though posts on Reddit tend to be longer than on X, the posts on both platforms share many similarities, e.g., the use of slang, emojis, unstructured text. Hence, we expect the trained models to perform well in either context (Priya et al. 2019).\n\nWe examine whether a comment is controversial using the Reddit controversiality feature. Reddit offers a measure that identifies a comment as controversial if it receives similar amounts of upvotes and downvotes. This metric is based on the users\u2019 approval, so care must be taken when considering already controversial subreddits. We expect trigger words to cause more controversiality.\n\nFor sentiment analysis, we rely on the model twitter-roberta-base-sentiment-latest (Loureiro et al. 2022) which identifies a post as negative, neutral, or positive. The sentiment in social media content has often been used to gain insights into the information related to controversial topics such as vaccination (Melton et al. 2021), politics (Guimaraes et al. 2019), and gender biases (Marjanovic, Sta\u0144czak, and Augenstein 2022). We expect trigger words to cause more negative sentiments.\n\nWe rely on twitter-roberta-base-emotion-multilabel-latest (Camacho-Collados et al. 2022) to assign one or more emotions to each tweet. This model is trained with data from the Semeval 2018 Affect in Tweets task (Mohammad et al. 2018), covering 11 different emotions. For our analysis we focus on anger specifically, and we expect to observe more anger after a trigger word.\n\nFinally, we use the twitter-roberta-base-hate-multiclass model (Antypas and Camacho-Collados 2023) to detect hate speech. It is trained on a combination of 13 different hate speech Twitter datasets and can predict hate towards seven target groups. The inclusion of hate speech detection as a feature is motivated by previous research revealing hateful trends in topics such as politics (Rieger et al. 2021), women\u2019s rights (Farrell et al. 2019), and health issues (Walter et al. 2022). Considering the connotations of some of the triggers, we specifically focus on sexism for Feminism/Feminist, racism for Rwanda, while we consider all hateful comments jointly for NHS, Brexit and Vaccine."
        },
        {
            "section_id": "3.4.x",
            "parent_section_id": "3.4",
            "section_name": "User engagement (Hypothesis 1)",
            "text": "Our first Hypothesis suggests that trigger words lead to higher user engagement. We simply rely on the number of posts to investigate this. That is, we expect to see an uptick in message frequency after the trigger words are mentioned for the first time in a thread."
        },
        {
            "section_id": "3.4.x",
            "parent_section_id": "3.4",
            "section_name": "Animosity (Hypothesis 2)",
            "text": "Hypothesis 2 suggests that trigger words cause more animosity in a debate. We operationalise this with controversial comments, the presence of negative sentiment, anger, and different forms of hate speech.\n\nWe examine whether a comment is controversial using the Reddit controversiality feature. Reddit offers a measure that identifies a comment as controversial if it receives similar amounts of upvotes and downvotes. This metric is based on the users\u2019 approval, so care must be taken when considering already controversial subreddits. We expect trigger words to cause more controversiality.\n\nFor sentiment analysis, we rely on the model twitter-roberta-base-sentiment-latest (Loureiro et al. 2022), which identifies a post as negative, neutral, or positive. The sentiment in social media content has often been used to gain insights into the information related to controversial topics such as vaccination (Melton et al. 2021), politics (Guimaraes et al. 2019), and gender biases (Marjanovic, Sta\u0144czak, and Augenstein 2022). We expect trigger words to cause more negative sentiments.\n\nWe rely on twitter-roberta-base-emotion-multilabel-latest (Camacho-Collados et al. 2022) to assign one or more emotions to each tweet. This model is trained with data from the Semeval 2018 Affect in Tweets task (Mohammad et al. 2018), covering 11 different emotions. For our analysis, we focus on anger specifically, and we expect to observe more anger after a trigger word.\n\nFinally, we use the twitter-roberta-base-hate-multiclass model (Antypas and Camacho-Collados 2023) to detect hate speech. It is trained on a combination of 13 different hate speech Twitter datasets and can predict hate towards seven target groups. The inclusion of hate speech detection as a feature is motivated by previous research revealing hateful trends in topics such as politics (Rieger et al. 2021), women\u2019s rights (Farrell et al. 2019), and health issues (Walter et al. 2022). Considering the connotations of some of the triggers, we specifically focus on sexism for Feminism/Feminist, racism for Rwanda, while we consider all hateful comments jointly for NHS, Brexit, and Vaccine."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "In this section, we present the results for the two hypotheses tested in this paper. We analyze the average controversiality of each thread and compare the proportion of controversial threads before and after the introduction of the trigger words. Our results indicate that, in general, there is an increase in controversial comments following each trigger word. Most terms display a statistically significant increase of DiD estimates when considering controversial messages in the treatment and control groups with the biggest difference observed on the semantic comparison for Rwanda, with a causal effect of 6.6% points.\n\nHowever, significant differences in controversiality do not appear across all settings. Notably, there seems to be no increase in controversial comments when considering Feminism, and in the temporal and semantic settings for Vaccine and NHS, respectively.\n\nFigure 4 shows the percentages of controversial threads related to the trigger word Rwanda in both the space control and treatment groups, before and after its occurrence. The data indicates a noticeable increase in thread controversiality caused by the trigger word, reflected by a DiD estimate of 2.3% points. A similar trend is seen in both the semantic and temporal control settings.\n\nWe further investigate the effect trigger words have on sentiment and emotion. Based on our hypothesis, trigger words cause users to express more negatively charged emotions. In our analysis, we focus on the presence of negative sentiment and, more narrowly, the presence of anger in emotions.\n\nWhen considering the negatively charged comments of the treatment and control groups for each trigger word, we observe a significant increase in comments conveying a negative sentiment after the trigger word appears in 11 of the 12 settings tested, with the sole exception being the semantics comparison for NHS. The biggest effect in sentiment, with a DiD estimate of 5.5% points, appears for the Rwanda trigger word within both the space and semantics control groups.\n\nLet us consider the vaccine trigger as an example. Here, we again plot the overall number of messages we observe. Both treatment and space control groups exhibit a similar pattern of negative sentiment density as time approaches the trigger word, with an increase in negative sentiment just before and after the trigger word is mentioned. However, the treatment group shows more messages than the control group, suggesting that introducing the trigger words affects the expression of negative sentiment.\n\nSimilarly to the increase in negative sentiment, trigger words systematically cause more anger. We observe a positive causal effect in all conditions, with only 3 non-significant effects. Substantively, we measure a causal effect that ranges from 1.4% points more anger (Feminist) to 5.5% points (Vaccine semantic comparison). As another visual example, the overall number of posts expressing anger in the Vaccine treatment and space control groups increase significantly following the appearance of the trigger word. This is further confirmed by the DiD estimates, which indicate a significant difference in anger-related comments in 9 out of 12 cases.\n\nAs a final indicator of animosity, we also consider hate speech. In our analysis, we use any kind of hate speech for Brexit, NHS, and Vaccine, and the more domain-specific racism-related hate speech for Rwanda and sexism-related hate speech for Feminism. The impact of trigger words regarding the occurrence of hateful comments is not as prevalent as anger and negative sentiment. We observe significant effects in those areas where we can count on a domain-specific hate speech measure, i.e., in the cases of Feminism (space and semantic comparison) and Rwanda (space group).\n\nWe exemplify our findings again in a visualization, where we consider all messages irrespective of the thread they are posted in. We observe a distinct pattern, particularly in the context of Rwanda and Feminism where the volume of hateful content increases post-trigger. This reveals the prevalence of racist and sexist comments within the treatment groups when compared to their control counterparts in regards to space."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "User engagement (Hypothesis 1)",
            "text": "Our initial hypothesis is that if trigger words significantly influence online discussions, there should be more messages after mentioning these terms. The DiD estimate reveals significant differences between the frequency of comments in the treatment and control groups for all our trigger words.\n\nIn the treatment-only subset, the difference between user engagement before and after the trigger word is apparent for all the terms we selected. It is largest with Rwanda where we observe a 10%-points increase. To identify the causal effect of the trigger word, we compare the difference in the treatment group to the difference in the control group. In every setting, we observe a significant difference in the number of comments in the threads between the treatment and control subsets, with the frequency of comments in the treatment group increasing by a larger factor according to the DiD estimate. The lowest value is a 3.2%-point increase for the semantic comparison in Brexit. The largest increase is for NHS in the semantic control group, where we observe a 13.4% point increase.\n\nWhen examining comment frequency across various control groups, the impact of specific trigger words becomes more apparent. For instance, Figure 3 illustrates the distribution of comments related to the trigger Rwanda in the space related experiment. It shows the overall number of messages across all threads. This trigger word leads to varying levels of engagement across different Reddit communities, with a notable increase in comments in identified communities where Rwanda is more controversial. Similar patterns are also observed in the semantic and temporal control groups for Rwanda.\n\nOverall, our analysis of the influence of trigger words across different control groups indicates a clear increase in engagement for each trigger word analysed, confirming our initial hypothesis on the effect of trigger words on online discussions."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Animosity (Hypothesis 2)",
            "text": "Our second hypothesis suggests that trigger words may increase the frequency of polarising messages and induce animosity in the discussion. We analyse the average controversiality of each thread and compare the proportion of controversial threads before and after the introduction of the trigger words. Our results indicate that, in general, there is an increase in controversial comments following each trigger word. Most terms display a statistically significant increase of DiD estimates when considering controversial messages in the treatment and control groups with the biggest difference observed on the semantic comparison for Rwanda, with a causal effect of 6.6% points. However, significant differences in controversiality do not appear across all settings. Notably, there seems to be no increase in controversial comments when considering Feminism, and in the temporal and semantic settings for Vaccine and NHS, respectively.\n\nThe data indicates a noticeable increase in thread controversiality caused by the trigger word, reflected by a DiD estimate of 2.3% points. A similar trend is seen in both the semantic and temporal control settings, as documented.\n\nWe further investigate the effect trigger words have on sentiment and emotion. Based on our hypothesis, trigger words cause users to express more negatively charged emotions. In our analysis, we focus on the presence of negative sentiment and, more narrowly, the presence of anger in emotions.\n\nWhen considering the negatively charged comments of the treatment and control groups for each trigger word, we observe a significant increase in comments conveying a negative sentiment after the trigger word appears in 11 of the 12 settings tested, with the sole exception being the semantics comparison for NHS. The biggest effect in sentiment, with a DiD estimate of 5.5% points, appears for the Rwanda trigger word within both the space and semantics control groups.\n\nLet us consider the vaccine trigger as an example. Both treatment and space control groups exhibit a similar pattern of negative sentiment density as time approaches the trigger word, with an increase in negative sentiment just before and after the trigger word is mentioned. However, the treatment group shows more messages than the control group, suggesting that introducing the trigger words affects the expression of negative sentiment.\n\nSimilarly to the increase in negative sentiment, trigger words systematically cause more anger. We observe a positive causal effect in all conditions, with only 3 non-significant effects. Substantively, we measure a causal effect that ranges from 1.4% points more anger (Feminist) to 5.5% points (Vaccine semantic comparison). As another visual example, the overall number of posts expressing anger in the Vaccine treatment and space control groups increase significantly following the appearance of the trigger word. This is further confirmed by the DiD estimates, which indicate a significant difference in anger-related comments in 9 out of 12 cases.\n\nAs a final indicator of animosity, we also consider hate speech. In our analysis, we use any kind of hate speech for Brexit, NHS, and Vaccine, and the more domain-specific racism-related hate speech for Rwanda and sexism-related hate speech for Feminism. The impact of trigger words regarding the occurrence of hateful comments is not as prevalent as anger and negative sentiment. We observe significant effects in those areas where we can count on a domain-specific hate speech measure, i.e., in the cases of Feminism (space and semantic comparison) and Rwanda (space group).\n\nWe exemplify our findings again in a visualisation, where we consider all messages irrespective of the thread they are posted in. We observe a distinct pattern, particularly in the context of Rwanda and Feminism, where the volume of hateful content increases post-trigger. This reveals the prevalence of racist and sexist comments within the treatment groups when compared to their control counterparts in regards to space."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we introduce the concept of trigger words, and study how they affect social media conversations. We reported empirical evidence based on a large-scale study on Reddit using five trigger words (Rwanda, Feminism, Brexit, NHS, and Vaccine). In short, we find that trigger words cause more user engagement, but that this activity is marked by higher levels of animosity.\n\nOur findings confirm the first Hypothesis (H1), which suggests more user interaction after the appearance of a trigger word. We consistently observe more posts for all trigger words and across all control settings. Regarding our second hypothesis (H2), which concerns the impact of trigger words on the animosity of the ensuing discussions, we examined four different features that can indicate such effects. Controversiality, negative sentiment, and anger all show strong evidence that trigger words influence the discussion along those lines. We also investigated the causal effect of trigger words on hate speech. Where we could rely on a model that is capable of identifying domain-specific forms of hate speech\u2014Sexism in the case of Feminism and Racism in the case of Rwanda\u2014we find evidence that suggest trigger words play a causal role, here, too."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "Despite potentially more accurate results, we refrained from using large language models (LLMs) for feature extraction mainly due to the vast size of our dataset (100 million entries). Using LLMs would have significantly increased the computational and financial cost and the ecological impact of our experiments. Moreover, existing literature indicates that the zero-shot capabilities of LLMs do not consistently outperform smaller models for specific social media tasks like ours (Antypas et al. 2023  ###reference_b3###).\nSelecting words as trigger points and the subreddits for analysing them offers room for improvement. The chosen subreddits may not fully represent the broader range of discussions these trigger words could provoke across the diverse Reddit community.\nThe goal of this paper is to verify trigger words as a concept. As such, we identify and manually select a few words as candidates according to our own expertise and social science literature. If we do not find evidence for the effects of trigger words in these easy cases, it would not make sense to study trigger words and their effects any further."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Ethics Statement",
            "text": "All the data utilised in our research is sourced from publicly available information or collected using the official Reddit API. All the information is provided in an aggregated fashion, without reporting sensitive information from individual users.\nThe automatic methods used can be abused by those with the power to repress people with opposing opinions. Hence, we offer a comprehensive study but do not share our data publicly to avoid potential misuse or commercial use. Researchers interested in replicating our experiments can contact us directly."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Paper Checklist",
            "text": "For most authors\u2026\nWould answering this research question advance science without violating social contracts, such as violating privacy norms, perpetuating unfair profiling, exacerbating the socio-economic divide, or implying disrespect to societies or cultures?\nYes\nDo your main claims in the abstract and introduction accurately reflect the paper\u2019s contributions and scope?\nYes\nDo you clarify how the proposed methodological approach is appropriate for the claims made?\nYes\nDo you clarify what are possible artifacts in the data used, given population-specific distributions?\nYes\nDid you describe the limitations of your work?\nYes\nDid you discuss any potential negative societal impacts of your work?\nYes\nDid you discuss any potential misuse of your work?\nYes\nDid you describe steps taken to prevent or mitigate potential negative outcomes of the research, such as data and model documentation, data anonymization, responsible release, access control, and the reproducibility of findings?\nYes\nHave you read the ethics review guidelines and ensured that your paper conforms to them?\nYes\nAdditionally, if your study involves hypotheses testing\u2026\nDid you clearly state the assumptions underlying all theoretical results?\nYes\nHave you provided justifications for all theoretical results?\nYes\nDid you discuss competing hypotheses or theories that might challenge or complement your theoretical results?\nNA\nHave you considered alternative mechanisms or explanations that might account for the same outcomes observed in your study?\nYes\nDid you address potential biases or limitations in your theoretical framework?\nYes\nHave you related your theoretical results to the existing literature in social science?\nYes\nDid you discuss the implications of your theoretical results for policy, practice, or further research in the social science domain?\nYes\nAdditionally, if you are including theoretical proofs\u2026\nDid you state the full set of assumptions of all theoretical results?\nNA\nDid you include complete proofs of all theoretical results?\nNA\nAdditionally, if you ran machine learning experiments\u2026\nDid you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?\nNO: The code used in the experiments will be released upon acceptance. While the data will be shared upon request (due to size).\nDid you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?\nNA\nDid you report error bars (e.g., with respect to the random seed after running experiments multiple times)?\nYes\nDid you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?\nYes\nDo you justify how the proposed evaluation is sufficient and appropriate to the claims made?\nYes\nDo you discuss what is \u201cthe cost\u201c of misclassification and fault (in)tolerance?\nYes\nAdditionally, if you are using existing assets (e.g., code, data, models) or curating/releasing new assets, without compromising anonymity\u2026\nIf your work uses existing assets, did you cite the creators?\nYes\nDid you mention the license of the assets?\nNA\nDid you include any new assets in the supplemental material or as a URL?\nNo\nDid you discuss whether and how consent was obtained from people whose data you\u2019re using/curating?\nNA\nDid you discuss whether the data you are using/curating contains personally identifiable information or offensive content?\nYes\nIf you are curating or releasing new datasets, did you discuss how you intend to make your datasets FAIR?\nNA\nIf you are curating or releasing new datasets, did you create a Datasheet for the Dataset?\nNA\nAdditionally, if you used crowdsourcing or conducted research with human subjects, without compromising anonymity\u2026\nDid you include the full text of instructions given to participants and screenshots?\nNA\nDid you describe any potential participant risks, with mentions of Institutional Review Board (IRB) approvals?\nNA\nDid you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?\nNA\nDid you discuss how data is stored, shared, and deidentified?\nNA"
        }
    ],
    "url": "http://arxiv.org/html/2405.10213v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.3.x",
            "3.4",
            "3.4.x"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.4",
            "3.4.x"
        ]
    },
    "research_context": {
        "paper_id": "2405.10213v1",
        "paper_title": "Words as Trigger Points in Social Media Discussions",
        "research_background": "### Motivation:\nThe motivation behind this paper is to deepen the understanding of how specific words, termed as \"trigger points,\" influence social media discussions. Trigger points correlate with individuals\u2019 deeply-held beliefs about societal norms, moral expectations, and affective political identity. By examining these trigger points in online discussions, researchers aim to uncover the conditions and mechanisms that lead to elevated user engagement and negative reactions, such as hate speech and abusive language. This understanding can facilitate better modeling of social media user engagement and inform strategies to manage and mitigate harmful online behavior.\n\n### Research Problem:\nThe central research problem the paper addresses is identifying and analyzing how individual words function as trigger points in online social media debates. Specifically, the study investigates:\n1. How these trigger words increase user engagement, leading to a higher volume of messages.\n2. How these trigger words enhance animosity, contributing to polarization, negativity, anger, and hate speech.\n\nThe paper focuses on five specific trigger words within the context of UK politics (Rwanda, Brexit, NHS, Vaccine, Feminism) and examines the response to these words using a dataset of 100 million Reddit posts.\n\n### Relevant Prior Work:\nThe concept of trigger points and their theoretical foundation are extensively based on the work of Mau, Lux, and Westheuser (2023). Their theory situates trigger points within affective political identity frameworks and explains how deeply-ingrained beliefs about moral expectations and social organization can provoke strong emotional responses:\n\n1. **Inequality:** Discontent arises when expectations about equality are not met, either due to unequal treatment of similar individuals or inappropriate equal treatment of different people.\n2. **Norm Transgression:** Trigger points also emerge from behaviors that deviate from what is considered \"normal,\" such as extravagant lifestyles of the elite.\n3. **Slippery Slope:** Anxiety about society developing undesired norms and a perceived helplessness in influencing these changes, like the fear of opening borders to excessive immigration.\n4. **Behavioral Demands:** Expectations for perceived unreasonable societal demands, such as controversies surrounding the use of gender-neutral pronouns.\n\nThe definitions and mechanisms provided by Mau, Lux, and Westheuser guide this study in identifying and analyzing the impact of trigger words on social media discussions, highlighting how these words provoke emotional and divisive responses.\n\nBy leveraging textual and Natural Language Processing (NLP) tools, the paper systematically compares:\n- Specific subreddits likely to be triggered by the selected words.\n- Words with similar meanings but without triggering potential.\n- Relevant time periods for each trigger word.\n\n### Contribution:\nThis research offers empirical evidence that specific words act as triggers in social media, leading to increased engagement and animosity. It provides a systematic methodology for analyzing trigger points using a comprehensive dataset and advanced NLP techniques. The findings contribute to the broader understanding of online hate speech, polarization, and the dynamics of social media engagement.",
        "methodology": "The proposed method aims to analyze the impact of certain trigger words within social media discussions, specifically focusing on the engagement and animosity these words generate. This involves several key components and innovations:\n\n1. **Conceptual Framework**: Building upon the concept of trigger points, initially developed to understand polarisation in German society, the study applies this idea to online communication on social media. The objective is to evaluate if individual words alone can spark reactions like increased engagement and animosity.\n\n2. **Selecting Trigger Words**: The study identifies specific words, such as \"Rwanda,\" \"Brexit,\" \"NHS,\" \"Vaccine,\" and \"Feminism,\" as trigger words. Each is chosen based on its recent association with controversial topics:\n   - \"Rwanda\" related to UK immigration policy.\n   - \"Brexit\" connected to ongoing debates about the UK's departure from the EU.\n   - \"NHS\" tied to the controversies over the UK healthcare system.\n   - \"Vaccine\" focusing on COVID-19 vaccination debates.\n   - \"Feminism\" linked to online misogyny and gender-related discussions.\n\n3. **Comparative Analysis**: The study involves three main experimental setups:\n   - **Subreddit Analysis**: Trigger words are tested within specific subreddits known to incite discussions on these topics and compared with general subreddits.\n   - **Control Terms**: Each trigger word is contrasted with a similar but less provocative term to examine differences in responses. For example, \"woman\" versus \"Feminism\" and \"Red Cross\" versus \"NHS.\"\n   - **Temporal Analysis**: Evaluates the same trigger words before and after they became controversial, e.g., \"Rwanda\" in 2020 vs. 2022 and \"Vaccine\" pre- and post-COVID-19.\n\n4. **Controversiality Metric**: Utilizes Reddit's controversiality feature to measure how evenly comments are received in terms of upvotes and downvotes. The expectation is that trigger words will increase the controversiality of discussions.\n\n5. **Sentiment Analysis**: Employs the twitter-roberta-base-sentiment-latest model, fine-tuned for sentiment analysis, to classify posts as negative, neutral, or positive. It is anticipated that trigger words will lead to more negative sentiments.\n\n6. **Emotion Detection**: Uses the twitter-roberta-base-emotion-multilabel-latest model to assign emotions to posts, focusing specifically on anger. The hypothesis is that trigger words will increase expressions of anger.\n\n7. **Hate Speech Detection**: Implements the twitter-roberta-base-hate-multiclass model to detect hate speech directed at specific groups. The study concentrates on sexism when analyzing \"Feminism,\" and racism with \"Rwanda,\" while aggregating all hate speech for the other trigger words.\n\nBy employing these components and methodologies, the study seeks to systematically understand the nuanced effects of trigger words on social media discussions, providing insights into engagement patterns and the propagation of animosity in online environments.",
        "main_experiment_and_results": "### Experiment Setup:\n**Datasets:** The datasets used in the study include social media discussions encompassing threads related to topics like Rwanda, Feminism, Vaccine, NHS, and Brexit. Specific types of hate speech are analyzed, such as any kind of hate speech for Brexit, NHS, and Vaccine, and domain-specific hate speech measures for Rwanda (racism-related) and Feminism (sexism-related).\n\n**Baselines:** The experiment uses control groups to compare the effects of trigger words on controversiality, sentiment, emotion, and hate speech. The control settings are differentiated by space (threads not containing the trigger word), semantic context (threads with semantically similar discussions but without the trigger word), and temporal context (comparing discussions before and after the trigger word is introduced).\n\n**Evaluation Metrics:** \n1. **Difference-in-Differences (DiD) Estimates:** Used to measure the causal effect of trigger words by comparing treatment and control groups.\n2. **Controversiality:** Percentage of controversial threads before and after the introduction of trigger words.\n3. **Sentiment Analysis:** Focused on the presence of negative sentiment in comments.\n4. **Emotion Analysis:** Specifically looking at the presence of anger in comments.\n5. **Hate Speech Prevalence:** The volume of hateful content, especially domain-specific hate speech.\n\n### Main Experimental Results:\n#### Controversiality:\n- Introduction of trigger words generally increases the controversiality of threads.\n- No significant increase in controversial comments is observed for Feminism.\n\n#### Sentiment and Emotion:\n- There is a significant increase in negatively charged comments in 11 out of the 12 settings tested.\n- Anger similarly increases with a positive causal effect in all conditions except three non-significant cases.\n\n#### Hate Speech:\n- Trigger words do not have as prevalent an impact on hate speech as they do on anger and negative sentiment.\n- Significant effects are observed particularly in domain-specific hate speech measures for Feminism and Rwanda."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "The goal is to test whether individual words can trigger reactions that lead to increased engagement and animosity in online discussions.",
            "experiment_process": "Researchers used Reddit posts from 2020-2022 and selected trigger words related to UK politics: NHS, Brexit, Rwanda, Vaccine, and Feminist/Feminism. The posts were divided into treatment and control groups. The treatment group consisted of subreddits related to trigger words, while control groups were the rest of Reddit. The study used a difference-in-difference estimator to measure engagement (post counts) and animosity (controversiality feature, sentiment, emotion detection, and hate speech analysis). Various models such as twitter-roberta-base-sentiment-latest, twitter-roberta-base-emotion-multilabel-latest, and twitter-roberta-base-hate-multiclass were used.",
            "result_discussion": "The study found that trigger words lead to higher engagement and increased animosity. Controversiality, negative sentiments, anger, and hate speech were more pronounced in subreddits related to trigger words compared to control terms. These findings confirm the hypotheses that specific words can significantly impact user engagement and the emotional tone of online discussions.",
            "ablation_id": "2405.10213v1.No1"
        },
        {
            "research_objective": "To identify and analyze the causal effect of trigger words on online discussions by comparing controversial comments, sentiments, anger, and hate speech.",
            "experiment_process": "The methodology involves selecting words like NHS, Brexit, Rwanda, Vaccine, Feminist/Feminism based on their relevance to public debates. Using Pushshift API, data from Reddit comments (2019-2022) matching these keywords were collected. The analysis focused on discussions 30 minutes before and after the mention of trigger words. Controlled terms were also used for comparison. Sentiment analysis, detection of anger and hate speech were performed using models such as twitter-roberta-base-emotion-multilabel-latest and twitter-roberta-base-hate-multiclass.",
            "result_discussion": "The outcome indicated that posts containing trigger words had higher controversiality, more negative sentiments, increased expressions of anger, and a higher incidence of hate speech. This suggests that the chosen trigger words effectively provoke stronger and more negative reactions in online discussions.",
            "ablation_id": "2405.10213v1.No2"
        }
    ]
}