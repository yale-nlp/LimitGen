{
    "title": "AraPoemBERT: A Pretrained Language Model for Arabic Poetry Analysis",
    "abstract": "Arabic poetry, with its rich linguistic features and profound cultural significance, presents a unique challenge to the Natural Language Processing (NLP) field. The complexity of its structure and context necessitates advanced computational models for accurate analysis. In this paper, we introduce AraPoemBERT, an Arabic language model pretrained exclusively on Arabic poetry text. To demonstrate the effectiveness of the proposed model, we compared AraPoemBERT with 5 different Arabic language models on various NLP tasks related to Arabic poetry. The new model outperformed all other models and achieved state-of-the-art results in most of the downstream tasks. AraPoemBERT achieved unprecedented accuracy in two out of three novel tasks: poet\u2019s gender classification (99.34% accuracy), and poetry sub-meter classification (97.79% accuracy). In addition, the model achieved an accuracy score in poems\u2019 rhyme classification (97.73% accuracy) which is almost equivalent to the best score reported in this study. Moreover, the proposed model significantly outperformed previous work and other comparative models in the tasks of poems\u2019 sentiment analysis, achieving an accuracy of 78.95%, and poetry meter classification (99.03% accuracy), while significantly expanding the scope of these two problems. The results demonstrate the effectiveness of the proposed model in understanding and analyzing Arabic poetry, achieving state-of-the-art results in several tasks and outperforming previous works and other language models included in the study. AraPoemBERT model is publicly available on https://huggingface.co/faisalq.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The Arabic language is one of the world\u2019s most widely spoken languages. It has a rich history and its influence is seen across various domains including media, politics, history, and art. Arabic poetry, a prominent part of Arabic literature and culture, serves as a window to the norms, values, and historical events of the Arab world. An Arabic poem typically consists of one or more verses, each verse usually composed of two halves known as hemistiches. All verses in a specific poem share the same rhyme and meter, creating a rhythmic pattern that adds to the beauty and depth of the poem. The rhyme is a repeating pattern of sounds that occurs at the end of each verse, while the meter dictates the rhythmic structure of the verse. Arabic poetry spans a wide range of topics, encapsulating the poet\u2019s genuine emotions and thoughts. These topics can range from romance and longing to spiritual devotion, each offering a unique perspective and depth of emotion.\n\nClassical Arabic poetry adheres to a set of established meters, each with its unique rhythmic pattern. These meters are very important as they provide the poem with its rhythmic structure and flow. The science of Arabic prosody involves the use of Tafaeil or poetic feet. These are groups of ten expressions that scholars have agreed upon as the standard for weighing Arabic poetry. The ten feet in Arabic prosody are composed of specific letters: Faa, Ain, Lam, Noon, Meem, Seen, Taa, and vowels. These feet correspond in weight to the letters of the measured words in the poem verse, matching vowels with vowels and consonants with consonants. However, modifications to these poetic feet can occur, altering their ideal image. These modifications can involve omitting, adding, or silencing parts of the feet. Scholars of \u2019Arud have differing views on these changes, with some approving and others disapproving.\n\nTypically, the meters used in Arabic poems are in their complete form, meaning all the original Tafaeil of a specific meter are used, except for a few meters that must be in a fragmented form. However, poets sometimes omit parts of the Tafaeil from the original meter, resulting in a variant of that meter. Scholars of \u2019Arud have identified seven different variants that could be derived from classical meters. Whereas some meters always appear with a certain variant such as \u2019Complete\u2019, while others may come in more than one variant. In this study, we will refer to the combination of meters and their existing variants as \u2019sub-meters\u2019, which include the name of the meter and its variant.\n\nIn contrast to classical meters, non-classical meters allow for more flexibility and diversity in constructing the poem. The usage of these patterns adds to the beauty of Arabic poetry, making it an enjoyable experience, either when reading silently or reciting out loud. Non-classical meters appeared chronologically after the classical meters and are often featured in Nabati poetry, which translates to folk poetry. The rise of these meters is closely associated with the prevalence of colloquial speech. The names and numbers of these meters vary among scholars, and their classification is often influenced by the era and region under consideration. However, identifying poems\u2019 meters manually poses a challenge. It necessitates an understanding of the language and its scientific study of \u2019Arud, as well as a keen ear for recognizing rhythm and sound patterns. This task becomes more demanding when dealing with non-classical rhythmic patterns (meters) that are more flexible and sometimes do not adhere to specific rules.\n\nThe goal of conducting a thorough analysis or solving different problems related to Arabic poetry has led to the development of various methods and techniques. One promising solution is the use of language models that can analyze and learn from text data. By pretraining a language model on a dataset of poems and fine-tuning it using verses text and their corresponding labels, we can create a system capable of accurately identifying the meter or rhyme of a given poem or a verse. This approach not only saves time and effort but also opens up new possibilities for analyzing and classifying Arabic poetry. In this paper, we present AraPoemBERT, a new BERT-based language model pretrained from scratch exclusively on Arabic poetry text. We provided a comprehensive evaluation of its performance in comparison with other Arabic language models on five different tasks related to Arabic poetry. We believe that AraPoemBERT has potential for the future of Arabic poetry analysis, serving as a valuable tool for scholars and researchers in linguistic, Arabic literature, and natural language processing (NLP) fields.\n\nThe main contributions of this paper can be summarized as follows:\n- Presenting a new language model pretrained from scratch, dedicated solely to Arabic poetry.\n- Reporting state-of-the-art results in 4 out of 5 different NLP tasks related to Arabic poetry using the proposed model compared to previous work and other prominent language models.\n- We are the first to explore and report the results for 3 new tasks: poet\u2019s gender, poetry sub-meters, and poetry"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Arabic Poetry Analysis",
            "text": "In recent years, the natural language processing (NLP) research related to Arabic poetry has focused mainly on two tasks: poems meters classification, and poems sentiment analysis using machine learning models. Several researchers in the past have proposed rule-based algorithms aiming to classify poetry meters. These approaches convert the input text into its Arudi form using regular expressions or Khashan\u2019s \u201dnumerical prosody\u201d method, and subsequently they determine the meter of the target verse or poem. These systems heavily rely on diacritizing the input text and necessitate an understanding of the Arudi field to create effective rules to be used in these systems.\n\nBerkani et al. suggested a pattern recognition extraction and matching approach for poems meter detection. This method involves extracting a group of patterns from a target verse and comparing them to a set of labeled patterns. If the extracted pattern matches any of the labeled ones, the system can identify the meter of the input verse. The reported accuracy of this approach reached 99.3% when tested on a dataset consisting of 2,711 verses. However, it should be noted that common poets\u2019 practices, such as text vocalization or minor imperfections in the poem, can potentially impact the system\u2019s accuracy.\n\nSimilarly, Shaibani et al. proposed a novel approach utilizing five bidirectional gated recurrent unit (BiGRU) layers, and used character-based encoding for text representation. The researchers collected a set of poems comprising 55,440 verses categorized into 14 meters only, and achieved an overall accuracy of 94.32%.\n\nSimilarly, Abdandah et al. introduced a new machine learning model that contains four bidirectional long short-term memory (BiLSTM) layers.\n\nSimilar to the attempts at classifying poem meters, numerous methods have been suggested in literature towards poems sentiment analysis. Mohammad presented a Naive Bayes approach for classifying poems into seven different categories Hekmah (Wisdom), Retha (Elegy poems), Ghazal (Spinning poems), Madeh (Praise), Heja (Satire), Wasef (Description poems), Fakher (self-glorification), and Naseeb (contentment) using 20 Arabic poems with six verses each, and had achieved an accuracy of 55%.\n\nAlsharif et al. classified Arabic poems into four classes: Retha (Elegy poems), Ghazal (Spinning poems), Fakhr (self-glorification), and Heja (Satire) using Naive Bayes and support vector machine (SVM) models. They used a dataset composed of 1231 poems comprising 20041 verses, and they have achieved an F1-score of 0.66 as the highest result reported in their work.\n\nSimilarly, Ahmed et al. proposed three machine learning models for classifying Arabic poems into 4 types: love, Islamic, political, and social. They have used Naive Bayes, SVM, and linear support vector classifier (SVC) and had achieved an average F1-score of 0.49, 0.18, and 0.51 respectively.\n\nShahriar et al. measured the performance of different deep learning models like LSTM, GRU, and CNN in the task of classifying Arabic poetry emotions. They have used 9452 poems divided into 3 classes: joy, sadness, and love. Additionally, the authors employed AraBERT model, a BERT-based model that was pretrained on Arabic text. The fine-tuned model has achieved an F1-score of 0.77 which is significantly higher compared to the other deep learning models used in the same study that have achieved an F1-score between 0.53 and 0.62."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Arabic Language Models",
            "text": "Looking specifically at Arabic language models, multiple models have been proposed in literature since the introduction of Transformers by Vaswani et al. AraBERT, introduced by Antoun et al., is a BERT-based language model that was pretrained on a large Arabic language corpus. The 24 GB dataset is composed of Arabic news articles obtained from two publicly available corpora: 1.5 billion words Arabic corpus, and OSIAN: the Open Source International Arabic News Corpus. Additionally, the authors scraped manually more news articles from various online sources. The main reason for creating AraBERT was the need for a large language model designed specifically for the Arabic language. The authors presented two variants of AraBERT: AraBERTv0.1 and AraBERTv1. The main difference between the two is that in AraBERTv1 the authors used an Arabic text segmenter before training the tokenizer and then tokenizing the text. Whereas in AraBERTv0.1, the Farasa segmenter was not used. When evaluating both models, AraBERTv1 outperformed AraBERTv0.1 on six different tasks, whereas the latter achieved higher results in the remaining three tasks. Each of these variants are available in two different sizes: \"base\" and \"large\", similar to the original BERT\u2019s two sizes. AraBERT has shown impressive performance on various Arabic NLP tasks, outperforming other multilingual models that were pretrained on multiple languages including Arabic. The model has achieved state-of-the-art (SOTA) results when tested on Arabic NLP tasks such as text classification, named entity recognition, and question answering.\n\nSimilarly, Chowdhury et al. proposed another BERT-based language model called QARiB. The presented model was pretrained on text acquired from different sources including posts from Arabic news channels written in modern standard Arabic (MSA), and tweets from well-known Twitter accounts that are written mostly in dialect Arabic. The QARiB model has achieved higher results than AraBERT in text classification tasks on newly prepared datasets containing some text written in dialectal Arabic which shows that language models can achieve better generalization when being trained on both formal and informal text.\n\nAbdul-Mageed et al. introduced a new BERT-based model called ARBERT pretrained on 61GB of MSA text collected from Arabic Wikipedia, online free books, and publicly available corpora, mainly OSCAR. The model employs a vocabulary of 100K different tokens, and was pretrained using the same configuration as the original BERT. The authors also introduced another model, MARBERT, that was pretrained on a different dataset composed only of tweets written in both MSA and diverse Arabic dialects. The model is designed for downstream tasks that involve dialectal Arabic. However, in this study, we excluded MARBERT because most of Arabic poetry is written in classical or standard Arabic. Both models, ARBERT and MARBERT, achieved SOTA results across the majority of tasks when compared with AraBERT and other multilingual models.\n\nInoue et al. proposed a new model under the name CAMeLBERT. In their paper, they developed four different variants of CAMeLBERT: MSA, dialectal Arabic (DA), classical Arabic (CA), and mix. Each variant was pretrained on different datasets that contain a certain type of Arabic text, except for the mix variant that was pretrained on all datasets combined (167 GB). The authors compared the new model variants with AraBERT, MARBERT, ARBERT, and other multilingual models. They showed when experimenting on tasks that involve dialectal Arabic, CAMeLBERT-DA outperformed all other models including MARBERT. Additionally, CAMeLBERT-CA outperformed all other models in the Arabic poetry task, which is the only task that is designed for evaluating language models on classical Arabic text.\n\nIn this study, we used AraBERTv1, AraBERTv0.1, ARBERT, and QARiB as comparative models to our proposed model due to their wide acceptance in literature when tackling various Arabic NLP problems. Also, we employed CAMeLBERT-CA model in this study for being mainly designed to tackle tasks that involve classical Arabic text."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Transformers",
            "text": "Transformers, introduced by Vaswani et al. [22  ###reference_b22###], are the building block for all modern language models. They primarily utilize a mechanism called self-attention to measure the significance of different parts in the input sequence to each other. Figure 1  ###reference_### shows the general architecture of Transformer model.\nGiven a sequence of input vectors, the self-attention mechanism computes a weighted sum of these vectors using attention scores. The core components of the self-attention mechanism are the query (Q), key (K), and value (V) matrices. These are derived from the input vectors.\n###figure_3### The attention scores are computed as:\nwhere  is the dimension of the key vectors.\nWhile the self-attention mechanism allows the model to focus on different parts of the input, the multi-head attention mechanism allows the model to focus on different parts in different representation subspaces of Q, K, and V matrices. Essentially, it runs the self-attention mechanism multiple times in parallel, each with different learned linear projections of the original Q, K, and V.\nGiven  different sets of Q, K, and V matrices, the multi-head attention is computed as:\nwhere each head is computed as:\nand , , , and  are the parameter matrices.\nHowever, since Transformers inherently lack a sense of order or position, the authors also proposed another mechanism called \u201dpositional encoding\u201d that can give the model information about the position of words in a sequence, since all words or tokens are being processed in parallel. To address this, positional encodings are added to the embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension as the input embeddings, allowing them to be summed. The word\u2019s position  and each dimension  of the word embedding, the positional encoding is defined as:\nWhere  is the dimension of the embeddings. These sinusoidal functions were chosen because they can be easily learned if needed, and they allow the model to interpolate positions of words in long sequences.\nThe original Transformer model follows the encoder-decoder structure, where the encoder processes the input sequence and the decoder generates the output sequence. This architecture makes the original Transformer model particularly suitable for text-to-text tasks such as machine translation and paraphrase generation.\nBuilding upon the Transformer\u2019s architecture, the Bidirectional Encoder Representations from Transformers (BERT) has brought about significant advancements in natural language processing (NLP) [31  ###reference_b31###]. BERT is an encoder-only Transformer that analyzes and processes input text bidirectionally, unlike the original encoder-decoder Transformer model that reads text sequentially. One of BERT\u2019s capabilities is the ability to grasp the complete context of a word by considering its surrounding words. This is achieved through the \u201dmasked language model\u201d (MLM) training objective, which randomly masks a percentage of input tokens and then asks the model to predict them based on the context provided by the other unmasked tokens. Figure 2  ###reference_### shows an example of the MLM training objective. This bidirectional approach allows BERT to accurately comprehend the context and the meaning of each word in a sentence, especially when dealing with words that have different meanings based on their usage and surrounding words.\n###figure_4### Additionally, BERT excels in transfer learning, which enables it to apply previously learned knowledge to different NLP tasks. Once the model has been pretrained using a large amount of text, it can be further refined by adding just one extra output layer. This allows for generating models for tasks including question answering and language inference without the need for significant modifications to the model architecture, or the need for re-traing the model for scratch. This adaptability makes BERT highly versatile and efficient, ensuring high performance across a wide range of NLP tasks. Figure 3  ###reference_### shows the pretraining process of BERT language model.\nThe authors of BERT model have developed two different sizes of BERT: BERT-base and BERT-large. BERT-base is a model with 12 transformer blocks (layers), 768 hidden units (output vector size), and 12 attention heads for each layer, resulting in a total of 110 million parameters. BERT-large is a much larger model with 24 layers, 1024 hidden units, and 16 attention heads. It contains a total of 340 million parameters, which is 3 times larger than the base model. Both variants have been pretrained on the same dataset, but due to its larger size and complexity, BERT-large generally achieves better performance on different NLP tasks. However, it requires more computational resources and longer training time.\n###figure_5###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Proposed Model: AraPoemBERT",
            "text": "In this section we describe the proposed model architecture, and the dataset used in the study."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Model Architecture",
            "text": "Building upon the success of AraBERT and many other Arabic language models, we developed AraPoemBERT, a BERT-based model that was pretrained from scratch exclusively on Arabic poetry text. The model follows the same architecture as the original BERT model in terms of the number of attention heads (12 attention heads per layer) and the size of the hidden layer (768 units). Also, we used wordpiece tokenizer [32  ###reference_b32###] similar to the original BERT model. However, AraPoemBERT contains 10 encoder layers, compared to 12 layers in BERT-base, and the vocabulary size of our model was set to 50,000, allowing it to capture a wide range of words and expressions found in Arabic poetry. Finally, the maximum sequence length is set to 32 tokens per sequence. The main reason for limiting the sequence length in AraPoemBERT to such a small number is due to the average length of poems\u2019 verses, where the majority of verses can be fully stored within a 32-token sequence. Figure 4  ###reference_### shows that 99.3% of sequences (a whole verse) contain between 6 and 18 tokens after tokenization. However, having a smaller sequence length does significantly reduce the model pretraining time, because it enables using larger batch sizes without causing any out-of-memory issues, even with a commodity GPU.\n###figure_6###"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Dataset",
            "text": "In this study, we used AraPoems555AraPoems dataset is available on https://doi.org/10.7910/DVN/PJPWOY  ###reference_### dataset for pretraining AraPoemBERT and fine-tuning different models used in the downstream tasks. The dataset is collected from two online sources specialized in Arabic poetry, Almausua [33  ###reference_b33###] and Aldiwan [34  ###reference_b34###], and it contains 2,090,907 verses associated with a variety of information such as meter, sub-meter, poet, rhyme, era, and topic. See Figure 5  ###reference_### for a sample of the dataset. Figure 6  ###reference_### shows the distribution of verses across different categories. Compared to the APCD dataset [11  ###reference_b11###], the new compiled dataset contains 14% more verses, and contains two new labels: sub-meter and the type of the poem\u2019s topic. Additionally, we translated all these information to the English language manually, and we also labeled the poets\u2019 gender based on their names.\nThe dataset underwent a cleaning process which includes removing duplicate verses, and removing any irrelevant characters from the corpus such as digits, English letters, and unwanted symbols like \u2018@\u2019, \u2018#\u2019, and \u2018$\u2019.\n###figure_7### ###figure_8###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments and Results",
            "text": "In this section, we present the text preprocessing steps, the pretraining procedures, and the downstream tasks along with the results. All experiments were conducted on a local machine equipped with AMD Ryzen-9 7950x processor, 64GB DDR5 memory, and two GeForce RTX 4090 GPUs with 24GB memory each. The software environment was set up on Ubuntu 22.04 operating system. The Huggingface transformers library was used in pretraining our model, in addition to downloading and fine-tuning the language models from the Huggingface hub that were used in this study. Additionally, CUDA 11.8 was used to take advantage of GPU acceleration for efficient computations."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Text Preprocessing",
            "text": "Before using the verses\u2019 text in pretraining the model, a few preprocessing steps were required to ensure the data is in a suitable format for the model. The first step in the preprocessing phase was to remove the diacritics, which simplifies the text and reduces the pretraining time. Removing the diacritics from the text was conducted using PyArabic, a specialized Python library for manipulating and normalizing Arabic text. The second step involves removing all symbols such as colons, brackets, and question marks from the poetry corpus. These symbols can introduce noise into the data and potentially affect the performance of the model. Moreover, due to the difference between the structure of regular text and Arabic poetry, where the verses are divided into two parts, known as hemistiches, and both hemistiches are required to form a complete sentence. And at the same time, we want to facilitate the model\u2019s understanding of the verse structure. Thus, we added two additional unique tokens: \u2019[s]\u2019 and \u2019[e]\u2019, where the \u2019[s]\u2019 token is used as a separator between the first and second hemistiches in a verse, and if a verse contains only the first hemistich, the \u2019[e]\u2019 token will be placed after the \u2019[s]\u2019 token to represent an empty second hemistich. This approach allows the model to recognize the structure of the verses and differentiate between verses with one or two hemistiches. The final step in the preprocessing phase involves training the tokenizer and then using it whenever required to tokenize the input text. For AraPoemBERT, we employed the Huggingface implementation of the WordPiece tokenizer, which we trained on the same poetry text with a vocabulary size of 50,000 wordpieces."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Model Pretraining",
            "text": "The original BERT model was pretrained on two objective tasks: the masked language model (MLM), and next sentence prediction (NSP). In the MLM task, a certain percentage of tokens in the input sequence are masked and the model\u2019s goal is to correctly predict what these masked tokens are. The NSP task involves providing the model with two sentences and asking it to determine if they are related (from the same paragraph) or not. However, AraPoemBERT was pretrained solely on the MLM task objective which reduces the pretraining time and potentially achieves better performance in the downstream tasks, following the recommendations of the RoBERTa model\u2019s authors [37]. Our model was pretrained by masking 15% of the sequences\u2019 tokens, a batch size of 256, \u2019AdamW\u2019 [38] as the model optimizer with a learning rate of 5e-5 and weight decay equal to zero, and a dropout rate of 0.1 for all dropout layers. To reduce training time and optimize GPU memory usage, we utilized the (mixed precision) datatype \u201dFP16\u201d for gradient computations. With the aforementioned configurations, the model was pretrained for 800k steps (980 epochs) and it took 142 GPU hours, whereas the minimum loss reached was 2.02. In this stage, we have used all the collected poetry text which is composed of more than 2.09 million verses. The size of the text file used in pretraining the model was 182 MB and contains more than 19.22 million words or 29 million tokens after tokenization. Even though the dataset is small in size compared to other BERT-based models, Arabic poems are very distinctive and diverse, and language models in general require between 10M and 100M words to learn most of the syntactic and semantic features [39]."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Downstream Tasks",
            "text": "To demonstrate the effectiveness of AraPoemBERT model, we assessed its performance on five different downstream tasks related to Arabic poetry analysis. In this study, we used AraBERTv1, AraBERTv0.1, ARBERT, CAMeLBERT-CA, and QARiB as comparative models to our proposed model. All language models were fine-tuned using the same settings and hyperparameters used in the pretraining process. The models were fine-tuned using 80% of the task-related labeled data, and the remaining 20% were reserved for validation, while ensuring that the same validation set was used when evaluating different models within the same experiment."
        },
        {
            "section_id": "5.3.1",
            "parent_section_id": "5.3",
            "section_name": "5.3.1 Sentiment Analysis",
            "text": "Many poems in the dataset are labeled with different types of topics such as: Romantic poems, Elegy poems, Invocation poems, etc. However, we grouped relevant types of poems into a single emotion, resulting in four different classes: Love, Sadness, Anger, and Spirituality. The resulting dataset labeled with these four emotions contains 21,230 poems composed of 315,877 verses.\n\nAs shown in Table 5, our model has outperformed all other language models with an accuracy score of 78.95% which is significantly higher than the second score achieved by CAMeLBERT-CA model, while the lowest accuracy results are achieved by both AraBERT models (75.35% and 75.79%).\n\nTable 6 presents the classification scores achieved by AraPoemBERT for each sentiment class in the validation set. The model attained high scores in the classes \"Love\" and \"Spirituality,\" while it wrongly classifies \"Sadness\" and \"Anger\" with the class \"Love\" as shown in the confusion matrix, which explains the low F1-score for these two classes. Overall, AraPoemBERT has achieved significantly higher results compared to previous work by Shahriar et al. In their work, they targeted three emotion classes only, and achieved an accuracy of 76.5% with a dataset composed of 9452 poems, which is less than half the number of poems used in this study. Additionally, in their work, the proposed models require processing whole poems to accurately detect the main sentiment. However, in this study, we achieved higher accuracy by merely evaluating individual verses to predict the related poem\u2019s main sentiment, which shows the significance of language models pretrained on domain-specific texts such as AraPoemBERT, or CAMeLBERT-CA which was pretrained solely on classical Arabic text."
        },
        {
            "section_id": "5.3.2",
            "parent_section_id": "5.3",
            "section_name": "5.3.2 Poetry Meters",
            "text": "The task of classifying Arabic poetry meters has proven to be quite challenging in literature, especially when tackling a dataset composed of a large number of verses, and at the same time, multiple meters are underrepresented in the dataset. We have conducted two classification tasks; one focused on classical meters only (16 meters) to compare our model accuracy with existing work, and the second classification task includes both classical and non-classical meters (28 meters). Table 7 shows a list of classical and non-classical meters found in the AraPoems dataset.\n\nIn Table 8, we compare our model with other language model and machine learning models presented in literature. AraPoemBERT has achieved the highest accuracy score in both tasks outperforming other models, including proposed approaches from previous work. Regarding the classification task that targets all poetry meters, which includes an additional 12 non-classical meters, we compared our model with other language models only, because to the best of our knowledge there were no published results in literature that cover this area.\n\nTable 9 shows the prediction results of our model in classifying the 16 classical meters. These results include precision, recall, and F1 scores for each class, in addition to the number of verses for each meter in the validation set. The model has successfully achieved an accuracy of 99% in six different meters (1.Taweel, 2.Kamel, 3.Baseet, 4.Khafif, 5.Wafer, and 8.Mutaqarib), setting a new state-of-the-art result that has not been reported in previous work except for the \u2019Taweel\u2019 meter only. The model also achieved an accuracy score of 97% and 98% in another five meters (6.Rajaz, 7.Ramel, 9.Saree, 10.Munsarih, and 11.Mujtath) despite the fact that the total number of verses related to \u2019Munsarih\u2019 and \u2019Mujtath\u2019 meters are relatively small compared to other meters. Similarly, the number of samples for \u201913.Madeed\u2019 and \u201914.Mutadarak\u2019 meters are also small but the model has achieved an accuracy score of 93.83% and 94.41% respectively. However, the accuracy scores of \u201912.Hazaj\u2019 and \u201915.Muqtadab\u2019 are a little lower, because the model wrongly classifies 15% of \u2019Hazaj\u2019 verses as \u2019Rajaz\u2019, and 10% \u2019Muqtadab\u2019 verses as \u2019Khafif\u2019 as shown in the confusion matrix in Figure 9. Lastly, the accuracy score of \u201916.Mudari\u2019 meter is the lowest; this is due to the small number of verses for this meter, and the model wrongly classified 55.6% of this meter\u2019s samples as \u2019Rajaz\u2019 as shown in the confusion matrix.\n\nThe prediction results of AraPoemBERT in classifying all meters (16 classical and 12 non-classical) are shown in Table 10. The model has achieved similar accuracy scores for the classical meters even when including the other meters in the classification task. Notably, the model achieved a score of 93.92% with the (20.Doubeet) meter, and an accuracy score between 70% and 85% in \u201917.Muashah\u2019, \u201918.Free form\u2019, and \u201919.Colloquial\u2019 meters. However, the model achieved an accuracy score between 49% and 61% in the meters \u201921.Mawalia\u2019, \u201922.Masehube\u2019, and \u201923.Selselah\u2019 because of the small number of samples for these meters, and the model wrongly classifies them as different meters such as \u2019Muashah\u2019, \u2019Colloquial\u2019, and \u2019Kamel\u2019 as shown in the confusion matrix in Figure 10. Finally, the model couldn\u2019t detect and correctly classify any of the last five meters (24.Zajal, 25.Kankan, 26.Hajini, 27.Sakhri, and 28.Luaihani) because the number of samples for each of these meters is less than 100 verses and detecting them by the model currently seems unfeasible."
        },
        {
            "section_id": "5.3.3",
            "parent_section_id": "5.3",
            "section_name": "5.3.3 Poetry Sub-Meters",
            "text": "To further expand the problem of classifying classical meters, we include what is called meter\u2019s variants or sub-meters to the poetry meters classification task. The majority of verses in the newly compiled dataset are labeled with a specific meter and a meter\u2019s variant. However, in order to reduce the classification problem complexity, we combined both meters and their variants to form a new set of labels. For example, \u201dKhafif\u201d meter comes in two variations: \u201dComplete\u201d and \u201dMajzuu\u201d, thus, their combination will result in two different classes: \u201dKhafif Complete\u201d and \u201dKhafif Majzuu\u201d. In this study, we will refer to the combination of meters and their variants as \u2019sub-meters\u2019. After combining the meters and their variants into combined classes, we ended up with a total of 33 different sub-meters. However, we excluded sub-meters classes from the classification task if they contain about 100 verses or less, which resulted in the removal of seven sub-meters from the experiment. The remaining 25 sub-meters, which account for approximately 88.48% of all verses in the dataset, will be the focus of the classification task.\n\nThe classification report of the validation set for AraPoemBERT shows that the model has achieved an F1-score over 0.98 in all classes with the \u2019Complete\u2019 variant, except for \u2019Mutadarak Complete\u2019 and \u2019Rajaz Complete\u2019 due to the low number of samples for these two sub-meters. Also, the F1-score was below 0.4 for the sub-meters that contain a low number of verses that are less than 1200 in the validation set. Figure 2 shows the confusion matrix of the validation set using AraPoemBERT in the sub-meters classification task. To the best of our knowledge, this is the first study in literature that directly focuses on the problem of classifying meters variants. However, compared to the results of classifying classical meters in previous work, the accuracy of classifying sub-meters using AraPoemBERT is better than the results reported by Abandah et al. (97.79% VS 97.27%) even after increasing the complexity of the problem, but significantly lower when compared with the best accuracy score reported in the previous task when plainly classifying classical meters without any consideration to their variants (97.79% VS 99.03%)."
        },
        {
            "section_id": "5.3.4",
            "parent_section_id": "5.3",
            "section_name": "5.3.4 Poet\u2019s Gender",
            "text": "The dataset originally did not contain any information regarding poets\u2019 gender. Thus, we manually annotated the poets\u2019 gender based on their names. The dataset contains a total of 5,383 poets, of which 5,023 are males and 360 poets are females. Almost all verses in the dataset, specifically 2,087,557 verses, are associated with known poets. Table 13 shows the overall accuracy results for all the models. Table 15 presents the classification report for AraPoemBERT, which shows that the model has achieved an F1-score of 0.2246 for the Female class even though it is extremely underrepresented compared to the Male class, and 99.12% weighted average accuracy for both classes."
        },
        {
            "section_id": "5.3.5",
            "parent_section_id": "5.3",
            "section_name": "5.3.5 Poem\u2019s Rhyme",
            "text": "In the task of rhyme classification, the verses in the dataset are labeled with 31 different rhymes. These rhymes include all 28 Arabic letters, in addition to the rhymes: Laa, Taa Marbutah, and Waw Hamza which are variants of the letters Lam, Taa, and Alif, respectively, but they are written differently and have slightly different sounds. This classification task aims to accurately identify the rhyme of each verse, providing further insight into the structure and style of the poem.\n\nIn this task, all models have achieved similar results. CAMelBERT-CA scored the highest with an accuracy of 97.76%, and AraPoemBERT achieved an accuracy of 97.73%. Figure 11 presents the confusion matrix for the validation set prediction results of AraPoemBERT which shows that the model can accurately identify the rhymes if it is one of the original 28 Arabic letters. The remaining three rhymes (Laa, Taa Marbutah, and Waw Hamza) which are variants of the letters (Lam, Taa, and Alif), are where the model scores the lowest. This is due to multiple reasons. For instance, the number of samples for these rhymes is very small especially for \u201dTaa Marbutah\u201d and \u201dWaw Hamza\u201d rhymes. Also, the model wrongly classifies \u201dLaa\u201d as \u201dLam\u201d for 24.5% of the samples."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this study, we presented AraPoemBERT, a new BERT-based language model pretrained from scratch on Arabic poetry text. In addition, we employed the proposed model along with the other Arabic language model on five different NLP tasks related to Arabic poetry. The target tasks include classifying poets\u2019 genders, classifying poetry meters and sub-meters, sentiment analysis, and detecting verses\u2019 rhymes. The presented results illustrate the effectiveness of utilizing transformer-based models in various tasks related to Arabic poetry, and the significance of using a domain-specific language model such as AraPoemBERT that was exclusively pretrained on poetry text compared to language models pretrained on general text such as AraBERT and CAMeLBERT. The model has achieved state-of-the-art results and outperformed the other language models in most of the tasks. Also, we have explored three new NLP tasks in Arabic poetry that have not been explored in literature before: classifying poets\u2019 genders, classifying sub-meters, and detecting verses\u2019 rhymes. The results achieved in these tasks will serve as a benchmark for future work. Additionally, more NLP tasks related to Arabic poetry should be explored, such as authorship attribution, era classification, automating the process of poem text diacritization, and distinguishing between poems written in standard or spoken Arabic. The dataset and the language model introduced in this paper will serve as valuable resources for future work in different domains and fields such as linguistics, artificial intelligence, Arabic literature, language processing, and cultural studies."
        }
    ],
    "appendix": [],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S1.T2\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>List of classical meters in Arabic poetry.</figcaption>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>List of basic feet in Arabic poetry (Tafaeil).</figcaption><div class=\"ltx_flex_figure ltx_flex_table\">\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<table class=\"ltx_tabular ltx_centering ltx_flex_size_2 ltx_align_middle\" id=\"S1.T2.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S1.T2.1.1\">\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T2.1.1.1\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_portrait\" height=\"418\" id=\"S1.T2.1.1.1.g1\" src=\"extracted/5480039/table1.png\" width=\"284\"/></td>\n</tr>\n</tbody>\n</table>\n</div>\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<table class=\"ltx_tabular ltx_centering ltx_flex_size_2 ltx_align_middle\" id=\"S1.T2.2\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S1.T2.2.1\">\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T2.2.1.1\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_square\" height=\"281\" id=\"S1.T2.2.1.1.g1\" src=\"extracted/5480039/table2.png\" width=\"308\"/></td>\n</tr>\n</tbody>\n</table>\n</div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>List of basic feet in Arabic poetry (Tafaeil).</figcaption>\n</figure>",
            "capture": "Table 1: List of classical meters in Arabic poetry."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S1.T3\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>List of meter variants.\u00a0</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S1.T3.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S1.T3.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"S1.T3.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S1.T3.1.1.1.1.1\">Variant Name</span></th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S1.T3.1.1.1.2\" style=\"width:142.3pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S1.T3.1.1.1.2.1\">Description</span></th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S1.T3.1.1.1.3\" style=\"width:199.2pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S1.T3.1.1.1.3.1\">Meters</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S1.T3.1.2.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"S1.T3.1.2.1.1\">Complete</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"S1.T3.1.2.1.2\" style=\"width:142.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S1.T3.1.2.1.2.1\">Has fulfilled all its original meter\u2019s feet</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"S1.T3.1.2.1.3\" style=\"width:199.2pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S1.T3.1.2.1.3.1\">All meters can be in its complete meter form except: Madeed, Mudari, Hajaz, Muqtadab, and Mujtath</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T3.1.3.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"S1.T3.1.3.2.1\">Majzuu</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"S1.T3.1.3.2.2\" style=\"width:142.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S1.T3.1.3.2.2.1\">The last foot in each hemistich is omitted</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"S1.T3.1.3.2.3\" style=\"width:199.2pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S1.T3.1.3.2.3.1\">Mandatory in: Madeed, Mudari, Hajaz, Muqtadab, and Mujtath. Also possible in all other meters except: Taweel, Munsarih, and Saree</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T3.1.4.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"S1.T3.1.4.3.1\">Mashture</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"S1.T3.1.4.3.2\" style=\"width:142.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S1.T3.1.4.3.2.1\">Dropping half the feet in both hemistiches</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"S1.T3.1.4.3.3\" style=\"width:199.2pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S1.T3.1.4.3.3.1\">Can occur in Rajaz, Saree, and rarely in Baseet</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T3.1.5.4\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"S1.T3.1.5.4.1\">Manhuk</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"S1.T3.1.5.4.2\" style=\"width:142.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S1.T3.1.5.4.2.1\">Dropping two thirds the feet in both hemistiches</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"S1.T3.1.5.4.3\" style=\"width:199.2pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S1.T3.1.5.4.3.1\">Can occur in Rajaz, Munsarih, and Mutadarak</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T3.1.6.5\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"S1.T3.1.6.5.1\">Maktuu</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"S1.T3.1.6.5.2\" style=\"width:142.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S1.T3.1.6.5.2.1\">Omitting the last syllable in the last foot and silencing what is before</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"S1.T3.1.6.5.3\" style=\"width:199.2pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S1.T3.1.6.5.3.1\">Can only occur in Kamel meter</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T3.1.7.6\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"S1.T3.1.7.6.1\">Ahuth</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"S1.T3.1.7.6.2\" style=\"width:142.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S1.T3.1.7.6.2.1\">Only omitting the last half in the last foot</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"S1.T3.1.7.6.3\" style=\"width:199.2pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S1.T3.1.7.6.3.1\">Can only occur in Kamel meter</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T3.1.8.7\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" id=\"S1.T3.1.8.7.1\">Mukhala</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t\" id=\"S1.T3.1.8.7.2\" style=\"width:142.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S1.T3.1.8.7.2.1\">A variation of Majzuu</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t\" id=\"S1.T3.1.8.7.3\" style=\"width:199.2pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S1.T3.1.8.7.3.1\">Can only occur in Baseet meter</p>\n</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 3: List of meter variants.\u00a0"
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T4\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>Poems types and emotions.\u00a0</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S5.T4.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" id=\"S5.T4.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.1.1.1\">Poem Types</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\" id=\"S5.T4.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.1.2.1\">Sentiment</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T4.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.1.3.1\">#Verses</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T4.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.1.4.1\">#Poems</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T4.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T4.1.2.1.1\">Slander Poems</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" id=\"S5.T4.1.2.1.2\">Anger</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.2.1.3\">5079</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.2.1.4\">631</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T4.1.3.2.1\">Romantic, Parting, Longing, and Spinning Poems</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S5.T4.1.3.2.2\">Love</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.3.2.3\">226368</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.3.2.4\">15228</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T4.1.4.3.1\">Religious, Invocation, and Mercy Poems</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S5.T4.1.4.3.2\">Spirituality</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.4.3.3\">43429</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.4.3.4\">2275</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T4.1.5.4.1\">Elegy Poems</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S5.T4.1.5.4.2\">Sadness</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.5.4.3\">41001</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.5.4.4\">3172</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.6.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\" id=\"S5.T4.1.6.5.1\">Total</th>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_b\" id=\"S5.T4.1.6.5.2\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T4.1.6.5.3\">315877</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T4.1.6.5.4\">21230</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 4: Poems types and emotions.\u00a0"
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T5\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>The accuracy results achieved by various language models in the sentiment analysis task.\u00a0</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S5.T5.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T5.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.1.1.1\">Name</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T5.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.1.2.1\">Accuracy</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T5.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.1.3.1\">Dataset Size</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T5.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.1.4.1\">Classes</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T5.1.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T5.1.2.1.1\">Shahriar et al. (AraBERTv0.2-base) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.12392v1#bib.bib20\" title=\"\">20</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T5.1.2.1.2\">76.50%</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T5.1.2.1.3\">9452 poems</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T5.1.2.1.4\">Sad, Love, Joy</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.3.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T5.1.3.2.1\">AraBERTv0.1-base</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T5.1.3.2.2\">75.79%</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T5.1.3.2.3\" rowspan=\"6\"><span class=\"ltx_text\" id=\"S5.T5.1.3.2.3.1\">21,230 poems (315,877 verses)</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T5.1.3.2.4\" rowspan=\"6\"><span class=\"ltx_text\" id=\"S5.T5.1.3.2.4.1\">Love, Sadness, Anger, Spirituality</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.4.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T5.1.4.3.1\">AraBERTv1-base</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T5.1.4.3.2\">75.35%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.5.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T5.1.5.4.1\">QARiB</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T5.1.5.4.2\">76.55%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.6.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T5.1.6.5.1\">ARBERT</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T5.1.6.5.2\">76.23%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.7.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T5.1.7.6.1\">CAMeLBERT-CA</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T5.1.7.6.2\">77.77%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.8.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T5.1.8.7.1\">AraPoemBERT-base (ours)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T5.1.8.7.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.8.7.2.1\">78.95%</span></td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 5: The accuracy results achieved by various language models in the sentiment analysis task.\u00a0"
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T6\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 6: </span>The classification report of AraPoemBERT in the sentiment analysis task.\u00a0</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S5.T6.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T6.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.1.1.1\">Class</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T6.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.1.2.1\">Precision</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T6.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.1.3.1\">Recall</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T6.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.1.4.1\">F1-Score</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T6.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.1.5.1\">Number of Samples</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T6.1.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.2.1.1\">Anger</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.2.1.2\">0.2952</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.2.1.3\">0.0610</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.2.1.4\">0.1011</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.2.1.5\">1016</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.3.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.3.2.1\">Love</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.3.2.2\">0.8215</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.3.2.3\">0.9348</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.3.2.4\">0.8745</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.3.2.5\">45274</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.4.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.4.3.1\">Spirituality</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.4.3.2\">0.7018</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.4.3.3\">0.6380</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.4.3.4\">0.6684</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.4.3.5\">8686</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.5.4\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.5.4.1\">Sadness</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.5.4.2\">0.5512</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.5.4.3\">0.2384</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.5.4.4\">0.3329</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.5.4.5\">8200</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.6.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.6.5.1\">Accuracy</td>\n<td class=\"ltx_td ltx_border_t\" id=\"S5.T6.1.6.5.2\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S5.T6.1.6.5.3\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.6.5.4\">0.7896</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.6.5.5\">63176</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.7.6\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.7.6.1\">Macro Avg</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.7.6.2\">0.5924</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.7.6.3\">0.4681</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.7.6.4\">0.4942</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.7.6.5\">63176</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.8.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T6.1.8.7.1\">Weighted Avg</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T6.1.8.7.2\">0.7615</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T6.1.8.7.3\">0.7896</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T6.1.8.7.4\">0.7634</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T6.1.8.7.5\">63176</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 6: The classification report of AraPoemBERT in the sentiment analysis task.\u00a0"
        },
        "6": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T7\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 7: </span><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.2.1\">List of Classical and Non-Classical meters</span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S5.T7.3\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T7.3.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" colspan=\"3\" id=\"S5.T7.3.1.1.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Classical Meters</td>\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" colspan=\"3\" id=\"S5.T7.3.1.1.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Non-Classical Meters</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.3.2.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"S5.T7.3.2.2.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">No.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T7.3.2.2.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Meter</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T7.3.2.2.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">No, of Verses</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T7.3.2.2.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">No.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T7.3.2.2.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Meter</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T7.3.2.2.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">No, of Verses</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.3.3.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"S5.T7.3.3.3.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T7.3.3.3.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Taweel</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T7.3.3.3.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">442763</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T7.3.3.3.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T7.3.3.3.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Muashah</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T7.3.3.3.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">38409</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.3.4.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T7.3.4.4.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.4.4.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Kamel</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.4.4.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">406908</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.4.4.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.4.4.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Free form</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.4.4.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">8431</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.3.5.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T7.3.5.5.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.5.5.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Baseet</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.5.5.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">265045</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.5.5.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">19</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.5.5.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Colloquial</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.5.5.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">7399</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.3.6.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T7.3.6.6.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.6.6.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Khafif</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.6.6.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">174105</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.6.6.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.6.6.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Doubeet</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.6.6.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">2937</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.3.7.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T7.3.7.7.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.7.7.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Wafer</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.7.7.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">154455</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.7.7.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.7.7.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Mawalia</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.7.7.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">1887</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.3.8.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T7.3.8.8.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.8.8.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Rajaz</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.8.8.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">111333</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.8.8.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.8.8.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Masehube</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.8.8.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">1283</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.3.9.9\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T7.3.9.9.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.9.9.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Ramel</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.9.9.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">89857</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.9.9.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.9.9.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Selselah</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.9.9.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">945</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.3.10.10\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T7.3.10.10.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.10.10.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Mutaqarib</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.10.10.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">69523</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.10.10.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.10.10.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Zajal</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.10.10.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">81</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.3.11.11\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T7.3.11.11.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.11.11.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Saree</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.11.11.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">61261</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.11.11.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.11.11.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Kankan</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.11.11.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">59</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.3.12.12\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T7.3.12.12.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.12.12.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Munsarih</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.12.12.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">29863</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.12.12.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">26</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.12.12.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Hajini</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.12.12.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">48</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.3.13.13\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T7.3.13.13.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.13.13.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Mujtath</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.13.13.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">19357</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.13.13.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.13.13.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Sakhri</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.13.13.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">16</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.3.14.14\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T7.3.14.14.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.14.14.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Hazaj</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.14.14.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">9166</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.14.14.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">28</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.14.14.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Luaihani</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.14.14.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.3.15.15\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T7.3.15.15.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.15.15.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Madeed</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.15.15.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">8775</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S5.T7.3.15.15.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"></td>\n<td class=\"ltx_td ltx_border_r\" id=\"S5.T7.3.15.15.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"></td>\n<td class=\"ltx_td ltx_border_r\" id=\"S5.T7.3.15.15.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.3.16.16\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T7.3.16.16.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.16.16.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Mutadarak</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.16.16.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">6631</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S5.T7.3.16.16.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"></td>\n<td class=\"ltx_td ltx_border_r\" id=\"S5.T7.3.16.16.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"></td>\n<td class=\"ltx_td ltx_border_r\" id=\"S5.T7.3.16.16.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.3.17.17\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T7.3.17.17.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.17.17.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Muqtadab</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.17.17.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">949</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S5.T7.3.17.17.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"></td>\n<td class=\"ltx_td ltx_border_r\" id=\"S5.T7.3.17.17.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"></td>\n<td class=\"ltx_td ltx_border_r\" id=\"S5.T7.3.17.17.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.3.18.18\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T7.3.18.18.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.18.18.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Mudari</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T7.3.18.18.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">360</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S5.T7.3.18.18.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"></td>\n<td class=\"ltx_td ltx_border_r\" id=\"S5.T7.3.18.18.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"></td>\n<td class=\"ltx_td ltx_border_r\" id=\"S5.T7.3.18.18.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.3.19.19\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" colspan=\"2\" id=\"S5.T7.3.19.19.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Total</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"S5.T7.3.19.19.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">1850351</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" colspan=\"2\" id=\"S5.T7.3.19.19.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Total</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"S5.T7.3.19.19.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">61502</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 7: List of Classical and Non-Classical meters"
        },
        "7": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T8\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 8: </span><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.2.1\">Poetry meters classification results achieved by various language models</span></figcaption>\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T8.3\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T8.3.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T8.3.1.1.1\">Name</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T8.3.1.1.2\">Architecture</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T8.3.1.1.3\">Accuracy (classical meters)</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T8.3.1.1.4\" style=\"width:48.4pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T8.3.1.1.4.1\">Dataset Size (No. Verses)</p>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T8.3.1.1.5\" style=\"width:79.7pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T8.3.1.1.5.1\">Accuracy (all meters)</p>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T8.3.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T8.3.2.1.1\">Al-Shaibani et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.12392v1#bib.bib13\" title=\"\">13</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T8.3.2.1.2\">5-BiGRU layers</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T8.3.2.1.3\">(14 meters only) 94.32%</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T8.3.2.1.4\" style=\"width:48.4pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T8.3.2.1.4.1\">55K</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T8.3.2.1.5\" style=\"width:79.7pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T8.3.2.1.5.1\">NA</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.3.3.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T8.3.3.2.1\">Yousuf et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.12392v1#bib.bib11\" title=\"\">11</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T8.3.3.2.2\">7-BiLSTM</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T8.3.3.2.3\">94.11%</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T8.3.3.2.4\" style=\"width:48.4pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T8.3.3.2.4.1\">1,72M</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T8.3.3.2.5\" style=\"width:79.7pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T8.3.3.2.5.1\">NA</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.3.4.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T8.3.4.3.1\">Abandah et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.12392v1#bib.bib14\" title=\"\">14</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T8.3.4.3.2\">4-BiLSTM</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T8.3.4.3.3\">97.27%</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T8.3.4.3.4\" style=\"width:48.4pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T8.3.4.3.4.1\">1.62M</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T8.3.4.3.5\" style=\"width:79.7pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T8.3.4.3.5.1\">NA</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.3.5.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T8.3.5.4.1\">AraBERTv01-base</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T8.3.5.4.2\">12L12H</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T8.3.5.4.3\">98.31%</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T8.3.5.4.4\" style=\"width:48.4pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T8.3.5.4.4.1\">1.85M</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T8.3.5.4.5\" style=\"width:79.7pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T8.3.5.4.5.1\">96.92%</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.3.6.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T8.3.6.5.1\">AraBERTv1-base</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T8.3.6.5.2\">12L12H</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T8.3.6.5.3\">98.25%</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T8.3.6.5.4\" style=\"width:48.4pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T8.3.6.5.4.1\">1.85M</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T8.3.6.5.5\" style=\"width:79.7pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T8.3.6.5.5.1\">96.87%</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.3.7.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T8.3.7.6.1\">QARiB</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T8.3.7.6.2\">12L12H</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T8.3.7.6.3\">98.26%</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T8.3.7.6.4\" style=\"width:48.4pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T8.3.7.6.4.1\">1.85M</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T8.3.7.6.5\" style=\"width:79.7pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T8.3.7.6.5.1\">96.88%</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.3.8.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T8.3.8.7.1\">ARBERT</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T8.3.8.7.2\">12L12H</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T8.3.8.7.3\">98.03%</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T8.3.8.7.4\" style=\"width:48.4pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T8.3.8.7.4.1\">1.85M</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T8.3.8.7.5\" style=\"width:79.7pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T8.3.8.7.5.1\">96.59%</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.3.9.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T8.3.9.8.1\">CAMeLBERT-CA</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T8.3.9.8.2\">12L12H</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T8.3.9.8.3\">98.50%</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T8.3.9.8.4\" style=\"width:48.4pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T8.3.9.8.4.1\">1.85M</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T8.3.9.8.5\" style=\"width:79.7pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T8.3.9.8.5.1\">97.23%</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.3.10.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T8.3.10.9.1\">AraPoemBERT-base (ours)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T8.3.10.9.2\">10L12H</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T8.3.10.9.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.3.10.9.3.1\">99.03%</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S5.T8.3.10.9.4\" style=\"width:48.4pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T8.3.10.9.4.1\">1.85M</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S5.T8.3.10.9.5\" style=\"width:79.7pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S5.T8.3.10.9.5.1\">97.82%</span></td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 8: Poetry meters classification results achieved by various language models"
        },
        "8": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T9\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 9: </span><span class=\"ltx_text ltx_font_bold\" id=\"S5.T9.2.1\">Prediction results of AraPoemBERT for classifying classical meters on an independent validation set</span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S5.T9.3\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T9.3.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\" id=\"S5.T9.3.1.1.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Label</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S5.T9.3.1.1.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Meter</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S5.T9.3.1.1.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Precision</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S5.T9.3.1.1.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Recall</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S5.T9.3.1.1.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">F1-score</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S5.T9.3.1.1.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">No. of Verses</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T9.3.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"S5.T9.3.2.1.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T9.3.2.1.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Taweel</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T9.3.2.1.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9972</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T9.3.2.1.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9977</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T9.3.2.1.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9975</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T9.3.2.1.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">88553</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.3.3.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T9.3.3.2.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.3.2.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Kamel</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.3.2.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9906</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.3.2.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9937</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.3.2.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9921</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.3.2.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">81382</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.3.4.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T9.3.4.3.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.4.3.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Baseet</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.4.3.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9961</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.4.3.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.995</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.4.3.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9955</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.4.3.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">53009</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.3.5.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T9.3.5.4.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.5.4.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Khafif</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.5.4.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9942</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.5.4.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9935</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.5.4.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9938</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.5.4.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">34821</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.3.6.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T9.3.6.5.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.6.5.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Wafer</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.6.5.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9876</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.6.5.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9935</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.6.5.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9905</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.6.5.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">30891</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.3.7.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T9.3.7.6.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.7.6.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Rajaz</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.7.6.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9709</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.7.6.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9709</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.7.6.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9709</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.7.6.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">22267</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.3.8.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T9.3.8.7.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.8.7.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Ramel</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.8.7.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9857</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.8.7.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9857</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.8.7.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9857</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.8.7.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">17971</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.3.9.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T9.3.9.8.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.9.8.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Mutaqarib</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.9.8.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9916</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.9.8.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9919</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.9.8.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9918</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.9.8.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">13905</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.3.10.9\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T9.3.10.9.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.10.9.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Saree</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.10.9.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9839</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.10.9.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9782</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.10.9.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9811</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.10.9.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">12252</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.3.11.10\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T9.3.11.10.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.11.10.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Munsarih</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.11.10.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9737</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.11.10.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9787</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.11.10.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9762</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.11.10.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">5973</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.3.12.11\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T9.3.12.11.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.12.11.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Mujtath</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.12.11.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9653</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.12.11.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9775</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.12.11.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9714</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.12.11.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3871</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.3.13.12\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T9.3.13.12.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.13.12.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Hazaj</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.13.12.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9112</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.13.12.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.8112</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.13.12.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.8583</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.13.12.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">1833</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.3.14.13\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T9.3.14.13.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.14.13.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Madeed</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.14.13.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9742</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.14.13.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9048</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.14.13.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9383</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.14.13.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">1755</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.3.15.14\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T9.3.15.14.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.15.14.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Mutadarak</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.15.14.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9524</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.15.14.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9359</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.15.14.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9441</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.15.14.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">1326</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.3.16.15\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T9.3.16.15.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.16.15.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Muqtadab</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.16.15.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.8914</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.16.15.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.8211</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.16.15.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.8548</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T9.3.16.15.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">190</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T9.3.17.16\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r\" id=\"S5.T9.3.17.16.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S5.T9.3.17.16.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Mudari</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S5.T9.3.17.16.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.7143</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S5.T9.3.17.16.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.2778</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S5.T9.3.17.16.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S5.T9.3.17.16.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">72</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 9: Prediction results of AraPoemBERT for classifying classical meters on an independent validation set"
        },
        "9": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T10\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 10: </span><span class=\"ltx_text ltx_font_bold\" id=\"S5.T10.2.1\">Prediction results of AraPoemBERT for classifying all meters on an independent validation set</span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S5.T10.3\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T10.3.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\" id=\"S5.T10.3.1.1.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">No.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S5.T10.3.1.1.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Meter</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S5.T10.3.1.1.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Precision</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S5.T10.3.1.1.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Recall</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S5.T10.3.1.1.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">F1-score</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S5.T10.3.1.1.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Num. of Verses</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T10.3.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"S5.T10.3.2.1.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T10.3.2.1.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Taweel</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T10.3.2.1.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9963</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T10.3.2.1.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9971</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T10.3.2.1.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9967</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T10.3.2.1.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">88553</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.3.3.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T10.3.3.2.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.3.2.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Kamel</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.3.2.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9863</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.3.2.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9937</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.3.2.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.99</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.3.2.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">81382</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.3.4.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T10.3.4.3.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.4.3.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Baseet</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.4.3.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9933</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.4.3.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9916</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.4.3.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9925</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.4.3.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">53009</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.3.5.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T10.3.5.4.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.5.4.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Khafif</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.5.4.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9908</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.5.4.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9909</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.5.4.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9908</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.5.4.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">34821</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.3.6.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T10.3.6.5.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.6.5.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Wafer</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.6.5.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.986</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.6.5.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9913</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.6.5.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9886</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.6.5.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">30891</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.3.7.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T10.3.7.6.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.7.6.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Rajaz</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.7.6.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.958</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.7.6.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.962</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.7.6.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.96</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.7.6.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">22267</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.3.8.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T10.3.8.7.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.8.7.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Ramel</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.8.7.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9384</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.8.7.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9725</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.8.7.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9552</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.8.7.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">17971</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.3.9.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T10.3.9.8.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.9.8.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Mutaqarib</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.9.8.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9863</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.9.8.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.99</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.9.8.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9882</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.9.8.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">13905</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.3.10.9\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T10.3.10.9.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.10.9.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Saree</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.10.9.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9793</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.10.9.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.971</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.10.9.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9752</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.10.9.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">12252</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.3.11.10\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T10.3.11.10.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.11.10.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Munsarih</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.11.10.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.982</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.11.10.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.969</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.11.10.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9755</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.11.10.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">5973</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.3.12.11\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T10.3.12.11.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.12.11.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Mujtath</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.12.11.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9513</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.12.11.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9432</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.12.11.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9472</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.12.11.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3871</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.3.13.12\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T10.3.13.12.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.13.12.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Hazaj</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.13.12.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.8813</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.13.12.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.8347</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.13.12.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.8574</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.13.12.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">1833</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.3.14.13\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T10.3.14.13.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.14.13.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Madeed</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.14.13.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9398</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.14.13.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9066</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.14.13.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9229</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.14.13.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">1755</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.3.15.14\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T10.3.15.14.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.15.14.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Mutadarak</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.15.14.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.8916</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.15.14.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9057</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.15.14.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.8986</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.15.14.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">1326</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.3.16.15\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T10.3.16.15.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.16.15.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Muqtadab</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.16.15.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.16.15.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.8526</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.16.15.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.8757</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.16.15.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">190</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.3.17.16\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T10.3.17.16.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.17.16.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Mudari</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.17.16.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.875</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.17.16.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.1944</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.17.16.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.3182</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.17.16.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">72</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.3.18.17\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T10.3.18.17.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.18.17.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Muashah</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.18.17.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.7505</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.18.17.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.7024</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.18.17.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.7257</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.18.17.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">7682</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.3.19.18\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T10.3.19.18.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.19.18.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Free form</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.19.18.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.857</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.19.18.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.8422</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.19.18.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.8495</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.19.18.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">1686</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.3.20.19\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T10.3.20.19.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">19</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.20.19.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Colloquial</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.20.19.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.7773</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.20.19.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.6392</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.20.19.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.7015</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.20.19.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">1480</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.3.21.20\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T10.3.21.20.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.21.20.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Doubeet</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.21.20.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9448</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.21.20.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9336</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.21.20.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9392</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.21.20.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">587</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.3.22.21\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T10.3.22.21.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.22.21.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Mawalia</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.22.21.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.698</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.22.21.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.5517</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.22.21.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.6163</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.22.21.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">377</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.3.23.22\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T10.3.23.22.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.23.22.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Masehube</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.23.22.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.8558</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.23.22.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.3463</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.23.22.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.4931</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.23.22.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">257</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.3.24.23\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T10.3.24.23.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.24.23.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Selselah</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.24.23.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.6875</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.24.23.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.4074</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.24.23.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.5116</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.24.23.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">189</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.3.25.24\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T10.3.25.24.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.25.24.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Zajal</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.25.24.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.25.24.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.25.24.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.25.24.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">16</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.3.26.25\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T10.3.26.25.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.26.25.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Kankan</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.26.25.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.26.25.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.26.25.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.26.25.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">12</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.3.27.26\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T10.3.27.26.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">26</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.27.26.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Hajini</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.27.26.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.27.26.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.27.26.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.27.26.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">10</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.3.28.27\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T10.3.28.27.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.28.27.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Sakhri</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.28.27.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.28.27.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.28.27.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T10.3.28.27.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.3.29.28\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r\" id=\"S5.T10.3.29.28.1\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">28</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S5.T10.3.29.28.2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Luaihani</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S5.T10.3.29.28.3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S5.T10.3.29.28.4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S5.T10.3.29.28.5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S5.T10.3.29.28.6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">1</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 10: Prediction results of AraPoemBERT for classifying all meters on an independent validation set"
        },
        "10": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T11\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 11: </span>List of excluded sub-meters.\u00a0</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S5.T11.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T11.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\" id=\"S5.T11.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T11.1.1.1.1.1\">Sub-Meter</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S5.T11.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T11.1.1.1.2.1\">No. of Verses</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T11.1.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"S5.T11.1.2.1.1\">Kamel Maktuu</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T11.1.2.1.2\">5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T11.1.3.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"S5.T11.1.3.2.1\">Mutadarak Manhuk</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T11.1.3.2.2\">7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T11.1.4.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"S5.T11.1.4.3.1\">Baseet Mashture</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T11.1.4.3.2\">12</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T11.1.5.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"S5.T11.1.5.4.1\">Munsarih Manhuk</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T11.1.5.4.2\">37</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T11.1.6.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"S5.T11.1.6.5.1\">Mutaqarib Majzuu</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T11.1.6.5.2\">40</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T11.1.7.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"S5.T11.1.7.6.1\">Mutadarak Majzuu</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T11.1.7.6.2\">45</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T11.1.8.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"S5.T11.1.8.7.1\">Rajaz Manhuk</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T11.1.8.7.2\">77</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T11.1.9.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" id=\"S5.T11.1.9.8.1\">Baseet Majzuu</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"S5.T11.1.9.8.2\">101</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 11: List of excluded sub-meters.\u00a0"
        },
        "11": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T12\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 12: </span>List of target sub-meters included in the study.\u00a0</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S5.T12.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T12.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"S5.T12.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T12.1.1.1.1.1\">No.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" id=\"S5.T12.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T12.1.1.1.2.1\">Sub-Meter</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S5.T12.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T12.1.1.1.3.1\">No. of Verses</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S5.T12.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T12.1.1.1.4.1\">No. of Poems</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T12.1.2.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"S5.T12.1.2.1.1\">1</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S5.T12.1.2.1.2\">Baseet Complete</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T12.1.2.1.3\">257226</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T12.1.2.1.4\">22442</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.1.3.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S5.T12.1.3.2.1\">2</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S5.T12.1.3.2.2\">Baseet Mukhala</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.3.2.3\">1986</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.3.2.4\">300</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.1.4.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S5.T12.1.4.3.1\">3</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S5.T12.1.4.3.2\">Hazaj Majzuu</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.4.3.3\">9027</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.4.3.4\">837</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.1.5.4\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S5.T12.1.5.4.1\">4</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S5.T12.1.5.4.2\">Kamel Ahuth</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.5.4.3\">1634</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.5.4.4\">227</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.1.6.5\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S5.T12.1.6.5.1\">5</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S5.T12.1.6.5.2\">Kamel Complete</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.6.5.3\">390976</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.6.5.4\">28945</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.1.7.6\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S5.T12.1.7.6.1\">6</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S5.T12.1.7.6.2\">Kamel Majzuu</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.7.6.3\">5592</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.7.6.4\">672</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.1.8.7\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S5.T12.1.8.7.1\">7</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S5.T12.1.8.7.2\">Khafif Complete</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.8.7.3\">169246</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.8.7.4\">13129</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.1.9.8\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S5.T12.1.9.8.1\">8</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S5.T12.1.9.8.2\">Khafif Majzuu</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.9.8.3\">1135</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.9.8.4\">130</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.1.10.9\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S5.T12.1.10.9.1\">9</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S5.T12.1.10.9.2\">Madeed Majzuu</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.10.9.3\">8459</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.10.9.4\">780</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.1.11.10\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S5.T12.1.11.10.1\">10</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S5.T12.1.11.10.2\">Mudari Majzuu</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.11.10.3\">358</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.11.10.4\">31</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.1.12.11\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S5.T12.1.12.11.1\">11</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S5.T12.1.12.11.2\">Mujtath Majzuu</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.12.11.3\">18820</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.12.11.4\">2463</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.1.13.12\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S5.T12.1.13.12.1\">12</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S5.T12.1.13.12.2\">Munsarih Complete</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.13.12.3\">29054</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.13.12.4\">3382</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.1.14.13\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S5.T12.1.14.13.1\">13</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S5.T12.1.14.13.2\">Muqtadab Majzuu</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.14.13.3\">945</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.14.13.4\">65</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.1.15.14\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S5.T12.1.15.14.1\">14</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S5.T12.1.15.14.2\">Mutadarak Complete</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.15.14.3\">6205</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.15.14.4\">275</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.1.16.15\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S5.T12.1.16.15.1\">15</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S5.T12.1.16.15.2\">Mutadarak Mashture</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.16.15.3\">168</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.16.15.4\">7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.1.17.16\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S5.T12.1.17.16.1\">16</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S5.T12.1.17.16.2\">Mutaqarib Complete</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.17.16.3\">68139</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.17.16.4\">6724</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.1.18.17\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S5.T12.1.18.17.1\">17</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S5.T12.1.18.17.2\">Rajaz Complete</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.18.17.3\">7586</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.18.17.4\">384</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.1.19.18\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S5.T12.1.19.18.1\">18</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S5.T12.1.19.18.2\">Rajaz Majzuu</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.19.18.3\">100358</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.19.18.4\">6871</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.1.20.19\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S5.T12.1.20.19.1\">19</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S5.T12.1.20.19.2\">Rajaz Mashture</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.20.19.3\">1351</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.20.19.4\">183</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.1.21.20\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S5.T12.1.21.20.1\">20</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S5.T12.1.21.20.2\">Ramel Complete</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.21.20.3\">84372</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.21.20.4\">6217</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.1.22.21\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S5.T12.1.22.21.1\">21</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S5.T12.1.22.21.2\">Ramel Majzuu</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.22.21.3\">3651</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.22.21.4\">393</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.1.23.22\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S5.T12.1.23.22.1\">22</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S5.T12.1.23.22.2\">Saree Complete</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.23.22.3\">60051</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.23.22.4\">8357</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.1.24.23\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S5.T12.1.24.23.1\">23</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S5.T12.1.24.23.2\">Taweel Complete</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.24.23.3\">433142</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.24.23.4\">36970</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.1.25.24\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\" id=\"S5.T12.1.25.24.1\">24</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S5.T12.1.25.24.2\">Wafer Complete</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.25.24.3\">150115</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T12.1.25.24.4\">14658</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.1.26.25\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\" id=\"S5.T12.1.26.25.1\">25</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\" id=\"S5.T12.1.26.25.2\">Wafer Majzuu</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S5.T12.1.26.25.3\">745</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S5.T12.1.26.25.4\">89</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 12: List of target sub-meters included in the study.\u00a0"
        },
        "12": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T13\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 13: </span>The accuracy results in three different classification tasks: sub-meter, poet\u2019s gender, and poetry rhymes.\u00a0</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S5.T13.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T13.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S5.T13.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T13.1.1.1.1.1\">Name</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T13.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T13.1.1.1.2.1\">Sub-Meter</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T13.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T13.1.1.1.3.1\">Gender</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T13.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T13.1.1.1.4.1\">Rhyme</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T13.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T13.1.2.1.1\">AraBERTv0.1-base</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T13.1.2.1.2\">97.16%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T13.1.2.1.3\">99.28%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T13.1.2.1.4\">97.57%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T13.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T13.1.3.2.1\">AraBERTv1-base</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.1.3.2.2\">97.15%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.1.3.2.3\">99.29%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.1.3.2.4\">97.33%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T13.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T13.1.4.3.1\">QARiB</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.1.4.3.2\">97.01%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.1.4.3.3\">99.32%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.1.4.3.4\">97.58%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T13.1.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T13.1.5.4.1\">ARBERT</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.1.5.4.2\">96.86%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.1.5.4.3\">99.28%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.1.5.4.4\">97.40%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T13.1.6.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T13.1.6.5.1\">CAMeLBERT-CA</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.1.6.5.2\">97.46%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.1.6.5.3\">99.28%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T13.1.6.5.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T13.1.6.5.4.1\">97.76%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T13.1.7.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S5.T13.1.7.6.1\">AraPoemBERT-base (ours)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T13.1.7.6.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T13.1.7.6.2.1\">97.79%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T13.1.7.6.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T13.1.7.6.3.1\">99.34%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T13.1.7.6.4\">97.73%</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 13: The accuracy results in three different classification tasks: sub-meter, poet\u2019s gender, and poetry rhymes.\u00a0"
        },
        "13": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T14\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 14: </span>Sub-meters classification report using AraPoemBERT model.\u00a0</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S5.T14.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T14.1.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"S5.T14.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T14.1.1.1.1.1\">Label</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T14.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T14.1.1.1.2.1\">Class</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T14.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T14.1.1.1.3.1\">Precision</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T14.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T14.1.1.1.4.1\">Recall</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T14.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T14.1.1.1.5.1\">F1-score</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T14.1.1.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T14.1.1.1.6.1\">Number of Samples</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T14.1.2.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"S5.T14.1.2.2.1\">0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T14.1.2.2.2\">Baseet complete</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T14.1.2.2.3\">0.9884</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T14.1.2.2.4\">0.9959</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T14.1.2.2.5\">0.9921</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T14.1.2.2.6\">52585</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T14.1.3.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T14.1.3.3.1\">1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.3.3.2\">Baseet mukhala</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.3.3.3\">0.5484</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.3.3.4\">0.0423</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.3.3.5\">0.0785</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.3.3.6\">402</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T14.1.4.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T14.1.4.4.1\">2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.4.4.2\">Hazaj majzuu</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.4.4.3\">0.8850</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.4.4.4\">0.8603</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.4.4.5\">0.8725</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.4.4.6\">1833</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T14.1.5.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T14.1.5.5.1\">3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.5.5.2\">Kamel ahuth</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.5.5.3\">0.5000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.5.5.4\">0.1206</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.5.5.5\">0.1943</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.5.5.6\">340</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T14.1.6.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T14.1.6.6.1\">4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.6.6.2\">Kamel complete</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.6.6.3\">0.9769</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.6.6.4\">0.9914</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.6.6.5\">0.9841</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.6.6.6\">79902</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T14.1.7.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T14.1.7.7.1\">5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.7.7.2\">Kamel majzuu</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.7.7.3\">0.5465</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.7.7.4\">0.1238</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.7.7.5\">0.2019</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.7.7.6\">1139</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T14.1.8.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T14.1.8.8.1\">6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.8.8.2\">Khafif complete</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.8.8.3\">0.9874</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.8.8.4\">0.9945</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.8.8.5\">0.9909</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.8.8.6\">34589</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T14.1.9.9\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T14.1.9.9.1\">7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.9.9.2\">Khafif majzuu</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.9.9.3\">0.3485</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.9.9.4\">0.0991</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.9.9.5\">0.1544</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.9.9.6\">232</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T14.1.10.10\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T14.1.10.10.1\">8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.10.10.2\">Madeed majzuu</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.10.10.3\">0.9580</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.10.10.4\">0.9225</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.10.10.5\">0.9399</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.10.10.6\">1755</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T14.1.11.11\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T14.1.11.11.1\">9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.11.11.2\">Mudari majzuu</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.11.11.3\">0.8462</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.11.11.4\">0.3056</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.11.11.5\">0.4490</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.11.11.6\">72</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T14.1.12.12\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T14.1.12.12.1\">10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.12.12.2\">Mujtath majzuu</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.12.12.3\">0.9783</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.12.12.4\">0.9773</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.12.12.5\">0.9778</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.12.12.6\">3871</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T14.1.13.13\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T14.1.13.13.1\">11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.13.13.2\">Munsarih complete</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.13.13.3\">0.9831</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.13.13.4\">0.9780</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.13.13.5\">0.9806</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.13.13.6\">5965</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T14.1.14.14\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T14.1.14.14.1\">12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.14.14.2\">Muqtadab majzuu</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.14.14.3\">0.9458</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.14.14.4\">0.8263</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.14.14.5\">0.8820</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.14.14.6\">190</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T14.1.15.15\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T14.1.15.15.1\">13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.15.15.2\">Mutadarak complete</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.15.15.3\">0.9501</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.15.15.4\">0.9353</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.15.15.5\">0.9426</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.15.15.6\">1282</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T14.1.16.16\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T14.1.16.16.1\">14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.16.16.2\">Mutadarak mashture</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.16.16.3\">0.5238</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.16.16.4\">0.3333</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.16.16.5\">0.4074</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.16.16.6\">33</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T14.1.17.17\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T14.1.17.17.1\">15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.17.17.2\">Mutaqarib complete</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.17.17.3\">0.9910</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.17.17.4\">0.9921</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.17.17.5\">0.9915</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.17.17.6\">13897</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T14.1.18.18\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T14.1.18.18.1\">16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.18.18.2\">Rajaz complete</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.18.18.3\">0.4725</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.18.18.4\">0.5063</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.18.18.5\">0.4888</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.18.18.6\">1580</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T14.1.19.19\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T14.1.19.19.1\">17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.19.19.2\">Rajaz majzuu</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.19.19.3\">0.9236</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.19.19.4\">0.9266</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.19.19.5\">0.9251</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.19.19.6\">20396</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T14.1.20.20\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T14.1.20.20.1\">18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.20.20.2\">Rajaz mashture</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.20.20.3\">0.4351</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.20.20.4\">0.2065</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.20.20.5\">0.2801</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.20.20.6\">276</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T14.1.21.21\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T14.1.21.21.1\">19</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.21.21.2\">Ramel complete</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.21.21.3\">0.9511</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.21.21.4\">0.9846</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.21.21.5\">0.9675</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.21.21.6\">17226</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T14.1.22.22\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T14.1.22.22.1\">20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.22.22.2\">Ramel majzuu</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.22.22.3\">0.5743</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.22.22.4\">0.1557</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.22.22.5\">0.2450</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.22.22.6\">745</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T14.1.23.23\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T14.1.23.23.1\">21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.23.23.2\">Saree complete</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.23.23.3\">0.9814</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.23.23.4\">0.9830</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.23.23.5\">0.9822</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.23.23.6\">12252</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T14.1.24.24\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T14.1.24.24.1\">22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.24.24.2\">Taweel complete</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.24.24.3\">0.9977</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.24.24.4\">0.9975</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.24.24.5\">0.9976</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.24.24.6\">88553</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T14.1.25.25\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T14.1.25.25.1\">23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.25.25.2\">Wafer complete</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.25.25.3\">0.9872</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.25.25.4\">0.9909</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.25.25.5\">0.9891</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.25.25.6\">30741</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T14.1.26.26\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S5.T14.1.26.26.1\">24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.26.26.2\">Wafer majzuu</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.26.26.3\">0.4400</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.26.26.4\">0.0733</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.26.26.5\">0.1257</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T14.1.26.26.6\">150</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T14.1.27.27\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_t\" colspan=\"3\" id=\"S5.T14.1.27.27.1\">Accuracy</td>\n<td class=\"ltx_td ltx_border_b ltx_border_r ltx_border_t\" id=\"S5.T14.1.27.27.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"S5.T14.1.27.27.3\">0.9780</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"S5.T14.1.27.27.4\">370006</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 14: Sub-meters classification report using AraPoemBERT model.\u00a0"
        },
        "14": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T15\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 15: </span>Poet\u2019s gender classification report using AraPoemBERT model. \u00a0</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S5.T15.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T15.1.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T15.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T15.1.1.1.1.1\">Class</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T15.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T15.1.1.1.2.1\">Precision</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T15.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T15.1.1.1.3.1\">Recall</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T15.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T15.1.1.1.4.1\">F1-Score</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T15.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T15.1.1.1.5.1\">Number of Samples</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T15.1.2.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T15.1.2.2.1\">Female</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T15.1.2.2.2\">0.7582</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T15.1.2.2.3\">0.1318</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T15.1.2.2.4\">0.2246</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T15.1.2.2.5\">2996</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T15.1.3.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.1.3.3.1\">Male</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.1.3.3.2\">0.9938</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.1.3.3.3\">0.9997</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.1.3.3.4\">0.9967</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.1.3.3.5\">414516</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T15.1.4.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T15.1.4.4.1\">Accuracy</td>\n<td class=\"ltx_td ltx_border_t\" id=\"S5.T15.1.4.4.2\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S5.T15.1.4.4.3\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T15.1.4.4.4\">0.9935</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T15.1.4.4.5\">417512</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T15.1.5.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.1.5.5.1\">Macro Avg</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.1.5.5.2\">0.8760</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.1.5.5.3\">0.5658</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.1.5.5.4\">0.6107</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.1.5.5.5\">417512</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T15.1.6.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T15.1.6.6.1\">Weighted Avg</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T15.1.6.6.2\">0.9921</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T15.1.6.6.3\">0.9935</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T15.1.6.6.4\">0.9912</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T15.1.6.6.5\">417512</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 15: Poet\u2019s gender classification report using AraPoemBERT model. \u00a0"
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.12392v1_figure_1.png",
            "caption": "Table 2: List of basic feet in Arabic poetry (Tafaeil)."
        },
        "2": {
            "figure_path": "2403.12392v1_figure_2.png",
            "caption": "Table 2: List of basic feet in Arabic poetry (Tafaeil)."
        },
        "3": {
            "figure_path": "2403.12392v1_figure_3.png",
            "caption": "Figure 1: Overview of the transformer model architecture [22]"
        },
        "4": {
            "figure_path": "2403.12392v1_figure_4.png",
            "caption": "Figure 2: An example of masked language model (MLM) training objective"
        },
        "5": {
            "figure_path": "2403.12392v1_figure_5.png",
            "caption": "Figure 3: An illustration of BERT pretraining process [31]."
        },
        "6": {
            "figure_path": "2403.12392v1_figure_6.png",
            "caption": "Figure 4: The distribution of number of tokens per verse in AraPoems dataset"
        },
        "7": {
            "figure_path": "2403.12392v1_figure_7.png",
            "caption": "Figure 5: A sample of AraPoems dataset"
        },
        "8": {
            "figure_path": "2403.12392v1_figure_8.png",
            "caption": "Figure 6: The distribution of verses across the categories: meter, type, gender, and era"
        },
        "9": {
            "figure_path": "2403.12392v1_figure_9.png",
            "caption": "Figure 7: The algorithm structure diagram of text preprocessing"
        },
        "10": {
            "figure_path": "2403.12392v1_figure_10.png",
            "caption": "Figure 8: The sentiment analysis confusion Matrix for the test set using AraPoemBERT."
        },
        "11": {
            "figure_path": "2403.12392v1_figure_11.png",
            "caption": "Figure 9: \nThe confusion matrix for the test set in the task of classifying classical meters using AraPoemBERT."
        },
        "12": {
            "figure_path": "2403.12392v1_figure_12.png",
            "caption": "Figure 10: \nThe confusion matrix for the test set in task of classifying all meters (both classical and non-classical meters) using AraPoemBERT."
        },
        "13": {
            "figure_path": "2403.12392v1_figure_13.png",
            "caption": "Figure 11: Confusion Matrix for the test set in the rhyme classification task using AraPoemBERT."
        }
    },
    "references": [
        {
            "1": {
                "title": "Arberry, A.J.:\nArabic Poetry: a Primer for Students.\nCUP Archive,\nCambridge\n(1965)\n\n\n\nScott [2010]\n\nScott, H.:\nPegs, cords, and ghuls: Meter of classical arabic poetry.\nPhD thesis\n(2010)\n\n\n\nAtiq [1987]\n\nAtiq, A.:\nElm al-arud wal qafiah (in arabic).\nDar Alnahda, Beirut, Lebanon\n(1987)\n\n\n\nAhmed Abdel Aziz and Juma Al Kubaisi [2018]\n\nAhmed Abdel Aziz, A.M.D.,\nJuma Al Kubaisi, M.S.:\nTypes of causes in the ancient arabic prosodic lesson.(in arabic).\nJournal of Anbar University for Languages & Literature/Magallat Gami\u2019at Al-Anbar Li-Lugat Wa-al-Adabl\n(26)\n(2018)\n\n\n\nSowayan et al. [2010]\n\nSowayan, S.A.,\nAl-Shaibani, M.,\nAl-Zumar, S.:\nNabati Poetry The Taste of the People and The Authority of The Text (in Arabic).\nAbu Dhabi Authority for Culture and Heritage,\nAbu Dhabi\n(2010)\n\n\n\nAlabbas et al. [2014]\n\nAlabbas, M.,\nKhalaf, Z.A.,\nKhashan, K.M.:\nBasrah: an automatic system to identify the meter of arabic poetry.\nNatural Language Engineering\n20(1),\n131\u2013149\n(2014)\n\n\n\nAbuata and Al-Omari [2018]\n\nAbuata, B.,\nAl-Omari, A.:\nA rule-based algorithm for the detection of arud meter in classical arabic poetry.\nInternational Arab Journal of Information Technology\n15(4),\n1\u20135\n(2018)\n\n\n\nAlnagdawi et al. [2013]\n\nAlnagdawi, M.A.,\nRashideh, H.,\nAburumman, F.:\nFinding arabic poem meter using context free grammar.\nJournal of Communication and Computer Engineering\n3(1),\n52\u201359\n(2013)\n\n\n\nBerkani et al. [2020]\n\nBerkani, A.,\nHolzer, A.,\nStoffel, K.:\nPattern matching in meter detection of arabic classical poetry.\nIn: 2020 IEEE/ACS 17th International Conference on Computer Systems and Applications (AICCSA),\npp. 1\u20138\n(2020).\nIEEE\n\n\n\nYousef et al. [2019]\n\nYousef, W.A.,\nIbrahime, O.M.,\nMadbouly, T.M.,\nMahmoud, M.A.:\nLearning meters of arabic and english poems with recurrent neural networks: a step forward for language understanding and synthesis.\narXiv preprint arXiv:1905.05700\n(2019)\n\n\n\n[12]\n\nArabic Poem Comprehensive Dataset (APCD).\nhttps://hci-lab.github.io/LearningMetersPoems/\n\nAl-Shaibani et al. [2020]\n\nAl-Shaibani, M.S.,\nAlyafeai, Z.,\nAhmad, I.:\nMeter classification of arabic poems using deep bidirectional recurrent neural networks.\nPattern Recognition Letters\n136,\n1\u20137\n(2020)\n\n\n\nAbandah et al. [2022]\n\nAbandah, G.A.,\nKhedher, M.Z.,\nAbdel-Majeed, M.R.,\nMansour, H.M.,\nHulliel, S.F.,\nBisharat, L.M.:\nClassifying and diacritizing arabic poems using deep recurrent neural networks.\nJournal of King Saud University-Computer and Information Sciences\n34(6),\n3775\u20133788\n(2022)\n\n\n\nAbboushi and Azzeh [2023]\n\nAbboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "author": "Zwettler, M.:\nOral Tradition of Classical Arabic Poetry: Its Character and Implications.\nThe Ohio State University Press,\nColumbus, Ohio\n(1978)\n\n\n\nArberry [1965]\n\nArberry, A.J.:\nArabic Poetry: a Primer for Students.\nCUP Archive,\nCambridge\n(1965)\n\n\n\nScott [2010]\n\nScott, H.:\nPegs, cords, and ghuls: Meter of classical arabic poetry.\nPhD thesis\n(2010)\n\n\n\nAtiq [1987]\n\nAtiq, A.:\nElm al-arud wal qafiah (in arabic).\nDar Alnahda, Beirut, Lebanon\n(1987)\n\n\n\nAhmed Abdel Aziz and Juma Al Kubaisi [2018]\n\nAhmed Abdel Aziz, A.M.D.,\nJuma Al Kubaisi, M.S.:\nTypes of causes in the ancient arabic prosodic lesson.(in arabic).\nJournal of Anbar University for Languages & Literature/Magallat Gami\u2019at Al-Anbar Li-Lugat Wa-al-Adabl\n(26)\n(2018)\n\n\n\nSowayan et al. [2010]\n\nSowayan, S.A.,\nAl-Shaibani, M.,\nAl-Zumar, S.:\nNabati Poetry The Taste of the People and The Authority of The Text (in Arabic).\nAbu Dhabi Authority for Culture and Heritage,\nAbu Dhabi\n(2010)\n\n\n\nAlabbas et al. [2014]\n\nAlabbas, M.,\nKhalaf, Z.A.,\nKhashan, K.M.:\nBasrah: an automatic system to identify the meter of arabic poetry.\nNatural Language Engineering\n20(1),\n131\u2013149\n(2014)\n\n\n\nAbuata and Al-Omari [2018]\n\nAbuata, B.,\nAl-Omari, A.:\nA rule-based algorithm for the detection of arud meter in classical arabic poetry.\nInternational Arab Journal of Information Technology\n15(4),\n1\u20135\n(2018)\n\n\n\nAlnagdawi et al. [2013]\n\nAlnagdawi, M.A.,\nRashideh, H.,\nAburumman, F.:\nFinding arabic poem meter using context free grammar.\nJournal of Communication and Computer Engineering\n3(1),\n52\u201359\n(2013)\n\n\n\nBerkani et al. [2020]\n\nBerkani, A.,\nHolzer, A.,\nStoffel, K.:\nPattern matching in meter detection of arabic classical poetry.\nIn: 2020 IEEE/ACS 17th International Conference on Computer Systems and Applications (AICCSA),\npp. 1\u20138\n(2020).\nIEEE\n\n\n\nYousef et al. [2019]\n\nYousef, W.A.,\nIbrahime, O.M.,\nMadbouly, T.M.,\nMahmoud, M.A.:\nLearning meters of arabic and english poems with recurrent neural networks: a step forward for language understanding and synthesis.\narXiv preprint arXiv:1905.05700\n(2019)\n\n\n\n[12]\n\nArabic Poem Comprehensive Dataset (APCD).\nhttps://hci-lab.github.io/LearningMetersPoems/\n\nAl-Shaibani et al. [2020]\n\nAl-Shaibani, M.S.,\nAlyafeai, Z.,\nAhmad, I.:\nMeter classification of arabic poems using deep bidirectional recurrent neural networks.\nPattern Recognition Letters\n136,\n1\u20137\n(2020)\n\n\n\nAbandah et al. [2022]\n\nAbandah, G.A.,\nKhedher, M.Z.,\nAbdel-Majeed, M.R.,\nMansour, H.M.,\nHulliel, S.F.,\nBisharat, L.M.:\nClassifying and diacritizing arabic poems using deep recurrent neural networks.\nJournal of King Saud University-Computer and Information Sciences\n34(6),\n3775\u20133788\n(2022)\n\n\n\nAbboushi and Azzeh [2023]\n\nAbboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "venue": "Scott, H.:\nPegs, cords, and ghuls: Meter of classical arabic poetry.\nPhD thesis\n(2010)\n\n\n\nAtiq [1987]\n\nAtiq, A.:\nElm al-arud wal qafiah (in arabic).\nDar Alnahda, Beirut, Lebanon\n(1987)\n\n\n\nAhmed Abdel Aziz and Juma Al Kubaisi [2018]\n\nAhmed Abdel Aziz, A.M.D.,\nJuma Al Kubaisi, M.S.:\nTypes of causes in the ancient arabic prosodic lesson.(in arabic).\nJournal of Anbar University for Languages & Literature/Magallat Gami\u2019at Al-Anbar Li-Lugat Wa-al-Adabl\n(26)\n(2018)\n\n\n\nSowayan et al. [2010]\n\nSowayan, S.A.,\nAl-Shaibani, M.,\nAl-Zumar, S.:\nNabati Poetry The Taste of the People and The Authority of The Text (in Arabic).\nAbu Dhabi Authority for Culture and Heritage,\nAbu Dhabi\n(2010)\n\n\n\nAlabbas et al. [2014]\n\nAlabbas, M.,\nKhalaf, Z.A.,\nKhashan, K.M.:\nBasrah: an automatic system to identify the meter of arabic poetry.\nNatural Language Engineering\n20(1),\n131\u2013149\n(2014)\n\n\n\nAbuata and Al-Omari [2018]\n\nAbuata, B.,\nAl-Omari, A.:\nA rule-based algorithm for the detection of arud meter in classical arabic poetry.\nInternational Arab Journal of Information Technology\n15(4),\n1\u20135\n(2018)\n\n\n\nAlnagdawi et al. [2013]\n\nAlnagdawi, M.A.,\nRashideh, H.,\nAburumman, F.:\nFinding arabic poem meter using context free grammar.\nJournal of Communication and Computer Engineering\n3(1),\n52\u201359\n(2013)\n\n\n\nBerkani et al. [2020]\n\nBerkani, A.,\nHolzer, A.,\nStoffel, K.:\nPattern matching in meter detection of arabic classical poetry.\nIn: 2020 IEEE/ACS 17th International Conference on Computer Systems and Applications (AICCSA),\npp. 1\u20138\n(2020).\nIEEE\n\n\n\nYousef et al. [2019]\n\nYousef, W.A.,\nIbrahime, O.M.,\nMadbouly, T.M.,\nMahmoud, M.A.:\nLearning meters of arabic and english poems with recurrent neural networks: a step forward for language understanding and synthesis.\narXiv preprint arXiv:1905.05700\n(2019)\n\n\n\n[12]\n\nArabic Poem Comprehensive Dataset (APCD).\nhttps://hci-lab.github.io/LearningMetersPoems/\n\nAl-Shaibani et al. [2020]\n\nAl-Shaibani, M.S.,\nAlyafeai, Z.,\nAhmad, I.:\nMeter classification of arabic poems using deep bidirectional recurrent neural networks.\nPattern Recognition Letters\n136,\n1\u20137\n(2020)\n\n\n\nAbandah et al. [2022]\n\nAbandah, G.A.,\nKhedher, M.Z.,\nAbdel-Majeed, M.R.,\nMansour, H.M.,\nHulliel, S.F.,\nBisharat, L.M.:\nClassifying and diacritizing arabic poems using deep recurrent neural networks.\nJournal of King Saud University-Computer and Information Sciences\n34(6),\n3775\u20133788\n(2022)\n\n\n\nAbboushi and Azzeh [2023]\n\nAbboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "url": null
            }
        },
        {
            "2": {
                "title": "Scott, H.:\nPegs, cords, and ghuls: Meter of classical arabic poetry.\nPhD thesis\n(2010)\n\n\n\nAtiq [1987]\n\nAtiq, A.:\nElm al-arud wal qafiah (in arabic).\nDar Alnahda, Beirut, Lebanon\n(1987)\n\n\n\nAhmed Abdel Aziz and Juma Al Kubaisi [2018]\n\nAhmed Abdel Aziz, A.M.D.,\nJuma Al Kubaisi, M.S.:\nTypes of causes in the ancient arabic prosodic lesson.(in arabic).\nJournal of Anbar University for Languages & Literature/Magallat Gami\u2019at Al-Anbar Li-Lugat Wa-al-Adabl\n(26)\n(2018)\n\n\n\nSowayan et al. [2010]\n\nSowayan, S.A.,\nAl-Shaibani, M.,\nAl-Zumar, S.:\nNabati Poetry The Taste of the People and The Authority of The Text (in Arabic).\nAbu Dhabi Authority for Culture and Heritage,\nAbu Dhabi\n(2010)\n\n\n\nAlabbas et al. [2014]\n\nAlabbas, M.,\nKhalaf, Z.A.,\nKhashan, K.M.:\nBasrah: an automatic system to identify the meter of arabic poetry.\nNatural Language Engineering\n20(1),\n131\u2013149\n(2014)\n\n\n\nAbuata and Al-Omari [2018]\n\nAbuata, B.,\nAl-Omari, A.:\nA rule-based algorithm for the detection of arud meter in classical arabic poetry.\nInternational Arab Journal of Information Technology\n15(4),\n1\u20135\n(2018)\n\n\n\nAlnagdawi et al. [2013]\n\nAlnagdawi, M.A.,\nRashideh, H.,\nAburumman, F.:\nFinding arabic poem meter using context free grammar.\nJournal of Communication and Computer Engineering\n3(1),\n52\u201359\n(2013)\n\n\n\nBerkani et al. [2020]\n\nBerkani, A.,\nHolzer, A.,\nStoffel, K.:\nPattern matching in meter detection of arabic classical poetry.\nIn: 2020 IEEE/ACS 17th International Conference on Computer Systems and Applications (AICCSA),\npp. 1\u20138\n(2020).\nIEEE\n\n\n\nYousef et al. [2019]\n\nYousef, W.A.,\nIbrahime, O.M.,\nMadbouly, T.M.,\nMahmoud, M.A.:\nLearning meters of arabic and english poems with recurrent neural networks: a step forward for language understanding and synthesis.\narXiv preprint arXiv:1905.05700\n(2019)\n\n\n\n[12]\n\nArabic Poem Comprehensive Dataset (APCD).\nhttps://hci-lab.github.io/LearningMetersPoems/\n\nAl-Shaibani et al. [2020]\n\nAl-Shaibani, M.S.,\nAlyafeai, Z.,\nAhmad, I.:\nMeter classification of arabic poems using deep bidirectional recurrent neural networks.\nPattern Recognition Letters\n136,\n1\u20137\n(2020)\n\n\n\nAbandah et al. [2022]\n\nAbandah, G.A.,\nKhedher, M.Z.,\nAbdel-Majeed, M.R.,\nMansour, H.M.,\nHulliel, S.F.,\nBisharat, L.M.:\nClassifying and diacritizing arabic poems using deep recurrent neural networks.\nJournal of King Saud University-Computer and Information Sciences\n34(6),\n3775\u20133788\n(2022)\n\n\n\nAbboushi and Azzeh [2023]\n\nAbboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "author": "Arberry, A.J.:\nArabic Poetry: a Primer for Students.\nCUP Archive,\nCambridge\n(1965)\n\n\n\nScott [2010]\n\nScott, H.:\nPegs, cords, and ghuls: Meter of classical arabic poetry.\nPhD thesis\n(2010)\n\n\n\nAtiq [1987]\n\nAtiq, A.:\nElm al-arud wal qafiah (in arabic).\nDar Alnahda, Beirut, Lebanon\n(1987)\n\n\n\nAhmed Abdel Aziz and Juma Al Kubaisi [2018]\n\nAhmed Abdel Aziz, A.M.D.,\nJuma Al Kubaisi, M.S.:\nTypes of causes in the ancient arabic prosodic lesson.(in arabic).\nJournal of Anbar University for Languages & Literature/Magallat Gami\u2019at Al-Anbar Li-Lugat Wa-al-Adabl\n(26)\n(2018)\n\n\n\nSowayan et al. [2010]\n\nSowayan, S.A.,\nAl-Shaibani, M.,\nAl-Zumar, S.:\nNabati Poetry The Taste of the People and The Authority of The Text (in Arabic).\nAbu Dhabi Authority for Culture and Heritage,\nAbu Dhabi\n(2010)\n\n\n\nAlabbas et al. [2014]\n\nAlabbas, M.,\nKhalaf, Z.A.,\nKhashan, K.M.:\nBasrah: an automatic system to identify the meter of arabic poetry.\nNatural Language Engineering\n20(1),\n131\u2013149\n(2014)\n\n\n\nAbuata and Al-Omari [2018]\n\nAbuata, B.,\nAl-Omari, A.:\nA rule-based algorithm for the detection of arud meter in classical arabic poetry.\nInternational Arab Journal of Information Technology\n15(4),\n1\u20135\n(2018)\n\n\n\nAlnagdawi et al. [2013]\n\nAlnagdawi, M.A.,\nRashideh, H.,\nAburumman, F.:\nFinding arabic poem meter using context free grammar.\nJournal of Communication and Computer Engineering\n3(1),\n52\u201359\n(2013)\n\n\n\nBerkani et al. [2020]\n\nBerkani, A.,\nHolzer, A.,\nStoffel, K.:\nPattern matching in meter detection of arabic classical poetry.\nIn: 2020 IEEE/ACS 17th International Conference on Computer Systems and Applications (AICCSA),\npp. 1\u20138\n(2020).\nIEEE\n\n\n\nYousef et al. [2019]\n\nYousef, W.A.,\nIbrahime, O.M.,\nMadbouly, T.M.,\nMahmoud, M.A.:\nLearning meters of arabic and english poems with recurrent neural networks: a step forward for language understanding and synthesis.\narXiv preprint arXiv:1905.05700\n(2019)\n\n\n\n[12]\n\nArabic Poem Comprehensive Dataset (APCD).\nhttps://hci-lab.github.io/LearningMetersPoems/\n\nAl-Shaibani et al. [2020]\n\nAl-Shaibani, M.S.,\nAlyafeai, Z.,\nAhmad, I.:\nMeter classification of arabic poems using deep bidirectional recurrent neural networks.\nPattern Recognition Letters\n136,\n1\u20137\n(2020)\n\n\n\nAbandah et al. [2022]\n\nAbandah, G.A.,\nKhedher, M.Z.,\nAbdel-Majeed, M.R.,\nMansour, H.M.,\nHulliel, S.F.,\nBisharat, L.M.:\nClassifying and diacritizing arabic poems using deep recurrent neural networks.\nJournal of King Saud University-Computer and Information Sciences\n34(6),\n3775\u20133788\n(2022)\n\n\n\nAbboushi and Azzeh [2023]\n\nAbboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "venue": "Atiq, A.:\nElm al-arud wal qafiah (in arabic).\nDar Alnahda, Beirut, Lebanon\n(1987)\n\n\n\nAhmed Abdel Aziz and Juma Al Kubaisi [2018]\n\nAhmed Abdel Aziz, A.M.D.,\nJuma Al Kubaisi, M.S.:\nTypes of causes in the ancient arabic prosodic lesson.(in arabic).\nJournal of Anbar University for Languages & Literature/Magallat Gami\u2019at Al-Anbar Li-Lugat Wa-al-Adabl\n(26)\n(2018)\n\n\n\nSowayan et al. [2010]\n\nSowayan, S.A.,\nAl-Shaibani, M.,\nAl-Zumar, S.:\nNabati Poetry The Taste of the People and The Authority of The Text (in Arabic).\nAbu Dhabi Authority for Culture and Heritage,\nAbu Dhabi\n(2010)\n\n\n\nAlabbas et al. [2014]\n\nAlabbas, M.,\nKhalaf, Z.A.,\nKhashan, K.M.:\nBasrah: an automatic system to identify the meter of arabic poetry.\nNatural Language Engineering\n20(1),\n131\u2013149\n(2014)\n\n\n\nAbuata and Al-Omari [2018]\n\nAbuata, B.,\nAl-Omari, A.:\nA rule-based algorithm for the detection of arud meter in classical arabic poetry.\nInternational Arab Journal of Information Technology\n15(4),\n1\u20135\n(2018)\n\n\n\nAlnagdawi et al. [2013]\n\nAlnagdawi, M.A.,\nRashideh, H.,\nAburumman, F.:\nFinding arabic poem meter using context free grammar.\nJournal of Communication and Computer Engineering\n3(1),\n52\u201359\n(2013)\n\n\n\nBerkani et al. [2020]\n\nBerkani, A.,\nHolzer, A.,\nStoffel, K.:\nPattern matching in meter detection of arabic classical poetry.\nIn: 2020 IEEE/ACS 17th International Conference on Computer Systems and Applications (AICCSA),\npp. 1\u20138\n(2020).\nIEEE\n\n\n\nYousef et al. [2019]\n\nYousef, W.A.,\nIbrahime, O.M.,\nMadbouly, T.M.,\nMahmoud, M.A.:\nLearning meters of arabic and english poems with recurrent neural networks: a step forward for language understanding and synthesis.\narXiv preprint arXiv:1905.05700\n(2019)\n\n\n\n[12]\n\nArabic Poem Comprehensive Dataset (APCD).\nhttps://hci-lab.github.io/LearningMetersPoems/\n\nAl-Shaibani et al. [2020]\n\nAl-Shaibani, M.S.,\nAlyafeai, Z.,\nAhmad, I.:\nMeter classification of arabic poems using deep bidirectional recurrent neural networks.\nPattern Recognition Letters\n136,\n1\u20137\n(2020)\n\n\n\nAbandah et al. [2022]\n\nAbandah, G.A.,\nKhedher, M.Z.,\nAbdel-Majeed, M.R.,\nMansour, H.M.,\nHulliel, S.F.,\nBisharat, L.M.:\nClassifying and diacritizing arabic poems using deep recurrent neural networks.\nJournal of King Saud University-Computer and Information Sciences\n34(6),\n3775\u20133788\n(2022)\n\n\n\nAbboushi and Azzeh [2023]\n\nAbboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "url": null
            }
        },
        {
            "3": {
                "title": "Atiq, A.:\nElm al-arud wal qafiah (in arabic).\nDar Alnahda, Beirut, Lebanon\n(1987)\n\n\n\nAhmed Abdel Aziz and Juma Al Kubaisi [2018]\n\nAhmed Abdel Aziz, A.M.D.,\nJuma Al Kubaisi, M.S.:\nTypes of causes in the ancient arabic prosodic lesson.(in arabic).\nJournal of Anbar University for Languages & Literature/Magallat Gami\u2019at Al-Anbar Li-Lugat Wa-al-Adabl\n(26)\n(2018)\n\n\n\nSowayan et al. [2010]\n\nSowayan, S.A.,\nAl-Shaibani, M.,\nAl-Zumar, S.:\nNabati Poetry The Taste of the People and The Authority of The Text (in Arabic).\nAbu Dhabi Authority for Culture and Heritage,\nAbu Dhabi\n(2010)\n\n\n\nAlabbas et al. [2014]\n\nAlabbas, M.,\nKhalaf, Z.A.,\nKhashan, K.M.:\nBasrah: an automatic system to identify the meter of arabic poetry.\nNatural Language Engineering\n20(1),\n131\u2013149\n(2014)\n\n\n\nAbuata and Al-Omari [2018]\n\nAbuata, B.,\nAl-Omari, A.:\nA rule-based algorithm for the detection of arud meter in classical arabic poetry.\nInternational Arab Journal of Information Technology\n15(4),\n1\u20135\n(2018)\n\n\n\nAlnagdawi et al. [2013]\n\nAlnagdawi, M.A.,\nRashideh, H.,\nAburumman, F.:\nFinding arabic poem meter using context free grammar.\nJournal of Communication and Computer Engineering\n3(1),\n52\u201359\n(2013)\n\n\n\nBerkani et al. [2020]\n\nBerkani, A.,\nHolzer, A.,\nStoffel, K.:\nPattern matching in meter detection of arabic classical poetry.\nIn: 2020 IEEE/ACS 17th International Conference on Computer Systems and Applications (AICCSA),\npp. 1\u20138\n(2020).\nIEEE\n\n\n\nYousef et al. [2019]\n\nYousef, W.A.,\nIbrahime, O.M.,\nMadbouly, T.M.,\nMahmoud, M.A.:\nLearning meters of arabic and english poems with recurrent neural networks: a step forward for language understanding and synthesis.\narXiv preprint arXiv:1905.05700\n(2019)\n\n\n\n[12]\n\nArabic Poem Comprehensive Dataset (APCD).\nhttps://hci-lab.github.io/LearningMetersPoems/\n\nAl-Shaibani et al. [2020]\n\nAl-Shaibani, M.S.,\nAlyafeai, Z.,\nAhmad, I.:\nMeter classification of arabic poems using deep bidirectional recurrent neural networks.\nPattern Recognition Letters\n136,\n1\u20137\n(2020)\n\n\n\nAbandah et al. [2022]\n\nAbandah, G.A.,\nKhedher, M.Z.,\nAbdel-Majeed, M.R.,\nMansour, H.M.,\nHulliel, S.F.,\nBisharat, L.M.:\nClassifying and diacritizing arabic poems using deep recurrent neural networks.\nJournal of King Saud University-Computer and Information Sciences\n34(6),\n3775\u20133788\n(2022)\n\n\n\nAbboushi and Azzeh [2023]\n\nAbboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "author": "Scott, H.:\nPegs, cords, and ghuls: Meter of classical arabic poetry.\nPhD thesis\n(2010)\n\n\n\nAtiq [1987]\n\nAtiq, A.:\nElm al-arud wal qafiah (in arabic).\nDar Alnahda, Beirut, Lebanon\n(1987)\n\n\n\nAhmed Abdel Aziz and Juma Al Kubaisi [2018]\n\nAhmed Abdel Aziz, A.M.D.,\nJuma Al Kubaisi, M.S.:\nTypes of causes in the ancient arabic prosodic lesson.(in arabic).\nJournal of Anbar University for Languages & Literature/Magallat Gami\u2019at Al-Anbar Li-Lugat Wa-al-Adabl\n(26)\n(2018)\n\n\n\nSowayan et al. [2010]\n\nSowayan, S.A.,\nAl-Shaibani, M.,\nAl-Zumar, S.:\nNabati Poetry The Taste of the People and The Authority of The Text (in Arabic).\nAbu Dhabi Authority for Culture and Heritage,\nAbu Dhabi\n(2010)\n\n\n\nAlabbas et al. [2014]\n\nAlabbas, M.,\nKhalaf, Z.A.,\nKhashan, K.M.:\nBasrah: an automatic system to identify the meter of arabic poetry.\nNatural Language Engineering\n20(1),\n131\u2013149\n(2014)\n\n\n\nAbuata and Al-Omari [2018]\n\nAbuata, B.,\nAl-Omari, A.:\nA rule-based algorithm for the detection of arud meter in classical arabic poetry.\nInternational Arab Journal of Information Technology\n15(4),\n1\u20135\n(2018)\n\n\n\nAlnagdawi et al. [2013]\n\nAlnagdawi, M.A.,\nRashideh, H.,\nAburumman, F.:\nFinding arabic poem meter using context free grammar.\nJournal of Communication and Computer Engineering\n3(1),\n52\u201359\n(2013)\n\n\n\nBerkani et al. [2020]\n\nBerkani, A.,\nHolzer, A.,\nStoffel, K.:\nPattern matching in meter detection of arabic classical poetry.\nIn: 2020 IEEE/ACS 17th International Conference on Computer Systems and Applications (AICCSA),\npp. 1\u20138\n(2020).\nIEEE\n\n\n\nYousef et al. [2019]\n\nYousef, W.A.,\nIbrahime, O.M.,\nMadbouly, T.M.,\nMahmoud, M.A.:\nLearning meters of arabic and english poems with recurrent neural networks: a step forward for language understanding and synthesis.\narXiv preprint arXiv:1905.05700\n(2019)\n\n\n\n[12]\n\nArabic Poem Comprehensive Dataset (APCD).\nhttps://hci-lab.github.io/LearningMetersPoems/\n\nAl-Shaibani et al. [2020]\n\nAl-Shaibani, M.S.,\nAlyafeai, Z.,\nAhmad, I.:\nMeter classification of arabic poems using deep bidirectional recurrent neural networks.\nPattern Recognition Letters\n136,\n1\u20137\n(2020)\n\n\n\nAbandah et al. [2022]\n\nAbandah, G.A.,\nKhedher, M.Z.,\nAbdel-Majeed, M.R.,\nMansour, H.M.,\nHulliel, S.F.,\nBisharat, L.M.:\nClassifying and diacritizing arabic poems using deep recurrent neural networks.\nJournal of King Saud University-Computer and Information Sciences\n34(6),\n3775\u20133788\n(2022)\n\n\n\nAbboushi and Azzeh [2023]\n\nAbboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "venue": "Ahmed Abdel Aziz, A.M.D.,\nJuma Al Kubaisi, M.S.:\nTypes of causes in the ancient arabic prosodic lesson.(in arabic).\nJournal of Anbar University for Languages & Literature/Magallat Gami\u2019at Al-Anbar Li-Lugat Wa-al-Adabl\n(26)\n(2018)\n\n\n\nSowayan et al. [2010]\n\nSowayan, S.A.,\nAl-Shaibani, M.,\nAl-Zumar, S.:\nNabati Poetry The Taste of the People and The Authority of The Text (in Arabic).\nAbu Dhabi Authority for Culture and Heritage,\nAbu Dhabi\n(2010)\n\n\n\nAlabbas et al. [2014]\n\nAlabbas, M.,\nKhalaf, Z.A.,\nKhashan, K.M.:\nBasrah: an automatic system to identify the meter of arabic poetry.\nNatural Language Engineering\n20(1),\n131\u2013149\n(2014)\n\n\n\nAbuata and Al-Omari [2018]\n\nAbuata, B.,\nAl-Omari, A.:\nA rule-based algorithm for the detection of arud meter in classical arabic poetry.\nInternational Arab Journal of Information Technology\n15(4),\n1\u20135\n(2018)\n\n\n\nAlnagdawi et al. [2013]\n\nAlnagdawi, M.A.,\nRashideh, H.,\nAburumman, F.:\nFinding arabic poem meter using context free grammar.\nJournal of Communication and Computer Engineering\n3(1),\n52\u201359\n(2013)\n\n\n\nBerkani et al. [2020]\n\nBerkani, A.,\nHolzer, A.,\nStoffel, K.:\nPattern matching in meter detection of arabic classical poetry.\nIn: 2020 IEEE/ACS 17th International Conference on Computer Systems and Applications (AICCSA),\npp. 1\u20138\n(2020).\nIEEE\n\n\n\nYousef et al. [2019]\n\nYousef, W.A.,\nIbrahime, O.M.,\nMadbouly, T.M.,\nMahmoud, M.A.:\nLearning meters of arabic and english poems with recurrent neural networks: a step forward for language understanding and synthesis.\narXiv preprint arXiv:1905.05700\n(2019)\n\n\n\n[12]\n\nArabic Poem Comprehensive Dataset (APCD).\nhttps://hci-lab.github.io/LearningMetersPoems/\n\nAl-Shaibani et al. [2020]\n\nAl-Shaibani, M.S.,\nAlyafeai, Z.,\nAhmad, I.:\nMeter classification of arabic poems using deep bidirectional recurrent neural networks.\nPattern Recognition Letters\n136,\n1\u20137\n(2020)\n\n\n\nAbandah et al. [2022]\n\nAbandah, G.A.,\nKhedher, M.Z.,\nAbdel-Majeed, M.R.,\nMansour, H.M.,\nHulliel, S.F.,\nBisharat, L.M.:\nClassifying and diacritizing arabic poems using deep recurrent neural networks.\nJournal of King Saud University-Computer and Information Sciences\n34(6),\n3775\u20133788\n(2022)\n\n\n\nAbboushi and Azzeh [2023]\n\nAbboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "url": null
            }
        },
        {
            "4": {
                "title": "Ahmed Abdel Aziz, A.M.D.,\nJuma Al Kubaisi, M.S.:\nTypes of causes in the ancient arabic prosodic lesson.(in arabic).\nJournal of Anbar University for Languages & Literature/Magallat Gami\u2019at Al-Anbar Li-Lugat Wa-al-Adabl\n(26)\n(2018)\n\n\n\nSowayan et al. [2010]\n\nSowayan, S.A.,\nAl-Shaibani, M.,\nAl-Zumar, S.:\nNabati Poetry The Taste of the People and The Authority of The Text (in Arabic).\nAbu Dhabi Authority for Culture and Heritage,\nAbu Dhabi\n(2010)\n\n\n\nAlabbas et al. [2014]\n\nAlabbas, M.,\nKhalaf, Z.A.,\nKhashan, K.M.:\nBasrah: an automatic system to identify the meter of arabic poetry.\nNatural Language Engineering\n20(1),\n131\u2013149\n(2014)\n\n\n\nAbuata and Al-Omari [2018]\n\nAbuata, B.,\nAl-Omari, A.:\nA rule-based algorithm for the detection of arud meter in classical arabic poetry.\nInternational Arab Journal of Information Technology\n15(4),\n1\u20135\n(2018)\n\n\n\nAlnagdawi et al. [2013]\n\nAlnagdawi, M.A.,\nRashideh, H.,\nAburumman, F.:\nFinding arabic poem meter using context free grammar.\nJournal of Communication and Computer Engineering\n3(1),\n52\u201359\n(2013)\n\n\n\nBerkani et al. [2020]\n\nBerkani, A.,\nHolzer, A.,\nStoffel, K.:\nPattern matching in meter detection of arabic classical poetry.\nIn: 2020 IEEE/ACS 17th International Conference on Computer Systems and Applications (AICCSA),\npp. 1\u20138\n(2020).\nIEEE\n\n\n\nYousef et al. [2019]\n\nYousef, W.A.,\nIbrahime, O.M.,\nMadbouly, T.M.,\nMahmoud, M.A.:\nLearning meters of arabic and english poems with recurrent neural networks: a step forward for language understanding and synthesis.\narXiv preprint arXiv:1905.05700\n(2019)\n\n\n\n[12]\n\nArabic Poem Comprehensive Dataset (APCD).\nhttps://hci-lab.github.io/LearningMetersPoems/\n\nAl-Shaibani et al. [2020]\n\nAl-Shaibani, M.S.,\nAlyafeai, Z.,\nAhmad, I.:\nMeter classification of arabic poems using deep bidirectional recurrent neural networks.\nPattern Recognition Letters\n136,\n1\u20137\n(2020)\n\n\n\nAbandah et al. [2022]\n\nAbandah, G.A.,\nKhedher, M.Z.,\nAbdel-Majeed, M.R.,\nMansour, H.M.,\nHulliel, S.F.,\nBisharat, L.M.:\nClassifying and diacritizing arabic poems using deep recurrent neural networks.\nJournal of King Saud University-Computer and Information Sciences\n34(6),\n3775\u20133788\n(2022)\n\n\n\nAbboushi and Azzeh [2023]\n\nAbboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "author": "Atiq, A.:\nElm al-arud wal qafiah (in arabic).\nDar Alnahda, Beirut, Lebanon\n(1987)\n\n\n\nAhmed Abdel Aziz and Juma Al Kubaisi [2018]\n\nAhmed Abdel Aziz, A.M.D.,\nJuma Al Kubaisi, M.S.:\nTypes of causes in the ancient arabic prosodic lesson.(in arabic).\nJournal of Anbar University for Languages & Literature/Magallat Gami\u2019at Al-Anbar Li-Lugat Wa-al-Adabl\n(26)\n(2018)\n\n\n\nSowayan et al. [2010]\n\nSowayan, S.A.,\nAl-Shaibani, M.,\nAl-Zumar, S.:\nNabati Poetry The Taste of the People and The Authority of The Text (in Arabic).\nAbu Dhabi Authority for Culture and Heritage,\nAbu Dhabi\n(2010)\n\n\n\nAlabbas et al. [2014]\n\nAlabbas, M.,\nKhalaf, Z.A.,\nKhashan, K.M.:\nBasrah: an automatic system to identify the meter of arabic poetry.\nNatural Language Engineering\n20(1),\n131\u2013149\n(2014)\n\n\n\nAbuata and Al-Omari [2018]\n\nAbuata, B.,\nAl-Omari, A.:\nA rule-based algorithm for the detection of arud meter in classical arabic poetry.\nInternational Arab Journal of Information Technology\n15(4),\n1\u20135\n(2018)\n\n\n\nAlnagdawi et al. [2013]\n\nAlnagdawi, M.A.,\nRashideh, H.,\nAburumman, F.:\nFinding arabic poem meter using context free grammar.\nJournal of Communication and Computer Engineering\n3(1),\n52\u201359\n(2013)\n\n\n\nBerkani et al. [2020]\n\nBerkani, A.,\nHolzer, A.,\nStoffel, K.:\nPattern matching in meter detection of arabic classical poetry.\nIn: 2020 IEEE/ACS 17th International Conference on Computer Systems and Applications (AICCSA),\npp. 1\u20138\n(2020).\nIEEE\n\n\n\nYousef et al. [2019]\n\nYousef, W.A.,\nIbrahime, O.M.,\nMadbouly, T.M.,\nMahmoud, M.A.:\nLearning meters of arabic and english poems with recurrent neural networks: a step forward for language understanding and synthesis.\narXiv preprint arXiv:1905.05700\n(2019)\n\n\n\n[12]\n\nArabic Poem Comprehensive Dataset (APCD).\nhttps://hci-lab.github.io/LearningMetersPoems/\n\nAl-Shaibani et al. [2020]\n\nAl-Shaibani, M.S.,\nAlyafeai, Z.,\nAhmad, I.:\nMeter classification of arabic poems using deep bidirectional recurrent neural networks.\nPattern Recognition Letters\n136,\n1\u20137\n(2020)\n\n\n\nAbandah et al. [2022]\n\nAbandah, G.A.,\nKhedher, M.Z.,\nAbdel-Majeed, M.R.,\nMansour, H.M.,\nHulliel, S.F.,\nBisharat, L.M.:\nClassifying and diacritizing arabic poems using deep recurrent neural networks.\nJournal of King Saud University-Computer and Information Sciences\n34(6),\n3775\u20133788\n(2022)\n\n\n\nAbboushi and Azzeh [2023]\n\nAbboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "venue": "Sowayan, S.A.,\nAl-Shaibani, M.,\nAl-Zumar, S.:\nNabati Poetry The Taste of the People and The Authority of The Text (in Arabic).\nAbu Dhabi Authority for Culture and Heritage,\nAbu Dhabi\n(2010)\n\n\n\nAlabbas et al. [2014]\n\nAlabbas, M.,\nKhalaf, Z.A.,\nKhashan, K.M.:\nBasrah: an automatic system to identify the meter of arabic poetry.\nNatural Language Engineering\n20(1),\n131\u2013149\n(2014)\n\n\n\nAbuata and Al-Omari [2018]\n\nAbuata, B.,\nAl-Omari, A.:\nA rule-based algorithm for the detection of arud meter in classical arabic poetry.\nInternational Arab Journal of Information Technology\n15(4),\n1\u20135\n(2018)\n\n\n\nAlnagdawi et al. [2013]\n\nAlnagdawi, M.A.,\nRashideh, H.,\nAburumman, F.:\nFinding arabic poem meter using context free grammar.\nJournal of Communication and Computer Engineering\n3(1),\n52\u201359\n(2013)\n\n\n\nBerkani et al. [2020]\n\nBerkani, A.,\nHolzer, A.,\nStoffel, K.:\nPattern matching in meter detection of arabic classical poetry.\nIn: 2020 IEEE/ACS 17th International Conference on Computer Systems and Applications (AICCSA),\npp. 1\u20138\n(2020).\nIEEE\n\n\n\nYousef et al. [2019]\n\nYousef, W.A.,\nIbrahime, O.M.,\nMadbouly, T.M.,\nMahmoud, M.A.:\nLearning meters of arabic and english poems with recurrent neural networks: a step forward for language understanding and synthesis.\narXiv preprint arXiv:1905.05700\n(2019)\n\n\n\n[12]\n\nArabic Poem Comprehensive Dataset (APCD).\nhttps://hci-lab.github.io/LearningMetersPoems/\n\nAl-Shaibani et al. [2020]\n\nAl-Shaibani, M.S.,\nAlyafeai, Z.,\nAhmad, I.:\nMeter classification of arabic poems using deep bidirectional recurrent neural networks.\nPattern Recognition Letters\n136,\n1\u20137\n(2020)\n\n\n\nAbandah et al. [2022]\n\nAbandah, G.A.,\nKhedher, M.Z.,\nAbdel-Majeed, M.R.,\nMansour, H.M.,\nHulliel, S.F.,\nBisharat, L.M.:\nClassifying and diacritizing arabic poems using deep recurrent neural networks.\nJournal of King Saud University-Computer and Information Sciences\n34(6),\n3775\u20133788\n(2022)\n\n\n\nAbboushi and Azzeh [2023]\n\nAbboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "url": null
            }
        },
        {
            "5": {
                "title": "Sowayan, S.A.,\nAl-Shaibani, M.,\nAl-Zumar, S.:\nNabati Poetry The Taste of the People and The Authority of The Text (in Arabic).\nAbu Dhabi Authority for Culture and Heritage,\nAbu Dhabi\n(2010)\n\n\n\nAlabbas et al. [2014]\n\nAlabbas, M.,\nKhalaf, Z.A.,\nKhashan, K.M.:\nBasrah: an automatic system to identify the meter of arabic poetry.\nNatural Language Engineering\n20(1),\n131\u2013149\n(2014)\n\n\n\nAbuata and Al-Omari [2018]\n\nAbuata, B.,\nAl-Omari, A.:\nA rule-based algorithm for the detection of arud meter in classical arabic poetry.\nInternational Arab Journal of Information Technology\n15(4),\n1\u20135\n(2018)\n\n\n\nAlnagdawi et al. [2013]\n\nAlnagdawi, M.A.,\nRashideh, H.,\nAburumman, F.:\nFinding arabic poem meter using context free grammar.\nJournal of Communication and Computer Engineering\n3(1),\n52\u201359\n(2013)\n\n\n\nBerkani et al. [2020]\n\nBerkani, A.,\nHolzer, A.,\nStoffel, K.:\nPattern matching in meter detection of arabic classical poetry.\nIn: 2020 IEEE/ACS 17th International Conference on Computer Systems and Applications (AICCSA),\npp. 1\u20138\n(2020).\nIEEE\n\n\n\nYousef et al. [2019]\n\nYousef, W.A.,\nIbrahime, O.M.,\nMadbouly, T.M.,\nMahmoud, M.A.:\nLearning meters of arabic and english poems with recurrent neural networks: a step forward for language understanding and synthesis.\narXiv preprint arXiv:1905.05700\n(2019)\n\n\n\n[12]\n\nArabic Poem Comprehensive Dataset (APCD).\nhttps://hci-lab.github.io/LearningMetersPoems/\n\nAl-Shaibani et al. [2020]\n\nAl-Shaibani, M.S.,\nAlyafeai, Z.,\nAhmad, I.:\nMeter classification of arabic poems using deep bidirectional recurrent neural networks.\nPattern Recognition Letters\n136,\n1\u20137\n(2020)\n\n\n\nAbandah et al. [2022]\n\nAbandah, G.A.,\nKhedher, M.Z.,\nAbdel-Majeed, M.R.,\nMansour, H.M.,\nHulliel, S.F.,\nBisharat, L.M.:\nClassifying and diacritizing arabic poems using deep recurrent neural networks.\nJournal of King Saud University-Computer and Information Sciences\n34(6),\n3775\u20133788\n(2022)\n\n\n\nAbboushi and Azzeh [2023]\n\nAbboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "author": "Ahmed Abdel Aziz, A.M.D.,\nJuma Al Kubaisi, M.S.:\nTypes of causes in the ancient arabic prosodic lesson.(in arabic).\nJournal of Anbar University for Languages & Literature/Magallat Gami\u2019at Al-Anbar Li-Lugat Wa-al-Adabl\n(26)\n(2018)\n\n\n\nSowayan et al. [2010]\n\nSowayan, S.A.,\nAl-Shaibani, M.,\nAl-Zumar, S.:\nNabati Poetry The Taste of the People and The Authority of The Text (in Arabic).\nAbu Dhabi Authority for Culture and Heritage,\nAbu Dhabi\n(2010)\n\n\n\nAlabbas et al. [2014]\n\nAlabbas, M.,\nKhalaf, Z.A.,\nKhashan, K.M.:\nBasrah: an automatic system to identify the meter of arabic poetry.\nNatural Language Engineering\n20(1),\n131\u2013149\n(2014)\n\n\n\nAbuata and Al-Omari [2018]\n\nAbuata, B.,\nAl-Omari, A.:\nA rule-based algorithm for the detection of arud meter in classical arabic poetry.\nInternational Arab Journal of Information Technology\n15(4),\n1\u20135\n(2018)\n\n\n\nAlnagdawi et al. [2013]\n\nAlnagdawi, M.A.,\nRashideh, H.,\nAburumman, F.:\nFinding arabic poem meter using context free grammar.\nJournal of Communication and Computer Engineering\n3(1),\n52\u201359\n(2013)\n\n\n\nBerkani et al. [2020]\n\nBerkani, A.,\nHolzer, A.,\nStoffel, K.:\nPattern matching in meter detection of arabic classical poetry.\nIn: 2020 IEEE/ACS 17th International Conference on Computer Systems and Applications (AICCSA),\npp. 1\u20138\n(2020).\nIEEE\n\n\n\nYousef et al. [2019]\n\nYousef, W.A.,\nIbrahime, O.M.,\nMadbouly, T.M.,\nMahmoud, M.A.:\nLearning meters of arabic and english poems with recurrent neural networks: a step forward for language understanding and synthesis.\narXiv preprint arXiv:1905.05700\n(2019)\n\n\n\n[12]\n\nArabic Poem Comprehensive Dataset (APCD).\nhttps://hci-lab.github.io/LearningMetersPoems/\n\nAl-Shaibani et al. [2020]\n\nAl-Shaibani, M.S.,\nAlyafeai, Z.,\nAhmad, I.:\nMeter classification of arabic poems using deep bidirectional recurrent neural networks.\nPattern Recognition Letters\n136,\n1\u20137\n(2020)\n\n\n\nAbandah et al. [2022]\n\nAbandah, G.A.,\nKhedher, M.Z.,\nAbdel-Majeed, M.R.,\nMansour, H.M.,\nHulliel, S.F.,\nBisharat, L.M.:\nClassifying and diacritizing arabic poems using deep recurrent neural networks.\nJournal of King Saud University-Computer and Information Sciences\n34(6),\n3775\u20133788\n(2022)\n\n\n\nAbboushi and Azzeh [2023]\n\nAbboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "venue": "Alabbas, M.,\nKhalaf, Z.A.,\nKhashan, K.M.:\nBasrah: an automatic system to identify the meter of arabic poetry.\nNatural Language Engineering\n20(1),\n131\u2013149\n(2014)\n\n\n\nAbuata and Al-Omari [2018]\n\nAbuata, B.,\nAl-Omari, A.:\nA rule-based algorithm for the detection of arud meter in classical arabic poetry.\nInternational Arab Journal of Information Technology\n15(4),\n1\u20135\n(2018)\n\n\n\nAlnagdawi et al. [2013]\n\nAlnagdawi, M.A.,\nRashideh, H.,\nAburumman, F.:\nFinding arabic poem meter using context free grammar.\nJournal of Communication and Computer Engineering\n3(1),\n52\u201359\n(2013)\n\n\n\nBerkani et al. [2020]\n\nBerkani, A.,\nHolzer, A.,\nStoffel, K.:\nPattern matching in meter detection of arabic classical poetry.\nIn: 2020 IEEE/ACS 17th International Conference on Computer Systems and Applications (AICCSA),\npp. 1\u20138\n(2020).\nIEEE\n\n\n\nYousef et al. [2019]\n\nYousef, W.A.,\nIbrahime, O.M.,\nMadbouly, T.M.,\nMahmoud, M.A.:\nLearning meters of arabic and english poems with recurrent neural networks: a step forward for language understanding and synthesis.\narXiv preprint arXiv:1905.05700\n(2019)\n\n\n\n[12]\n\nArabic Poem Comprehensive Dataset (APCD).\nhttps://hci-lab.github.io/LearningMetersPoems/\n\nAl-Shaibani et al. [2020]\n\nAl-Shaibani, M.S.,\nAlyafeai, Z.,\nAhmad, I.:\nMeter classification of arabic poems using deep bidirectional recurrent neural networks.\nPattern Recognition Letters\n136,\n1\u20137\n(2020)\n\n\n\nAbandah et al. [2022]\n\nAbandah, G.A.,\nKhedher, M.Z.,\nAbdel-Majeed, M.R.,\nMansour, H.M.,\nHulliel, S.F.,\nBisharat, L.M.:\nClassifying and diacritizing arabic poems using deep recurrent neural networks.\nJournal of King Saud University-Computer and Information Sciences\n34(6),\n3775\u20133788\n(2022)\n\n\n\nAbboushi and Azzeh [2023]\n\nAbboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "url": null
            }
        },
        {
            "6": {
                "title": "Alabbas, M.,\nKhalaf, Z.A.,\nKhashan, K.M.:\nBasrah: an automatic system to identify the meter of arabic poetry.\nNatural Language Engineering\n20(1),\n131\u2013149\n(2014)\n\n\n\nAbuata and Al-Omari [2018]\n\nAbuata, B.,\nAl-Omari, A.:\nA rule-based algorithm for the detection of arud meter in classical arabic poetry.\nInternational Arab Journal of Information Technology\n15(4),\n1\u20135\n(2018)\n\n\n\nAlnagdawi et al. [2013]\n\nAlnagdawi, M.A.,\nRashideh, H.,\nAburumman, F.:\nFinding arabic poem meter using context free grammar.\nJournal of Communication and Computer Engineering\n3(1),\n52\u201359\n(2013)\n\n\n\nBerkani et al. [2020]\n\nBerkani, A.,\nHolzer, A.,\nStoffel, K.:\nPattern matching in meter detection of arabic classical poetry.\nIn: 2020 IEEE/ACS 17th International Conference on Computer Systems and Applications (AICCSA),\npp. 1\u20138\n(2020).\nIEEE\n\n\n\nYousef et al. [2019]\n\nYousef, W.A.,\nIbrahime, O.M.,\nMadbouly, T.M.,\nMahmoud, M.A.:\nLearning meters of arabic and english poems with recurrent neural networks: a step forward for language understanding and synthesis.\narXiv preprint arXiv:1905.05700\n(2019)\n\n\n\n[12]\n\nArabic Poem Comprehensive Dataset (APCD).\nhttps://hci-lab.github.io/LearningMetersPoems/\n\nAl-Shaibani et al. [2020]\n\nAl-Shaibani, M.S.,\nAlyafeai, Z.,\nAhmad, I.:\nMeter classification of arabic poems using deep bidirectional recurrent neural networks.\nPattern Recognition Letters\n136,\n1\u20137\n(2020)\n\n\n\nAbandah et al. [2022]\n\nAbandah, G.A.,\nKhedher, M.Z.,\nAbdel-Majeed, M.R.,\nMansour, H.M.,\nHulliel, S.F.,\nBisharat, L.M.:\nClassifying and diacritizing arabic poems using deep recurrent neural networks.\nJournal of King Saud University-Computer and Information Sciences\n34(6),\n3775\u20133788\n(2022)\n\n\n\nAbboushi and Azzeh [2023]\n\nAbboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "author": "Sowayan, S.A.,\nAl-Shaibani, M.,\nAl-Zumar, S.:\nNabati Poetry The Taste of the People and The Authority of The Text (in Arabic).\nAbu Dhabi Authority for Culture and Heritage,\nAbu Dhabi\n(2010)\n\n\n\nAlabbas et al. [2014]\n\nAlabbas, M.,\nKhalaf, Z.A.,\nKhashan, K.M.:\nBasrah: an automatic system to identify the meter of arabic poetry.\nNatural Language Engineering\n20(1),\n131\u2013149\n(2014)\n\n\n\nAbuata and Al-Omari [2018]\n\nAbuata, B.,\nAl-Omari, A.:\nA rule-based algorithm for the detection of arud meter in classical arabic poetry.\nInternational Arab Journal of Information Technology\n15(4),\n1\u20135\n(2018)\n\n\n\nAlnagdawi et al. [2013]\n\nAlnagdawi, M.A.,\nRashideh, H.,\nAburumman, F.:\nFinding arabic poem meter using context free grammar.\nJournal of Communication and Computer Engineering\n3(1),\n52\u201359\n(2013)\n\n\n\nBerkani et al. [2020]\n\nBerkani, A.,\nHolzer, A.,\nStoffel, K.:\nPattern matching in meter detection of arabic classical poetry.\nIn: 2020 IEEE/ACS 17th International Conference on Computer Systems and Applications (AICCSA),\npp. 1\u20138\n(2020).\nIEEE\n\n\n\nYousef et al. [2019]\n\nYousef, W.A.,\nIbrahime, O.M.,\nMadbouly, T.M.,\nMahmoud, M.A.:\nLearning meters of arabic and english poems with recurrent neural networks: a step forward for language understanding and synthesis.\narXiv preprint arXiv:1905.05700\n(2019)\n\n\n\n[12]\n\nArabic Poem Comprehensive Dataset (APCD).\nhttps://hci-lab.github.io/LearningMetersPoems/\n\nAl-Shaibani et al. [2020]\n\nAl-Shaibani, M.S.,\nAlyafeai, Z.,\nAhmad, I.:\nMeter classification of arabic poems using deep bidirectional recurrent neural networks.\nPattern Recognition Letters\n136,\n1\u20137\n(2020)\n\n\n\nAbandah et al. [2022]\n\nAbandah, G.A.,\nKhedher, M.Z.,\nAbdel-Majeed, M.R.,\nMansour, H.M.,\nHulliel, S.F.,\nBisharat, L.M.:\nClassifying and diacritizing arabic poems using deep recurrent neural networks.\nJournal of King Saud University-Computer and Information Sciences\n34(6),\n3775\u20133788\n(2022)\n\n\n\nAbboushi and Azzeh [2023]\n\nAbboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "venue": "Abuata, B.,\nAl-Omari, A.:\nA rule-based algorithm for the detection of arud meter in classical arabic poetry.\nInternational Arab Journal of Information Technology\n15(4),\n1\u20135\n(2018)\n\n\n\nAlnagdawi et al. [2013]\n\nAlnagdawi, M.A.,\nRashideh, H.,\nAburumman, F.:\nFinding arabic poem meter using context free grammar.\nJournal of Communication and Computer Engineering\n3(1),\n52\u201359\n(2013)\n\n\n\nBerkani et al. [2020]\n\nBerkani, A.,\nHolzer, A.,\nStoffel, K.:\nPattern matching in meter detection of arabic classical poetry.\nIn: 2020 IEEE/ACS 17th International Conference on Computer Systems and Applications (AICCSA),\npp. 1\u20138\n(2020).\nIEEE\n\n\n\nYousef et al. [2019]\n\nYousef, W.A.,\nIbrahime, O.M.,\nMadbouly, T.M.,\nMahmoud, M.A.:\nLearning meters of arabic and english poems with recurrent neural networks: a step forward for language understanding and synthesis.\narXiv preprint arXiv:1905.05700\n(2019)\n\n\n\n[12]\n\nArabic Poem Comprehensive Dataset (APCD).\nhttps://hci-lab.github.io/LearningMetersPoems/\n\nAl-Shaibani et al. [2020]\n\nAl-Shaibani, M.S.,\nAlyafeai, Z.,\nAhmad, I.:\nMeter classification of arabic poems using deep bidirectional recurrent neural networks.\nPattern Recognition Letters\n136,\n1\u20137\n(2020)\n\n\n\nAbandah et al. [2022]\n\nAbandah, G.A.,\nKhedher, M.Z.,\nAbdel-Majeed, M.R.,\nMansour, H.M.,\nHulliel, S.F.,\nBisharat, L.M.:\nClassifying and diacritizing arabic poems using deep recurrent neural networks.\nJournal of King Saud University-Computer and Information Sciences\n34(6),\n3775\u20133788\n(2022)\n\n\n\nAbboushi and Azzeh [2023]\n\nAbboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "url": null
            }
        },
        {
            "7": {
                "title": "Abuata, B.,\nAl-Omari, A.:\nA rule-based algorithm for the detection of arud meter in classical arabic poetry.\nInternational Arab Journal of Information Technology\n15(4),\n1\u20135\n(2018)\n\n\n\nAlnagdawi et al. [2013]\n\nAlnagdawi, M.A.,\nRashideh, H.,\nAburumman, F.:\nFinding arabic poem meter using context free grammar.\nJournal of Communication and Computer Engineering\n3(1),\n52\u201359\n(2013)\n\n\n\nBerkani et al. [2020]\n\nBerkani, A.,\nHolzer, A.,\nStoffel, K.:\nPattern matching in meter detection of arabic classical poetry.\nIn: 2020 IEEE/ACS 17th International Conference on Computer Systems and Applications (AICCSA),\npp. 1\u20138\n(2020).\nIEEE\n\n\n\nYousef et al. [2019]\n\nYousef, W.A.,\nIbrahime, O.M.,\nMadbouly, T.M.,\nMahmoud, M.A.:\nLearning meters of arabic and english poems with recurrent neural networks: a step forward for language understanding and synthesis.\narXiv preprint arXiv:1905.05700\n(2019)\n\n\n\n[12]\n\nArabic Poem Comprehensive Dataset (APCD).\nhttps://hci-lab.github.io/LearningMetersPoems/\n\nAl-Shaibani et al. [2020]\n\nAl-Shaibani, M.S.,\nAlyafeai, Z.,\nAhmad, I.:\nMeter classification of arabic poems using deep bidirectional recurrent neural networks.\nPattern Recognition Letters\n136,\n1\u20137\n(2020)\n\n\n\nAbandah et al. [2022]\n\nAbandah, G.A.,\nKhedher, M.Z.,\nAbdel-Majeed, M.R.,\nMansour, H.M.,\nHulliel, S.F.,\nBisharat, L.M.:\nClassifying and diacritizing arabic poems using deep recurrent neural networks.\nJournal of King Saud University-Computer and Information Sciences\n34(6),\n3775\u20133788\n(2022)\n\n\n\nAbboushi and Azzeh [2023]\n\nAbboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "author": "Alabbas, M.,\nKhalaf, Z.A.,\nKhashan, K.M.:\nBasrah: an automatic system to identify the meter of arabic poetry.\nNatural Language Engineering\n20(1),\n131\u2013149\n(2014)\n\n\n\nAbuata and Al-Omari [2018]\n\nAbuata, B.,\nAl-Omari, A.:\nA rule-based algorithm for the detection of arud meter in classical arabic poetry.\nInternational Arab Journal of Information Technology\n15(4),\n1\u20135\n(2018)\n\n\n\nAlnagdawi et al. [2013]\n\nAlnagdawi, M.A.,\nRashideh, H.,\nAburumman, F.:\nFinding arabic poem meter using context free grammar.\nJournal of Communication and Computer Engineering\n3(1),\n52\u201359\n(2013)\n\n\n\nBerkani et al. [2020]\n\nBerkani, A.,\nHolzer, A.,\nStoffel, K.:\nPattern matching in meter detection of arabic classical poetry.\nIn: 2020 IEEE/ACS 17th International Conference on Computer Systems and Applications (AICCSA),\npp. 1\u20138\n(2020).\nIEEE\n\n\n\nYousef et al. [2019]\n\nYousef, W.A.,\nIbrahime, O.M.,\nMadbouly, T.M.,\nMahmoud, M.A.:\nLearning meters of arabic and english poems with recurrent neural networks: a step forward for language understanding and synthesis.\narXiv preprint arXiv:1905.05700\n(2019)\n\n\n\n[12]\n\nArabic Poem Comprehensive Dataset (APCD).\nhttps://hci-lab.github.io/LearningMetersPoems/\n\nAl-Shaibani et al. [2020]\n\nAl-Shaibani, M.S.,\nAlyafeai, Z.,\nAhmad, I.:\nMeter classification of arabic poems using deep bidirectional recurrent neural networks.\nPattern Recognition Letters\n136,\n1\u20137\n(2020)\n\n\n\nAbandah et al. [2022]\n\nAbandah, G.A.,\nKhedher, M.Z.,\nAbdel-Majeed, M.R.,\nMansour, H.M.,\nHulliel, S.F.,\nBisharat, L.M.:\nClassifying and diacritizing arabic poems using deep recurrent neural networks.\nJournal of King Saud University-Computer and Information Sciences\n34(6),\n3775\u20133788\n(2022)\n\n\n\nAbboushi and Azzeh [2023]\n\nAbboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "venue": "Alnagdawi, M.A.,\nRashideh, H.,\nAburumman, F.:\nFinding arabic poem meter using context free grammar.\nJournal of Communication and Computer Engineering\n3(1),\n52\u201359\n(2013)\n\n\n\nBerkani et al. [2020]\n\nBerkani, A.,\nHolzer, A.,\nStoffel, K.:\nPattern matching in meter detection of arabic classical poetry.\nIn: 2020 IEEE/ACS 17th International Conference on Computer Systems and Applications (AICCSA),\npp. 1\u20138\n(2020).\nIEEE\n\n\n\nYousef et al. [2019]\n\nYousef, W.A.,\nIbrahime, O.M.,\nMadbouly, T.M.,\nMahmoud, M.A.:\nLearning meters of arabic and english poems with recurrent neural networks: a step forward for language understanding and synthesis.\narXiv preprint arXiv:1905.05700\n(2019)\n\n\n\n[12]\n\nArabic Poem Comprehensive Dataset (APCD).\nhttps://hci-lab.github.io/LearningMetersPoems/\n\nAl-Shaibani et al. [2020]\n\nAl-Shaibani, M.S.,\nAlyafeai, Z.,\nAhmad, I.:\nMeter classification of arabic poems using deep bidirectional recurrent neural networks.\nPattern Recognition Letters\n136,\n1\u20137\n(2020)\n\n\n\nAbandah et al. [2022]\n\nAbandah, G.A.,\nKhedher, M.Z.,\nAbdel-Majeed, M.R.,\nMansour, H.M.,\nHulliel, S.F.,\nBisharat, L.M.:\nClassifying and diacritizing arabic poems using deep recurrent neural networks.\nJournal of King Saud University-Computer and Information Sciences\n34(6),\n3775\u20133788\n(2022)\n\n\n\nAbboushi and Azzeh [2023]\n\nAbboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "url": null
            }
        },
        {
            "8": {
                "title": "Alnagdawi, M.A.,\nRashideh, H.,\nAburumman, F.:\nFinding arabic poem meter using context free grammar.\nJournal of Communication and Computer Engineering\n3(1),\n52\u201359\n(2013)\n\n\n\nBerkani et al. [2020]\n\nBerkani, A.,\nHolzer, A.,\nStoffel, K.:\nPattern matching in meter detection of arabic classical poetry.\nIn: 2020 IEEE/ACS 17th International Conference on Computer Systems and Applications (AICCSA),\npp. 1\u20138\n(2020).\nIEEE\n\n\n\nYousef et al. [2019]\n\nYousef, W.A.,\nIbrahime, O.M.,\nMadbouly, T.M.,\nMahmoud, M.A.:\nLearning meters of arabic and english poems with recurrent neural networks: a step forward for language understanding and synthesis.\narXiv preprint arXiv:1905.05700\n(2019)\n\n\n\n[12]\n\nArabic Poem Comprehensive Dataset (APCD).\nhttps://hci-lab.github.io/LearningMetersPoems/\n\nAl-Shaibani et al. [2020]\n\nAl-Shaibani, M.S.,\nAlyafeai, Z.,\nAhmad, I.:\nMeter classification of arabic poems using deep bidirectional recurrent neural networks.\nPattern Recognition Letters\n136,\n1\u20137\n(2020)\n\n\n\nAbandah et al. [2022]\n\nAbandah, G.A.,\nKhedher, M.Z.,\nAbdel-Majeed, M.R.,\nMansour, H.M.,\nHulliel, S.F.,\nBisharat, L.M.:\nClassifying and diacritizing arabic poems using deep recurrent neural networks.\nJournal of King Saud University-Computer and Information Sciences\n34(6),\n3775\u20133788\n(2022)\n\n\n\nAbboushi and Azzeh [2023]\n\nAbboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "author": "Abuata, B.,\nAl-Omari, A.:\nA rule-based algorithm for the detection of arud meter in classical arabic poetry.\nInternational Arab Journal of Information Technology\n15(4),\n1\u20135\n(2018)\n\n\n\nAlnagdawi et al. [2013]\n\nAlnagdawi, M.A.,\nRashideh, H.,\nAburumman, F.:\nFinding arabic poem meter using context free grammar.\nJournal of Communication and Computer Engineering\n3(1),\n52\u201359\n(2013)\n\n\n\nBerkani et al. [2020]\n\nBerkani, A.,\nHolzer, A.,\nStoffel, K.:\nPattern matching in meter detection of arabic classical poetry.\nIn: 2020 IEEE/ACS 17th International Conference on Computer Systems and Applications (AICCSA),\npp. 1\u20138\n(2020).\nIEEE\n\n\n\nYousef et al. [2019]\n\nYousef, W.A.,\nIbrahime, O.M.,\nMadbouly, T.M.,\nMahmoud, M.A.:\nLearning meters of arabic and english poems with recurrent neural networks: a step forward for language understanding and synthesis.\narXiv preprint arXiv:1905.05700\n(2019)\n\n\n\n[12]\n\nArabic Poem Comprehensive Dataset (APCD).\nhttps://hci-lab.github.io/LearningMetersPoems/\n\nAl-Shaibani et al. [2020]\n\nAl-Shaibani, M.S.,\nAlyafeai, Z.,\nAhmad, I.:\nMeter classification of arabic poems using deep bidirectional recurrent neural networks.\nPattern Recognition Letters\n136,\n1\u20137\n(2020)\n\n\n\nAbandah et al. [2022]\n\nAbandah, G.A.,\nKhedher, M.Z.,\nAbdel-Majeed, M.R.,\nMansour, H.M.,\nHulliel, S.F.,\nBisharat, L.M.:\nClassifying and diacritizing arabic poems using deep recurrent neural networks.\nJournal of King Saud University-Computer and Information Sciences\n34(6),\n3775\u20133788\n(2022)\n\n\n\nAbboushi and Azzeh [2023]\n\nAbboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "venue": "Berkani, A.,\nHolzer, A.,\nStoffel, K.:\nPattern matching in meter detection of arabic classical poetry.\nIn: 2020 IEEE/ACS 17th International Conference on Computer Systems and Applications (AICCSA),\npp. 1\u20138\n(2020).\nIEEE\n\n\n\nYousef et al. [2019]\n\nYousef, W.A.,\nIbrahime, O.M.,\nMadbouly, T.M.,\nMahmoud, M.A.:\nLearning meters of arabic and english poems with recurrent neural networks: a step forward for language understanding and synthesis.\narXiv preprint arXiv:1905.05700\n(2019)\n\n\n\n[12]\n\nArabic Poem Comprehensive Dataset (APCD).\nhttps://hci-lab.github.io/LearningMetersPoems/\n\nAl-Shaibani et al. [2020]\n\nAl-Shaibani, M.S.,\nAlyafeai, Z.,\nAhmad, I.:\nMeter classification of arabic poems using deep bidirectional recurrent neural networks.\nPattern Recognition Letters\n136,\n1\u20137\n(2020)\n\n\n\nAbandah et al. [2022]\n\nAbandah, G.A.,\nKhedher, M.Z.,\nAbdel-Majeed, M.R.,\nMansour, H.M.,\nHulliel, S.F.,\nBisharat, L.M.:\nClassifying and diacritizing arabic poems using deep recurrent neural networks.\nJournal of King Saud University-Computer and Information Sciences\n34(6),\n3775\u20133788\n(2022)\n\n\n\nAbboushi and Azzeh [2023]\n\nAbboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "url": null
            }
        },
        {
            "9": {
                "title": "Berkani, A.,\nHolzer, A.,\nStoffel, K.:\nPattern matching in meter detection of arabic classical poetry.\nIn: 2020 IEEE/ACS 17th International Conference on Computer Systems and Applications (AICCSA),\npp. 1\u20138\n(2020).\nIEEE\n\n\n\nYousef et al. [2019]\n\nYousef, W.A.,\nIbrahime, O.M.,\nMadbouly, T.M.,\nMahmoud, M.A.:\nLearning meters of arabic and english poems with recurrent neural networks: a step forward for language understanding and synthesis.\narXiv preprint arXiv:1905.05700\n(2019)\n\n\n\n[12]\n\nArabic Poem Comprehensive Dataset (APCD).\nhttps://hci-lab.github.io/LearningMetersPoems/\n\nAl-Shaibani et al. [2020]\n\nAl-Shaibani, M.S.,\nAlyafeai, Z.,\nAhmad, I.:\nMeter classification of arabic poems using deep bidirectional recurrent neural networks.\nPattern Recognition Letters\n136,\n1\u20137\n(2020)\n\n\n\nAbandah et al. [2022]\n\nAbandah, G.A.,\nKhedher, M.Z.,\nAbdel-Majeed, M.R.,\nMansour, H.M.,\nHulliel, S.F.,\nBisharat, L.M.:\nClassifying and diacritizing arabic poems using deep recurrent neural networks.\nJournal of King Saud University-Computer and Information Sciences\n34(6),\n3775\u20133788\n(2022)\n\n\n\nAbboushi and Azzeh [2023]\n\nAbboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "author": "Alnagdawi, M.A.,\nRashideh, H.,\nAburumman, F.:\nFinding arabic poem meter using context free grammar.\nJournal of Communication and Computer Engineering\n3(1),\n52\u201359\n(2013)\n\n\n\nBerkani et al. [2020]\n\nBerkani, A.,\nHolzer, A.,\nStoffel, K.:\nPattern matching in meter detection of arabic classical poetry.\nIn: 2020 IEEE/ACS 17th International Conference on Computer Systems and Applications (AICCSA),\npp. 1\u20138\n(2020).\nIEEE\n\n\n\nYousef et al. [2019]\n\nYousef, W.A.,\nIbrahime, O.M.,\nMadbouly, T.M.,\nMahmoud, M.A.:\nLearning meters of arabic and english poems with recurrent neural networks: a step forward for language understanding and synthesis.\narXiv preprint arXiv:1905.05700\n(2019)\n\n\n\n[12]\n\nArabic Poem Comprehensive Dataset (APCD).\nhttps://hci-lab.github.io/LearningMetersPoems/\n\nAl-Shaibani et al. [2020]\n\nAl-Shaibani, M.S.,\nAlyafeai, Z.,\nAhmad, I.:\nMeter classification of arabic poems using deep bidirectional recurrent neural networks.\nPattern Recognition Letters\n136,\n1\u20137\n(2020)\n\n\n\nAbandah et al. [2022]\n\nAbandah, G.A.,\nKhedher, M.Z.,\nAbdel-Majeed, M.R.,\nMansour, H.M.,\nHulliel, S.F.,\nBisharat, L.M.:\nClassifying and diacritizing arabic poems using deep recurrent neural networks.\nJournal of King Saud University-Computer and Information Sciences\n34(6),\n3775\u20133788\n(2022)\n\n\n\nAbboushi and Azzeh [2023]\n\nAbboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "venue": "Yousef, W.A.,\nIbrahime, O.M.,\nMadbouly, T.M.,\nMahmoud, M.A.:\nLearning meters of arabic and english poems with recurrent neural networks: a step forward for language understanding and synthesis.\narXiv preprint arXiv:1905.05700\n(2019)\n\n\n\n[12]\n\nArabic Poem Comprehensive Dataset (APCD).\nhttps://hci-lab.github.io/LearningMetersPoems/\n\nAl-Shaibani et al. [2020]\n\nAl-Shaibani, M.S.,\nAlyafeai, Z.,\nAhmad, I.:\nMeter classification of arabic poems using deep bidirectional recurrent neural networks.\nPattern Recognition Letters\n136,\n1\u20137\n(2020)\n\n\n\nAbandah et al. [2022]\n\nAbandah, G.A.,\nKhedher, M.Z.,\nAbdel-Majeed, M.R.,\nMansour, H.M.,\nHulliel, S.F.,\nBisharat, L.M.:\nClassifying and diacritizing arabic poems using deep recurrent neural networks.\nJournal of King Saud University-Computer and Information Sciences\n34(6),\n3775\u20133788\n(2022)\n\n\n\nAbboushi and Azzeh [2023]\n\nAbboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "url": null
            }
        },
        {
            "10": {
                "title": "Yousef, W.A.,\nIbrahime, O.M.,\nMadbouly, T.M.,\nMahmoud, M.A.:\nLearning meters of arabic and english poems with recurrent neural networks: a step forward for language understanding and synthesis.\narXiv preprint arXiv:1905.05700\n(2019)\n\n\n\n[12]\n\nArabic Poem Comprehensive Dataset (APCD).\nhttps://hci-lab.github.io/LearningMetersPoems/\n\nAl-Shaibani et al. [2020]\n\nAl-Shaibani, M.S.,\nAlyafeai, Z.,\nAhmad, I.:\nMeter classification of arabic poems using deep bidirectional recurrent neural networks.\nPattern Recognition Letters\n136,\n1\u20137\n(2020)\n\n\n\nAbandah et al. [2022]\n\nAbandah, G.A.,\nKhedher, M.Z.,\nAbdel-Majeed, M.R.,\nMansour, H.M.,\nHulliel, S.F.,\nBisharat, L.M.:\nClassifying and diacritizing arabic poems using deep recurrent neural networks.\nJournal of King Saud University-Computer and Information Sciences\n34(6),\n3775\u20133788\n(2022)\n\n\n\nAbboushi and Azzeh [2023]\n\nAbboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "author": "Berkani, A.,\nHolzer, A.,\nStoffel, K.:\nPattern matching in meter detection of arabic classical poetry.\nIn: 2020 IEEE/ACS 17th International Conference on Computer Systems and Applications (AICCSA),\npp. 1\u20138\n(2020).\nIEEE\n\n\n\nYousef et al. [2019]\n\nYousef, W.A.,\nIbrahime, O.M.,\nMadbouly, T.M.,\nMahmoud, M.A.:\nLearning meters of arabic and english poems with recurrent neural networks: a step forward for language understanding and synthesis.\narXiv preprint arXiv:1905.05700\n(2019)\n\n\n\n[12]\n\nArabic Poem Comprehensive Dataset (APCD).\nhttps://hci-lab.github.io/LearningMetersPoems/\n\nAl-Shaibani et al. [2020]\n\nAl-Shaibani, M.S.,\nAlyafeai, Z.,\nAhmad, I.:\nMeter classification of arabic poems using deep bidirectional recurrent neural networks.\nPattern Recognition Letters\n136,\n1\u20137\n(2020)\n\n\n\nAbandah et al. [2022]\n\nAbandah, G.A.,\nKhedher, M.Z.,\nAbdel-Majeed, M.R.,\nMansour, H.M.,\nHulliel, S.F.,\nBisharat, L.M.:\nClassifying and diacritizing arabic poems using deep recurrent neural networks.\nJournal of King Saud University-Computer and Information Sciences\n34(6),\n3775\u20133788\n(2022)\n\n\n\nAbboushi and Azzeh [2023]\n\nAbboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "venue": "Arabic Poem Comprehensive Dataset (APCD).\nhttps://hci-lab.github.io/LearningMetersPoems/\n\nAl-Shaibani et al. [2020]\n\nAl-Shaibani, M.S.,\nAlyafeai, Z.,\nAhmad, I.:\nMeter classification of arabic poems using deep bidirectional recurrent neural networks.\nPattern Recognition Letters\n136,\n1\u20137\n(2020)\n\n\n\nAbandah et al. [2022]\n\nAbandah, G.A.,\nKhedher, M.Z.,\nAbdel-Majeed, M.R.,\nMansour, H.M.,\nHulliel, S.F.,\nBisharat, L.M.:\nClassifying and diacritizing arabic poems using deep recurrent neural networks.\nJournal of King Saud University-Computer and Information Sciences\n34(6),\n3775\u20133788\n(2022)\n\n\n\nAbboushi and Azzeh [2023]\n\nAbboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "url": null
            }
        },
        {
            "11": {
                "title": "Arabic Poem Comprehensive Dataset (APCD).\nhttps://hci-lab.github.io/LearningMetersPoems/\n\nAl-Shaibani et al. [2020]\n\nAl-Shaibani, M.S.,\nAlyafeai, Z.,\nAhmad, I.:\nMeter classification of arabic poems using deep bidirectional recurrent neural networks.\nPattern Recognition Letters\n136,\n1\u20137\n(2020)\n\n\n\nAbandah et al. [2022]\n\nAbandah, G.A.,\nKhedher, M.Z.,\nAbdel-Majeed, M.R.,\nMansour, H.M.,\nHulliel, S.F.,\nBisharat, L.M.:\nClassifying and diacritizing arabic poems using deep recurrent neural networks.\nJournal of King Saud University-Computer and Information Sciences\n34(6),\n3775\u20133788\n(2022)\n\n\n\nAbboushi and Azzeh [2023]\n\nAbboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "author": "Yousef, W.A.,\nIbrahime, O.M.,\nMadbouly, T.M.,\nMahmoud, M.A.:\nLearning meters of arabic and english poems with recurrent neural networks: a step forward for language understanding and synthesis.\narXiv preprint arXiv:1905.05700\n(2019)\n\n\n\n[12]\n\nArabic Poem Comprehensive Dataset (APCD).\nhttps://hci-lab.github.io/LearningMetersPoems/\n\nAl-Shaibani et al. [2020]\n\nAl-Shaibani, M.S.,\nAlyafeai, Z.,\nAhmad, I.:\nMeter classification of arabic poems using deep bidirectional recurrent neural networks.\nPattern Recognition Letters\n136,\n1\u20137\n(2020)\n\n\n\nAbandah et al. [2022]\n\nAbandah, G.A.,\nKhedher, M.Z.,\nAbdel-Majeed, M.R.,\nMansour, H.M.,\nHulliel, S.F.,\nBisharat, L.M.:\nClassifying and diacritizing arabic poems using deep recurrent neural networks.\nJournal of King Saud University-Computer and Information Sciences\n34(6),\n3775\u20133788\n(2022)\n\n\n\nAbboushi and Azzeh [2023]\n\nAbboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "venue": "Al-Shaibani, M.S.,\nAlyafeai, Z.,\nAhmad, I.:\nMeter classification of arabic poems using deep bidirectional recurrent neural networks.\nPattern Recognition Letters\n136,\n1\u20137\n(2020)\n\n\n\nAbandah et al. [2022]\n\nAbandah, G.A.,\nKhedher, M.Z.,\nAbdel-Majeed, M.R.,\nMansour, H.M.,\nHulliel, S.F.,\nBisharat, L.M.:\nClassifying and diacritizing arabic poems using deep recurrent neural networks.\nJournal of King Saud University-Computer and Information Sciences\n34(6),\n3775\u20133788\n(2022)\n\n\n\nAbboushi and Azzeh [2023]\n\nAbboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "url": null
            }
        },
        {
            "12": {
                "title": "Al-Shaibani, M.S.,\nAlyafeai, Z.,\nAhmad, I.:\nMeter classification of arabic poems using deep bidirectional recurrent neural networks.\nPattern Recognition Letters\n136,\n1\u20137\n(2020)\n\n\n\nAbandah et al. [2022]\n\nAbandah, G.A.,\nKhedher, M.Z.,\nAbdel-Majeed, M.R.,\nMansour, H.M.,\nHulliel, S.F.,\nBisharat, L.M.:\nClassifying and diacritizing arabic poems using deep recurrent neural networks.\nJournal of King Saud University-Computer and Information Sciences\n34(6),\n3775\u20133788\n(2022)\n\n\n\nAbboushi and Azzeh [2023]\n\nAbboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "author": "Arabic Poem Comprehensive Dataset (APCD).\nhttps://hci-lab.github.io/LearningMetersPoems/\n\nAl-Shaibani et al. [2020]\n\nAl-Shaibani, M.S.,\nAlyafeai, Z.,\nAhmad, I.:\nMeter classification of arabic poems using deep bidirectional recurrent neural networks.\nPattern Recognition Letters\n136,\n1\u20137\n(2020)\n\n\n\nAbandah et al. [2022]\n\nAbandah, G.A.,\nKhedher, M.Z.,\nAbdel-Majeed, M.R.,\nMansour, H.M.,\nHulliel, S.F.,\nBisharat, L.M.:\nClassifying and diacritizing arabic poems using deep recurrent neural networks.\nJournal of King Saud University-Computer and Information Sciences\n34(6),\n3775\u20133788\n(2022)\n\n\n\nAbboushi and Azzeh [2023]\n\nAbboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "venue": "Abandah, G.A.,\nKhedher, M.Z.,\nAbdel-Majeed, M.R.,\nMansour, H.M.,\nHulliel, S.F.,\nBisharat, L.M.:\nClassifying and diacritizing arabic poems using deep recurrent neural networks.\nJournal of King Saud University-Computer and Information Sciences\n34(6),\n3775\u20133788\n(2022)\n\n\n\nAbboushi and Azzeh [2023]\n\nAbboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "url": null
            }
        },
        {
            "13": {
                "title": "Abandah, G.A.,\nKhedher, M.Z.,\nAbdel-Majeed, M.R.,\nMansour, H.M.,\nHulliel, S.F.,\nBisharat, L.M.:\nClassifying and diacritizing arabic poems using deep recurrent neural networks.\nJournal of King Saud University-Computer and Information Sciences\n34(6),\n3775\u20133788\n(2022)\n\n\n\nAbboushi and Azzeh [2023]\n\nAbboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "author": "Al-Shaibani, M.S.,\nAlyafeai, Z.,\nAhmad, I.:\nMeter classification of arabic poems using deep bidirectional recurrent neural networks.\nPattern Recognition Letters\n136,\n1\u20137\n(2020)\n\n\n\nAbandah et al. [2022]\n\nAbandah, G.A.,\nKhedher, M.Z.,\nAbdel-Majeed, M.R.,\nMansour, H.M.,\nHulliel, S.F.,\nBisharat, L.M.:\nClassifying and diacritizing arabic poems using deep recurrent neural networks.\nJournal of King Saud University-Computer and Information Sciences\n34(6),\n3775\u20133788\n(2022)\n\n\n\nAbboushi and Azzeh [2023]\n\nAbboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "venue": "Abboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "url": null
            }
        },
        {
            "14": {
                "title": "Abboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "author": "Abandah, G.A.,\nKhedher, M.Z.,\nAbdel-Majeed, M.R.,\nMansour, H.M.,\nHulliel, S.F.,\nBisharat, L.M.:\nClassifying and diacritizing arabic poems using deep recurrent neural networks.\nJournal of King Saud University-Computer and Information Sciences\n34(6),\n3775\u20133788\n(2022)\n\n\n\nAbboushi and Azzeh [2023]\n\nAbboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "venue": "Antoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "url": null
            }
        },
        {
            "15": {
                "title": "Antoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "author": "Abboushi, O.,\nAzzeh, M.:\nToward fluent arabic poem generation based on fine-tuning aragpt2 transformer.\nArabian Journal for Science and Engineering,\n1\u201313\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "venue": "Mohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "url": null
            }
        },
        {
            "16": {
                "title": "Mohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "author": "Antoun, W.,\nBaly, F.,\nHajj, H.:\nAragpt2: Pre-trained transformer for arabic language generation.\narXiv preprint arXiv:2012.15520\n(2020)\n\n\n\nMohammad [2009]\n\nMohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "venue": "Alsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "url": null
            }
        },
        {
            "17": {
                "title": "Alsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "author": "Mohammad, I.:\nNaive bayes for classical arabic poetry classification.\nAl-Nahrain Journal of Science\n12(4),\n217\u2013225\n(2009)\n\n\n\nAlsharif et al. [2013]\n\nAlsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "venue": "Ahmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "url": null
            }
        },
        {
            "18": {
                "title": "Ahmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "author": "Alsharif, O.,\nAlshamaa, D.,\nGhneim, N.:\nEmotion classification in arabic poetry using machine learning.\nInternational Journal of Computer Applications\n65(16)\n(2013)\n\n\n\nAhmed et al. [2019]\n\nAhmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "venue": "Shahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "url": null
            }
        },
        {
            "19": {
                "title": "Shahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "author": "Ahmed, M.A.,\nHasan, R.A.,\nAli, A.H.,\nMohammed, M.A.:\nThe classification of the modern arabic poetry using machine learning.\nTELKOMNIKA (Telecommunication Computing Electronics and Control)\n17(5),\n2667\u20132674\n(2019)\n\n\n\nShahriar et al. [2023]\n\nShahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "venue": "Antoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "url": null
            }
        },
        {
            "20": {
                "title": "Antoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "author": "Shahriar, S.,\nAl Roken, N.,\nZualkernan, I.:\nClassification of arabic poetry emotions using deep learning.\nComputers\n12(5),\n89\n(2023)\n\n\n\nAntoun et al. [2020]\n\nAntoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "venue": "Vaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "url": null
            }
        },
        {
            "21": {
                "title": "Vaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "author": "Antoun, W.,\nBaly, F.,\nHajj, H.:\nArabert: Transformer-based model for arabic language understanding.\narXiv preprint arXiv:2003.00104\n(2020)\n\n\n\nVaswani et al. [2017]\n\nVaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "venue": "El-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "url": null
            }
        },
        {
            "22": {
                "title": "El-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "author": "Vaswani, A.,\nShazeer, N.,\nParmar, N.,\nUszkoreit, J.,\nJones, L.,\nGomez, A.N.,\nKaiser, \u0141.,\nPolosukhin, I.:\nAttention is all you need.\nAdvances in neural information processing systems\n30\n(2017)\n\n\n\nEl-Khair [2016]\n\nEl-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "venue": "Zeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "url": null
            }
        },
        {
            "23": {
                "title": "Zeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "author": "El-Khair, I.A.:\n1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033\n(2016)\n\n\n\nZeroual et al. [2019]\n\nZeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "venue": "Abdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "url": null
            }
        },
        {
            "24": {
                "title": "Abdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "author": "Zeroual, I.,\nGoldhahn, D.,\nEckart, T.,\nLakhouaja, A.:\nOsian: Open source international arabic news corpus-preparation and integration into the clarin-infrastructure.\nIn: Proceedings of the Fourth Arabic Natural Language Processing Workshop,\npp. 175\u2013182\n(2019)\n\n\n\nAbdelali et al. [2016]\n\nAbdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "venue": "Chowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "url": null
            }
        },
        {
            "25": {
                "title": "Chowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "author": "Abdelali, A.,\nDarwish, K.,\nDurrani, N.,\nMubarak, H.:\nFarasa: A fast and furious segmenter for arabic.\nIn: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,\npp. 11\u201316\n(2016)\n\n\n\nChowdhury et al. [2020]\n\nChowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "venue": "Abdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "url": null
            }
        },
        {
            "26": {
                "title": "Abdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "author": "Chowdhury, S.A.,\nAbdelali, A.,\nDarwish, K.,\nSoon-Gyo, J.,\nSalminen, J.,\nJansen, B.J.:\nImproving arabic text categorization using transformer training diversification.\nIn: Proceedings of the Fifth Arabic Natural Language Processing Workshop,\npp. 226\u2013236\n(2020)\n\n\n\nAbdul-Mageed et al. [2020]\n\nAbdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "venue": "Su\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "url": null
            }
        },
        {
            "27": {
                "title": "Su\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "author": "Abdul-Mageed, M.,\nElmadany, A.,\nNagoudi, E.M.B.:\nArbert & marbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785\n(2020)\n\n\n\nSu\u00e1rez et al. [2019]\n\nSu\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "venue": "Inoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "url": null
            }
        },
        {
            "28": {
                "title": "Inoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "author": "Su\u00e1rez, P.J.O.,\nSagot, B.,\nRomary, L.:\nAsynchronous pipeline for processing huge corpora on medium to low resource infrastructures.\nIn: 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)\n(2019).\nLeibniz-Institut f\u00fcr Deutsche Sprache\n\n\n\nInoue et al. [2021]\n\nInoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "venue": "Alammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "url": null
            }
        },
        {
            "29": {
                "title": "Alammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "author": "Inoue, G.,\nAlhafni, B.,\nBaimukan, N.,\nBouamor, H.,\nHabash, N.:\nThe interplay of variant, size, and task type in arabic pre-trained language models.\narXiv preprint arXiv:2103.06678\n(2021)\n\n\n\nAlammary [2022]\n\nAlammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "venue": "Devlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "url": null
            }
        },
        {
            "30": {
                "title": "Devlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "author": "Alammary, A.S.:\nBert models for arabic text classification: a systematic review.\nApplied Sciences\n12(11),\n5720\n(2022)\n\n\n\nDevlin et al. [2018]\n\nDevlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "venue": "Wu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "url": null
            }
        },
        {
            "31": {
                "title": "Wu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "author": "Devlin, J.,\nChang, M.-W.,\nLee, K.,\nToutanova, K.:\nBert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n(2018)\n\n\n\nWu et al. [2016]\n\nWu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "venue": "Almausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "url": null
            }
        },
        {
            "32": {
                "title": "Almausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "author": "Wu, Y.,\nSchuster, M.,\nChen, Z.,\nLe, Q.V.,\nNorouzi, M.,\nMacherey, W.,\nKrikun, M.,\nCao, Y.,\nGao, Q.,\nMacherey, K., et al.:\nGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144\n(2016)\n\n\n\n[33]\n\nAlmausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "venue": "Aldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "url": null
            }
        },
        {
            "33": {
                "title": "Aldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "author": "Almausua.\nhttps://poetry.dctabudhabi.ae/.\nAccessed: 2023-01-10\n\n\n\n[34]\n\nAldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "venue": "Wolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "url": null
            }
        },
        {
            "34": {
                "title": "Wolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "author": "Aldiwan.\nhttps://www.aldiwan.net/.\nAccessed: 2023-01-19\n\n\n\nWolf et al. [2020]\n\nWolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "venue": "Zerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "url": null
            }
        },
        {
            "35": {
                "title": "Zerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "author": "Wolf, T.,\nDebut, L.,\nSanh, V.,\nChaumond, J.,\nDelangue, C.,\nMoi, A.,\nCistac, P.,\nRault, T.,\nLouf, R.,\nFuntowicz, M., et al.:\nTransformers: State-of-the-art natural language processing.\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 38\u201345\n(2020)\n\n\n\nZerrouki [2010]\n\nZerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "venue": "Liu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "url": null
            }
        },
        {
            "36": {
                "title": "Liu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "author": "Zerrouki, T.:\nPyarabic, an Arabic language library for Python.\nPyarabic\n(2010)\n\n\n\nLiu et al. [2019]\n\nLiu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "venue": "Loshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "url": null
            }
        },
        {
            "37": {
                "title": "Loshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "author": "Liu, Y.,\nOtt, M.,\nGoyal, N.,\nDu, J.,\nJoshi, M.,\nChen, D.,\nLevy, O.,\nLewis, M.,\nZettlemoyer, L.,\nStoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\n(2019)\n\n\n\nLoshchilov and Hutter [2017]\n\nLoshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "venue": "Zhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "url": null
            }
        },
        {
            "38": {
                "title": "Zhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "author": "Loshchilov, I.,\nHutter, F.:\nDecoupled weight decay regularization.\narXiv preprint arXiv:1711.05101\n(2017)\n\n\n\nZhang et al. [2020]\n\nZhang, Y.,\nWarstadt, A.,\nLi, H.-S.,\nBowman, S.R.:\nWhen do you need billions of words of pretraining data?\narXiv preprint arXiv:2011.04946\n(2020)",
                "venue": null,
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.12392v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "4",
            "4.1",
            "4.2"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.1",
            "5.2",
            "5.3",
            "5.3.1",
            "5.3.2",
            "5.3.3",
            "5.3.4",
            "5.3.5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "5.3.1",
            "5.3.2",
            "5.3.3",
            "5.3.4",
            "5.3.5"
        ]
    },
    "research_context": {
        "paper_id": "2403.12392v1",
        "paper_title": "AraPoemBERT: A Pretrained Language Model for Arabic Poetry Analysis",
        "research_background": "### Motivations\nArabic poetry holds a significant cultural and historical value within the Arab world, encapsulating the norms, values, and historical events of the culture. However, analyzing and identifying the rhythmic patterns (meters) and rhymes in Arabic poetry presents a substantial challenge due to the complexity and variety of classical and non-classical meters. Manual analysis of these patterns is time-consuming and requires both linguistic expertise and a keen ear for rhythm. The development and application of advanced language models can help automate this process and provide more accurate and efficient analysis tools.\n\n### Research Problem\nThe primary research problem this paper addresses is the difficulty in analyzing and classifying the rhythmic structures and rhymes in Arabic poetry. This task necessitates substantial linguistic knowledge and presents additional challenges when faced with non-classical meters. Existing methods are not optimized to handle the complexities of Arabic poetry effectively. Therefore, there is a need for a specialized language model that can accurately identify various poetic features including meter, rhyme, sentiment, and the poet\u2019s gender.\n\n### Relevant Prior Work\nPrior work has touched on various aspects of Arabic poetry and its analysis:\n1. **Arabic Poetry Structure**: Studies have established the structure of Arabic poetry, including the concept of verses composed of two halves (hemistiches) and the importance of rhyme (qafiyah) and meter (taqti\u2019) [1][2].\n2. **Classical Meters**: Research has outlined classical meters and their rhythmic patterns, emphasizing the use of 'tafa\u2019eel' or poetic feet which form the basis of the rhythmic structure [3].\n3. **Variations in Meters**: Scholars have identified different variants and modifications of classical meters [4][5], which further complicate the manual analysis process given their nuanced and often irregular patterns.\n4. **Non-Classical Meters**: The advent of non-classical meters associated with colloquial speech has added flexibility but also increased the complexity of identifying rhythmic patterns [6].\n5. **Technological Approaches**: Existing technological approaches in the realm of natural language processing (NLP) often fall short in effectively dealing with the intricacies of Arabic poetry.\n\nBy developing AraPoemBERT, the authors seek to provide a comprehensive tool trained specifically on Arabic poetry, positioning it to outperform existing models and methods on multiple NLP tasks related to Arabic poetry. This tailored model promises improvements in classification tasks and opens new research avenues for automated analysis in Arabic literary studies.",
        "methodology": "I apologize, but it seems like you have not yet provided the full content of the methodology section for AraPoemBERT. Could you please provide the complete methodology section from the paper? This will help me accurately describe the proposed method or model, including its key components and innovations while retaining the original wording and phrases as much as possible.",
        "main_experiment_and_results": "In this main experiment, the setup included the text preprocessing steps, pretraining procedures, and evaluation on downstream tasks. The experiments were executed on a local machine with an AMD Ryzen-9 7950x processor, 64GB DDR5 memory, and dual GeForce RTX 4090 GPUs (24GB each), running on Ubuntu 22.04. The Huggingface transformers library facilitated the pretraining and fine-tuning of language models. CUDA 11.8 was employed for GPU acceleration.\n\n### Datasets\nThe datasets used in the main experiment would typically comprise collections of Arabic poetry texts. These texts were preprocessed to standardized formats suitable for model training and evaluation.\n\n### Baselines\nWhile specific baselines are not detailed in the provided excerpt, typical baselines in such studies might include other state-of-the-art pretrained language models such as BERT, RoBERTa, or models specifically fine-tuned for Arabic NLP tasks. The performance of these models would be compared against the newly introduced AraPoemBERT.\n\n### Evaluation Metrics\nCommon evaluation metrics for language models include accuracy, precision, recall, and F1 score, particularly on downstream tasks like text classification, sentiment analysis, or other relevant NLP tasks specific to Arabic poetry analysis.\n\n### Main Experimental Results\nThe results would involve comparing the performance of AraPoemBERT against the aforementioned baselines on the given evaluation metrics. These results should demonstrate the effectiveness and improvements brought by AraPoemBERT in analyzing Arabic poetry texts.\n\nWith these steps and tools, the main experiment was capable of showcasing the efficacy and advancements introduced by the new pretrained model, AraPoemBERT, in the field of Arabic poetry analysis."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Assess the effectiveness of AraPoemBERT in performing sentiment analysis on Arabic poetry.",
            "experiment_process": "The dataset consists of 21,230 poems, composed of 315,877 verses, labeled with four emotions: Love, Sadness, Anger, and Spirituality. Relevant types of poems were grouped into these emotion classes. The model's performance was compared against other Arabic language models using accuracy as the main evaluation metric.",
            "result_discussion": "AraPoemBERT achieved an accuracy of 78.95%, significantly higher than the next best model, CAMeLBERT-CA. The model performed well in 'Love' and 'Spirituality' categories but struggled with 'Sadness' and 'Anger' due to misclassifications, as shown in the confusion matrix. Compared to a previous work that used a smaller dataset, AraPoemBERT demonstrated its ability to predict sentiment at the verse level with higher accuracy.",
            "ablation_id": "2403.12392v1.No1"
        },
        {
            "research_objective": "Evaluate AraPoemBERT's capacity to classify Arabic poetry meters.",
            "experiment_process": "Two classification tasks were performed: one focusing on the 16 classical meters and another including both classical and non-classical meters (28 meters). The dataset used included more than 1.85 million verses. The model's performance was evaluated in terms of accuracy, precision, recall, and F1 scores.",
            "result_discussion": "AraPoemBERT achieved the highest accuracy scores in both tasks, with 99% accuracy in six classical meters. For non-classical meters, the model attained accuracies between 49% and 93.92%, depending on the number of verses available. Issues in accuracy for some meters were attributed to the smaller sample sizes.",
            "ablation_id": "2403.12392v1.No2"
        },
        {
            "research_objective": "Investigate the ability of AraPoemBERT to classify sub-meters of Arabic poetry.",
            "experiment_process": "The task combined classical meters and their variants, resulting in a total of 33 sub-meters. The experiment focused on 25 sub-meters (with adequate sample sizes), dropping classes with fewer than 100 verses. The performance of six models, including AraPoemBERT, was compared using accuracy and F1 scores.",
            "result_discussion": "AraPoemBERT achieved the highest accuracy of 97.79% and an F1-score above 0.98 for most classes. The model struggled with sub-meters that had a lower number of samples, achieving an F1-score below 0.4 in those cases. This study is the first to address sub-meter classification and demonstrated better results compared to simpler meter classification tasks reported in previous works.",
            "ablation_id": "2403.12392v1.No3"
        },
        {
            "research_objective": "Determine the effectiveness of AraPoemBERT in classifying the poets' gender based on their poetry.",
            "experiment_process": "The poets' gender, absent in the original dataset, was manually annotated. The dataset contained 5,023 male and 360 female poets. The performance was evaluated based on overall accuracy and F1 scores, particularly noting model performance on the underrepresented Female class.",
            "result_discussion": "AraPoemBERT achieved a weighted average accuracy of 99.12% and an F1-score of 0.2246 for the Female class. The low F1-score for the Female class was due to its underrepresentation in the dataset.",
            "ablation_id": "2403.12392v1.No4"
        },
        {
            "research_objective": "Assess AraPoemBERT's performance in classifying the rhyme of Arabic poetry verses.",
            "experiment_process": "The dataset labeled verses with 31 different rhymes, including 28 Arabic letters and three variants of letters (Laa, Taa Marbutah, and Waw Hamza). The models' performance was measured using accuracy metrics and confusion matrices to show detailed classification results.",
            "result_discussion": "AraPoemBERT achieved an accuracy of 97.73%, slightly lower than CAMeLBERT-CA's 97.76%. The model correctly classified rhymes for the 28 Arabic letters but had lower accuracy on the three variant rhymes due to small sample sizes and misclassifications. These results highlight the model's proficiency and the areas needing improvement.",
            "ablation_id": "2403.12392v1.No5"
        }
    ]
}