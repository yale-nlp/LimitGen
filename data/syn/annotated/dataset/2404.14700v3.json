{
    "title": "FlashSpeech: Efficient Zero-Shot Speech Synthesis",
    "abstract": "Recent progress in large-scale zero-shot speech synthesis has been significantly advanced by language models and diffusion models. However, the generation process of both methods is slow and computationally intensive. Efficient speech synthesis using a lower computing budget to achieve quality on par with previous work remains a significant challenge. In this paper, we present FlashSpeech, a large-scale zero-shot speech synthesis system with approximately 5% of the inference time compared with previous work. FlashSpeech is built on the latent consistency model and applies a novel adversarial consistency training approach that can train from scratch without the need for a pre-trained diffusion model as the teacher. Furthermore, a new prosody generator module enhances the diversity of prosody, making the rhythm of the speech sound more natural. The generation processes of FlashSpeech can be achieved efficiently with one or two sampling steps while maintaining high audio quality and high similarity to the audio prompt for zero-shot speech generation. Our experimental results demonstrate the superior performance of FlashSpeech. Notably, FlashSpeech can be about 20 times faster than other zero-shot speech synthesis systems while maintaining comparable performance in terms of voice quality and similarity. Furthermore, FlashSpeech demonstrates its versatility by efficiently performing tasks like speech editing and diverse speech sampling. Audio samples can be found in https://flashspeech.github.io/.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In recent years, the landscape of speech synthesis has been transformed by the advent of large-scale generative models. Consequently, the latest research efforts have achieved notable advancements in zero-shot speech synthesis systems by significantly increasing the size of both datasets and models. Zero-shot speech synthesis, such as text-to-speech (TTS) and Editing, aims to generate speech that incorporates unseen speaker characteristics from a reference audio segment during inference, without the need for additional training. Current advanced zero-shot speech synthesis systems typically leverage language models (LMs) Wang et al. (2023a); Yang et al. (2023); Zhang et al. (2023); Kharitonov et al. (2023); Wang et al. (2023b); Peng et al. (2024); Kim et al. (2024) and diffusion-style models Shen et al. (2024); Kim et al. (2023b); Le et al. (2023); Jiang et al. (2023b) for in-context speech generation on the large-scale dataset. However, the generation process of these methods needs a long-time iteration. For example, VALL-E Wang et al. (2023a) builds on the language model to predict 75 audio token sequences for a 1-second speech, in its first-stage autoregressive (AR) token sequence generation. When using a non-autoregressive (NAR) latent diffusion model Rombach et al. (2022) based framework, NaturalSpeech 2 Shen et al. (2024) still requires 150 sampling steps. As a result, although these methods can produce human-like speech, they require significant computational time and cost. Some efforts have been made to accelerate the generation process. ClaM-TTS Kim et al. (2024) proposes a mel-codec with a superior compression rate and a latent language model that generates a stack of tokens at once. Although the slow generation speed issue has been somewhat alleviated, the inference speed is still far from satisfactory for practical applications. Moreover, the substantial computational time of these approaches leads to significant computational cost overheads, presenting another challenge. The fundamental limitation of speech generation stems from the intrinsic mechanisms of language models and diffusion models, which require considerable time either auto-regressively or through a large number of denoising steps. Hence, the primary objective of this work is to accelerate inference speed and reduce computational costs while preserving generation quality at levels comparable to the prior research. In this paper, we propose FlashSpeech as the next step towards efficient zero-shot speech synthesis. To address the challenge of slow generation speed, we leverage the latent consistency model (LCM) Luo et al. (2023), a recent advancement in generative models. Building upon the previous non-autoregressive TTS system Shen et al. (2024), we adopt the encoder of a neural audio codec to convert speech waveforms into latent vectors as the training target for our LCM. To train this model, we propose a novel technique called adversarial consistency training, which utilizes the capabilities of pre-trained speech language models Chen et al. (2022b); Hsu et al. (2021); Baevski et al. (2020) as discriminators. This facilitates the transfer of knowledge from large pre-trained speech language models to speech generation tasks, efficiently integrating adversarial and consistency training to improve performance. The LCM is conditioned on prior vectors obtained from a phoneme encoder, a prompt encoder, and a prosody generator. Furthermore, we demonstrate that our proposed prosody generator leads to more diverse expressions and prosody while preserving stability. Our contributions can be summarized as follows: We propose FlashSpeech, an efficient zero-shot speech synthesis system that generates voice with high audio quality and speaker similarity in zero-shot scenarios. We introduce adversarial consistency training, a novel combination of consistency and adversarial training leveraging pre-trained speech language models, for training the latent consistency model from scratch, achieving speech generation in one or two steps. We propose a prosody generator module that enhances the diversity of prosody while maintaining stability. FlashSpeech significantly outperforms strong baselines in audio quality and matches them in speaker similarity. Remarkably, it achieves this at a speed approximately 20 times faster than comparable systems, demonstrating unprecedented efficiency."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Large-Scale Speech Synthesis",
            "text": "Motivated by the success of the large language model, the speech research community has recently shown increasing interest in scaling the sizes of model and training data to bolster generalization capabilities, producing natural speech with diverse speaker identities and prosody under zero-shot settings.\nThe pioneering work is VALL-E Wang et al. (2023a  ###reference_b60###), which adopts the Encodec D\u00e9fossez et al. (2022  ###reference_b9###) to discretize the audio waveform into tokens. Therefore, a language model can be trained via in-context learning that can generate the target utterance where the style is consistent with prompt utterance. However, generating audio in such an autoregressive manner Wang et al. (2023b  ###reference_b62###); Peng et al. (2024  ###reference_b40###)can lead to unstable prosody, word skipping, and repeating issues Ren et al. (2020  ###reference_b45###); Tan et al. (2021  ###reference_b58###); Shen et al. (2024  ###reference_b53###). To ensure the robustness of the system, non-autoregressive methods such as NaturalSpeech2 Shen et al. (2024  ###reference_b53###) and Voicebox Le et al. (2023  ###reference_b27###) utilize diffusion-style model (VP-diffusion Song et al. (2020  ###reference_b56###) or flow-matching Lipman et al. (2022  ###reference_b30###)) to learn the distribution of a continuous intermediate vector such as mel-spectrogram or latent vector of codec. Both LM-based methods Zhao et al. (2023  ###reference_b68###) and diffusion-based methods show superior performance in speech generation tasks. However, their generation is slow due to the iterative computation.\nConsidering that many speech generation scenarios require real-time inference and low computational costs, we employ the latent consistency model for large-scale speech generation that inference with one or two steps while maintaining high audio quality."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Acceleration of Speech Synthesis",
            "text": "Since early neural speech generation models Tan et al. (2021  ###reference_b58###) use autoregressive models such as Tacotron Wang et al. (2017  ###reference_b61###) and TransformerTTS Li et al. (2019  ###reference_b28###), causing slow inference speed, with  computation, where  is the sequence length.\nTo address the slow inference speed, FastSpeech  Ren et al. (2020  ###reference_b45###, 2019  ###reference_b46###) proposes to generate a mel-spectrogram in a non-autoregressive manner.\nHowever, these models Ren et al. (2022  ###reference_b47###) result in blurred and over-smoothed mel-spectrograms due to the regression loss they used and the capability of modeling methods. To further enhance the speech quality, diffusion models are utilized Popov et al. (2021a  ###reference_b41###); Jeong et al. (2021  ###reference_b14###); Popov et al. (2021b  ###reference_b42###) which increase the computation to , where T is the diffusion steps. Therefore, distillation techniques Luo (2023  ###reference_b34###) for diffusion-based methods such as CoMoSpeech Ye et al. (2023  ###reference_b65###), CoMoSVC Lu et al. (2024  ###reference_b32###) and Reflow-TTS Guan et al. (2023  ###reference_b11###) emerge to reduce the sampling steps back to , but require additional pre-trained diffusion as the teacher model. Unlike previous distillation techniques, which require extra training for the diffusion model as a teacher and are limited by its performance, our proposed adversarial consistency training technique can directly train from scratch, significantly reducing training costs. In addition, previous acceleration methods only validate speaker-limited recording-studio datasets with limited data diversity. To the best of our knowledge, FlashSpeech is the first work that reduces the computation of a large-scale speech generation system back to ."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Consistency Model",
            "text": "The consistency model is proposed in Song et al. (2023  ###reference_b55###); Song and Dhariwal (2023  ###reference_b54###) to generate high-quality samples by directly mapping noise to data. Furthermore, many variants Kong et al. (2023  ###reference_b26###); Lu et al. (2023  ###reference_b31###); Sauer et al. (2023  ###reference_b52###); Kim et al. (2023a  ###reference_b22###) are proposed to further increase the generation quality of images.\nThe latent consistency model is proposed by Luo et al. (2023  ###reference_b33###) which can directly predict the solution of PF-ODE in latent space.\nHowever, the original LCM employs consistency distillation on the pre-trained latent diffusion model (LDM) which leverages large-scale off-the-shelf image diffusion models Rombach et al. (2022  ###reference_b48###). Since there are no pre-trained large-scale TTS models in the speech community, and inspired by the techniques Song and Dhariwal (2023  ###reference_b54###); Kim et al. (2023a  ###reference_b22###); Lu et al. (2023  ###reference_b31###); Sauer et al. (2023  ###reference_b52###); Kong et al. (2023  ###reference_b26###), we propose the novel adversarial consistency training method which can directly train the large-scale latent consistency model from scratch utilizing the large pre-trained speech language model Chen et al. (2022b  ###reference_b7###); Hsu et al. (2021  ###reference_b12###); Baevski et al. (2020  ###reference_b2###) such as WavLM for speech generation."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "FlashSpeech",
            "text": "###figure_2###"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Overview",
            "text": "Our work is dedicated to advancing the speech synthesis efficiency, achieving computation cost while maintaining comparable performance to prior studies that require or computations. The framework of the proposed method, FlashSpeech, is illustrated in Fig. 2. FlashSpeech integrates a neural codec, an encoder for phonemes and prompts, a prosody generator, and an LCM, which are utilized during both the training and inference stages. Exclusively during training, a conditional discriminator is employed. FlashSpeech adopts the in-context learning paradigm, initially segmenting the latent vector z, extracted from the codec, into components. Subsequently, the phoneme and components are processed through the encoder to produce the hidden feature. A prosody generator then predicts pitch and duration based on the hidden feature. The pitch and duration embeddings are combined with the hidden feature and inputted into the LCM as the conditional feature. The LCM model is trained from scratch using adversarial consistency training. After training, FlashSpeech can achieve efficient generation within one or two sampling steps."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Latent Consistency Model",
            "text": "The consistency model Song et al. (2023 ###reference_b55###) is a new family of generative models that enables one-step or few-step generation. Let us denote the data distribution by \\(\\mathcal{D}\\). The core idea of the consistency model is to learn the function that maps any points on a trajectory of the PF-ODE to that trajectory\u2019s origin, which can be formulated as:\n\n\\[\n    \\mathbf{x}_0(\\mathbf{x}_t) \\approx \\mathbf{f(\\cdot)}\n\\]\n\nwhere \\(\\mathbf{f}\\) is the consistency function and \\(\\mathbf{x}_t\\) represents the data \\(\\mathbf{x}_0\\) perturbed by adding zero-mean Gaussian noise with standard deviation \\(t\\). \\(\\epsilon\\) is a fixed small positive number. Then \\(\\mathbf{x}_0(\\mathbf{x}_t)\\) can then be viewed as an approximate sample from the data distribution \\(\\mathcal{D}\\). To satisfy the property, we parameterize the consistency model as\n\n\\[\n    \\mathbf{x}_0(\\mathbf{x}_t)\\approx \\mathbf{f}_\\theta(\\mathbf{x}_t, t), t \\in [0, \\epsilon]\n\\]\n\nwhere \\(\\mathbf{f}_\\theta\\) is to estimate consistency function \\(\\mathbf{f}\\) by learning from data, \\(\\theta\\) is a deep neural network with parameter \\(\\theta\\), \\(\\alpha(t)\\) and \\(\\sigma(t)\\) are differentiable functions with \\(\\alpha(0)=0\\) and \\(\\sigma(0)=1\\) to ensure boundary condition. A valid consistency model should satisfy the self-consistency property Song et al. (2023 ###reference_b55###):\n\n\\[\n    \\mathbf{f}_\\theta(\\mathbf{x}_q, q)=\\mathbf{f}_\\theta(\\mathbf{f}_\\theta(\\mathbf{x}_p, p), q)\n\\]\n\nwhere \\(0 \\leq q < p \\leq \\epsilon\\), following Karras et al. (2022 ###reference_b20###); Song et al. (2023 ###reference_b55###); Song and Dhariwal (2023 ###reference_b54###). Then the model can generate samples in one step by evaluating\n\n\\[\n    \\mathbf{x}_0(\\mathbf{x}_\\epsilon) \\approx \\mathbf{f}_\\theta(\\mathbf{x}_\\epsilon, \\epsilon)\n\\]\n\nfrom distribution \\(\\mathcal{D}\\). As we apply a consistency model on the latent space of audio, we use the latent features \\(\\mathbf{z}\\) which are extracted prior to the residual quantization layer of the codec,\n\n\\[\n    \\mathbf{z}=\\mathrm{Encoder}( \\mathbf{y}),\n\\]\n\nwhere \\(\\mathbf{y}\\) is the speech waveform. Furthermore, we add the feature from the prosody generator and encoder as the conditional feature \\(\\mathbf{c}\\),\n\n\\[\n    \\mathbf{c} = \\mathbf{c}_p+\\mathbf{c}_e\n\\]\n\nour objective has changed to achieve\n\n\\[\n    \\mathbf{z}_0(t)=\\mathbb{E}_{\\mathbf{z}(t)|\\mathbf{c}}\\left[\\mathbf{z}_0\\mid\\mathbf{z}(t),\\mathbf{c}\\right];\n\\]\n\nDuring inference, the synthesized waveform \\(\\hat{\\mathbf{y}}\\) is transformed from \\(\\hat{\\mathbf{z}}\\) via the codec decoder. The predicted \\(\\hat{\\mathbf{z}}\\) is obtained by one sampling step\n\n\\[\n\\hat{\\mathbf{z}}=\\mathbf{f}_\\theta(\\mathbf{z}_\\epsilon, \\epsilon, \\mathbf{c})\n\\]\n\nor two sampling steps\n\n\\[\n\\hat{\\mathbf{z}}=\\mathbf{f}_\\theta(\\mathbf{f}_\\theta(\\mathbf{z}_\\epsilon, \\frac{\\epsilon}{2}, \\mathbf{c}), \\epsilon, \\mathbf{c}),\n\\]\n\nwhere \\(\\frac{\\epsilon}{2}\\) means the intermediate step, \\(\\epsilon\\) is set to 2 empirically. \\(\\mathbf{z}_\\epsilon\\) is sampled from a standard Gaussian distribution."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Adversarial Consistency Training",
            "text": "A major drawback of the LCM Luo et al. (2023 ###reference_b33###) is that it needs to pre-train a diffusion-based teacher model in the first stage, and then perform distillation to produce the final model. This would make the training process complicated, and the performance would be limited as a result of the distillation. To eliminate the reliance on the teacher model training, in this paper, we propose a novel adversarial consistency training method to train LCM from scratch. Our training procedure is outlined in Fig. 3 ###reference_###, which has three parts:"
        },
        {
            "section_id": "3.3.1",
            "parent_section_id": "3.3",
            "section_name": "3.3.1 Consistency Training",
            "text": "To achieve the property in equation (3), we adopt the following consistency loss where \\( \\sigma_t \\) represents the noise level at discrete time step \\( t \\), \\( d(\\cdot, \\cdot) \\) is the distance function, \\( \\mathbf{x}_s \\) and \\( \\mathbf{x}_t \\) are the student with the higher noise level and the teacher with the lower noise level, respectively. The discrete time steps denoted as \\( \\{0, \\ldots, T\\} \\) are divided from the time interval \\([0, 1]\\), where the discretization curriculum \\( \\mathcal{C}(n) \\) increases correspondingly as the number of training steps grows: \n\n\\[\n\\mathcal{C}(n) = \\left\\lfloor T \\cdot \\left(\\frac{n}{N}\\right)^{w}\\right\\rfloor \n\\]\n\nwhere \\( n \\) is the current training step and \\( N \\) is the total training steps. \\( w \\) and \\( T \\) are hyperparameters to control the size of \\(\\mathcal{C}(n)\\). The distance function \\( d(\\cdot, \\cdot) \\) uses the Pseudo-Huber metric Charbonnier et al. (1997):\n\n\\[\nd(a, b) = \\sqrt{1 + \\left(\\frac{a-b}{\\delta}\\right)^2} - 1\n\\]\n\nwhere \\( \\delta \\) is an adjustable constant, making the training more robust to outliers as it imposes a smaller penalty for large errors than \\( \\ell_2 \\) loss. The parameters \\(\\theta_t\\) of teacher model are:\n\n\\[\n\\theta_t = \\alpha \\theta_s + (1 - \\alpha) \\theta_t\n\\]\n\nwhich are identical to the student parameters \\(\\theta_s\\). This approach Song and Dhariwal (2023) has been demonstrated to improve sample quality of previous strategies that employ varying decay rates Song et al. (2023).\n\nThe weighting function refers to:\n\n\\[\nw(t) = \\frac{1}{\\sigma_t}\n\\]\n\nwhich emphasizes the loss of smaller noise levels. LCM through consistency training can generate speech with acceptable quality in a few steps, but it still falls short of previous methods. Therefore, to further enhance the quality of the generated samples, we integrate adversarial training."
        },
        {
            "section_id": "3.3.2",
            "parent_section_id": "3.3",
            "section_name": "3.3.2 Adversarial Training",
            "text": "For the adversarial objective, the generated samples and real samples are passed to the discriminator, which aims to distinguish between them, where refers to the trainable parameters. Thus, we employ adversarial training loss. In this way, the error signal from the discriminator guides to produce more realistic outputs.\n\nFor details, we use a frozen pre-trained speech language model and a trainable lightweight discriminator head to build the discriminator. Since the current is trained on the speech waveform, we convert both to ground truth waveform and predicted waveform using the codec decoder.\n\nTo further increase the similarity between prompt audio and generated audio, our discriminator is conditioned on the prompt audio feature. This prompt feature is extracted using on prompt audio and applies average pooling on the time axis. Therefore, where and refer to feature extracted through for ground truth waveform and predicted waveform.\n\nThe discriminator head consists of several 1D convolution layers. The input feature of the discriminator is conditioned on via projection Miyato and Koyama (2018)."
        },
        {
            "section_id": "3.3.3",
            "parent_section_id": "3.3",
            "section_name": "3.3.3 Combined Together",
            "text": "Since there is a large gap on the loss scale between consistency loss and adversarial loss, it can lead to instability and failure in training. Therefore, we follow Esser et al. (2021) to compute the adaptive weight with where is the last layer of the neural network in LCM. The final loss of training LCM is defined as This adaptive weighting significantly stabilizes the training by balancing the gradient scale of each term."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Prosody Generator",
            "text": "###figure_4###"
        },
        {
            "section_id": "3.4.1",
            "parent_section_id": "3.4",
            "section_name": "3.4.1 Analysis of Prosody Prediction",
            "text": "Previous regression methods for prosody prediction, due to their deterministic mappings and assumptions of unimodal distribution, often fail to capture the inherent diversity and expressiveness of human speech prosody. This leads to predictions that lack variation and can appear over-smoothed. On the other hand, diffusion methods for prosody prediction offer a promising alternative by providing greater prosody diversity. However, they come with challenges regarding stability, and the potential for unnatural prosody. Additionally, the iterative inference process in DMs requires a significant number of sampling steps that may also hinder real-time application. Meanwhile, LM-based methods also need a long time for inference. To alleviate these issues, our prosody generator consists of a prosody regression module and a prosody refinement module to enhance the diversity of prosody regression results with efficient one-step consistency model sampling."
        },
        {
            "section_id": "3.4.2",
            "parent_section_id": "3.4",
            "section_name": "3.4.2 Prosody Refinement via Consistency Model",
            "text": "As shown in 4, our prosody generator consists of two parts which are prosody regression and prosody refinement. We first train the prosody regression module to get a deterministic output. Next, we freeze the parameters of the prosody regression module and use the residual of ground truth prosody and deterministic predicted prosody as the training target for prosody refinement. We adopt a consistency model as a prosody refinement module. The conditional feature of the consistency model is the feature from prosody regression before the final projection layer. Thus, the residual from a stochastic sampler refines the output of a deterministic prosody regression and produces a diverse set of plausible prosody under the same transcription and audio prompt.\n\nOne option for the final prosody output can be represented as:\nwhere denotes the final prosody output, represents the residual output from the prosody refinement module, capturing the variations between the ground truth prosody and the deterministic prediction, is the initial deterministic prosody prediction from the prosody regression module.\n\nHowever, this formulation may negatively affect prosody stability, a similar observation is found in Vyas et al. (2023); Le et al. (2023). More specifically, higher diversity may cause less stability and sometimes produce unnatural prosody.\n\nTo address this, we introduce a control factor that finely tunes the balance between stability and diversity in the prosodic output:\nwhere is a scalar value ranging between 0 and 1. This adjustment allows for controlled incorporation of variability into the prosody, mitigating issues related to stability while still benefiting from the diversity offered by the prosody refinement module."
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "Applications",
            "text": "This section elaborates on the practical applications of FlashSpeech. We delve into its deployment across various tasks such as zero-shot TTS, speech editing, and diverse speech sampling. All the sample audios of applications are available on the demo page."
        },
        {
            "section_id": "3.5.1",
            "parent_section_id": "3.5",
            "section_name": "3.5.1 Zero-Shot TTS",
            "text": "Given a target text and reference audio, we first convert the text to phoneme using g2p (grapheme-to-phoneme conversion). Then we use the codec encoder to convert the reference audio into . Speech can be synthesized efficiently through FlashSpeech with the phoneme input and , achieving high-quality text-to-speech results."
        },
        {
            "section_id": "3.5.2",
            "parent_section_id": "3.5",
            "section_name": "3.5.2 Voice Conversion",
            "text": "Following Shen et al. (2024 ###reference_b53###); Preechakul et al. (2022 ###reference_b44###), we first apply the reverse of ODE to diffuse the source audio into a starting point that still maintains some information in the source audio. After that, we run the sampling process from this starting point with the reference audio as  and condition . The condition  uses the phoneme and duration from the source audio and the pitch is predicted by the prosody generator."
        },
        {
            "section_id": "3.5.3",
            "parent_section_id": "3.5",
            "section_name": "3.5.3 Speech Editing",
            "text": "Given the speech, the original transcription, and the new transcription, we first use MFA (Montreal Forced Aligner) to align the speech and the original transcription to get the duration of each word. Then we remove the part that needs to be edited to construct the reference audio. Next, we use the new transcription and reference to synthesize new speech. Since this task is consistent with the in-context learning, we can concatenate the remaining part of the raw speech and the synthesized part as the final speech, thus enabling precise and seamless speech editing."
        },
        {
            "section_id": "3.5.4",
            "parent_section_id": "3.5",
            "section_name": "3.5.4 Diverse Speech Sampling",
            "text": "FlashSpeech leverages its inherent stochasticity to generate a variety of speech outputs under the same conditions. By employing stochastic sampling in its prosody generation and LCM, FlashSpeech can produce diverse variations in pitch, duration, and overall audio characteristics from the same phoneme input and audio prompt. This feature is particularly useful for generating a wide range of speech expressions and styles from a single input, enhancing applications like voice acting, synthetic voice variation for virtual assistants, and more personalized speech synthesis. In addition, the synthetic data via speech sampling can also benefit other tasks such as ASR Rossenbach et al. (2020)."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiment",
            "text": "###figure_5### In the experimental section, we begin by introducing the datasets and the configurations for training in our experiments. Following this, we show the evaluation metrics and demonstrate the comparative results against various zero-shot TTS models. Subsequently, ablation studies are conducted to test the effectiveness of several design choices. We show our speech editing and diverse speech sampling results on our demo page."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experimental Settings",
            "text": ""
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1 Data and Preprocessing",
            "text": "We use the English subset of Multilingual LibriSpeech (MLS) Pratap et al. (2020 ###reference_b43###), including 44.5k hours of transcribed audiobook data and it contains 5490 distinct speakers. The audio data is resampled at a frequency of 16kHz. The input text is transformed into a sequence of phonemes through grapheme-to-phoneme conversion Sun et al. (2019 ###reference_b57###) and then we use our internal alignment tool aligned with speech to obtain the phoneme-level duration. We adopt a hop size of 200 for all frame-level features. The pitch sequence is extracted using PyWorld222https://github.com/JeremyCCHsu/Python-Wrapper-for-World-Vocoder. We adopt Encodec D\u00e9fossez et al. (2022 ###reference_b9###) as our audio codec. We use a modified version 333https://github.com/yangdongchao/UniAudio/tree/main/codec and train it on MLS. We use the dense features extracted before the residual quantization layer as our latent vector."
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "4.1.2 Training Details",
            "text": "Our training consists of two stages. In the first stage, we train LCM and the prosody regression part. We use 8 H800 80GB GPUs with a batch size of 20k frames of latent vectors per GPU for 650k steps. We use the AdamW optimizer with a learning rate of 3e-4, warm up the learning rate for the first 30k updates and then linear decay it. We deactivate adversarial training with  = 0 before 600K training iterations. For hyper-parameters, we set  in Equation (12) to 0.03. In equation (10), where ,   . For N(k) in Equation (11), we set . After 600k steps, we activate adversarial loss, and N(k) can be considered as fixed to 1280. We crop the waveform length fed into the discriminator into minimum waveform length in a minibatch. In addition, the weight of the feature extractor WavLM and the codec decoder are frozen.\n\nIn the second stage, we train 150k steps for the prosody refinement module with consistency training in Equation (10). Different from the above setting, we empirically set , . During training, only the weight of the prosody refinement part is updated."
        },
        {
            "section_id": "4.1.3",
            "parent_section_id": "4.1",
            "section_name": "4.1.3 Model Details",
            "text": "The model structures of the prompt encoder and phoneme encoder follow Shen et al. (2024). The neural function part in LCM is almost the same as in Shen et al. (2024). We rescale the sinusoidal position embedding in the neural function part by a factor of 1000. As for the prosody generator, we adopt 30 non-causal wavenet layers Oord et al. (2016) for the neural function part in the prosody refinement module and the same configurations for prosody regression parts as in Shen et al. (2024). We empirically set parameters for the prosody refinement module. For the discriminator\u2019s head, we stack 5 convolutional layers with weight normalization for binary classification Salimans and Kingma (2016)."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Evaluation Metrics",
            "text": "We use both objective and subjective evaluation metrics, including RTF: Real-time-factor (RTF) measures the time taken for the system to generate one second of speech. This metric is crucial for evaluating the efficiency of our system, particularly for applications requiring real-time processing. We measure the time of our system end-to-end on an NVIDIA V100 GPU following Shen et al. (2024).\n\nWER (Word Error Rate): To evaluate the accuracy and clarity of synthesized speech from the TTS system, we employ the Automatic Speech Recognition (ASR) model Wang et al. (2023a) to transcribe generated audio. The discrepancies between these transcriptions and original texts are quantified using the Word Error Rate (WER), a crucial metric indicating intelligibility and robustness.\n\nCMOS, SMOS, UTMOS: We rank the comparative mean option score (CMOS) and similarity mean option score (SMOS) using mturk. The prompt for CMOS refers to 'Please focus on the audio quality and naturalness and ignore other factors.\u2019. Each audio has been listened to by at least 10 listeners. UTMOS Saeki et al. (2022) is a Speech MOS predictor to measure the naturalness of speech. We use it in ablation studies which reduced the cost for evaluation.\n\nProsody JS Divergence: To evaluate the diversity and accuracy of the prosody prediction in our TTS system, we include the Prosody JS Divergence metric. This metric employs the Jensen-Shannon (JS) divergence Men\u00e9ndez et al. (1997) to quantify the divergence between the predicted and ground truth prosody feature distributions. Prosody features, including pitch and duration, are quantized and their distributions in both synthesized and natural speech are compared. Lower JS divergence values indicate closer similarity between the predicted prosody features and those of the ground truth, suggesting a higher diversity of the synthesized speech."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Experimental Results on Zero-shot TTS",
            "text": "We employ LibriSpeech Panayotov et al. (2015  ###reference_b39###) test-clean for zero-shot TTS evaluation. We adopt the cross-sentence setting that involves randomly selecting 3-second clips as prompts from the same speaker\u2019s speech. The results are summarized in table 1  ###reference_### and figure 5  ###reference_###."
        },
        {
            "section_id": "4.3.1",
            "parent_section_id": "4.3",
            "section_name": "4.3.1 Evaluation Baselines",
            "text": "VALL-E Wang et al. (2023a ###reference_b60###): VALL-E predicts codec tokens using both AR and NAR models. RTF777In CLaM-TTS and Voicebox, they report the inference time for generating 10 seconds of speech. Therefore, we divide by 10 to obtain the time for generating 1 second of speech (RTF). is obtained from Kim et al. (2024 ###reference_b23###); Le et al. (2023 ###reference_b27###). We use our reproduced results for MOS, Sim, and WER. Additionally, we do a preference test with their official demo.\n\nNaturalSpeech2 Shen et al. (2024 ###reference_b53###): NaturalSpeech2 uses a latent diffusion model to predict latent features of codec. The RTF is from the original paper. the Sim, WER and samples for MOS are obtained through communication with the authors. We also do a preference test with their official demo.\n\nMega-TTS Jiang et al. (2023a ###reference_b18###): Mega-TTS uses both language model and GAN to predict mel-spectrogram. We obtain RTF from mobilespeech Ji et al. (2024 ###reference_b15###) and WER from the original paper. We do a preference test with their official demo."
        },
        {
            "section_id": "4.3.2",
            "parent_section_id": "4.3",
            "section_name": "4.3.2 Generation Quality",
            "text": "FlashSpeech stands out significantly in terms of speaker quality, surpassing other baselines in both CMOS and audio quality preference tests. Notably, our method closely approaches ground truth recordings, underscoring its effectiveness. These results affirm the superior quality of FlashSpeech in speech synthesis."
        },
        {
            "section_id": "4.3.3",
            "parent_section_id": "4.3",
            "section_name": "4.3.3 Generation Similarity",
            "text": "Our evaluation of speaker similarity utilizes Sim, SMOS, and speaker similarity preference tests, where our methods achieve 1st, 2nd, and 3rd place rankings, respectively. These findings validate our methods\u2019 ability to achieve comparable speaker similarity to other methods. Despite our training data (MLS) containing approximately 5k speakers, fewer than most other methods (e.g., Librilight with about 7k speakers or self-collected data), we believe that increasing the number of speakers in our methods can further enhance speaker similarity."
        },
        {
            "section_id": "4.3.4",
            "parent_section_id": "4.3",
            "section_name": "4.3.4 Robustness",
            "text": "Our methods achieve a WER of 2.7, placing them in the first echelon. This is due to the non-autoregressive nature of our methods, which ensures robustness."
        },
        {
            "section_id": "4.3.5",
            "parent_section_id": "4.3",
            "section_name": "4.3.5 Generation Speed",
            "text": "FlashSpeech achieves a remarkable approximately 20x faster inference speed compared to previous work. Considering its excellent audio quality and robustness, our method stands out as an efficient and effective solution in the field of large-scale speech synthesis."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Ablation Studies",
            "text": ""
        },
        {
            "section_id": "4.4.1",
            "parent_section_id": "4.4",
            "section_name": "4.4.1 Ablation studies of LCM",
            "text": "We explored the impact of different pre-trained models in adversarial training on UTMOS and Sim-O. As shown in table 2, the baseline, which employs consistency training alone, achieved a UTMOS of 3.62 and a Sim-O of 0.45. Incorporating adversarial training using wav2vec2-large, hubert-large, and wavlm-large as discriminators significantly improved both UTMOS and Sim-O scores. Notably, the application of adversarial training with Wavlm-large achieved the highest scores (UTMOS: 4.00, Sim-O: 0.52), underscoring the efficacy of this pre-trained model in enhancing the quality and speaker similarity of synthesized speech. Additionally, without using the audio prompt\u2019s feature as a condition, the discriminator shows a slight decrease in performance (UTMOS: 3.97, Sim-O: 0.51), highlighting the importance of conditional features in guiding the adversarial training process.\n\nAs shown in table 3, the effect of sampling steps (NFE) on UTMOS and Sim-O revealed that increasing NFE from 1 to 2 marginally improves UTMOS (3.99 to 4.00) and Sim-O (0.51 to 0.52). However, further increasing to 4 sampling steps slightly reduced UTMOS to 3.91 due to the accumulation of score estimation errors. Therefore, we use 2 steps as the default setting for LCM."
        },
        {
            "section_id": "4.4.2",
            "parent_section_id": "4.4",
            "section_name": "4.4.2 Ablation studies of Prosody Generator",
            "text": "In this part, we investigated the effects of a control factor, denoted as , on the prosodic features of pitch and duration in speech synthesis, by setting another influencing factor to zero. Our study specifically conducted an ablation analysis to assess how  influences these features, emphasizing its critical role in balancing stability and diversity within our framework\u2019s prosodic outputs.\n\nTable 4 elucidates the effects of varying  on the pitch component. With  set to 0, indicating no inclusion of the residual output from prosody refinement, we observed a Pitch JSD of 0.072 and a WER of 2.8. A slight modification to  resulted in a reduced Pitch JSD of 0.067, maintaining the same WER. Notably, setting  to 1, fully incorporating the prosody refinement\u2019s residual output, further decreased the Pitch JSD to 0.063, albeit at the cost of increased WER to 3.7, suggesting a trade-off between prosody diversity and speech intelligibility.\n\nSimilar trends in table 5 are observed in the duration component analysis. With , the Duration JSD was 0.0175 with a WER of 2.8. Adjusting  to 0.2 slightly improved the Duration JSD to 0.0168, without affecting WER. However, fully embracing the refinement module\u2019s output by setting  yielded the most significant improvement in Duration JSD to 0.0153, which, similar to pitch analysis, came with an increased WER of 3.9. The results underline the delicate balance required in tuning  to optimize between diversity and stability of prosody without compromising speech intelligibility."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Evaluation Results for Voice Conversion",
            "text": "In this section, we present the evaluation results of our system compared to state-of-the-art methods. We conduct experiments using their official checkpoints in our internal test set. Our system demonstrates high-quality results, confirming the effectiveness of our approach in tasks."
        },
        {
            "section_id": "4.6",
            "parent_section_id": "4",
            "section_name": "Conclusions and Future Work",
            "text": "In this paper, we presented FlashSpeech, a novel speech generation system that significantly reduces computational costs while maintaining high-quality speech output. Utilizing a novel adversarial consistency training method and an LCM, FlashSpeech outperforms existing zero-shot TTS systems in efficiency, achieving speeds about 20 times faster without compromising on voice quality, similarity, and robustness. In the future, we aim to further refine the model to improve the inference speed and reduce computational demands. In addition, we will expand the data scale and enhance the system\u2019s ability to convey a broader range of emotions and more nuanced prosody. For future applications, FlashSpeech can be integrated for real-time interactions in applications such as virtual assistants and educational tools."
        }
    ],
    "url": "http://arxiv.org/html/2404.14700v3",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2",
            "2.3"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.3.1",
            "3.3.2",
            "3.3.3",
            "3.4",
            "3.4.1",
            "3.4.2",
            "3.5",
            "3.5.1",
            "3.5.2",
            "3.5.3",
            "3.5.4"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.1.1",
            "4.1.2",
            "4.1.3",
            "4.2",
            "4.3",
            "4.3.1",
            "4.3.2",
            "4.3.3",
            "4.3.4",
            "4.3.5",
            "4.4",
            "4.4.1",
            "4.4.2",
            "4.5",
            "4.6"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.4",
            "4.4.1",
            "4.4.2"
        ]
    },
    "research_context": {
        "paper_id": "2404.14700v3",
        "paper_title": "FlashSpeech: Efficient Zero-Shot Speech Synthesis",
        "research_background": "Based on the provided introduction, the paper's main elements can be articulated as follows:\n\n### Motivation\nThe motivation behind the paper is rooted in the limitations of current advanced zero-shot speech synthesis systems. Although these systems, which include text-to-speech (TTS), voice conversion (VC), and editing, have achieved significant progress by leveraging large-scale generative models, they suffer from lengthy generation times and high computational costs. The primary driver is the intrinsic mechanisms of the language and diffusion models, which require extensive iterative processes, either autoregressively or through numerous denoising steps. These issues hinder the practical application of zero-shot speech synthesis systems despite their ability to produce high-quality, human-like speech.\n\n### Research Problem\nThe research problem focuses on enhancing the efficiency of zero-shot speech synthesis systems. Specifically, it's about accelerating inference speed and reducing computational costs without compromising the quality of the generated speech. The goal is to develop a system that maintains high audio quality and speaker similarity while vastly improving inference speed to make these systems more feasible for practical applications.\n\n### Relevant Prior Work\nKey contributions from prior research that contextualize this work include:\n1. **Language Models and Diffusion Models**: Systems like VALL-E (Wang et al., 2023a) and NaturalSpeech 2 (Shen et al., 2024) utilize language models and non-autoregressive (NAR) latent diffusion models for speech synthesis but suffer from long iteration times.\n2. **Acceleration Techniques**: Efforts like Voicebox (Le et al., 2023), which uses optimal transport paths to reduce sampling steps, and ClaM-TTS (Kim et al., 2024), which employs a mel-codec and latent language model for token generation, have made strides in improving speed but still fall short of practical requirements.\n3. **Neural Audio Codecs**: Building on neural audio codecs for converting speech waveforms into latent vectors (Shen et al., 2024).\n4. **Pre-trained Speech Language Models**: Utilizing pre-trained speech language models (Chen et al., 2022b; Hsu et al., 2021; Baevski et al., 2020) for tasks such as discrimination, facilitating knowledge transfer and improving generation performance.\n\n### Proposed Solution\nTo address the identified challenges, the paper introduces FlashSpeech, an efficient zero-shot speech synthesis system that:\n- Utilizes a latent consistency model (LCM) to achieve rapid speech generation.\n- Leverages adversarial consistency training, a novel method that combines consistency and adversarial training with pre-trained speech language models to improve speech generation performance.\n- Incorporates a prosody generator to produce diverse and stable prosody in the synthesized speech.\n\n### Contributions\nThe paper emphasizes that FlashSpeech achieves an unprecedented level of efficiency, generating high-quality and speaker-similar audio approximately 20 times faster than current leading systems. This demonstrates a significant advancement towards practical zero-shot speech synthesis applications.",
        "methodology": "In order to provide an accurate description of the proposed method or model from the methodology section of the paper \"FlashSpeech: Efficient Zero-Shot Speech Synthesis,\" I would need specific details from the actual text of the section referenced. However, based on the title \"FlashSpeech: Efficient Zero-Shot Speech Synthesis,\" the methodology likely involves a novel approach to synthesizing speech in a way that does not require prior exposure to the specific voice being synthesized (zero-shot). This suggests the method employs techniques that generalize well to new, unseen voices without additional training. Key components and innovations might include: - A neural network architecture optimized for speech synthesis. - A training process utilizing massive datasets to generalize well across various voices. - Techniques for efficient computation to ensure quick response times. Without concrete excerpts, a general assumption would be: 1. **Model Architecture**: Descriptions of any specific neural network models, such as sequence-to-sequence models, convolutional neural networks, or transformers specifically adapted for the speech synthesis task. 2. **Data Preparation and Processing**: Steps involved in pre-processing audio data to train the model effectively, possibly including feature extraction techniques like Mel-frequency cepstral coefficients (MFCC). 3. **Training Mechanism**: Information on how the model is trained in a way that supports zero-shot learning, perhaps by incorporating adversarial training, variational autoencoders, or other state-of-the-art techniques. 4. **Inference Pipeline**: Explanation of how the trained model is used to generate speech from text in real-time or near-real-time scenarios. To provide an accurate description, it would be best to have the actual wording from the methodology section or a detailed caption. Please provide more text or details!",
        "main_experiment_and_results": "### Main Experiment Setup\n\n#### Datasets\nWe conduct our experiments using multiple datasets designed for text-to-speech synthesis (TTS) tasks, ensuring a diversity of speech samples and text inputs.\n\n#### Baselines\nThe comparative analysis involves several zero-shot TTS models, which are well-known in the field. We compare our proposed FlashSpeech model against these existing baselines to establish our model's efficiency and effectiveness.\n\n#### Evaluation Metrics\nTo evaluate the performance of the TTS models, we employ several standard metrics:\n1. **Mean Opinion Score (MOS)** - used to assess the naturalness and overall quality of the synthesized speech as perceived by human listeners.\n2. **Character Error Rate (CER)** - quantifies the accuracy of the generated speech text compared to the reference text. It identifies insertion, deletion, and substitution errors.\n3. **Speaker Similarity** - measures how closely the synthesized voice matches the target speaker's voice using similarity scores.\n\n### Main Experimental Results\n\nFlashSpeech outperforms existing zero-shot TTS models across several key metrics. \n\n1. **Mean Opinion Score (MOS)**: Our model achieves a higher MOS, indicating that listeners perceive the speech synthesized by FlashSpeech to be more natural and of higher quality compared to other models.\n2. **Character Error Rate (CER)**: FlashSpeech demonstrates lower CER, showing its capability to generate speech that closely matches the intended text with fewer errors.\n3. **Speaker Similarity**: The similarity scores reveal that our model excels in maintaining the characteristics of the target speaker's voice, outperforming the baseline models in preserving speaker identity.\n\nAdditionally, our qualitative results, showcased on our demo page, highlight the speech editing and diverse speech sampling capabilities of FlashSpeech, further reinforcing the model's robustness and adaptability to different speech synthesis scenarios."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To explore the impact of different pre-trained models in adversarial training on the audio quality and speaker similarity of synthesized speech.",
            "experiment_process": "The baseline model employs consistency training alone and is evaluated on two metrics: UTMOS and Sim-O. Then, adversarial training is incorporated using different pre-trained models: wav2vec2-large, hubert-large, and wavlm-large as discriminators. These models are compared in terms of their UTMOS and Sim-O scores. Moreover, the effect of sampling steps (NFE) is evaluated, ranging from 1, 2, to 4 steps, observing their impact on the UTMOS and Sim-O metrics.",
            "result_discussion": "The baseline model achieved a UTMOS of 3.62 and a Sim-O of 0.45. Adversarial training significantly improved these scores, especially using the Wavlm-large model, which achieved the highest scores (UTMOS: 4.00, Sim-O: 0.52). However, not using the audio prompt\u2019s feature as a condition showed a slight decrease in performance. Additionally, while increasing the sampling steps (NFE) from 1 to 2 marginally improved both UTMOS and Sim-O scores, further increasing to 4 steps reduced UTMOS due to the accumulation of score estimation errors.",
            "ablation_id": "2404.14700v3.No1"
        },
        {
            "research_objective": "To investigate the effects of a control factor on the prosodic features of pitch and duration in speech synthesis, and understand its role in balancing stability and diversity.",
            "experiment_process": "An ablation analysis was performed by setting another influencing factor to zero and varying the control factor to observe its impact on the prosodic features of pitch and duration. The experiment measured Pitch JSD, Duration JSD, and WER across different settings of the control factor, ranging from 0 to fully incorporating the prosody refinement's residual output.",
            "result_discussion": "With the control factor set to 0, Pitch JSD was 0.072 and WER was 2.8. Slight modification reduced Pitch JSD to 0.067 with the same WER. Fully incorporating prosody refinement's residual output resulted in further decrease in Pitch JSD to 0.063, but increased WER to 3.7, indicating a trade-off between prosody diversity and speech intelligibility. Similar trends were noted in duration analysis where Duration JSD improved from 0.0175 to 0.0153, while WER increased from 2.8 to 3.9, underscoring the need to balance diversity and stability in prosody without compromising intelligibility.",
            "ablation_id": "2404.14700v3.No2"
        }
    ]
}