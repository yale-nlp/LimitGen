{
    "title": "Towards Practical Tool Usage for Continually Learning LLMs",
    "abstract": "Large language models (LLMs) show an innate skill for solving language-based tasks. However, they struggle with adjusting to changes, as their knowledge, embedded within their parameters, does not automatically update over time. The use of external tools can alleviate some of these issues by allowing LLMs to delegate certain tasks to specialized systems through an interface. Despite this, LLMs still need to adapt to dynamic environments because tools can evolve, with new ones emerging and existing ones changing. Since tools require less specialized knowledge, we hypothesize they are more conducive to continual learning (CL) because they focus less on memorizing information and more on determining when to use specific tools. To test this, we create a synthetic benchmark and compile existing NLP tasks to simulate a realistic evaluation scenario. Our findings reveal that merely increasing model size does not address the challenge, even with tool integration. However, continual learning strategies help tool-enhanced LLMs adjust more swiftly while retaining their learned knowledge longer, showcasing their promise as adaptive learners.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Performance of pre-trained LLMs on a variety of domains validates that LLMs possess a representation of knowledge in their parameters. However, such knowledge across domains expires at differential rates\u2014What is the current population of USA? becomes obsolete in a decade while Who is the President of X expires in expectation around every few years, and say What is the current interest rate? expires more frequently. This affects model performance largely because these models store information directly as parametric knowledge and retrieve them when prompted. Alternatively, even if the information within the world does not change at once, the world may change in such a way that the goal of the LLM changes. Hence the consensus is that the generated responses from pre-trained LLMs become unreliable and the LLMs have to adapt to make its generated texts relevant.\n\nThe vanilla approach to avoid staleness is to collect more data that better reflects the current world and re-train from scratch. The disadvantage is that the necessary resources grow with the data and since models store information directly within parameters, additional parameters are needed to hold the new knowledge.\n\nTwo popular alternative solutions are pursued:\nOne\u2014knowledge editing\u2014is based on the assumption that knowledge in LLMs\u2019 parameters can be updated by modifying the parameters directly. But editing factual knowledge can warp the innate knowledge structure of LLMs and approaches that do not directly intervene on the parameters require the use of additional memory. Another is the usage of low-rank adapters, which freezes a base model and introduces smaller adapters which can be used to fine-tune the model for downstream tasks without needing to train it explicitly. However, adapters are task-specific, meaning this can be costly once the number of tasks has grown, and it is the adapter that is tasked with handling changes in the data rather than the model itself.\n\nTangential to the knowledge forgetting problem, LLMs are trained to use tools through APIs and retrieve information from outside sources rather than parameters directly. With tool API, the information being stored outside of LLMs allows for independent updates and a model using it only requires maintaining updates to the tools usage to remain up-to-date. Though this provides a reasonable simplification to the differential expiry rates in knowledge, tool-use itself does not make LLMs everlasting, as both the tools themselves and the set of existing tools can change, which tools LLMs must adapt to. As such, tool-use itself is insufficient for the non-stationary setups as discussed in the continual learning literature, where it is the model that must learn to autonomously adapt to change in either the state of the world as well as downstream tasks. Within this setting, this points at the non-stationarity in the tool definition which can inherently lead to difficulties adjusting to distribution shifts, as learned features for specific tasks often cannot adapt to new ones.\n\nSuch simplification of complex tasks also runs the risk of overfitting to present tasks, leading to forgetting the past by large parametric models. A careful treatment is therefore needed to modify the static knowledge repository of LLMs into models capable of continually adapting to the non-stationarity involved in learning tools that vary in complexity.\n\nWe summarize our work as follows:\nWe propose a synthetic arithmetic dataset with Easy and Difficult splits, and benchmark LLMs of size 125M-13B on using the tools in a task of continual API learning.\nWe show that even with scale, LLMs are incapable of naively adapting to task shifts through sequential fine-tuning highlighting the drawback of mere parametric knowledge to handle distribution shifts.\nHowever, with a replay buffer, we demonstrate that tool LLMs can adapt to these task shifts, whereas standard LLMs still fall short."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Works",
            "text": "Learning in a non-stationary setting has been treated formally in the continual learning (Chen and Liu, 2018  ###reference_b10###) (CL) paradigm. The objective of CL (Thrun, 1998  ###reference_b71###; Kirkpatrick et al., 2017  ###reference_b35###) is to learn from a sequence of tasks without the\nforgetting (French, 1993  ###reference_b17###) of previously seen tasks. With growing emphasis on language based applications, CL in training of LLMs has focused on two main directions:\nTask learning, where LLMs must learn multiple downstream tasks in sequence (Huang et al., 2021  ###reference_b25###; Mehta et al., 2023  ###reference_b46###).\nDomain adaptation, where the LLM is trained on multiple data domains (Gururangan et al., 2020  ###reference_b22###; Ke et al., 2023  ###reference_b33###) and must remain knowledgeable about each.\nHowever, LLMs with large parameteric spaces limit the applicability of regularization-based techniques (Li and Hoiem, 2018  ###reference_b40###; Lopez-Paz and Ranzato, 2017  ###reference_b42###; Zenke et al., 2017  ###reference_b78###; Aljundi et al., 2018  ###reference_b1###) while the few-shot abilities of LLMs (Brown et al., 2020  ###reference_b5###) suggest accommodating replay buffers (Rebuffi et al., 2017  ###reference_b60###; Lopez-Paz and Ranzato, 2017  ###reference_b42###; Shin et al., 2017  ###reference_b67###; Chaudhry et al., 2019a  ###reference_b7###; Wang et al., 2019b  ###reference_b75###) of intractable sizes.\nBecause LLMs are so costly to train (Strubell et al., 2019  ###reference_b69###), delaying their expiry date requires being able to update knowledge cheaply (Zhang et al., 2024  ###reference_b80###).\nWithin this space, two types of methods, parameter-preserving and parameter-editing, have emerged. Parameter-preserving methods, focus on keeping the underlying model intact (Dong et al., 2022  ###reference_b14###; Huang et al., 2023  ###reference_b26###; Hartvigsen et al., 2023  ###reference_b23###; Zhong et al., 2023  ###reference_b82###).\nAdditional parameters or memory to track stale facts could quickly become impractical as the number of edits increases.\nAlternatively, parameter-editing methods directly modify the model parameters through fine-tuning the model to update only a select set of parameters (Zhu et al., 2021  ###reference_b84###; Lee et al., 2022  ###reference_b38###), meta-learning the parameters to edit (Mitchell et al., 2022a  ###reference_b47###), or locating and modifying the relevant parameters (Santurkar et al., 2021  ###reference_b64###; Tanno et al., 2022  ###reference_b70###). This results in fast edits with little to no memory overhead. Yet the complicated structure of LLMs makes this a risky proposition, as modifying even one parameter can have various unknown downstream effects that can affect the usability of the model (Chen et al., 2023  ###reference_b9###).\nLLMs are generalist agents that can be adapted to perform on a wide range of natural language tasks (Brown et al., 2020  ###reference_b5###; Chowdhery et al., 2022  ###reference_b11###).\nHowever, they still struggle in specialized settings (Patel et al., 2021  ###reference_b52###; Lin et al., 2022  ###reference_b41###)\nand have issues disassociating entities from extra-linguistic (Zhang and Choi, 2021  ###reference_b79###) or even spurious (Joshi et al., 2022  ###reference_b31###) contexts.\nTool-augmented LLMs (Schick et al., 2023  ###reference_b65###) address this by learning to manipulate specialized tools to handle the knowledge-based computations. Wang et al. (2022  ###reference_b76###); Imani et al. (2023  ###reference_b27###); Paranjape et al. (2023  ###reference_b51###) have shown improved zero-shot performance across a variety of downstream tasks without drops in language modeling abilities. Tools simplify tasks for LLMs,\npotentially reducing solving a task to learning to route to appropriate tools. However, these prior works do not study how tool LLMs adapt to new tasks or settings.\nThis work attempts to measure the issues that stem from LLMs forgetting by directly learning sequentially through the task samples. By replacing direct-learning with learning with tools,\nthe work reposes the tasks in the tool space, and solves a unified non-stationarity problem of continual learning of tools as a proxy to solve the challenge of continual learning using task samples directly.\n###figure_1###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Motivating Questions",
            "text": "More formally, continually adapting LLMs to the changing world and domain knowledge is a complex but relevant problem, as forgetting prior information can limit the applicability of LLMs. Further, with shifts in domain being aperiodic for diverse knowledge and LLMs being the generalist model they are leads us to the pertinent question:\nCan learning to use tools alleviate sequential learning challenges?\nand the sub-questions that need to be answered:\nHow far can we push by simply increasing parametric knowledge space help for continual learning?\nAre there limits to how much both tool LLMs and vanilla LLMs can learn continually?\nHow do tool LLMs fare with imperfect tools?\nWe use these questions to build our methodology and experimental design in the following sections."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "We use causal Transformer-based language models in a text-generation setup, in particular, the OPT (Zhang et al., 2022  ###reference_b81###) family of pre-trained LLMs up to B parameters. This allows us to compare the powerful similar generative language models with scale.\nTo properly assess the usefulness of tool learning, each sample  consists of a query , the raw answer to the query, , and an API call answer , which can be executed by a task-specific API to obtain a response that is compared with  using exact string matching.\nLanguage models are trained either with tools or without tools to solve a sequence of  tasks\u2014. Each task  defines a specific tool and a dataset  which contains the examples associated with learning the -th tool.\nWith Tools the model learns to generate the API calls, as mentioned previously, that gets routed to appropriate API to generate the answer. Without tools, the model is fine-tuned to predict the answer directly, such as a numerical or textual response.\nIterating over tasks in sequence, at every iteration,, a model is trained with examples corresponding to  and evaluated on test sets of all the tasks the model has seen until then. Each task uses a learning rate warm-up followed by a decay to , i.e. the learning rate warm-up and decay repeats for each task in the set. We use the AdamW (Loshchilov and Hutter, 2019  ###reference_b43###) optimizer with a peak learning rate based on the model size111Hyper-parameters are provided in Appendix C  ###reference_###.\nThe model sees a stream of tasks  in an order without repetition. The model is explicitly fine-tuned on each task and once complete moves to the next task for training.\nAll tasks are mixed into a single task to train a model. This is equivalent to \u201cseeing\u201d all tasks at once and is a strong upper bound, where model learns from all available data at once.\nChaudhry et al. (2019b  ###reference_b8###) augment models with a replay buffer that retains examples from the previous tasks. With the buffer, the model continually takes some of the recent data and randomly replaces older samples. When training, the model will randomly sample a batch from the replay buffer and calculate a replay loss which is added to the standard loss before performing a gradient update. Motivating the usage of this method are observations that LLMs are few-shot learners (Brown et al., 2020  ###reference_b5###), suggesting that this may be an efficient use case of the method given the smaller number of examples and subsequent buffer size that may be necessary.\n: Given that each task  consists of a train and test set, we can measure the accuracy on each test set individually.\nWe report the average accuracy on test sets up to the most recent task on which the model was trained.\nIn particular, suppose a model is being trained on task . The average accuracy is measured as in Equation 1  ###reference_###,\nwhere  denotes the performance of the model on the test set associated with task  after having trained on task .\nIn the tool setup,  is measured by parsing and executing the generated API calls and computed exact match (Rajpurkar et al., 2016  ###reference_b58###) with the true answers.\n(Chaudhry et al., 2018  ###reference_b6###): Forgetting is the average degradation () in performance on all seen tasks excluding the most recent task on which the model was trained.\nEquation 2  ###reference_### measures the average forgetting a model has after trained on .\n(Riemer et al., 2019  ###reference_b61###): Models have limited capacity and can learn a limited number of tasks.\nEquation 3  ###reference_### defines learning accuracy (L-A) to approximate the learning capacity by measuring average performance on each task immediately after being trained on it."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Preliminaries",
            "text": "We use causal Transformer-based language models in a text-generation setup, in particular, the OPT (Zhang et al., 2022  ###reference_b81###  ###reference_b81###) family of pre-trained LLMs up to B parameters. This allows us to compare the powerful similar generative language models with scale.\nTo properly assess the usefulness of tool learning, each sample  consists of a query , the raw answer to the query, , and an API call answer , which can be executed by a task-specific API to obtain a response that is compared with  using exact string matching.\nLanguage models are trained either with tools or without tools to solve a sequence of  tasks\u2014. Each task  defines a specific tool and a dataset  which contains the examples associated with learning the -th tool.\nWith Tools the model learns to generate the API calls, as mentioned previously, that gets routed to appropriate API to generate the answer. Without tools, the model is fine-tuned to predict the answer directly, such as a numerical or textual response.\nIterating over tasks in sequence, at every iteration,, a model is trained with examples corresponding to  and evaluated on test sets of all the tasks the model has seen until then. Each task uses a learning rate warm-up followed by a decay to , i.e. the learning rate warm-up and decay repeats for each task in the set. We use the AdamW (Loshchilov and Hutter, 2019  ###reference_b43###  ###reference_b43###) optimizer with a peak learning rate based on the model size111Hyper-parameters are provided in Appendix C  ###reference_###  ###reference_###."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Baselines",
            "text": "For each setup, we train under a number of settings:\nThe model sees a stream of tasks  in an order without repetition. The model is explicitly fine-tuned on each task and once complete moves to the next task for training.\nAll tasks are mixed into a single task to train a model. This is equivalent to \u201cseeing\u201d all tasks at once and is a strong upper bound, where model learns from all available data at once.\nChaudhry et al. (2019b  ###reference_b8###  ###reference_b8###) augment models with a replay buffer that retains examples from the previous tasks. With the buffer, the model continually takes some of the recent data and randomly replaces older samples. When training, the model will randomly sample a batch from the replay buffer and calculate a replay loss which is added to the standard loss before performing a gradient update. Motivating the usage of this method are observations that LLMs are few-shot learners (Brown et al., 2020  ###reference_b5###  ###reference_b5###), suggesting that this may be an efficient use case of the method given the smaller number of examples and subsequent buffer size that may be necessary."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Evaluation Metrics",
            "text": "We evaluate using the following metrics for measuring performance222Formulas and further details are provided in Appendix D  ###reference_###.:\n: Given that each task  consists of a train and test set, we can measure the accuracy on each test set individually.\nWe report the average accuracy on test sets up to the most recent task on which the model was trained.\nIn particular, suppose a model is being trained on task . The average accuracy is measured as in Equation 1  ###reference_###  ###reference_###,\nwhere  denotes the performance of the model on the test set associated with task  after having trained on task .\nIn the tool setup,  is measured by parsing and executing the generated API calls and computed exact match (Rajpurkar et al., 2016  ###reference_b58###  ###reference_b58###) with the true answers.\n(Chaudhry et al., 2018  ###reference_b6###  ###reference_b6###): Forgetting is the average degradation () in performance on all seen tasks excluding the most recent task on which the model was trained.\nEquation 2  ###reference_###  ###reference_### measures the average forgetting a model has after trained on .\n(Riemer et al., 2019  ###reference_b61###  ###reference_b61###): Models have limited capacity and can learn a limited number of tasks.\nEquation 3  ###reference_###  ###reference_### defines learning accuracy (L-A) to approximate the learning capacity by measuring average performance on each task immediately after being trained on it."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We design our experiments to address Q1-3 in \u00a73  ###reference_### as follows.\n\nUnderstanding the effects of scale requires ensuring that larger models can adequately solve the task when not presented in a sequential learning setting.\nFor Q1, we first construct a synthetic arithmetic dataset consisting of four functions, each with a single template and limited to integers between  to . Every sample in the set adheres to the format defined in \u00a74  ###reference_###. Each operation has an associated API format answer\u2014 ex. Add(, ) where  and  are arguments provided to the tool\u2014 and explicit numerical answers. Non-integers are expressed in decimal format, e.g. . Tasks have the same number of examples, divided into training and test sets333Additional hyper-parameters are found in Appendix C  ###reference_###. We refer to this as our Toy Arithmetic Task.\nUsing this task, we verify if increasing the number of parameters of LLM improves performance, measured through with accuracy and forgetting. If accuracy increases while forgetting decreases with the parameters, then we can infer that link between the size of the parametric space and performance is in fact linked. However, we observe in \u00a76  ###reference_### that this is far from the case.\nFollowing the observations from our previous question, we further the study to find the extent LLMs can learn continually when the task gets difficult.\nAs such, the next goal is to observe some of these limits in both with and without using tools by increasing the task difficulty, for which we build a more difficult arithmetic benchmark.\nWe add difficulty through expanding input and output space along with adding ambiguous templates. To expand the input space, we create additional functions and templates for existing functions which must be learned to properly use the tools. Furthermore, we add more ambiguity to the tool templates by increasing the token similarity for many templates. The output space now covers operands which can include real numbers, hence requiring the models to be capable of identifying which parts of the input properly constitute parts of the tools calls and use this to create the output. We refer to this as our Advanced Arithmetic Task.\nThe goal here is to observe if performance (as measured in the same way in the previous task) can remain constant between the two tasks both with and without using the tools. We expect that although accuracy may drop in part due to the task difficulty, but does forgetting remain consistent between the toy task and this more difficult one? In particular, if forgetting is consistently greater for the advanced task, this highlights limitations in the LLM with respect to becoming more general, autonomous multi-task learners.\nWhile our arithmetic benchmarks are useful for evaluating LLMs in a continual learning setting, the goal remains to use them for more practical use cases. In such cases, oracles are not readily available in the same way as in the arithmetic settings. Explicit rules exist for arithmetic tasks; if one designs a tool to follow said rules, then mastering the tool is equivalent to mastering the task. But this assumption fails to hold in many cases, which motivates our next question: do the benefits of tool learning still hold once we move away from perfect tools."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Results and Analysis",
            "text": "In Figure 2  ###reference_###, we compare the performances of the different sized architectures on the synthetic arithmetic datasets, and the realistic task as described in \u00a75  ###reference_###.\nAs we experiment both directly learning over the samples and learning to use APIs, we observe that generalizing on arithmetic tasks is challenging to learn directly from samples (Tools=No in Figure 2  ###reference_###). Also the forgetting (2(b)  ###reference_sf2###) is significant irrespective of the models using tools to solve the task. Though the learning accuracy for even smaller sized LMs was higher with tools as compared to  larger model without using tools, we observe the retention of past tasks as observed in Accuracy in 2(a)  ###reference_sf1### appears as a prevalent issue across the model sizes.\n###figure_5### While the results demonstrate the effect of LLMs struggle with sequential learning, we look at whether the performance degradation is an artifact that comes with the learning set up. To that, we compare the performances of the models in a mixed dataset setting where the models learn all the tasks at once with and without using the tools. The hypothesis is that if the LMs showed significant retention as indicated with the comparable performances to using tools, it can be regarded that more data potentially solves the forgetting problem. But, to the contrary in Figure 3  ###reference_### we observe that the gap does exist in the different tasks. So, irrespective of using tools or task seen all at once or not LLMs struggle with the generalizing to the tasks.\n4(a)  ###reference_sf1### indicate the effect of model size on the ability of learning tasks to increase with model size. However, from 4(b)  ###reference_sf2###, we fail to see any systematic decrease in the forgetting of the model, suggesting that being able to learn tasks sequentially remains a concern despite the increase in model capacity. Nevertheless, the greater learning accuracy observed with larger models can be useful to unleash the potential of tool LLMs.\n###figure_6### ###figure_7### In particular, we observe in 2(c)  ###reference_sf3### that tool LLMs\u2019 learning accuracy to be consistently higher than vanilla LLMs, suggesting a faster adaptation with tools. Even more encouraging is the fact that learning accuracy for the smallest tool LLMs is often far superior compared to the largest vanilla LLMs. This is promising, as it demonstrates that if one can overcome the forgetting concern that plagues LLMs in general, then using tool LLMs may be much more efficient than vanilla LLMs as they can replace ones that are larger for similar performance. This observation not only is evident when the tools are non-parametric oracles as in our arithmetic tasks but also in the case of our continual GLUE task where tools themselves are parametric models. Though models are no longer oracles, as demonstrated by imperfect learning accuracy (2(c)  ###reference_sf3###), the combined parametric space with smaller experts is still significantly smaller than a vanilla LLM that achieves equivalent performance.\nBy reposing problems in the tool space, models learn only to make the correct API calls and we see smaller models with tools to perform on par with larger models not using tools. Beyond a simplistic comparison, this could also be seen as an economic way to guarantee consistency and truthfulness to the results while not incurring the cost of pre-training larger LLMs as the reliance is less on the more riskier LLMs\u2019 parametric knowledge (Kazemnejad et al., 2023  ###reference_b32###).\nThese results motivate potential opportunities in building smaller models and learnable API calls that can outsmart large LLMs in terms of efficiency with cheaper training costs. While LLMs trained for more complex interaction and usage exist, such as instruction fine-tuned ones (Askell et al., 2021  ###reference_b2###; Ouyang et al., 2022  ###reference_b50###; Dubois et al., 2023  ###reference_b15###), they still rely on the assumption that the underlying world does not change; one can still expect false statements unless they are explicitly trained to rely on outside data sources accessible in a predetermined manner. As such, tool LLMs present an opportunity to move away from larger models and towards smaller, more accessible ones with comparable use.\nBy adopting more wide-spread techniques from continual learning, tool LLMs display significant advantages over prototypical LLMs. In particular, by using replay buffer, we observe that forgetting is alleviated to a significantly higher degree when learning with tools. In Figure 5  ###reference_###, we observe that forgetting drops by  in all tasks.\nBy comparison, forgetting remains in the -% range for arithmetic tasks and % for the GLUE task when not using tools (as observed in Figure 7  ###reference_### in Appendix F  ###reference_###), which are all greater than  the amount of forgetting that occurs with tools and replay. Though we observe that tool LLMs forget more than vanilla LLMs without replay, the amount of forgetting remains significant (over %, % and % for the three tasks) and limits their practical viability.\n###figure_8### What remains important, however, is that models appear capable of learning tools to a much greater capacity, shown by superior learning accuracy throughout.\nThese benefits can be observed when using replay (line in Figure 2  ###reference_###), where we note the models learn to use the tools almost perfectly,\nand the tool LLM can significantly outperform vanilla LLMs in our arithmetic tasks. Even in the case of the more nuanced GLUE task, where the tool is not always correct, benefits are still visible as errors in the final answer result only from the imperfections with the tool, which we can remark due to the fact that the API call accuracy is perfect in these scenarios (see Appendix F  ###reference_###).\nThese observations bring us to hypothesize that through tool use, LLMs become better at utilizing their parametric knowledge, leading to greater task transfer during CL and allowing them to adapt more effectively."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "Studies into language models have shown that pre-training data is oftentimes directly available within trained parameters (Brown et al., 2020  ###reference_b5###; Jiang et al., 2020  ###reference_b30###; Qin and Eisner, 2021  ###reference_b56###) as parametric knowledge.\nHowever, if the knowledge stored is very example dependent, then it is likely not usable Kazemnejad et al. (2023  ###reference_b32###) in many instances, as there is no direct link between the context in which the knowledge was seen and other examples which are presented to the model (Prato et al., 2023  ###reference_b55###). As such, one may question whether this knowledge space could be better used.\nIn contrast, tool learning can generalize the output space, as the learned samples can be more clearly separated into categories based on the tools that are used to solve them. This can make it easier to understand how to handle individual examples from the model perspective and maintain some memory of prior tasks. These observations can explain many of our results, such as improved learning accuracy but greater forgetting when learning tools without replay. If answers are all either numerical values or similar natural language words, there possibly exists a smaller distribution shift that occurs when moving from one task to another. As a result, over-fitting to the answer format may result in a smaller performance degradation.\nTool LLMs assume that the tools themselves are accurate for the task of interest as otherwise it\u2019s existence would be meaningless. But teaching LLMs to make use of tools as auxiliary systems remains a nuanced process; how does it know when to trust the system and take the system response as the truth? There is often a trade-off that exists between speed and performance in these cases; the faster we want the response to be then the more trust we must place in the system to be accurate and not double-guess it.\nTool LLMs can further be seen as an alternative to mixture of expert models (Jacobs et al., 1991  ###reference_b28###; Shazeer et al., 2017  ###reference_b66###; Fedus et al., 2022  ###reference_b16###), which route examples to different experts. However, one can view tool LLMs as a case where the expert exists externally; this leads to a system that may be less coupled with the task.\nHowever, introducing auxiliary systems bring about additional questions. For example, how do we ensure that the model can continuously maintain the ability to use the system properly?\nHow is the knowledge for using tools stored and what does it inform us about how much the LLM knows about the tool?\nThese require further analysis which are necessary both for practical use as well as for understanding LLMs in general.\nForgetting is a natural phenomenon, both in humans (Wang et al., 2020  ###reference_b74###) and neural networks (French, 1999  ###reference_b18###). While it is commonly agreed upon that a symbiotic relationship exists between learning and forgetting within humans (Bjork and Allen, 1970  ###reference_b3###; Bjork and Bjork, 2019  ###reference_b4###; Gravitz, 2019  ###reference_b20###), forgetting is still treated as the cause of various failure modes within machine learning (McCloskey and Cohen, 1989b  ###reference_b45###; Ratcliff, 1990  ###reference_b59###). However works have began to show how forgetting and learning can work together symbiotically (Zhou et al., 2022  ###reference_b83###).\nForgetting is deemed a negative phenomena which hinders models. However, in the real world, this assessment may not hold in many settings. Recall that updating models with ease is important. For this, unnecessary information should be forgotten as quickly as new information is learnt. This shows that forgetting is not a simple black-or-white issue. When information can become out-dated or incorrect, it may be the case that forgetting is desirable, given that it is no longer useful. Therefore, tool-based models displaying higher forgetting but greater learning accuracy may in fact be desirable, as it demonstrates that models can maintain an ability to learn new information but simultaneously discard information that is no longer relevant."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this work, we explore the potential use of tools in continual learning for LLMs. We apply this setup within a simple arithmetic reasoning setup, where a language model is taught multiple math functions. Our results demonstrate that LLMs that learn to generate answers based on tools both adapt faster to new tasks while also maintaining greater performance on prior tasks. Continual learning remains unsolved, as cases still exist where all models fail to demonstrate the ability to autonomously solve tasks. This emphasizes the need for models which can adapt to the world in the same manner as conscious humans. By highlighting current limitations and the potential for tool LLMs in this setting, these results hopefully delineate paths for future research which can allow for more practical LLMs deployed in the real world."
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "Some limitations of this work can be noted in the simplicity of the tools that are explored as well as the degree of relatedness that exists between each tool and how they are used.\nFirst, we note that there exists some relatedness between a number of different functions which we learn due to the granularity at which they are used, which may or may not have resulted in some potential benefits in terms of avoiding catastrophic forgetting. We maintain, however, that we provide enough experimental analysis and results such that this should not pose an issue with the results, hence we believe this to have had minimal effect on potentially producing optimistic results.\nSecond, forgetting (Chaudhry et al., 2018  ###reference_b6###) is a limited metric, as the concept of \u2018more\u2019 forgetting is not well-defined. For example, suppose we take accuracy as our performance metric and are comparing two scenarios. Scenario A has performance degrade by 25% from a peak performance of 80% (80%  60%). Scenario B observes a 10% performance degradation from a peak performance of 30% (30%  27%). In this case, despite scenario B observing less forgetting, we may consider it more problematic as the performance was initially significantly worse than A. As such, developing better metrics for capturing these types of phenomena is important for better analysis within continual learning settings. Additionally, as discussed within the paper, it is unclear if zero forgetting is in fact a desirable property and to what extent this metric is able to capture robustness in learning."
        }
    ],
    "url": "http://arxiv.org/html/2404.09339v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "3"
        ],
        "methodology_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "6"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "4.1",
            "4.2",
            "5"
        ]
    },
    "research_context": {
        "paper_id": "2404.09339v1",
        "paper_title": "Towards Practical Tool Usage for Continually Learning LLMs",
        "research_background": "**Motivation:**\nThe paper addresses the challenge of keeping Large Language Models (LLMs) up-to-date with current knowledge, as the information encoded within these models can become obsolete over time. The dynamic nature of the world\u2014where facts change at different rates\u2014necessitates continual adaptation of LLMs to maintain their reliability and relevance.\n\n**Research Problem:**\nThe central research problem is how to enable LLMs to continually adapt to new information and tasks in a manner that mitigates the issue of knowledge staleness and manages the non-stationarity in tool usage effectively, without excessively increasing resource consumption or distorting the models\u2019 intrinsic knowledge structures.\n\n**Relevant Prior Work:**\n1. **Pre-trained LLM Performance:**\n   - Previous work validated that LLMs possess knowledge within their parameters, such as studies by Raffel et al. (2020), Chung et al. (2022), and Touvron et al. (2023).\n\n2. **Knowledge Expiry and Model Reliability:**\n   - Petroni et al. (2021, 2019) and Roberts et al. (2020) demonstrated that LLMs' knowledge retrieval accuracy diminishes as the information becomes outdated.\n   - Zhang and Choi (2021), and Komeili et al. (2022) discussed the unreliability in generated responses due to knowledge expiration.\n\n3. **Vanilla and Alternative Approaches for LLM Adaptation:**\n   - The conventional method to avoid knowledge staleness involves collecting new data and re-training models from scratch (Gao et al., 2020), which is resource-intensive.\n   - Knowledge editing (De Cao et al., 2021) has been explored but may distort the models\u2019 knowledge structure (Gupta et al., 2023).\n   - Low-rank adapters offer another solution, as proposed by Hu et al. (2022), though are task-specific and costly.\n\n4. **Tool Usage for LLM Adaptation:**\n   - LLMs can be trained to use external tools and APIs to retrieve current information (Schick et al., 2023; Lewis et al., 2020).\n   - Despite reducing reliance on static parametric knowledge, tool usage creates its own challenges in continual learning settings (Ring, 1998; Thrun, 1998; Kumar et al., 2022), such as the need for the model to autonomously adapt to changing tools and dynamic task requirements.\n\n5. **Continual Learning and Task Adaptation:**\n   - McCloskey and Cohen (1989a), French (1993), and Xie et al. (2021) highlighted the risks of overfitting to present tasks, leading to catastrophic forgetting of previous knowledge.\n\nThrough these points, the paper aims to develop practical solutions for tool usage in continually learning LLMs by proposing a synthetic dataset, benchmarking different size models, and demonstrating the advantages of using replay buffers for adaptation.",
        "methodology": "The methodology section of the paper outlines the proposed approach for continually learning large language models (LLMs) with and without the use of tools. Here's a detailed description of the method, including its key components and innovations:\n\n### Models and Setup\n\n- **Causal Transformer-based Models:**\n  - The study employs causal Transformer-based language models in a text-generation setup, specifically using the OPT family of pre-trained LLMs (Zhang et al., 2022). The model sizes used go up to B parameters.\n  - This setup allows for comparisons between different generative language models at scale.\n\n### Data and Evaluation\n\n- **Sample Structure:**\n  - Each sample comprises a query, a raw answer (\\( A \\)), and an API call answer (\\( A^T \\)).\n  - \\( A^T \\) can be obtained by executing a task-specific API, and its response is compared with \\( A \\) via exact string matching.\n\n### Training Methodology\n\n- **With and Without Tools:**\n  - **With Tools:**\n    - The model learns to generate API calls that are routed to appropriate APIs to generate the answer.\n  - **Without Tools:**\n    - The model is fine-tuned to predict the answer directly (numerical or textual).\n\n- **Sequential Training:**\n  - Models are trained sequentially on a series of tasks (\\( T \\)).\n  - Each task \\( t_k \\) defines a specific tool and an associated dataset.\n  - Iteratively, the model is trained on the examples of task \\( t_k \\) and evaluated on all previously seen tasks.\n\n### Training Dynamics\n\n- **Adaptive Learning Rates:**\n  - Each task uses a learning rate warm-up followed by a decay, repeating for each task.\n  - The AdamW optimizer (Loshchilov and Hutter, 2019) is used with a peak learning rate based on model size. Hyper-parameters are detailed in Appendix C.\n\n### Data Handling\n\n- **Replay Buffer:**\n  - The study integrates a replay buffer (Chaudhry et al., 2019b) to retain examples from previous tasks.\n  - The buffer continually updates by adding recent data and replacing older samples. During training, batches are randomly sampled from this buffer, and a replay loss is computed and added to the standard loss before a gradient update.\n\n### Metrics\n\n- **Accuracy Measurement:**\n  - Accuracy is measured on test sets for each task. Specifically, after training on task \\( t_k \\), the model's average accuracy on test sets up to \\( t_k \\) is reported.\n  - In the tool setup, accuracy (\\( A^T_k \\)) is determined by parsing and executing API calls and comparing the results with true answers using exact match (Rajpurkar et al., 2016).\n\n- **Forgetting:**\n  - Forgetting is defined as the average degradation in performance on all previously seen tasks excluding the most recent task. This is formally measured as per Equation 2.\n\n- **Learning Accuracy:**\n  - Learning accuracy (L-A) measures the average performance on each task immediately after training on it and approximates the learning capacity of the model (i.e., how well it retains information from each task). This is defined by Riemer et al., 2019 and measured with Equation 3.\n\n### Innovative Features\n\n- **Tool-based Learning:**\n  - This method introduces a novel approach where the model learns to generate API calls, leveraging external tools to enhance the generated responses.\n\n- **Replay Buffer for Continual Learning:**\n  - The method employs a replay buffer to mitigate forgetting, leveraging the known capacity of LLMs for few-shot learning.\n\nOverall, the proposed methodology aims to improve the practical application of continual learning in large language models by efficiently integrating external tools and managing memory through a replay buffer mechanism. This approach is systematically evaluated through metrics that account for learning capacity, accuracy, and forgetting.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\nThe main experiments are designed to address three key research questions (Q1-3) through a series of tasks and benchmarks. We provide an overview of the experimental setup and the results obtained.\n\n**Toy Arithmetic Task:**\n- **Dataset:** The synthetic arithmetic dataset consists of four functions with a single template and integers between two specified limits. Each task adheres to a defined format, using API call formats like Add(x, y). Datasets are split into training and test sets.\n- **Evaluation Metrics:** Performance is measured using accuracy and forgetting.\n- **Goal:** To verify if increasing the number of LLM parameters improves performance (higher accuracy and lower forgetting).\n- **Key Observation:** Increasing model parameters does not necessarily lead to improved performance, contradicting initial assumptions linking parametric space size to performance.\n\n**Advanced Arithmetic Task:**\n- **Dataset:** This benchmark increases difficulty by expanding the input/output space and adding ambiguous templates. This includes additional functions, templates, and real numbers in operands.\n- **Evaluation Metrics:** Same as the Toy Arithmetic Task (accuracy and forgetting).\n- **Goal:** To determine if performance remains constant with and without using tools as task difficulty increases.\n- **Key Observation:** Although expected that accuracy might drop due to increased difficulty, we aim to see if forgetting remains consistent. Consistently higher forgetting in the advanced task indicates limits in the LLM's capability to become a general multi-task learner.\n\n**Natural Language Understanding (GLUE Subset):**\n- **Dataset:** A subset of tasks from the GLUE benchmark was used, specifically MNLI, QQP, SST-2, and CoLA.\n- **Evaluation Metrics:** Accuracy and forgetting.\n- **Setup:** Each task assumes a specific tool, e.g., QQP requires a paraphrase tool. Tools are not perfect and can make errors, simulating more realistic use cases.\n- **Goal:** To verify if the benefits of tool learning still hold when moving away from perfect tools.\n- **Key Observation:** The study aims to see if gaps observed in the arithmetic setup (perfect tools) disappear when using imperfect tools. This would indicate the robustness of tool learning in more practical applications.\n\nOverall, the main experiments involve synthetic arithmetic datasets and natural language understanding tasks to explore LLM performance in continual learning, especially focusing on accuracy and forgetting as the primary metrics. The results illustrate challenges in scalability, task difficulty, and practical application of tool learning, highlighting the need for further research in these areas."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the effectiveness of tool learning for continually learning large language models (LLMs) compared to traditional methods by investigating if tools enhance the adaptability and reduce forgetting in nonstationary environments.",
            "experiment_process": "The study employs causal Transformer-based language models, specifically the OPT family of pre-trained LLMs. Models are trained either using tools to generate API calls or without tools where they predict answers directly. The models are tested across a sequence of tasks, each featuring distinct datasets and tools. Evaluations are made after every task using test sets from all previously seen tasks, and metrics such as accuracy, forgetting, and learning accuracy are computed. The AdamW optimizer is used, with learning rate warm-up and decay for each task. Replay buffers to retain examples from previous tasks are also used in some setups.",
            "result_discussion": "The study found that models trained with tools showed potential for continual learning by adapting faster and forgetting less compared to those trained without tools. However, simply increasing the number of parameters did not enhance performance significantly. The use of realistic GLUE benchmark tasks further supported that tool learning provided some benefits even when the tools were imperfect, demonstrating a capacity for better continual learning in LLMs.",
            "ablation_id": "2404.09339v1.No1"
        },
        {
            "research_objective": "To investigate whether increasing the parameter size of large language models (LLMs) improves performance measured through accuracy and forgetting in a continual learning setup.",
            "experiment_process": "A synthetic arithmetic dataset with four functions was constructed. Language models of different parameter sizes were trained on tasks with integer inputs and outputs. Performance was measured in terms of accuracy and forgetting on each task after sequential training, examining if larger models enhance these metrics. The AdamW optimizer was employed, and models were iterated over tasks with learning rate adjustments as per the setup\u2019s requirements.",
            "result_discussion": "The results demonstrated that increasing the parameter size did not improve performance in terms of accuracy and forgetting, indicating that scaling model size alone does not address continual learning challenges. This finding emphasizes the need for other methods to enhance the capability of LLMs in nonstationary environments.",
            "ablation_id": "2404.09339v1.No2"
        }
    ]
}