{
    "title": "Chasing COMET: Leveraging Minimum Bayes Risk Decoding for Self-Improving Machine Translation",
    "abstract": "This paper explores Minimum Bayes Risk (MBR) decoding for self-improvement in machine translation (MT), particularly for domain adaptation and low-resource languages. We implement the self-improvement process by fine-tuning the model on its MBR-decoded forward translations. By employing COMET as the MBR utility metric, we aim to achieve the reranking of translations that better aligns with human preferences. The paper explores the iterative application of this approach and the potential need for language-specific MBR utility metrics. The results demonstrate significant enhancements in translation quality for all examined language pairs, including successful application to domain-adapted models and generalisation to low-resource settings. This highlights the potential of COMET-guided MBR for efficient MT self-improvement in various scenarios.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Machine translation (MT) bridges the gap between languages, fostering global communication and information exchange. However, achieving high-quality translations across diverse languages and domains remains a significant challenge, especially for low-resource languages where limited training data hinders model performance. Even in well-resourced settings, continuous improvement and adaptation to specific domains are ongoing research efforts.\n\nThis paper explores the potential of Minimum Bayes Risk (MBR) decoding as a self-improvement strategy for MT models. MBR decoding leverages the model\u2019s predictions to select the best translation from a set of candidates, potentially improving overall translation quality.\n\nWe employ COMET as the utility function in MBR decoding and rerank candidate translations generated by an MT model. This approach creates a synthetic parallel dataset from monolingual data in the source language, enabling further model self-improvement.\n\nThis study examines the effectiveness of MBR decoding for self-improvement in two language pairs: English\u2013German (high-resource) and English\u2013Hausa (low-resource). For English\u2013German, the focus is on the biomedical domain, incorporating additional monolingual data. In the case of English\u2013Hausa, we compare the use of COMET, a massively multilingual metric, with a metric specifically tailored to African languages, i.e., AfriCOMET.\n\nTo determine the optimal configuration for MBR decoding, we investigate two decoding algorithms and various numbers of translation candidates."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "MBR decoding, a technique commonly used in Statistical Machine Translation (SMT), has gained traction in Neural Machine Translation (NMT) in recent years. Proposed methodologies have utilized reference-based metrics, such as BLEURT, and Quality Estimation (QE) models, such as COMET-QE, for reranking the set of hypotheses produced by the NMT model. Past research explored various reranking strategies, including pre-ranking of the set of hypotheses with QE models before passing them into MBR decoding. Improvements have been observed using a MERT-tuned reranker, where multiple QE metrics and model log-likelihood scores are linearly combined with learned weights to maximize a reference-based metric on a validation set.\n\nResearchers have identified biases and weaknesses in COMET models through MBR decoding, noting that early COMET models lack sensitivity to discrepancies in numbers and named entities. The effectiveness of MBR decoding is heavily contingent on the number of samples and sampling strategy employed. Studies have shown that epsilon sampling outperforms other strategies by discarding tokens with probabilities below a certain threshold (epsilon), thereby enhancing sample diversity and ensuring fairer inclusion of tokens in final samples.\n\nQE-fusion is a recent technique introduced to combine spans from different sampled candidates using QE metrics, resulting in consistent improvements in translation quality, particularly with LLMs that can generate diverse outputs. Due to its straightforward implementation, MBR and QE reranking have been successfully applied in machine translation shared tasks, highlighting their potential in enhancing translation quality significantly.\n\nRecent advancements focus on leveraging model outputs for self-improvement, particularly in low-resource and domain-specific scenarios where source-language data is available, but target-language data is limited. One approach, the reinforcement self-training (ReST) method, involves creating a fine-tuning dataset by sampling from the model and scoring samples with a QE metric. Subsequently, offline reinforcement learning is deployed using a reward-weighted loss based on QE scores, demonstrating increases in translation quality.\n\nConcurrent research explored self-tuning NMT models on hypotheses reranked using either MBR, QE, or a combination, discovering that utilizing LLM as the teacher model surpasses using a self-teacher and fine-tuning based on references. Our research builds on these developments by investigating MBR-based fine-tuning in domain-specific translation tasks, specifically focusing on English\u2013German translation in the biomedical domain. We also examine the use of neural QE metrics tailored for specific languages, illustrated by AfriCOMET."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experiment Overview",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Model Self-Improvement",
            "text": "The self-improvement process leverages MBR decoding to guide the model to select high-quality translations according to the utility function. The process consists of 3 steps:\n\n1. Using beam search decoding with beam size equal to N, generate N translation candidates using the base model for each source sentence. Beam search allows for generating a smaller set of high-quality candidates while providing sufficient data for effective MBR decoding.\n\n2. Select a single translation for each source sentence from the list of candidates through MBR decoding utilizing COMET to guide the selection towards high-quality translations. For an efficient implementation of the MBR decoding algorithm, we use the code from the Marian framework.\n\n3. Fine-tune the base model on the synthetically created dataset. Use COMET as an early stopping metric during training to ensure fitting to this metric."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "English\u2013German",
            "text": "The English\u2013German experiment simulates a real-world domain adaptation scenario. In such settings, while a large general-purpose parallel corpus might be available, the specific domain often lacks extensive parallel data. To address this challenge, we leveraged both a smaller parallel dataset and a larger monolingual dataset in the source language containing biomedical terminology.\n\nTo leverage the monolingual data in the source language, we propose a two-step approach:\n\nFine-Tuning: We fine-tune a general-purpose English\u2013German model on a small parallel biomedical dataset.\n\nSelf-improvement: To enhance the model performance in the biomedical domain, we incorporate a larger monolingual biomedical dataset during the self-improvement process. This involves creating a synthetic parallel dataset via MBR decoding and subsequently fine-tuning the biomedical translation model on the generated data.\n\nTo assess the robustness of the self-improvement method, we conducted an additional experiment in which we applied this method to a model that was fine-tuned to the biomedical domain using general domain data for MBR decoding. This evaluated whether the model would retain its translation capabilities in the biomedical domain despite improvements based solely on out-of-domain data."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Czech\u2013Ukrainian",
            "text": "In this setting, we used only the parallel data set without incorporating any additional monolingual data. To employ MBR decoding in this data-scarce environment, we directly translated the entire source side of the parallel dataset using the baseline translation model. This created a set of synthetic candidate translations, which were then reranked through MBR decoding.\n\nIn contrast to our English\u2013German experiments where we incorporated external monolingual data, this setup explored self-improvement without relying on additional datasets. We achieved this by solely leveraging the information present within the data of the base model. This demonstrates the potential for self-improvement even in resource-constrained scenarios."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "English\u2013Hausa",
            "text": "The English\u2013Hausa experiment delves into the critical question of how the choice of a quality evaluation metric influences the effectiveness of self-improvement with MBR decoding. We explored the impact of language coverage in the evaluation metric by comparing two approaches:\nMBR decoding with WMT22 COMET: utilizing the wmt22-comet-da model, which has been trained on direct assessments between a diverse set of language pairs.\nMBR decoding with AfriCOMET: using AfriCOMET-STL, a novel COMET-like metric specifically designed for evaluating translations to and from multiple African languages, including Hausa.\nThe objective of this study was to investigate the effect of language contribution in the neural evaluation metric on the quality of translations decoded using MBR. The comparison of these two approaches specifically addresses whether self-improvement guided by the WMT22 COMET metric, which is trained on a diverse range of language pairs, can effectively generalize to low-resource language pairs. Furthermore, we explore the potential need to use language-specific metrics, such as AfriCOMET-STL for Hausa, to achieve better performance in such scenarios."
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "Iterative MBR Self-Improvement",
            "text": "Following the initial self-improvement through MBR decoding, we explored the possibility of applying it iteratively to further enhance the model\u2019s translation quality. We started each iteration by selecting the best model checkpoint based on the WMT22 COMET metric on the validation set. Next, we performed MBR decoding on the entire training set using this checkpoint, generating a new iteration of the synthetic training set. Finally, we resumed the training of the model using the new training set, starting from the previously selected checkpoint. The iterative process was repeated until a decrease was observed in the evaluation scores of metrics other than WMT22 COMET. In the case of English\u2013German biomedical translation, the process was continued until the model\u2019s quality improved solely on an in-domain test set and decreased on a general domain test set, as this could indicate potential overfitting to the biomedical domain."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experimental Setup",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Data Filtering",
            "text": "We filtered the general training data using the following heuristic filters:\naverage length of words in each sentence (character-wise) ;\nnumber of characters in each sentence ;\ndigits in a sentence (character-wise) ;\nnumber of characters in the longest word ;\nnumber of words in sentence ;\nLevenshtein distance between source and target sentences ;\nnumber of characters in each sentence ;\nprobability that each sentence is in the correct language .\nTo ensure that each sentence is in the correct language, we have used the fastText LID-201 language identification model [Burchell et al. (2023  ###reference_bx4###].\nThe Bicleaner-AI model [Zaragoza-Bernabeu et al. (2022  ###reference_bx38###] is also used to filter the English\u2013German dataset. This tool estimates the likelihood that a sentence pair constitutes a mutual translation. A threshold of 50% is established for the Bicleaner score within this language pair. Bicleaner-AI is not utilized for other language pairs due to the unavailability of open-source models for those languages."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Vocabulary",
            "text": "We employed SentencePiece [Kudo and Richardson (2018  ###reference_bx17###], a subword tokenization library, to train unigram tokenizers for each language pair in our experiments.\nFor the English\u2013German and English\u2013Hausa setups, we created a joint vocabulary containing 32,000 subword tokens and tied all embeddings during the training of the MT model. In contrast, for Czech\u2013Ukrainian, due to different scripts (Latin and Cyrillic), we created separate vocabularies of 32,000 subword tokens and tied only the target and output layer embeddings."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Baseline Model Hyperparameters",
            "text": "For all experiments, we trained Transformer (big) [Vaswani et al. (2017  ###reference_bx34###] models using the Marian framework. These models were trained on four NVIDIA A100 GPUs, each equipped with 80GB of VRAM.\nHyperparameter Settings:\nlearning rate: 2e-4;\nlearning rate warmup: 8000 updates;\nlearning rate decay: inverse square root;\nmini-batch size determined automatically to fit GPU memory;\nearly stopping after 10 consecutive validations with no improvement in mean word cross-entropy score."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Evaluation metrics",
            "text": "We use sacreBLEU [Post (2018  ###reference_bx26###] to calculate BLEU222BLEU signature: nrefs:1case:mixedeff:notok:13asmooth:expversion:2.3.1 [Papineni et al. (2002  ###reference_bx24###] and chrF333chrF signature: nrefs:1case:mixedeff:yesnc:6nw:0space:noversion:2.3.1 [Popovi\u0107 (2015  ###reference_bx25###].\nWe acknowledge the potential for overfitting to the WMT22 COMET444https://huggingface.co/Unbabel/wmt22-comet-da metric used for MBR decoding. Therefore, we extended the evaluation to also include CometKiwi555https://huggingface.co/Unbabel/wmt22-cometkiwi-da [Rei et al. (2022  ###reference_bx29###], UniTE666https://huggingface.co/Unbabel/unite-mup [Wan et al. (2022  ###reference_bx36###], UniTE-DA777https://huggingface.co/Unbabel/wmt22-unite-da [Rei et al. (2023  ###reference_bx30###] and BLEURT-20888https://storage.googleapis.com/bleurt-oss-21/BLEURT-20.zip [Sellam et al. (2020b  ###reference_bx32###].\nFor the English\u2013Hausa experiments, we additionally calculated scores using AfriCOMET-STL [Wang et al. (2023  ###reference_bx37###], which was specifically trained to evaluate translations involving certain African languages."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "English to German",
            "text": "To train the baseline model, we used all corpora from the MTData toolkit (version 0.4.0) [Gowda et al. (2021  ###reference_bx11###], excluding the validation sets and the test sets from the available datasets. Our filters described in Section 4.1  ###reference_### reduced the dataset from approximately 800 million sentences to 400 million.\nIn the context of domain adaptation, we employed the following list of domain data:\n40 thousand sentences from biomedical-translation-corpora [Neves et al. (2016  ###reference_bx20###];\n3 million sentences from Ufal medical corpus shared in WMT23 [Kocmi et al. (2023  ###reference_bx15###];\n2 million sentences from EMEA corpus downloaded from OPUS [Tiedemann and Nygaard (2004  ###reference_bx33###].\nAfter deduplication, we were left with 3 million sentences which we split into two datasets. We considered a scenario with 1 million bilingual parallel sentences and approximately 2 million monolingual sentences in the source language. Khresmoi-dev [Du\u0161ek et al. (2017  ###reference_bx6###] concatenated with FLORES-200 [NLLB Team et al. (2022  ###reference_bx21###] was utilized as the validation set during training. We did not apply any filtering to the domain data.\nWe used the above data to train the following models:\nBaseline (Baseline) \u2013 model trained only on data from the MTdata toolkit.\nBaseline + mix-tuning (Mix-tune) \u2013 fine-tuned Baseline model on 1 million in-domain bilingual data concatenated with 1 million general-domain data randomly sampled from the Baseline training set.\nBaseline + domain MBR (Base-domain-mbr) \u2013 fine-tuned Baseline model on 2 million domain-specific sentences from MBR-decoded forward translations.\nMix-tuned + domain MBR (Mix-tune-domain-mbr) \u2013 fine-tuned Mix-tune model on 2 million domain-specific sentences from MBR-decoded forward translations.\nMix-tuned + MBR-iteration2 (Mix-tune-domain-mbr-iter2) \u2013 fine-tuned Mix-tune-domain-mbr on the 2 million domain-specific sentences from MBR-decoded forward translations.\nMix tuned + general-MBR (Mix-tune-general-mbr) \u2013 fine-tuned Mix-tune model on 2 million sentences sampled from the general-domain corpora from the Baseline training set as MBR-decoded forward translations.\nWhen fine-tuning the Mix-tune model, we tailor the learning rate setup to meet specific requirements: learn-rate: 1e-7, lr-decay-inv-sqrt: 16000, lr-warmup: 16000. All remaining fine-tuning procedures employ an adjusted learning rate set to 5e-6."
        },
        {
            "section_id": "4.6",
            "parent_section_id": "4",
            "section_name": "Czech to Ukrainian",
            "text": "We leveraged all of the Czech\u2013Ukrainian parallel data from the WMT23 MTData recipe, resulting in approximately 8 million sentence pairs after filtering as described in Section 4.1  ###reference_###. We did not include any additional monolingual data in this experiment.\nWe utilized the FLORES-200 dataset for validation during training, while the WMT22 test set served as an additional benchmark.\nWe trained the baseline model only on the parallel data, using hyperparameters as described in Section 4.3  ###reference_###. Next, we translated the source side of the parallel corpus used in training with our baseline model, saving a list of translation candidates. We performed MBR decoding, selecting the best translation of each set of candidate translations, resulting in a synthetic training dataset.\nWe investigated the following approaches to leverage the MBR-decoded data for model improvement:\nStandard fine-tuning (MBR-finetuned) \u2013 we fine-tuned the baseline model on the MBR-decoded data, using a learning rate of  5e-6.\nFine-tuning with a high learning rate (MBR-ft-high-lr) \u2013 we fine-tune the baseline model on MBR-decoded data, using a learning rate of 2e-4.\nResuming training with MBR-decoded data (MBR-resumed) \u2013 we switched the training set to the MBR-decoded version and resumed training, restoring the optimizer state and effectively continuing its training with the improved data."
        },
        {
            "section_id": "4.7",
            "parent_section_id": "4",
            "section_name": "English to Hausa",
            "text": "To train the models in the English\u2013Hausa direction, we used data from the WMT shared tasks from previous years. Specifically, we used:\n7 million sentences from OPUS;\n2.4 million data from the WMT23 African MT Shared Task  [Kocmi et al. (2023  ###reference_bx15###];\n150 thousand sentences from ParaCrawl v8.0 [Ba\u00f1\u00f3n et al. (2020  ###reference_bx3###].\nThe deduplication process reduced the data size to approximately 9 million sentences. Following the filtering criteria detailed in Section 4.1  ###reference_###, a total of 3.1 million sentences were retained. We used FLORES-200 for validation during training. After training, we evaluated the model on the FLORES-200 and NTREX test sets.\nWe took similar steps as in the Czech\u2013Ukrainian experiment, training a baseline model with hyperparameters set as described in Section 4.3  ###reference_###. We conducted experiments employing MBR decoding, comparing its performance using two distinct metrics as the utility function:\nWMT22 COMET \u2013 based on XLM-RoBERTa [Conneau et al. (2020  ###reference_bx5###], covering a diverse set of 100 languages,\nAfriCOMET-STL \u2013 based on AfroXLM-RoBERTa [Alabi et al. (2022  ###reference_bx1###], covering 17 African languages and 3 high-resource languages.\nWe investigated the impact of the chosen metric for MBR decoding by training two models using the refined translations:\nMBR-COMET \u2013 training resumed with the training set switched to the WMT22 COMET MBR-decoded version.\nMBR-AfriCOMET \u2013 training resumed with the training set switched to the AfriCOMET-STL MBR-decoded version."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "The statistical significance of the evaluation results is assessed using a paired bootstrap resampling test [Koehn (2004  ###reference_bx16###], involving 1000 resampling trials to confirm the statistical significance of the model improvements (p )."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Number of translation samples and search algorithm",
            "text": "To determine the optimal setup for MBR decoding, we conducted experiments involving the translation and evaluation of chosen test sets with various MBR decoding sample sizes and two decoding algorithms. This approach offers the advantages of being both representative and computationally efficient compared to training MT models on the entire MBR-decoded training set.\nWe evaluated two decoding algorithms \u2013 beam search and top-k. For the top-k setup, we experimented with temperature values of 0.1 and 1, keeping the k parameter equal to 10. These choices were based on the work\ndone by ?). To determine the best number of samples for MBR decoding we conducted experiments with the following numbers of samples: 10, 25, 50, 100, 200, 300, 400, 500.\nFirstly we noted that beam search is the preferred option, given its high scores and greater stability across different metric results, as observed in Figure 1  ###reference_### and 2  ###reference_###. We provide more specific results in the Appendix Figures 4  ###reference_###, 5  ###reference_###.\n###figure_1### ###figure_2### Secondly, we decided to train our models on MBR-decoded data from 50 candidates selected by the beam search decoding algorithm. We considered the balance between improvement in evaluation metrics based on neural language models, stability across lexical metrics, and the execution time of MBR decoding, as shown in Figure 3  ###reference_###. We provide more detailed results in the Appendix Figures 6  ###reference_###, 7  ###reference_###, 8  ###reference_###, 9  ###reference_###, 10  ###reference_###, 11  ###reference_###, 12  ###reference_###.\n###figure_3###"
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "English to German",
            "text": "Table 1  ###reference_### shows the evaluation results on the in-domain test set khresmoi. All models self-improved with MBR decoding have shown enhanced performance. However, model Mix-tune-domain-mbr-iter2 did not exhibit improvement over its first iteration Mix-tune-domain-mbr, even on COMET, which was the utility metric of MBR decoding. Mix-tune-general-mbr model shows a slightly better performance on BLEURT metric compared to models fine-tuned on in-domain MBR-decoded forward translations.\nTable 2  ###reference_### presents the evaluation results on the FLORES-200 test set. Although chrF did not increase, the neural evaluation metrics showed improvement. Similar to the khresmoi test set, the Mix-tune-domain-mbr-iter2 model showed a decrease in quality during the second iteration of self-improvement. Mix-tune-general-mbr showed superior performance over other models.\nIn summary, our findings demonstrate that applying MBR decoding significantly improves the performance of the high-resource English\u2013German model for low-resource biomedical domain translation, particularly on neural network metrics. While lexical metrics show lower stability, they also hold potential for improvement.\nExperiments demonstrated the robustness of self-improving models with the MBR decoding technique. Model fine-tuned on general forward translation had great performance on the in-domain test set and the model fine-tuned on domain-specific forward translation maintained performance on the general domain test set. We provide a broader evaluation in the Appendix Tables 9  ###reference_###, 10  ###reference_###, 11  ###reference_###, 12  ###reference_###."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Czech to Ukrainian",
            "text": "###table_1### ###table_2### ###table_3### ###table_4### The results of the three MBR self-improvement approaches described in Section 4.6  ###reference_### are presented in Tables 3  ###reference_### and 4  ###reference_### for the FLORES-200 and WMT22 test sets, respectively.\nWe find that standard fine-tuning of the baseline model with MBR-decoded data yields the smallest improvements across all metrics, suggesting its limited effectiveness in this context. We note that both fine-tuning with a higher learning rate and resuming the training exhibit comparable performance, with resumed training achieving slightly better results on the WMT22 test set. This may indicate that resuming training helps mitigate overfitting to the FLORES-200 validation set used during training.\nTables 5  ###reference_### and 6  ###reference_### showcase the impact of iterative training with MBR decoding on the FLORES-200 and WMT22 test sets, respectively. The second iteration consistently improves scores across all metrics, demonstrating the effectiveness of the iterative self-improvement process in refining the model\u2019s translation capabilities. However, the third iteration leads to a decrease in both chrF and BLEURT scores. This suggests potential overfitting to the MBR decoding utility metric, where the model prioritizes aspects that score well according to COMET but may not translate to overall translation quality.\nWe provide extended evaluations in the Appendix in Tables 13  ###reference_###, 14  ###reference_###, 15  ###reference_###, 16  ###reference_###."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "English to Hausa",
            "text": "This section compares the performance of two MBR decoding self-improvement approaches for English\u2013Hausa translation: one utilizing the WMT22 COMET model and another using the AfriCOMET model. The results are presented in Tables 7  ###reference_### and 8  ###reference_### for the FLORES-200 and NTREX test sets, respectively.\nWe observe that the AfriCOMET MBR-tuned model achieves gains over the WMT22 COMET MBR-tuned model on chrF for the FLORES-200 test set, but this advantage is not replicated on the NTREX test set. Additionally, the gains from AfriCOMET MBR-tuning are mainly limited to the AfriCOMET metric.\nOur analysis reveals that the MBR-AfriCOMET model exhibits improvements over the MBR-COMET model primarily on lexical metrics in the case of the FLORES-200 test set, but not in the case of NTREX. The gains of the MBR-AfriCOMET model are mainly limited to AfriCOMET metrics, while other neural-based metrics consistently favour the MBR-COMET model.\nWhile WMT22 COMET might exhibit a lower correlation with human judgment for the English\u2013Hausa language pair than AfriCOMET, as reported by ?), both self-improved models achieved significant and comparable gains on AfriCOMET. This suggests that WMT22 COMET, can still correctly rerank translation candidates and effectively guide the self-improvement process, leading to improvements on AfriCOMET, a metric specifically designed for African languages. This finding suggests that self-improvement guided by WMT22 COMET, with its diverse language coverage, might be effective even in low-resource settings, potentially reducing the need for additional adaptation of neural evaluation models to individual languages.\nAdditional evaluations are provided in the Appendix in Tables 17  ###reference_###, 18  ###reference_###."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "This study demonstrated the effectiveness of model self-improvement through MBR decoding in improving translation quality. This approach proves beneficial for both high and low-resource languages, offering versatility in its application across diverse scenarios. Examples include domain-specific translation and the enhancement of general translation models. We conducted experiments with various sample sizes for MBR decoding, using two decoding algorithms: beam search and top-k. The aim was to find a balance between automatic metric gains and time efficiency. Our experiments have shown that the beam search algorithm with a beam size set to 50 is the optimal choice.\n\nIn the field of high-resource English-to-German biomedical translation, we investigated the impact of domain adaptation using various self-improvement approaches on MBR-decoded forward-translated data. Experiments showed that all MBR-based fine-tuning, regardless of the domain of the test set, improved performance compared to the baseline model. This finding highlights the robustness of the self-improvement technique.\n\nIn the English\u2013Hausa experiments, we employed two models for MBR decoding: WMT22 COMET and AfriCOMET. Both models yielded comparable and significant improvements in automatic metrics, indicating their effectiveness in guiding the self-improvement process. While AfriCOMET, specifically trained on African languages, might intuitively seem favourable for this language pair, the performance of the MBR-COMET model highlights the potential of utilizing more widely applicable metrics like WMT22 COMET even for low-resource settings."
        }
    ],
    "url": "http://arxiv.org/html/2405.11937v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3.1",
            "3.2",
            "3.3",
            "3.4",
            "3.5",
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ],
        "main_experiment_and_results_sections": [
            "4.5",
            "4.6",
            "4.7",
            "5",
            "5.1",
            "5.2",
            "5.3",
            "5.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "1",
            "3.1",
            "3.2",
            "3.3",
            "3.4",
            "3.5",
            "4.5",
            "4.6",
            "4.7",
            "5",
            "5.1",
            "5.2",
            "5.3",
            "5.4"
        ]
    },
    "research_context": {
        "paper_id": "2405.11937v1",
        "paper_title": "Chasing COMET: Leveraging Minimum Bayes Risk Decoding for Self-Improving Machine Translation",
        "research_background": "### Motivation:\nMachine Translation (MT) is crucial for fostering global communication and information exchange, but achieving high-quality translations across different languages and domains remains a significant challenge. This challenge is especially pronounced for low-resource languages where limited training data hampers model performance. Even in well-resourced settings, continuous improvement and adaptation to specific domains are ongoing research efforts. The motivation behind this paper is rooted in addressing these limitations by exploring innovative self-improvement strategies for MT models.\n\n### Research Problem:\nThe central research problem explored in this paper is the potential of Minimum Bayes Risk (MBR) decoding as a self-improvement strategy for MT models. Specifically, the paper investigates whether leveraging MBR decoding can enhance the overall translation quality by reranking candidate translations and creating synthetic parallel datasets from monolingual data. The study also examines the effectiveness of this approach across different language pairs, including both high-resource and low-resource languages, and evaluates the use of different utility functions (COMET and AfriCOMET) in the MBR process.\n\n### Relevant Prior Work:\n- **MBR Decoding:** The paper builds upon the foundational work of Kumar and Byrne (2004), which introduced Minimum Bayes Risk decoding as a method for selecting the best translation from a set of candidates based on expected loss minimization.\n- **COMET:** The utility function employed for MBR decoding in this study is COMET, as proposed by Rei et al. (2020), which has been shown to effectively evaluate translation quality.\n- **AfriCOMET:** For African languages, the paper references Wang et al. (2023), who developed AfriCOMET, a metric tailored to evaluate translations in African languages.\n\nOverall, this paper aims to leverage these prior advancements in MBR decoding and quality evaluation metrics to develop a self-improving machine translation approach that can be applied to various language pairs and domains.",
        "methodology": "### Methodology\n\nThe proposed methodology employs a self-improvement process driven by Minimum Bayes Risk (MBR) decoding to aid the model in selecting high-quality translations based on a utility function. The method comprises three primary steps:\n\n1. **Generating Translation Candidates:**\n   Using beam search decoding with a preset beam size \\( N \\), the base model generates \\( N \\) translation candidates for each source sentence. While prior studies suggested epsilon sampling as potentially more effective with MBR decoding, it often necessitates reranking a substantially larger number of translation candidates, thus becoming computationally prohibitive for large datasets. Conversely, beam search strikes a balance by generating a smaller set of high-quality candidates while ensuring adequate data for efficient MBR decoding.\n\n2. **Selecting High-Quality Translations:**\n   For each source sentence, a single translation is chosen from the generated candidates through MBR decoding, which uses COMET to guide the selection process towards high-quality translations. To implement the MBR decoding algorithm efficiently, we utilize the code available at [Marian's GitHub repository](https://github.com/marian-nmt/marian-dev/tree/master/scripts/mbr) from the Marian framework, as referenced by Junczys-Dowmunt et al. (2018).\n\n3. **Fine-Tuning the Base Model:**\n   The base model is then fine-tuned on the synthetically created dataset. During this training phase, COMET serves as an early stopping metric to ensure the model is well-aligned with this metric.\n\nBy integrating MBR decoding and COMET evaluations, this methodology aims to iteratively enhance the machine translation model's performance by systematically selecting and training on superior translations.",
        "main_experiment_and_results": "**Main Experiment Setup:**\n\n1. **Datasets:**\n   - **General Domain Data:** Data used from the MTData toolkit (version 0.4.0), filtered down from approximately 800 million sentences to 400 million sentences.\n   - **Domain-Specific Data:** \n     - 40 thousand sentences from biomedical-translation-corpora.\n     - 3 million sentences from Ufal medical corpus (WMT23).\n     - 2 million sentences from the EMEA corpus (OPUS).\n     - After deduplication, reduced to 3 million sentences.\n\n   - **Validation Set:** Khresmoi-dev concatenated with FLORES-200.\n\n2. **Model Configurations:**\n   Six different models were trained and fine-tuned:\n   - **Baseline:** Trained only on data from the MTdata toolkit.\n   - **Baseline + mix-tuning (Mix-tune):** Fine-tuned Baseline model on 1 million in-domain bilingual data and 1 million randomly sampled general-domain data.\n   - **Baseline + domain MBR (Base-domain-mbr):** Fine-tuned Baseline model on 2 million domain-specific sentences from MBR-decoded forward translations.\n   - **Mix-tuned + domain MBR (Mix-tune-domain-mbr):** Fine-tuned Mix-tune model on 2 million domain-specific sentences from MBR-decoded forward translations.\n   - **Mix-tuned + MBR-iteration2 (Mix-tune-domain-mbr-iter2):** Fine-tuned Mix-tune-domain-mbr model further on 2 million domain-specific sentences from MBR-decoded forward translations.\n   - **Mix tuned + general-MBR (Mix-tune-general-mbr):** Fine-tuned Mix-tune model on 2 million sentences sampled from the general-domain corpora from the Baseline training set as MBR-decoded forward translations.\n\n3. **Fine-Tuning Specifics:**\n   - Learning rate setup for Mix-tune model: `learn-rate: 1e-7, lr-decay-inv-sqrt: 16000, lr-warmup: 16000`.\n   - For other fine-tuning procedures: Learning rate set to `5e-6`.\n\n**Evaluation Metrics:**\n\n- Metrics used for evaluation were not explicitly mentioned in the provided section, but common metrics for machine translation tasks generally include BLEU scores, COMET, or other accuracy-based metrics.\n\n**Main Experimental Results:**\n\n- The specific experimental results were omitted in the provided text.\n\n**Summary:**\n\nThe main experiment involved training and fine-tuning various models on a combination of general domain and specialized domain datasets. These models were evaluated to see how well they leveraged Minimum Bayes Risk (MBR) decoding and fine-tuning strategies for improving domain-specific translations. However, detailed experimental outcomes and specific evaluation metrics were not provided in the excerpt."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To assess the potential improvements in translation quality through Minimum Bayes Risk (MBR) decoding, specifically in a real-world domain adaptation scenario for the English-German language pair focusing on the biomedical domain.",
            "experiment_process": "The experiment was conducted in two steps: First, fine-tuning a general-purpose English-German model on a small parallel biomedical dataset. Then, the self-improvement process involved incorporating a larger monolingual biomedical dataset to create a synthetic parallel dataset through MBR decoding. Subsequent fine-tuning was done on this synthetic dataset. The study used the Marian framework for MBR decoding and evaluated the robustness of this self-improvement method by applying it to a model already fine-tuned on biomedical domain data using general domain data for MBR decoding.",
            "result_discussion": "All models self-improved with MBR decoding showed enhanced performance on the in-domain test set (khresmoi). However, the model Mix-tune-domain-mbr-iter2 did not exhibit improvement over its first iteration Mix-tune-domain-mbr, even on COMET, the utility metric of MBR decoding. On the general domain test set (FLORES-200), neural evaluation metrics improved while chrF did not. The Mix-tune-general-mbr model outperformed in several metrics. The findings highlight the robustness and potential efficiency of MBR decoding in domain adaptation, though some challenges with stability on lexical metrics remain.",
            "ablation_id": "2405.11937v1.No1"
        },
        {
            "research_objective": "To evaluate the effectiveness of MBR decoding for self-improvement in translation quality between Czech and Ukrainian, two low-resource languages, without incorporating additional monolingual data.",
            "experiment_process": "The baseline model was trained on approximately 8 million parallel sentence pairs from the WMT23 MTData recipe. The source side of the parallel corpus was translated by the baseline model, forming synthetic candidate translations, which were then reranked through MBR decoding. Three different fine-tuning techniques were employed: standard fine-tuning, fine-tuning with a higher learning rate, and resuming training with MBR-decoded data.",
            "result_discussion": "Standard fine-tuning with MBR-decoded data yielded the smallest improvements, indicating limited effectiveness. Fine-tuning with a higher learning rate and resuming training showed comparable performance, with resumed training performing slightly better. Iterative training with MBR decoding consistently improved scores in the second iteration but led to a decrease in chrF and BLEURT scores in the third iteration, suggesting potential overfitting to the MBR utility metric.",
            "ablation_id": "2405.11937v1.No2"
        },
        {
            "research_objective": "To compare the effect of different quality evaluation metrics on the performance of MBR decoding in self-improvement for the English-Hausa language pair.",
            "experiment_process": "Two quality evaluation metrics were compared: the WMT22 COMET, trained on diverse language pairs, and AfriCOMET-STL, specifically designed for African languages. Models were trained on approximately 3.1 million sentences after filtering, and the impact of chosen metrics was studied by training two models on MBR-decoded translations guided by these metrics.",
            "result_discussion": "The AfriCOMET MBR-tuned model showed gains on chrF for the FLORES-200 test set but not on the NTREX set. The gains were mainly limited to the AfriCOMET metric, while neural-based metrics consistently favored the MBR-COMET model. Both models achieved significant gains on AfriCOMET, suggesting that the WMT22 COMET can effectively guide self-improvement even in low-resource scenarios, reducing the necessity for language-specific evaluation models.",
            "ablation_id": "2405.11937v1.No3"
        },
        {
            "research_objective": "To determine the impact of iterative application of MBR decoding on the translation quality of models.",
            "experiment_process": "After the initial self-improvement via MBR decoding, further iterations were applied. Each iteration began by selecting the best model checkpoint, performing MBR decoding on the entire training set using this checkpoint, and resuming training with the new synthetic training set. The process was continued until a decrease was observed in evaluation scores of other metrics.",
            "result_discussion": "Iterative self-improvement led to enhanced scores across all metrics in the first two iterations but showed a decrease in quality during the third iteration, indicating potential overfitting to the MBR utility metric. The iterative process was effective in refining the model's translation capabilities but requires careful monitoring of metric scores to prevent overfitting.",
            "ablation_id": "2405.11937v1.No4"
        }
    ]
}