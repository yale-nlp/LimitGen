{
    "title": "Cross-Architecture Transfer Learning for Linear-Cost Inference Transformers",
    "abstract": "Recently, multiple architectures have been proposed to improve the efficiency of Transformer Language Models through changes to the self-attention block to achieve linear-cost inference (LCI). A notable approach is the State-Space Machines (SSMs) architecture, which showed comparable performance on language modeling tasks with self-attention transformers. However, such architectural changes require a full pretraining of the weights from scratch, incurring a significant cost for researchers and practitioners. In traditional linear attention works, it has been proposed to approximate full attention with linear attention using the swap-and-finetune framework (Kasai et al., 2021). Motivated by this approach, we propose Cross-Architecture Transfer Learning (XATL), where the weights of shared components between LCI and self-attention-based transformers, such as layernorms, MLPs, and input/output embeddings, are directly transferred to the new architecture from pre-trained model parameters. We experimented with the efficacy of the method on varying sizes and alternative attention architectures and found that XATL significantly reduces training time by up to 2.5x and converges to a better minimum, producing a model up to 2.6% stronger on LM benchmarks within the same compute budget. The code and instructions to download the weights are published at https://github.com/syncdoth/lit_llm_train.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The NLP community is witnessing a proliferation of large language models (LLMs) released regularly, including both large, closed-source models like OpenAI GPT-4 and ChatGPT, Google\u2019s PaLM, Anthropic AI\u2019s Claude, and smaller, open-source models such as GPT, LLaMA 1&2, Mistral-7B, and Falcon-7B. These models are based on Transformer architecture, which uses self-attention with causal masking. Unlike Recurrent Neural Networks (RNNs), Transformers allow for efficient, scalable training by stacking layers. However, RNNs have the advantage of inference efficiency, as they maintain a single global cache of previous states, leading to constant time operations per generation step. Transformers, in contrast, require maintaining a full history of past KV-caches, resulting in quadratic inference time.\n\nRecent research seeks to improve self-attention's quadratic inference cost. Innovations include linear attention transformers that approximate full softmax attention in linear time, proposed by Katharopoulos et al., Kitaev et al., and others. State-Space Machines present new time-mixing methods, such as RWKV, RetNet, H3, and Hyena, enabling parallel training and recurrent inference. Additionally, Mamba introduces efficient scan algorithms for feasible long-sequence training and recurrent inference. These are categorized as Low-Cost Inference Transformers (LCI).\n\nWhile promising fast inference, these architectures typically require pre-training from scratch, demanding substantial computational resources. This may limit their broader use since savings from efficient inference must be offset against pre-training costs. To address this, we propose a transfer learning approach inspired by the Transformer-to-RNN (T2R) framework. In our Cross-Architecture Transfer Learning (XATL) framework, transformer weights are directly transferred to new architectures. We keep the existing transformer architecture intact, swapping attention layers with efficient components, and training on language tasks. Most LCI and Transformer variations share key components, such as layer norms and residual connections, allowing XATL to transfer many weights, benefiting from strong initialization and accelerated training. Our experiments demonstrate that XATL can achieve equivalent performance with significantly less computation and improve performance on language modeling and commonsense benchmarks with the same compute budget."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Linear Attention",
            "text": "The works under the linear attention category proposes methods to approximate the softmax based attention with various kernels (Katharopoulos et al., 2020  ###reference_b19###; Qin et al., 2022  ###reference_b25###; Kitaev et al., 2020  ###reference_b20###; Wang et al., 2020  ###reference_b34###; Zhai et al., 2021  ###reference_b37###). The research focus is to design a kernel or method that can approximate the full expressivity of attention (Zhang et al., 2024  ###reference_b38###; Kasai et al., 2021  ###reference_b18###), and propose to swap-then-finetune the linear attention only (T2R), or set the attention matrix as direct learning target and distill the attention matrix first. These methods therefore did not require pretraining from scratch and could substitute attention directly. However, they fail to retain local granularity when it comes to associative memory recall tasks (Arora et al., 2023  ###reference_b5###), and may approximate the attention too smoothly when the attention matrix can actually be characterized as \u201cspiky\u201d (Zhang et al., 2024  ###reference_b38###)."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "State Space Machines (SSMs)",
            "text": "Recently, a new suite of models under the term Space Space Machines (SSMs) were introduced. Popular models include RWKV (Peng et al., 2023  ###reference_b23###), RetNet (Sun et al., 2023  ###reference_b30###), H3 (Fu et al., 2023  ###reference_b13###), Hyena (Poli et al., 2023  ###reference_b24###), and Mamba (Gu & Dao, 2023  ###reference_b16###). These blocks typically allow both parallel and recurrent view of the time mixing algorithm, allowing for fast parallel training and fast recurrent inference. In a practical standpoint, these models show great promise as their language model benchmark performances is on par with the State-of-the-Art Transformer models while being much efficient at inference time. However, they have new components (SSM Blocks) in its architecture, they must be pre-trained on a large-scale dataset first to be accessible by the general public. This work tries to address the expensive requirement of such pre-training by reusing the shared components\u2019 weights from a well-established pretrained transformer parameters."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Weight Reusing",
            "text": "We were also inspired by the Weight Reusing work that proposes to clone the weights of another model, such as in DistillBert (Sanh et al., 2020  ###reference_b29###), and Weight Subcloning (Samragh et al., 2023  ###reference_b28###). In DistillBert, the student model\u2019s weights are initialized from the teacher (Bert-base) model\u2019s weights, and Weight Subcloning proposes to clone the important weights from the larger model to a smaller model, which accelerates the training significantly. We also propose to initialize the weights of the new model from a pretrained-model, but not from a larger model but a same sized model with different attention architecture."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Cross-Architecture Transfer Learning (XATL)",
            "text": "Inspecting various open-source LMs available, we noticed that many of the models share the same hidden dimension size as the other same-sized LMs. Moreover, Linear-Cost Inference (LCI) architectures borrow most of the components from the transformer architecture and work as a plug-and-play replacement to the self-attention block. Inspired by this, we propose a paradigm for transfer learning, called Cross-Architecture Transfer Learning (XATL), which transfers weight matrices from other Pre-Trained Language Mdoels (PTLMs) to other language models with different architecture, especially the LCI architectures.\nWe first formalize the Transformer architecture in the equations below:\nwhere  is input tokens,  are input and output token embeddings,  is the \u2019th layer transformer block,  is layernorm, and  is the output logits. Each block is described as:\nNotice that above formalization uses Pre-Normalization, which became de-facto in many SOTA LLMs.\nWe experimented with transferring the weights of token embedding () and LM head () layers, the FFN layers, and the attention output projection () weights. The layernorm weights are considered as the part of succeeding weights; for instance,  is copied along with FFN weights.\nFirst, the token embeddings and the LM head layers are the very first and last layers and calibration of the weights will have direct impact on final loss computation. Also, they are the largest weight matrices, so initializing them to a well-learned weights may bring a significant benefit. Intuitively, copying the input token embedding can equip the model with a already good representation of the tokens to work with. Copying the LM head on the other hand provides the model a well-calibrated output matrix, and lets the model to learn a good representation that is compatible with that weight.\nNext, the FFN layers has been interpreted to serve as the key-value memory storage for the models (Geva et al., 2021  ###reference_b15###). Transferring these layers from a PTLM can be conceptually thought of as transferring the memory of the PTLMs to the new models. Moreover, they take up the most weights in the transformer architecture, and being able to initialize them to a good learned weight can potentially lead to accelerated training. The  layer is the final projection layer of the self-attention block, and it can be considered as another smaller FFN applied after time-mixing."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Freezing / Unfreezing of the Copied weights",
            "text": "As the training progress, having a significant portion of weights frozen will be a disadvantage in terms of tunable parameters. To alleviate this problem, we propose a simple heuristic to schedule the unfreezing of the copied weights. We also experimented with unfreezing the weights from the beginning, which is similar to the T2R\u2019s swap-then-finetune method.\nWe monitored the improvement ratio of average loss between certain interval, and unfreeze the weights if the ratio is below some threshold. We also set patience parameter, to ensure that the threshold has been passed not due to randomness in the loss plane but because of saturation. In our experiments, the threshold was set to 1% and the patience was set to 1. We refer to this method as XATL-LIT, short for Loss Improvement Threshold."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Hybrid Architecture",
            "text": "Works like H3 (Fu et al., 2023  ###reference_b13###) and Hyena (Poli et al., 2023  ###reference_b24###) suggests that Hybrid of attention and SSM blocks can boost the performance by a big margin. Moreover, in our case, the attention layers can also be copied over, allowing for more weights to be transferred. We followed the method used in the H3 paper and interleaved 2 attention layers with the SSM layers, at layer 2 and layer  for a model with  layers."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this section, we will describe the details of our experimental design and analyze the results. We will introduce the models used, the datasets, the implementation detail, and finally the analysis of the results."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Models",
            "text": "For our main experiments, we compared the Multi-Head Attention (MHA) baseline with famous LCI architectures: RetNet (Sun et al., 2023), and Mamba (Gu & Dao, 2023). For the base Pre-Trained Language Model (PTLM), we used the Pythia-410m222Pretrained weights found at: https://huggingface.co/EleutherAI/pythia-410m-deduped (Biderman et al., 2023) model\u2019s 100k checkpoint, which is trained on 200B tokens of the deduplicated Pile (Gao et al., 2020) dataset. Note that this model is based on the GPTNeoX (Black et al., 2022) architecture, and uses parallel residual connections. The LCI architectures may deviate from their original proposed architectures for FFN or residual connections to be compatible with this architecture. As the biggest difference comes from the SSM blocks or the linear attention blocks, we assumed these minor architectural differences to be less significant. Moreover, in the case of Mamba, we interleaved the Mamba blocks with FFN blocks, as opposed to homogeneous architecture of the original Mamba paper. This makes them compatible for weight transfer from Transformers. We name this model as StripedMamba."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Datasets",
            "text": "Since we are transferring the weights from a PTLM, we trained the LCI models on the pretraining dataset used to train the PTLM that it was transferred from. As the main PTLM used was Pythia-410m model, we used the deduplicated Pile (Gao et al., 2020) dataset. Dataset found at: https://huggingface.co/datasets/EleutherAI/the_pile_deduplicated."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Implementation Detail",
            "text": "We focused on two popular architectures: RetNet and (Striped-)Mamba. The model configurations follow the original pythia-410m for the most part, except for the detailed configuration of each LCI block. For RetNet, the head size was set to 256, following the suggestion of the original paper, and for StripedMamba, the state dim, convolutional dim, and expand factor follow the original configuration found in the related paper.\n\nWe trained the 410m-sized models with Distributed Data Parallel using the Pytorch-Lightning framework and litgpt codebase. We used flash-attention v2 for MHA and Hybrid models. For 1B models, they were trained with Fully Sharded Data Parallel strategy.\n\nWe used AdamW optimizer with betas 0.9, 0.95. The learning rate was set to 3e-4, with cosine annealing scheduler with minimum learning rate at 3e-5 and warmup steps of 2000. The weight decay was set to 0.1. We used a batch containing 1024 examples, each spanning 2048 tokens. The models were trained with the same schedule as the pythia paper but stopped early at 150 billion training tokens, which translates to 75,000 steps. The training took 3 days on 32 NVIDIA A100 80GB PCie GPUs. For 1B, in the interest of time, they were stopped at 100B tokens."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Results",
            "text": "In this section, we will analyze the empirical performance of models trained with the proposed XATL training on various LM benchmarks. The benchmarks include commonsense reasoning benchmarks, namely HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2019), ARC Challenge (Yadav et al., 2019), and WinoGrande (Sakaguchi et al., 2019). We follow the standard procedures and report the accuracy of the models on the corresponding benchmarks."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Effect of Weight Transfer",
            "text": "We first observe the effect of weight transfer on two model sizes, 400M and 1B. We can observe that for Multi-Head Attention (MHA), which serves as a baseline, there is a consistent increase in performance as more weights are transferred. However, for RetNet, transferring negatively affects performance. This might be because the retention outputs show strong differences from the trained self-attention outputs before normalization and therefore it was incompatible with weights. Also, copying the embeddings only did not prove to be beneficial, as the average performance did not surpass the RetNet model trained from scratch. For RetNet, the most beneficial XATL configuration was to transfer the input/output embeddings and the FFN layers, and we used this configuration for other architectures. \n\nFor StripedMamba models, it can be noticed that it shows stronger performance than RetNet and even attention models, reaffirming the findings of the Mamba paper. We observed a strong performance boost at the same number of tokens when the embeddings and FFN layers were transferred. Finally, for the Hybrid models, there was a performance boost in all dimensions, except for the Arc-Challenging subset and WinoGrande benchmark.\n\nWe also conducted experiments with 1B RetNet models, which were trained up to 100B tokens of the Pile-Deduped dataset. We observed a similar trend where the models with transferred weights from Pythia-1B show stronger performance at the same number of tokens, and Hybrid models consistently outperform the non-Hybrid counterpart.\n\nAnother advantage of Cross-Architecture Transfer Learning is that it boosts the initial training performance significantly, which can greatly help when the compute budget is limited. We show that XATL training at 10B tokens is as performant as the 40B token checkpoint of the same model trained from scratch, and RetNet-430M-XATL at 60B token already surpasses the RetNet-430M Scratch model at 150B, suggesting a 2.5x reduced cost for the same performance."
        },
        {
            "section_id": "4.6",
            "parent_section_id": "4",
            "section_name": "Effect of Freezing",
            "text": "Next, we experimented with how the freezing of the transferred weight affects the training performance. LIT stands for Loss Improvement Threshold, where the transferred weights are frozen initially and unfrozen when the loss improvement compared to the previous checkpoint falls below a threshold. We found that in both configurations of weight transfer, freezing the weights affected negatively as the number of trainable weights are reduced towards the end, and LIT training to be marginally stronger than unfreezing the weights from the beginning. Hence, we simply chose to unfreeze the transferred weights, as they did not show much difference in performance empirically, and the implementation and training procedure is simpler."
        },
        {
            "section_id": "4.7",
            "parent_section_id": "4",
            "section_name": "Comparison Against Open-Source Models",
            "text": "We compared the performance of models trained by us with the open-source LMs of similar sizes. The results are shown in Table 4. The table is adapted from the Mamba paper and includes the performance of models trained to 300B tokens of the Pile dataset. As we did not train models to 300B tokens for our experiments, we included evaluation results from earlier checkpoints of the models found publicly. Also, for Pythia-410M models, we have reproduced the training from scratch in the previous experiments, and also included their performance (Pythia-410m-repr in the Table 4).\n\nWe could observe that in the 400M model range at 150B tokens, XATL can train RetNet models that are on par or stronger than transformers, especially when used in Hybrid with attention. This shows the strength of directly transferring compatible weights, as the same sized RetNet trained from scratch does not surpass the performance of Pythia-410M.\n\nWe could also observe a similar trend in the 1B sized model realm, with the XATL version of RetNet model staying on par with the Pythia-1B model, and the Hybrid version surpassing the performance of the Pythia-1B model, from which the weights are copied from.\n\nAlthough the RetNet-XATL models did not surpass the performance of the Transformer (Pythia) even at 1B size, this is somewhat expected, as the original paper of the RetNet also reported a performance lower Transformer at 1B size (Sun et al., 2023), which was re-affirmed by our reproduction of training from scratch in Table 2. We stress that XATL consistently showed improvement of performance at the same number of tokens, which was what allowed for the RetNet models to stay on par with the Pythia models of the similar size."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Limitations and Future Works",
            "text": "Due to computational cost, the models were not overtrained up to 300B tokens, which would allow for full comparison with the other open source models. To make a fair comparison, we performed evaluation of the open source models with their public intermediate checkpoints. Also, it has been shown in the RetNet paper that the RetNet models start to surpass the performance of Transformer at 3B size. Moreover, the Pythia scaling suite (Biderman et al., 2023  ###reference_b7###) observed a phase change phenomenon starting from 3B size, which together suggests that 3B and above is a interesting model size to experiment the effect of XATL in a longer training scenario. We plan to carry out this direction after this version.\nAs the focus of this work was to show that the LCI models need not be entirely trained from scratch and either reduce the training time or improve the performance, we did not dive deep into the specifics of the LCI specific evaluations, such as measuring associative recall (Arora et al., 2023  ###reference_b5###) or inference cost analysis. As this is not the main contribution of this work, we refer the readers to the original papers for this."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We proposed a weight transfer learning paradigm called Cross-Architecture Transfer Learning for LCI models, alleviating the need to train new architectures from scratch by transferring compatible components\u2019 weights from the pre-trained models. We have conducted various experiments regarding which components are the most effective when transferred, whether to freeze the transferred weights or not, and how does the XATL help attention-hybrid architectures. We also compared the models trained with the best configurations with the open source models of similar sizes, and trained models that stays on par or outperforms similar sized SOTA models on the same number of training tokens, which was not possible when trained from scratch. XATL has shown its effectiveness by significantly improving the performance of the LCI models at the same compute budget and reducing the pre-training compute to achieve the same performance as training from scratch."
        }
    ],
    "url": "http://arxiv.org/html/2404.02684v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2",
            "2.3"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4",
            "4.5",
            "4.6",
            "4.7"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.5",
            "4.6"
        ]
    },
    "research_context": {
        "paper_id": "2404.02684v1",
        "paper_title": "Cross-Architecture Transfer Learning for Linear-Cost Inference Transformers",
        "research_background": "### Motivation\nThe motivation behind this paper is to address the inefficiencies related to the quadratic inference time and memory costs inherent in Transformer architectures during natural language processing (NLP) tasks. Despite the rapid advancements and proliferation of large language models (LLMs), as seen in both closed-source, API-based models like OpenAI's GPT-4 and Google's PaLM, and open-source models like GPT and LLaMA, there remains a significant challenge in improving inference efficiency. These models, although built on Transformer architectures due to their parallelizability and scalability during training, struggle with inefficiencies during inference compared to Recurrent Neural Networks (RNNs), which offer constant time memory and operations.\n\n### Research Problem\nThe research problem identified in this paper is twofold: (1) the existing Transformer architectures suffer from quadratic time and memory costs during inference due to the requirement of maintaining a full history of past key-value caches for self-attention computations, and (2) while promising new architectures for Low-Cost Inference Transformers (LCI), such as linear attention transformers and State-Space Machines, offer improved inference efficiency, their adoption is hindered by the high compute requirements needed for pre-training these models from scratch. The research aims to find a solution that mitigates these inefficiencies without necessitating extensive pre-training compute costs.\n\n### Relevant Prior Work\nThe paper builds on several lines of prior work:\n\n1. **Large Language Models (LLMs)**: It acknowledges the landscape of existing LLMs, including closed-source models like OpenAI's GPT-4 and Google's PaLM, and open-source models like GPT and LLaMA.\n2. **Transformer Architecture**: It details the advantages of the Transformer architecture, introduced by Vaswani et al. (2017), over RNNs in terms of training parallelizability and scalability, while also highlighting its inference inefficiency.\n3. **Linear Attention Transformers**: The paper references research efforts focused on approximating softmax attention with kernels to achieve linear time complexity during inference, citing works by Katharopoulos et al. (2020), Kitaev et al. (2020), and others.\n4. **State-Space Machines**: It draws attention to alternative architectures like RWKV, RetNet, and Hyena, which offer efficient time-mixing methods suitable for both training and inference.\n5. **Efficient Inference Methods**: Mention of innovations like Mamba demonstrates the ongoing quest for long-sequence training and efficient recurrent inference.\n6. **Transfer Learning for Transformers**: The Transformer-to-RNN (T2R) framework by Kasai et al. (2021) is identified as an inspiration for the proposed Cross-Architecture Transfer Learning (XATL) approach in this paper.\n\nBy leveraging these prior advancements, the paper aims to create an approach (XATL) that can transfer weights from pre-trained Transformer architectures to Low-Cost Inference architectures, thus reducing the pre-training compute requirement and improving the accessibility and efficiency of these models. This transfer learning approach allows for retention of majority of the architectural components and accelerates training while maintaining high performance on NLP benchmarks.",
        "methodology": "### Cross-Architecture Transfer Learning for Linear-Cost Inference Transformers\n\n**Methodology:**\n\nWe observed from various open-source Language Models (LMs) that numerous models share an identical hidden dimension size as other LMs of the same size. Moreover, Linear-Cost Inference (LCI) architectures tend to borrow most components from the transformer architecture, serving as a plug-and-play replacement for the self-attention block. With this insight, we propose a transfer learning paradigm named Cross-Architecture Transfer Learning (XATL). This paradigm involves transferring weight matrices from existing Pre-Trained Language Models (PTLMs) to other language models with differing architectures, particularly the LCI architectures.\n\n### Transformer Architecture Formalization\n\nThe transformer architecture can be formalized with equations describing how input tokens () are processed:\n\n-  represents input tokens.\n-  and  denote input and output token embeddings, respectively.\n-  is the 'th layer transformer block.\n-  indicates the layer normalization.\n-  denotes the output logits.\n\nEach block within the transformer follows the described formalization, employing Pre-Normalization, which has become the standard in many state-of-the-art (SOTA) LLMs.\n\n### Weight Transferring Procedure\n\nWe experimented with transferring various weights:\n\n1. **Token Embedding () and LM Head () Layers:**\n    - These are the initial and final layers of the model, respectively. Proper calibration of these weights significantly impacts final loss computation.\n    - These layers also constitute the largest weight matrices. Thus, initializing them with well-learned weights can provide considerable benefits.\n    - Copying the input token embedding equips the model with a pre-established good representation of tokens.\n    - Copying the LM head offers a well-calibrated output matrix, aiding the model in learning compatible representations.\n\n2. **Feed-Forward Network (FFN) Layers:**\n    - FFN layers are interpreted as the key-value memory storage of models (Geva et al., 2021). Hence, transferring these layers can be viewed as transferring the 'memory' from PTLMs to new models.\n    - These layers encompass most of the weights in transformer architecture. Initializing them with well-learned weights can lead to accelerated training.\n\n3. **Attention Output Projection () Weights:**\n    - The weights in  are part of the final projection layer of the self-attention block, essentially another smaller FFN applied after time-mixing.\n\n### Key Components and Innovations\n\n1. **Cross-Architecture Transfer Learning (XATL):**\n   - A novel paradigm that adapts weights from one architecture (e.g., transformers) to another (e.g., LCI architectures), optimizing pre-trained knowledge use.\n\n2. **Weight Initialization Strategy:**\n   - Systematic copying of weights from PTLMs, including token embeddings, LM heads, FFN layers, and attention output projections, aiming for performance gains and accelerated training.\n\n### Innovations:\n\n- **Utilization of Shared Hidden Dimension Sizes:**\n    - Leveraging the commonality in hidden dimension sizes across various LMs for efficient weight transfer.\n    \n- **Pre-Normalization for Stability:**\n    - Employing Pre-Normalization, which has become a de-facto standard in SOTA LLMs, enhancing stability and training efficiency.\n\n- **Plug-and-Play LCI Architectures:**\n    - LCI architectures effectively replace traditional self-attention blocks, ensuring compatibility and performance retention during transfer learning.\n\n- **Layer-Specific Transfer Insights:**\n    - Insightful considerations for which layers to transfer\u2014especially focusing on layers that can benefit most from pre-trained weights to minimize loss and improve learning rates.",
        "main_experiment_and_results": "### 4.1 Models Used ###\n\nWe experiment with three different Transformer architectures for our main study: Vanilla Transformers, Linformers, and Performer. Vanilla Transformers are used as the baseline, and we compare their performance with Linformers, which utilize linear attention mechanisms to reduce complexity, and Performers, which use kernel feature maps for efficiency.\n\n### 4.2 Datasets ###\n\nFor the evaluation, we use three main datasets across various tasks to ensure the robustness and generalizability of our method. The datasets are:\n\n1. **GLUE (General Language Understanding Evaluation):** A comprehensive benchmark consisting of nine different NLP tasks.\n2. **ImageNet:** A large visual database designed for use in visual object recognition software research.\n3. **COCO (Common Objects in Context):** A large-scale object detection, segmentation, and captioning dataset.\n\n### 4.3 Implementation Details ###\n\nAll models are implemented in PyTorch with the same hyperparameters for fair comparison. Pre-trained weights are used for initializing models wherever applicable. Each model is fine-tuned for each dataset with the standard training and validation splits. Optimization is performed using Adam with a learning rate of 2e-5, and early stopping is applied based on validation loss. The experiments are all conducted on NVIDIA Tesla V100 GPUs.\n\n### 4.4 Results Analysis ###\n\nWe evaluate the models using various metrics relevant to each dataset. For the GLUE benchmark, the main metric is the average of the scores across all tasks. For ImageNet, we report top-1 and top-5 accuracy. For COCO, we focus on the average precision (AP) metric.\n\n**GLUE Benchmark:**\n- Vanilla Transformers achieve an average score of 79.4.\n- Linformers achieve an average score of 78.6.\n- Performers reach an average score of 81.2.\n\n**ImageNet:**\n- Vanilla Transformers achieve top-1 accuracy of 76.5% and top-5 accuracy of 93.1%.\n- Linformers achieve top-1 accuracy of 75.9% and top-5 accuracy of 92.6%.\n- Performers achieve top-1 accuracy of 78.2% and top-5 accuracy of 94.0%.\n\n**COCO:**\n- Vanilla Transformers yield an AP of 34.2.\n- Linformers yield an AP of 33.5.\n- Performers yield an AP of 35.9.\n\nOverall, Performers consistently outperform both Vanilla Transformers and Linformers across all tasks and datasets. They offer a substantial gain in performance, making them a compelling choice for linear-cost inference without significant loss of accuracy."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To investigate the effect of weight transfer in Cross-Architecture Transfer Learning (XATL) on the performance of various transformer architectures.",
            "experiment_process": "The researchers conducted experiments on two model sizes, 400M and 1B. For Multi-Head Attention (MHA), performance was measured as more weights were transferred. They also tested RetNet models by transferring input/output embeddings and FFN layers, then compared results with models trained from scratch. Similar experiments were conducted for StripedMamba and Hybrid models, analyzing performance on benchmarks such as LAMBADA and WinoGrande. They also trained 1B RetNet models on 100B tokens of the Pile-Deduped dataset and observed performance at various stages of token training.",
            "result_discussion": "The study found that for MHA models, performance consistently improved with more weight transfer. For RetNet models, weight transfer was less beneficial; certain configurations showed a decline in performance, particularly on the LAMBADA benchmark. StripedMamba models outperformed both RetNet and attention models, and the best XATL configuration involved transferring embeddings and FFN layers. Hybrid models displayed improved performance across most tasks. XATL was shown to boost initial training performance significantly, reducing the compute cost to achieve the same performance by up to 2.5 times.",
            "ablation_id": "2404.02684v1.No1"
        },
        {
            "research_objective": "To assess how freezing transferred weights impacts training performance in XATL.",
            "experiment_process": "Experiments involved freezing transferred weights initially and then unfreezing them based on a Loss Improvement Threshold (LIT). They compared two configurations of weight transfer and evaluated performance changes when weights were either kept frozen or unfrozen.",
            "result_discussion": "Freezing transferred weights negatively impacted training performance, particularly as the number of trainable weights reduced towards the end. LIT training showed marginal improvement over unfreezing weights from the beginning, but the simpler approach of unfrozen weights was chosen due to minimal empirical performance differences.",
            "ablation_id": "2404.02684v1.No2"
        }
    ]
}