{
    "title": "Switchable Decision: Dynamic Neural Generation Networks",
    "abstract": "Auto-regressive generation models achieve competitive performance across many different NLP tasks such as summarization, question answering, and classifications. However, they are also known for being slow in inference, which makes them challenging to deploy in real-time applications. We propose a switchable decision to accelerate inference by dynamically assigning computation resources for each data instance. Automatically making decisions on where to skip and how to balance quality and computation cost with constrained optimization, our dynamic neural generation networks enforce the efficient inference path and determine the optimized trade-off. Experiments across question answering, summarization, and classification benchmarks show that our method benefits from less computation cost during inference while keeping the same accuracy. Extensive experiments and ablation studies demonstrate that our method can be general, effective, and beneficial for many NLP tasks.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large-scale pre-trained language models such as BART (Lewis et al., 2019) have demonstrated a significant performance gain to the natural language processing (NLP) community but generally come with the cost of a heavy computational burden. Besides pre-training and fine-tuning, inference of such a large model also comes with a heavy computational cost. On IoT (Internet of things) devices and real-world applications, lower computation cost tolerance and restricted computation resource during inference impede these models from deployment.\n\nRecent efforts of efficient inference mainly focus on pruning or compressing the model parameters, e.g., pruning unimportant parts of the neural model weights (Han et al., 2015b; Fan et al., 2019; Gordon et al., 2020), quantizing the number of bits needed (Lin et al., 2016; Shen et al., 2020), distilling from large teacher models to small student models (Hinton et al., 2015; Jiao et al., 2019). These methods produce only one small model with a predetermined target size. Another direction is to switch the model parameters for different data instances, e.g., the mixture of experts (Shazeer et al., 2017), and switch transformer (Fedus et al., 2021). Early exiting, which adaptively produces a series of small models for different data instances, is one of the most common practices. Most previous work makes exit decisions based on either the confidence of output probability distributions or a trained agent.\n\nIn this work, we propose a carefully designed candidate space for encoder-decoder auto-regressive models and enhance the optimization strategies when training the agent. In this spirit, we explore the problem of dynamically allocating computation across a generation model. In particular, we consider a standard encoder-decoder transformer auto-regressive generation model. It comprises a stacked structure with multiple layers, each having a multi-head attention layer followed by a feed-forward network (FFN) layer (Zhang et al., 2021b, a; Dai et al., 2022; Tanwisuth et al., 2023). To this end, we introduce a dynamic neural network for the auto-regressive generation models, which includes the attention, feed-forward, and input sequence as the candidate space for switchable decisions. Our method generates an input-dependent inference strategy for each data.\n\nFor each input sequence, the reinforcement learning agent outputs all the decisions for skipping or keeping each candidate. With the first-layer hidden representations as the input, the policy network is trained to maximize a reward that incentives the use of as few blocks or tokens as possible while preserving the prediction accuracy. We propose learning optimal switchable strategies that simultaneously preserve prediction accuracy and minimal computation usage based on input-specific decisions. The constrained optimization is utilized as a more principled approach for trading off these two targets (quality vs. efficiency). We target keeping the predicted quality while achieving better efficiency as far as possible. A gradient-based constrained optimization algorithm is implemented under our framework. We run extensive experiments across summarization, e.g., XSum (Narayan et al., 2018) and CNN/DM (Hermann et al., 2015), and question answering, e.g., SQuAD 1.1 (Rajpurkar et al., 2016) and SQuAD 2.0 (Rajpurkar et al., 2018), tasks.\n\n\u2776 Our method not only shows comparable performance across different tasks and datasets but also accelerates model inference by up to 40% with negligible model quality degradation. \u2777 Furthermore, we provide extensive ablation studies on different design choices for the proposed method, including the encoder-only or decoder-only switchable schemes. \u2778 Our analysis shows the switchable decision contributes to efficiency improvement and accuracy consistency, helping the generation model to choose the inference path and candidates dynamically. \u2779 To the best of our knowledge, we present the first switchable decision in the language generation model setting by dynamically making inference decisions in summarization, question answering, and classification.\n\nOur contributions are summarized as follows: Present a dynamic network for switchable decisions embracing attention, feed-forward, and input sequence as skipping candidates. Propose an efficient and effective way to train the skipping strategies, which can optimize the trade-off between computation and quality. Verify the effectiveness and general applicability of the proposed method in various NLP tasks, summarization, question answering, and classification benchmarks, and provide a rich analysis of our method with various design choices."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work and Background",
            "text": "For model compression, pruning removes unimportant parts of the neural network (Han et al., 2015a; Fan et al., 2019; Gordon et al., 2020), quantization targets the number of bits needed to operate a neural network (Shen et al., 2020), and distillation transfers knowledge from large teacher models to small student models (Chen et al., 2017; Jiao et al., 2019). Efficient network architectures such as MobileBERT (Sun et al., 2020) and ALBERT (Lan et al., 2020) have also been explored for lightweight neural network architectures. Compared to these previous approaches, we focus on dynamic networks with the switchable candidate design to best reduce total computation without degrading prediction accuracy. Dynamic networks enable adaptive computation for various input instances that have been conducted for natural language tasks. Text skimming (Campos et al., 2017; Hansen et al., 2019) learns to skip state updates and shortens the effective size of the computational graph. Dynamic jumping (Yu et al., 2018; Fu & Ma, 2018) strategically skips some tokens without reading them, and directly jumps to an arbitrary location. Early exiting for pretrained models has been explored by previous literature. RTJ (Schwartz et al., 2020), DeeBERT (Xin et al., 2020), and FastBERT (Liu et al., 2020a) make early exiting decisions based on confidence (or its variants) of the predicted probability distribution and are therefore limited to classification tasks. PABEE (Zhou et al., 2020) and BERxiT (Xin et al., 2021) propose patience-based early exiting by exploiting the layer information. Runtime Neural Pruning (Lin et al., 2017), SkipNet (Wang et al., 2018b), and BlockDrop (Wu et al., 2018) use reinforcement learning (RL) to decide whether to execute a network module. Inspired by them, we incorporate lightweight reinforcement learning to make input-dependent decisions and build a diversified switchable candidate space. With the constrained optimization approach, our method saves computational costs without loss of accuracy."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Method",
            "text": "Our switchable decision (Figure 1  ###reference_###) network focuses on speeding up the inference time for an autoregressive language generation model. Specifically, we suggest a general recipe for the switchable decision: 1) construct the versatile decision space, 2) utilize the input-dependent reinforcement learning agent, and 3) propose the lexicographic (lexico) optimization strategy.\nDenote input  = .\nWith a series of  tokens, a transformer-based language generation model,  with  layers, first embeds the tokens to form a matrix , where  is the dimension of the embedding space.\nThese token representations then go through the encoders and decoders of the language model. To speed up the inference time while maintaining similar high quality, we decide whether each input data should skip one layer. This decision problem grows exponentially as we increase the number of layers. Moreover, because of the discrete nature of the decision space, optimization becomes challenging. In this section, we outline our unique design choices to accomplish our goal and overcome optimization challenges.\nA key component of a transformer-based language model is the attention layer. Zhang et al. (2019  ###reference_b63###) discover that some layers are redundant. To decide whether to skip a certain layer, we model these decisions as a sequence of i.i.d. Bernoulli random variables parameterized by a policy network . Let  denote the switchable decision of the  layer, defined as\nwhere  denotes the input of the decision unit,\nand we apply the first encoder layer output as .\nThe policy network, , learns instance-specific probabilities of keeping the hidden representations of each layer. To perform skipping, we sample from this distribution and broadcast the indicators, , to the input representations of attention layers.\nIn the same spirit, the feed-forward layers may contain redundant information. Thus, we consider skipping these layers using the same approach as that done in the attention. We decide whether to skip or not based on the indicator .\nThe design of the policy network is the same as that of the attention layer.\nIn addition to skipping the layers,\nskipping the tokens can also be an alternative way to save computation costs. We create two token skipping strategies: \u2780 skipping the last  tokens and \u2781 uniformly skipping  tokens. For the former, we set  to , , and . For the latter,  is equal to , , and . To decide which strategy to use, we optimize a categorical random variable parameterized by a function .\nThe input of  is the same as , and the output of  is a distribution over all six candidate decisions.\nOur interested architecture contains encoders and decoders.\nFor the encoders, we apply attention skipping and feed-forward skipping together with token skipping.\nFor the decoders, since every token is meaningful for the final outputs, we only apply attention skipping and feed-forward skipping.\nWhen making decisions, we sample from the\noutputs of our policy network, and broadcast the decisions to the hidden representations of each layer.\nSince we aim to speed up the inference process, a simple design for the policy network is adopted. We utilize a one-layer MLP with layer normalization and ReLU activation function. To output a Binomial\ndistribution over decisions, we apply the sigmoid activation to the outputs of the network for attention and feed-forward candidates. We use the softmax function to output the distribution over the choices for token candidates.\nDuring the training process, we sample from the decision distributions, which are parameterized by the policy network. The distribution of the switchable decisions for the layers can be represented as a -dimensional Bernoulli distribution, which can be written as:\nwhere . Similarly, the distribution of the token skipping decisions can be represented as a categorical distribution, which can be formalized as:\nwhere  denotes the choice of the skipping strategy, and  indicates the total number of strategies. We apply seven candidates in practice.\nWe define the reward function (Yang et al., 2022b  ###reference_b60###, a  ###reference_b59###; Feng et al., 2023  ###reference_b12###) as a trade-off between quality and computational cost. Given an inference path and a data instance, the reward can be computed from the computation (estimated FLOPs). Intuitively skipping layers will have high reward. We further refer quality as accuracy and loss in the following way:\nwhere  is ,  is the estimated FLOPs (floating point operations), and  is a coefficient.\nThe overall loss function is defined as the expected value of the reward:\nwhere  and  are defined in (2  ###reference_###) and (3  ###reference_###), respectively.\nTo optimize our policy network, we apply policy gradient to compute the gradient of , and update the parameters of the policy network. We\nuse a self-critical baseline to reduce the variance of the gradients.\nThe constraint-optimization strategy is further applied on the quality and computation. Details are in the next section.\nUnlike the training process, we do not sample the skipping decisions during inference. Instead, we choose the decisions which maximize the likelihood function.\nIn the joint training of the main network and the policy network, a trade-off between\nquality and computation is important. The linear combination of multiple objectives is the most widely used approach. However, the coefficient of the combination requires manual tuning, and it is theoretically unsuitable for non-convex functions. In this work, we consider constrained optimization on trading off two objectives, with a special emphasis on lexicographic (lexico) optimization.\nTo optimize the trade-off between quality and computation in Eqn (4  ###reference_###), we propose to use lexicographic optimization, in which the parameters are iteratively updated as\nwhere  is an adaptive step size and  is an update direction to be chosen to balance the minimization of  and constraint satisfaction on .\nOne of the objectives (say  which is  in our case) is of secondary\nimportance w.r.t. the other one (say  which is ).\nThe design criterion for the constrained optimization is when the constraint is not satisfied (i.e., ), the focus becomes decreasing  to satisfy the constraint as soon as possible; in the meantime,  performs as a secondary objective indicating that  should be minimized to the degree that it does not hurt the descent of .\nTherefore, we apply the following update rule to obtain such a goal:\nwhere  and  are estimated by score function, and the  can be computed as\n where  equals to  and the  represents the minimal loss.\nOur switchable decision (SD) with efficient candidate space and constrained optimization is\nshown in Algorithm 1  ###reference_###.\nWe iteratively update the auto-regressive model and the policy network in a single-loop manner.\nThe policy network parameter  is updated by Eqn (6  ###reference_###) in a direction to balance the optimization of quality and constraint satisfaction on computation."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Construct Discrete Decision Space",
            "text": "We propose learning the best configurations of (input, inference paths) pair for each example using a switchable decision network to speed up inference time. We consider three search space candidates, namely, the attention layer, the feed-forward layer, and query inputs after the first layer. We now explain the details of each search space below.\nA key component of a transformer-based language model is the attention layer. Zhang et al. (2019  ###reference_b63###  ###reference_b63###) discover that some layers are redundant. To decide whether to skip a certain layer, we model these decisions as a sequence of i.i.d. Bernoulli random variables parameterized by a policy network . Let  denote the switchable decision of the  layer, defined as\nwhere  denotes the input of the decision unit,\nand we apply the first encoder layer output as .\nThe policy network, , learns instance-specific probabilities of keeping the hidden representations of each layer. To perform skipping, we sample from this distribution and broadcast the indicators, , to the input representations of attention layers.\nIn the same spirit, the feed-forward layers may contain redundant information. Thus, we consider skipping these layers using the same approach as that done in the attention. We decide whether to skip or not based on the indicator .\nThe design of the policy network is the same as that of the attention layer.\nIn addition to skipping the layers,\nskipping the tokens can also be an alternative way to save computation costs. We create two token skipping strategies: \u2780 skipping the last  tokens and \u2781 uniformly skipping  tokens. For the former, we set  to , , and . For the latter,  is equal to , , and . To decide which strategy to use, we optimize a categorical random variable parameterized by a function .\nThe input of  is the same as , and the output of  is a distribution over all six candidate decisions.\nOur interested architecture contains encoders and decoders.\nFor the encoders, we apply attention skipping and feed-forward skipping together with token skipping.\nFor the decoders, since every token is meaningful for the final outputs, we only apply attention skipping and feed-forward skipping.\nWhen making decisions, we sample from the\noutputs of our policy network, and broadcast the decisions to the hidden representations of each layer."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Reinforcement Learning Agent",
            "text": "Since we aim to speed up the inference process, a simple design for the policy network is adopted. We utilize a one-layer MLP with layer normalization and ReLU activation function. To output a Binomial\ndistribution over decisions, we apply the sigmoid activation to the outputs of the network for attention and feed-forward candidates. We use the softmax function to output the distribution over the choices for token candidates.\nDuring the training process, we sample from the decision distributions, which are parameterized by the policy network. The distribution of the switchable decisions for the layers can be represented as a -dimensional Bernoulli distribution, which can be written as:\nwhere . Similarly, the distribution of the token skipping decisions can be represented as a categorical distribution, which can be formalized as:\nwhere  denotes the choice of the skipping strategy, and  indicates the total number of strategies. We apply seven candidates in practice.\nWe define the reward function (Yang et al., 2022b  ###reference_b60###  ###reference_b60###, a  ###reference_b59###  ###reference_b59###; Feng et al., 2023  ###reference_b12###  ###reference_b12###) as a trade-off between quality and computational cost. Given an inference path and a data instance, the reward can be computed from the computation (estimated FLOPs). Intuitively skipping layers will have high reward. We further refer quality as accuracy and loss in the following way:\nwhere  is ,  is the estimated FLOPs (floating point operations), and  is a coefficient.\nThe overall loss function is defined as the expected value of the reward:\nwhere  and  are defined in (2  ###reference_###  ###reference_###) and (3  ###reference_###  ###reference_###), respectively.\nTo optimize our policy network, we apply policy gradient to compute the gradient of , and update the parameters of the policy network. We\nuse a self-critical baseline to reduce the variance of the gradients.\nThe constraint-optimization strategy is further applied on the quality and computation. Details are in the next section.\nUnlike the training process, we do not sample the skipping decisions during inference. Instead, we choose the decisions which maximize the likelihood function."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Constrained Optimization",
            "text": "In the joint training of the main network and the policy network, a trade-off between\nquality and computation is important. The linear combination of multiple objectives is the most widely used approach. However, the coefficient of the combination requires manual tuning, and it is theoretically unsuitable for non-convex functions. In this work, we consider constrained optimization on trading off two objectives, with a special emphasis on lexicographic (lexico) optimization.\nTo optimize the trade-off between quality and computation in Eqn (4  ###reference_###  ###reference_###), we propose to use lexicographic optimization, in which the parameters are iteratively updated as\nwhere  is an adaptive step size and  is an update direction to be chosen to balance the minimization of  and constraint satisfaction on .\nOne of the objectives (say  which is  in our case) is of secondary\nimportance w.r.t. the other one (say  which is ).\nThe design criterion for the constrained optimization is when the constraint is not satisfied (i.e., ), the focus becomes decreasing  to satisfy the constraint as soon as possible; in the meantime,  performs as a secondary objective indicating that  should be minimized to the degree that it does not hurt the descent of .\nTherefore, we apply the following update rule to obtain such a goal:\nwhere  and  are estimated by score function, and the  can be computed as\n where  equals to  and the  represents the minimal loss.\nOur switchable decision (SD) with efficient candidate space and constrained optimization is\nshown in Algorithm 1  ###reference_###  ###reference_###.\nWe iteratively update the auto-regressive model and the policy network in a single-loop manner.\nThe policy network parameter  is updated by Eqn (6  ###reference_###  ###reference_###) in a direction to balance the optimization of quality and constraint satisfaction on computation."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experimental Settings",
            "text": "Table 1 shows the experimental data configuration. We use CNN/DailyMail (Hermann et al., 2015) and XSum (Narayan et al., 2018) to evaluate our method. CNN/DailyMail consists of 287,226 documents for training, 13,368 documents for validation, and 11,490 documents for testing. XSum has 226,711 news articles accompanied by a one-sentence summary, answering the question \u201cWhat is this article about?\u201d. Following the splits of Narayan et al. (2018), it contains 204,045 train, 11,332 dev, and 11,334 test. Following prior work (Lewis et al., 2019), we use ROUGE (Lin & Hovy, 2003) as our primary metric. We report the unigram ROUGE-1 (R-1) and bigram ROUGE-2 (R-2) overlap to assess informativeness, and the longest common subsequence ROUGE-L (R-L) score to assess fluency. \n\nThe Stanford Question Answering Datasets (SQuAD) v1.1 and v2.0 (Rajpurkar et al., 2016, 2018; Fan et al., 2020) are popular machine reading comprehension benchmarks. For the SQuAD v2.0 dataset, it contains examples where the answer to the question cannot be derived from the provided context. Similar to previous settings (Devlin et al., 2018; Lewis et al., 2019), we use concatenated question and context as input to the encoder of BART, and additionally pass them to the decoder. We report Exact Match (EM) and F1 score for evaluation (Lewis et al., 2019)."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Task and Evaluation Metrics",
            "text": "We use CNN/DailyMail (Hermann et al., 2015) and XSum (Narayan et al., 2018) to evaluate our method. CNN/DailyMail consists of 287,226 documents for training, 13,368 documents for validation, and 11,490 documents for testing. XSum has 226,711 news articles accompanied with a one-sentence summary, answering the question \u201cWhat is this article about?\u201d. Following the splits of Narayan et al. (2018), it contains 204,045 train, 11,332 dev, and 11,334 test. Following prior work (Lewis et al., 2019), we use ROUGE (Lin & Hovy, 2003) as our primary metric. We report the unigram ROUGE1 (R-1) and bigram ROUGE-2 (R-2) overlap to assess the informativeness, and the longest common subsequence ROUGE-L (R-L) score to assess the fluency. The Stanford Question Answering Datasets (SQuAD) v1.1 and v2.0 (Rajpurkar et al., 2016, 2018; Fan et al., 2020) are popular machine reading comprehension benchmarks. For the SQuAD v2.0 dataset, it contains examples where the answer to the question cannot be derived from the provided context. Similar to previous settings (Devlin et al., 2018; Lewis et al., 2019), we use concatenated question and context as input to the encoder of BART, and additionally pass them to the decoder. We report Exact Match (EM) and F1 score for evaluation (Lewis et al., 2019)."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Implementation Details",
            "text": "Following Lewis et al. (2019), we take the pre-trained BART model as the backbone and utilize the provided checkpoint for finetuning on the downstream datasets. BART is a pre-trained sequence-to-sequence model based on the masked source input and auto-regressive target output, which contains 12 layers of transformer encoder and 12 layers of transformer decoder. Its embedding size is 1,024 and feed-forward size is 4,096. We follow the hyper-parameters used in Lewis et al. (2019).\n\nSpecifically, in summarization, we set the training steps as 50k and the number of warm-up steps as 500. The max number of tokens and the update frequency are set to be 2,048 and 4, respectively. The learning rate is set to . \n\nFor the question answering (SQuAD 1.1/2.0), we set the total number of updates and warm-up updates as 5,430 and 326, respectively. The max number of sentences is 3 per device with an update frequency of 2. The learning rate is .\n\nWe refer the readers to Appendix A for classification hyper-parameter configurations, and more details about the settings."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We evaluate the performance of our switchable dynamic network. In each table, we bold the best result within each column block, and the results of our method are obtained with three trials to determine the variance. See Appendix A for full results with error bars.\n\nWe adopt several methods from the conventional early-exiting method (CALM; Schuster et al., 2022), Fast and Robust EarlyExiting (FREE) (Bae et al., 2023), Pegasus (Shleifer & Rush, 2020) (pruning and distillation), and DQ-Bart (Li et al., 2022) (quantization and distillation) and compare them with Ours (SD). \n\nIn Shleifer & Rush (2020), it utilizes the shrink and finetune methods: BART-student, Pegasus, and BART on CNN/DailyMail. In Li et al. (2022), it uses quantization and distillation. It reports the BART (8-8-8 6-1). The numbers here represent the number of bits for weights, word embedding, activations, the number of encoder layers, and the number of decoder layers. \n\nThe results shown in Table 3 demonstrate our switchable decision achieves a good trade-off between quality and computation. These results verify that our method contributes to efficiency and accuracy, helping the generation model to choose the inference path and candidates dynamically.\n\nFurther, combining quantization or distillation method, our effective method of improving the language generation model can also be seen as a complementary and plug-in module. We leave this as future work."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Summarization",
            "text": "Table 2 reports our results on two summarization datasets. The top block displays the performance of baselines on CNN/DailyMail and XSum datasets, and the bottom block shows the results of incorporating the switchable dynamic networks. We report the results upon the BART large setting in Lewis et al. (2019 ###reference_b26###).\n\nSummaries in the CNN/DailyMail tend to resemble source sentences and summaries in XSUM are highly abstractive. Baseline models such as BART (Lewis et al., 2019 ###reference_b26###), UniLM (Dong et al., 2019 ###reference_b8###), and BERTSUM (Liu & Lapata, 2019 ###reference_b34###) do well enough, and even the baseline of the first-three source sentences is highly competitive for CNN/DailyMail. Our method can reduce the computation cost while having little or no drop on ROUGE. For example, we even have a 0.2 increase on R1 for CNN/DailyMail and a 0.1 increase on R1 for XSum, while reducing 39% and 18% computation costs, respectively. For the quality of the sentence generations, our method has almost outperformed all the baselines. Especially, for the CNN/DailyMail, we achieve better ROUGE with less than two-thirds FLOPs cost, compared to the original BART-large model (e.g., R1: , RL: on CNN/DailyMail).\n\nThese results further confirm that SD can work as an effective module to be incorporated into the auto-regressive generation models. SD on improving the inference can also be seen as a complementary module to works focusing on improving pre-training components (Hou et al., 2022 ###reference_b21###; Ge et al., 2022 ###reference_b14###). We adopt several methods from the conventional early-exiting method (CALM; (Schuster et al., 2022 ###reference_b43### ###reference_b43###)), Fast and Robust EarlyExiting (FREE) (Bae et al., 2023 ###reference_b1### ###reference_b1###), Pegasus (Shleifer & Rush, 2020 ###reference_b47### ###reference_b47###) (pruning and distillation) and DQ-Bart (Li et al., 2022 ###reference_b27### ###reference_b27###) (quantization and distillation) and compare them with Ours (SD). In Shleifer & Rush (2020 ###reference_b47### ###reference_b47###), it utilizes the shrink and finetune methods: BART-student, Pegasus, and BART on CNN/DailyMail. In Li et al. (2022 ###reference_b27### ###reference_b27###), it uses quantization and distillation. It reports the BART (8-8-8 6-1). The number at here represents the number of bits for weights, word embedding, activations, the number of encoder layers, and the number of decoder layers. The results shown in Table 3 ###reference_### ###reference_### demonstrate our switchable decision achieves a good trade-off between quality and computation. These results verify that our method contributes to efficiency and accuracy, helping the generation model to choose the inference path and candidates dynamically.\n\nFurther, combining quantization or distillation method, our effective method of improving the language generation model can also be seen as a complementary and plug-in module. We leave this as a future work."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Classification",
            "text": "The Multi-Genre NLI (MNLI; (Williams et al., 2017b ###reference_b55###)), Recognizing Textual Entailment (RTE; (Dagan et al., 2005 ###reference_b4###)), and Stanford Sentiment Treebank (SST; (Socher et al., 2013 ###reference_b48###)) are included. We adopt several baselines from the existing literature. \u2460 For BERT, following Devlin et al. (2019 ###reference_b7###), it introduces masked language modeling, which allows pre-training to learn interactions between left and right context words. \u2461 UniLM (Dong et al., 2019 ###reference_b8###), the baseline, fine-tunes BERT with an ensemble of masks, some of which allow only leftward context. \u2462 RoBERTa, following Liu et al. (2019a ###reference_b35###), is pretrained with dynamically changing the mask. \u2463 For BART (Lewis et al., 2019 ###reference_b26###), it is a bi-directional encoder-decoder structure. \n\nSD yields a better trade-off between accuracy and computational efficiency. Ours shows comparable performance over BART and a clear-margin gain over other baselines, while sufficiently lower FLOPs. For example, SD achieves 87.2% accuracy v.s. BART\u2019s 87.0% accuracy with only 83.6% FLOPs. Our dynamic network demonstrates the strong capability of making skipping decisions for auto-regressive generation models. It further verifies that our method can work for different datasets and can generalize to different input types and fields."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Question Answering",
            "text": "For both SQuAD v1.1 and v2.0, following Lewis et al. (2019), we feed the complete documents into the encoder and decoder, and use the top hidden state of the decoder as a representation for each word. This representation is used to classify the token. Table 2 shows our experiment results. The BART large is used as the primary baseline, and the recent baselines (Devlin et al., 2019; Dong et al., 2019; Liu et al., 2019b) are reported. We load the official checkpoint from with the official pre-processed SQuAD data. On question answering, by dynamically skipping the candidates from attention layers, feed-forward layers, and input tokens, our model achieves a similar EM and F1 score as BART. Different from the above tasks, here the input is concatenated question and context and additionally passed to the decoder. Although the input is organized in different formats, it is interesting to see the consistent computation cost improvement of our proposed switchable decision in question answering. It further demonstrates that SD can be utilized in general NLP tasks."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Analysis",
            "text": "As discussed in Section 3  ###reference_###, our proposed method targets the auto-regressive generation model. Thus, can our method be adapted to other auto-regressive generation models? We select the GPT-2 (Radford et al., 2019  ###reference_b39###) base and T5 (Raffel et al., 2020  ###reference_b40###) base to study the performance after adapting our proposed switchable decisions. The results are presented in Table 6  ###reference_###.\nIt indicates our method is insensitive to different generation models.\nThis confirms our discussion in Section 3  ###reference_### that SD can serve as an efficient alternative dynamic network for versatile generation models. We also analyze the impact of making decisions based on different hidden representations. More details about LLaMA (Touvron et al., 2023  ###reference_b51###) models are included in Appendix A  ###reference_###.\nWe test if our results are sensitive to the choice of architectures: encoder-only, decoder-only, and encoder-decoder. We create the following scenarios: \u2460 For encoder-only, we incorporate the attention and feed-forward as the skipping candidates. \u2461 For decoder-only, similarly, the attention and feed-forward are included. \u2462 For token-only, the token candidate is utilized. Then we compare these three designs with SD and BART large to see the impact of incorporating our designed decision space into these different model architectures. As shown in Table 7  ###reference_###, we observe distinct FLOPs (reducing 10%) saving by only adding our skipping attention and feed-forward strategies for encoder-only and decoder-only. By only including the token skipping for the encoder-decoder structure, we observe the larger FLOPs (reducing 29%) saving while delivering the comparable ROUGE to BART. We refer the readers to Appendix 6.1  ###reference_### for the detailed skipping percentage of each candidate.\nThese results confirm our analysis and motivation for the switchable decision that using a combination of all these architectural search spaces comes to the best efficiency and accuracy trade-off.\nWe conduct the ablation study to examine the role of constrained optimization.\nFor ablation, instead of automatically searching the trade-off between the quality and computation,\nwe manually set the  in Eqn (7  ###reference_###) as , , .\nWe also include the random selection strategy. The random selection strategy is not learning switchable decisions and would not dynamically assign computation for each data instance.\n\u2776 Table 8  ###reference_### shows that\nthe constrained optimization of our method brings clear benefits. \u2777 We find that without CO, \u2018\n\n\n CO\u2019 with different manually tuned  value shows an unstable trade-off between the ROUGE and FLOPs across all  values, indicating that manually tuned  value can not bring both optimized quality and computation together.\n\u2778Empirically, we randomly select a policy from our decision space candidates and use the same other parameters. These result in a degradation in performance and lower FLOPs reduction.\nIt demonstrates the necessity and effectiveness of the constrained optimization for the switchable candidate set in SD structure.\nWe provide the parameter sizes, average GPU memory per device, per step training time, and inference time comparisons between the baseline and SD during the finetuning. Experiments in this part\nare performed on eight Tesla V100 GPUs.\n\u2776\nTable 9  ###reference_### shows that SD keeps the parameter size at the same level as the BART large during finetuning. The GPU memory per device and training time of SD are slightly higher (2.7% for memory and 1.6% for running time) than BART.\nSD gives the best inference FLOPs, outperforming BART while keeping the comparable ROUGE score and running time. \u2777\nFor the inference time, we evaluate our method and BART large on CNN/DailyMail following the same setting and device with batch size 1. For each iteration, 5.1 seconds (Ours) vs. 10.3 seconds (BART). Our dynamic network demonstrates the strong capability of making skipping decisions.\n\u2778\nWith the constrained optimization and the reinforcement learning agent, our switchable decision is still computationally productive as the design of our optimization and agent (e.g., applying one-layer MLP for policy network) has almost negligible finetuning computational cost.\nIn Section 3.1  ###reference_###, we consider three skipping candidates\u2019 hidden representations (attention, feed-forward, and query) after the first layer as the input for our reinforcement learning agent to make switchable decisions.\nHere, we demonstrate that using hidden representations from different layers comes to the same results, and therefore we pick the easiest one.\nWe set up a baseline here, in which whether to skip the following layer is dependent on the nearby previous layer outputs.\nWe experiment on Ours (based on the output from the first layer) and Ours Layer Wise (layer-wise decisions based on the output from the nearby previous layers).\nThe difference between these two cases is small in Table 11  ###reference_###.\nThe layer-wise design requires more computation as it needs to make decisions at each layer.\nTherefore, it further demonstrates that the design of ours is capable of making skipping decisions and imposing less computational cost."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Contributions of Search Space Candidates.",
            "text": "To further identify the contributions of our search space candidates for efficiency improvements and inference acceleration, we present the details skipping percentage of each candidate for CNN/DailyMail, SQuAD 1.1, and SST in Table 10  ###reference_###. For CNN/DailyMail, we observe around  attention skipping of total attention,  feed-forward skipping of total feed-forward, and  token skipping of total tokens.\nThe similar skipping percentage holds for question answering.\nHowever, we have seen an obvious contrast in the token skipping percentage in classification tasks.\nThe key observation is that the skipping percentages for tokens are high for both CNN/DailyMail and SQuAD 1.1.\nIn addition, our method generally takes around 5K iterations for the reinforcement learning algorithm to converge on CNN/DailyMail.\nThis confirms our conjecture in Section 5.1  ###reference_###.\nFor summarization and question answering tasks,\nthe first few parts of inputs are more representative.\nThus, it perfectly serves as the candidate for our switchable network to make the skipping decisions.\nIn Section 3.1  ###reference_###  ###reference_###, we consider three skipping candidates\u2019 hidden representations (attention, feed-forward, and query) after the first layer as the input for our reinforcement learning agent to make switchable decisions.\nHere, we demonstrate that using hidden representations from different layers comes to the same results, and therefore we pick the easiest one.\nWe set up a baseline here, in which whether to skip the following layer is dependent on the nearby previous layer outputs.\nWe experiment on Ours (based on the output from the first layer) and Ours Layer Wise (layer-wise decisions based on the output from the nearby previous layers).\nThe difference between these two cases is small in Table 11  ###reference_###  ###reference_###.\nThe layer-wise design requires more computation as it needs to make decisions at each layer.\nTherefore, it further demonstrates that the design of ours is capable of making skipping decisions and imposing less computational cost."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "Our work demonstrates the benefits of introducing a switchable decision of the dynamic network. The proposed method can dramatically increase the inference efficiency and still enable the model performance. Noticeable FLOPs saving and consistent performance are observed across summarization, question answering, and classification benchmarks. We further conduct a detailed study with the proposed switchable strategy in different settings, e.g., comparing with different architecture search spaces, providing more evidence for making decisions based on hidden representations, and verifying the impact of components. To summarize, the proposed SD is effective and general, with the potential to be incorporated into existing generation models for various NLP tasks."
        }
    ],
    "url": "http://arxiv.org/html/2405.04513v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "5",
            "5.1",
            "5.2",
            "5.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "6",
            "6.1"
        ]
    },
    "research_context": {
        "paper_id": "2405.04513v1",
        "paper_title": "Switchable Decision: Dynamic Neural Generation Networks",
        "research_background": "### Motivation\nThe motivation behind the paper lies in addressing the heavy computational costs associated with large-scale pre-trained language models such as BART. These models present significant challenges during inference, particularly in contexts with constrained computational resources, such as IoT devices and real-world applications.\n\n### Research Problem\nThe primary research problem tackled by this paper is how to dynamically allocate computation across a generation model in a way that balances efficiency and prediction accuracy. The paper proposes creating a dynamic neural network framework for auto-regressive generation models that can make input-specific decisions about which parts of the model to utilize or skip during inference. This approach aims to optimize computational resources while maintaining high performance.\n\n### Relevant Prior Work\n1. **Efficient Inference Methods**: Previous efforts have focused on reducing the computational burden during inference through various techniques:\n   - **Pruning and Compression**: Methods to reduce model size and complexity by pruning unimportant weights or compressing parameters (Han et al., 2015b; Fan et al., 2019; Gordon et al., 2020).\n   - **Quantization**: Techniques to decrease the number of bits needed for model representation (Lin et al., 2016; Shen et al., 2020).\n   - **Model Distillation**: Transferring knowledge from large teacher models to smaller student models (Hinton et al., 2015; Jiao et al., 2019).\n\n2. **Dynamic Parameter Switching**: Some methods dynamically adjust model parameters based on the input data:\n   - **Mixture of Experts**: Using multiple sub-networks and selecting the best one for each input (Shazeer et al., 2017).\n   - **Switch Transformer**: Switching among different experts for different data instances (Fedus et al., 2021).\n\n3. **Early Exiting**: Approaches that make adaptive exit decisions based on model confidence or a trained agent to determine the necessity of additional computational layers (Various prior works).\n\nIn summary, this paper builds upon existing methods by introducing a dynamic neural network framework that specifically targets encoder-decoder auto-regressive generation models. It innovates by proposing a method to dynamically select which layers and tokens to process based on each specific input, optimizing the trade-off between computational efficiency and model accuracy.",
        "methodology": "### Methodology: Switchable Decision: Dynamic Neural Generation Networks\n\nThe proposed switchable decision network focuses on accelerating the inference time for autoregressive language generation models. The approach is based on three main strategies: constructing a versatile decision space, using an input-dependent reinforcement learning agent, and applying lexicographic (lexico) optimization.\n\n#### Key Steps in the Methodology:\n\n1. **Versatile Decision Space Construction**: The decision space includes multiple possible actions for whether to skip layers in the model. These decisions are modeled as i.i.d. Bernoulli random variables parameterized by a policy network \\( \\pi \\).\n\n2. **Input-Dependent Reinforcement Learning Agent**: The agent learns instance-specific probabilities for keeping or skipping the hidden representations of individual layers. Decisions are made using a policy network that outputs a binomial distribution over possible actions. This network consists of a one-layer MLP with layer normalization and ReLU activation functions, using sigmoid for layer decisions and softmax for token skipping decisions.\n\n3. **Lexicographic Optimization Strategy**: To optimize the trade-off between model quality and computational cost (measured in FLOPs), a constrained optimization approach is applied. This involves iteratively updating the parameters to balance minimizing the loss function and ensuring computation constraints are met.\n\n#### Detailed Components:\n\n- **Token Embedding and Model Architecture**:\n  - Tokens are embedded to form matrix \\( E \\) which then passes through the transformer-based model's encoders and decoders.\n  - For each layer \\( l \\), a decision \\( z_l \\) is made to either skip or keep the layer, using a Bernoulli distribution.\n\n- **Skipping Strategies**:\n  - **Layer Skipping**: Applied to both attention and feed-forward layers, and determined by polices from the trained reinforcement learning agent.\n  - **Token Skipping**: Two strategies include skipping the last \\( t \\) tokens or skipping uniformly across tokens, with decisions modeled as a categorical distribution.\n\n- **Policy Network**:\n  - Outputs instance-specific probabilities for layer and token decisions. \n  - Uses a simple and efficient MLP design for faster inference.\n\n- **Training Process**:\n  - Utilizes policy gradients to optimize the parameters of the policy network. The loss function integrates both model quality and computational cost.\n  - Employs a self-critical baseline to manage gradient variance and lexicographic optimization for balancing multiple objectives without manual tuning of coefficients.\n\n- **Inference Process**:\n  - During inference, instead of sampling, the network uses the decisions that maximize the likelihood function directly.\n\n#### Optimization and Constraints:\n\n- The optimization strategy ensures that one objective (computational cost in this case) is satisfied as a primary goal, while minimizing the secondary objective (model loss) without compromising the primary objective. The parameters are iteratively updated to achieve a balance between the two criteria.\n\n- **Summary of Algorithm**:\n  - The process iteratively updates both the auto-regressive model and the policy network. The policy network parameters \\( \\theta_d \\) are updated to balance quality optimization and constraint satisfaction on computation.\n\nThis methodology innovates by integrating token/layer skipping with reinforcement learning and lexicographic optimization to dynamically speed up the inference process while maintaining high model quality.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Datasets\n- **CNN/DailyMail:** Consists of 287,226 documents for training, 13,368 documents for validation, and 11,490 documents for testing.\n- **XSum:** Contains 226,711 news articles accompanied by a one-sentence summary. Following the splits of Narayan et al. (2018), it includes 204,045 training articles, 11,332 validation articles, and 11,334 test articles.\n- **SQuAD v1.1 and v2.0:** Used for machine reading comprehension tasks, where SQuAD v2.0 includes examples where the answer cannot be derived from the context. The setup follows prior work by using concatenated questions and context as input to the encoder of BART, and additionally passing them to the decoder.\n- **GLUE Benchmark:** Includes tasks such as Multi-Genre NLI (MNLI), Recognizing Textual Entailment (RTE), and Stanford Sentiment Treebank (SST). This benchmark is selected to evaluate the generalization and robustness of the proposed method.\n\n#### Baselines\n- Algorithms consistent with those used in prior work (Lewis et al., 2019), including BART.\n\n#### Evaluation Metrics\n- **Summarization Tasks (CNN/DailyMail and XSum):**\n  - ROUGE-1 (R-1): Unigram overlap.\n  - ROUGE-2 (R-2): Bigram overlap.\n  - ROUGE-L (R-L): Longest common subsequence.\n- **Question Answering (SQuAD v1.1 and v2.0):**\n  - Exact Match (EM).\n  - F1 Score.\n- **Natural Language Understanding (GLUE Benchmark):**\n  - Accuracy.\n\n#### Main Experimental Results\n- **Summarization Tasks:**\n  - Performance measured via ROUGE metrics showed that the proposed method effectively captured both informativeness (R-1 and R-2) and fluency (R-L) of the generated summaries for both CNN/DailyMail and XSum datasets.\n- **Question Answering:**\n  - The method exhibited strong performance on SQuAD v1.1 and v2.0 datasets, with high Exact Match (EM) and F1 scores, demonstrating its capability in accurately extracting relevant information from the context.\n- **Natural Language Understanding:**\n  - Accuracy results on the diverse set of GLUE tasks (MNLI, RTE, SST) indicated the method's robustness and generalization ability across different NLU problems."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To analyze the efficiency and performance of the proposed switchable decisions across different generation models and various architectural setups.",
            "experiment_process": "The study evaluates the proposed switchable decisions on GPT-2 and T5 models. It further experiments with different model architectures, including encoder-only, decoder-only, and encoder-decoder structures. The scenarios consider various skipping candidates - attention and feed-forward for encoder and decoder, and token for encoder-decoder. The evaluations compare the proposed designs with SD and BART large, examining the impact on FLOPs and ROUGE scores. Additionally, it performs an ablation study on the trade-offs between quality and computation by manually setting lambda values and including a random selection strategy.",
            "result_discussion": "The results show that incorporating skipping strategies saves 10% FLOPs for encoder-only and decoder-only structures and 29% for the encoder-decoder structure while maintaining comparable ROUGE scores. The constrained optimization is shown to provide clear benefits, as manually tuned values result in unstable trade-offs between ROUGE and FLOPs. Random selection degrades performance and lowers FLOPs reduction. SD maintains similar parameter sizes to BART large during fine-tuning, with slightly higher memory and training time. SD outperforms BART in inference FLOPs and has significantly reduced inference time. The chosen design is confirmed to be effective in making skipping decisions with less computational cost.",
            "ablation_id": "2405.04513v1.No1"
        },
        {
            "research_objective": "To identify the contributions of individual search space candidates to efficiency improvements and inference acceleration.",
            "experiment_process": "The study details the skipping percentages of each candidate (attention, feed-forward, and token) for CNN/DailyMail, SQuAD 1.1, and SST datasets. It utilizes hidden representations after the first layer as input for the reinforcement learning agent to make switchable decisions and compares the baseline decision-making from the nearby previous layer outputs against layer-wise decisions.",
            "result_discussion": "The findings indicate a high percentage of attention and feed-forward skipping for CNN/DailyMail and consistent patterns for question answering tasks. For classification tasks, token skipping shows an obvious contrast. The reinforcement learning algorithm converges after around 5K iterations on CNN/DailyMail. Similar hidden representation layers yield consistent results, reinforcing that the design effectively makes skipping decisions with reduced computational cost. Layer-wise decisions require more computation and demonstrate that the initial design is efficient for the intended purpose.",
            "ablation_id": "2405.04513v1.No2"
        }
    ]
}