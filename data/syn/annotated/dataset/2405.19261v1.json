{
    "title": "Faster Cascades via Speculative Decoding",
    "abstract": "Cascades and speculative decoding are two common approaches to improving language models\u2019 inference efficiency. Both approaches involve interleaving models of different sizes, but via fundamentally distinct mechanisms: cascades employ a deferral rule that invokes the larger model only for \u201chard\u201d inputs, while speculative decoding uses speculative execution to primarily invoke the larger model in parallel verification mode. These mechanisms offer different benefits: empirically, cascades are often capable of yielding better quality than even the larger model, while theoretically, speculative decoding offers a guarantee of quality-neutrality. In this paper, we leverage the best of both these approaches by designing new speculative cascading techniques that implement their deferral rule through speculative execution. We characterize the optimal deferral rule for our speculative cascades and employ a plug-in approximation to the optimal rule. Through experiments with T5 models on benchmark language tasks, we show that the proposed approach yields better cost-quality trade-offs than cascading and speculative decoding baselines.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large language models (LLMs) have demonstrated significant advances in quality on a range of natural language processing tasks, at the cost of a significant increase in inference latency. This has sparked a growing body of literature on reducing LMs\u2019 inference costs without (overly) compromising on quality. One such line of work involves constructing a family of models of various sizes (e.g., a small and large model), and suitably orchestrating amongst them to make a prediction. Two canonical instantiations of this strategy are model cascading and speculative decoding.\n\nWhile similar in spirit, cascades and speculative decoding are fundamentally different in details. Cascades employ a deferral rule to identify \u201chard\u201d inputs, and only invoke larger models on such inputs. For example, in a two-model cascade, one first invokes the smaller model, and uses its associated probability of the generated output to decide whether to defer to the larger model. By contrast, speculative decoding uses a small model to draft a block of tokens via standard auto-regressive decoding, which are then verified in parallel by a large model. One then accepts all drafted tokens until the first \u201cimplausible\u201d one, which is rolled back based on the larger LM\u2019s prediction. Owing to their different mechanisms, both methods have complementary strengths. Cascades seek to output distributions that have the best quality for a given cost budget, and are empirically observed to often yield better accuracies than even the individual models they are constructed with. By contrast, speculative decoding is theoretically guaranteed to match the output distribution (or a close approximation thereof), and are practically observed to provide impressive speed-ups.\n\nGiven the complementary nature of these two approaches, a natural question that arises is whether we can leverage the best of both techniques. In this paper, we do so by designing new techniques for two-model cascades that implement their deferral rule in a speculative manner: we have the smaller model generate drafts auto-regressively, and the larger model execute in parallel on the drafts to decide whether or not to defer on them. We show that this speculative cascading approach yields better cost-quality trade-offs than both standard cascades and speculative decoding. In detail, we make the following contributions:\n\nWe introduce a general recipe for speculative execution, where we seek to mimic a general target distribution that interleaves the drafter\u2019s and verifier\u2019s output distributions. Lossy speculative sampling is a special case of this recipe for a particular target distribution. We show how common cascading deferral rules, such as Chow\u2019s rule and confidence-difference thresholding, can be implemented speculatively by plugging in the corresponding target distribution into the proposed framework. We refer to these as speculative cascades.\n\nWe characterize the theoretically optimal deferral rule for a speculative cascade, and design a speculative cascading technique that implements a plug-in estimate to the optimal rule.\n\nThrough experiments on benchmark language tasks, we show that speculative cascades constructed from T5 models of different sizes are able to provide better cost-quality trade-offs than their sequential cascade and speculative decoding counterparts."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "A Tale of Two Efficient LM Inference Strategies",
            "text": "Let  denote a finite vocabulary of tokens,\nwith  the set of all sequences generated by this vocabulary.\nLet  denote the set of all probability distributions over tokens in .\nGiven an arbitrary length sequence  and index , denote by .\nA language model (LM)\nis a\nprobability distribution over .\nLet  denote the ground-truth probability distribution over . This could be, for example, a distribution over prompt-response pairs\nthat the LM may encounter during deployment, or a distribution of sequences used to pre-train the LM.\nWe will measure the quality of an LM based on how closely it mimics .\nSuppose we are provided two LMs  and ,\nwhere  is the larger model. Our goal is to design an inference strategy that selectively invokes  and  to trade-off between quality and latency (which may be approximated by the fraction of times that  is invoked).\nWe will denote by  the probability  associates to token  given prefix\n, and by \nthe same distribution from model . Whenever it is clear from context, we will hide the conditioning on prefix , and use the shorthand  for  and  for .\nCascades are an effective strategy to trade-off cost and quality by having the smaller model  handle the \u201ceasy\u201d samples,\nand the larger model  handle the \u201cdifficult\u201d samples [18  ###reference_b18###, 50  ###reference_b50###]. A common approach to cascading is confidence-based thresholding or Chow\u2019s rule [10  ###reference_b10###, 22  ###reference_b22###],\nwhere we first run  on the given input, and defer to  when \u2019s confidence for its generated response is sufficiently low. This strategy is typically\nimplemented at the sequence-level, where for a given prefix  we invoke  to generate a complete response . We evaluate the predicted probability from  for this response, and check whether it falls below a threshold :\nIf the above holds,\nwe defer to  to generate a new response;\notherwise,\nwe retain \u2019s response. One may then tune the threshold to achieve a desired cost-quality trade-off. The literature also offers variants of Chow\u2019s rule that use a more nuanced aggregation of per-token uncertainties [18  ###reference_b18###].\nSpeculative decoding is an alternate inference strategy that applies token-level interleaving between  and ,\nresulting in provably matching the larger model quality at a reduced inference cost [39  ###reference_b39###, 26  ###reference_b26###].\nGiven a prefix , we sample  draft tokens  auto-regressively from , and run  in parallel on the  prefixes , and verify if the generated tokens can be accepted. We then rollback to the first token  that was rejected, replace with a new token, and repeat the drafting-verification process with  as the new prefix.\nDuring the verification stage, a draft token  generated by  is accepted\nwith probability\n\nand rejected otherwise,\nwhere recall the shorthand\n and .\nA rejected token is then replaced by a new token sampled from a modified distribution\n\nwhere \ndenotes normalization to sum to 1.\nThis sampling process is\nprovably\nequivalent to sampling  tokens auto-regressively from  for prefix  [26  ###reference_b26###].\nWe summarize this speculative sampling procedure in Algorithm 1  ###reference_###.\nEach invocation of this algorithm generates at most  next tokens for a given prefix . One may run this algorithm multiple times to generate a complete output sequence.\nIn practice, one may employ a lossy variant [44  ###reference_b44###] of the above sampling that allows some deviation from verifier\u2019s distribution . In this case, a draft token  is accepted\nwith probability\n,\nwhere  is a strictness parameter, with higher values indicating greater deviation from .\nA rejected token may then be replaced by a token sampled from the residual distribution\n\nwhere  is a parameter that depends on ,  and .\nA common heuristic is to simply set  [52  ###reference_b52###]."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Cascades Meet Speculative Decoding",
            "text": "Both cascades and speculative decoding interleave models of different sizes to reduce inference cost, but fundamentally differ in the mechanisms they use.\nAs a step towards comparing the strengths and weaknesses of these approaches, we first\ndescribe how one may design a token-level cascade."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Warm-up: Token-level cascades",
            "text": "It is straight-forward to extend\nthe sequence-level\nChow\u2019s rule from \u00a72  ###reference_### to form a token-level cascade between  and . For a prefix , we first compute the smaller model\u2019s distribution , and check whether  is below a pre-chosen threshold.\nif so,\nwe evaluate , and sample ;\notherwise,\nwe sample\n.\nMore generally, we may design a token-level deferral rule  that takes the prefix  as input and outputs a binary decision, with  indicating that we\ndefer to  (i.e., draw a sample from  rather than ).\nFor example, token-level Chow\u2019s rule can be written as:\nwhere  is a cost parameter; the higher the value, the lower is the frequency of deferral to . One may also use a confidence measure different from the maximum probability, such as the entropy of the small model\u2019s probability distribution.\nWe elaborate in \u00a7B  ###reference_###\nthat the choice of confidence measure would depend on the evaluation metric of interest; (2  ###reference_###) is typically prescribed when the cascade\u2019s quality\nis evaluated in terms of its accuracy against the ground-truth distribution on individual tokens, whereas entropy is prescribed when the metric of interest is the cross-entropy loss."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Optimal token-level cascade deferral",
            "text": "While Chow\u2019s rule (2  ###reference_###) is easy to implement,\nit can be sub-optimal if the smaller model\u2019s maximum token probability is not reflective of which of the two models are better equipped to predict the next token for a given prefix [22  ###reference_b22###].\nGiven this, it is natural to ask what the optimal deferral rule  for a token-cascade looks like,\nand whether we can reasonably approximate this rule.\nFor this, we must first specify an objective to minimize at each step .\nFollowing the prior cascade literature [22  ###reference_b22###, 18  ###reference_b18###], a reasonable objective to minimize is the expected loss from the deferral rule against the ground-truth distribution\n,\nwith an added cost for deferring to the larger model. We state this below for a fixed prefix ,\nusing the short-hand  for  and  for :\nfor a cost penalty \nand loss function .\nCommon choices for  include the 0-1 loss  and the log loss\nThe minimizer of (3  ###reference_###) is of the form:\nIntuitively, we compare the expected loss from  with the expected cost of invoking , and decide to defer when the latter is smaller.\nWe note here that this optimization problem is set up for a fixed prefix . One may also consider the coupled optimization problem across all prefixes from 1 to .\nPlug-in estimator for (4  ###reference_###). The optimal rule in (4  ###reference_###) requires computing expectations over the ground-truth distribution , which is not available during inference time.\nA common approach in the cascades literature is to replace the expected losses with the models\u2019 confidence estimates [22  ###reference_b22###]. For example, when , it may be reasonable to use  as an estimate of the expected 0-1 loss  and  as an estimate of . The extent to which these estimates are accurate depend on how well  and  are calibrated [17  ###reference_b17###]. The resulting plug-in estimator for (4  ###reference_###) is given by:\nSimilarly, when , we may use the entropy  from  as an estimate of its expected log-loss, and similarly for  (see \u00a7C  ###reference_###).\ncannot be directly used in a token-level cascade,\nas it needs the larger model to be invoked at every position .\nHowever, it serves as an oracle that allows to analyze the head-room available to improve upon Chow\u2019s rule.\nSee also Remark 2  ###reference_ark2###."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "When do token-level cascades outperform speculative decoding?",
            "text": "Token-level cascades and\nspeculative decoding differ in the distribution over tokens they seek to mimic. Speculative decoding seeks to mimic the larger model\u2019s output distribution\n(or an approximation to it). As a result, the quality of the sampled output is limited by the accuracy of the larger model. On the other hand, token-level cascades seek to output distributions that closely approximate the ground-truth label distribution, and can potentially yield better quality than even the larger model.\nIndeed, speculative decoding would be ideal in applications where the verification model  is uniformly better than the draft model  on all inputs. However, when there are inputs where the draft model fares better than the verifier, one may want to retain the drafter\u2019s predictions even when it disagrees with the verifier. Similarly, in settings where both the drafter and verifier fare poorly on some inputs (e.g., due to label noise), one may again want to ignore the disagreement between the drafter and verifier, and avoid triggering unnecessary roll-backs.\nAs a concrete example, we consider token-level cascades of T5 models [35  ###reference_b35###] of two different sizes finetuned on a WMT EN  DE translation [3  ###reference_b3###] and an extreme summarization (XSum) task [31  ###reference_b31###]. We construct these cascades using both\n(token-level)\nChow\u2019s rule in (2  ###reference_###) and the oracle Diff rule in (5  ###reference_###), and also apply speculative decoding with the smaller (larger) model as the drafter (verifier). In Figure 1  ###reference_###, we plot quality as a function of fraction of samples deferred to the large model, as we vary the cost parameter .\nNote that with speculative decoding, each verification step is counted as a single call to the large model.\nWhile speculative decoding matches the quality of the large model (right-most point), the oracle rule yields significantly better quality on a wide range of operating points. Even Chow\u2019s rule, which is sub-optimal for cascading [22  ###reference_b22###],\noutperforms speculative decoding in a small region.\nHowever, as also evident from the plots, token-level cascades require a significantly larger number of calls to the larger model to achieve the same quality.\nThis is because token-level cascades are executed sequentially: whenever  defers, we execute  once to generate one next token for the prefix accumulated so far, and the control transfers back to . In contrast, speculative decoding runs  in scoring mode to verify multiple draft tokens from  in parallel. Moreover, the stochastic verification algorithm in speculative decoding often results in fewer tokens from  getting rejected compared to the deterministic deferral rules used in a cascade. These observations motivate a natural question: given their complementary strengths, how can we leverage the best of both these techniques?\n###figure_1### ###figure_2###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Speculative Cascades: Leveraging the Best of Both Worlds",
            "text": "In addressing the above question, we present our main contribution: a principled approach to combining the superior quality of cascades with the faster execution of speculative decoding approaches."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Speculative decoding with general target distributions",
            "text": "We begin by considering a generic version of speculative sampling that seeks to mimic a general target distribution derived from the drafter\u2019s and verifier\u2019s distributions.\nIn the proposed sampling procedure outlined in Algorithm 4  ###reference_###, we sample tokens auto-regressively as before from the drafter\u2019s distribution. During the verification step, however, we do not compare the drafter\u2019s token probabilities against the verifier\u2019s distribution. Instead, we use a user-specified target distribution  derived from the drafter\u2019s and verifier\u2019s distributions at position , for some function  that is inexpensive to compute.\nWe accept a draft token  when  and reject it otherwise with probability . Upon rejection, we re-sample from the residual distribution .\nThis general procedure not only encompasses standard speculative decoding [26  ###reference_b26###] for , but also includes lossy speculative decoding [44  ###reference_b44###] as a special case:\nAlgorithm 4  ###reference_### reduces to the lossy speculative sampling procedure in [44  ###reference_b44###] with parameters  and  when ."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "From sequential to speculative cascades",
            "text": "Equipped with Algorithm 4  ###reference_###, we now propose new cascading techniques that implement their deferral rule in a speculative manner. Recall that a\ntoken-level\ncascade of two models  and  is defined by a deferral rule .\nFor a prefix , the next-token distribution at position  modeled by this cascade can be written as:\nIn fact, for all the deferral rules described in \u00a72  ###reference_###, the resulting distribution can be described by a target distribution function  of the form:\nfor some function  that maps distributions  to a binary decision. For example, for Chow, , and for Diff,  See Table 1  ###reference_### for a summary of target distributions for different deferral rules.\nOur proposal is to then invoke the speculative sampling procedure in Algorithm 4  ###reference_### with  as the target distribution function.\nWe outline this generic speculative cascading approach in Algorithm 5  ###reference_###, and contrast it with the sequential execution of a deferral rule in Algorithm 2  ###reference_###.\nUnlike a sequential cascade, where the larger model\u2019s distribution  is not available at the time the deferral decision is made (see Remark 1  ###reference_ark1###), with a speculative cascade, we can accommodate deferral rules like Diff that depend on both  and . This is because we run the larger model  in parallel on drafts generated by the smaller model , allowing us to compute both  and  on every prefix.\nSo far we have considered deferral rules designed for sequential cascades. In what follows, we derive the optimal deferral rule  for a speculative cascade, where we sample speculatively from a target distribution  using  as the drafter."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Deferral risk for speculative cascades",
            "text": "As with sequential cascades (\u00a72  ###reference_###), we begin by defining an objective to minimize. We seek a deferral rule  that minimizes a loss against the ground-truth distribution, while limiting the inference cost to be within a budget.\n(Per above, this deferral rule implicitly defines a target distribution .)\nThe inference cost crucially depends on how frequently a draft token is rejected in the verification phase, triggering a rollback.\nTo this end, we derive the probability that a token sampled from  is rejected during verification, for a target distribution resulting from a deferral rule .\nFor a given prefix , and target distribution , the probability of a token drawn from draft distribution  being rejected is equal to:\nwhere  is the TV distance between  and .\nIntuitively, whenever ,  and therefore there is no rejection or roll-back; when , the rejection rate equals .\nFor a fixed prefix , we formulate the goal of finding a solution to:\nfor some budget . Equivalently, one may minimize an unconstrained objective similar to (3  ###reference_###),\nfor suitable cost parameter  (see \u00a7C.4  ###reference_###):\nContrasting (8  ###reference_###) with the deferral risk in (3  ###reference_###) for a sequential cascade, a key difference is that the cost of deferring to the larger model is no longer a constant, but depends on the similarity between  and , as measured by the TV distance between them."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Optimal speculative deferral",
            "text": "We next derive the optimal deferral rule for (8  ###reference_###), and construct a feasible estimator for it.\nThe minimizer of (8  ###reference_###) is of the form:\nWhen  and  are similar, the rejection rate for  is low, and hence the deferral decision will depend largely on which of the two models yields a lower expected loss. When  and  are very different, the optimal decision is to defer to  only when it yields a substantially lower loss than .\nPlug-in estimator for (9  ###reference_###).\nThe optimal rule requires estimating expectations with respect the ground-truth distribution  We employ similar plug-in estimators as the ones used with sequential cascades (\u00a73  ###reference_###). When , we replace the expected 0-1 loss with (one minus) the maximum probability from the model, giving us:\nThe efficacy of the plug-in estimator depends on how closely the individual models approximate the ground-truth distribution ; this is formalized by the following regret bound:\nSuppose . Then for a fixed prefix :\nOne can now run the speculative cascading procedure in Algorithm 5  ###reference_### using (10  ###reference_###) as the deferral rule; the corresponding  is listed in Table 1  ###reference_###. See \u00a7C.2  ###reference_### for a similar derivation for .\nIn practice it is common to apply a temperature scaling to both  and ,\nand sample from\n\nand\n\nrespectively, for a temperature parameter . In this case, the constrained problem in (7  ###reference_###) would use the TV distance between the temperature-scaled distributions  and  to measure the resulting rejection rate, and the optimal deferral rule in Lemma 4  ###reference_8### would now use  instead of . To construct a plug-in estimator to this optimal rule, we still prescribe using the unscaled probabilities  and  to estimate the expected loss, giving us, for :\nOne may run Algorithm 5  ###reference_### with  as the deferral rule and the temperature scaled  as the drafter.\nWhen temperature ,  whenever , and is zero otherwise. In this case, running Algorithm 5  ###reference_### with  as the deferral rule (and  as the drafter) is equivalent to running it with  in (5  ###reference_###) as the deferral rule. In other words, for greedy decoding, the optimal deferral rules for a speculative cascade coincides with that for a sequential cascade. We formalize this in Lemma 8  ###reference_ma8### in \u00a7C.3  ###reference_###."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Further related work",
            "text": "There has been a stream of work on improving\nthe draft generation process in speculative decoding; these include having the drafter and verifier share the same backbone [39  ###reference_b39###, 25  ###reference_b25###, 6  ###reference_b6###, 30  ###reference_b30###, 21  ###reference_b21###, 51  ###reference_b51###, 16  ###reference_b16###, 27  ###reference_b27###], using multiple small draft models [9  ###reference_b9###, 46  ###reference_b46###], using tree-structured draft batches [38  ###reference_b38###, 29  ###reference_b29###], distilling the drafter with the verifier [52  ###reference_b52###], and leveraging multiple sampled candidates from the drafter [40  ###reference_b40###].\nThe work that is most closely related to our specific proposal is the Big Little Decoder (BiLD) [24  ###reference_b24###], which can be seen as another lossy variant of speculative decoding [26  ###reference_b26###, 44  ###reference_b44###, 52  ###reference_b52###]. BiLD has two phases: a fallback phase, during which the drafter  is run auto-regressively until its maximum predicted probability is sufficiently low; and a rollback phase, during which the verifier  is run in parallel on the prefixes generated by  and rolls back to the point where , for a metric  that measures discrepancy\nand threshold . The fallback phase can be seen as implementing Chow\u2019s deferral rule (2  ###reference_###), and allows for the draft window size to vary dynamically based on an estimate of how likely the draft tokens will be accepted; the rollback phase can be seen as a deterministic variant of the rejection sampling algorithm of [26  ###reference_b26###].\nAn advantage of BiLD over the rejection sampling algorithm in [26  ###reference_b26###] is the use of Chow\u2019s rule to vary the draft window size. However, the final target distribution it seeks to mimic, , is an approximation to , in that the target distribution  is chosen to satisfy . Hence, in cases where  deviates substantially from , BiLD would choose  as the target distribution, even  offers better quality (as captured by a suitable loss function in \u00a74.4  ###reference_###) on a prefix. In contrast, our proposed approach in \u00a74  ###reference_### uses speculative decoding to approximate target distributions that optimally cascade between  and .\nIn our experiments, we compare the efficacy of using  as the target distribution with the target distributions we propose in this paper (see Table 1  ###reference_###).\npropose in this paper (see Table 1  ###reference_###)."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Experimental results",
            "text": "We present experiments to demonstrate that the proposed speculative cascading techniques yield better cost-quality trade-offs compared to both sequential token-level cascades and standard speculative decoding. We construct speculative cascades with three different deferral rules: (i) Chow in (2  ###reference_###), (ii) Diff in (5  ###reference_###) and (iii) OPT in (10  ###reference_###). Our experimental setup is based on [52  ###reference_b52###]; see D.1  ###reference_### for details.\n\nWe consider CNN/Daily Mail summarization [20  ###reference_b20###] and XSum abstractive summarization [31  ###reference_b31###] as benchmark language datasets. We construct cascades from T5 v1.1 family of encoder-decoder models [34  ###reference_b34###], with T5-small (77M) as the small model, and either T5-large (800M) or T5-XL (3B) as the large model. In each case, we supervised fine-tune these models on the respective task. We use ROUGE-2 as the metric for the summarization tasks. To measure latency, we follow the protocol in [26  ###reference_b26###, 52  ###reference_b52###], and evaluate the wall-clock decoding time with a batch size of 1.\n\nSince our primary goal is to demonstrate the benefits of combining both cascading and speculative decoding, we compare against representative methods from both paradigms:\n\nSequence-level cascade [22  ###reference_b22###, 18  ###reference_b18###] based on sequence-level Chow\u2019s rule in (1  ###reference_###) (SeqCascade [Chow])\n\nToken-level cascade outlined in Algorithm 2  ###reference_###, with token-level Chow\u2019s rule in (2  ###reference_###) used for deferral [10  ###reference_b10###, 19  ###reference_b19###] (TokenCascade [Chow])\n\nLossy speculative decoding described in \u00a72  ###reference_###, with both  [26  ###reference_b26###, 52  ###reference_b52###] (SpecDecode [Lossy]) and with  tuned using the procedure in [44  ###reference_b44###] (SpecDecode [Lossy\u22c6])\n\nA variant of the Big-Little Decoder approach [24  ###reference_b24###], which applies Algorithm 4  ###reference_### to the target distribution  in \u00a75  ###reference_### (BiLD\u2217).\n\nWe do not include the oracle approach in Algorithm 3  ###reference_### as it is impractical to evaluate its running time. We also note that, while one could potentially integrate other recent variants of the speculative decoding (e.g., those in \u00a75  ###reference_### that warrant changes to the model architectures) into our speculative cascades framework, in the interest of a fair comparison, we use the original version by [26  ###reference_b26###].\n\nWe evaluate all methods under both greedy decoding (), and temperature sampling with temperature . As noted in \u00a74.3  ###reference_###, with greedy decoding, the OPT deferral rule coincides with the Diff deferral rule. We set the block size  for the methods based on speculative execution.\n\nIn Figure 2  ###reference_###, we present plots of quality versus latency for the different methods. In each case, we vary the lenience parameter , and plot the ROUGE-2 metric as a function of the relative latency to the larger model. For brevity, we include the three main baselines; in \u00a7D.3  ###reference_### we compare to SpecDecode [Lossy\u22c6] [44  ###reference_b44###].\n\nClearly, methods that use speculative execution are considerably faster than sequential token-level cascades, although sequential cascades do have a slight advantage in the low-latency regimes (unlike speculative approaches, which always call the large model after every  steps, a sequential cascade only invokes the large model when the small model defers). Under temperature sampling (), the OPT speculative cascading strategy is seen to provide the best quality metrics for most latency values, with Diff coming in a close second. A similar trend is also seen with greedy decoding, where the optimal deferral strategy (SpecCascade [Diff]) often yields better quality trade-offs than the other methods, although the gap to speculative decoding is much smaller.\n\nIn Table 2  ###reference_###, we report (i) the reduction in latency from different methods when matching the quality of the large model, and (ii) the best quality that each method can deliver without exceeding the latency of the large model (for temperature ). SpecCascade [OPT] is seen to yield the maximum speed-up and the best quality metrics. The cascading approaches are often seen to fare poorly on both quality"
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusions",
            "text": "We have proposed new speculative cascading techniques that use a combination of auto-regressive drafting and parallel verification to implement their deferral rule, and shown that they yield better cost-quality trade-offs than standard cascades and speculative decoding. In the future, we wish to replace our plug-in estimators with a router model trained explicitly on ground-truth samples to approximate the optimal rule. We also wish to improve the deferral objective we seek to optimize at each position, and replace it with a global (coupled) deferral objective that takes all prefixes from 1 to into account. Another useful direction to explore would be to extend our proposal to handle a general cascade with more than two models."
        }
    ],
    "url": "http://arxiv.org/html/2405.19261v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "5"
        ],
        "methodology_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "6"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.3",
            "6"
        ]
    },
    "research_context": {
        "paper_id": "2405.19261v1",
        "paper_title": "Faster Cascades via Speculative Decoding",
        "research_background": "### Motivation:\nThe paper is motivated by the significant advances in large language models (LLMs) in terms of quality for numerous natural language processing tasks, which, however, come with increased inference latency. There is a pressing need to reduce inference costs while maintaining quality. Existing techniques like model cascading and speculative decoding each offer unique benefits, but the authors recognize potential in combining these methods to leverage their complementary strengths.\n\n### Research Problem:\nThe central research problem addressed in this paper is how to combine the benefits of model cascading and speculative decoding to achieve better cost-quality trade-offs in LLMs. Specifically, the authors aim to design techniques that integrate speculative methods into the deferral rules of cascades to harness both quality improvements and speed-ups.\n\n### Relevant Prior Work:\n1. **Advancements in LLMs**: Mentioned works illustrate the progress made in LLMs across various tasks [33, 34, 5, 2, 11, 48, 12, 41, 1, 43, 42]. \n2. **Cost Reduction**: Studies focusing on reducing inference costs without significantly compromising quality [15, 32, 36, 26, 7, 37, 40].\n3. **Model Cascading**: Past works on model cascading demonstrate the strategy of deferring to larger models for harder inputs [47, 28, 45, 23, 14, 8, 18, 13].\n4. **Speculative Decoding**: Techniques where small models draft token blocks for large models to verify, known for speed-ups and theoretical guarantees on output quality [39, 7, 26, 40, 49, 44].\n5. **Deferral Rules**: Techniques such as Chow's rule and confidence-difference thresholding for determining when a smaller model should defer to a larger one [10, 22].\n\nIn conclusion, the paper aims to design new speculative cascading techniques, demonstrate their theoretical optimality and practical advantages, and validate the effectiveness of these techniques through experiments on language tasks.",
        "methodology": "The methodology for the proposed method, \"Faster Cascades via Speculative Decoding,\" centers on a strategic integration of cascades' high-quality results with the rapid performance of speculative decoding techniques. Let's break down the key components and innovations:\n\n1. **Principled Approach**: The proposal isn't just an arbitrary combination but relies on theoretical grounding and structured methodology to achieve the desired balance.\n\n2. **Cascades**: These are known for their superior quality in the context of the tasks they perform. Typically, they might take more time but produce highly reliable results.\n\n3. **Speculative Decoding**: This approach is characterized by its speed, offering faster execution times. The trade-off usually involves a reduction in the quality of the results compared to cascades.\n\n4. **Combining the Approaches**: The novelty lies in how the method combines these two approaches. It specifically aims to leverage the high-quality results from cascades while maintaining the quick performance guaranteed by speculative decoding methods.\n\nBy integrating cascades and speculative decoding, the proposed methodology seeks to overcome the limitations of each approach when used in isolation: improving execution speed without sacrificing the quality of the outcomes. The fusion of these methods is a significant innovation intended to harness the strengths of both processes effectively.",
        "main_experiment_and_results": "Main Experiment Setup and Results: To compare the strengths and weaknesses of cascades and speculative decoding, the experiment is set up to evaluate a token-level cascade design with distinct models of different sizes. The objective is to reduce the inference cost while maintaining performance quality.\n\n**Datasets:**\n- The main experiment does not specify the exact datasets used, but typical language model evaluation involves datasets like OpenAI's GPT evaluations, including tasks such as text generation, translation, summarization, etc.\n\n**Baselines:**\n- The baseline for this experiment consists of standard heavily-parameterized models evaluated in isolation without the integration of smaller models (i.e., normal single-pass decoding without speculative or cascade methods).\n\n**Evaluation Metrics:**\n- Inference Cost: Measures the computational resources (e.g., time, energy) needed to generate text.\n- Performance Quality: Includes metrics for language model outputs such as BLEU score (for translation), ROUGE score (for summarization), perplexity (for language modeling), etc.\n\n**Main Experimental Results:**\n- The results highlight the efficiency gains from using cascades and speculative decoding over traditional models. In particular, cascades can effectively reduce the inference cost by only using the larger model for more challenging tokens, while still maintaining the high quality of generated text.\n- Speculative decoding may show different trade-offs, potentially offering faster results for certain tasks where a single-pass approach suffices.\n- The comparative analysis demonstrates that token-level cascades can significantly decrease computational resources without compromising the overall performance, making them suitable for deployment in resource-constrained environments. \n\nThe findings underscore the potential of these methods to accelerate language model inference and make high-quality text generation more accessible."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To determine when token-level cascades of language models outperform speculative decoding by comparing their respective approaches to approximating output distributions.",
            "experiment_process": "The experiment involves token-level cascades and speculative decoding techniques for T5 models of different sizes, fine-tuned on WMT EN-DE translation and XSum tasks. The cascades are constructed using Chow\u2019s rule and an oracle Diff rule. Speculative decoding employs the smaller model as the drafter and the larger model as the verifier. Quality is plotted against the fraction of samples deferred to the larger model at varying cost parameters.",
            "result_discussion": "Token-level cascades exhibit better quality than the larger model across several operating points, with even Chow\u2019s rule outperforming speculative decoding in some instances. However, they require more calls to the larger model compared to speculative decoding, which verifies multiple tokens in parallel, resulting in fewer rejections of drafter tokens. The complementary strengths of both techniques prompt further exploration into their combined use.",
            "ablation_id": "2405.19261v1.No1"
        },
        {
            "research_objective": "To demonstrate that the proposed speculative cascading techniques provide better cost-quality trade-offs than sequential token-level cascades and standard speculative decoding.",
            "experiment_process": "Speculative cascades using three deferral rules (Chow, Diff, OPT) are tested on WMT EN-DE translation, CNN/Daily Mail summarization, and XSum datasets using T5-small and either T5-large or T5-XL models, fine-tuned for each task. BLEU scores evaluate translation tasks and ROUGE-2 scores evaluate summarization tasks. Latency is measured by wall-clock decoding time with batch size 1. Comparisons are made against sequence-level cascades, token-level cascades, lossy speculative decoding, and big-little decoder approaches under greedy decoding and temperature sampling settings.",
            "result_discussion": "Methods using speculative execution are considerably faster than sequential token-level cascades, particularly in non-low latency regimes. The OPT speculative cascading strategy generally provides the best quality for most latency values, with Diff being a close second. For temperature sampling and greedy decoding, the optimal SpecCascade [Diff] often achieves better quality trade-offs than other methods. Additionally, SpecCascade [OPT] yields the maximum speed-up and best quality metrics, while cascading approaches generally underperform in terms of both quality and latency, except in specific cases like WMT.",
            "ablation_id": "2405.19261v1.No2"
        }
    ]
}