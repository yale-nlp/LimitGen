{
    "title": "Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs",
    "abstract": "Multimodal LLMs are the natural evolution of LLMs, and enlarge their capabilities so as to work beyond the pure textual modality. As research is being carried out to design novel architectures and vision-and-language adapters, in this paper we concentrate on endowing such models with the capability of answering questions that require external knowledge. Our approach, termed Wiki-LLaVA, aims at integrating an external knowledge source of multimodal documents, which is accessed through a hierarchical retrieval pipeline. Relevant passages, using this approach, are retrieved from the external knowledge source and employed as additional context for the LLM, augmenting the effectiveness and precision of generated dialogues. We conduct extensive experiments on datasets tailored for visual question answering with external data and demonstrate the appropriateness of our approach. \u2020\u2020\u2217Equal contribution.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Recently, Large Language Models (LLMs) have demonstrated impressive performance in zero-shot textual tasks. Specifically, recent literature has devised models capable of tackling diverse tasks, as instructed by the user. In this context, the classical approach is that of fine-tuning a model on varied tasks that are described through natural language, thus empowering the model to assimilate externally provided instructions and facilitating robust generalization across multiple domains. Following these advancements, the computer vision community has started to investigate the extension of such models to vision-and-language contexts, thus generating Multimodal Large Language Models (MLLMs). On this line, the fusion of visual features into LLM backbones through vision-to-language adapters has induced notable performance improvements, enabling extensive generalization to vision-and-language tasks requiring elaborate visual descriptions.\n\nIn this context, MLLMs excel by simply including a small module (i.e., an adapter) that aligns visual features with textual ones. However, despite these models being built upon LLMs trained on large-scale data, they exhibit notable limitations when confronted with highly specific user queries or when a certain degree of compositional reasoning is required to formulate the response. Moreover, certain knowledge proves itself challenging to be encoded within the parameters of an MLLM, due to the scarcity of long-tail information in the training data.\n\nDriving from these considerations, in this paper we propose the first MLLM augmented with a retrieval module, thus shifting the focus towards teaching the model to leverage diverse information in its responses and learning to discern the relative importance of each. In particular, our model retrieves appropriate information from an external knowledge base of documents and employs a hierarchical retrieval approach to identify relevant passages. This additional knowledge is then fed to an MLLM, without changing its structure but improving its answering capabilities. To the best of our knowledge, our work represents the first MLLM to harness the retrieval capability of external sources. We assess the quality of the proposed approach by conducting extensive experiments and comparisons with respect to recent MLLMs and by showcasing the effectiveness of our design choices. Experimental results demonstrate the advantage of retrieving from external sources and the appropriateness of our model design. Overall, we conceive our work as a first step in the direction of retrieval-augmented MLLMs, which could foster future works in the same area."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Multimodal LLMs.\nLLMs have significantly reshaped the landscape of AI research and applications, spearheaded by notable examples like OpenAI\u2019s ChatGPT and GPT-4. These models leverage alignment techniques such as instruction tuning [30  ###reference_b30###] and reinforcement learning from human feedback [39  ###reference_b39###] and achieve remarkable capabilities in language understanding and reasoning. Open-source LLMs like Flan-T5 [7  ###reference_b7###], Vicuna [6  ###reference_b6###], LLaMA [41  ###reference_b41###], and Alpaca [40  ###reference_b40###] have further accelerated the advancement within the research community. This surge in the development of LLMs subsequently led to the emergence of MLLMs [3  ###reference_b3###], which can combine the understating of visual inputs with natural language generation.\nEarly attempts of building MLLMs such as VisualGPT [4  ###reference_b4###] and Frozen [42  ###reference_b42###] used pre-trained language models to enhance vision-and-language models specifically for tasks like image captioning and visual question answering. This initial investigation paved the way for subsequent research in this domain, with the introduction of solutions such as Flamingo [1  ###reference_b1###] or BLIP-2 [21  ###reference_b21###] which allowed the integration of image features into LLMs respectively through trainable cross-attention layers directly within the LLM or Q-Former blocks that instead combine image and textual features via learnable queries. Building upon these advancements, subsequent models like FROMAGe [19  ###reference_b19###], Kosmos-1 [14  ###reference_b14###], and MiniGPT-4 [48  ###reference_b48###] have been introduced to further refine the interplay between visual and language modalities within the LLM architecture.\nConcurrently, the LLaVA family of models [24  ###reference_b24###, 23  ###reference_b23###, 25  ###reference_b25###] introduced the usage of instruction tuning in the multimodal domain, by training on a curated dataset collected with GPT-4. This strategy is now among the most promising recipes for building MLLMs.\nRetrieval-augmented language models.\nIn recent years, retrieval-augmentation has been applied to language models by expanding their input space with relevant text passages extracted from external sources [10  ###reference_b10###] or eventually retrieved directly from the web [29  ###reference_b29###]. These techniques have demonstrated large\nimprovements in knowledge-intensive tasks and significant savings in terms of model size.\nTraditionally, the integration of external knowledge into textual generation has been confined to the initial stages. Different solutions [17  ###reference_b17###] proposed to adaptively retrieve passages for generation on top of a proprietary LLM. Some works [10  ###reference_b10###], instead, focused on capturing knowledge in a more modular and interpretable way, by augmenting the language model pre-training with a latent knowledge retriever. This\nallows the model to retrieve and attend documents taken from a large corpus such as Wikipedia.\nWhile much attention has been directed towards textual augmentation, similar research efforts have recently been dedicated in the context of vision-and-language tasks [2  ###reference_b2###, 37  ###reference_b37###, 13  ###reference_b13###, 31  ###reference_b31###]. Following this direction, the work presented in [13  ###reference_b13###] proposed a retrieval-augmented visual-language model that encodes world knowledge into a large-scale memory. Other approaches [36  ###reference_b36###, 35  ###reference_b35###] also apply retrieval to specific downstream tasks such as image captioning. Differently from all the aforementioned approaches, our work is the first to apply retrieval-augmentation to MLLMs. We do this by applying a hierarchical retrieval strategy on top of a knowledge base made of multimodal documents.\n###figure_2### Knowledge-based visual question answering.\nRecently, the emergence of new benchmarks like Encyclopedic-VQA [28  ###reference_b28###] and InfoSeek [5  ###reference_b5###] has raised the difficulty of standard knowledge-based VQA [27  ###reference_b27###, 16  ###reference_b16###, 38  ###reference_b38###] with questions that require intensive knowledge about specific entities, such that even LLM-based models perform poorly without retrieving information from external sources. Often, contrastive image-text encoders are employed to retrieve the target entity given the query image [44  ###reference_b44###, 46  ###reference_b46###]. Then, the entity name is used as a key to access an external knowledge base, which is typically composed of several text passages that encompass the correct answer. In this work, we design a hierarchical retrieval scheme based on CLIP [33  ###reference_b33###] and the Contriever model [15  ###reference_b15###] to extrapolate relevant passages, and we feed them to an MLLM to help the answer generation."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Proposed Method",
            "text": "Our goal is to equip Multimodal LLMs (MLLMs) with the ability to answer complex and specific questions that cannot be addressed solely through the image content and pre-trained knowledge. To achieve this, we propose Wiki-LLaVA, which integrates external knowledge derived from an external memory into the LLaVA model, without significantly altering its design. Instead, we augment the capabilities of the model by incorporating retrieval information as additional input context.\nOverall, Wiki-LLaVA comprises three components, as shown in Fig. 2  ###reference_###: a visual encoder, which is employed to provide the MLLM with visual context and as a query to retrieve from an external knowledge base, the knowledge base itself (e.g., Wikipedia), and a hierarchical retrieval module which retrieves relevant documents and passages from the external knowledge base, to be employed as additional context for the MLLM."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Knowledge-based Augmentation",
            "text": "Multimodal integration and autoregressive generation.\nAn MLLM usually takes as input a multimodal input query, comprising both image and text, and generates a textual output in an autoregressive manner. Formally, the architecture is trained to model a probability distribution , where  denotes the parameters of the model,  represents an input image, and  denotes the textual prompt. The textual prompt usually includes a pre-defined system-level prompt and a question related to the input image, given by the user. Clearly, a standard MLLM can only rely on the user prompt, the input image, and the knowledge stored in its internal parameters (i.e., ) to accommodate requests, thus limiting its ability to answer questions that rely on external knowledge.\nIn the rest of the paper, we employ LLaVA [24  ###reference_b24###] as our reference MLLM. LLaVA exploits the capabilities of a pre-trained LLM (i.e., Vicuna [6  ###reference_b6###]) and a pre-trained visual model (i.e., a CLIP-based visual encoder [33  ###reference_b33###]), which are interconnected through an MLP adapter, in charge of converting CLIP features to dense input tokens. For an input image , therefore, LLaVA utilizes a pre-trained CLIP visual encoder , extracts a dense grid of visual features , which is then projected via a learnable MLP to produce a sequence of dense embedding tokens . Finally, these are prepended to the system prompt, and the full sequence of visual and textual tokens is then given as input to the LLM component of the model.\nAugmentation with external knowledge.\nTo augment the MLLM with external knowledge, we enrich the input context by injecting relevant textual data from an external memory composed of documents. Formally, the distribution of the MLLM is conditioned on additional textual retrieval-knowledge tokens, leading to\nwhere  represents the added tokens retrieved from the external memory. Differently from the standard formulation of MLLMs, by enriching the input context we allow the model to generate more specific answers by exploiting tokens retrieved from the memory.\nHierarchical retrieval from an external memory.\nThe external memory comprises a collection of (document, image, text-title) triplets taken from documents, denoted as . Within this memory, we conduct a hierarchical two-step search to retrieve appropriate information. Initially, we locate the most pertinent document, followed by identifying the relevant passage inside a particular document, which is subsequently exploited as additional input context in the MLLM.\nIn the first stage, given an input query image  we perform an approximate -nearest neighbor search into the external memory, using document titles as retrievable keys. The similarity between the query image and the text titles is modeled as the inner product between their respective embeddings, which are computed through the visual and textual CLIP encoders (i.e.,  and ), as follows:\nThen, the knowledge retriever returns the top- documents associated with the most relevant items retrieved using the aforementioned procedure.\nRetrieving document passages.\nIn the second step, we analyze each of the retrieved documents to identify the most relevant passages corresponding to the user\u2019s question.\nEach document is defined as a sequence of chunks, denoted as , and, given the input question, we retrieve the chunks with the highest similarity to the question. We employ the Contriever architecture [15  ###reference_b15###] to embed each chunk of the selected document, along with the query (i.e., the question provided by the user), and compute the similarity as an inner product between embeddings. By retrieving the  most appropriate passages inside each of the retrieved documents, overall we obtain  passages.\nContext enrichment.\nOnce we find the most relevant chunks, we employ their raw contents as an additional input to the MLLM. Specifically, the final prompt that we employ includes the image tokens, the retrieved raw chunks, the system-level prompt, and the user question. Formally, considering three retrieved passages, the final prompt is defined as follows:"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Training",
            "text": "While the aforementioned approach could work in a zero-shot fashion, using the original weights  of the pre-trained MLLM, we also investigate the case of fine-tuning the model to augment its capabilities of exploiting retrieved passages. In particular, in this case, the model is trained on pairs of questions and ground-truth answers requiring external knowledge. As this would potentially reduce the capabilities of the MLLM on tasks not requiring external knowledge (i.e., all the other tasks on which the model has been originally trained), we apply a data mixing approach in which ground-truth pairs requiring external knowledge are mixed with ground-truth pairs not requiring external knowledge in the same mini-batch."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this section, we introduce the experimental settings, describing the datasets employed, the evaluation protocol, and the implementation and training details used to perform the experiments. Then, we present our experimental results, analyzing the effectiveness of CLIP fine-tuning and evaluating how it is possible to incorporate retrieved knowledge in an MLLM. Finally, limitations of the proposed approach and possible future works are reported."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Datasets",
            "text": "InfoSeek [5  ###reference_b5###]. The dataset contains 1.3M image-question-answer triplets corresponding to around 11k different entities (i.e., Wikipedia articles). The vast majority of questions have been obtained with an almost entirely automatic procedure, by filling human-authored templates with knowledge triples from Wikidata. In this case, images are derived from the OVEN dataset [12  ###reference_b12###]. Triplets are divided into training, validation, and test sets, with around 934k, 73k, and 348k samples respectively. At the time of the submission, the ground-truth answers and entities from the test set were not available. Therefore, we report our results on the validation split. Both validation and test sets contain questions related to new entities not included in the training split and questions not seen during training.\nAlong with image-question-answer triplets, a knowledge base composed of 6M Wikipedia entities is provided. In our experiments, we consider a randomly extracted subset of 100k entities, in which we guarantee the presence of the 6,741 entities associated with questions from the training and validation splits."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Implementation Details",
            "text": "LLaVA fine-tuning. We employ two distinct fine-tuning approaches, with each being exclusively applied to one of the datasets. In order to maintain the performance of the LLaVA model on well-established MLLM datasets, we supplement fine-tuning data with samples from the LLaVA-Instruct dataset. Specifically, given its size of 158k, we double the probability of having examples from this dataset in each mini-batch. To reduce the number of trainable parameters, we train using low-rank adapters with a total batch size of 512 samples.\n\nRetrieval. Textual documents sourced from Wikipedia content are embedded using the Contriever architecture, segmenting the text into chunks of 600 characters each. Furthermore, for streamlined efficiency, the process involves utilizing a single visual encoder. Specifically, following the LLaVA architecture, we employ the CLIP ViT-L/14@336 backbone to embed images to give as input to the MLLM, while simultaneously leveraging it to extract query visual features in the initial hierarchical retrieval step, facilitating the integration of an external memory component. To perform entity retrieval, we employ approximate kNN search rather than exact kNN search because it significantly improves the computational speed of the entire pipeline. To this aim, we employ the Faiss library and a graph-based HNSW index with 32 links per vertex."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Evaluation Protocol",
            "text": "We evaluate our models in two settings: without external knowledge base and with external knowledge base. The former means that we ask the model to directly answer a visual question, by solely relying on the competencies learned during pre-training and/or fine-tuning. On the other hand, in the latter setting, we leverage the proposed hierarchical retrieval method to search for additional information in the external knowledge base. In practice, this is represented by two dumps of Wikipedia comprehending 2M and 100k pages, respectively for our experiments. Concerning the evaluation metrics, we report the accuracy over the test split and the validation split, following the official evaluation scripts provided along with the datasets."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Experimental Results",
            "text": "Analyzing CLIP performance. We start by evaluating entity retrieval results using CLIP. In this setting, we consider images from the InfoSeek validation set and measure the CLIP ability to find the correct entity within the knowledge base of 100k entries. We perform retrieval using images as queries and Wikipedia titles as retrievable items. \n\nResults are reported in Table 1 in terms of recall@ (R@), which measures the percentage of times the correct entity is found in the top-retrieved elements. Correctly retrieving the Wikipedia entity associated with the input image strongly depends on the size of the employed knowledge base. When using 100k items, as in the case of InfoSeek, the correct entity is retrieved as the first item 36.9% of the time and among the top-10 66.1% of the time.\n\nWe then report visual question-answering results in Table 2. We include the performance of zero-shot models like BLIP-2, InstructBLIP, and the LLaVA-1.5 baseline model, which are not fine-tuned on the considered datasets and that do not leverage the external knowledge base. Moreover, we consider the accuracy results of LLaVA-1.5 when fine-tuned on the training set of InfoSeek, but not augmented with retrieved context. The results of our approach are reported both in the standard setting in which CLIP is used to retrieve the most representative entity from the knowledge base and in its oracle version, which employs the entity corresponding to the input image-question pair.\n\nAs it can be seen, zero-shot MLLMs face difficulties in correctly answering the given questions as these models can only rely on the knowledge embedded inside the LLM. When instead using an external knowledge base, the accuracy results significantly increase especially on the InfoSeek dataset with 100k retrievable items. The limited performance of the CLIP model in retrieving the correct entity on larger knowledge bases leads to a slight degradation of accuracy scores. This is due to the noisy textual passages that are provided to the MLLM as additional external context which, being related to a different entity, often do not contain informative content.\n\nOverall, retrieving passages from different entities does not always help increase the results. Instead, using more than one textual chunk as additional context for the MLLM generally improves the final accuracy on the InfoSeek validation set with an overall improvement of 2.1 and 3.4 accuracy points respectively. Furthermore, it is worth noting that employing oracle entities significantly boosts the final accuracy.\n\nEvaluating the importance of the fine-tuning datasets. As described in Sec. 3.2 and Sec. 4.2, the MLLM fine-tuning is done with a mixture of data containing image-question-answer triples from the InfoSeek training set and visual instruction tuning data from LLaVA-Instruct, which has been used to originally fine-tune the LLaVA model. In Table 3, we evaluate the effect of mixing fine-tuning data for the knowledge-based VQA task. In this setting, we only report the results of the fine-tuned models without external knowledge retrieval. Notably, using visual instruction tuning data can help to regularize the fine-tuning phase on the InfoSeek dataset, leading to an overall improvement of 1.9 accuracy points compared to the model fine-tuned only on image-question-answer triplets from the training set of the dataset.\n\nPreservation of LLaVA performance. Finally, we analyze the impact of LLaVA fine-tuning on knowledge-based VQA datasets when evaluating the model on common MLLM evaluation benchmarks. In particular, we include results on MME, MMMU, MMBench (MMB), and POPE benchmarks. More details about the evaluation metrics and number of samples can be found in the original paper of each dataset.\n\nResults are shown in Table 4 comparing the original LLaVA model with the two fine-tuned versions on InfoSeek, with and without the use of visual instruction tuning data. Overall, employing samples from the LLaVA-Instruct dataset can better preserve the results of the original model, only partially degrading the performance on the considered benchmarks compared to the original model. While the most significant deterioration is achieved on the MME dataset, in the other settings the original performances are better preserved, also leading to a slight improvement on MMMU and POPE benchmarks compared to the LLaVA-1.5 results."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Limitations and Future Works",
            "text": "While our work provides an initial step towards MLLM which can properly exploit external multimodal data, it is worthwhile mentioning that significant research is needed in two directions. The first is defining proper embedding spaces in which documents can be retrieved from questions and input images, so as to improve the performance of the higher level of our hierarchical retrieval. The second is modeling an efficient and sustainable paradigm to select from one or more documents. Here, the challenge is to increase the capability of the MLLM of distinguishing the appropriateness of retrieved items. This point might also require novel architectural design, which might go beyond the pure inclusion of retrieved items in the context.\n\nRegardless of its current limitations, our research testifies the potential of adding multimodal external knowledge to a MLLM and inherits all the advantages of retrieval-augmented approaches, such as the adaptability to different domains and the loosely-coupled relationship between pre-trained information and retrievable data."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We have presented Wiki-LLaVA, an architecture for augmenting an existing MLLM with external knowledge. Our proposal leverages an external knowledge source of documents to improve the effectiveness of an MLLM when tasked with questions and dialogues. In particular, we devise a hierarchical architecture for retrieving documents and eliciting selected parts to be included in the MLLM input context. Extensive experiments demonstrate the effectiveness of the proposed solution, and its capability to maintain the proficiency of the MLLM across different tasks."
        }
    ],
    "url": "http://arxiv.org/html/2404.15406v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.2",
            "4",
            "4.2",
            "4.3",
            "4.4",
            "4.5"
        ]
    },
    "research_context": {
        "paper_id": "2404.15406v2",
        "paper_title": "Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs",
        "research_background": "### Motivation\nThe motivation for this paper arises from the limitations of current Multimodal Large Language Models (MLLMs), particularly when addressing highly specific user queries or tasks requiring compositional reasoning. Despite significant advancements in creating MLLMs that fuse visual and textual information, these models still struggle with encoding infrequent or long-tail knowledge within their parameters due to the limitations in training data. The paper aims to address these limitations by proposing a model that can effectively leverage external knowledge sources to enhance its response capabilities.\n\n### Research Problem\nThe core research problem addressed in this paper is the enhancement of MLLMs' answering capabilities by integrating external knowledge, specifically through a retrieval-augmented approach. Current MLLMs, while successful to an extent, face difficulties with:\n1. Handling highly specific queries.\n2. Performing compositional reasoning to formulate responses.\n3. Encapsulating long-tail or infrequent knowledge within their training data.\n\n### Relevant Prior Work\nThe paper acknowledges several areas of prior work:\n1. **Large Language Models (LLMs):** References [30, 6, 41, 34, 7] discuss models that handle diverse tasks as instructed by users, often relying on fine-tuning for robust generalization.\n2. **Multimodal Large Language Models (MLLMs):** Prior art [23, 21, 1, 48] has explored incorporating vision-to-language adapters to align visual and textual features, achieving substantial performance improvements in visual description tasks.\n3. **Benchmarks:** Benchmarks such as InfoSeek [5] and Encyclopedic-VQA [28] have been introduced to evaluate MLLMs on their ability to handle queries involving external data. Existing works [32, 20, 8, 21] underscore the importance of this capability but have not specifically designed architectures for incorporating external knowledge.\n4. **Challenges:** Previous efforts have shown that while current MLLMs can perform well in general contexts, they are limited by the inability to encode specialized knowledge, particularly due to data sparsity and the intrinsic limitations of encoding every piece of knowledge within model parameters.\n\nIn summary, the paper seeks to advance the state-of-the-art in MLLMs by proposing a novel retrieval-augmented model architecture, which retrieves and integrates external knowledge to improve the quality of responses. This bridges the gap identified in prior work, emphasizing the need for models specifically designed to leverage external information sources.",
        "methodology": "The proposed method, Wiki-LLaVA, aims to enhance Multimodal Large Language Models (MLLMs) with the ability to answer complex and specific questions that extend beyond the information contained within an image and the model's pre-trained knowledge. This enhancement is achieved by integrating external knowledge from an external memory source into the LLaVA model without significantly modifying its original architecture.\n\nKey components and innovations of Wiki-LLaVA include:\n\n1. **Visual Encoder**: This component is responsible for providing the MLLM with visual context from an input image. Additionally, it functions as a query generator for retrieving relevant information from an external knowledge base.\n\n2. **Knowledge Base**: The system utilizes a substantial external knowledge base, such as Wikipedia, to supply detailed and expansive information that the MLLM can draw upon for generating responses.\n\n3. **Hierarchical Retrieval Module**: This innovative module plays a crucial role in the method by retrieving pertinent documents and passages from the external knowledge base. The retrieved information is then used as additional context to inform the MLLM's responses.\n\nBy incorporating these components, Wiki-LLaVA effectively augments the MLLM's abilities, enabling it to deliver more comprehensive and accurate answers by leveraging both visual content and external textual knowledge.",
        "main_experiment_and_results": "The main experiment setup of the paper involves several key components:\n\n**Datasets:**\n1. **MSCOCO:** A large-scale dataset for image captioning and other vision-language tasks.\n2. **NoCaps:** A dataset used to evaluate the generalization of image captioning models to novel objects.\n3. **Flickr30k:** A dataset primarily used for image-captioning tasks, with a variety of visual and textual content.\n\n**Baselines:**\n1. **CLIP:** A model that learns visual concepts from natural language descriptions.\n2. **LLaVA:** A multimodal language model designed to integrate language and vision, used without retrieval augmentation as a baseline.\n\n**Evaluation Metrics:**\n1. **BLEU:** A metric to evaluate the quality of text that has been machine-translated from one language to another, widely used in image captioning for sentence matching.\n2. **METEOR:** Another metric for evaluating machine translation, considering synonyms, stemming, and paraphrasing.\n3. **CIDEr:** Specifically designed for evaluating image captioning, focusing on consensus and precision recall.\n\n**Implementation and Training Details:**\n- **CLIP Fine-Tuning:** Fine-tuning the CLIP model with the datasets mentioned for better alignment with the retrieval tasks.\n- **Incorporating Retrieved Knowledge:** Utilizing a hierarchical retrieval-augmented generation approach to enhance the LLaVA model's performance by integrating external knowledge effectively.\n\n**Main Experimental Results:**\n- The effectiveness of CLIP fine-tuning shows a notable improvement across the board, yielding higher BLEU, METEOR, and CIDEr scores compared to the non-fine-tuned versions.\n- Incorporating retrieved knowledge into the MLLM via the hierarchical retrieval-augmented generation approach demonstrates a significant boost in performance on image captioning tasks.\n- The proposed method outperforms the baselines on all datasets, indicating the success of integrating external knowledge into multimodal language models.\n\n**Analysis:**\n- The results highlight the importance of retrieval-augmented techniques in enhancing the capabilities of multimodal language models.\n- The proposed approach demonstrates robustness and adaptability across different datasets, showcasing improved language and vision integration.\n\nIn summary, the experiments show that the proposed hierarchical retrieval-augmented generation strategy significantly enhances the performance of multimodal language models on various evaluation metrics, outperforming established baselines. Limitations and potential future directions are also discussed, focusing on areas for further improvement and broader applicability."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To investigate the impact of fine-tuning the MLLM on its capability of exploiting retrieved passages, and to ensure that it maintains its performance on tasks not requiring external knowledge by applying a data mixing approach.",
            "experiment_process": "The model is trained on pairs of questions and ground-truth answers requiring external knowledge. Ground-truth pairs requiring external knowledge are mixed with ground-truth pairs not requiring external knowledge in the same mini-batch to preserve performance on original tasks.",
            "result_discussion": "Fine-tuning helps the model better exploit retrieved passages for answering questions requiring external knowledge, while the data mixing approach aims to maintain the model's original performance on tasks not requiring external knowledge.",
            "ablation_id": "2404.15406v2.No1"
        },
        {
            "research_objective": "To evaluate the effect of different datasets and methods for incorporating external knowledge through hierarchical retrieval on the performance of the MLLM.",
            "experiment_process": "The models are evaluated with and without an external knowledge base. Wikipedia dumps of 2M and 100k pages are used respectively for Encyclopedic-VQA and InfoSeek datasets. Accuracy is reported over the test split of Encyclopedic-VQA and the validation split of InfoSeek. Entity retrieval results using CLIP and MLLM performance with varied numbers of textual chunks as additional context are analyzed.",
            "result_discussion": "The use of an external knowledge base significantly increases accuracy, especially on the InfoSeek dataset with smaller knowledge bases. However, using larger knowledge bases can introduce noise, slightly degrading performance. Employing oracle entities boosts accuracy, particularly highlighting the importance of good entity retrieval models to provide relevant content.",
            "ablation_id": "2404.15406v2.No2"
        },
        {
            "research_objective": "To evaluate the importance of mixing fine-tuning datasets for the knowledge-based VQA task on the performance of the MLLM.",
            "experiment_process": "The MLLM is fine-tuned with a mixture of data containing image-question-answer triples from Encyclopedic-VQA or InfoSeek training set and visual instruction tuning data from LLaVA-Instruct. The impact of this mixing on performance is evaluated without external knowledge retrieval.",
            "result_discussion": "Using visual instruction tuning data helps improve the fine-tuning phase on the InfoSeek dataset, leading to a gain of 1.9 accuracy points. For the Encyclopedic-VQA, this does not improve performance but does not degrade it either.",
            "ablation_id": "2404.15406v2.No3"
        },
        {
            "research_objective": "To assess the impact of fine-tuning the MLLM on its performance in common MLLM evaluation benchmarks, with and without visual instruction tuning data.",
            "experiment_process": "The fine-tuned model is evaluated on the MME, MMMU, MMBench, and POPE datasets. The results are compared to the original LLaVA model to analyze performance preservation across a range of tasks.",
            "result_discussion": "Employing samples from the LLaVA-Instruct dataset better preserves original performance while fine-tuning. The most significant degradation occurs in the MME dataset, but other benchmarks show preserved or slightly improved results compared to the original model.",
            "ablation_id": "2404.15406v2.No4"
        }
    ]
}