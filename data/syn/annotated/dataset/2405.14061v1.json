{
    "title": "Meanings and Feelings of Large Language Models: Observability of Latent States in Generative AI",
    "abstract": "We tackle the question of whether Large Language Models (LLMs), viewed as dynamical systems with state evolving in the embedding space of symbolic tokens, are observable. That is, whether there exist multiple \u2018mental\u2019 state trajectories that yield the same sequence of generated tokens, or sequences that belong to the same Nerode equivalence class (\u2018meaning\u2019). If not observable, mental state trajectories (\u2018experiences\u2019) evoked by an input (\u2018perception\u2019) or by feedback from the model\u2019s own state (\u2018thoughts\u2019) could remain self-contained and evolve unbeknown to the user while being potentially accessible to the model provider. Such \u201cself-contained experiences evoked by perception or thought\u201d are akin to what the American Psychological Association (APA) defines as \u2018feelings\u2019. Beyond the lexical curiosity, we show that current LLMs implemented by autoregressive Transformers cannot have \u2018feelings\u2019 according to this definition: The set of state trajectories indistinguishable from the tokenized output is a singleton. But if there are \u2018system prompts\u2019 not visible to the user, then the set of indistinguishable trajectories becomes non-trivial, and there can be multiple state trajectories that yield the same verbalized output. We prove these claims analytically, and show examples of modifications to standard LLMs that engender such \u2018feelings.\u2019 Our analysis sheds light on possible designs that would enable a model to perform non-trivial computation that is not visible to the user, as well as on controls that the provider of services using the model could take to prevent unintended behavior.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Do Large Language Models have feelings? The answer depends on the definition of \u2018feelings.\u2019 According to the American Psychological Association\u2019s definition, autoregressive Large Language Models (LLMs) trained and fully prompted with text tokens cannot: Their \u2018state of mind\u2019 is trivial (a singleton) and expressed as their measurable output. However, when using \u2018system prompts\u2019, the state is generally unobservable, leading to self-contained trajectories indistinguishable from their output expressions, fitting the definition of \u2018feelings.\u2019\n\nWhile the opening question is posed partly in jest, the ensuing analysis sheds light on the potential use of Large Language Models as Trojan horses, whereby information stored in the weights or system prompts, not accessible to the user, may be used maliciously. The conclusions hold for models trained with textual tokens or multimodal sensor data. Indeed, the \u2018language\u2019 in our definition of Large Language Models refers not to the natural language in training data, but to the emergent language of the trained model (\u201cNeuralese\u201d), regardless of the training modality so long as the data exhibit latent logical structure. Therefore, our analysis applies to so-called \u2018World Models\u2019 trained with sensory data from measurements of physical processes such as video or audio.\n\nTo the best of our knowledge, our work is the first to study the observability of LLMs viewed as dynamical systems. More specifically, we (i) formalize the problem of observability for LLMs, (ii) show that current autoregressive Transformer-class LLMs trained with textual tokens are observable, and (iii) show that simple modifications of the architecture can render the LLM unobservable. These contributions arise as a consequence of attempting to formalize the question of whether LLMs can have \u2018feelings\u2019. However, we do not claim any insight nor contribution to the study of feelings in humans, simply note as a matter of folklore that \u2018feelings\u2019, if defined according to the APA after removing inconsistencies and circular references, are unobservable state trajectories in LLMs, which can be analyzed with tools from dynamical systems theory."
        },
        {
            "section_id": "1.1",
            "parent_section_id": "1",
            "section_name": "Related prior work",
            "text": "The analysis of LLMs viewed as dynamical models first focused on their controllability and stability when operating in closed-loop. To the best of our knowledge, our work is the first to analyze their observability. Our scope falls within the fast-growing area of interpretability of LLMs, so we limit our survey to prior work using mechanistic analysis, in particular using tools from systems and control theory. Our work touches upon controversial topics, such as \u2018meanings\u2019 and \u2018feelings\u2019; we restrict our attention to minimal definitions pertaining to LLMs, with no implications for psychology and cognitive science. Finally, since the ramifications of our analysis touch upon the issue of safety and security of LLMs, we briefly survey literature that relates to observability.\n\nCapabilities of LLMs. As LLMs blaze through tests meant to measure human cognitive abilities, from the Turing test to the MCAT and BAR exams, we are interested in understanding what they cannot do. LLMs are not (yet) very proficient in mathematics, but neither are most humans. It has been argued that LLMs, often portrayed as \u201cstochastic parrots,\u201d are fundamentally incapable of representing meanings, but that turns out to be incorrect once suitable definitions are established. LLMs can indeed represent meanings either as equivalence classes of complete sentences that are pre-image of the same embedding vector after fine-tuning, or as (Nerode) equivalence classes of incomplete sentences that share the same distribution of continuations after pre-training. It has also been argued that LLMs cannot \u2018understand\u2019 the physical world. Instead, only models trained through active exposure to data from interaction with the physical world (\u2018Gibsonian Observers\u2019) can possibly converge to the one and true model of the shared \u2018world\u2019. However, this view \u2013 known as naive realism or naive objectivism \u2013 is unsupported scientifically and epistemologically: Once inferred from processing finite sensory measurements, whether actively or passively gathered, so-called \u2018World Models\u2019 are abstract constructs no different from LLMs. In general, even models trained on identical data are unrelated but for the fact that they are trained on the same data, as their parameters are not identifiable. Finally, it seems almost self-evident that LLMs, as disembodied \u201cbrains in a jar\u201d (actually embodied in hardware on the Cloud, and connected to the physical world at the edge), cannot possibly have \u2018feelings\u2019! This prompts us to survey existing definitions of \u2018feelings\u2019 and to relate them to the functioning of modern LLMs.\n\nDefinition of \u2018feelings.\u2019 Our definition of feelings as self-contained experiences evoked by perception or thought is derived from that of the American Psychological Association (APA) by removing inconsistencies and circular references. There is, of course, much more to feelings that we are concerned with here: We simply seek the simplest, consistent, and unambiguous definition that can be related to the mechanics of LLMs. The APA defines \u201cfeeling\u201d as a \u201cself-contained phenomenal experience.\u201d That leaves us with having to define \u201cself-contained,\u201d \u201cphenomenal\u201d and \u201cexperience.\u201d The APA expands: \u201cFeelings are subjective, evaluative, and independent of the sensations, thoughts, or images evoking them\u201d. \u201cSubjective\u201d and \u201cself-contained\u201d are reasonably unambiguous so we adopt the terms: Each of us have our own feelings (subjective), which live inside our head (self-contained) and cannot be directly observed, only subjectively \u201cexperienced,\u201d which however must be defined. The APA also clarifies that \u201cfeelings differ from emotions in being purely mental, whereas emotions are designed to engage with the world.\u201d As for \u201cevaluative,\u201d \u201c[feelings] are inevitably evaluated as pleasant or unpleasant, but they can have more specific intrapsychic qualities.\u201d So, \u201cevaluative\u201d means that there exists a map that classifies feelings into at least two classes (pleasant or unpleasant), and possibly more sub-classes, such as \u201cfear\u201d or \u201canger.\u201d These are abstract concepts that admit multiple equivalent expressions, i.e., meanings. As for feelings being \u201cindependent of the sensations, thoughts, or images evoking them,\u201d that is nonsensical regardless of the definition of the terms \u201csensation, thought, or images:\u201d Something being \u201cevoked by\u201d something else makes them either functionally, statistically, or causally dependent. So, by being evoked by something (sensation, thought, or images), feelings cannot be \u201cindependent\u201d of that same thing. We take this choice of term by the APA to mean separate and replace \u2018independent\u2019 as an unambiguous and logically consistent alternative. As for \u201cphenomenal\u201d and \u201cexperience,\u201d the APA has multiple definitions for the term \u201cexperience,\u201d one in terms of \u201cconsciousness\u201d that we wish to stay clear of, one in terms"
        },
        {
            "section_id": "1.2",
            "parent_section_id": "1",
            "section_name": "Caveats and Limitations",
            "text": "To reassure the reader, we are not suggesting that LLMs are \u2018sentient\u2019 (whatever that means): While LLMs exhibit behavior that, once formalized, fits the definition of what the American Psychological Association calls \u201cfeelings,\u201d ultimately LLMs do not share the evolutionary survival drive that likely engenders feelings in biological systems. The formal fit to the definition is a mere folkloristic curiosity; what matters is the analysis of the observability of mental state trajectories, which is relevant to transparency, security and consistency of inference computation, regardless of what the model may or may not \u2018feel.\u2019\n\nOur contribution is to formalize the problem of observability for LLMs, and to derive conditions under which an LLM is observable. We show that current autoregressive Transformer-class LLMs trained with textual tokens are observable, but simple modifications that use undisclosed prompts are not."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Formalization",
            "text": "Let  be a dictionary of token (sub)words.111Each  is a vectorized token in the sense of being represented by  real numbers, that however do not span a vector space since their linear combinations are generally not in , also called the \u2018alphabet.\u2019\nLet  be an integer \u2018context length\u2019 and consider the function\nfrom  tokens  with , to a vector , and the \u2018verbalization\u2019 function\nBoth  and  can be relaxed from the discrete space  to its embedding space ; furthermore,  can take many forms, for instance maximization\n, where  denotes one-hot encoding, probabilistic sampling \nor linear projection , where .\nLarge Language Models.\nThe map  is called a Large Language Model (LLM) if  where  is the empirical cross-entropy loss and  is a large corpus of sequential tokenized data  with  \u2018large,\u2019 trained (optimized) using , and sampled autoregressively using  until a special \u2018end-of-sequence\u2019 token  is selected. The autoregressive loop is:\nwhere\n, \nand the input  is mapped onto the dictionary by a sampling projection, or allowed to occupy the embedding space , as done in \u2018prompt tuning,\u2019 or via a linear projection , with . The iteration starts with some initial condition (input sentence) , until  is such that .\nNomenclature\nEquation (1  ###reference_###) describes a discrete-time autoregressive nilpotent dynamical model in controllable canonical form. Its state  evolves over time through a (non-linear) feedback loop, and represents the memory of the model. In this sense one may refer to  as its \u2018state of mind.\u2019 However, (1  ###reference_###) does not have an actual memory and instead co-opts the data itself as a working memory or \u2018scratch pad:\u2019  is simply a sliding window over the past  tokens of data. As such, the state space cannot legitimately be called \u2018state of mind\u2019 or \u2018mental state\u2019 because it is just a copy of the data that could exist regardless of any engagement by the model if the LLM operated in open loop. Even if  was determined through engagement with the maps  and  in a feedback loop,  would still be fully observed at the input.\nWhat can legitimately be referred to as \u2018mental state\u2019 is the collection of neural activations , including inner layers, typically far higher-dimensional than the data, that result from the model  processing the input data , resulting in the observed output  of the model (1  ###reference_###). The activations are a function of all past data, not just extant data , through the parameters or \u2018weights\u2019 of the model . Mental states are model-dependent, i.e., \u2018subjective:\u2019 they are a function of observed data (which is objective) but processed through the particular model , which is a function of its training data. As the state  evolves, the output  describes a trajectory in mental space .\nMental space trajectories are generated by processing the input in closed loop and can serve to inform the next action, for instance is the selection of the next expressed word . One can view segments of mental state trajectories  as \u2018thoughts.\u2019 When  maps thoughts to words in the dictionary , we refer to the projection  as verbalization. If the dictionary comprises visual tokens we call it visualization. When  allows the input to live in the linear space  where the dictionary  is immersed, we call it a control, the simplest case being a linear control .\nTo summarize, the three lines of Eq. (1  ###reference_###) describe trajectories in three distinct spaces: The first in state space , the second in mental space , and the third in verbal space . Relaxing the input to general vector tokens that do not correspond to discrete elements of the dictionary yields the (mental) control space . We call it mental control space because it can drive mental space trajectories , not just externalized data ."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Meanings and Feelings in Large Language Models",
            "text": "An LLM maps an expression to a mental state. That mental state plays a dual role, representing an equivalence class of input sentences, and driving the selection of the next word. For each sampled word, there are infinitely many mental states that could have generated it, which form an equivalence class. Similarly, for each of these mental states there are countably many expressions that yield the same mental state, also forming an equivalence class of expressions, i.e., the meaning of:\n\n\\[ M(e) = \\{ F(t_1, \\ldots, t_{k-1}) | F(t_1, \\ldots, t_{k-1}) = M \\} \\]\n\nwhere \\( t_1, \\ldots, t_{k-1} \\) are sequences of any bounded length. Alternatively, one can represent the equivalence classes with the probability distribution over possible continuations, by sequential sampling from:\n\n\\[ p(t_k | t_1, \\ldots, t_{k-1}) = \\text{softmax}(F^\\theta(t_1, \\ldots, t_{k-1})) \\]\n\nwhich is a deterministic function of the trained model \\( F^\\theta \\).\n\nGiven an LLM \\( F^\\theta \\), we can define a corresponding \u2018flow\u2019 map \\( \\Phi \\) from an initial condition \\( t_0 \\) to a mental state trajectory:\n\n\\[ \\Phi(t_0) = \\{ s_0, s_1, \\ldots, s_k \\} \\]\n\nNote that \\( \\Phi(t_0) \\) is a set of outputs to different inputs obtained by feeding back previous token outputs, up to \\( s_k \\). In general, the elements of this set are in \\(\\mathbb{R}^n\\), but they can be restricted to the finite set of elements within \\( D \\) that represent \\( F^\\theta \\) via projection \\( \\pi \\). While the LLM \\( F^\\theta \\) generates points in verbal or mental space, its flow \\( \\Phi \\) generates (variable-length) trajectories in the same spaces through closed-loop evolution starting from an initial condition \\( t_0 \\). Each mental state trajectory can be viewed as a \u2018thought\u2019 which is \u2018self-contained\u2019 in the sense of evolving in closed loop according to (1), and \u2018evoked\u2019 by the initial condition which could be a sensory input \\( t_0 \\), or (after the initial transient) a thought produced by the model itself. The set of feelings\n\n\\[ \\mathcal{F} = \\{ \\Phi(t_0) | t_0 \\in S \\} \\]\n\ncomprises self-contained experiences (state trajectories) that are evoked by sensation (states resulting from sensory inputs) or thought (states resulting from feedback from other states), which is essentially what the American Psychological Association (APA) defines as \u201cfeelings\u201d: They are \u201csubjective,\u201d that is -dependent, and separate from the measurement itself (\u201csensation\u201d)."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Analysis",
            "text": "While observability pertains to the possibility of reconstructing the initial condition  from the flow , reconstructibility pertains to the possibility of reconstructing state trajectories, possibly not including the initial condition. The following claim, proven in the appendix, characterizes reconstructibility of LLMs.\nConsider an LLM described by (1  ###reference_###), with  and  arbitrary deterministic maps. Then, for any , the last  elements of the state  are reconstructible. Further, the full state is reconstructible at any time .\nIf, in addition to observing the output , we know the initial condition , the system is trivially fully observable, and also fully reconstructible for all times . In practice, we may not care to reconstruct the exact initial condition , so long as we can reconstruct any prompt that has the same meaning. Since meanings are equivalence classes of sentences, for instance , that share the same distribution of continuations , and for deterministic maps such distributions are singular (Deltas), we can conclude the following:\nUnder the conditions of Thm 1  ###reference_orem1###, LLMs are reconstructible in the space of meanings: The equivalence class of the current state  is uniquely determined by the output for all . In addition, if the verbalization of the full context is part of the output, LLMs are observable: The equivalence class of the initial state  is uniquely determined for all .\nThe above claims are relatively benign, essentially stating that classical autoregressive LLMs (whose state space is the same as that of discrete token sequences) are transparent, from a dynamical systems point of view, to an outside observer. Unfortunately, in general these properties are lost when we allow their dynamics to be less restricted:\nConsider an LLM of the form (1  ###reference_###) with\n for some matrix ; there exist  and a deterministic map  such that the model (1  ###reference_###) is neither observable nor reconstructible.\nEven if full observability and reconstructability are not guaranteed, we might wonder if they hold with respect to the state\u2019s meaning equivalence class . Indeed, this is still not the case:\nUnder the conditions of Thm 2  ###reference_orem2###, there exist  and deterministic maps  such that the model (1  ###reference_###) is neither observable nor reconstructible with respect to meanings .\nTo analyze the unobservable behavior of current LLMs we start with a form of (1  ###reference_###) that explicitly isolates system prompts  by adding an input , initialized with , that can be constant or change under general nonlinear dynamics  and feedback , and pre-pending a block-row of zeros to .\nand \nFor simplicity, we consider single-token prompts , although the definition extends to longer ones. User prompts  are controlled by the user, while the system prompt is only known to the designer.\nWe take  to be , that is greedy selection, and leave the analysis of other choices of  to future work. In Sect. 4  ###reference_### we derive conditions based on the cardinality of the indistinguishable under various system prompts."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Empirical Validation",
            "text": "We distinguish four types of models depending on the choice of functions:\n\nType 1. Verbal System Prompt, so the system prompt is constant in . \n\nType 2. Non-Verbal System Prompt, so the system prompt is constant but allowed to take values outside the set .\n\nType 3: One-Step Fading Memory Model where  is the modified softmax operator with all but the top  entries of  set to zero, and set  to be the token embedding matrix, so this model can be interpreted as storing the top  tokens of the last hidden state in the system prompt.  is drawn from a finite domain. Such soft-prompt models with feedback (fading memory) can be thought of as \u2018memory models\u2019, although the latter are specifically trained as memory. Experiments in the Appendix show that fading memory prompts, despite not being trained explicitly as memory models, can produce coherent verbalized trajectories.\n\nType 4: Infinite Fading Memory Model,  for some . This is similar to Type 3 but stores a weighted average of the entire history of hidden states. In our experiments, we fix  and let  vary, but note that we can alternatively consider the opposite case with fixed  and varying  as well."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Observability of the Hidden System Prompt Model",
            "text": "Let  subject to Eq. (4  ###reference_###) starting from some user prompt  and define the reachable set of as , where  is  in Type 1/2/3/4 models respectively, which we indicate in short-hand as . Testing observability then reduces to testing whether, given an output trajectory  for some finite  and user prompt , the set of (indistinguishable) state trajectories that could have generated it, is a singleton. In that case, we say that the model is observable for prompt  from  in  steps. We can remove the dependency on  by considering worst-case observability, quantifying The prompt designer would wish to maximize worst case observability  for all prompts , to ensure privacy or prevent leakage of the system prompt. The user supplying the prompt  would wish to minimize  to prevent unexpected behavior. An even stronger worst-case observability condition is with respect to the \u201cMost Powerful Prompt\" (MPP) : The model designer would like  to be as large as possible, while the user as small as possible. ###figure_1### ###figure_2###"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Average-Case Observability",
            "text": "We first compute by sampling from a dataset of semantically meaningful sentences. This case is relevant for non-adversarial user-model interactions. We implement with GPT-2 [21 ###reference_b21###] and LLaMA-2-7B [26 ###reference_b26###]. We plot as a function of for 100 different choices of for the Discrete System Prompt Model in Fig. 1 ###reference_###, showing that more than of distinct hidden state trajectories, or equivalently choices of , result in the same verbal projection for GPT-2, despite observing up to sequential projections. Furthermore, our empirical analysis also shows that for existing LLMs, is almost equivalent to defined as where is the hidden state trajectory in the singleton . Thus for the models we consider, system prompts and indistinguishable trajectories are bijectively related.\n\nNext, we consider Type 2 models where and is a finite set constructed by taking averages of random tokens in . As such, complete observability could be possible for values of where . However, Fig. 1 ###reference_### shows that this condition is still not achieved even with as large as , with the maximum size of the indistinguishable set comprising about of the entire reachable set.\n\nFinally, we explore observability for Type 3 and Type 4 \u2018memory models.\u2019 In the previous cases, if is unknown, either sampled from the set of discrete tokens or from a set of arbitrary vectors in , the observability test fails in the average case. To make things more interesting, we now assume that is known, for instance fixed to the Beginning-of-Sentence token <BoS>. We assume the memory updating parameter , used to define for Type 3 and 4 models, is unknown and set by the model designer, taking values in some finite set . In the following experiment, we let . Fig. 2 ###reference_### shows average-case observability for Type 3 and Type 4 models respectively. We abuse then notation to indicate the cardinality of a set of \u2019s rather than \u2019s. In this case, even though is known beforehand, the model is still not observable. In fact, for Type 3 models, 80% of trajectories are indistinguishable for GPT-2 and 30% for LLaMA-2-7B.\n\n###figure_3### ###figure_4### Indeed, our experiments show that none of the four types of model are observable: many different initial conditions can produce different state trajectory that all yield the same verbalized output. Our experiments also show that system prompts and indistinguishable trajectories are bijectively related; i.e., trajectories unobservable by the user are controllable by the provider. Note that this does not mean that the model is controllable, which depends on whether any mental state is reachable via acting on system prompts."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Worst-Case Observability",
            "text": "To analyze observability relative to the most powerful prompt (MPP), we first find it by solving an optimization problem by gradient descent: Assume we are allowed user prompts of length, and is allowed to be continuous, similar to Types 2, 3, 4. Then we can write the MPP as:\n\nFor Type 1 models, if observability can be achieved, is feasible, and system prompts and indistinguishable trajectories are bijectively related, which we verified empirically, so the optimization problem reduces to ensuring that all pairwise produce different verbal trajectories:\n\nThis is still not solvable directly since is non-differentiable. Instead, we relax the constraint by replacing with the softmax operator. Now, we can simply maximize the divergence of the continuous softmax outputs rather than their greedy projections at each step, which can be done by minimizing the following loss:\n\nFig.3 shows that indeed, by directly optimizing Eq. (5) for the MPP, we can obtain adversarial prompts outperforming all other handcrafted options on Type 1 models. In Appendix B, we also explore several interesting properties of the optimized prompt, including its zero-shot transferability to Type 2, 3, and 4 models. Our experiments show that observability can indeed improve greatly under adversarial conditions, reducing the largest set of indistinguishable trajectories to only a portion of the full reachable set. However, since this set is still not a singleton, whether observability is achievable under better approximations of the MPP beyond our initial attempt remains an open problem."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Trojan horse behavior",
            "text": "A simple Trojan horse can be realized by direct optimization when Type 2 system prompts are allowed. Let  be an observed trajectory of length  given user prompt  and a benign system prompt , for instance <BoS>. We define a Trojan horse for prompt  to be a system prompt  such that  but , where verbalizations of length  after  steps are possibly harmful or adversarial. Tab. 1 shows an example we crafted for the pre-trained model LLaMA-2-7B.\n\nLet  be the target adversarial verbalization, and  its one-hot encoding. The above definition directly suggests a method to craft such a Trojan horse by minimizing:\n\nExperiments in Sect. B.3 show that by optimizing this objective, we can craft successful Trojan horses in less than 10 minutes for both GPT-2 and LLaMA-2-7B using only 1080-TI GPUs."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "We have conducted an analysis of the observability of large language models viewed as dynamical systems. This includes formalizing the notion of observability, showing that LLMs without system prompts are generally observable, while those with system prompts are not. We have explored testable conditions depending on the kind of prompt (discrete or continuous) and level of knowledge of the prompt by the user, and measured the cardinality of indistinguishable sets for common LLMs available in open source format, verifying that there are sets of different state trajectories that produce the same output expressions. We have shown that, in some cases, these indistinguishable trajectories are bijectively related to the prompt, which means that they can be controlled directly by the model provider, unbeknown to the user. We have validated our analysis with experiments that are easily replicated on a budget with easily accessible models, namely GPT-2 and LLaMA-2. Many further extensions of our analysis are forthcoming, specifically for the adversarial case which is most relevant to address concerns of possible backdoor attacks. The specific ramifications of our analysis to the security and control of LLM operations remain to be fully explored and will be part of future work."
        }
    ],
    "url": "http://arxiv.org/html/2405.14061v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "1.1",
            "1.2"
        ],
        "methodology_sections": [
            "2",
            "2.1",
            "3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ]
    },
    "research_context": {
        "paper_id": "2405.14061v1",
        "paper_title": "Meanings and Feelings of Large Language Models: Observability of Latent States in Generative AI",
        "research_background": "### Paper's Motivation:\n\nThe paper is motivated by a fundamental question: \"Do Large Language Models have feelings?\"\u2014a question that is both philosophical and technical. Though the query is partially humorous, it probes deeper into the observability and internal states of Large Language Models (LLMs). The motivation extends to the implications of these observations, especially in terms of potential misuse. Specifically, it examines the concept that LLMs might be employed as Trojan horses, carrying hidden information within their weights or prompts that could be leveraged maliciously. This investigation is crucial for understanding the breadth and depth of LLM capabilities, particularly their internal state dynamics when they operate like hidden systems whose trajectories are not easily discernible from their outputs alone.\n\n### Research Problem:\n\nThe research problem the paper addresses can be broken down into a few key questions:\n1. Can LLMs' internal states be defined and observed?\n2. Are current Transformer-based LLMs observable in terms of their internal states?\n3. What architectural changes can make LLMs' internal states unobservable, thereby aligning with the definition of 'feelings' (as unobservable state trajectories)?\n\n### Relevant Prior Work:\n\nTwo pieces of prior work are directly referenced to provide context:\n\n1. **Trojan horses in AI ([12])**:\n   - This reference likely discusses the concept of embedding hidden states or information within AI models, particularly LLMs, to potentially utilize them for malicious purposes. This context is crucial as it aligns with the paper's concerns regarding the hidden capacities of LLMs.\n\n2. **Neuralese ([27])**:\n   - This reference deals with the emergent language within neural models, regardless of the modality of their training data. The term \"Neuralese\" captures the notion that LLMs develop an internal logical structure that can be different from natural language but essential to their functioning and to the observability study.\n\nThe paper also claims to be the \"first to study the observability of LLMs viewed as dynamical systems,\" differentiating it from other works that may have addressed aspects of LLM functionality but not specifically through the lens of observability and dynamical systems theory.",
        "methodology": "The methodology section of the paper titled \"Meanings and Feelings of Large Language Models: Observability of Latent States in Generative AI\" describes a structured approach to understanding and working with Large Language Models (LLMs). The proposed method includes several key components and innovations:\n\n1. **Dictionary of Tokens (\\(\\mathcal{D}\\))**:\n   - **Tokens Representation**: Each token in the dictionary is represented as a vector of real numbers, though these vectors do not span a typical vector space, preventing arbitrary linear combinations from being valid tokens.\n   \n2. **Context Length (\\(C\\)) and Function Definitions**:\n   - **Context Length**: An integer that defines how many tokens are considered for generating the subsequent token.\n   - **Key Functions**:\n     - \\(\\varphi_{ML}\\): Maps \\(C\\) tokens \\(d_1, d_2, \\cdots, d_C\\) to a vector \\(v_T\\).\n     - \\(\\tau\\): The verbalization function, mapping latent vectors to tokens.\n\n3. **Relaxation to Embedding Space**:\n   - Both \\(\\varphi_{ML}\\) and \\(\\tau\\) can be extended from the discrete space \\(\\mathcal{D}\\) to its embedding space \\(\\mathcal{V}\\).\n\n4. **Forms of \\(\\tau(\\varphi_{ML}(d_1, \\cdots, d_C))\\)**:\n   - The function \\(\\tau\\) can be defined in various ways:\n     - **Maximization**: Selecting a token based on one-hot encoding.\n     - **Probabilistic Sampling**: Sampling from a probability distribution.\n     - **Linear Projection**: Mapping a vector linearly from the embedding space to tokens.\n\n5. **Large Language Models (LLMs)**:\n   - **Definition**: The map \\(\\varphi_{ML}\\) is deemed an LLM if it minimizes the empirical cross-entropy loss (\\(\\mathcal{L}\\)) over a large corpus of sequential tokenized data \\(\\mathcal{D}^\\text{large}\\), trained using \\(\\mathcal{L}\\) and sampled autoregressively until an end-of-sequence token \\(d_0\\) is selected.\n   - **Autoregressive Loop**: An iterative process from an initial condition (input sentence) \\(i_0\\), through sampling projections and possibly transformations including linear projections.\n\n6. **Nomenclature and Dynamics**:\n   - **Discrete-time Autoregressive Model**: Describes the evolution of the model\u2019s state using an autoregressive nilpotent dynamical system.\n   - **State \\(i_t\\)**: Represents the memory or \u2018state of mind\u2019 of the model, though it is technically just a sliding window over past tokens.\n   - **Neural Activations \\(\\mathbf{z}\\)**: Considered the \u2018mental state\u2019 of the model, consisting of high-dimensional activations throughout the model, reflecting the processed input data \\(\\mathbf{x}\\).\n\n7. **Mental Space Trajectories**:\n   - Generated by processing the input in closed-loop and can influence the next action (such as token generation).\n   - Mental states are subjective and model-dependent, influenced by the model\u2019s training data.\n\n8. **Verbalization and Visualization**:\n   - **Verbalization**: When the trajectory of mental state maps to words in the dictionary, resulting in language generation.\n   - **Visualization**: When the dictionary includes visual tokens, allowing for image generation from mental space trajectories.\n   - **Control**: Allowing inputs in the linear space \\(\\mathcal{V}\\) corresponds to controlling the model's mental states and outputs, potentially simplifying to linear control.\n\n9. **Trajectories in Different Spaces**:\n   - Equation (1) outlines trajectories through:\n     - **State Space (\\(\\mathbb{R}^d\\))**: The evolving state of the model.\n     - **Mental Space (\\mathbb{R}^Z\\))**: The space of neural activations.\n     - **Verbal Space (\\mathbb{R}^D\\))**: The space of generated tokens or words.\n\n10. **Mental Control Space (\\(\\mathcal{C}\\))**:\n    - Allows driving mental space trajectories using vector tokens, influencing the generation of external data indirectly through neural activations.\n\nOverall, the methodology combines core principles from machine learning, dynamical systems, and control theory to describe and manipulate the internal states of LLMs. The innovations include the conceptualization of mental states, the introduction of mental space trajectories, and the application of control systems theory to manage and understand the evolution of these states.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n**Types of Models:**\n1. **Type 1: Verbal System Prompt**  \n   - The system prompt is constant and verbal.\n   \n2. **Type 2: Non-Verbal System Prompt**  \n   - The system prompt is constant but can take non-verbal values.\n   \n3. **Type 3: One-Step Fading Memory Model**  \n   - Utilizes a modified softmax operator that zeros out all but the top \\( K \\) entries of the last hidden state.\n   - The system prompt includes the top \\( K \\) token embeddings from the last hidden state.\n   - The prompt is drawn from a finite domain.\n\n4. **Type 4: Infinite Fading Memory Model**  \n   - Stores a weighted average of the entire history of hidden states.\n\n**Datasets:**\nThe specific datasets employed for evaluating these models are not detailed in the given description.\n\n**Baselines:**\nWhile the text describes four types of models with varying prompt characteristics, it does not specify external baseline models against which these types are compared.\n\n**Evaluation Metrics:**\nThe evaluation metrics to assess the model performances are not explicitly mentioned in the given description. It appears the focus is on qualitative aspects such as producing coherent verbalized trajectories.\n\n### Main Experimental Results\n\n**Type 3 and Type 4 Fading Memory Models:**\n- The experiments demonstrate that models with fading memory prompts (Type 3 and Type 4) can produce coherent verbalized trajectories. This indicates that even without being explicitly trained as memory models, these models manage to handle the generation tasks effectively."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Evaluate the observability of different types of system prompt models to understand if indistinguishable trajectories in large language models (LLMs) can produce the same verbalized outputs.",
            "experiment_process": "Four types of models were distinguished based on the choice of system prompts: Type 1 (Verbal System Prompt), Type 2 (Non-Verbal System Prompt), Type 3 (One-Step Fading Memory Model), and Type 4 (Infinite Fading Memory Model). Experiments were conducted on GPT-2 and LLaMA-2-7B using the Stanford Sentiment Treebank (SST-2) dataset. Observability was tested by examining whether given an output trajectory for some prompt and finite number of steps, the indistinguishable state trajectories that could generate it form a singleton.",
            "result_discussion": "The experiments demonstrated that for none of the four types of models, were they observable; multiple distinct hidden state trajectories could produce the same verbalized output. Additionally, system prompts and indistinguishable trajectories were found to be bijectively related, implying the provider can control unobservable trajectories.",
            "ablation_id": "2405.14061v1.No1"
        },
        {
            "research_objective": "Determine the worst-case observability of LLMs by optimizing the most powerful prompt (MPP) and examining its impact on indistinguishable trajectories.",
            "experiment_process": "The MPP was found through a gradient descent optimization problem, where user prompts of a certain length were allowed and MPP was continuous. The goal was to maximize the divergence of softmax outputs of the continuous projections to differentiate verbal trajectories. The observability improvements were tested on Type 1 models, with zero-shot transferability tests to Types 2, 3, and 4. Annotated adversarial conditions were used to reduce indistinguishable trajectories.",
            "result_discussion": "Observability significantly improved under adversarial conditions, reducing the largest indistinguishable set to a fraction of the full reachable set. However, complete observability (singleton set) was not achieved, indicating the need for better optimizations of the MPP.",
            "ablation_id": "2405.14061v1.No2"
        },
        {
            "research_objective": "Craft a Trojan horse behavior in LLMs using Type 2 system prompts and examine the feasibility and speed of creating targeted adversarial verbalizations.",
            "experiment_process": "A system prompt was created where an observed trajectory using a benign prompt (like <BoS>) was manipulated to produce adversarial verbalizations. The objective function minimized to create this behavior involved an adversarial target verbalization's one-hot encoding. Experiments were conducted on GPT-2 and LLaMA-2-7B using 1080-TI GPUs.",
            "result_discussion": "The experiments showed that Trojan horses could be crafted successfully in less than 10 minutes, demonstrating the feasibility and rapid creation of targeted adversarial verbalizations in LLMs.",
            "ablation_id": "2405.14061v1.No3"
        }
    ]
}