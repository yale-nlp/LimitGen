{
    "title": "Multi-Stage Multi-Modal Pre-Training For Automatic Speech Recognition",
    "abstract": "Recent advances in machine learning have demonstrated that multi-modal pre-training can improve automatic speech recognition (ASR) performance compared to randomly initialized models, even when models are fine-tuned on uni-modal tasks. Existing multi-modal pre-training methods for the ASR task have primarily focused on single-stage pre-training where a single unsupervised task is used for pre-training followed by fine-tuning on the downstream task. In this work, we introduce a novel method combining multi-modal and multi-task unsupervised pre-training with a translation-based supervised mid-training approach. We empirically demonstrate that such a multi-stage approach leads to relative word error rate (WER) improvements of up to 38.45% over baselines on both Librispeech and SUPERB. Additionally, we share several important findings for choosing pre-training methods and datasets. \n\nKeywords:\u2009Multi-modal, Speech Recognition, Self-supervised, Pre-training, Mid-training",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "Despite progress in large-scale pre-training for automatic speech recognition (ASR) Chen et al. (2022); Hsu and Shi (2022); Chan et al. (2022), uni-modal (speech-only) ASR remains a challenging task, particularly when faced with rare words and noisy acoustic conditions. When understanding spoken phonemes, the model must correctly discern both speaker-specific patterns (e.g., accent, prosody) and global noise patterns (e.g., background noise, intermittent interruptions, confounding speakers). Recent work in natural language processing (NLP) Tu et al. (2020); Hoffmann et al. (2022), robotics Mandlekar et al. (2022); Kuhar et al. (2023); Khazatsky et al. (2024) and computer vision Goyal et al. (2022); Ramanujan et al. (2023); Jain et al. (2024) has demonstrated that exposing models to a high diversity of data during pre-training is essential in building robust representations. \n\nSimilarly, recent works in the ASR community have corroborated these results. Shi et al. (2022) and Hsu and Shi (2022) demonstrated that pre-training on large-scale audio-visual data (or audio-only data), in the form of lip-reading videos, leads to better performance on the lip-reading task. Chan et al. (2022) showed that exposing models to video data during pre-training led to performance improvements not only when visual input is available at training time, but also when only audio is available at test time.\n\nChan et al. (2022) also demonstrated that adding visual information from non-speech specific videos (leveraging the Kinetics dataset Carreira and Zisserman (2017)) is only a small portion of the possible augmentations that can be made during pre-training. In this work, we not only explore two new audio-visual pre-training sources, but also leverage a translation task with English speech input as a new mid-training task to consolidate information learned during the pre-training phase. Further, while Chan et al. (2022) explore an attention-based transfer-learning framework based on k-means clustering for pre-training, we simplify the pre-training architecture significantly and explore several pre-training objectives beyond masked cluster prediction. Our primary contributions are as follows:\n\nWe perform large-scale evaluation of multiple audio-visual pre-training methods (MAE, CLR) using several pre-training datasets (Kinetics, VoxCeleb2, LRS3) with varying characteristics. We evaluate them on the ASR task and the SUPERB benchmark, showing how multi-modal pre-training is affected by key dataset characteristics.\n\nWe show that pre-training with audio-visual data, particularly data from speech-specific audio-visual datasets can improve word error rate (WER) up to 30.8% relative compared to randomly initialized baseline models on speech-only test data.\n\nWe introduce a novel mid-training stage between the pre-training and fine-tuning steps, using speech translation as the mid-training task. The mid-training stage improves WER by 38.45% relative on the Librispeech test-clean dataset, and by 26.18% relative on the test-other dataset compared to audio-visual pre-training only baseline. The technique also shows improvements on several tasks (Keyword Spotting, Intent Classification, Phoneme Recognition, and Speaker Diarization) in the SUPERB Yang et al. (2021) benchmark."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Background",
            "text": "Representation learning methods like Contrastive Predictive Coding and Wav2Vec have shown significant promise when applied to ASR. Methods for large-scale pre-training for ASR can be categorized into masked autoencoding methods and contrastive learning. While traditionally self-supervised methods are trained on a single target loss, other methods have been proposed which leverage multiple pre-training targets. Some approaches optimize a combination of uni-modal supervised losses, and recently, approaches such as W2v-BERT and JUST have combined contrastive approaches with masked auto-encoding to build robust self-supervised speech representations. Similarly, while most self-supervised methods are pre-trained on a single dataset, some have demonstrated that a wide mix of data is essential for pre-training. In this work, we target both of these problems: use a combination of losses, and pre-training stages under different datasets to improve the learned multi-modal representations.\n\nAudio-visual data provides diverse information for representation learning. Improvements on ASR have been demonstrated when visual input is available both at training and test time, and methods such as u-HuBERT extend such pre-training approaches to applications where both uni-modal and multi-modal data are available at training-time but still require multi-modal data for inference. Later work demonstrated that pre-training with paired audio-visual data can even improve performance on uni-modal datasets.\n\nIn addition to multiple modalities, pre-training with multiple languages has also been explored in the literature. One study demonstrates that pre-training with a wide range of inputs from several languages improves ASR performance across all studied languages. It has been shown that leveraging self-supervised learning (SSL) for knowledge transfer across languages can yield WER improvements of up to 3.55% relative WER on target languages, and in almost all cases, even out-of-domain multi-lingual data can improve WER in single and multi-speaker conversations and dictation tasks."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   Methods",
            "text": "Our method (Figure 1 ###reference_###) consists of a multi-stage multi-modal pre-training approach, followed by a fine-tuning stage on downstream tasks. We describe our method in this section."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   Pre-Training Tasks",
            "text": "We experiment with two pre-training strategies that differ in the granularity of information they extract. The first method, Masked Autoencoder (MAE), learns local features by reconstructing masked parts of speech and video. The second method, Contrastive Learning (CLR), focuses on global features by using pooled audio-visual features from the same video as positive pairs while other combinations of audio-visual pairs as negatives. The two pre-training strategies help us compare the effects of local and global feature learning against the visual-audio dataset characteristics, for eg., Kinetics dataset Carreira et al. (2018  ###reference_b11###) has non-speech audio streams, while LRS-3 Afouras et al. (2018  ###reference_b2###) and Voxceleb2 Chung et al. (2018  ###reference_b21###) datasets have videos with speech.\n\nMasked Autoencoding (MAE): Traditional MAE approaches for ASR pre-training have focused on token-based reconstruction Hsu et al. (2021  ###reference_b35###); Shi et al. (2022  ###reference_b63###); Chan et al. (2022  ###reference_b14###). However, these methods have the drawback of requiring a separate quantization method, which can add significant training complexity. We simplify the encoder to directly reconstruct features from the original masked signal.\n\nOur MAE approach consists of three encoders: a masked audio-specific encoder based on the encoder in Chen et al. (2022  ###reference_b17###), a masked video-specific encoder based on Tong et al. (2022  ###reference_b68###), and a joint transformer decoder with the same structure as in Devlin et al. (2018  ###reference_b23###).\n\nLet be the set of audio input frames (we use f-dimensional log-filterbank energies (LFBE)), and be a set of video frames, which have been subdivided into voxels. refers to the number of audio frames and are the number of video frames of height and width. To generate the input sequence to , we randomly mask a fraction of the audio frames with 0s (masking), and generate the embedded audio. We use a similar process to mask voxels, to generate.\n\nThe encoded representations and are passed through the common decoder to produce and respectively. The common decoder ensures that the representations and are projected to the same representation space.\n\nThe final MAE loss is computed as the squared L2 distance between and, and and:\n\nContrastive Learning (CLR): Contrastive Learning aims to learn representations using a contrastive loss that minimizes the distance between similar points and maximizes the distance between dissimilar points in a latent space. For contrastive learning, following Radford et al. (2021  ###reference_b58###) and Xu et al. (2021  ###reference_b76###), we use the modality specific encodings and to generate, where the pooling operation is a temporal average, and, where the pooling operation is a spatio-temporal average. While other pooling operations like attention pooling are possible, we found that the spatio-temporal average captures consistent low-frequency global information, which correlates well with the information shared with the visual modality (unlike high-frequency information, which is often not evident from the visual modality). The self-supervised contrastive loss for a batch of samples, and is computed as\n\nMAE + CLR: In this setup, we combine the benefits of learning local features using MAE with learning global features using CLR as shown in Figure 1  ###reference_###. Both pre-training losses are added with equal weights, similar to Chung et al. (2021  ###reference_b22###) to compute the final loss as\n\nPre-training Datasets: We use three datasets for pre-training. The Kinetics-600 dataset Carreira et al. (2018  ###reference_b11###) has 966 hours of audio-visual data for activity recognition, with a focus on the environment or instrument used. The videos contain non-speech audio data and have been used previously for audio-visual training Chan et al. (2022  ###reference_b14###). Voxceleb2 Chung et al. (2018  ###reference_b21###) provides 2380 hours of multi-lingual speaker recognition data with challenging acoustics and comprehensive lip and facial movements. LRS3 Afouras et al. (2018  ###reference_b2###) features 346 hours of clean, multi-modal spoken sentence data from TED and TEDx videos. The speech data in Voxceleb2 has noisy acoustic conditions whereas LRS-3 has clean speech with speakers talking to a close-talk microphone. These datasets allow for exploring the impact of clean-speech/noisy-speech/non-speech videos and pre-training techniques on the ASR task (subsection 3.3"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   Mid-Training: Speech Translation",
            "text": "To improve performance of the pre-trained audio-visual models on the downstream tasks, we introduce a mid-training task that bridges the gap between pre-training and fine-tuning. Our approach transfers the learned distribution of the pre-trained model towards the distribution required for the downstream task, while discarding irrelevant information. The mid-training task is designed to provide a low-cost warm-up for the pre-trained model, which can accurately represent various characteristics of the data.\n\nUsing translation as a mid-training task is only one possible instantiation of the mid-training approach. In addition to translation, future work can explore other speech-centric tasks like speaker identification, implied by Chan and Ghosh (2022)), speaker/source separation, text to speech, and others. While we found that translation is effective in this work, we expect that each additional task will impact the downstream training process in unique ways."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "3.3.   Fine-Tuning",
            "text": "We evaluated our models by testing their performance on several downstream tasks. The fine-tuning task is distinct from the pre-training task of masked reconstruction (MAE) or contrastive learning (CLR), and the mid-training task designed to bridge the gap. Primarily, we evaluate the performance of the models on the test-clean and test-other Librispeech Panayotov et al. (2015) datasets for ASR, as well as four tasks from the SUPERB Yang et al. (2021) benchmark: Intent Classification (IC), Keyword Spotting (KS), Phoneme Recognition (PR) and Speaker Diarization (SD). Because our aim was to evaluate how both the pre-training and mid-training data distributions impact the final learned representations, we freeze the encoder weights during task specific fine-tuning, and fine-tune only the task specific decoder using the LS-960 dataset (for ASR) following Baevski et al. (2020) or the default datasets specified in the SUPERB benchmark Yang et al. (2021)."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "3.4.   Model Details",
            "text": "In this section, we discuss the implementation details of the different training setups across the datasets.\n\nVideo Data Pre-processing: Videos are first resized to a resolution of pixels, with a temporal stride, and frames sampled temporally. We apply random resized cropping with scale from to, and random horizontal flipping following standard computer vision techniques for visual data augmentation.\n\nVideo Encoder: Our video encoding approach is similar to that of Feichtenhofer et al. (2022). Firstly, we divide the video into a regular grid of space-time patches of dimensions in the direction, respectively. These patches are then flattened and augmented with spatio-temporal positional embeddings Vaswani et al. (2017).\n\nFor the Masked Autoencoder, we randomly select 60% of the patches for masking and mask patches without replacement, while keeping the selection agnostic in the space-time domain. The remaining patches are then passed through 12 ViT encoder blocks Dosovitskiy et al. (2020) with a hidden dimension of 768. We obtain the video encoded features of the remaining spatio-temporal patches, which are later reconstructed using a common decoder.\n\nFor Contrastive Learning, we reduce the spatial patches to a single embedding for each frame Xu et al. (2021); Radford et al. (2021). The reduced patches are passed through a video encoder with 12 ViT encoder blocks Dosovitskiy et al. (2020) with a hidden dimension of 768. The encoded embeddings are temporally pooled following Xu et al. (2021), resulting in single-vector video features that can be contrasted against corresponding audio embeddings.\n\nAudio Data Pre-processing: The audio input is re-sampled to a frequency of 16kHz. Subsequently, 80-dimensional Log-Filterbank Energy (LFBE) features are computed from the resulting audio frames. To ensure consistency in feature size, we selected the first 1000 LFBE frames for downstream processing. The frames are further sub-sampled using a 1D convolutional layer, reducing the number of audio frames to 250, following the approach of Gulati et al. (2020).\n\nAudio Encoder: We use positional embeddings in the sub-sampled audio frames similar to video encoding, as proposed by Vaswani et al. (2017). In the Masked Autoencoder, a random mask without replacement is applied to a percentage of the frames, with the visual and audio modalities sharing the same masking ratio to maintain balance in the amount of information across both modalities. The remaining frames are encoded by a Conformer Gulati et al. (2020) with 16 layers, 4 heads, and a depth-wise convolutional kernel of size 31. Audio features are then up-sampled by a linear layer and normalized for reconstruction.\n\nIn Contrastive Learning, the sub-sampled frames are directly featurized by the Conformer blocks without any masking involved. The audio features are then temporally pooled to obtain a single feature for the audio clip, which is up-sampled and normalized. For both the Mid-training and Fine-tuning tasks, the feature output from Conformer blocks is used as input to task-specific decoders. The weights of the convolutional sub-sampling layer and Conformer blocks are the only components re-used from the pre-training stage for further steps.\n\nCommon Decoder: The Masked Autoencoder pre-training step uses a relatively small vanilla ViT Dosovitskiy et al. (2020) decoder of hidden dimension size of 512 and 4 ViT blocks. The decoder processes a combination of the encoded and masked patches and outputs the original reconstructed signal. A shared decoder is used to sequentially reconstruct each patch."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Results, Analysis & Limitations",
            "text": "Our main results on the Librispeech dataset are shown in Table 1  ###reference_### and Figure 3  ###reference_###, and demonstrate several interesting learnings:\nAudio-visual Pre-training is Effective: Table 1  ###reference_### shows that on average in all cases, audio-visual pre-training is effective. Averaging the performance across all methods results in 6.34  0.94 WER for test-clean, and 12.18  0.98 for test-other. Under the null hypothesis that audio-visual pre-training is ineffective, we find significant improvements () over the baseline.\nMid-training with all translation pairs improve ASR performance: Table 1  ###reference_### shows that the mid-training approach leads to significant  improvements over pre-trained models alone, leading to relative WER improvements of 8.59%/6.77% (test-clean/test-other) with English-German pair, 18.55%/10.28% for English-Italian pair, and 13.11%/7.71% for English-Dutch pair. Surprisingly, Italian is the most effective, suggesting that choosing languages which are complementary to English may be more useful than languages which are closer to the target downstream language (English, Dutch and German all have Germanic roots, while Italian has Latin roots - see Tyshchenko et al. (2000  ###reference_b71###) for a discussion on linguistic distance).\nWe leave it to future work to explore languages that retain very little shared information, such as Russian or Chinese. The relative performance improvements with mid-training are shown in Figure 3  ###reference_###. The figure shows several effects which we discuss in the following sections: the model pre-trained on Kinetics dataset is most improved with mid-training, English-Italian translation is the best mid-training pair, and the model pre-trained with CLR benefits the most with mid-training.\nHow do pre-training datasets impact performance (Is dataset size the only factor)? Despite differences in pre-training dataset sizes, it is interesting to understand how the input mix of data impacts the overall performance of the model. Without mid-training, models pre-trained on LRS-3, the smalleset dataset, outperform all other models (6.19%/11.64% WER) on the test-other dataset. LRS-3 is a small fraction of the size of the VoxCeleb2 dataset, suggesting that the distributional makeup of the multi-modal dataset is key to pre-training performance, and dataset size is not all that matters. VoxCeleb2 (6.16%/12.01% WER) outperforms LRS-3 slightly on the test-clean dataset. Kinetics trails both in aggregate (6.65%/12.9% WER), which could be due to both the size of the dataset (only half the size of VoxCeleb2), or the makeup of the dataset (no speech-specific data).\nAll three pre-training datasets outperform from scratch training for ASR (even Kinetics), indicating that pre-training on any amount or type of audio-visual data can be helpful. We note that while Kinetics has the worst overall performance, it improves the most with mid-training (Rel. WER improvement of 14.03%) vs VoxCeleb2 (9.45%) and LRS-3 (6.17%) (Figure 3  ###reference_###). These results confirm that the model pre-trained on Kinetics has the most to gain from language-representation alignment (as it contains no speech data), and training on LRS-3, which consists of primarily clean data, has less to gain.\nThe best ASR results with MAE and CLR are obtained on the LRS-3 pre-training dataset. However the best MAE+CLR performance was in using the Kinetics dataset. While it can be difficult to disentangle the results from pre-training dataset size, this result may suggest that multi-task learning is more effective on out-of-domain data, where modalities contain non-redundant audio information, compared to VoxCeleb/LRS-3, where modalities consist of primarily redundant information.\nMAE outperforms CLR, MAE+CLR on ASR: For ASR results averaged over all pre-training datasets, we find that MAE (5.63%/11.53% WER) alone outperforms both CLR (6.00%/11.67%) and MAE+CLR (6.09%/11.85%), suggesting that pre-training with masked auto-encoding objectives remains a promising approach for future exploration. Following intuition from Chan et al. (2022  ###reference_b14###), it is likely that CLR-augmented methods outperform on more global downstream tasks, whereas MAE encodes more local information which is useful for ASR, and MAE+CLR is a useful mix of both. This hypothesis is validated in our experiments on SUPERB Yang et al. (2021  ###reference_b77###), where we found MAE+CLR most effective when aggregated across the mix of global (Intent Classification, Keyword Spotting), and local (Phoneme Recognition) tasks.\n###figure_2### Mid-Training is most effective with multi-task pre-training: We explore the performance of our methods on four tasks from the SUPERB Yang et al. (2021  ###reference_b77###) benchmark in Figure 2  ###reference_###. For SUPERB, mid-training improves performance for MAE+CLR models across most tasks. The notable exception is speaker diarization (SD), where there is minimal task overlap between SD and the mid-training target. Intent Classification (IC) is most improved (results not show in the tables), primarily due to a improvements in models pre-trained on the Kinetics (+80.17%) and LRS-3 (+102.30%) datasets, which benefit from the additional textual alignment. Keyword spotting (KS) improvements can also be largely attributed improvements on models pre-trained on Kinetics (+27.52%), for similar reasons. Models pre-trained on VoxCeleb2 improve less with mid-training compared to models pre-trained with both Kinetics and LRS-3 for all tasks. We posit that since VoxCeleb2 dataset is already multi-lingual, and benefits less from further multi-lingual training.\n###figure_3### Note on baseline conformer performance: In this work, we note that our baseline conformer models do not match the performance of Gulati et al. (2020  ###reference_b31###). Note that our primary goals was not to attain state of the art models, but study the impact of pre-training methods and datasets on ASR performance. The higher WER can be attributed to lower batch size used in out experiments, which was done to account for the large number of ablation studies done for this paper. While the overall baseline performance may be worse, the insights learned from the relative performance comparisons across the large-scale ablation are transferable to larger, more expensive models.\nIn summary, our results indicate the following:\nAudio-visual pre-training is effective in almost all scenarios.\nMid-training is useful and including data which is complementary is more effective than including data similar to pre-training data.\nClean speech audio-visual dataset LRS-3 is an effective pre-training dataset given its size, compared to Kinetics and Voxceleb2.\nMAE pre-training is more effective than contrastive learning in ASR, while augmenting pre-training with CLR can help with downstream tasks that use global information."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Discussion",
            "text": "Recently, the size of pre-trained models and the datasets have increased to such an extent that it is cost-prohibitive to pre-train these models on datasets aligned with the downstream tasks of interest. Hence, a light-weight mid-training strategy can tune the pre-trained features strengthening the downstream performance.\nAn alternative to the mid-training strategy is to include task during pre-training itself. This alternative strategy has two drawbacks; first, the amount of labeled data available for the mid-training task is typically not large enough to have significant impact when jointly learned in the pre-training stage. Secondly, the mid-training approach is more practical as it can be applied to already available pre-trained models instead of training the models from scratch which requires large amounts of time and compute."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Conclusion & Future Directions",
            "text": "This work presents a multi-lingual mid-training objective and a large-scale analysis of multiple audio-visual pre-training methods and datasets. We demonstrate how large-scale audio-visual pre-training significantly improves downstream ASR performance and how a well-chosen mid-training task can benefit the final downstream task.\n\nWhile this paper presents initial insights into the impact of mid-training tasks on multi-modal pre-trained models, we believe extensive future work is necessary to fully comprehend how sequences of training tasks can effectively align large pre-trained models with downstream tasks. \n\nAn intriguing direction for future research is the exploration of additional mid-training tasks. We highlight that translation has the potential to bridge gaps between multi-modal pre-trained models and language-based ASR tasks. However, paired data for translation may often be scarce and might not serve as the optimal choice for future mid-training tasks. It may be valuable to explore mid-training tasks centered around synthetic data\u2014such as TTS data derived from text datasets or text generated by large language models\u2014or self-supervised approaches to mid-training.\n\nAnother closely related area for future work involves examining how pre-training tasks influence the performance of downstream and mid-trained models. While our focus is on multi-modal pre-training, it is a pivotal direction for ASR research. Nevertheless, mid-training can also be applied to uni-modal pre-training or even zero-shot transfer from foundational models.\n\nIn conclusion, this study highlights the impact of mid-training tasks within the context of multi-modal pre-training and demonstrates the significant improvement in downstream automatic speech recognition performance achieved through large-scale audio-visual pre-training. By delving deeper into these areas, we gain a better understanding of how to effectively align pre-trained models with diverse downstream tasks, unlocking new possibilities for multi-modal ASR research."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7.   Bibliographical References",
            "text": ""
        }
    ],
    "appendix": [],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T1\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S3.T1.5\">\n<tr class=\"ltx_tr\" id=\"S3.T1.3.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S3.T1.3.3.4\"><span class=\"ltx_text\" id=\"S3.T1.3.3.4.1\" style=\"font-size:90%;\">Method</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" id=\"S3.T1.3.3.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T1.3.3.5.1\">\n<span class=\"ltx_p\" id=\"S3.T1.3.3.5.1.1\"><span class=\"ltx_text\" id=\"S3.T1.3.3.5.1.1.1\" style=\"font-size:90%;\">PT</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.3.3.6\"><span class=\"ltx_text\" id=\"S3.T1.3.3.6.1\" style=\"font-size:90%;\">MT</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S3.T1.1.1.1\">\n<span class=\"ltx_text\" id=\"S3.T1.1.1.1.1\" style=\"font-size:90%;\">en-de </span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S3.T1.2.2.2\">\n<span class=\"ltx_text\" id=\"S3.T1.2.2.2.1\" style=\"font-size:90%;\">en-it </span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S3.T1.3.3.3\">\n<span class=\"ltx_text\" id=\"S3.T1.3.3.3.1\" style=\"font-size:90%;\">en-nl </span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.5.6\">\n<td class=\"ltx_td\" id=\"S3.T1.5.6.1\"></td>\n<td class=\"ltx_td\" id=\"S3.T1.5.6.2\"></td>\n<td class=\"ltx_td\" id=\"S3.T1.5.6.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.6.4\"><span class=\"ltx_text\" id=\"S3.T1.5.6.4.1\" style=\"font-size:90%;\">Test-clean</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.6.5\"><span class=\"ltx_text\" id=\"S3.T1.5.6.5.1\" style=\"font-size:90%;\">Test-other</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.6.6\"><span class=\"ltx_text\" id=\"S3.T1.5.6.6.1\" style=\"font-size:90%;\">Test-clean</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.6.7\"><span class=\"ltx_text\" id=\"S3.T1.5.6.7.1\" style=\"font-size:90%;\">Test-other</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.6.8\"><span class=\"ltx_text\" id=\"S3.T1.5.6.8.1\" style=\"font-size:90%;\">Test-clean</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.6.9\"><span class=\"ltx_text\" id=\"S3.T1.5.6.9.1\" style=\"font-size:90%;\">Test-other</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.5.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.5.5.3\"><span class=\"ltx_text\" id=\"S3.T1.5.5.3.1\" style=\"font-size:90%;\">No Pre-training</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S3.T1.5.5.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T1.5.5.4.1\">\n<span class=\"ltx_p\" id=\"S3.T1.5.5.4.1.1\"><span class=\"ltx_text\" id=\"S3.T1.5.5.4.1.1.1\" style=\"font-size:90%;\">None</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.5.5.5\"><span class=\"ltx_text\" id=\"S3.T1.5.5.5.1\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.4.4.1\">\n<span class=\"ltx_text\" id=\"S3.T1.4.4.1.1\" style=\"font-size:90%;\">6.84 </span><span class=\"ltx_text\" id=\"S3.T1.4.4.1.2\" style=\"font-size:90%;\"> 0.22</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.5.5.2\">\n<span class=\"ltx_text\" id=\"S3.T1.5.5.2.1\" style=\"font-size:90%;\">12.91 </span><span class=\"ltx_text\" id=\"S3.T1.5.5.2.2\" style=\"font-size:90%;\"> 0.47</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.5.5.6\"><span class=\"ltx_text\" id=\"S3.T1.5.5.6.1\" style=\"font-size:90%;\">6.84</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.5.5.7\"><span class=\"ltx_text\" id=\"S3.T1.5.5.7.1\" style=\"font-size:90%;\">12.91</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.5.5.8\"><span class=\"ltx_text\" id=\"S3.T1.5.5.8.1\" style=\"font-size:90%;\">6.84</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.5.5.9\"><span class=\"ltx_text\" id=\"S3.T1.5.5.9.1\" style=\"font-size:90%;\">12.91</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.5.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.5.7.1\"><span class=\"ltx_text\" id=\"S3.T1.5.7.1.1\" style=\"font-size:90%;\">MAE</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S3.T1.5.7.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T1.5.7.2.1\">\n<span class=\"ltx_p\" id=\"S3.T1.5.7.2.1.1\"><span class=\"ltx_text\" id=\"S3.T1.5.7.2.1.1.1\" style=\"font-size:90%;\">K600</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.5.7.3\"><span class=\"ltx_text\" id=\"S3.T1.5.7.3.1\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.5.7.4\"><span class=\"ltx_text\" id=\"S3.T1.5.7.4.1\" style=\"font-size:90%;\">7.54</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.5.7.5\"><span class=\"ltx_text\" id=\"S3.T1.5.7.5.1\" style=\"font-size:90%;\">13.88</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.5.7.6\"><span class=\"ltx_text\" id=\"S3.T1.5.7.6.1\" style=\"font-size:90%;\">7.54</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.5.7.7\"><span class=\"ltx_text\" id=\"S3.T1.5.7.7.1\" style=\"font-size:90%;\">13.88</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.5.7.8\"><span class=\"ltx_text\" id=\"S3.T1.5.7.8.1\" style=\"font-size:90%;\">7.54</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.5.7.9\"><span class=\"ltx_text\" id=\"S3.T1.5.7.9.1\" style=\"font-size:90%;\">13.88</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.5.8\">\n<td class=\"ltx_td\" id=\"S3.T1.5.8.1\"></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T1.5.8.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T1.5.8.2.1\">\n<span class=\"ltx_p\" id=\"S3.T1.5.8.2.1.1\"><span class=\"ltx_text\" id=\"S3.T1.5.8.2.1.1.1\" style=\"font-size:90%;\">K600</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.8.3\"><span class=\"ltx_text\" id=\"S3.T1.5.8.3.1\" style=\"font-size:90%;\">\u2713</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.8.4\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.8.4.1\" style=\"font-size:90%;\">5.69</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.8.5\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.8.5.1\" style=\"font-size:90%;\">11.34</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.8.6\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.8.6.1\" style=\"font-size:90%;\">5.95</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.8.7\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.8.7.1\" style=\"font-size:90%;\">12.55</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.8.8\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.8.8.1\" style=\"font-size:90%;\">5.73</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.8.9\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.8.9.1\" style=\"font-size:90%;\">12.04</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.5.9\">\n<td class=\"ltx_td\" id=\"S3.T1.5.9.1\"></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T1.5.9.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T1.5.9.2.1\">\n<span class=\"ltx_p\" id=\"S3.T1.5.9.2.1.1\"><span class=\"ltx_text\" id=\"S3.T1.5.9.2.1.1.1\" style=\"font-size:90%;\">VC2</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.9.3\"><span class=\"ltx_text\" id=\"S3.T1.5.9.3.1\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.9.4\"><span class=\"ltx_text\" id=\"S3.T1.5.9.4.1\" style=\"font-size:90%;\">5.28</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.9.5\"><span class=\"ltx_text\" id=\"S3.T1.5.9.5.1\" style=\"font-size:90%;\">11.51</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.9.6\"><span class=\"ltx_text\" id=\"S3.T1.5.9.6.1\" style=\"font-size:90%;\">5.28</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.9.7\"><span class=\"ltx_text\" id=\"S3.T1.5.9.7.1\" style=\"font-size:90%;\">11.51</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.9.8\"><span class=\"ltx_text\" id=\"S3.T1.5.9.8.1\" style=\"font-size:90%;\">5.28</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.9.9\"><span class=\"ltx_text\" id=\"S3.T1.5.9.9.1\" style=\"font-size:90%;\">11.51</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.5.10\">\n<td class=\"ltx_td\" id=\"S3.T1.5.10.1\"></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T1.5.10.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T1.5.10.2.1\">\n<span class=\"ltx_p\" id=\"S3.T1.5.10.2.1.1\"><span class=\"ltx_text\" id=\"S3.T1.5.10.2.1.1.1\" style=\"font-size:90%;\">VC2</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.10.3\"><span class=\"ltx_text\" id=\"S3.T1.5.10.3.1\" style=\"font-size:90%;\">\u2713</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.10.4\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.10.4.1\" style=\"font-size:90%;\">5.11</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.10.5\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.10.5.1\" style=\"font-size:90%;\">11.12</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.10.6\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.10.6.1\" style=\"font-size:90%;\">5.56</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.10.7\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.10.7.1\" style=\"font-size:90%;\">10.42</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.10.8\"><span class=\"ltx_text\" id=\"S3.T1.5.10.8.1\" style=\"font-size:90%;\">5.64</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.10.9\"><span class=\"ltx_text\" id=\"S3.T1.5.10.9.1\" style=\"font-size:90%;\">12.46</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.5.11\">\n<td class=\"ltx_td\" id=\"S3.T1.5.11.1\"></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T1.5.11.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T1.5.11.2.1\">\n<span class=\"ltx_p\" id=\"S3.T1.5.11.2.1.1\"><span class=\"ltx_text\" id=\"S3.T1.5.11.2.1.1.1\" style=\"font-size:90%;\">LRS3</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.11.3\"><span class=\"ltx_text\" id=\"S3.T1.5.11.3.1\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.11.4\"><span class=\"ltx_text\" id=\"S3.T1.5.11.4.1\" style=\"font-size:90%;\">4.73</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.11.5\"><span class=\"ltx_text\" id=\"S3.T1.5.11.5.1\" style=\"font-size:90%;\">10.27</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.11.6\"><span class=\"ltx_text\" id=\"S3.T1.5.11.6.1\" style=\"font-size:90%;\">4.73</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.11.7\"><span class=\"ltx_text\" id=\"S3.T1.5.11.7.1\" style=\"font-size:90%;\">10.27</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.11.8\"><span class=\"ltx_text\" id=\"S3.T1.5.11.8.1\" style=\"font-size:90%;\">4.73</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.11.9\"><span class=\"ltx_text\" id=\"S3.T1.5.11.9.1\" style=\"font-size:90%;\">10.27</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.5.12\">\n<td class=\"ltx_td\" id=\"S3.T1.5.12.1\"></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T1.5.12.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T1.5.12.2.1\">\n<span class=\"ltx_p\" id=\"S3.T1.5.12.2.1.1\"><span class=\"ltx_text\" id=\"S3.T1.5.12.2.1.1.1\" style=\"font-size:90%;\">LRS3</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.12.3\"><span class=\"ltx_text\" id=\"S3.T1.5.12.3.1\" style=\"font-size:90%;\">\u2713</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.12.4\"><span class=\"ltx_text\" id=\"S3.T1.5.12.4.1\" style=\"font-size:90%;\">5.61</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.12.5\"><span class=\"ltx_text\" id=\"S3.T1.5.12.5.1\" style=\"font-size:90%;\">10.85</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.12.6\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S3.T1.5.12.6.1\" style=\"font-size:90%;\">4.21</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.12.7\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" id=\"S3.T1.5.12.7.1\" style=\"font-size:90%;\">9.53</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.12.8\"><span class=\"ltx_text\" id=\"S3.T1.5.12.8.1\" style=\"font-size:90%;\">5.32</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.12.9\"><span class=\"ltx_text\" id=\"S3.T1.5.12.9.1\" style=\"font-size:90%;\">10.33</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.5.13\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.5.13.1\"><span class=\"ltx_text\" id=\"S3.T1.5.13.1.1\" style=\"font-size:90%;\">CLR</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S3.T1.5.13.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T1.5.13.2.1\">\n<span class=\"ltx_p\" id=\"S3.T1.5.13.2.1.1\"><span class=\"ltx_text\" id=\"S3.T1.5.13.2.1.1.1\" style=\"font-size:90%;\">K600</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.5.13.3\"><span class=\"ltx_text\" id=\"S3.T1.5.13.3.1\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.5.13.4\"><span class=\"ltx_text\" id=\"S3.T1.5.13.4.1\" style=\"font-size:90%;\">6.85</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.5.13.5\"><span class=\"ltx_text\" id=\"S3.T1.5.13.5.1\" style=\"font-size:90%;\">12.92</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.5.13.6\"><span class=\"ltx_text\" id=\"S3.T1.5.13.6.1\" style=\"font-size:90%;\">6.85</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.5.13.7\"><span class=\"ltx_text\" id=\"S3.T1.5.13.7.1\" style=\"font-size:90%;\">12.92</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.5.13.8\"><span class=\"ltx_text\" id=\"S3.T1.5.13.8.1\" style=\"font-size:90%;\">6.85</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.5.13.9\"><span class=\"ltx_text\" id=\"S3.T1.5.13.9.1\" style=\"font-size:90%;\">12.92</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.5.14\">\n<td class=\"ltx_td\" id=\"S3.T1.5.14.1\"></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T1.5.14.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T1.5.14.2.1\">\n<span class=\"ltx_p\" id=\"S3.T1.5.14.2.1.1\"><span class=\"ltx_text\" id=\"S3.T1.5.14.2.1.1.1\" style=\"font-size:90%;\">K600</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.14.3\"><span class=\"ltx_text\" id=\"S3.T1.5.14.3.1\" style=\"font-size:90%;\">\u2713</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.14.4\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.14.4.1\" style=\"font-size:90%;\">5.02</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.14.5\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.14.5.1\" style=\"font-size:90%;\">10.85</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.14.6\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.14.6.1\" style=\"font-size:90%;\">4.72</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.14.7\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.14.7.1\" style=\"font-size:90%;\">10.62</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.14.8\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.14.8.1\" style=\"font-size:90%;\">4.65</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.14.9\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.14.9.1\" style=\"font-size:90%;\">10.41</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.5.15\">\n<td class=\"ltx_td\" id=\"S3.T1.5.15.1\"></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T1.5.15.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T1.5.15.2.1\">\n<span class=\"ltx_p\" id=\"S3.T1.5.15.2.1.1\"><span class=\"ltx_text\" id=\"S3.T1.5.15.2.1.1.1\" style=\"font-size:90%;\">VC2</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.15.3\"><span class=\"ltx_text\" id=\"S3.T1.5.15.3.1\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.15.4\"><span class=\"ltx_text\" id=\"S3.T1.5.15.4.1\" style=\"font-size:90%;\">6.47</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.15.5\"><span class=\"ltx_text\" id=\"S3.T1.5.15.5.1\" style=\"font-size:90%;\">12.42</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.15.6\"><span class=\"ltx_text\" id=\"S3.T1.5.15.6.1\" style=\"font-size:90%;\">6.47</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.15.7\"><span class=\"ltx_text\" id=\"S3.T1.5.15.7.1\" style=\"font-size:90%;\">12.42</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.15.8\"><span class=\"ltx_text\" id=\"S3.T1.5.15.8.1\" style=\"font-size:90%;\">6.47</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.15.9\"><span class=\"ltx_text\" id=\"S3.T1.5.15.9.1\" style=\"font-size:90%;\">12.42</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.5.16\">\n<td class=\"ltx_td\" id=\"S3.T1.5.16.1\"></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T1.5.16.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T1.5.16.2.1\">\n<span class=\"ltx_p\" id=\"S3.T1.5.16.2.1.1\"><span class=\"ltx_text\" id=\"S3.T1.5.16.2.1.1.1\" style=\"font-size:90%;\">VC2</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.16.3\"><span class=\"ltx_text\" id=\"S3.T1.5.16.3.1\" style=\"font-size:90%;\">\u2713</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.16.4\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.16.4.1\" style=\"font-size:90%;\">6.43</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.16.5\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.16.5.1\" style=\"font-size:90%;\">12.31</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.16.6\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.16.6.1\" style=\"font-size:90%;\">5.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.16.7\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.16.7.1\" style=\"font-size:90%;\">10.61</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.16.8\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.16.8.1\" style=\"font-size:90%;\">4.62</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.16.9\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.16.9.1\" style=\"font-size:90%;\">10.77</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.5.17\">\n<td class=\"ltx_td\" id=\"S3.T1.5.17.1\"></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T1.5.17.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T1.5.17.2.1\">\n<span class=\"ltx_p\" id=\"S3.T1.5.17.2.1.1\"><span class=\"ltx_text\" id=\"S3.T1.5.17.2.1.1.1\" style=\"font-size:90%;\">LRS3</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.17.3\"><span class=\"ltx_text\" id=\"S3.T1.5.17.3.1\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.17.4\"><span class=\"ltx_text\" id=\"S3.T1.5.17.4.1\" style=\"font-size:90%;\">6.35</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.17.5\"><span class=\"ltx_text\" id=\"S3.T1.5.17.5.1\" style=\"font-size:90%;\">12.12</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.17.6\"><span class=\"ltx_text\" id=\"S3.T1.5.17.6.1\" style=\"font-size:90%;\">6.35</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.17.7\"><span class=\"ltx_text\" id=\"S3.T1.5.17.7.1\" style=\"font-size:90%;\">12.12</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.17.8\"><span class=\"ltx_text\" id=\"S3.T1.5.17.8.1\" style=\"font-size:90%;\">6.35</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.17.9\"><span class=\"ltx_text\" id=\"S3.T1.5.17.9.1\" style=\"font-size:90%;\">12.12</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.5.18\">\n<td class=\"ltx_td\" id=\"S3.T1.5.18.1\"></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T1.5.18.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T1.5.18.2.1\">\n<span class=\"ltx_p\" id=\"S3.T1.5.18.2.1.1\"><span class=\"ltx_text\" id=\"S3.T1.5.18.2.1.1.1\" style=\"font-size:90%;\">LRS3</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.18.3\"><span class=\"ltx_text\" id=\"S3.T1.5.18.3.1\" style=\"font-size:90%;\">\u2713</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.18.4\"><span class=\"ltx_text\" id=\"S3.T1.5.18.4.1\" style=\"font-size:90%;\">6.74</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.18.5\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.18.5.1\" style=\"font-size:90%;\">10.59</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.18.6\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.18.6.1\" style=\"font-size:90%;\">5.84</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.18.7\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.18.7.1\" style=\"font-size:90%;\">11.33</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.18.8\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.18.8.1\" style=\"font-size:90%;\">6.01</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.18.9\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.18.9.1\" style=\"font-size:90%;\">10.13</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.5.19\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.5.19.1\"><span class=\"ltx_text\" id=\"S3.T1.5.19.1.1\" style=\"font-size:90%;\">MAE + CLR</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S3.T1.5.19.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T1.5.19.2.1\">\n<span class=\"ltx_p\" id=\"S3.T1.5.19.2.1.1\"><span class=\"ltx_text\" id=\"S3.T1.5.19.2.1.1.1\" style=\"font-size:90%;\">K600</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.5.19.3\"><span class=\"ltx_text\" id=\"S3.T1.5.19.3.1\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.5.19.4\"><span class=\"ltx_text\" id=\"S3.T1.5.19.4.1\" style=\"font-size:90%;\">5.56</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.5.19.5\"><span class=\"ltx_text\" id=\"S3.T1.5.19.5.1\" style=\"font-size:90%;\">11.91</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.5.19.6\"><span class=\"ltx_text\" id=\"S3.T1.5.19.6.1\" style=\"font-size:90%;\">5.56</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.5.19.7\"><span class=\"ltx_text\" id=\"S3.T1.5.19.7.1\" style=\"font-size:90%;\">11.91</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.5.19.8\"><span class=\"ltx_text\" id=\"S3.T1.5.19.8.1\" style=\"font-size:90%;\">5.56</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.5.19.9\"><span class=\"ltx_text\" id=\"S3.T1.5.19.9.1\" style=\"font-size:90%;\">11.91</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.5.20\">\n<td class=\"ltx_td\" id=\"S3.T1.5.20.1\"></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T1.5.20.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T1.5.20.2.1\">\n<span class=\"ltx_p\" id=\"S3.T1.5.20.2.1.1\"><span class=\"ltx_text\" id=\"S3.T1.5.20.2.1.1.1\" style=\"font-size:90%;\">K600</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.20.3\"><span class=\"ltx_text\" id=\"S3.T1.5.20.3.1\" style=\"font-size:90%;\">\u2713</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.20.4\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.20.4.1\" style=\"font-size:90%;\">5.02</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.20.5\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.20.5.1\" style=\"font-size:90%;\">11.68</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.20.6\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.20.6.1\" style=\"font-size:90%;\">5.23</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.20.7\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.20.7.1\" style=\"font-size:90%;\">11.37</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.20.8\"><span class=\"ltx_text\" id=\"S3.T1.5.20.8.1\" style=\"font-size:90%;\">6.39</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.20.9\"><span class=\"ltx_text\" id=\"S3.T1.5.20.9.1\" style=\"font-size:90%;\">12.03</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.5.21\">\n<td class=\"ltx_td\" id=\"S3.T1.5.21.1\"></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T1.5.21.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T1.5.21.2.1\">\n<span class=\"ltx_p\" id=\"S3.T1.5.21.2.1.1\"><span class=\"ltx_text\" id=\"S3.T1.5.21.2.1.1.1\" style=\"font-size:90%;\">VC2</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.21.3\"><span class=\"ltx_text\" id=\"S3.T1.5.21.3.1\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.21.4\"><span class=\"ltx_text\" id=\"S3.T1.5.21.4.1\" style=\"font-size:90%;\">6.75</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.21.5\"><span class=\"ltx_text\" id=\"S3.T1.5.21.5.1\" style=\"font-size:90%;\">12.11</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.21.6\"><span class=\"ltx_text\" id=\"S3.T1.5.21.6.1\" style=\"font-size:90%;\">6.75</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.21.7\"><span class=\"ltx_text\" id=\"S3.T1.5.21.7.1\" style=\"font-size:90%;\">12.11</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.21.8\"><span class=\"ltx_text\" id=\"S3.T1.5.21.8.1\" style=\"font-size:90%;\">6.75</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.21.9\"><span class=\"ltx_text\" id=\"S3.T1.5.21.9.1\" style=\"font-size:90%;\">12.11</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.5.22\">\n<td class=\"ltx_td\" id=\"S3.T1.5.22.1\"></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T1.5.22.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T1.5.22.2.1\">\n<span class=\"ltx_p\" id=\"S3.T1.5.22.2.1.1\"><span class=\"ltx_text\" id=\"S3.T1.5.22.2.1.1.1\" style=\"font-size:90%;\">VC2</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.22.3\"><span class=\"ltx_text\" id=\"S3.T1.5.22.3.1\" style=\"font-size:90%;\">\u2713</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.22.4\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.22.4.1\" style=\"font-size:90%;\">5.36</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.22.5\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.22.5.1\" style=\"font-size:90%;\">11.22</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.22.6\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.22.6.1\" style=\"font-size:90%;\">4.77</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.22.7\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.22.7.1\" style=\"font-size:90%;\">10.84</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.22.8\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.22.8.1\" style=\"font-size:90%;\">5.03</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.22.9\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.22.9.1\" style=\"font-size:90%;\">10.73</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.5.23\">\n<td class=\"ltx_td\" id=\"S3.T1.5.23.1\"></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T1.5.23.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T1.5.23.2.1\">\n<span class=\"ltx_p\" id=\"S3.T1.5.23.2.1.1\"><span class=\"ltx_text\" id=\"S3.T1.5.23.2.1.1.1\" style=\"font-size:90%;\">LRS3</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.23.3\"><span class=\"ltx_text\" id=\"S3.T1.5.23.3.1\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.23.4\"><span class=\"ltx_text\" id=\"S3.T1.5.23.4.1\" style=\"font-size:90%;\">7.51</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.23.5\"><span class=\"ltx_text\" id=\"S3.T1.5.23.5.1\" style=\"font-size:90%;\">12.54</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.23.6\"><span class=\"ltx_text\" id=\"S3.T1.5.23.6.1\" style=\"font-size:90%;\">7.51</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.23.7\"><span class=\"ltx_text\" id=\"S3.T1.5.23.7.1\" style=\"font-size:90%;\">12.54</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.23.8\"><span class=\"ltx_text\" id=\"S3.T1.5.23.8.1\" style=\"font-size:90%;\">7.51</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.5.23.9\"><span class=\"ltx_text\" id=\"S3.T1.5.23.9.1\" style=\"font-size:90%;\">12.54</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.5.24\">\n<td class=\"ltx_td ltx_border_bb\" id=\"S3.T1.5.24.1\"></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S3.T1.5.24.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T1.5.24.2.1\">\n<span class=\"ltx_p\" id=\"S3.T1.5.24.2.1.1\"><span class=\"ltx_text\" id=\"S3.T1.5.24.2.1.1.1\" style=\"font-size:90%;\">LRS3</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.5.24.3\"><span class=\"ltx_text\" id=\"S3.T1.5.24.3.1\" style=\"font-size:90%;\">\u2713</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.5.24.4\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.24.4.1\" style=\"font-size:90%;\">7.16</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.5.24.5\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.24.5.1\" style=\"font-size:90%;\">12.29</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.5.24.6\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.24.6.1\" style=\"font-size:90%;\">5.08</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.5.24.7\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.24.7.1\" style=\"font-size:90%;\">11.13</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.5.24.8\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.24.8.1\" style=\"font-size:90%;\">6.17</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.5.24.9\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.5.24.9.1\" style=\"font-size:90%;\">12.32</span></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Performance (WER) on the Librispeech test-clean and test-other datasets with and without mid-training, and across Kinetics (K600), Voxceleb2 (VC2) and LRS-3 pre-training datasets. MT: With mid-training. MAE: Masked Autoencoding, CLR: Contrastive Learning. PT: Pre-Training. <span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T1.10.1\">Underline</span> denote the consistent WER drop through mid-training alone across the 3 datasets and PT strategies. We observe that translation Mid-training task benefit the global representations of CLR more consistently compared to MAE. Overall, it improves the \u2018only pre-trained\u2019 performance by aligning the learnt features towards the downstream task through auxiliary translation task. Further, italian language is the most effective as a mid-training task, suggesting that the languages that are complimentary to English may be more useful than others. </figcaption>\n</figure>",
            "capture": "Table 1: Performance (WER) on the Librispeech test-clean and test-other datasets with and without mid-training, and across Kinetics (K600), Voxceleb2 (VC2) and LRS-3 pre-training datasets. MT: With mid-training. MAE: Masked Autoencoding, CLR: Contrastive Learning. PT: Pre-Training. Underline denote the consistent WER drop through mid-training alone across the 3 datasets and PT strategies. We observe that translation Mid-training task benefit the global representations of CLR more consistently compared to MAE. Overall, it improves the \u2018only pre-trained\u2019 performance by aligning the learnt features towards the downstream task through auxiliary translation task. Further, italian language is the most effective as a mid-training task, suggesting that the languages that are complimentary to English may be more useful than others. "
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.19822v1_figure_1.png",
            "caption": "Figure 1: Overview of multi-modal training strategy. Raw audio and video features are extracted from source data. These features are then passed through the audio and video encoders to get features which are further processed as (1) MAE: the masked encoded features are reconstructed through a common decoder successively and are compared against original input using L2 loss, (2) CLR: contrastive learning applied to spatio-temporally pooled audio and video encoded features, and (3) the trained audio encoder is further used for mid-training (translation task) and then for downstream tasks."
        },
        "2": {
            "figure_path": "2403.19822v1_figure_2.png",
            "caption": "Figure 2: Aggregate (dataset/language) relative performance improvement (higher is better) under mid-training for MAE + CLR on SUPERB. KS: Keyword spotting, IC: Intent Classification, PR: Phoneme Recognition, SD: Speaker Diarization. We observe consistent improvement in performance due to translation mid-training on tasks which require local feature information (KS, IC and PR) whereas global task SD observe a decrease in performance. It further shows that translation mid-training task enhances the pre-trained model\u2019s performance for local feature tasks while hurts the global feature task."
        },
        "3": {
            "figure_path": "2403.19822v1_figure_3.png",
            "caption": "Figure 3: Average relative WER improvement on the Librispeech test-clean and test-other datasets with mid-training to show the effect of pre-training methods (left), mid-training translation pairs (center), and pre-training datasets (right). Translation mid-training improves upon CLR pre-training the most as it aligns its features for the local information required for ASR. Among the translation languages, Italian provides the best improvement, suggesting a complimentary language to English gains the most compared to languages that shares its roots with English. Models pre-trained on non-speech dataset Kinetics benefit the most from translation mid-training followed by noisy speech dataset Voxceleb2 and then clean speech dataset LRS3."
        }
    },
    "references": [
        {
            "1": {
                "title": "Tensorflow: A system for large-scale machine learning.",
                "author": "Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. 2016.",
                "venue": "In 12th USENIX symposium on operating systems design and implementation (OSDI 16), pages 265\u2013283.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Lrs3-ted: a large-scale dataset for visual speech recognition.",
                "author": "Triantafyllos Afouras, Joon Son Chung, and Andrew Zisserman. 2018.",
                "venue": "arXiv:1809.00496.",
                "url": null
            }
        },
        {
            "3": {
                "title": "Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text.",
                "author": "Hassan Akbari, Linagzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and Boqing Gong. 2021.",
                "venue": "arXiv:2104.11178.",
                "url": null
            }
        },
        {
            "4": {
                "title": "Self-supervised multimodal versatile networks.",
                "author": "Jean-Baptiste Alayrac, Adria Recasens, Rosalia Schneider, Relja Arandjelovic, Jason Ramapuram, Jeffrey De Fauw, Lucas Smaira, Sander Dieleman, and Andrew Zisserman. 2020.",
                "venue": "NeurIPS, 2(6):7.",
                "url": null
            }
        },
        {
            "5": {
                "title": "Vivit: A video vision transformer.",
                "author": "Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu\u010di\u0107, and Cordelia Schmid. 2021.",
                "venue": "arXiv:2103.15691.",
                "url": null
            }
        },
        {
            "6": {
                "title": "Layer normalization.",
                "author": "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016.",
                "venue": "arXiv:1607.06450.",
                "url": null
            }
        },
        {
            "7": {
                "title": "Wav2vec 2.0: A framework for self-supervised learning of speech representations.",
                "author": "Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. 2020.",
                "venue": "Advances in Neural Information Processing Systems, 33:12449\u201312460.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Joint unsupervised and supervised training for multilingual asr.",
                "author": "Junwen Bai, Bo Li, Yu Zhang, Ankur Bapna, Nikhil Siddhartha, Khe Chai Sim, and Tara N Sainath. 2022.",
                "venue": "In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6402\u20136406. IEEE.",
                "url": null
            }
        },
        {
            "9": {
                "title": "Is space-time attention all you need for video understanding?",
                "author": "Gedas Bertasius, Heng Wang, and Lorenzo Torresani. 2021.",
                "venue": "arXiv:2102.05095.",
                "url": null
            }
        },
        {
            "10": {
                "title": "Data diversity matters for robust instruction tuning.",
                "author": "Alexander Bukharin and Tuo Zhao. 2023.",
                "venue": "arXiv preprint arXiv:2311.14736.",
                "url": null
            }
        },
        {
            "11": {
                "title": "A short note about kinetics-600.",
                "author": "Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, and Andrew Zisserman. 2018.",
                "venue": "arXiv:1808.01340.",
                "url": null
            }
        },
        {
            "12": {
                "title": "Quo vadis, action recognition? a new model and the kinetics dataset.",
                "author": "Joao Carreira and Andrew Zisserman. 2017.",
                "venue": "In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6299\u20136308.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Content-context factorized representations for automated speech recognition.",
                "author": "David M. Chan and Shalini Ghosh. 2022.",
                "venue": null,
                "url": "http://arxiv.org/abs/2205.09872"
            }
        },
        {
            "14": {
                "title": "Multi-modal pre-training for automated speech recognition.",
                "author": "David M Chan, Shalini Ghosh, Debmalya Chakrabarty, and Bj\u00f6rn Hoffmeister. 2022.",
                "venue": "In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 246\u2013250. IEEE.",
                "url": null
            }
        },
        {
            "15": {
                "title": "Using external off-policy speech-to-text mappings in contextual end-to-end automated speech recognition.",
                "author": "David M. Chan, Shalini Ghosh, Ariya Rastrow, and Bj\u00f6rn Hoffmeister. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2301.02736"
            }
        },
        {
            "16": {
                "title": "Speechstew: Simply mix all available speech recognition data to train one large neural network.",
                "author": "William Chan, Daniel Park, Chris Lee, Yu Zhang, Quoc Le, and Mohammad Norouzi. 2021.",
                "venue": "arXiv:2104.02133.",
                "url": null
            }
        },
        {
            "17": {
                "title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing.",
                "author": "Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al. 2022.",
                "venue": "IEEE Journal of Selected Topics in Signal Processing, 16(6):1505\u20131518.",
                "url": null
            }
        },
        {
            "18": {
                "title": "A simple framework for contrastive learning of visual representations.",
                "author": "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020.",
                "venue": "In International conference on machine learning, pages 1597\u20131607. PMLR.",
                "url": null
            }
        },
        {
            "19": {
                "title": "Rnn-t models fail to generalize to out-of-domain audio: Causes and solutions.",
                "author": "Chung-Cheng Chiu, Arun Narayanan, Wei Han, Rohit Prabhavalkar, Yu Zhang, Navdeep Jaitly, Ruoming Pang, Tara N Sainath, Patrick Nguyen, Liangliang Cao, et al. 2021.",
                "venue": "In 2021 IEEE Spoken Language Technology Workshop (SLT), pages 873\u2013880. IEEE.",
                "url": null
            }
        },
        {
            "20": {
                "title": "Palm: Scaling language modeling with pathways.",
                "author": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022.",
                "venue": "arXiv:2204.02311.",
                "url": null
            }
        },
        {
            "21": {
                "title": "Voxceleb2: Deep speaker recognition.",
                "author": "Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. 2018.",
                "venue": "arXiv:1806.05622.",
                "url": null
            }
        },
        {
            "22": {
                "title": "W2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training.",
                "author": "Yu-An Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming Pang, and Yonghui Wu. 2021.",
                "venue": "In 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 244\u2013250. IEEE.",
                "url": null
            }
        },
        {
            "23": {
                "title": "Bert: Pre-training of deep bidirectional transformers for language understanding.",
                "author": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.",
                "venue": "arXiv:1810.04805.",
                "url": null
            }
        },
        {
            "24": {
                "title": "MuST-C: a Multilingual Speech Translation Corpus.",
                "author": "Mattia A. Di Gangi, Roldano Cattoni, Luisa Bentivogli, Matteo Negri, and Marco Turchi. 2019.",
                "venue": "In NAACL 2019, pages 2012\u20132017.",
                "url": "https://doi.org/10.18653/v1/N19-1202"
            }
        },
        {
            "25": {
                "title": "An image is worth 16x16 words: Transformers for image recognition at scale.",
                "author": "Alexey Dosovitskiy, Lucas Beyer, et al. 2020.",
                "venue": "arXiv:2010.11929.",
                "url": null
            }
        },
        {
            "26": {
                "title": "Masked autoencoders as spatiotemporal learners.",
                "author": "Christoph Feichtenhofer, Haoqi Fan, Yanghao Li, and Kaiming He. 2022.",
                "venue": "arXiv:2205.09113.",
                "url": null
            }
        },
        {
            "27": {
                "title": "Slowfast networks for video recognition.",
                "author": "Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. 2019.",
                "venue": "In Proceedings of the IEEE/CVF international conference on computer vision, pages 6202\u20136211.",
                "url": null
            }
        },
        {
            "28": {
                "title": "Trusted machine learning for probabilistic models.",
                "author": "Shalini Ghosh, Patrick Lincoln, Ashish Tiwari, and Xiaojin Zhu. 2016.",
                "venue": "ICML Workshop on Reliable Machine Learning in the Wild.",
                "url": null
            }
        },
        {
            "29": {
                "title": "Vision models are more robust and fair when pretrained on uncurated images without supervision.",
                "author": "Priya Goyal, Quentin Duval, Isaac Seessel, Mathilde Caron, Ishan Misra, Levent Sagun, Armand Joulin, and Piotr Bojanowski. 2022.",
                "venue": "arXiv preprint arXiv:2202.08360.",
                "url": null
            }
        },
        {
            "30": {
                "title": "Speech recognition with deep recurrent neural networks.",
                "author": "Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. 2013.",
                "venue": "In 2013 IEEE international conference on acoustics, speech and signal processing, pages 6645\u20136649. Ieee.",
                "url": null
            }
        },
        {
            "31": {
                "title": "Conformer: Convolution-augmented transformer for speech recognition.",
                "author": "Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et al. 2020.",
                "venue": "arXiv:2005.08100.",
                "url": null
            }
        },
        {
            "32": {
                "title": "Contextnet: Improving convolutional neural networks for automatic speech recognition with global context.",
                "author": "Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming Pang, and Yonghui Wu. 2020.",
                "venue": "arXiv:2005.03191.",
                "url": null
            }
        },
        {
            "33": {
                "title": "Training compute-optimal large language models.",
                "author": "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022.",
                "venue": "arXiv:2203.15556.",
                "url": null
            }
        },
        {
            "34": {
                "title": "U-hubert: Unified mixed-modal speech pretraining and zero-shot transfer to unlabeled modality.",
                "author": "Wei-Ning Hsu and Bowen Shi. 2022.",
                "venue": "In Advances in Neural Information Processing Systems.",
                "url": null
            }
        },
        {
            "35": {
                "title": "Hubert: How much can a bad teacher benefit asr pre-training?",
                "author": "Wei-Ning Hsu, Yao-Hung Hubert Tsai, Benjamin Bolte, Ruslan Salakhutdinov, and Abdelrahman Mohamed. 2021.",
                "venue": "In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6533\u20136537. IEEE.",
                "url": null
            }
        },
        {
            "36": {
                "title": "Unit: Multimodal multitask learning with a unified transformer.",
                "author": "Ronghang Hu and Amanpreet Singh. 2021.",
                "venue": "arXiv:2102.10772.",
                "url": null
            }
        },
        {
            "37": {
                "title": "Damex: Dataset-aware mixture-of-experts for visual understanding of mixture-of-datasets.",
                "author": "Yash Jain, Harkirat Behl, Zsolt Kira, and Vibhav Vineet. 2024.",
                "venue": "Advances in Neural Information Processing Systems, 36.",
                "url": null
            }
        },
        {
            "38": {
                "title": "Deploying self-supervised learning in the wild for hybrid automatic speech recognition.",
                "author": "Mostafa Karimi, Changliang Liu, Kenichi Kumatani, Yao Qian, Tianyu Wu, and Jian Wu. 2022.",
                "venue": "arXiv:2205.08598.",
                "url": null
            }
        },
        {
            "39": {
                "title": "A comparative study on transformer vs rnn in speech applications.",
                "author": "Shigeki Karita, Nanxin Chen, Tomoki Hayashi, Takaaki Hori, Hirofumi Inaguma, Ziyan Jiang, Masao Someki, Nelson Enrique Yalta Soplin, Ryuichi Yamamoto, Xiaofei Wang, et al. 2019.",
                "venue": "In 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 449\u2013456. IEEE.",
                "url": null
            }
        },
        {
            "40": {
                "title": "Droid: A large-scale in-the-wild robot manipulation dataset.",
                "author": "Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, et al. 2024.",
                "venue": "arXiv preprint arXiv:2403.12945.",
                "url": null
            }
        },
        {
            "41": {
                "title": "Adam: A method for stochastic optimization.",
                "author": "Diederik P Kingma and Jimmy Ba. 2014.",
                "venue": "arXiv:1412.6980.",
                "url": null
            }
        },
        {
            "42": {
                "title": "Learning to discern: Imitating heterogeneous human demonstrations with preference and representation learning.",
                "author": "Sachit Kuhar, Shuo Cheng, Shivang Chopra, Matthew Bronars, and Danfei Xu. 2023.",
                "venue": "In Conference on Robot Learning, pages 1437\u20131449. PMLR.",
                "url": null
            }
        },
        {
            "43": {
                "title": "Multilingual speech recognition using knowledge transfer across learning processes.",
                "author": "Rimita Lahiri, Kenichi Kumatani, Eric Sun, and Yao Qian. 2021.",
                "venue": "arXiv:2110.07909.",
                "url": null
            }
        },
        {
            "44": {
                "title": "Developing rnn-t models surpassing high-performance hybrid models with customization capability.",
                "author": "Jinyu Li, Rui Zhao, Zhong Meng, Yanqing Liu, Wenning Wei, Sarangarajan Parthasarathy, Vadim Mazalov, Zhenghao Wang, Lei He, Sheng Zhao, et al. 2020.",
                "venue": "arXiv:2007.15188.",
                "url": null
            }
        },
        {
            "45": {
                "title": "Scalable and accurate self-supervised multimodal representation learning without aligned video and text data.",
                "author": "Vladislav Lialin, Stephen Rawls, David Chan, Shalini Ghosh, Anna Rumshisky, and Wael Hamza. 2023.",
                "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) Workshops.",
                "url": null
            }
        },
        {
            "46": {
                "title": "Rethinking evaluation in asr: Are our models robust enough?",
                "author": "Tatiana Likhomanenko, Qiantong Xu, Vineel Pratap, Paden Tomasello, Jacob Kahn, Gilad Avidov, Ronan Collobert, and Gabriel Synnaeve. 2020.",
                "venue": "arXiv:2010.11745.",
                "url": null
            }
        },
        {
            "47": {
                "title": "A unified framework for domain adaptation using metric learning on manifolds.",
                "author": "Sridhar Mahadevan, Bamdev Mishra, and Shalini Ghosh. 2018.",
                "venue": "CoRR, abs/1804.10834.",
                "url": null
            }
        },
        {
            "48": {
                "title": "What matters in learning from offline human demonstrations for robot manipulation.",
                "author": "Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei, Silvio Savarese, Yuke Zhu, and Roberto Mart\u00edn-Mart\u00edn. 2022.",
                "venue": "In Conference on Robot Learning, pages 1678\u20131690. PMLR.",
                "url": null
            }
        },
        {
            "49": {
                "title": "A teacher-student learning approach for unsupervised domain adaptation of sequence-trained asr models.",
                "author": "Vimal Manohar, Pegah Ghahremani, Daniel Povey, and Sanjeev Khudanpur. 2018.",
                "venue": "In 2018 IEEE Spoken Language Technology Workshop (SLT), pages 250\u2013257. IEEE.",
                "url": null
            }
        },
        {
            "50": {
                "title": "Unified modeling of multi-domain multi-device asr systems.",
                "author": "Soumyajit Mitra, Swayambhu Nath Ray, Bharat Padi, Arunasish Sen, Raghavendra Bilgi, Harish Arsikere, Shalini Ghosh, Ajay Srinivasamurthy, and Sri Garimella. 2022.",
                "venue": null,
                "url": "http://arxiv.org/abs/2205.06655"
            }
        },
        {
            "51": {
                "title": "Improving noise robustness of automatic speech recognition via parallel data and teacher-student learning.",
                "author": "Ladislav Mo\u0161ner, Minhua Wu, Anirudh Raju, Sree Hari Krishnan Parthasarathi, Kenichi Kumatani, Shiva Sundaram, Roland Maas, and Bj\u00f6rn Hoffmeister. 2019.",
                "venue": "In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6475\u20136479. IEEE.",
                "url": null
            }
        },
        {
            "52": {
                "title": "Toward domain-invariant speech recognition via large scale training.",
                "author": "Arun Narayanan, Ananya Misra, Khe Chai Sim, Golan Pundak, Anshuman Tripathi, Mohamed Elfeky, Parisa Haghani, Trevor Strohman, and Michiel Bacchiani. 2018.",
                "venue": "In 2018 IEEE Spoken Language Technology Workshop (SLT), pages 441\u2013447. IEEE.",
                "url": null
            }
        },
        {
            "53": {
                "title": "Representation learning with contrastive predictive coding.",
                "author": "Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018.",
                "venue": "arXiv:1807.03748.",
                "url": null
            }
        },
        {
            "54": {
                "title": "Librispeech: an asr corpus based on public domain audio books.",
                "author": "Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015.",
                "venue": "In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 5206\u20135210. IEEE.",
                "url": null
            }
        },
        {
            "55": {
                "title": "Specaugment: A simple data augmentation method for automatic speech recognition.",
                "author": "Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk, and Quoc V Le. 2019.",
                "venue": "arXiv:1904.08779.",
                "url": null
            }
        },
        {
            "56": {
                "title": "Learning problem-agnostic speech representations from multiple self-supervised tasks.",
                "author": "Santiago Pascual, Mirco Ravanelli, Joan Serra, Antonio Bonafonte, and Yoshua Bengio. 2019.",
                "venue": "arXiv:1904.03416.",
                "url": null
            }
        },
        {
            "57": {
                "title": "Combining subjective probabilities and data in training Markov Logic Networks.",
                "author": "Tivadar P\u00e1pai, Shalini Ghosh, and Henry Kautz. 2012.",
                "venue": "volume 7523, pages 90\u2013105.",
                "url": null
            }
        },
        {
            "58": {
                "title": "Learning transferable visual models from natural language supervision.",
                "author": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021.",
                "venue": "In International Conference on Machine Learning, pages 8748\u20138763. PMLR.",
                "url": null
            }
        },
        {
            "59": {
                "title": "Robust speech recognition via large-scale weak supervision.",
                "author": "Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2022.",
                "venue": "arXiv:2212.04356.",
                "url": null
            }
        },
        {
            "60": {
                "title": "On the connection between pre-training data diversity and fine-tuning robustness.",
                "author": "Vivek Ramanujan, Thao Nguyen, Sewoong Oh, Ali Farhadi, and Ludwig Schmidt. 2023.",
                "venue": "In Advances in Neural Information Processing Systems, volume 36, pages 66426\u201366437. Curran Associates, Inc.",
                "url": "https://proceedings.neurips.cc/paper_files/paper/2023/file/d1786f5246c67eefde011599d31b2006-Paper-Conference.pdf"
            }
        },
        {
            "61": {
                "title": "Contrastive learning of general-purpose audio representations.",
                "author": "Aaqib Saeed, David Grangier, and Neil Zeghidour. 2021.",
                "venue": "In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3875\u20133879. IEEE.",
                "url": null
            }
        },
        {
            "62": {
                "title": "Wav2vec: Unsupervised pre-training for speech recognition.",
                "author": "Steffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli. 2019.",
                "venue": "arXiv:1904.05862.",
                "url": null
            }
        },
        {
            "63": {
                "title": "Learning audio-visual speech representation by masked multimodal cluster prediction.",
                "author": "Bowen Shi, Wei-Ning Hsu, Kushal Lakhotia, and Abdelrahman Mohamed. 2022.",
                "venue": "arXiv:2201.02184.",
                "url": null
            }
        },
        {
            "64": {
                "title": "Videobert: A joint model for video and language representation learning.",
                "author": "Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. 2019.",
                "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7464\u20137473.",
                "url": null
            }
        },
        {
            "65": {
                "title": "End-to-end asr: from supervised to semi-supervised learning with modern architectures.",
                "author": "Gabriel Synnaeve, Qiantong Xu, Jacob Kahn, Tatiana Likhomanenko, Edouard Grave, Vineel Pratap, Anuroop Sriram, Vitaliy Liptchinsky, and Ronan Collobert. 2019.",
                "venue": "arXiv:1911.08460.",
                "url": null
            }
        },
        {
            "66": {
                "title": "Going deeper with convolutions.",
                "author": "Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. 2015.",
                "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1\u20139.",
                "url": null
            }
        },
        {
            "67": {
                "title": "Joint masked cpc and ctc training for asr.",
                "author": "Chaitanya Talnikar, Tatiana Likhomanenko, Ronan Collobert, and Gabriel Synnaeve. 2021.",
                "venue": "In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3045\u20133049. IEEE.",
                "url": null
            }
        },
        {
            "68": {
                "title": "Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training.",
                "author": "Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. 2022.",
                "venue": "arXiv:2203.12602.",
                "url": null
            }
        },
        {
            "69": {
                "title": "Multimodal transformer for unaligned multimodal language sequences.",
                "author": "Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, J Zico Kolter, Louis-Philippe Morency, and Ruslan Salakhutdinov. 2019.",
                "venue": "In Proceedings of the conference. Association for Computational Linguistics. Meeting, volume 2019, page 6558. NIH Public Access.",
                "url": null
            }
        },
        {
            "70": {
                "title": "An empirical study on robustness to spurious correlations using pre-trained language models.",
                "author": "Lifu Tu, Garima Lalwani, Spandana Gella, and He He. 2020.",
                "venue": "Transactions of the Association for Computational Linguistics, 8:621\u2013633.",
                "url": null
            }
        },
        {
            "71": {
                "title": "Metatheory of linguistics.",
                "author": "Kostiantyn Tyshchenko et al. 2000.",
                "venue": "Osnovy.",
                "url": null
            }
        },
        {
            "72": {
                "title": "Attention is all you need.",
                "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.",
                "venue": "Advances in neural information processing systems, 30.",
                "url": null
            }
        },
        {
            "73": {
                "title": "Unispeech: Unified speech representation learning with labeled and unlabeled data.",
                "author": "Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael Zeng, and Xuedong Huang. 2021a.",
                "venue": "In International Conference on Machine Learning, pages 10937\u201310947. PMLR.",
                "url": null
            }
        },
        {
            "74": {
                "title": "Multimodal self-supervised learning of general audio representations.",
                "author": "Luyu Wang, Pauline Luc, Adria Recasens, Jean-Baptiste Alayrac, and Aaron van den Oord. 2021b.",
                "venue": "arXiv:2104.12807.",
                "url": null
            }
        },
        {
            "75": {
                "title": "Student-teacher network learning with enhanced features.",
                "author": "Shinji Watanabe, Takaaki Hori, Jonathan Le Roux, and John R Hershey. 2017.",
                "venue": "In 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5275\u20135279. IEEE.",
                "url": null
            }
        },
        {
            "76": {
                "title": "Videoclip: Contrastive pre-training for zero-shot video-text understanding.",
                "author": "Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and Christoph Feichtenhofer. 2021.",
                "venue": "arXiv:2109.14084.",
                "url": null
            }
        },
        {
            "77": {
                "title": "Superb: Speech processing universal performance benchmark.",
                "author": "Shu-wen Yang, Po-Han Chi, Yung-Sung Chuang, Cheng-I Jeff Lai, Kushal Lakhotia, Yist Y Lin, Andy T Liu, Jiatong Shi, Xuankai Chang, Guan-Ting Lin, et al. 2021.",
                "venue": "arXiv:2105.01051.",
                "url": null
            }
        },
        {
            "78": {
                "title": "Regularize, expand and compress: Nonexpansive continual learning.",
                "author": "Jie Zhang, Junting Zhang, Shalini Ghosh, Dawei Li, Jingwen Zhu, Heming Zhang, and Yalin Wang. 2020a.",
                "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV).",
                "url": null
            }
        },
        {
            "79": {
                "title": "Transformer transducer: A streamable speech recognition model with transformer encoders and rnn-t loss.",
                "author": "Qian Zhang, Han Lu, Hasim Sak, Anshuman Tripathi, Erik McDermott, Stephen Koo, and Shankar Kumar. 2020b.",
                "venue": "In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7829\u20137833. IEEE.",
                "url": null
            }
        },
        {
            "80": {
                "title": "Google usm: Scaling automatic speech recognition beyond 100 languages.",
                "author": "Yu Zhang, Wei Han, James Qin, Yongqiang Wang, Ankur Bapna, Zhehuai Chen, Nanxin Chen, Bo Li, Vera Axelrod, Gary Wang, et al. 2023.",
                "venue": "arXiv preprint arXiv:2303.01037.",
                "url": null
            }
        },
        {
            "81": {
                "title": "Semi-supervised end-to-end asr via teacher-student learning with conditional posterior distribution.",
                "author": "Zi-qiang Zhang, Yan Song, Jian-shu Zhang, Ian Vince McLoughlin, and Li-rong Dai. 2020c.",
                "venue": "In INTERSPEECH, pages 3580\u20133584.",
                "url": null
            }
        },
        {
            "82": {
                "title": "Simple multi-dataset detection.",
                "author": "Xingyi Zhou, Vladlen Koltun, and Philipp Kr\u00e4henb\u00fchl. 2022.",
                "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 7571\u20137580.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.19822v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.4"
        ],
        "main_experiment_and_results_sections": [
            "4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.1",
            "3.2",
            "3.3",
            "4"
        ]
    },
    "research_context": {
        "paper_id": "2403.19822v1",
        "paper_title": "Multi-Stage Multi-Modal Pre-Training For Automatic Speech Recognition",
        "research_background": "### Motivation\nThe primary motivation behind this paper is to address the continuing challenges in automatic speech recognition (ASR), particularly in situations involving rare words and noisy acoustic conditions. Current uni-modal (speech-only) ASR systems struggle with discerning speaker-specific and global noise patterns during speech recognition. Inspiration is drawn from recent breakthroughs in other fields such as natural language processing, robotics, and computer vision which have shown that exposing models to diverse data during pre-training builds robust representations. Incorporating multi-modal data, specifically audio-visual data, appears promising based on prior work, but there remains a need for a simpler and more effective pre-training approach.\n\n### Research Problem\nThe research problem tackled by this paper is to enhance the performance of ASR models through an innovative pre-training regimen involving multiple stages and multiple modalities. Specifically, the paper aims to:\n1. Assess the effectiveness of various audio-visual pre-training methodologies on ASR performance.\n2. Explore the impact of introducing a novel mid-training stage using speech translation as a consolidating task between pre-training and fine-tuning.\n3. Simplify the pre-training architecture while broadening the objectives beyond masked cluster prediction.\n\n### Relevant Prior Work\nThe paper builds on several prior findings:\n- Chen et al. (2022), Hsu and Shi (2022), and Chan et al. (2022) have shown progress with large-scale pre-training for ASR, highlighting the benefits of multi-modal data.\n- Shi et al. (2022) and Hsu and Shi (2022) have demonstrated improved performance on lip-reading tasks by pre-training on large-scale audio-visual data.\n- Chan et al. (2022) found that using visual data in pre-training boosts performance even when only audio is available during testing.\n- The efficacy of integrating non-speech specific visual data, such as the Kinetics dataset, was also explored by Chan et al. (2022), showing that integrating diverse augmentations can be beneficial, though it left room for further exploration.\n\nThis paper extends previous work by evaluating the multi-modal pre-training on a larger scale and introducing a novel mid-training stage. It also focuses on simplifying the pre-training architecture to make the process more efficient and scalable.",
        "methodology": "Our proposed method is a multi-stage multi-modal pre-training approach developed for improving Automatic Speech Recognition (ASR). Here, we outline the methodology, detailing each component and innovation integral to the process.\n\n**Stage 1: Multi-Modal Pre-Training**\nIn the first stage, the model undergoes pre-training on a diverse set of modalities. This stage leverages large-scale, multi-modal datasets to enhance the model's ability to understand and integrate different types of input such as audio, text, and visual data. The objective here is to build a robust, versatile model adept at processing varied and complex data forms.\n\n**Stage 2: Intermediate Representations**\nFollowing the initial pre-training, the model transitions into learning intermediate representations. This involves refining the model\u2019s understanding and processing capabilities, ensuring that it can derive rich, contextualized features from the multi-modal inputs. The focus at this stage is on deepening the model\u2019s comprehension and inter-modality correlations, which enhances contextual learning and feature relevancy.\n\n**Stage 3: Task-Specific Pre-Training**\nAfter the development of intermediate representations, the model enters a task-specific pre-training phase. This stage emphasizes tailoring the pre-trained model to specific characteristics and requirements of ASR tasks. By doing so, the model is better prepared to handle the nuances and specificities of speech recognition, leading to improved performance and accuracy when deployed in real-world applications.\n\n**Fine-Tuning Stage**\nThe final stage involves fine-tuning the pre-trained model on downstream ASR tasks. This process includes optimizing the model\u2019s parameters specifically for speech recognition, using targeted datasets that mirror the operational context and usage scenarios of the desired application. The fine-tuning process ensures that the model\u2019s pre-learned capabilities are fully leveraged and adapted for optimal ASR performance.\n\nEach stage of this multi-stage multi-modal pre-training approach contributes critical improvements to the model\u2019s overall performance in ASR tasks, by progressively honing its ability to interpret, integrate, and accurately transcribe spoken language.",
        "main_experiment_and_results": "### Main Experiment Setup ###\n\n1. **Datasets:**\n\n- **Librispeech**: The primary dataset used for evaluation, containing 'test-clean' and 'test-other' splits.\n- **Pre-training Datasets**:\n- **LRS-3**: A smaller, clean-speech dataset.\n- **VoxCeleb2**: A larger and more diverse audio-visual dataset.\n- **Kinetics**: A dataset significantly smaller than VoxCeleb2 and not speech-specific.\n\n2. **Baselines:**\n\n- Models trained from scratch.\n- Various pre-training configurations such as audio-visual pre-training, mid-training, and different objectives (MAE, CLR, MAE+CLR).\n\n3. **Evaluation Metrics:**\n\n- **Word Error Rate (WER)**: The main metric used for evaluating ASR performance, reported for both 'test-clean' and 'test-other' conditions.\n- **Relative WER Improvements**: Used to assess the impact of mid-training and pre-training methodologies by comparing performance deviations from baselines.\n\n### Main Experimental Results ###\n\n1. **Effectiveness of Audio-Visual Pre-Training:**\n- The audio-visual pre-trained models demonstrated significant improvements over training from scratch. Averaging the performance across methods showed WERs of 6.34 \u00b1 0.94 for 'test-clean' and 12.18 \u00b1 0.98 for 'test-other'.\n\n2. **Mid-Training with Translation Pairs:**\n- Mid-training led to significant WER improvements over pre-trained models alone:\n- **English-German**: 8.59% (test-clean) / 6.77% (test-other)\n- **English-Dutch**: 13.11% (test-clean) / 7.71% (test-other)\n\n3. **Insights on Future Research Directions:**\n- Audio-visual pre-training is generally effective.\n- Mid-training using complementary, rather than similar, language data offers significant gains.\n\nIn conclusion, these results support the utility of both audio-visual pre-training and mid-training, with strategic choices in dataset composition and pre-training methods crucial for optimizing ASR performance."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To compare the effects of local and global feature learning on visual-audio dataset characteristics for ASR pre-training.",
            "experiment_process": "Two pre-training strategies, Masked Autoencoder (MAE) and Contrastive Learning (CLR), were employed. MAE learns local features by reconstructing masked parts of speech and video using a simplified encoder and joint transformer decoder. CLR uses pooled audio-visual features from the same video as positive pairs and different combinations as negatives. Three datasets were employed: Kinetics-600 (non-speech audio), Voxceleb2 (multi-lingual speaker recognition), and LRS-3 (clean spoken sentence data).",
            "result_discussion": "The findings indicate that both pre-training methods are effective, with MAE showing a relative performance improvement. LRS-3 dataset, despite being the smallest, outperformed other datasets on the test-other dataset, whereas Voxceleb2 was slightly better on the test-clean dataset.",
            "ablation_id": "2403.19822v1.No1"
        },
        {
            "research_objective": "To align the learned speech representations with the text modality for improved ASR performance.",
            "experiment_process": "The audio encoder was mid-trained on a speech translation task using the MuST-C dataset in German, Italian, and Dutch until convergence. This mid-training task serves to bridge the distribution gap between pre-training and fine-tuning.",
            "result_discussion": " Mid-training with all translation pairs significantly improved ASR performance, with Italian being the most effective language pair. This suggests the benefits of using complementary languages over similar languages to the downstream language.",
            "ablation_id": "2403.19822v1.No2"
        },
        {
            "research_objective": "To analyze the impact of pre-training datasets and tasks on the final learned representations and ASR performance.",
            "experiment_process": "The models were evaluated on Librispeech test-clean and test-other datasets for ASR, and four tasks from the SUPERB benchmark: Intent Classification, Keyword Spotting, Phoneme Recognition, and Speaker Diarization. The encoder weights were frozen, and only the task-specific decoder was fine-tuned.",
            "result_discussion": "Audio-visual pre-training was generally effective. Mid-training showed significant WER improvements, especially with the English-Italian translation pair. Pre-training on LRS-3, despite its smaller size, yielded better results, indicating that dataset distribution is crucial. MAE alone outperformed CLR and MAE+CLR in ASR tasks, but MAE+CLR was better for a mix of tasks requiring both local and global information. Pre-training on Kinetics improved the most with mid-training, due to its non-speech data nature.",
            "ablation_id": "2403.19822v1.No3"
        }
    ]
}