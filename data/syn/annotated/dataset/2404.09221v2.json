{
    "title": "\\scalerel* Exploring and Improving Drafts in Blockwise Parallel Decoding",
    "abstract": "Despite the remarkable strides made by autoregressive language models, their potential is often hampered by the slow inference speeds inherent in sequential token generation. Blockwise parallel decoding (BPD) was proposed by Stern et al.\u2009[38] as a method to improve inference speed of language models by simultaneously predicting multiple future tokens, termed block drafts, which are subsequently verified and conditionally accepted by the autoregressive model. This paper contributes to the understanding and improvement of block drafts in two ways. First, we analyze the token distributions produced by multiple prediction heads. Secondly, we leverage this analysis to develop algorithms to improve BPD inference speed by refining the block drafts using n-gram and neural language models. Experiments demonstrate that refined block drafts yield a +5-21% increase in block efficiency (i.e., the number of accepted tokens from the block draft) across diverse datasets.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The landscape of natural language processing has been profoundly reshaped by recent advances in autoregressive language models. These models have shown remarkable proficiency across a range of text generation tasks, including applications like question answering and summarization. However, a significant obstacle to their wider application is high inference latency, particularly for extremely deep models with hundreds of billions of parameters. This latency, intrinsic to decoding with autoregressive language models (LMs), imposes considerable computational burdens and limits real-time deployment.\n\nIn response to these latency challenges, the field has seen a shift towards decoding methods aimed at reducing the inference latency in large language models (LLM). One promising development is the concept of blockwise parallel decoding (BPD). Unlike autoregressive decoding, which generates one token at a time, blockwise parallel LMs are outfitted with a set of prediction heads that propose and verify a draft, a block of subsequent tokens, in parallel. While BPD offers one solution to accelerated text generation, it also poses a challenge in ensuring that the proposed drafts are fluent and natural.\n\nBPD inference speed depends on both the time it takes to produce a block draft and the draft\u2019s agreement with the base LM\u2019s output. Unlike standard autoregressive LMs that generate tokens sequentially\u2014ensuring consistency with all preceding tokens (e.g., \u2018Messi\u2019 following \u2018Lionel\u2019)\u2014BPD employs a parallel strategy. Here, blockwise parallel LMs simultaneously predict multiple token drafts (e.g., \u2018Lionel\u2019 and \u2018Ronaldo\u2019), each independently. The primary challenge in BPD is ensuring that these concurrently generated tokens maintain consistency. Effective block drafters should prioritize coherent sequences such as \u2018Lionel Messi\u2019 over less likely combinations like \u2018Lionel Ronaldo\u2019, which a robust LM would not decode. The focus of this research is improving the quality of block drafts without altering the underlying model parameters."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Our contributions",
            "text": "In this paper, we first investigate properties made by the prediction heads of blockwise parallel LMs across several tasks; given these observations, we propose rescoring algorithms to produce higher quality block drafts."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Observations on block drafts",
            "text": "Consecutive repetitions \u2009All heads within a block make predictions independently in a blockwise parallel LM. Unsurprisingly, we observe that this leads to block drafts with significant token repetition across heads. Consecutive repetition is pervasive across tasks, ranging from 20% to 75% of all neighboring draft tokens, depending on the task (subsection 6.1  ###reference_###).\nConfidence of different heads \u2009We analyze the distribution of probabilities within each softmax head. Our empirical analysis reveals a key property of BPD: the block drafter tends to be more confident with initial tokens, and becomes progressively less confident for subsequent tokens. We find that the confidence of block heads correlates strongly with the quality of the block drafter (subsection 6.2  ###reference_###).\nOracle top- block efficiency \u2009In the standard BPD algorithm (Algorithm 1  ###reference_###), the most likely token at each head is generated as the draft. As highlighted before, this approach is prone to two issues: (1) there might be consecutive repetitions and (2) the model might not be confident about the prediction at some of the heads.\nWe use block efficiency, the average number of draft tokens accepted during decoding, to measure the quality of a given drafter [24  ###reference_b24###, 41  ###reference_b41###].\nWe ask if the block efficiency can be improved by considering the top- most likely tokens at each head. To measure the potential benefit of considering top- tokens, we measure the block efficiency of the oracle path through this top- lattice, oracle top- block efficiency, and show that there is significant headroom for improvement across tasks (subsection 6.3  ###reference_###)."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "New algorithms",
            "text": "Based on these observations, we propose two algorithms to leverage the top- predictions at each head and improve BPD latency\u2009(1(b)  ###reference_sf2###). Neither of these algorithms require changes to the underlying blockwise parallel LMs.\nLocal rescoring via neural LMs \u2009Given the top- predictions at each head, we refine the block draft by using a small neural, autoregressive LM to greedily rescore these local predictions (subsection 7.1  ###reference_###).\nWhile the block prediction scores are produced independent of each other, neural rescoring should favor sequences that are fluent, encouraging coherence between the predictions at each head.\nGlobal rescoring via n-gram LMs with multi-drafts \u2009\nIf the blockwise parallel LM has  heads and we consider the top- tokens from each head, then there are  candidate drafts of length  that can be formed. We propose to use an n-gram model to efficiently rescore all paths, via dynamic programming, and generate the  most probable rescored paths as a batch of draft candidates. These  drafts can then be verified in parallel by the blockwise parallel LM (subsection 7.2  ###reference_###).\n###figure_3### There are two critical distinctions between the proposed algorithms: the amount of context/expressive power available to each class of rescoring model, and fundamental limitations of decoding with each class. While neural rescoring models are potentially more expressive and can leverage unbounded context, n-gram  LMs can be used to efficiently find the globally most likely rescored drafts from the exponentially-sized set of possible draft candidates.\nFigure 2  ###reference_### shows that our proposed methods enhance block efficiency, with one approach increasing it by up to +21.30%. This same method also optimizes resource usage, reducing key-value (KV) cache I/O by -2.54% and additionally using FLOPs per token by +4.04%.\nDescription of each algorithm is given in subsection 7.1  ###reference_###."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Organization",
            "text": "The remainder of this paper organized as follows. In section 3  ###reference_###, we discuss previous literature in reducing LLM latency. In section 4  ###reference_###, we define foundational concepts and terminology. section 5  ###reference_### describes our experimental setup, datasets, on methods. section 6  ###reference_### describes our analysis of the block drafts. In subsection 7.1  ###reference_###, we present the proposed BPD rescoring algorithms and empirical results, followed by a final discussion in section 8  ###reference_###."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Efficient transformer inference",
            "text": "Works on improving transformer efficiency encompass both optimization of an existing set of model weights, or a fundamental change to the model architecture. Examples of the former include techniques such as quantization [44  ###reference_b44###, 45  ###reference_b45###, 9  ###reference_b9###] and model pruning [40  ###reference_b40###, 26  ###reference_b26###]. In parallel, neural architecture search has played a crucial role in identifying network structures that balance performance with efficiency [22  ###reference_b22###, 46  ###reference_b46###]. Relatedly, Elbayad et al.\u2009[10  ###reference_b10###] propose early-exiting at intermediate layers for faster inference, while Schuster et al.\u2009[35  ###reference_b35###] explore confidence thresholding for balancing speed and accuracy. These methods offer insights into optimizing decoding under resource constraints.\nOne important line of work has focused on modifying the decoding method in LMs. The adoption of non-autoregressive (parallel) decoding strategies [38  ###reference_b38###, 14  ###reference_b14###] marks a pivotal shift in this domain, addressing inference latency by simultaneously generating multiple tokens. Subsequent innovations have sought to refine this approach by incorporating additional context [6  ###reference_b6###], iterative refinement [20  ###reference_b20###], and tree-based attention mechanism [4  ###reference_b4###]. However, these refinements often require complex training or additional inference data."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Efficient and effective decoding",
            "text": "There are several recent works that improve the speed of LLM decoding, including pioneering works like BPD and speculative decoding. Speculative decoding leverages a smaller \u2018draft\u2019 model to anticipate the outputs of a larger target model, improving average decode latency without loss in generation quality [24  ###reference_b24###, 5  ###reference_b5###, 20  ###reference_b20###]. The draft model is typically trained on the same corpus as the LLM, thus autoregressively generates similar drafts as the target model with reduced latency. Speculative decoding is most successful when a long sequence of speculated tokens are accepted by the target LM during verification, avoiding multiple serial calls to the target LM to generate the same sequence.\nOn the surface, contrastive decoding algorithms share some similarities with our proposed draft rescoring approach, insofar as a weaker model is used to modify the predictions of the target LM [25  ###reference_b25###, 21  ###reference_b21###]. However, in this work, we refine block drafts solely to improve latency. Like speculative decoding, our proposals have no effect on the quality of the target LM\u2019s generated text."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Preliminaries",
            "text": "This section introduces notation and concepts, including algorithms for standard autoregressive decoding and BPD.\nAutoregressive decoding\u2009 Let  be an autoregressive LM parameterized by . The objective is to generate an output sequence  conditioned on an input sequence .\n is a vector of logits, , where  is the vocabulary over tokens. These logits define a conditional probability distribution at each time step , which by the chain rule yields .\nSequences are generated autoregressively, either through ancestral sampling from some form of the conditional next token distribution [17  ###reference_b17###], or by a beam search through the space of possible sequences to return a probable sequence. Greedy decoding, a special case of beam search, generates each token as . In this work, we consider greedy decoding exclusively, as this is the setting that Stern et al.\u2009[38  ###reference_b38###] was designed to accelerate.\nBlockwise parallel decoding\u2009 Let  be a blockwise parallel LM with block size . This model employs  distinct feedforward neural (FFN) layer with a single hidden layer, atop the target LM\u2019s final hidden layer. The output of each FFN is followed by a softmax layer over the vocabulary to predict each of the  subsequent tokens in the block. In our experiments, the parameters of the FFNs are learned jointly with the base LM during training, and the weights of all softmax layers are tied to the input embedding table. Algorithm 1  ###reference_### describes the BPD greedy decoding procedure:\nPredict:  is used to generate a draft of  token predictions , conditioned on the prompt, , and existing generated text, .  is identical to the target LM greedy decode.\nVerify: At this stage, the target LM greedily generates next-token logits  conditioned on the existing prefix and block draft . Verification amounts to checking which block draft tokens match the autoregressive greedy decode from the target LM: (. Verification of all positions can be performed in parallel under the assumption that the target LM is a decoder-only transformer.\nAccept: Finally, the length of the longest contiguous prefix of draft tokens that match the target LM greedy decode is identified: . The decoded sequence is extended by  tokens and we iterate.111The decoded sequence is extended by  tokens since during verification we generate the token from the target LM, , at the first position where the draft differs from the target LM greedy decode. Note that in general, not all  tokens are accepted, and many of the draft tokens in each block are discarded. Since the additional time required to generate a block of tokens is fast relative to the time it takes for the forward pass of the target LM, a modest gain in accepted prefix length justifies the cost of draft generation."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experimental setup",
            "text": "In this paper, we use billion (B) parameter decoder-only transformer LMs with up to 9 blockwise heads. This study is based on the original BPD framework, with a modification: we use decoder-only models instead of the T5 encoder-decoder architecture. Other setups are consistent with the approach in Stern et al.\n\nThe 1.5B model and all other LMs were pretrained on (English) C4 with the causal next token prediction objective tokenized with the GPT3 subword vocabulary. For the 1.5B blockwise parallel LMs, all heads were trained jointly to predict the following tokens at each iteration. During pretraining, we use batches of 2048 subword sequences, each 512 tokens in length, amounting to B input tokens in total on TPUv3/TPUv4 with Jax.\n\nWe evaluate the potential latency improvement of block drafts by block efficiency. In this context, block efficiency represents the theoretical speedup compared to standard greedy decoding. It is defined as the average number of tokens decoded per serial call to the blockwise parallel LMs. \n\nIn this definition, the total number of decoded tokens is the sum of the number of accepted tokens across decoding steps, not necessarily all predicted tokens in each block. Only the tokens that pass the \u2018Verify\u2019 stage and align with the base model\u2019s predictions are accepted and integrated into the final sequence. This ensures that generated text is identical to the target LM, while achieving speedup. The total number of serial calls is the number of times the model processes a block of tokens. A block efficiency of 1 means that one is achieving no speedup relative to standard decoding.\n\nIn addition to a standard language modeling dataset, we conduct experiments across several classes of downstream tasks. In the realm of text summarization, we evaluate models on the XSUM, MultiNews, SAMSum, NewsRoom, and CNN/DailyMail datasets. Each of these datasets is characterized by distinct summary lengths and styles. For extractive QA, the SQuAD V1 dataset serves as our testbed. For each task aside from language modeling, we finetune the blockwise parallel LM for that task. Details are given in Appendix C. Table 1 shows that block efficiency varies dramatically across tasks and as a function of the number of block draft heads. We use all 9 block draft heads for subsequent experiments as this acts as an upper bound on possible block efficiency.\n\nTable 2 sketches how BPD acts on three examples from each class of tasks.\n\nLM: BPD excels at generating common multi-word expressions in a single step. For example, (no) \u2018thing more than\u2019, and (take) \u2018his word for the\u2019 are each drafted and accepted in a single step.\n\nQA: BPD also attains high block efficiency in extractive QA, where it correctly drafts multi-token entities copied from the input sequence. In SQuAD V1, it accurately completes the answer \u2018Grumman\u2019 from \u2018Gru\u2019 by adding \u2018mman\u2019, highlighting its ability to process multiple tokens at once and quickly extend answers.\n\nSUM: BPD\u2019s effectiveness in SUM tasks varies by dataset. For formulaic summaries like CNN/DailyMail, it performs well, reflecting its alignment with LM and QA tasks. However, in narrative-driven datasets like SAMSum and XSUM, where concise summaries are required, the block efficiency of BPD is little better than standard decoding."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Exploration of predictive dynamics in BPD",
            "text": ""
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Consecutive repetition",
            "text": "###table_1### We observe that vanilla block drafts are prone to significant token repetition. This is due to the fact that each head\u2019s prediction is independent of the others, and is a limitation shared with non-autoregressive generation in general [14  ###reference_b14###]. Table 3  ###reference_### shows the proportion of consecutive tokens in block drafts that are identical to each other, along with the average maximum length of repeated sequences in block drafts across all decode time steps. We compare these statistics before and after rescoring with a 2-gram LM: a trivial rescorer, but one that can encourage local consistency between consecutive draft tokens. Strings of repeated tokens are unnatural, and unlikely to be generated by a strong base language model. Rescoring the top- lattice with even a simple language model eliminates a significant amount of repetition, reducing the percentage of consecutive repeated tokens from between 9.9% to 24.5%, depending on the task.\n###figure_4### ###figure_5###"
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Confidence across multiple heads",
            "text": "Intuitively, predicting the identity of the  future token becomes harder as  increases. To better understand this phenomenon, we measure the confidence of the predictions by the entropy of the probability distribution. In 3(a)  ###reference_sf1###, we plot the normalized histogram of entropy of each head on the LAMBADA task. From the normalized histogram, it is clear that the entropy increases as we move from first head to the last head, which agrees with our intuition that hardness of predictions increases as  increases.\nHowever, we observed that the entropy of heads does not increase monotonically for all tasks. Let  be the average entropy of head  on a particular corpus, and let , be the index of the largest head such that the average entropy of each head increases monotonically to that point. We observed a strong correlation between  and block efficiency (3(b)  ###reference_sf2###). Heads with lower entropy (indicating more confident predictions) intuitively contribute more to efficiency. Nonetheless, simply maximizing the number of low-entropy heads is not optimal, but rather incorporating progressively higher entropy heads, up to a certain point, can benefit decoding efficiency. A linear regression confirms this with an R-value of . This analysis suggests that BPD head entropy could be used as a proxy for block efficiency, and thus inference latency."
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "Oracle top-k block efficiency",
            "text": "###figure_6### Oracle efficiency\u2009 The concept of oracle block efficiency in BPD serves as a theoretical benchmark, illustrating the headroom available from improving the quality of the block draft. To compute oracle block efficiency, we consider the top- most probable tokens at each head, and form a \u201csausage\u201d lattice from these. This data structure is a weighted directed graph, which succinctly represents all possible drafts (and their score under the BPD model) that could be formed from selecting one of  tokens from each of the  heads (Figure 4  ###reference_###). In the automatic speech recognition and machine translation communities, it is known as a \u201cconfusion network\u201d [36  ###reference_b36###, 23  ###reference_b23###].\n###figure_7### Given the top- lattice at each decoding step, we identify an oracle path that represents the path through the lattice that maximizes the length of the accepted prefix. This exercise, as shown in Figure 5  ###reference_###, gives us insight into how much headroom exists in improving block drafts.\n###figure_8### ###figure_9### ###figure_10### ###figure_11### ###figure_12### ###figure_13### Potential headroom from oracle selection\u2009 Oracle drafting is not practical, but rather a reference point. Analyzing the gap between actual BPD performance and the oracle upper bound (Figure 6  ###reference_###) helps us to understand the limitations of the original block drafts and potential areas for improvement. Additionally, exploring oracle efficiency as a function of the  in the top- lattice, demonstrates how \u201cclose\u201d the block draft was to producing a stronger draft."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Lattice rescoring for improved block efficiency",
            "text": "Having explored BPD\u2019s prediction dynamics, we propose two drafting algorithms to improve block efficiency through rescoring of the top- lattice. This section presents techniques for rescoring the top- lattice along with empirical results.\nEach of these algorithms is a modification of the block drafted in Stage 1 in Algorithm 1  ###reference_###. Instead of using the most likely token at each head as the prediction, we construct the top- sausage lattice of likely drafts from each head, where the set of top- tokens is denoted as  for head . This approach allows any token within  to be chosen for position , yielding a total possible combinations of:\nIn this lattice, any path from the start to final state represents a viable draft. Two algorithms are proposed to select a small number of -length drafts from this lattice, which are then passed to the verification step. The first algorithm employs neural autoregressive transformers (subsection 7.1  ###reference_###), while the second utilizes n-gram language models (subsection 7.2  ###reference_###)."
        },
        {
            "section_id": "7.1",
            "parent_section_id": "7",
            "section_name": "Local rescoring via neural models",
            "text": "A simple approach uses a small neural rescorer, interpolating between the logits of the rescorer LM and vanilla block draft logits with an interpolation weight (algorithm 2  ###reference_###). The rescored prediction is given by:\nwhere  represents the logit of the block draft at head , and  is the corresponding logit predicted by the small neural rescoring model, which is conditioned on the sequence . The parameter  is the weight placed on the rescorer\u2019s prediction. We experiment with decoder-only transformers having 32, 61, and 94 million (M) weight parameters (Appendix C  ###reference_###). We use greedy rescoring when generating the neural draft."
        },
        {
            "section_id": "7.2",
            "parent_section_id": "7",
            "section_name": "Global n-gram rescoring",
            "text": "We also evaluate the quality of drafts generated by rescoring with an n-gram LM.\nRecall that blockwise parallel LMs can be used to compute a lattice representing  possible sequences. We rescore all of these sequences with an n-gram  model, select the top  sequences and pass them to the verification stage. When , we refer to this as n-gram rescoring and when , we refer to this as -best n-gram BPD.\nWhile global rescoring typically yields better results compared to local rescoring, rescoring  sequences with a neural LM and selecting the most likely sequence would take time , which is computationally prohibitive in most cases.\nHence, we take advantage of n-gram models, which are unique in that we can select the most likely sequence in time  using dynamic programming.\nWe use the OpenFST library [1  ###reference_b1###] to represent each n-gram model as a weighted finite state automaton and apply finite state composition with the top- lattice followed by extraction of the  most likely draft sequences. Training details for the n-gram models are given in Appendix C.3  ###reference_###.\n###figure_14### ###figure_15### ###figure_16### ###figure_17### ###figure_18### ###figure_19###"
        },
        {
            "section_id": "7.3",
            "parent_section_id": "7",
            "section_name": "Empirical evaluation",
            "text": "Block efficiency\u2009 Table 4  ###reference_### and Figure 7  ###reference_### demonstrate the impact of lattice rescoring on block efficiency across various tasks. Autoregressive neural, n-gram LM, and -best n-gram BPD rescoring all demonstrate improvements in block efficiency, although gains are task-dependent.\nHigh initial block efficiency (LAMBADA, CNN/Daily): Both rescoring methods show little to no improvement, suggesting that vanilla BPD already produces high quality drafts.\nLow initial block efficiency (SQuAD V1, SAMSUM, XSUM, NewsRoom): Both neural and n-gram rescoring lead to block efficiency gains, particularly with neural LMs achieving the best performance in some cases. This suggests that rescoring helps refine predictions and navigate the lattice more efficiently in these scenarios.\n###table_2### Repairing repetitions\u2009 In subsection 6.1  ###reference_###, we note that vanilla block drafts are prone to token-level repetition and that rescoring with a simple language model reduces the incidence of this. Although rescoring reduces repetition overall in drafts, is this driving improvements in block efficiency? To answer this, we compared the drafts generated by greedy rescoring with the 61M parameter neural rescorer against vanilla drafts. Time step instances were considered wins/ties/losses based on the accepted prefix length of the rescored draft vs. vanilla draft. Table 5  ###reference_### displays the win frequency across tasks along with the percentage of wins/losses attributed to introducing/eliminating repetition.\nNote that in the tasks where rescoring improves block efficiency the most, NewsRoom and MultiNews, a high percentage of those repaired instances are driven by fixing erroneously repeated tokens. In fact, for MultiNews, 66.23% of block drafts are improved through repetition repair. We also evaluated the performance of rescoring with in-domain trained rescoring LMs, but found that they tended to perform no better than C4-trained LMs (Appendix D  ###reference_###)."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "This paper presents a comprehensive analysis of BPD, highlighting its predictive dynamics and proposing methods to refine the generation of block drafts. Our study offers insights into BPD\u2019s behavior, particularly the tendency for drafts to contain consecutive repetitions and its heads to exhibit varying confidence levels in predictions. We introduce a novel measure, oracle top-block efficiency, to explore potential improvements in block efficiency. Two algorithms are proposed for generating higher quality drafts: one for local rescoring with small neural models (i.e., neural BPD) and another for global rescoring with an n-gram LM and generating multiple drafts (i.e., -best n-gram BPD). These algorithms leverage the strengths of both blockwise parallel LMs and small rescoring models to reduce average decoding latency, pushing the boundaries of efficient text generation with BPD. We believe that this paper lays the groundwork for future exploration in optimizing LM decoding speed."
        }
    ],
    "url": "http://arxiv.org/html/2404.09221v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "3",
            "3.1",
            "3.2"
        ],
        "methodology_sections": [
            "2",
            "2.1",
            "2.2",
            "7",
            "7.1",
            "7.2"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "7.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "2.1",
            "2.2",
            "7",
            "7.1",
            "7.2",
            "7.3"
        ]
    },
    "research_context": {
        "paper_id": "2404.09221v2",
        "paper_title": "\\scalerel* Exploring and Improving Drafts in Blockwise Parallel Decoding",
        "research_background": "The motivation for this paper arises from the need to address the high inference latency inherent in autoregressive language models (LMs), which is particularly problematic for extremely deep models with hundreds of billions of parameters. This latency is a significant obstacle that hampers the real-time deployment of LMs in various applications such as question answering and summarization.\n\nThe research problem is centered on the development and refinement of blockwise parallel decoding (BPD) methods to enhance inference speed. Although BPD allows multiple token drafts to be generated in parallel, which theoretically speeds up the text generation process, it introduces the challenge of ensuring these drafts are coherent and consistent. The aim is to improve the quality of block drafts to facilitate accelerated yet fluent and natural text generation.\n\nRelevant prior work includes:\n1. Advances in autoregressive language models, which have demonstrated impressive abilities in text generation tasks (references [3], [43], [30], [33], and [42]).\n2. Applications of autoregressive LMs in question answering (reference [34]) and summarization (reference [15]).\n3. Recognition of high inference latency as a limiting factor for deep LMs with significant computational demands (references [16], [31], and [7]).\n4. Introduction of blockwise parallel decoding as a method to reduce inference latency in large language models (references [38], [27], and [4]).\n\nIn summary, the paper seeks to explore and improve the quality of block drafts in BPD to mitigate the latency issue while ensuring output fluency and consistency, without modifying the base model parameters.",
        "methodology": "In this section, we will outline and delve into the proposed methodology for exploring and improving drafts in blockwise parallel decoding, focusing on the critical components and innovations. Our paper investigates the properties generated by the prediction heads of blockwise parallel language models (LMs) across various tasks. Based on our observations, we then introduce rescoring algorithms aimed at improving the quality of these block drafts.\n\nKey components and innovations of our proposed method include:\n\n1. **Blockwise Parallel Language Models (LMs)**: We employ blockwise parallel language models, which facilitate parallel processing, leading to significant reductions in overall decoding time. These models predict text in blocks, leveraging multiple prediction heads that work simultaneously on different parts of the text sequence.\n\n2. **Investigation of Prediction Head Properties**: We conduct an in-depth examination of the outputs produced by the prediction heads used in blockwise parallel LMs across diverse tasks. This investigation allows us to understand the strengths and weaknesses of the predictions made by each head, identifying patterns and areas for potential quality improvement in the generated drafts.\n\n3. **Rescoring Algorithms**: Based on our observations from the prediction head analysis, we propose rescoring algorithms designed to enhance the quality of block drafts. These algorithms reassess and revise the initial predictions made by the heads, potentially leading to more accurate and coherent text outputs.\n\n4. **Higher Quality Block Drafts**: The combined effort of analyzing prediction head properties and applying rescoring algorithms aims to produce higher quality block drafts. The refinement provided by the rescoring algorithms addresses inconsistencies and errors in the initial drafts, resulting in more polished and reliable texts.\n\nBy leveraging these components and innovations, our methodology aims to enhance the efficiency and quality of text generation in blockwise parallel decoding, offering a more robust framework for various language modeling tasks.",
        "main_experiment_and_results": "## Main Experiment Setup and Results\n\n### Experiment Setup\n\n- **Models**: The experiments utilize billion-parameter (1.5B) decoder-only transformer language models (LMs) with up to nine blockwise heads. The models were pretrained using the causal next token prediction objective, tokenized with the GPT-3 subword vocabulary.\n\n- **Pretraining**: The 1.5B blockwise parallel LMs were pretrained on the English C4 dataset with batches of 2048 subword sequences, each consisting of 512 tokens, amounting to billions of input tokens processed on TPUv3/TPUv4 with Jax.\n\n- **Datasets**:\n  - **Language Modeling**: Evaluated on LAMBADA.\n  - **Text Summarization**: Evaluated on XSUM, MultiNews, SAMSum, NewsRoom, and CNN/DailyMail datasets.\n  - **Extractive Question Answering (QA)**: Evaluated using the SQuAD V1 dataset.\n\n- **Evaluation Metrics**:\n  - **Language Modeling**: Perplexity\n  - **QA**: Exact Match\n  - **Summarization**: ROUGE-L\n\n- **Block Efficiency**: The primary measure for potential latency improvement, defined as the average number of tokens decoded per serial call to the blockwise parallel LMs. A block efficiency of 1 indicates no speedup compared to standard greedy decoding.\n\n### Results\n\n- **Language Modeling (LM)**: Blockwise Parallel Decoding (BPD) excels in generating common multi-word expressions in a single step, showing substantial efficiency improvements in drafting and verifying multi-token sequences, such as seamlessly completing phrases.\n\n- **Extractive QA**: The block efficiency is notably high in this task. For instance, BPD accurately extends partial answers (e.g., completing \u2018Gru\u2019 to \u2018Grumman\u2019), demonstrating its effectiveness in processing and post-processing multi-token entities.\n\n- **Text Summarization (SUM)**: \n  - **Formulaic Summaries**: For datasets like CNN/DailyMail, BPD performs well, reflecting its alignment with LM and QA tasks, thus achieving significant block efficiencies.\n  - **Narrative-driven Summaries**: In datasets requiring concise summaries, such as SAMSum and XSUM, the block efficiency of BPD is only marginally better than standard decoding, indicating variability in performance depending on the summary style and length.\n\nOverall, the experiment highlights the strengths of BPD in speeding up multi-token predictions while maintaining the quality of generated sequences across various tasks, though its effectiveness can vary significantly depending on the nature of the task."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Investigate the impact of token repetitions and confidence levels in blockwise parallel decoding and measure the potential benefit of considering top- tokens at each head for improving block efficiency.",
            "experiment_process": "The study involved analyzing block drafts from a blockwise parallel language model across various tasks to observe token repetition and the confidence of softmax head predictions. An oracle top- block efficiency was measured by considering the top- most likely tokens at each head. The datasets and the standard BPD algorithm were used to evaluate the consequences of these observations.",
            "result_discussion": "The experiments revealed that consecutive token repetitions were prevalent, and confidence levels decreased for subsequent tokens in block drafts. By considering the top- tokens at each head for potential drafter candidates, there was significant headroom for improvement in block efficiency across tasks.",
            "ablation_id": "2404.09221v2.No1"
        },
        {
            "research_objective": "Develop algorithms to refine block drafts using top- predictions to enhance BPD latency without altering the underlying blockwise parallel LMs.",
            "experiment_process": "Two algorithms were proposed: Local rescoring via neural language models (LMs) and global rescoring via n-gram LMs with multi-drafts. The local rescoring method used a small neural, autoregressive LM to rescore top- drafts, promoting fluency and coherence. The global rescoring method used an n-gram model to dynamically generate the most probable rescored paths from an exponentially-sized set of candidate drafts, which were then verified in parallel.",
            "result_discussion": "The proposed methods increased block efficiency by up to 21.30% and optimized resource usage by reducing KV cache I/O by 2.54% and increasing FLOPs per token by 4.04%. These improvements demonstrated the efficacy of leveraging top- token predictions for refining block drafts.",
            "ablation_id": "2404.09221v2.No2"
        },
        {
            "research_objective": "Evaluate the effectiveness of lattice rescoring using both neural and n-gram models on block efficiency and token-level repetition repair.",
            "experiment_process": "The proposed algorithms established a top- sausage lattice of likely drafts. The first method, local rescoring, used small autoregressive transformers for rescoring. The second method, global rescoring, applied n-gram language models to select the most likely sequences via dynamic programming. Block efficiency was measured across various tasks, and the impact of rescoring on token repetitions was analyzed using win/tie/loss metrics based on the accepted prefix length of rescored vs. vanilla drafts.",
            "result_discussion": "Lattice rescoring improved block efficiency in tasks with low initial efficiency, notably with neural LMs achieving the best results in some cases. Rescoring effectively reduced token-level repetitions, significantly driving improvements in block efficiency for tasks such as NewsRoom and MultiNews, where a majority of improvements were attributed to fixing erroneous repetitions.",
            "ablation_id": "2404.09221v2.No3"
        }
    ]
}