{
    "title": "Automating the Information Extraction from Semi-Structured Interview Transcripts",
    "abstract": "This paper explores the development and application of an automated system designed to extract information from semi-structured interview transcripts. Given the labor-intensive nature of traditional qualitative analysis methods, such as coding, there exists a significant demand for tools that can facilitate the analysis process. We present a user-friendly software prototype that enables researchers, including those without programming skills, to efficiently process and visualize the thematic structure of interview data. This tool not only facilitates the initial stages of qualitative analysis but also offers insights into the interconnectedness of themes revealed, thereby enhancing the depth of qualitative analysis.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1. Introduction",
            "text": "Qualitative methods such as interviews or focus groups with customers are an integral part of the research arsenal in a number of fields: marketing, social science, and medical studies (Avjyan, 2005  ###reference_b7###; Leeson et al., 2019  ###reference_b10###). This approach differs significantly from quantitative techniques in its ability to draw on individual experiences and delve deeper into the issue under study. However, unlike the results of quantitative surveys, in interviews, there is no ready-made information, no statistics, and no clear answers to the questions posed. The researcher unwittingly faces the problem of interpretational objectivity, and the question arises as to how to tackle it. The general analysis of collected data in interviews mainly uses open coding technology (Fig 1  ###reference_###), which involves repeatedly reading the text to identify \u201dcodes\u201d that are in essence important thoughts, ideas, attitudes, and subjects. Further, the axial coding procedure is applied, where the relationships between the codes and their aggregation into higher-level categories are found (Saldana, 2016  ###reference_b15###). Several other coding methods are present and all of them involve independent work with the text, consisting of re-reading and finding the key thoughts of the informant in a large number of documents. This process often takes several weeks (Alshenqeeti, 2014  ###reference_b6###). Hence, it can be seen that this procedure requires a lot of human effort to process the text by oneself. So the research issue of implementing automatization of the whole process or pre-processing of the text corpus to facilitate the subsequent analysis appears. That\u2019s why the main goal of current research is to automate the analysis of qualitative research results and elaborate the appropriate software that will help organizations and researchers dealing with large clients\u2019 text corpora. To begin with, it is necessary to consider the existing solutions on the market and describe how the future service will differ."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2. Current coding practices",
            "text": "Each statement or significant segment of dialogue within an interview is assigned a \u2019code\u2019 that summarizes its main idea. Codes are not just words, phrases, sentences, or thoughts but represent a unit of meaning that encapsulates key aspects of the data (Miles and Huberman, 1994  ###reference_b12###). Once coded, these segments are then organized into broader categories that reflect the underlying patterns and relationships within the dataset (Glaser and Strauss, 2017  ###reference_b9###). In practice categories and codes consist of one or two words to encapsulate the main meaning of a citation. However if the main thought of a sentence can be described only in a phrase, that is also allowed.\nDescribing the process in simpler terms, first, we summarize the main idea of each citation in the interview(Ryan and Bernard, 2003  ###reference_b14###). Then, we start grouping them into bigger categories. This means looking at all the little ideas we\u2019ve found and seeing how they fit together into larger themes. We ask questions like, \u201dDo these codes share something in common?\u201d or \u201dAre they talking about the same bigger idea?\u201d This helps us organize our findings better.\nThese categories serve as the pillars for constructing a conceptual framework (Lochmiller, 2021  ###reference_b11###), which researchers often visualize in the form of a graph. Such a graph, akin to a mind map, interlinks individual responses, highlighting the associations and hierarchies amongst different thematic codes. This visualization assists in better understanding the collective narrative of the participants. Thus, the coding process is a critical interpretive phase in qualitative research, helping in developing conclusions and theoretical insights."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3. Overview of methods and tools",
            "text": "In general, researchers use software to facilitate the coding of interviews as follows: the text is conveniently placed on the screen with the possibility of highlighting parts of the sentence with a color marker and tagging them with codes, the number of which then counts itself (Atl, 2024  ###reference_b3###; MAX, 2024  ###reference_b5###). These programs also make space for drawing diagrams with codes and connecting them with arrows and written relationships. Thus, the software does not replace the analysis process as such but simply puts the researcher in a comfortable environment for the same workflow.\nWhen it comes to programs aimed at replacing some of the work some softwares replace part of the work by giving analytical tools such as word statistics and word cloud (Ded, 2024  ###reference_b4###). It is worth noting that all of these programs are paid, so not all researchers are inclined to use them. For example, in one of the works already described, coding was done in MS Word and Excel (Leeson et al., 2019  ###reference_b10###). Some programs focus on text processing in general, with clustering and collocation search capabilities (Ant, 2024  ###reference_b2###). However, the format of interview analysis is very specific, as it requires the building of models based on a set of answers to one question from several informants. Therefore, it is difficult to use such general-purpose software for the analysis of transcripts.\nThus, the purpose of this work is to develop a method for analyzing transcripts of qualitative research results, as well as to write user-friendly software that can be used by researchers who do not know the skills of programming."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1. Existing softwares",
            "text": "In general, researchers use software to facilitate the coding of interviews as follows: the text is conveniently placed on the screen with the possibility of highlighting parts of the sentence with a color marker and tagging them with codes, the number of which then counts itself (Atl, 2024  ###reference_b3###  ###reference_b3###; MAX, 2024  ###reference_b5###  ###reference_b5###). These programs also make space for drawing diagrams with codes and connecting them with arrows and written relationships. Thus, the software does not replace the analysis process as such but simply puts the researcher in a comfortable environment for the same workflow.\nWhen it comes to programs aimed at replacing some of the work some softwares replace part of the work by giving analytical tools such as word statistics and word cloud (Ded, 2024  ###reference_b4###  ###reference_b4###). It is worth noting that all of these programs are paid, so not all researchers are inclined to use them. For example, in one of the works already described, coding was done in MS Word and Excel (Leeson et al., 2019  ###reference_b10###  ###reference_b10###). Some programs focus on text processing in general, with clustering and collocation search capabilities (Ant, 2024  ###reference_b2###  ###reference_b2###). However, the format of interview analysis is very specific, as it requires the building of models based on a set of answers to one question from several informants. Therefore, it is difficult to use such general-purpose software for the analysis of transcripts.\nThus, the purpose of this work is to develop a method for analyzing transcripts of qualitative research results, as well as to write user-friendly software that can be used by researchers who do not know the skills of programming."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4. Research design",
            "text": "In this paper, several methods will be applied and compared with each other, and finally, the most appropriate method for analyzing qualitative research transcripts will be chosen. To achieve the goal, it is necessary to perform the following tasks: Create a framework for visualizing selected keywords from transcripts Write a frontend understandable to researchers.\n\nThe paper will compare various methods to find the most suitable one for analyzing interview transcripts. Semi-structured interviews present a unique challenge as they include characteristics of both: they are short like tweets but can contain rich narratives similar to longer documents.\n\nBefore proceeding with analysis, the necessary preprocessing of the natural text is done. First, the sentences were tokenized and lemmatized. Stop words, which are included by default, were also removed. However, a complementary set of stop words was created, compiled independently after building the frequency tables of the tokens. Many undesirable words occurring in interviews could skew the analysis. Such words as \"probably, it turns out, in general, supposedly, like\" were removed, as well as some verbs that refer to the process of reflection \"think, suppose\"."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1. Analysis of interview transcript data and selection of the best topic model",
            "text": "This paper will compare various methods to find the most suitable one for analyzing interview transcripts. Semi-structured interviews present a unique challenge as they include characteristics of both short and rich narratives, making some standard analytical approaches less effective.\n\nBefore analysis, necessary preprocessing of the natural text is done. First, the sentences were tokenized and lemmatized. Stop words, which are included by default, were also removed. However, in addition to this, a complementary set of stop words was created, which was compiled independently after building the frequency tables of the tokens. Words such as \"probably,\" \"it turns out,\" \"in general,\" \"supposedly,\" \"like,\" were removed, as well as some verbs that refer to the process of reflection \"think,\" \"suppose.\"\n\nUsing the transcripts and combining them into larger formats for analysis was tested and found to give unsatisfactory results when based on individual questions, as it resulted in similar outputs. Therefore, it was decided to abandon this method and focus on the aggregate of all documents for future model development."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5. Experiments",
            "text": "Initial language for interview texts is Russian, and the Russian language BERT model from Deep Pavlov and an autoencoder for dimensionality reduction were used. For the English version of interviews, the BERT base uncased model was used. Clustering was done using K-means, resulting in challenges with clarity.\n\nThe BERT+UMAP+HDBSCAN algorithm was selected as the most suitable for semi-structured interviews, despite its longer processing time. BERT embeddings and UMAP for dimensionality reduction were found to be beneficial for interpretability. Across various sets of interviews, the BERT+HDBSCAN model consistently showed better results. However, for the prototype, model tuning was omitted in the frontend due to efficiency and time considerations."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6. The prototype",
            "text": "The prototype was based on completed objectives. Firstly, the researcher uploads a\ndocument with all the interviews to the website. Then he has a choice to either save or dismiss the Interviewer\u2019s phrases. This heavily depends on the researcher\u2019s perspective and type of interview. Then he has to press the lemmatize button and preprocess. Shortly after that, the program will give him the most frequently used words in the text, which he can load into the file \u201dadditional stop words\u201d if he wishes and add to the website again111https://github.com/Likich/TM_graph  ###reference_###.\nNext, he can choose several suggested methods for analyzing the interview. LDA is the classic method and is fast, while BERT gives higher-quality results but takes more time. The user can also choose how many topics he would like to see in the result. Next, the researcher will be provided with an interactive graph of connections (Figure 3  ###reference_###). At the moment it is only available in English and Russian languages, but in the future, there will be support for more languages.\n###figure_3### Having the set of Topics: , the graph is constructed as follows: each vertex of the graph is a keyword that belongs to the topic : . For each keyword in the topic we have its weight that demonstrates the importance of a word in describing the topic : .\nThus, the graph is constructed as follows:\nIn the middle is the keyword with the highest weight. The question arises as to whether the word with the highest weight characterizes the entire topic. The answer is no, but in practice, the word with the most weight in the topic is less likely to occur in other topics and is less likely to affect the quality of visualization. Moreover, using for example three keywords will not promise that they play a key role in determining the semantics of the topic, and visualization will be even more difficult.\nIf a researcher is interested in knowing what citations mentioned this exact word, he can double-click the vertex of the network and citations will be shown. This phase is important to facilitate the interpretation of topics.\nAccording to this visualization, you can also see which topics are linked by co-occurrence and which are not. For example, in a series of interviews about social expectations, it was seen how the central keyword in the family topic, \u201dparent,\u201d was related to \u201deducation\u201d as the central word in the education and work topic, and to \u201dshame,\u201d the central word in the emotion topic. In this way, the researcher can understand which codes affect which, and which tops are completely disconnected and represent a separate topic. Based on these results, we can already hypothesize about the influence of indicators on others and prepare the methodology for the quantitative phase of the study."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7. Empirical perspective",
            "text": "Automating the coding process in qualitative research can be an advantage for various fields such as market research, customer feedback analysis, and clinical data analysis. These areas often deal with vast amounts of unstructured data like interviews, focus groups, and open-ended survey responses, where traditional manual coding is time-consuming and subject to human biases.\nIn market research automated coding can analyze customer interviews and group discussions faster, helping businesses find new trends and customer preferences more quickly. As for customer feedback analysis, automated coding can process customer feedback from various channels (social media, customer surveys, etc.) in real-time, enabling companies to respond promptly to customer needs and complaints. Analyzing qualitative feedback at scale allows for more personalized marketing strategies based on nuanced customer preferences and experiences. Creating a concept network from qualitative feedback can help in creating detailed customer journey maps, identifying pain points, and enhancing customer experience.\nIn healthcare, automated coding of patient interviews and feedback can provide insights into patient experiences, leading to improved care and treatment strategies reducing the time for data analysis, and accelerating research outcomes. Analysis of patient narratives and feedback can reveal insights into the efficacy of treatments and patient feedback, aiding in the improvement of therapeutic approaches."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "8. Conclusion",
            "text": "The purpose of this study was to automate the analysis of semi-structured interviews, which is currently very time-consuming for individual researchers. It was also intended to write an application that would allow qualitative researchers with no knowledge of programming skills to use automatic text-processing methods. To achieve the goal, this paper considered methods of coding texts.\n\nThe theoretical significance of this work consists of testing more advanced methods of interview analysis. The main practical contribution is that researchers in qualitative studies now will have access to automatic analysis of their work, or a convenient basis for subsequent analysis. It cannot be argued that such work completely replaces the individual researcher, who, firstly, is more familiar with the topic, and secondly, can analyze the truthfulness of the answers. However, automated analysis frees the researcher from his or her subjectivity and can help avoid judgmental attitudes."
        }
    ],
    "appendix": [],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T1\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1. </span>Partial topic modeling results for one interview question</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.1.1.1\">Topic</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.1.2.1\">W 0</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.1.3.1\">W 1</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.1.4.1\">W 2</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.1.5.1\">W 3</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.1.1.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.1.6.1\">W 4</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T1.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T1.1.2.1.1\">1</th>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T1.1.2.1.2\">work</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T1.1.2.1.3\">to work</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T1.1.2.1.4\">to study</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T1.1.2.1.5\">career</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.1.2.1.6\">electric</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T1.1.3.2.1\">2</th>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.1.3.2.2\">good</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.1.3.2.3\">knowledge</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.1.3.2.4\">to study</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.1.3.2.5\">saying</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.3.2.6\">succeed</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T1.1.4.3.1\">3</th>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.1.4.3.2\">work</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.1.4.3.3\">to work</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.1.4.3.4\">educate</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.1.4.3.5\">do</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.4.3.6\">receive</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r\" id=\"S4.T1.1.5.4.1\">4</th>\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\" id=\"S4.T1.1.5.4.2\">to work</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\" id=\"S4.T1.1.5.4.3\">do</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\" id=\"S4.T1.1.5.4.4\">work</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\" id=\"S4.T1.1.5.4.5\">favorite</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S4.T1.1.5.4.6\">business</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 1. Partial topic modeling results for one interview question"
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T2\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2. </span>Model Comparison for Two Sets of Interviews</figcaption><div class=\"ltx_flex_figure ltx_flex_table\">\n<div class=\"ltx_flex_cell\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S5.T2.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T2.1.1.1\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" id=\"S5.T2.1.1.1.1\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"2\" id=\"S5.T2.1.1.1.2\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.1.1.1.2.1\">LDA</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"2\" id=\"S5.T2.1.1.1.3\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.1.1.1.3.1\">BERT+LDA+Kmeans</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"2\" id=\"S5.T2.1.1.1.4\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.1.1.1.4.1\">BERT+HDBSCAN</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"2\" id=\"S5.T2.1.1.1.5\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.1.1.1.5.1\">BERT+LDA+HDBSCAN</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\" id=\"S5.T2.1.1.1.6\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.1.1.1.6.1\">BERT+Kmeans</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r\" id=\"S5.T2.1.2.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.1.2.2.1.1\">Metrics</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T2.1.2.2.2\">1st</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\" id=\"S5.T2.1.2.2.3\">2nd</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T2.1.2.2.4\">1st</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\" id=\"S5.T2.1.2.2.5\">2nd</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T2.1.2.2.6\">1st</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\" id=\"S5.T2.1.2.2.7\">2nd</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T2.1.2.2.8\">1st</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\" id=\"S5.T2.1.2.2.9\">2nd</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T2.1.2.2.10\">1st</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T2.1.2.2.11\">2nd</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T2.1.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S5.T2.1.3.1.1\">C_v</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.3.1.2\">0.456</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T2.1.3.1.3\">0.413</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.3.1.4\">0.487</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T2.1.3.1.5\">0.435</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.3.1.6\">0.512</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T2.1.3.1.7\">0.478</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.3.1.8\">0.501</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T2.1.3.1.9\">0.465</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.3.1.10\">0.435</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.3.1.11\">0.398</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.4.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T2.1.4.2.1\">Umass</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.4.2.2\">-12.354</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T2.1.4.2.3\">-11.346</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.4.2.4\">-3.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T2.1.4.2.5\">-3.15</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.4.2.6\">-3.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T2.1.4.2.7\">-4.18</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.4.2.8\">-6.763</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T2.1.4.2.9\">-5.476</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.4.2.10\">-2.891</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.4.2.11\">-4.021</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.5.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T2.1.5.3.1\">NPMI</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.5.3.2\">-0.289</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T2.1.5.3.3\">-0.453</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.5.3.4\">-0.034</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T2.1.5.3.5\">-0.023</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.5.3.6\">-0.042</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T2.1.5.3.7\">-0.011</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.5.3.8\">-0.112</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T2.1.5.3.9\">-0.098</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.5.3.10\">-0.056</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.5.3.11\">-0.021</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.6.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T2.1.6.4.1\">UCI</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.6.4.2\">-8.984</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T2.1.6.4.3\">-10.354</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.6.4.4\">-2.135</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T2.1.6.4.5\">-2.817</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.6.4.6\">-0.698</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T2.1.6.4.7\">-0.453</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.6.4.8\">-4.120</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T2.1.6.4.9\">-2.867</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.6.4.10\">-2.409</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.6.4.11\">-1.309</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.7.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T2.1.7.5.1\">Topic diversity</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.7.5.2\">0.679</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T2.1.7.5.3\">0.690</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.7.5.4\">0.890</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T2.1.7.5.5\">0.789</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.7.5.6\">0.950</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T2.1.7.5.7\">0.984</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.7.5.8\">0.835</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T2.1.7.5.9\">0.905</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.7.5.10\">0.756</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.7.5.11\">0.740</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.8.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T2.1.8.6.1\">Silhouette</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.8.6.2\">NA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T2.1.8.6.3\">NA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.8.6.4\">0.389</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T2.1.8.6.5\">0.209</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.8.6.6\">NA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T2.1.8.6.7\">NA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.8.6.8\">NA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T2.1.8.6.9\">NA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.8.6.10\">0.067</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.8.6.11\">0.056</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.9.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r\" id=\"S5.T2.1.9.7.1\">DBCV</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T2.1.9.7.2\">NA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S5.T2.1.9.7.3\">NA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T2.1.9.7.4\">NA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S5.T2.1.9.7.5\">NA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T2.1.9.7.6\">0.516</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S5.T2.1.9.7.7\">0.602</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T2.1.9.7.8\">0.679</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S5.T2.1.9.7.9\">0.714</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T2.1.9.7.10\">NA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T2.1.9.7.11\">NA</td>\n</tr>\n</tbody>\n</table>\n</div>\n<div class=\"ltx_flex_cell\">\n<p class=\"ltx_p ltx_align_center\" id=\"S5.T2.2\"><span class=\"ltx_text\" id=\"S5.T2.2.1\" style=\"font-size:80%;\">Note: C_v measures topic coherence; Umass and UCI are coherence scores; NPMI is normalized pointwise mutual information; Topic diversity indicates the uniqueness of topics; Silhouette measures cluster separation; DBCV is density-based clustering validation.</span></p>\n</div>\n</div>\n</figure>",
            "capture": "Table 2. Model Comparison for Two Sets of Interviews"
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.04819v1_figure_1.png",
            "caption": "Figure 1. The coding process visualized"
        },
        "2": {
            "figure_path": "2403.04819v1_figure_2.png",
            "caption": "Figure 2. Comparison of model performance (Size of the bubble is Topic Diversity)"
        },
        "3": {
            "figure_path": "2403.04819v1_figure_3.png",
            "caption": "Figure 3. Output of the model on the set of interviews (Parfenova and Kozlova, 2023)"
        }
    },
    "references": [
        {
            "1": {
                "title": "AntConc: A Freeware Corpus Analysis Toolkit for Concording and Text Analysis.",
                "author": "2024.",
                "venue": "https://www.laurenceanthony.net/software/antconc/.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Atlas.ti: The Qualitative Data Analysis & Research Software.",
                "author": "2024.",
                "venue": "https://atlasti.com/.",
                "url": null
            }
        },
        {
            "3": {
                "title": "Dedoose: Web Application for Managing, Analyzing, and Presenting Qualitative and Mixed Method Research Data.",
                "author": "2024.",
                "venue": "https://www.dedoose.com/.",
                "url": null
            }
        },
        {
            "4": {
                "title": "MAXQDA: Qualitative & Quantitative Data Analysis Software.",
                "author": "2024.",
                "venue": "https://www.maxqda.com/.",
                "url": null
            }
        },
        {
            "5": {
                "title": "Interviewing as a data collection method: A critical review.",
                "author": "H. Alshenqeeti. 2014.",
                "venue": "English Linguistics Research 3 (2014), 39\u201345.",
                "url": null
            }
        },
        {
            "6": {
                "title": "Asynchronous on-line focus group: technology and procedures of conducting.",
                "author": "E.G. Avjyan. 2005.",
                "venue": "Southern Russian Journal of Social Sciences 1 (2005), 116\u2013129.",
                "url": null
            }
        },
        {
            "7": {
                "title": "Probabilistic topic models.",
                "author": "D. M. Blei. 2012.",
                "venue": "Commun. ACM 55, 4 (2012), 77\u201384.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Discovery of grounded theory: Strategies for qualitative research.",
                "author": "Barney Glaser and Anselm Strauss. 2017.",
                "venue": "Routledge.",
                "url": null
            }
        },
        {
            "9": {
                "title": "Natural Language Processing (NLP) in qualitative public health research: a proof of concept study.",
                "author": "W. Leeson, A. Resnick, D. Alexander, and J. Rovers. 2019.",
                "venue": "International Journal of Qualitative Methods 18 (2019).",
                "url": null
            }
        },
        {
            "10": {
                "title": "Conducting thematic analysis with qualitative data.",
                "author": "C. R. Lochmiller. 2021.",
                "venue": "The Qualitative Report 26, 6 (2021), 2029\u20132044.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Qualitative data analysis: An expanded sourcebook.",
                "author": "Matthew B Miles and A Michael Huberman. 1994.",
                "venue": "sage.",
                "url": null
            }
        },
        {
            "12": {
                "title": "The regulatory power of social expectations: developing a measurement scale.",
                "author": "Angelina Parfenova and Maria Kozlova. 2023.",
                "venue": "International Journal of Sociology and Social Policy 43, 5/6 (2023), 569\u2013585.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Techniques to identify themes.",
                "author": "G. W. Ryan and H. R. Bernard. 2003.",
                "venue": "Field Methods 15, 1 (2003), 85\u2013109.",
                "url": null
            }
        },
        {
            "14": {
                "title": "The Coding Manual for Qualitative Researchers (3rd ed.).",
                "author": "J. Saldana. 2016.",
                "venue": "Sage, Los Angeles, CA.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.04819v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "3"
        ],
        "methodology_sections": [
            "4",
            "4.1"
        ],
        "main_experiment_and_results_sections": [
            "5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "4.1",
            "5"
        ]
    },
    "research_context": {
        "paper_id": "2403.04819v1",
        "paper_title": "Automating the Information Extraction from Semi-Structured Interview Transcripts",
        "research_background": "**Motivation:**\nThe motivation behind this paper arises from the extensive human effort and time required to manually analyze qualitative interview data. Unlike quantitative data, qualitative information does not provide clear-cut answers or statistics, necessitating a rigorous and time-consuming coding process. This labor-intensive approach hinders efficiency in research fields like marketing, social science, and medical studies. Therefore, there is a demonstrated need for automating the information extraction process from semi-structured interview transcripts to streamline the analysis and enhance objectivity.\n\n**Research Problem:**\nThe primary research problem addressed in this study is how to automate the analysis of qualitative research results, specifically interviews and focus groups. The manual methods currently in use, such as open and axial coding, require repetitive reading and coding of text, which is time-consuming and prone to the issue of interpretational objectivity. The paper seeks to develop an automated solution or software to facilitate this process, thereby reducing the manual workload and providing more consistent and efficient outcomes.\n\n**Relevant Prior Work:**\n1. **Qualitative Methods in Various Fields:** The paper references the work of Avjyan (2005) and Leeson et al. (2019), which highlight the significance of qualitative methods like interviews in fields such as marketing, social science, and medical studies.\n2. **Coding Techniques:** Several existing coding techniques are mentioned, including open coding and axial coding. Saldana (2016) provides an overview of these methodologies, detailing the repetitive reading and categorization required to identify key thoughts and relationships within the text.\n3. **Time-consuming Nature of Manual Coding:** Alshenqeeti (2014) emphasizes the considerable time investment needed to manually code interview data, often taking several weeks to process a large number of documents.\n\nBy drawing on this prior work, the paper justifies its goal to create an automated solution that can effectively analyze qualitative data, intending to offer a more efficient alternative to manual transcription and coding methods.",
        "methodology": "The paper \"Automating the Information Extraction from Semi-Structured Interview Transcripts\" aims to identify the most effective method for analyzing qualitative research transcripts by comparing several topic modeling techniques. The proposed methodology includes three main tasks:\n\n1. **Comparison of Topic Modeling Methods:** The paper will compare various topic modeling methods to determine which is most suitable for semi-structured interview transcripts. The challenge here is that these transcripts share characteristics with both short texts (like tweets) and lengthy narratives (like large documents), thus making standard topic modeling methods less effective. \n\n2. **Text Preprocessing:** Before constructing topic models, the data undergoes essential preprocessing steps. This includes tokenizing and lemmatizing sentences, removing default stop words, and creating a complementary set of stop words\u2014compiled independently by analyzing token frequency tables. Unwanted words commonly appearing in interviews (e.g., \"probably\", \"it turns out\", \"in general\", \"supposedly\", \"like\", and reflective verbs like \"think\" and \"suppose\") are removed to avoid skewing the topic models.\n\n3. **Document Compilation and Topic Modeling:** The methodology tests two approaches for document compilation:\n   - **Combining Answers to Questions:** In this approach, answers to questions are combined into one large document for each question, and topic models are built from these combined documents.\n   - **Aggregate Model:** All interview transcripts are aggregated to build a single large model.\n\n**Key Components and Innovations:**\n- **Customized Preprocessing:** The tailored preprocessing that includes creating an interview-specific stop word list is a critical innovation to enhance the clarity and relevance of the extracted topics.\n- **Aggregate Model Approach:** Moving away from combining individual question responses to leveraging the entire body of transcripts is a methodological innovation aimed at capturing the richness and diversity of the topics discussed across the interviews.\n- **Visualizing Keywords Framework and User-Friendly Frontend:** While not deeply explored in the provided excerpt, the methodology outlines the intention to create a framework for visualizing selected keywords and developing an accessible frontend for researchers, which signifies an emphasis on practical utility and usability for end-users.\n\nThese components collectively aim to create a robust and appropriate framework for the automated analysis of qualitative research data obtained from semi-structured interviews.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\n**Datasets**:\n- The experiment involved semi-structured interview transcripts.\n- Initial language for the texts was Russian, but English versions were also considered.\n\n**Baselines**:\n- The baseline for the study was established using the standard LDA (Latent Dirichlet Allocation) package from Gensim.\n\n**Main Model and Methods**:\n1. **LDA with BERT Embeddings**:\n   - Purpose: Enhance the contextual understanding of topics.\n   - Tools: Russian language BERT model from Deep Pavlov and an English BERT base uncased model.\n   - Extra Processing: Autoencoder used for dimensionality reduction.\n   - Clustering Technique: K-means.\n\n2. **Top2Vec with BERT Embeddings**:\n   - Purpose: Improve topic clarity and interpretability.\n   - Embedding: BERT model for embeddings.\n   - Dimensionality Reduction: UMAP.\n   - Clustering: HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise).\n\n**Evaluation Metrics**:\n- **C_v**: Topic coherence metric.\n- **Umass and UCI**: Coherence scores for measuring the quality of topics.\n- **NPMI**: Normalized pointwise mutual information, another coherence measure.\n- **Topic Diversity**: Indicates uniqueness of topics.\n- **Silhouette Score**: Measures cluster separation.\n- **DBCV (Density-Based Clustering Validation)**: Evaluates clustering quality based on density.\n\n**Results**:\n- The LDA model showed moderate quality metrics but had issues with clearly distinguishing topics.\n- Combining LDA with BERT embeddings did not significantly resolve the clarity issues.\n- The Top2Vec method with BERT, UMAP for dimensionality reduction, and HDBSCAN for clustering resulted in more interpretable topics.\n- The BERT+UMAP+HDBSCAN algorithm was found to be the most suitable despite having a longer processing time.\n- Across different sets of interviews, the BERT+HDBSCAN model consistently ranked high in topic diversity and interpretability.\n\nNote: For the prototype, model tuning was omitted in the frontend to enhance efficiency and save time."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To determine the most appropriate method for analyzing qualitative research transcripts, particularly semi-structured interview transcripts, by comparing different topic modeling techniques.",
            "experiment_process": "Various preprocessing steps were undertaken before constructing topic models, including tokenization, lemmatization, and the removal of default and complementary stop words. Two approaches were tested: one combining answers to questions into large documents and another using all transcripts to build one large model. The first approach, combining answers to questions, was deemed unsatisfactory due to similar received topics. Therefore, the second approach, using the aggregate of all documents, was deemed more appropriate.",
            "result_discussion": "The topics obtained by compiling answers to individual questions were very similar and unsatisfactory due to each question usually being about one specific topic. Therefore, future model development was conducted on the aggregate of all documents.",
            "ablation_id": "2403.04819v1.No1"
        },
        {
            "research_objective": "To enhance the quality and interpretability of topic modeling for semi-structured interview transcripts by incorporating advanced embedding and clustering techniques.",
            "experiment_process": "The initial baseline was established using the standard LDA package from Gensim. Subsequently, the study explored combining LDA with BERT embeddings for better contextual understanding, using models suitable for Russian and English texts. K-means was used for clustering but struggled with topic clarity. The Top2Vec approach with BERT embeddings, UMAP for dimensionality reduction, and HDBSCAN for clustering was also tested. Throughout, various quality metrics like coherence scores, topic diversity, and others were evaluated to determine the best-performing model.",
            "result_discussion": "While the initial LDA model presented moderate quality metrics, it failed to distinguish topics effectively. Combining LDA with BERT did not significantly improve clarity. The Top2Vec method with BERT+UMAP+HDBSCAN yielded the most interpretable topics but required longer processing times. Across various interviews, the BERT+HDBSCAN model showed high topic diversity and better interpretability. For prototype implementation, model tuning was omitted for efficiency.",
            "ablation_id": "2403.04819v1.No2"
        }
    ]
}