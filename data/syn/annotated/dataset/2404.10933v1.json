{
    "title": "LLMem: Estimating GPU Memory Usage for Fine-Tuning Pre-Trained LLMs",
    "abstract": "Fine-tuning pre-trained large language models (LLMs) with limited hardware presents challenges due to GPU memory constraints. Various distributed fine-tuning methods have been proposed to alleviate memory constraints on GPU. However, determining the most effective method for achieving rapid fine-tuning while preventing GPU out-of-memory issues in a given environment remains unclear. To address this challenge, we introduce LLMem, a solution that estimates the GPU memory consumption when applying distributed fine-tuning methods across multiple GPUs and identifies the optimal method. We conduct GPU memory usage estimation prior to fine-tuning, leveraging the fundamental structure of transformer-based decoder models and the memory usage distribution of each method. Experimental results show that LLMem accurately estimates peak GPU memory usage on a single GPU, with error rates of up to 1.6%. Additionally, it shows an average error rate of 3.0% when applying distributed fine-tuning methods to LLMs with more than a billion parameters on multi-GPU setups.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Since the introduction of the Transformer model Vaswani et al. (2017), researchers have proposed numerous language models based on it. As the model\u2019s performance has improved, its size has grown exponentially, necessitating a substantial dataset for training. However, training emerging large language models (LLMs) is infeasible without a dedicated infrastructure with high-performance hardware due to memory constraints. Instead, it is preferred to utilize a small dataset to fine-tune a pre-trained model for a specific application.\n\nTo efficiently handle small datasets and reduce training time, the conventional method of data parallelism places the entire model on each GPU, splits the dataset, and trains simultaneously. Nevertheless, the model size remains huge, potentially causing GPU out-of-memory (OOM) issues. Therefore, it is necessary to reduce the amount of memory a GPU uses by splitting the model and distributing it to each GPU.\n\nZeRO Rajbhandari et al. (2020) Stage 3 is an advanced data parallelism method that partitions the model parameters, gradients, and optimizer states to each GPU for memory advantage while maintaining the distribution of the dataset across GPUs. Although ZeRO Stage 3 saves memory by using only partitioned model data on each GPU during non-computation phases, there are limitations in preventing GPU OOM issues because partitioned parameters/gradients must be all-gathered during computation.\n\nTensor parallelism divides each parameter tensor in the model into rows or columns and distributes them to each GPU, using only partitioned parameters on each GPU during computation. For example, Megatron-LM Shoeybi et al. (2019), a representative tensor parallelism method, splits a tensor along its rows or columns considering the position and connection of operators. By doing so, it can reduce GPU memory usage more than data parallelism when the model size is large.\n\nAs we described above, various distributed fine-tuning methods have been proposed, but the GPU memory usage and fine-tuning time required for each are different. For instance, conventional data parallelism provides the shortest fine-tuning time but requires the highest GPU memory usage. On the other hand, tensor parallelism has no benefit in saving fine-tuning time but can significantly reduce GPU memory usage. Users may want to select an appropriate method that avoids GPU OOM and has a short fine-tuning time. However, it is difficult to determine in advance whether there is enough GPU memory to fine-tune a given pre-trained LLM.\n\nDNNMem Gao et al. (2020) is the most recent work detailing procedures to estimate GPU memory usage on a single GPU. DNNMem provides key equations for GPU memory estimation when training various DNN models by analyzing the connections between operators and live tensors in the forward and backward passes. However, it has limitations for fine-tuning LLMs. GPU memory estimation for fine-tuning transformer-based LLM is challenging for two reasons.\n\nFirst, when fine-tuning an LLM in multi-GPU, distributed fine-tuning methods should be used to overcome GPU memory constraints due to large model sizes. Depending on the method used, the distribution of parameters, gradients, and optimizer states to each GPU is different, as is the amount of GPU memory used during the calculation process. Therefore, GPU memory usage estimates from a single GPU cannot be used in a multi-GPU environment.\n\nSecond, GPU memory consumption must be predicted by distinguishing between transformer and language modeling head (lm_head) parts. The transformer part is the central part of fine-tuning, where chunk memory management for memory sharing of model parameters and gradients is applied, and parameters are updated. On the other hand, the lm_head part requires separate analysis because it does not apply distributed methods directly and consumes a lot of memory due to its large dictionary size.\n\nTo address these challenges, we propose LLMem that estimates the GPU memory consumption when applying distributed fine-tuning methods to multiple GPUs.\n\nLLMem considers several factors to estimate GPU memory usage for each method, including recombining parameters prior to computation when applying advanced data parallelism and the output driven by all-gather in the backward pass when using tensor parallelism. Additionally, LLMem analyzes the difference in memory allocation method between the transformer and the lm_head part and reflects it in GPU memory estimation.\n\nTo the best of our knowledge, this is the first work to estimate the peak GPU memory consumption for LLM fine-tuning.\n\nIn summary, our contributions are:\n\nWe propose a GPU memory usage estimation method for LLM fine-tuning on single and multiple GPUs.\n\nWe provide an algorithm to determine the most efficient distributed fine-tuning method based on GPU memory usage estimation.\n\nExperimental results show that LLMem estimates peak GPU memory usage to fine-tune LLM on a single GPU with error rates of up to 1.6%, which is significantly smaller than the state-of-the-art DNNMem\u2019s average error rate of 42.6%. When applying distributed fine-tuning methods to LLM"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Works",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "GPU Memory Estimation",
            "text": "There have been several attempts to avoid GPU OOM issues by predicting the GPU memory usage that will be used to train a given model in advance. DNNMem Gao et al. (2020  ###reference_b8###) sequentially traverses the computation graph of a DL model and computes the GPU memory consumption by taking into account previously allocated but still in-use tensors, newly allocated tensors for the currently visited operator, and resident buffers of the CUDA context and allocator reservation. Our LLMem is inspired by DNNMem, whose mechanism is described in more detail in Section 3  ###reference_###. TSplit Nie et al. (2022  ###reference_b11###) also calculates the total size of live tensors for the visiting operator. However, TSplit lacks an explanation of the detailed memory estimation process and its accuracy. SchedTune Albahar et al. (2022  ###reference_b1###) predicts GPU memory usage not only based on DL model characteristics but also on different GPU types running the job. However, using measured GPU memory as data for prediction does not align with the purpose of estimating memory usage before fine-tuning a model."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Distributed Fine-Tuning with GPUs",
            "text": "Data parallelism can enhance fine-tuning speed in proportion to the number of GPUs. However, LLM often runs into memory constraints, so the ZeRO optimizer Rajbhandari et al. (2020  ###reference_b13###), described in Section 1  ###reference_###, is widely used as an alternative.\nThe ZeRO optimizer selectively gathers only the model parameters or gradients required during the computation process and utilizes reduce-scatter after the computation to maintain their partitioning on each GPU.\nTensor parallelism can further reduce peak GPU memory usage by sharding tensors under certain conditions, eliminating the need to gather all model parameters and gradients even during computation. Tensor parallelism results in each GPU producing only partial results, necessitating that all GPUs receive the same input data. The widely adopted tensor parallelism method, Megatron-LM Shoeybi et al. (2019  ###reference_b15###), splits each model parameter tensor by row or column. Other proposed methods Xu et al. (2021  ###reference_b22###) Wang et al. (2021  ###reference_b20###) Bian et al. (2021  ###reference_b3###) achieve additional memory savings by sharding both input and model parameters.\nIf GPU memory constraints cannot be met with any distributed fine-tuning method on GPUs alone, we can use heterogeneous fine-tuning utilizing CPU memory. ZeRO-offload Ren et al. (2021  ###reference_b14###) manages gradients, optimizer states, and optimizer computation on the CPU while retaining parameters and forward and backward computation on the GPU."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Motivation",
            "text": "To select distributed fine-tuning methods, it is crucial to estimate GPU memory usage accurately. Existing approaches for estimating GPU memory usage do not consider scenarios where advanced data parallelism, such as the ZeRO Stage 3 optimizer (Rajbhandari et al., 2020), or tensor parallelism is applied across multiple GPUs. Relying solely on estimated GPU memory usage on a single GPU when estimating on multiple GPUs can lead to significant errors. In this section, we implement the existing DNNMem (Gao et al., 2020), validate the implementation results, and discuss factors causing substantial GPU memory estimation errors during the fine-tuning of pre-trained transformer-based language models."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "DNNMem Implementation",
            "text": "DNNMem source codes are not publicly available and are mainly based on TensorFlow, so we implement DNNMem based on the description in the paper Gao et al. (2020). First, we extract the corresponding computation graph from a given pre-trained DL model to identify the output size in each operator based on parameters, batch size, and sequence length. We also compute pre-allocated GPU memory, including CUDA context and weight tensors of the model, before operator execution. In particular, since PyTorch does not release the loaded model parameters until the end of the fine-tuning, the initial GPU memory is retained throughout the fine-tuning process. The next step is to compute peak GPU memory usage at each operator while traversing the graph. We compute additional GPU memory with the input/output tensors and previously unreleased tensors in each operator during the forward propagation. Additionally, we reflect that PyTorch aligns with multiples of 512 bytes for internal tensor fragmentation, and DNNMem treats the buffer size as a constant (64 MB by default) as memory block management."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Limitations of DNNMem for LLM Fine-Tuning Memory Estimation",
            "text": "DNNMem Gao et al. (2020) does not handle mixed precision, which is commonly used in fine-tuning pre-trained language models. In addition, it does not consider how memory chunks are managed to ensure that forward pass parameters and backward pass gradients share the same GPU memory space Fang et al. (2022). Furthermore, DNNMem overlooks extra GPU memory usage during the initial fine-tuning iteration due to optimizer states.\n\nComparison results for estimating peak GPU memory usage of our proposed LLMem and DNNMem on a single GPU are shown in Figure 1. The experimental environment is summarized in Section 7.1. LLMem predicts peak GPU memory usage with minimal error rates compared to ground truth, outperforming DNNMem. DNNMem exhibits larger errors as the total parameter size increases. Furthermore, DNNMem fails to predict GPU memory consumption in the context of distributed fine-tuning methods across multiple GPUs. As a result, existing approaches for estimating GPU memory usage face challenges when using the current transformer-based LLM for distributed fine-tuning."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Single-GPU Memory Usage Estimation",
            "text": "This section outlines considerations for estimating GPU memory usage of transformer-based language models on a single GPU. The symbols used in the explanation are organized in Table 1  ###reference_###.\n###figure_2###"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Workflow for Fine-Tuning Pre-Trained Models",
            "text": "Initialization phase. The initialization phase preceding fine-tuning involves allocating memory for the CUDA context, responsible for managing information to control GPU devices, and memory for applying chunk-based memory management Fang et al. (2022  ###reference_b7###). The initially used GPU memory is denoted as . The chunk manager determines the optimal chunk size to minimize GPU memory waste based on the parameters of the provided pre-trained language model. GPU memory spaces for param fp16 (float-16) and param fp32 (float-32) are allocated in units of the chunk size ( 1 in Figure 4  ###reference_###).\nFine-tuning phase. During the fine-tuning phase, param fp16 goes through forward and backward passes, and param fp16 is converted to gradient fp16, as illustrated in Figure 2  ###reference_###. Consequently, param fp16 and gradient fp16 share the same GPU memory space. After the backward pass, the ADAM optimizer updates parameters using optimizer states, including param fp32, momentum fp32, and variance fp32 tensors. Momentum fp32 and variance fp32 tensors, which are not allocated memory during the initialization process before fine-tuning, consume GPU memory based on the actual tensor size, not the chunk size. GPU memory occupied by these tensor types is allocated in the first iteration for fine-tuning ( 5 in Figure 4  ###reference_###). Subsequently, similar to chunk-based parameters, the GPU memory is retained until the fine-tuning process is complete.\n###figure_3###"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Memory Consumption with Structure of Transformer-based Decoder Model",
            "text": "The peak GPU memory usage on a single GPU () is\nEach variable in this formula, except  described in Section 4.1  ###reference_###, is calculated as follows.\nFirst,  is the GPU memory used by param/gradient fp16 and param fp32. Considering that param fp16 and fp32 use 2 bytes () and 4 bytes () per value, respectively,  is\n, where  is the input embedding param size,  is the chunk size,  is the remaining param size, and  is the CUDA memory page size, typically  bytes. Transformer-based decoder models Vaswani et al. (2017  ###reference_b18###) are largely divided into a transformer model for fine-tuning and lm_head for output, as shown in Figure 3  ###reference_###. The part that uses the chunk memory is the transformer model in which the parameters are updated. The  is huge because the input embedding has a large dictionary. Therefore,  is managed separately.\nSecond,  is the GPU memory used by momentum fp32 and variance fp32 of optimizer states.  is\n, where  is the operator of the given transformer model,  is Embedding,  is Linear, and  is the parameter size of . The system allocates GPU memory based on the actual size of each momentum fp32 and variance fp32, so GPU memory must be calculated for each tensor of each operator. Since the amount of GPU memory consumed by Bias or LayerNorm is very small, they can use space with other memory fragmentation. Therefore, we only calculate the GPU memory usage due to Embedding or Linear operator parameters.\nThird,  is the peak GPU memory usage due to output tensors. If the number of Embedding, layers, and model\u2019s output features are , , and , respectively, then  is\nPyTorch provides gradient checkpointing333Gradient checkpointing reduces GPU memory usage by clearing specific outputs and recomputing them during a backward pass. as an option to save memory during fine-tuning. Therefore, we support estimating GPU memory usage due to each operator\u2019s input/output tensors considering gradient checkpointing. Since the output tensors of the current operator are the input tensors of the next operator, we focus on the output. It is challenging to accurately predict GPU memory consumption due to the outputs of operators within a model. We observed that the layer and embedding outputs of the transformer model are kept in GPU memory for efficient gradient checkpointing, which minimizes the increase in fine-tuning time. The estimation error rate is reduced using the  equation, which accounts for our observation.\nLastly,  is the GPU memory used in the lm_head part including the loss calculation part. If the size of the embedding dictionary is ,  is\n, where  is the lm_head param size, and  is either  or  depending on the model type. The lm_head converts the transformer outputs into logits. Then, the value obtained by shifting the sequence length of the logits by one space is stored in a separate temporary variable and used for the loss calculation.  is the output phase ( 3 in Figure 4  ###reference_###).\n###figure_4### ###figure_5### ###figure_6### ###figure_7###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Multi-GPU Memory Usage Estimation",
            "text": "This section outlines the factors for estimating peak GPU memory usage during distributed fine-tuning on multiple GPUs and summarizes the estimation process.\nConventional data parallelism (CDP). Since CDP places the entire model on each GPU, its peak GPU memory usage estimation equals the peak single-GPU memory usage , as shown in Figure 4  ###reference_###.\nAdvanced data parallelism (ADP). The peak GPU memory usage with ADP on multiple GPUs () is\n, where  and  are the GPU memory consumed by the entire param fp16/fp32, and  is the number of GPUs in use. ZeRO-3 optimizer, a method of advanced data parallelism, evenly distributes parameters, gradients, and optimizer states by , reducing GPU memory usage. Among these, gradient fp16 shares GPU memory with param fp16 as explained in Section 4  ###reference_###, so we only need to divide the GPU memory usage of parameters and optimizer states by . However, during the calculation process, each GPU must have all the values of param fp16 (Figure 5(a)  ###reference_sf1### and  2 in Figure 4  ###reference_###), so the whole  is allocated to the GPU memory.\nTensor parallelism (TP). The peak GPU memory usage with 1D TP on multiple GPUs () is\n, where  ( 4 in Figure 4  ###reference_### and Figure 6  ###reference_###) is the additional GPU memory usage due to the temporary buffer through the backward all-gather. If the number of GPUs used for tensor parallelism is ,  is\nTensor parallelism divides the parameter values of each operator by  and does not combine them again, as shown in Figure 5(b)  ###reference_sf2###. It splits each model parameter tensor by row or column to apply tensor parallelism to multiple pre-trained language models. We call this one-dimension tensor parallelism (1D TP). Let us assume that we apply 1D TP to a linear operation on four GPUs. The linear operator\u2019s equation is , where  is output,  is input,  is params/gradients, and  is bias. The linear matrix multiplication process when each parameter tensor is split into columns is shown in Figure 6  ###reference_###. We shard parameters by column because the output size after multiplication is the same as the size of the bias without sharding, so it is not affected by the use of bias. In the backward pass, the fine-tuning goes through an all-gather process.  is the total temporary buffer size for tensors imported from the other GPUs, calculated by multiplying the output size of each layer by the number of layers.\nCombination of DP+TP. The peak GPU memory usage with the combination of DP+TP on multiple GPUs () is\n, as shown in Figure 4  ###reference_###. It is possible to achieve hybrid parallelism by fine-tuning through a combination of data and tensor parallelism."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Distributed Fine-Tuning Method Decision",
            "text": "Algorithm 1  ###reference_### describes the process for selecting the optimal method to fine-tune a pre-trained model based on the results of estimating the peak GPU memory usage. In Sections 4  ###reference_### and 5  ###reference_###, We estimated , , , and . Here,  represents CDP, and the remaining estimations are connected to ADP, TP, and DP+TP, respectively. Of these methods, the optimal one is the method that requires the shortest time for fine-tuning while avoiding GPU OOM.\nLLMem takes a pre-trained model , the total number of GPUs to fine-tune , and the maximum sequence length .  is a list that stores the performance evaluation score of each method. , , , and  correspond to CDP, ADP, TP, and DP+TP, respectively. LLMem increments the batch size  for each method and gets the value of  when it reaches the total GPU memory capacity. Then,  is the largest batch size to avoid GPU OOM. CDP uses  amount of data for fine-tuning in one iteration. In addition, since the ZeRO-3 optimizer increases the total communication volume of a baseline DP to  Rajbhandari et al. (2020  ###reference_b13###), the performance score of CDP is . In one iteration, ADP uses , TP uses , and DP+TP uses  of data for fine-tuning.  is the number of GPUs used for DP. These values become the performance scores of each method. Finally, LLMem selects the method with the highest performance score (If the scores are tied, select CDP, ADP, TP, and DP+TP in that order). If the performance scores of all methods are 0, heterogeneous training using CPU memory is selected as an alternative to avoid GPU OOM.\nInput: Pre-trained model , , and \nOutput: Selected fine-tuning method and the optimal"
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this section, we compare the peak GPU memory usage estimate of LLMem with the ground truth data when applying various distributed fine-tuning methods. In addition, our DNNMem implementation is included in comparing GPU memory usage estimation on a single GPU."
        },
        {
            "section_id": "7.1",
            "parent_section_id": "7",
            "section_name": "Experimental Setup",
            "text": "For a multi-GPU environment, we use a Tesla V100 (total GPU memory capacity: 16384 MB) with 4 GPUs in CloudLab CloudLab (2023). We also use the Colossal-AI Li et al. (2023), a widely used framework for applying distributed fine-tuning methods, and PyTorch 2.0.1 with CUDA 11.7. The models we used in the experiment are OPT Zhang et al. (2022), BLOOM Workshop et al. (2022), CodeGen Nijkamp et al. (2022), BioGPT Luo et al. (2022), GPTBigCode Allal et al. (2023), GPT Neo Black et al. (2021), and LLaMA Touvron et al. (2023). The dataset used is alpaca data Taori et al. (2023), which is 52K instruction-following data. For the ground truth data, we measure peak GPU memory usage using only the maximum sequence length of 512."
        },
        {
            "section_id": "7.2",
            "parent_section_id": "7",
            "section_name": "Estimation of Single-GPU Memory Usage",
            "text": "First, we compare the peak GPU memory usage estimate from LLMem for a single GPU with the DNNMem estimate and the actual peak GPU memory usage. Since we used gradient checkpointing for LLM fine-tuning, the same approach was applied to DNNMem. Figure 7 compares the peak GPU memory usage estimation results of LLMem and DNNMem for various pre-trained LLMs that cause GPU OOM during fine-tuning on a single GPU. LLMem predicts GPU OOM for all models, while DNNMem predicts peak GPU memory usage that falls short of. Table 2 shows the predicted and actual GPU memory usage peaks when applying the maximum batch size to obtain the ground truth data for each model during fine-tuning on a single GPU. DNNMem underestimates the peak GPU memory usage for all models because it does not account for factors considered when fine-tuning Transformer-based LLM, as explained in Section 3.2. LLMem\u2019s GPU memory estimation helps approximate the peak GPU memory usage close to the ground truth."
        },
        {
            "section_id": "7.3",
            "parent_section_id": "7",
            "section_name": "Estimation of Multi-GPU Memory Usage",
            "text": "CDP. The experimental results are the same as the memory usage estimation results on a single GPU in Section 7.2.\n\nADP. Figure 8 shows the predicted and actual GPU memory usage peaks when applying the maximum batch size to obtain ground truth data for each model during fine-tuning with ADP on four GPUs. The error rate between the predicted value of LLMem and the actual GPU memory usage tends to increase on multi-GPU setups. One reason is the gap in memory usage between the GPUs. ADP places tensors separately on each GPU instead of being sharded, so not all GPUs can use precisely the same amount of memory. Second, the error tends to be slightly larger when the model size is large. A larger number of layers and outputs in large models can lead to larger error rates due to memory allocator characteristics.\n\nTP and DP+TP. Figure 9 shows the predicted and actual GPU memory usage peaks when applying the maximum batch size to obtain ground truth data for each model during fine-tuning with 4TP or 2DP+2TP on four GPUs. 4TP uses 4 GPUs in TP, and 2DP+2TP uses 2 GPUs in DP and 2 GPUs in TP for hybrid parallelism. We focus on estimating the peak GPU memory usage of the large-size model for TP because LLMem can select DP for quick fine-tuning of models that are small and do not have OOM problems.\n\nTP applies the all-gather operation in the backward pass, as shown in Figure 6. The all-gather operation allocates temporary buffers in GPU memory and collects values in those buffers, consuming additional GPU memory. If the model size is large and the possible batch size is small, the system can use the allocated but currently empty memory space for a temporary buffer. Therefore, the GPU memory consumed due to the temporary buffer does not increase excessively, leading to smaller errors as shown in Figure 9. 2DP+2TP shows slightly larger errors than 4TP in most cases. This is because GPU memory usage due to the temporary buffer may be additionally affected in 2 and 4 of Figure 4 while applying both DP and TP."
        },
        {
            "section_id": "7.4",
            "parent_section_id": "7",
            "section_name": "Fine-Tuning Method Selection with LLMem",
            "text": "Table 3 assesses whether LLMem finds the optimal fine-tuning method to achieve the fastest fine-tuning while avoiding GPU OOM for various models. When measuring the time taken for each method, we applied the maximum batch size that can prevent GPU OOM. LLMem typically selects TP when DP causes GPU OOM. It is challenging for LLMem to choose DP+TP because only 4 GPUs were used in the experiment. DP+TP allows for more diverse combinations depending on the number of GPUs used and is more likely to be selected. LLMem also suggests CPU offloading when GPU memory is insufficient."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "This paper introduces LLMem, a method for estimating GPU memory consumption during fine-tuning of large language models (LLMs) on multi-GPU setups. We analyze factors affecting GPU memory usage, considering different memory allocation methods for the transformer and output sections. Experimental results demonstrate that LLMem achieves accurate peak GPU memory usage estimation on both single and multiple GPUs with minimal error rates."
        }
    ],
    "url": "http://arxiv.org/html/2404.10933v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2",
            "3",
            "3.1",
            "3.2"
        ],
        "methodology_sections": [
            "4",
            "4.1",
            "4.2",
            "5",
            "6"
        ],
        "main_experiment_and_results_sections": [
            "7",
            "7.1",
            "7.2",
            "7.3",
            "7.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "3.1",
            "3.2",
            "7",
            "7.1",
            "7.2",
            "7.3",
            "7.4"
        ]
    },
    "research_context": {
        "paper_id": "2404.10933v1",
        "paper_title": "LLMem: Estimating GPU Memory Usage for Fine-Tuning Pre-Trained LLMs",
        "research_background": "### LLMem: Estimating GPU Memory Usage for Fine-Tuning Pre-Trained LLMs\n\n#### Motivation:\nThe rapid advances and widespread adoption of the Transformer model, as introduced by Vaswani et al. (2017), have led to a proliferation of large language models (LLMs) that require substantial computational resources to train. However, training these large models from scratch is often infeasible due to hardware constraints, making fine-tuning pre-trained models on specific tasks a more practical approach. This process can result in GPU out-of-memory (OOM) errors because of the large model sizes, even when datasets are small.\n\nTwo prominent methods exist to distribute the training load across GPUs to mitigate memory usage issues:\n1. **Data Parallelism** - This method places the entire model on each GPU and splits the dataset. While reducing training time, it still results in high memory usage.\n2. **Tensor Parallelism** - This method divides each parameter tensor among GPUs, significantly reducing memory usage but not affecting fine-tuning time.\n\nDespite the existence of these methods, there's a notable gap in tools that can accurately estimate GPU memory usage for fine-tuning LLMs in a multi-GPU environment, making it difficult for researchers and practitioners to choose the appropriate method upfront. \n\n#### Research Problem:\nThe primary research problem addressed in this paper is the lack of precise tools and methods to estimate GPU memory consumption when fine-tuning large language models using distributed training methods (data parallelism and tensor parallelism) in multi-GPU setups. Accurate estimation is critical to prevent GPU OOM issues and optimize fine-tuning time. Existing methods, like DNNMem, are limited to single GPU environments and cannot be applied directly to LLMs fine-tuned across multiple GPUs due to differences in memory distribution and consumption patterns.\n\n#### Relevant Prior Work:\n1. **Transformer Model (Vaswani et al., 2017)** - The foundational architecture on which many LLMs are based.\n2. **ZeRO (Rajbhandari et al., 2020)** - Proposes advanced data parallelism by partitioning model components (parameters, gradients, optimizer states) across GPUs to save memory but encounters limitations due to the requirement of all-gather operations during computation phases.\n3. **Megatron-LM (Shoeybi et al., 2019)** - Demonstrates tensor parallelism by splitting tensors along rows or columns to reduce GPU memory usage more effectively than traditional data parallelism.\n4. **DNNMem (Gao et al., 2020)** - The most recent approach to estimating GPU memory usage on a single GPU, providing key equations for this purpose by analyzing the interconnections of operators and live tensors during forward and backward passes. However, it falls short in multi-GPU environments and the specific context of LLM fine-tuning.\n \n#### Proposed Solution (LLMem):\nThe paper proposes **LLMem**, a novel tool for estimating GPU memory consumption when fine-tuning pre-trained LLMs using distributed methods across multiple GPUs. LLMem accounts for various factors such as:\n- **Recombining Parameters** - Before computation in advanced data parallelism.\n- **All-Gather Operations** - During the backward pass in tensor parallelism.\n- **Structural Differences** - Between transformer components and the language modeling head (lm_head) in LLMs.\n\nLLMem provides insightful GPU memory usage predictions with high accuracy (within a 1.6% error rate for single GPU and an average of 3.0% for multi-GPU environments on models with over a billion parameters). \n\n#### Contributions:\n1. An innovative method for estimating GPU memory usage during LLM fine-tuning on single and multiple GPUs.\n2. An algorithm for selecting the most efficient distributed fine-tuning method based on these estimations.\n3. Experimental validation demonstrating LLMem\u2019s superior accuracy compared to existing tools like DNNMem.\n\nThe source code for LLMem is made available at the linked GitHub repository, encouraging further development and practical application of the proposed techniques.",
        "methodology": "The methodology section of the paper proposes a method to estimate the GPU memory usage associated with fine-tuning pre-trained transformer-based language models on a single GPU. The key components and innovations of the proposed method are as follows:\n\n1. **Transformer-Based Language Models**: The focus is on estimating memory usage specifically for transformer-based language models, which are prevalent in tasks related to natural language processing.\n\n2. **Single GPU Consideration**: The methodology is tailored to scenarios where a single GPU is employed for fine-tuning tasks, which is a common setup for many researchers and practitioners.\n\n3. **Symbols and Table**: The explanation uses a set of symbols organized in Table 1. Although Table 1 is referenced, the exact symbols and their meanings are not provided in the provided text.\n\nTo summarize the proposed model or method, it systematically estimates the GPU memory usage by taking into account the architecture and operations specific to transformer-based models, ensuring accuracy and practicality for single GPU environments. The inclusion of symbols likely aims to clarify and support the explanation provided in the methodology.",
        "main_experiment_and_results": "Main Experiment Setup:\n- Dataset: The specific datasets used for model fine-tuning were not detailed in the provided text.\n- Baselines: The comparison is made against ground truth data (actual GPU memory usage) and a previously implemented method called DNNMem for GPU memory usage estimation on a single GPU.\n- Methods: Various distributed fine-tuning methods are employed to assess memory usage.\n- Evaluation Metrics: The primary metric is the peak GPU memory usage estimate compared with the actual measured peak GPU memory usage (ground truth).\n\nMain Experimental Results:\nThe results focus on the comparison between LLMem's estimated GPU memory usage and the actual (ground truth) memory usage. Additionally, comparisons are made with the DNNMem implementation for single GPU scenarios. The goal is to demonstrate the accuracy and efficacy of LLMem's memory usage estimation in different fine-tuning setups and distributed environments, though specific numerical results or performance metrics are not provided in the excerpt."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To select distributed fine-tuning methods, it is crucial to estimate GPU memory usage accurately.",
            "experiment_process": "Implemented DNNMem based on the description in the paper. Extracted the corresponding computation graph from a given pre-trained model to identify the output size in each operator. Computed pre-allocated GPU memory, including CUDA context and weight tensors of the model, before operator execution. Next, computed peak GPU memory usage at each operator while traversing the graph. Accounted for internal tensor fragmentation and memory block management. Validated the implementation by comparing GPU memory estimation results for the BERT model on the GLUE benchmark using PyTorch 2.0.1 with CUDA 11.7 on NVIDIA RTX2060.",
            "result_discussion": "Our DNNMem implementation shows 34.38% and 20.48% error rates in comparison to the DNNMem paper's 31.42% and 19.12% error rates, respectively. The similar error rates suggest that our implementation for single-GPU comparisons is valid.",
            "ablation_id": "2404.10933v1.No1"
        },
        {
            "research_objective": "To demonstrate the limitations of DNNMem for LLM fine-tuning memory estimation and the improvements offered by LLMem.",
            "experiment_process": "Compared the GPU memory usage estimation results of LLMem and DNNMem on a single GPU using various pre-trained LLMs. The estimation was conducted considering factors like mixed precision, shared GPU memory space for forward and backward gradients, and extra GPU memory usage during the initial fine-tuning iteration due to optimizer states.",
            "result_discussion": "LLMem predicts peak GPU memory usage with minimal error rates compared to ground truth, outperforming DNNMem, especially as the total parameter size increases. DNNMem fails to predict GPU memory consumption accurately in distributed fine-tuning across multiple GPUs.",
            "ablation_id": "2404.10933v1.No2"
        },
        {
            "research_objective": "To evaluate the accuracy of LLMem's memory usage estimates and its ability to select appropriate fine-tuning methods while avoiding GPU OOM issues.",
            "experiment_process": "Utilized a Tesla V100 with 4 GPUs, Colossal-AI, and PyTorch 2.0.1 for the experiments. Fine-tuned models like OPT, BLOOM, CodeGen, BioGPT, GPTBigCode, GPT Neo, and LLaMA on alpaca data. Compared the peak GPU memory usage estimated by LLMem and DNNMem for single and multi-GPU setups, using various distributed fine-tuning methods like TP, DP+TP, ADP, and checked ground truth memory usage.",
            "result_discussion": "LLMem consistently predicted GPU OOM for all models and closely approximated the peak GPU memory usage for ground truth data. For multi-GPU setups, LLMem showed slight error increases due to memory usage gaps between GPUs and memory allocator characteristics. The method selection experiment indicated LLMem\u2019s effectiveness in choosing the optimal fine-tuning method while preventing GPU OOM, though it faced challenges with DP+TP due to limited GPU count.",
            "ablation_id": "2404.10933v1.No3"
        }
    ]
}