{
    "title": "1 Introduction",
    "abstract": "Transformers have emerged as the underpinning architecture for Large Language Models (LLMs). In generative language models, the inference process involves two primary phases: prompt processing and token generation. Token generation, which constitutes the majority of the computational workload, primarily entails vector-matrix multiplications and interactions with the Key-Value () Cache. This phase is constrained by memory bandwidth due to the overhead of transferring weights and values from the memory system to the computing units. This memory bottleneck becomes particularly pronounced in applications that require long-context and extensive text generation, both of which are increasingly crucial for LLMs.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Transformers have proven to be particularly successful in tasks such as language modeling Lewis et al. (2019  ###reference_b32###); Brown et al. (2020  ###reference_b9###); Raffel et al. (2020  ###reference_b43###), image recognition Dosovitskiy et al. (2020  ###reference_b18###), recommendations Sun et al. (2019  ###reference_b53###); de Souza Pereira Moreira et al. (2021  ###reference_b15###); Adnan et al. (2023  ###reference_b2###); Zhao et al. (2023  ###reference_b66###), and text generation with the advent of Large Language Models (LLMs). Unfortunately, LLM deployment presents critical inference latency and throughput concerns. This is primarily attributed to the sequential autoregressive nature of generative inference, particularly when handling inputs with larger contexts. Despite advancements, modern LLMs face challenges in efficiently processing longer input sequences, as evidenced by recent studies Bai et al. (2023  ###reference_b6###); Li et al. (2023  ###reference_b33###); Chen et al. (2023  ###reference_b10###); Huang et al. (2021a  ###reference_b23###). Unfortunately, the increased memory and compute requirements associated with longer sequences exacerbate LLM inference latency and reduce throughput. This paper proposes inference-time strategies for accuracy-preserving, low-latency, high-throughput LLM systems.\n\nLLMs employ Transformers and rely on the \u2018attention mechanism\u2019 to understand the relationships between words within a given input sequence Vaswani et al. (2017  ###reference_b55###). As the attention mechanism scales quadratically with the size of the input sequence, it tends to present the largest latency overhead during inference Sukhbaatar et al. (2019  ###reference_b52###); Dao et al. (2022  ###reference_b14###); Choromanski et al. (2020  ###reference_b12###). Additionally, due to the autoregressive nature of token generation in LLMs, there is a need to recompute key and value vectors for all previous tokens. To mitigate this, LLMs utilize a storage structure known as a Key-Value Cache ( ) Ott et al. (2019  ###reference_b40###).   retains previously computed key-value pairs, eliminating the need for costly re-computation of these vectors.\n\nHowever,   presents scalability challenges. Accessing the   from off-chip memory during token generation introduces additional memory latencies and is constrained by memory bandwidth limitations. For instance, in the MPT-7B model illustrated in Figure 1  ###reference_###(a), increasing the sequence length by 16 (from 512 to 8K) results in a more than 50 increase in inference latency. Moreover, approximately 40% of the total inference time (highlighted in green) is consumed by   data movement. Importantly, a larger context not only increases the size of the   but also prolongs the time required for other operations (depicted in blue). Similarly, as shown in Figure 1  ###reference_###(b) for the MPT-7B model, the   size surpasses the model size when the sequence length exceeds 8K. Thus,   sizes present a roadblock to enabling low-latency, high-throughput inference for large sequences.\n\nPrevious studies have explored mitigating attention mechanisms\u2019 memory and computation requirements when dealing with longer sequences Zaheer et al. (2020  ###reference_b64###); Kitaev (2020  ###reference_b30###); Wang et al. (2020  ###reference_b59###); Beltagy et al. (2020  ###reference_b7###). While system-level optimizations like FlexGen Sheng et al. (2023  ###reference_b49###), Flash Attention Dao et al. (2022  ###reference_b14###), Paged Attention Kwon et al. (2023  ###reference_b31###), and multi-dimensional partitioning Pope et al. (2023  ###reference_b41###) aim to improve the scalability of generative AI, they often overlook the fundamental challenge of expanding   size. Techniques like multi-query Shazeer (2019  ###reference_b48###) and group-query attention Ainslie et al. (2023  ###reference_b4###) propose reducing   size by eliminating specific attention heads from writing to the  , but these methods typically require resource-intensive model retraining or fine-tuning. This becomes complex as various accelerators are already deployed in the field. Thus, there is a pressing need for inference-time techniques for   reduction. This is even more challenging as any proposed technique must conform to the strict constraints for model accuracy. For instance, MLPerf Red"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background and Motivation",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Inference Process in Large Language Models",
            "text": "In language modeling, the task involves estimating the probability of the next token based on preceding tokens . For generative Large Language Models (LLMs), the inference process unfolds in two phases:\nPrompt Processing Phase: This phase helps input context undergo causal processing, enabling the model to generate keys and values for all tokens within the context. These key-value pairs are then stored in the  .\nToken Generation Phase: This phase sequentially and auto-regressively generates text. Each token is produced by passing through all layers of the generative model. Notably, the generation of the next token relies on the previously generated tokens and their order.\nTo enhance inference efficiency, repeated and complex computations of Key () and Value () tensors across all layers are avoided by caching these tensors. This is referred to as the  . The   is sequentially populated during each token generation step Strati et al. (2024  ###reference_b50###), until the text generation process is completed."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Reducing KV Cache Size by Exploiting Sparsity",
            "text": "To address the challenge posed by the expanding  , let us examine a sequence, denoted as , comprising  tokens and its   contents for a single attention head and layer. In full attention, the   components involve  keys and values. These grow proportionally with . To mitigate this, we can shrink the   size to accommodate shorter sequences, designated as . This involves using a reduced number of tokens, transitioning from  to , where  is a subset of , and  is less than . This reduction can be achieved by leveraging the inherent sparsity within the attention mechanism of LLMs.\nDespite the substantial computational demands during the training of transformers, there exists inherent sparsity within the attention mechanism. However, the extent of sparsity may vary depending on the particular downstream task. Figure 3a  ###reference_sf1### illustrates the diverse levels of attention sparsity among different models utilized for summarization tasks with the CNN/DailyMail dataset. This variability manifests across various levels of the model, including the overall model, individual layers, and distinct sections of the model."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Improving Performance by Using Key Tokens",
            "text": "In Figure 3b  ###reference_sf2###, the Cumulative Distribution Function (CDF) depicts the relationship between attention score and the fraction of the total context. Notably, a small subset of tokens receives the most attention during text generation. This underscores the significance of specific   and their pivotal role in comprehending context and facilitating text generation. However, dynamically determining which tokens serve as  , especially in cases where the input sequence contains unknown or unseen tokens during inference, presents a considerable challenge."
        },
        {
            "section_id": "2.3.1",
            "parent_section_id": "2.3",
            "section_name": "2.3.1 Leveraging Score Function to Identify Key Tokens",
            "text": "We introduce a score function  for each token to identify the    out of a total of  tokens. In the multi-head attention mechanism, attention scores determine the degree of connection between a single token and all other tokens. This is described by Equation 1  ###reference_###.\n###figure_6### The natural choice is to utilize the attention score as the score function, denoted as . This method is commonly observed in previous state-of-the-art work, such as HO Zhang et al. (2023  ###reference_b65###). It identifies tokens that consistently receive higher attention scores during the prompt and token generation phases as the most critical or  .\nWe can choose and retain these  tokens based on their accumulated attention scores, creating what we refer to as \u201cKey Attention\u201d. However, relying solely on these  tokens during attention does not provide the necessary accuracy and yields poor performance. This is shown in Figure 3c  ###reference_sf3###.\nIn this comparison, both \u2018Window Attention\u2019 and \u2018Key Attention\u2019 demonstrate inferior performance compared to full attention, even when the window and   parameters are reduced by . While reducing the sizes of the window and   relative to the total tokens () is crucial for minimizing the size of the  , it also leads to a significant decrease in accuracy. This decline primarily stems from the loss of recent context in key-token attention and crucial context in window attention. Building on this observation, we propose a mixed approach that combines selected   with recent tokens to reduce the   size while also preserving accuracy."
        },
        {
            "section_id": "2.3.2",
            "parent_section_id": "2.3",
            "section_name": "2.3.2 Problem: Uneven Score Distribution",
            "text": "Figure 4  ###reference_### shows the distribution of attention scores  for full attention, as described in Equation 2  ###reference_###. When   is reduced, tokens with lower scores are discarded. This alters the score function, shown in Equation 3  ###reference_###, as the term  becomes zero.\n###figure_7### This removal of tokens disrupts the distribution of the score function. This is because the attention weight of the discarded tokens is unevenly distributed among the tokens within the reduced  . This uneven distribution arises due to the nature of the inherent softmax function. Figure 4  ###reference_### illustrates this phenomenon by comparing the distribution of the score function for full attention with that after   reduction. When the score distribution is uneven, the model may not attend to the most relevant tokens in the sequence. Consequently, this can result in a loss of contextual information, reduced accuracy, and potentially lower-quality text generation."
        },
        {
            "section_id": "2.3.3",
            "parent_section_id": "2.3",
            "section_name": "2.3.3 Motivation: Damping the Score Function",
            "text": "A straightforward approach involves damping the score function using a damping factor to counteract the excess attention score resulting from discarded tokens. Assume  is the damping factor. It modifies the score function as . Ideally, we aim to dampen the score function by a factor equivalent to , where  represents the tokens that have been discarded.\nFigure 3b  ###reference_sf2### shows that, even with a 50% reduction in the   size, the average accumulated attention score of   remains consistently high, ranging from approximately 90% to 95%. We conduct a sweep across a range of values to explore the impact of different damping factors () on overall model quality. This analysis is performed with a   size set at 50% and a recent ratio of 20% (representing the percentage of recently generated tokens) for the Cerebras-GPT-6.7B Dey et al. (2023  ###reference_b16###) model.\nHowever, as depicted in Figure 5  ###reference_###, even after the application of a damping factor, it is not possible to achieve the same quality as the full attention model. This discrepancy stems from a significant change in the score distribution of the remaining tokens within the reduced  . These findings underscore the inadequacy of relying solely on the accumulated attention score-based score function  for identifying  . Hence, addressing the impact of discarded tokens within the score function is crucial to achieve higher model accuracy or meet the accuracy requirements of benchmarks like MLPerf."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Keyformer: Intuition and Design",
            "text": "leverages the inherent sparsity within decoder layers by identifying   using a mixture of recent tokens. It adjusts the changes in the score function resulting from discarded tokens by applying regularization to the unnormalized logits for the identification of  .\nOur choice of distribution is inspired by the Gumbel distribution Cooray (2010  ###reference_b13###). The Gumbel distribution is particularly well-suited for our   identification task, as it characterizes the distribution of maximum values within a set of samples and is skewed towards initial tokens. This makes it an ideal candidate for modeling   for long sequences.\nEquation 5  ###reference_### presents the standard Gumbel pdf applied to unnormalized logits, while Equation 6  ###reference_### displays the pdf of logits adjusted with Gumbel addition. Additionally, it is noteworthy that the Gumbel distribution holds significance in statistical theory. It captures the essence of the Gumbel limit theorem, which asserts that common probability distributions (such as normal, exponential, uniform, etc.) converge to the Gumbel distribution. This underscores its appropriateness for modeling the identification of  .\n###figure_8### In theory, selecting a regularization distribution that promotes uniformity after normalization aids in   identification. This is crucial during inference when information about discarded tokens is unavailable. To quantify the spread of probability distributions post-normalization, we employ entropy, defined as . Our analysis indicates that Gumbel-based logit adjustment fosters a more uniform distribution, suggesting its effectiveness as a regularization technique for   identification, as demonstrated in Equation 7  ###reference_### and Equation 8  ###reference_###."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Logits Regularization",
            "text": "We strategically remove  tokens from the context in the prompt processing phase. This helps us maintain a constant   size with  tokens during generation and prevents unwarranted memory expansion. Thereafter,  uses logits regularization technique. The introduction of added distribution () to regularize the reduced logits enables our model to remain robust and adaptive. It helps identify the   even in the presence of unknown contexts during inference-time.  adds this noise to the unnormalized logits derived from , as illustrated in Equation 4  ###reference_###. Also, the type of distribution added significantly impacts the resulting probability distribution.\nHere,  are the adjusted logits,  are the unnormalized logits, and  is the added distribution for regularization."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Choice of Distribution for Regularization",
            "text": "The regularization distribution added to unnormalized logits impacts   identification and model quality. Thus, we aim to draw intuition using the semantics of LLMs.\nOur choice of distribution is inspired by the Gumbel distribution Cooray (2010  ###reference_b13###  ###reference_b13###). The Gumbel distribution is particularly well-suited for our   identification task, as it characterizes the distribution of maximum values within a set of samples and is skewed towards initial tokens. This makes it an ideal candidate for modeling   for long sequences.\nEquation 5  ###reference_###  ###reference_### presents the standard Gumbel pdf applied to unnormalized logits, while Equation 6  ###reference_###  ###reference_### displays the pdf of logits adjusted with Gumbel addition. Additionally, it is noteworthy that the Gumbel distribution holds significance in statistical theory. It captures the essence of the Gumbel limit theorem, which asserts that common probability distributions (such as normal, exponential, uniform, etc.) converge to the Gumbel distribution. This underscores its appropriateness for modeling the identification of  .\n###figure_9### In theory, selecting a regularization distribution that promotes uniformity after normalization aids in   identification. This is crucial during inference when information about discarded tokens is unavailable. To quantify the spread of probability distributions post-normalization, we employ entropy, defined as . Our analysis indicates that Gumbel-based logit adjustment fosters a more uniform distribution, suggesting its effectiveness as a regularization technique for   identification, as demonstrated in Equation 7  ###reference_###  ###reference_### and Equation 8  ###reference_###  ###reference_###."
        },
        {
            "section_id": "3.2.1",
            "parent_section_id": "3.2",
            "section_name": "3.2.1 Intuition: Bias Towards Initial Tokens",
            "text": "Previous research, such as streaming LLMs Xiao et al. (2023  ###reference_b62###) and the HO model Zhang et al. (2023  ###reference_b65###), has shown a bias towards initial tokens. This bias stems from accumulated attention scores favoring initial tokens due to cumulative effects during decoding iterations. We propose using a skewed distribution to leverage this bias and effectively model the distribution of maximum values ( ). This distribution favors initial tokens while maintaining an asymmetric profile, enhancing the representation of tokens drawn from the recent context window.\nOur choice of distribution is inspired by the Gumbel distribution Cooray (2010  ###reference_b13###  ###reference_b13###  ###reference_b13###). The Gumbel distribution is particularly well-suited for our   identification task, as it characterizes the distribution of maximum values within a set of samples and is skewed towards initial tokens. This makes it an ideal candidate for modeling   for long sequences.\nEquation 5  ###reference_###  ###reference_###  ###reference_### presents the standard Gumbel pdf applied to unnormalized logits, while Equation 6  ###reference_###  ###reference_###  ###reference_### displays the pdf of logits adjusted with Gumbel addition. Additionally, it is noteworthy that the Gumbel distribution holds significance in statistical theory. It captures the essence of the Gumbel limit theorem, which asserts that common probability distributions (such as normal, exponential, uniform, etc.) converge to the Gumbel distribution. This underscores its appropriateness for modeling the identification of  .\n###figure_10### In theory, selecting a regularization distribution that promotes uniformity after normalization aids in   identification. This is crucial during inference when information about discarded tokens is unavailable. To quantify the spread of probability distributions post-normalization, we employ entropy, defined as . Our analysis indicates that Gumbel-based logit adjustment fosters a more uniform distribution, suggesting its effectiveness as a regularization technique for   identification, as demonstrated in Equation 7  ###reference_###  ###reference_###  ###reference_### and Equation 8  ###reference_###  ###reference_###  ###reference_###."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Keyformer Score Function",
            "text": "We propose a novel score function for , denoted as , to address the limitations of the accumulated attention-based score function (). This new score function integrates the Gumbel noise distribution into the unnormalized logits. However, it fails to account for the discarded tokens in forming the underlying probability distribution. To rectify this, we introduce a temperature parameter, denoted as , as shown in Equation 9  ###reference_###.\nThe probabilistic score functions described above are akin to the concept of Gumbel Softmax Jang et al. (2016  ###reference_b25###). This score function offers a continuous relaxation of discrete random variables Maddison et al. (2016  ###reference_b36###). This alignment corresponds with our primary objective of identifying a subset of past tokens  that conveys the same semantic information as the original complete set of tokens."
        },
        {
            "section_id": "3.3.1",
            "parent_section_id": "3.3",
            "section_name": "3.3.1 Significance of the Temperature Parameter",
            "text": "The \u2018temperature\u2019 parameter  is pivotal in regulating the smoothness of the probabilistic distribution. Higher values of   yield uniform probabilities, assigning equal scores to all tokens. Conversely, lower values of   produce a sharper distribution, prioritizing specific tokens based on their unnormalized logits. This parameter governs the degree of randomness in probabilities. It is crucial when tokens are removed from the  , as they cannot be reintroduced without recomputing their keys and values.\nIn Equation 10  ###reference_###, we illustrate the dynamic nature of  at each decoding iteration . To achieve this, we define a range for  spanning from  to . In each decoding step, we increment  by , a value determined by the range of  and the length of the text being generated, denoted as .\nThis strategy is based on the premise that we need a more uniform or randomized probability distribution as more tokens are discarded. Through empirical analysis, we discovered that setting  and  produces optimal outcomes (refer to Appendix  A.8  ###reference_###). This decision aligns with our objective of maintaining a non-random score function during the prompt phase, where all tokens are available. When  is set to one, the Gumbel softmax approach is nearly equivalent to a standard softmax. As we advance through decoding iterations and discard more tokens to maintain a static   size, we systematically increase the randomness in our score function . This is achieved by incrementally raising  with ."
        },
        {
            "section_id": "3.3.2",
            "parent_section_id": "3.3",
            "section_name": "3.3.2 Leveraging Score Function Accumulation",
            "text": "The accumulation of the score function is essential for identifying   based on their consistent behavior throughout decoding steps. Without accumulation, token discarding would rely solely on the current token\u2019s correlation with previous tokens. Although the correlation of the current token is significant in identifying  , their behavior should remain consistent across most generated tokens. To discern   based on this consistent behavior, we accumulate the score function  across both the prompt and token generation phases, as depicted in Figure 6  ###reference_###."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Keyformer Algorithm",
            "text": "Figure 6  ###reference_### presents an overview of . We highlight its key functionalities in discarding tokens based on sparsification, using a mixture of recent and  , and introducing a novel Gumbel softmax-based score function for   identification. During the prompt processing phase,  calculates keys and values for all  tokens within the prompt length  to predict the first token. Given the   budget,  retains a recent window of  recent tokens while discarding  tokens from the  tokens window, thereby identifying  tokens. The top-() tokens from the  window are selected based on the  score function. The combination of   () and recent tokens () forms the reduced  . As there are no discarded tokens during the prompt processing phase,  uses a temperature parameter, , to approximate the softmax probability distribution. This is illustrated in decoding step 1.\nIn the token generation phase,  operates with a reduced  . The first generated token attends solely to the  tokens within the  , as depicted in decoding step 2. The recent window  shifts right by a single token, while the score function  accumulates with the score function from the previous decoding step. During each decoding step of the token generation phase,    are identified from a window of size . Consequently, one token has been added, and another has been removed from the recent window. Since we add and remove tokens from the \u2018 \u2019 window, we can improve accuracy while maintaining a static   size equal to . Moreover, the temperature parameter  increases by  to adjust for the number of removed tokens in the probability distribution of the score function. The detailed algorithm for  is provided in Algorithm 1  ###reference_###.\n###figure_11###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Evaluation",
            "text": "We evaluate across three significant model families: GPT-J Wang & Komatsuzaki (2021 ###reference_b56###), Cerebras-GPT Dey et al. (2023 ###reference_b16###), and MPT Team et al. (2023 ###reference_b54###), each using distinct position encoding techniques. GPT-J incorporates RoPE Su et al. (2022 ###reference_b51###), Cerebras-GPT employs learnable position embeddings, and MPT utilizes ALiBi Press et al. (2021 ###reference_b42###). By including models with varied position encoding methods, we ensure the robustness and generalizability of our findings across representative model families. We employed a fixed beam size of 4 for all evaluations.\n\nWe conducted evaluations on two representative text generation tasks: summarization, utilizing the GovReport Huang et al. (2021b ###reference_b24###) dataset, and conversation, employing the SODA dataset Kim et al. (2022 ###reference_b29###). The GPT-J model was fine-tuned specifically for summarization, while Cerebras-GPT and MPT are pre-trained models. We utilized the MPT-chat version of the MPT model for conversation tasks, which was fine-tuned for dialogue generation. All models were pre-trained with a sequence length of 2k.\n\nTo address long document summarization, we utilized the MPT-storywriter version of the MPT model, fine-tuned for writing fictional stories. This model accommodates a context length of 65k and can generate content up to 84k tokens long. Additionally, we evaluated four tasks from the lm-eval-harness Gao et al. (2021 ###reference_b19###) framework: PIQA Bisk et al. (2020 ###reference_b8###), Winogrande Sakaguchi et al. (2021 ###reference_b46###), OpenBookQA Mihaylov et al. (2018 ###reference_b37###), and COPA Roemmele et al. (2011 ###reference_b45###). These tasks involve few-shot evaluation of autoregressive language models and were executed using the NVIDIA A100 (80GB) GPUs.\n\nTo evaluate the accuracy of, we compared it against Full Attention. Full Attention acts as our benchmark and represents the gold standard for accuracy. We aim to achieve an accuracy target within the range of 99% to 99.9% of Full Attention. This goal aligns with the high-quality standards set by industry benchmarking entities like MLPerf Reddi et al. (2020 ###reference_b44###). Additionally, we performed comparisons with Window Attention and the recent HO model Zhang et al. (2023 ###reference_b65###), adjusting the size from 20% to 90% of the prompt length.\n\nWe assessed the effectiveness of reduction in while maintaining accuracy for handling long contexts. This evaluation was conducted on the MPT-7B-story writer model, pre-trained with a context length of 65k. We utilized the Government report Huang et al. (2021b ###reference_b24###) dataset, which contains reports authored by government research agencies and features longer summaries and documents. This dataset requires a deep understanding of context to extract crucial information for summarization. Figure 8 ###reference_### shows the accuracy comparison among, HO, and Full Attention. Notably, even with a 50% size, maintains the desired 99% accuracy threshold, while HO shows significantly lower accuracy at the same size.\n\n###figure_12### To understand the sources of performance benefits with -based reduction, we primarily consider two factors:\nReduced KV cache: A smaller significantly reduces the data movement from off-chip GPU HBM.\nScaled Dot Product Optimization: The number of tokens in the is reduced from to.\n\nThe above two factors reduce the overall smaller size of matrices. Thus, they enable an optimized scaled dot product within the multi-head attention block.\n\n###figure_13### It is worth noting that in LLMs, which are memory-bound, the main performance boost comes from reducing data movement rather than matrix multiplication. However, \u2019s identification process introduces some overhead due to Gumbel softmax. Figure 10 ###reference_### illustrates the normalized performance improvement for, considering both reduced data movement and optimized scaled dot product. These enhancements are demonstrated for the MPT-7B-storywriter model with a 50% reduction, including the additional overhead from \u2019s Gumbel softmax. The results indicate that -based reduction decreases data movement by 2.9 and improves computational efficiency in the attention module\u2019s scaled dot product by 1.3, particularly for sequences of length 4k.\n\nTo examine the effect of omitting regularization on unnormalized logits, we experimented without logit adjustment, where, mirroring the approach used in HO Zhang et al. (2023 ###reference_b65###).\n\nTo study the impact of constant regularization on"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Accuracy Results",
            "text": "To assess the impact of reduction on text generation quality, we relied on the ROUGE score Lin (2004), a widely-used metric for evaluating fluency and coherence. ROUGE measures the overlap of n-grams between generated and reference text, providing a standardized measure for text quality. According to MLPerf, ROUGE scores, including ROUGE-1, ROUGE-2, and ROUGE-L, should reach 99% to 99.9% of their original values for summarization tasks. Thus, even with reduced , our model should maintain the desired ROUGE scores. Figure 7 depicts accuracy comparisons between and other methods (Full Attention, Window Attention, and HO) across different sizes. The illustration focuses on the ROUGE-2 score, which measures bi-gram overlap. Trends for ROUGE-1 and ROUGE-L are detailed in Appendix A.5.\n\nThe results highlight the importance of the previous context for model performance. For instance, Window Attention relies solely on recent tokens, leading to a significant loss of accuracy. Thus, identifying is crucial for achieving desired model accuracy. Across various budgets, consistently outperforms the state-of-the-art HO. It shows that the it identifies are more important than the heavy hitters identified by HO. For instance, attains the target ROUGE score with just 70% of the , whereas HO fails to reach this goal even with a larger budget. Furthermore, surpasses the baseline accuracy by up to 1.73% (0.9% for GPT-J-6B and 1.73% for Cerebras-GPT-6.7B for summarization task) achieved with full attention. This demonstrates the regularization effect of the introduced Gumbel noise in the score function of and its positive impact on identification.\n\nWe assessed the effectiveness of reduction in while maintaining accuracy for handling long contexts. This evaluation was conducted on the MPT-7B-story writer model, pre-trained with a context length of 65k. We utilized the Government report Huang et al. (2021b) dataset, which contains reports authored by government research agencies and features longer summaries and documents. This dataset requires a deep understanding of context to extract crucial information for summarization. Figure 8 shows the accuracy comparison among , HO, and Full Attention. Notably, even with a 50% size, maintains the desired 99% accuracy threshold, while HO shows significantly lower accuracy at the same size."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Performance Results",
            "text": "To assess the performance advantages of our implementation, we considered two critical inference metrics: inference latency and generation throughput for the target models. Our implementation is seamlessly integrated with Huggingface model cards, ensuring ease of adoption. We disabled CPU offloading in cases where the model and data exceeded GPU HBM memory capacity, ensuring consistent evaluation. We generated a synthetic dataset to maintain evaluation consistency, where all prompts were padded with synthetic text. We employed the MPT-7B-storywriter model to generate an equal number of tokens for each prompt. We tested various combinations of prompt and generation lengths.\n\nFigure 9 presents inference latency speedup while Table 1 shows improvement in generation throughput in comparison to a Full Attention-based method. With a 50% reduction, the method significantly reduces inference latency, achieving 2.1 times speedup with the same batch size. Moreover, the reduced size allows handling twice the batch size compared to full attention, increasing token generation throughput by 2 times with the same batch size and 2.4 times with a bigger batch size.\n\nTo understand the sources of performance benefits with our reduction method, we primarily consider two factors:\n\n1. Reduced KV cache: A smaller size significantly reduces the data movement from off-chip GPU HBM.\n2. Scaled Dot Product Optimization: The number of tokens is reduced, optimizing the scaled dot product within the multi-head attention block.\n\nThe above two factors reduce the overall size of matrices, enabling an optimized scaled dot product within the multi-head attention block.\n\nIt is worth noting that in LLMs, which are memory-bound, the main performance boost comes from reducing data movement rather than matrix multiplication. The identification process introduces some overhead due to Gumbel softmax. Figure 10 illustrates the normalized performance improvement, considering both reduced data movement and optimized scaled dot product. These enhancements are demonstrated for the MPT-7B-storywriter model with a 50% reduction, including the additional overhead from the Gumbel softmax. The results indicate that reduction decreases data movement by 2.9 times and improves computational efficiency in the attention module\u2019s scaled dot product by 1.3 times, particularly for sequences of length 4k."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Few-Shot Evaluation",
            "text": "We performed few-shot experiments using four tasks from the lm-eval-harness framework and pre-trained models to evaluate performance under varying numbers of shots during inference. Table 2 presents the results for 0 and 5 shots, demonstrating that consistently surpasses previous approaches across all tasks and shot settings. Even with a 50% reduction in size, it achieves accuracy close to the full attention baseline."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Ablation Studies",
            "text": "To examine the effect of omitting regularization on unnormalized logits, we experimented without logit adjustment, mirroring the approach used in HO Zhang et al. (2023). To study the impact of constant regularization on all unnormalized logits, we experimented with constant logit adjustment, where a constant is being added to every unnormalized logit. We also used a symmetric Gaussian distribution for logit adjustment. The Gaussian probability density function (pdf) with mean and variance is applied to unnormalized logits, while the pdf of logits adjusted with Gaussian addition is also considered.\n\nEmpirical evidence shows that the Gumbel distribution, known for its skewness to initial tokens, is an effective regularization mechanism for identification."
        },
        {
            "section_id": "4.4.1",
            "parent_section_id": "4.4",
            "section_name": "4.4.1 Shared versus Per-Layer Score Function",
            "text": "The score function defines what constitutes a choice in the context. In generative LLMs with stacked decoder layers, the score function can either be shared across all layers (Shared) or dedicated to each layer (Per-Layer). In the Per-Layer approach, a dedicated score function is assigned to each decoder layer, with accumulation occurring at each decoding stage. Conversely, the Shared method uses a single global score function for all decoder layers, with accumulation across decoder layers and decoding stages.\n\nTable 3 illustrates the accuracy comparison between Per-Layer and Shared score functions, maintaining the original positional information and size constant. Notably, using the Per-Layer score function yields better accuracy than the shared score function. This aligns with the intuition that transformers learn hierarchical text representations across layers, with lower layers capturing local syntactic and semantic features and higher layers capturing more abstract and complex patterns. Therefore, having a Per-Layer score function for each layer aids in choice identification specific to that layer."
        },
        {
            "section_id": "4.4.2",
            "parent_section_id": "4.4",
            "section_name": "4.4.2 New vs. Original Positional Information",
            "text": "We investigated how reducing the size affects the positional information used for the keys. We explored two approaches: (Org Pos) and (New Pos). In (Org Pos), the position information reflects the original positions of tokens within the text. Conversely, (New Pos) uses positions based on the new arrangement of tokens.\n\nTable 3 presents the accuracy comparison with consistent size and score function. Notably, when using the original positional information, there is a noticeable improvement in accuracy. However, incorporating new positional information during inference leads to a slight drop in accuracy. Nevertheless, even with new positional information, the performance outperforms the state-of-the-art methods available."
        },
        {
            "section_id": "4.4.3",
            "parent_section_id": "4.4",
            "section_name": "4.4.3 Comparison with Alternative Distributions",
            "text": "We conducted an ablation study to assess how different logit adjustment distributions affect model accuracy or identification. We evaluated three regularization strategies and compared them with Gumbel-based logit adjustment. These are no logit adjustment, constant logit adjustment, and Gaussian distribution-based logit adjustment.\n\nTo examine the effect of omitting regularization on unnormalized logits, we experimented without logit adjustment, where , mirroring the approach used in HO Zhang et al. (2023).\n\nTo study the impact of constant regularization on all unnormalized logits, we experimented with constant logit adjustment, setting , where  is the constant that is being added to every unnormalized logit.\n\nWe also used a symmetric Gaussian distribution for logit adjustment. Equation 11 presents the Gaussian probability density function (pdf) with mean  and variance  applied to unnormalized logits, while Equation 12 displays the pdf of logits adjusted with Gaussian addition.\n\nWe established a baseline comparison using a standard Gumbel pdf with  and . For comparison, the Gaussian pdf had an identical mean and variance, and the constant logit adjustment employed a constant value. The \u201cNo logit adjustment\u201d approach uses the method in prior work, HO.\n\nThus, empirical evidence shows that the Gumbel distribution, known for its skewness to initial tokens, is an effective regularization mechanism for identification."
        },
        {
            "section_id": "4.4.4",
            "parent_section_id": "4.4",
            "section_name": "4.4.4 Recent Window versus Key token Window Ratio",
            "text": "We conducted a sensitivity study to examine the impact of varying the ratio of recent tokens. Results in Appendix A.4 indicate that the models perform better when the recent tokens ratio falls within the range of 20% to 30%. This observation aligns with our hypothesis that recent tokens are critically important for LLM inference."
        },
        {
            "section_id": "4.4.5",
            "parent_section_id": "4.4",
            "section_name": "4.4.5 Comparison with Attention Sinks",
            "text": "Recent research introduced StreamingLLM (Xiao et al., 2023), which introduced the concept of \u201cattention sinks.\u201d StreamingLLM enables Language Models (LLMs) trained with a finite-length attention window to handle infinite sequence lengths without fine-tuning. This is achieved by retaining the first four tokens (known as \u201cattention sinks\u201d) and a moving window of recent tokens. To compare StreamingLLM with , we maintained a size of 60% for both techniques. Table 3 displays the accuracy comparison, showing that StreamingLLM struggles in summarizing text by relying on only the first four tokens as attention sinks and the remaining tokens from a recent window (Appendix A.7)."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Prior work focus on improving inference speed for transformer Vaswani et al. (2017  ###reference_b55###) based models. PoWER-BERT Goyal et al. (2020  ###reference_b22###) utilizes word-vector elimination by exploiting redundancy for encoder-based models. Linformer Wang et al. (2020  ###reference_b59###) tries to reduce the attention mechanism from quadratic to linear. Reformer Kitaev (2020  ###reference_b30###) reduces attention complexity by locality-sensitive hash (LSH). Linear transformers Katharopoulos et al. (2020  ###reference_b28###) store accumulated states rather than preserving every representation. FLAT Kao et al. (2023  ###reference_b27###) suggests optimized dataflow, while other research Wang et al. (2022  ###reference_b60###) overlaps communication with dependent computation to enhance attention execution. In contrast,  aims to reduce  , speeds up attention by reducing the tokens.\nOne line of work sparsifies the attention mechanism to reduce the computational and memory capacity of the attention block. BigBird Zaheer et al. (2020  ###reference_b64###) combines random, windowed, and global attention to maintain the accuracy for transformers while sparsifying the attention block. LongFormer Beltagy et al. (2020  ###reference_b7###) also utilizes windowed attention with task-based local attention to achieve sparse attention. Spatten Wang et al. (2021  ###reference_b57###) introduces sparsity at both the head and token levels. However, it needs a dedicated architecture to exploit sparsity. Furthermore, these works do not address inference optimizations.\nEl-Attention Yan et al. (2021  ###reference_b63###) modifies the multi-head attention module to reduce the   size, leveraging key and value stability during incremental decoding for reuse across layers. In contrast, HO Zhang et al. (2023  ###reference_b65###) identifies heavy-hitters and keeps them in the   to reduce its size, neglecting the attention score distribution shift that occurs post elimination of previous tokens from the  , leading to accuracy trade-offs. Other approaches Liu et al. (2023  ###reference_b35###); Anagnostidis et al. (2023  ###reference_b5###) introduce sparsity at both coarse and fine-grained levels, targeting the elimination of specific heads and tokens during inference. However, these methods require task-specific predictors and fine-tuning of pre-trained models. Another method Mu et al. (2023  ###reference_b39###) compresses prompts into gist tokens to reduce  . Landmark Attention Mohtashami & Jaggi (2023  ###reference_b38###) represents token blocks with an additional landmark token in the vocabulary, necessitating computationally intensive retraining or fine-tuning for gist or landmark token integration."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Future Work",
            "text": "Recent techniques like Multi-Query Attention (MQA) Shazeer (2019  ###reference_b48###) and Group-Query Attention (GQA) Ainslie et al. (2023  ###reference_b4###) aim to train foundation models with fewer attention heads.\nHowever, such models are typically used after fine-tuning for specific tasks.\nWhile the detailed evaluation of  with these models is deferred to future work, it is worth noting that  can still be applied on top of MQA or GQA-based models. This is because it discards redundant tokens regardless of the number of heads. Additionally, we plan to integrate  into the LLM\u2019s attention block by replacing the standard softmax with a -based softmax. This introduces sparsity during training, addressing the quadratic computational and memory complexities of transformers.\nThis direction aims to enhance scalability to longer contexts without sacrificing accuracy."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "Advancements in large language models (LLMs) are pushing for longer contexts and extensive text generation, with models trained on sequences of millions of tokens. However, this trend strains system memory bandwidth, leading to execution costs. In longer contexts, the attention state size, primarily responsible for memory bandwidth consumption and inference latency, exceeds the model parameters\u2019 size. To address this, we proposed a technique that effectively reduces the attention state size up to 50% without sacrificing accuracy by discarding tokens across heads, layers, and beams, identifying essential tokens based on a novel score function. This method can be applied to LLMs at inference time, without requiring fine-tuning, while also improving latency and token generation throughput."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A Appendix",
            "text": ""
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T1\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>The generation throughput (tokens/sec) for the MPT-7B model across different sequence lengths. \u201c1024 + 1024\u201d shows the sum of the prompt length and the token generation length. \u201cOOM\u201d stands for out-of-memory and \u201cBS\u201d for batch size.</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T1.6\" style=\"width:433.6pt;height:164.5pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(74.5pt,-28.3pt) scale(1.52354963004582,1.52354963004582) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T1.6.6\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T1.2.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S4.T1.2.2.2.3\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.2.2.2.3.1\">Sequence Length</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T1.2.2.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.2.2.2.4.1\">Full Attention</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T1.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.1.1.1\">HO</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T1.2.2.2.2\"></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.6.6.6\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T1.6.6.6.5\">Original cache</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T1.4.4.4.2\">90%  \n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T1.6.6.6.4\">50%  \n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T1.6.6.7.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T1.6.6.7.1.1\">1024 + 1024</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.6.6.7.1.2\">24.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.6.6.7.1.3\">27.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.6.6.7.1.4\">32.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.6.6.8.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.6.6.8.2.1\">2048 + 2048</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.6.6.8.2.2\">15.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.6.6.8.2.3\">20.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.6.6.8.2.4\">24.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.6.6.9.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.6.6.9.3.1\">4096 + 4096 (BS=1)</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.6.6.9.3.2\">8.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.6.6.9.3.3\">14.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.6.6.9.3.4\">17.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.6.6.10.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S4.T1.6.6.10.4.1\">4096 + 4096 (BS=2)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.6.6.10.4.2\">OOM</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.6.6.10.4.3\">OOM</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.6.6.10.4.4\">19.85</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 1: The generation throughput (tokens/sec) for the MPT-7B model across different sequence lengths. \u201c1024 + 1024\u201d shows the sum of the prompt length and the token generation length. \u201cOOM\u201d stands for out-of-memory and \u201cBS\u201d for batch size."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T2\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Few shot results for different tasks. HO and  are with 50%  .</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T2.16\" style=\"width:411.9pt;height:432.3pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(85.9pt,-90.1pt) scale(1.71541011502258,1.71541011502258) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T2.16.8\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T2.16.8.9.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" id=\"S4.T2.16.8.9.1.1\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.16.8.9.1.1.1\">Task</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" id=\"S4.T2.16.8.9.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.16.8.9.1.2.1\">Attention</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"2\" id=\"S4.T2.16.8.9.1.3\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.16.8.9.1.3.1\">Cerebras-GPT-6.7B</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S4.T2.16.8.9.1.4\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.16.8.9.1.4.1\">MPT-7B</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.16.8.10.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r\" id=\"S4.T2.16.8.10.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.16.8.10.2.1.1\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T2.16.8.10.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.16.8.10.2.2.1\">0-Shots</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\" id=\"S4.T2.16.8.10.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.16.8.10.2.3.1\">5-Shots</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T2.16.8.10.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.16.8.10.2.4.1\">0-Shots</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T2.16.8.10.2.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.16.8.10.2.5.1\">5-Shots</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T2.16.8.11.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T2.16.8.11.1.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.16.8.11.1.1.1\">COPA</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T2.16.8.11.1.2\">Full</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.16.8.11.1.3\">73.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.16.8.11.1.4\">73.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.16.8.11.1.5\">80.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.16.8.11.1.6\">83.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.9.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.9.1.1.1\">HO</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.9.1.1.2\">68.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.9.1.1.3\">74.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.9.1.1.4\">75.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.9.1.1.5\">82.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.10.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.10.2.2.1\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.10.2.2.2\">70.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.10.2.2.3\">74.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.10.2.2.4\">76.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.10.2.2.5\">84.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.16.8.12.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T2.16.8.12.2.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.16.8.12.2.1.1\">OpenBookQA</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T2.16.8.12.2.2\">Full</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.16.8.12.2.3\">34.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.16.8.12.2.4\">36.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.16.8.12.2.5\">41.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.16.8.12.2.6\">43.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.11.3.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.11.3.3.1\">HO</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.11.3.3.2\">32.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.11.3.3.3\">38.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.11.3.3.4\">37.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.11.3.3.5\">44.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.12.4.4\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.12.4.4.1\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.4.4.2\">32.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.12.4.4.3\">38.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.4.4.4\">40.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.4.4.5\">43.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.16.8.13.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T2.16.8.13.3.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.16.8.13.3.1.1\">Winogrande</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T2.16.8.13.3.2\">Full</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.16.8.13.3.3\">60.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.16.8.13.3.4\">58.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.16.8.13.3.5\">68.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.16.8.13.3.6\">71.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.13.5.5\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.13.5.5.1\">HO</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.13.5.5.2\">56.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.13.5.5.3\">59.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.13.5.5.4\">63.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.13.5.5.5\">72.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.14.6.6\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.14.6.6.1\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.14.6.6.2\">57.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.14.6.6.3\">59.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.14.6.6.4\">64.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.14.6.6.5\">72.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.16.8.14.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T2.16.8.14.4.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.16.8.14.4.1.1\">PIQA</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T2.16.8.14.4.2\">Full</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.16.8.14.4.3\">74.2-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.16.8.14.4.4\">74.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.16.8.14.4.5\">79.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.16.8.14.4.6\">80.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.15.7.7\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.15.7.7.1\">HO</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.15.7.7.2\">72.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.15.7.7.3\">73.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.15.7.7.4\">79.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.15.7.7.5\">79.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.16.8.8\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S4.T2.16.8.8.1\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.16.8.8.2\">73.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T2.16.8.8.3\">73.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.16.8.8.4\">79.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.16.8.8.5\">80.1</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 2: Few shot results for different tasks. HO and  are with 50%  ."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T3\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>ROUGE Score comparison with different methods and score functions for summarization task using the CNN/DailyMail dataset.</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T3.5\" style=\"width:585.4pt;height:291pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(111.7pt,-55.5pt) scale(1.61690984949129,1.61690984949129) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T3.5.5\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T3.5.5.6.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S4.T3.5.5.6.1.1\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.5.5.6.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S4.T3.5.5.6.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.5.5.6.1.2.1\">Attention</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.5.5.6.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.5.5.6.1.3.1\">Score fn</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.5.5.6.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.5.5.6.1.4.1\">KV Cache</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.5.5.6.1.5\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.5.5.6.1.5.1\">ROUGE-1</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.5.5.6.1.6\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.5.5.6.1.6.1\">ROUGE-2</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.5.5.6.1.7\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.5.5.6.1.7.1\">ROUGE-L</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row\" id=\"S4.T3.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.2.1\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T3.1.1.1.1\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T3.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.3.1\">Size</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T3.5.5.7.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" id=\"S4.T3.5.5.7.1.1\" rowspan=\"8\"><span class=\"ltx_text\" id=\"S4.T3.5.5.7.1.1.1\">MPT-7B</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" id=\"S4.T3.5.5.7.1.2\">Full</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.5.5.7.1.3\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.5.5.7.1.4\">Original</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.5.5.7.1.5\">38.6373</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.5.5.7.1.6\">17.6329</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.5.5.7.1.7\">24.506</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.5.5.8.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S4.T3.5.5.8.2.1\">Full (99% Accuracy)</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.5.5.8.2.2\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.5.5.8.2.3\">Original</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.5.5.8.2.4\">38.2509</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.5.5.8.2.5\">17.4565</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.5.5.8.2.6\">24.2609</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.5.5.9.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S4.T3.5.5.9.3.1\">Window</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.5.5.9.3.2\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.5.5.9.3.3\">60%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.5.5.9.3.4\">18.1296</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.5.5.9.3.5\">4.2655</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.5.5.9.3.6\">11.5288</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.2.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S4.T3.2.2.2.1\">HO</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.2.2.2.2\">Per-Layer</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.2.2.2.3\">60%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.2.2.2.4\">36.9616</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.2.2.2.5\">16.3865</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.2.2.2.6\">24.2301</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.5.5.10.4\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S4.T3.5.5.10.4.1\">StreamingLLM</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.5.5.10.4.2\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.5.5.10.4.3\">60%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.5.5.10.4.4\">1.3572</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.5.5.10.4.5\">0.0179</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.5.5.10.4.6\">1.0281</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.3.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S4.T3.3.3.3.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S4.T3.3.3.3.1.1\"> (New Pos)</em></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.3.3.3.2\">Per-Layer</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.3.3.3.3\">60%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.3.3.3.4\">36.9152</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.3.3.3.5\">16.9092</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.3.3.3.6\">23.7218</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.4.4.4\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S4.T3.4.4.4.1\"><em class=\"ltx_emph ltx_font_bold ltx_font_italic\" id=\"S4.T3.4.4.4.1.1\"> (Org Pos)</em></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.4.4.4.2\">Per-Layer</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.4.4.4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.4.4.4.3.1\">60%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.4.4.4.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.4.4.4.4.1\">38.7134</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.4.4.4.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.4.4.4.5.1\">17.5976</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.4.4.4.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.4.4.4.6.1\">24.5724</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.5.5.5\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\" id=\"S4.T3.5.5.5.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S4.T3.5.5.5.1.1\"> (Org Pos)</em></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.5.5.5.2\">Shared</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.5.5.5.3\">60%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.5.5.5.4\">38.2537</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.5.5.5.5\">17.3732</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.5.5.5.6\">24.2579</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 3: ROUGE Score comparison with different methods and score functions for summarization task using the CNN/DailyMail dataset."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T4\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>Empirical evaluation with different logit adjustments. Summarization task with 60%  .</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T4.5\" style=\"width:433.6pt;height:172pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(103.4pt,-41.0pt) scale(1.91126910649004,1.91126910649004) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T4.5.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T4.5.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" id=\"S4.T4.5.1.1.1.1\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.5.1.1.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"4\" id=\"S4.T4.5.1.1.1.2\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.5.1.1.1.2.1\">ROUGE-2 Score</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.5.1.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T4.5.1.2.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.5.1.2.2.1.1\">Gumbel</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T4.5.1.2.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.5.1.2.2.2.1\">Gaussian</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T4.5.1.2.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.5.1.2.2.3.1\">Constant</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T4.5.1.2.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.5.1.2.2.4.1\">None</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T4.5.1.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T4.5.1.3.1.1\">GPT-J-6B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.5.1.3.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.5.1.3.1.2.1\">19.44</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.5.1.3.1.3\">14.53</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.5.1.3.1.4\">12.49</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.5.1.3.1.5\">18.87</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.5.1.4.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T4.5.1.4.2.1\">Cerebras-GPT-6.7B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.5.1.4.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.5.1.4.2.2.1\">15.25</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.5.1.4.2.3\">9.54</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.5.1.4.2.4\">8.98</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.5.1.4.2.5\">12.73</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.5.1.5.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S4.T4.5.1.5.3.1\">MPT-7B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T4.5.1.5.3.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.5.1.5.3.2.1\">17.57</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T4.5.1.5.3.3\">10.17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T4.5.1.5.3.4\">7.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T4.5.1.5.3.5\">16.38</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 4: Empirical evaluation with different logit adjustments. Summarization task with 60%  ."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.09054v2_figure_1.png",
            "caption": "Figure 1: (a) Inference latency normalized to sequence length of 512. We measure the \ud835\uddaa\ud835\uddb5\ud835\uddaa\ud835\uddb5\\mathsf{KV}sansserif_KV \ud835\uddbc\ud835\uddba\ud835\uddbc\ud835\uddc1\ud835\uddbe\ud835\uddbc\ud835\uddba\ud835\uddbc\ud835\uddc1\ud835\uddbe\\mathsf{cache}sansserif_cache data movement for MPT-7B Team et al. (2023) model with varying sequence length (50% context + 50% text generation). (b) The \ud835\uddaa\ud835\uddb5\ud835\uddaa\ud835\uddb5\\mathsf{KV}sansserif_KV \ud835\uddbc\ud835\uddba\ud835\uddbc\ud835\uddc1\ud835\uddbe\ud835\uddbc\ud835\uddba\ud835\uddbc\ud835\uddc1\ud835\uddbe\\mathsf{cache}sansserif_cache size and model size as sequence length varies. The studies are performed on an NVIDIA A100 GPU with a batch size of 1 and beam size of 4."
        },
        "2": {
            "figure_path": "2403.09054v2_figure_2.png",
            "caption": "Figure 2: Attention block for generative inference. (a) Full attention with current token attending all previous tokens. (b) Window attention (w=4\ud835\udc644w=4italic_w = 4): Focusing on the most recent 4 tokens. (c) Dilated window attention (w=4\ud835\udc644w=4italic_w = 4, dilation = 1). (d) \ud835\uddaa\ud835\uddbe\ud835\uddd2\ud835\uddbf\ud835\uddc8\ud835\uddcb\ud835\uddc6\ud835\uddbe\ud835\uddcb\ud835\uddaa\ud835\uddbe\ud835\uddd2\ud835\uddbf\ud835\uddc8\ud835\uddcb\ud835\uddc6\ud835\uddbe\ud835\uddcb\\mathsf{Keyformer}sansserif_Keyformer (w=2\ud835\udc642w=2italic_w = 2, k=2\ud835\udc582k=2italic_k = 2): A mix of recent window (w) and \ud835\uddc4\ud835\uddbe\ud835\uddd2\ud835\uddc4\ud835\uddbe\ud835\uddd2\\mathsf{key}sansserif_key \ud835\uddcd\ud835\uddc8\ud835\uddc4\ud835\uddbe\ud835\uddc7\ud835\uddcc\ud835\uddcd\ud835\uddc8\ud835\uddc4\ud835\uddbe\ud835\uddc7\ud835\uddcc\\mathsf{tokens}sansserif_tokens (k). White color indicates no attention, while blue color indicates attention. The green color identifies the \ud835\uddc4\ud835\uddbe\ud835\uddd2\ud835\uddc4\ud835\uddbe\ud835\uddd2\\mathsf{key}sansserif_key \ud835\uddcd\ud835\uddc8\ud835\uddc4\ud835\uddbe\ud835\uddc7\ud835\uddcc\ud835\uddcd\ud835\uddc8\ud835\uddc4\ud835\uddbe\ud835\uddc7\ud835\uddcc\\mathsf{tokens}sansserif_tokens and their respective attention. The values of the three consecutive token generation iterations are t\u22121,t,t+1\ud835\udc611\ud835\udc61\ud835\udc611t-1,t,t+1italic_t - 1 , italic_t , italic_t + 1."
        },
        "3": {
            "figure_path": "2403.09054v2_figure_3.png",
            "caption": "(a) Attention Sparsity"
        },
        "4": {
            "figure_path": "2403.09054v2_figure_4.png",
            "caption": "(b) Average Attention Score"
        },
        "5": {
            "figure_path": "2403.09054v2_figure_5.png",
            "caption": "(c) Accuracy"
        },
        "6": {
            "figure_path": "2403.09054v2_figure_6.png",
            "caption": "Figure 4: Reducing the \ud835\uddaa\ud835\uddb5\ud835\uddaa\ud835\uddb5\\mathsf{KV}sansserif_KV \ud835\uddbc\ud835\uddba\ud835\uddbc\ud835\uddc1\ud835\uddbe\ud835\uddbc\ud835\uddba\ud835\uddbc\ud835\uddc1\ud835\uddbe\\mathsf{cache}sansserif_cache introduces a change in the distribution of attention scores. As tokens are removed, their distribution becomes uneven among the remaining cached tokens. Thus, it affects the identification of \ud835\uddc4\ud835\uddbe\ud835\uddd2\ud835\uddc4\ud835\uddbe\ud835\uddd2\\mathsf{key}sansserif_key \ud835\uddcd\ud835\uddc8\ud835\uddc4\ud835\uddbe\ud835\uddc7\ud835\uddcc\ud835\uddcd\ud835\uddc8\ud835\uddc4\ud835\uddbe\ud835\uddc7\ud835\uddcc\\mathsf{tokens}sansserif_tokens according to the score function f\u03b8\u2062(a\u2062c\u2062c\u2062a\u2062t\u2062t\u2062n)subscript\ud835\udc53\ud835\udf03\ud835\udc4e\ud835\udc50\ud835\udc50\ud835\udc4e\ud835\udc61\ud835\udc61\ud835\udc5bf_{\\theta}(acc\\;attn)italic_f start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_a italic_c italic_c italic_a italic_t italic_t italic_n ). The figure shows this effect for the attention scores for the MPT-7B Team et al. (2023) model with a 50% reduction in \ud835\uddaa\ud835\uddb5\ud835\uddaa\ud835\uddb5\\mathsf{KV}sansserif_KV \ud835\uddbc\ud835\uddba\ud835\uddbc\ud835\uddc1\ud835\uddbe\ud835\uddbc\ud835\uddba\ud835\uddbc\ud835\uddc1\ud835\uddbe\\mathsf{cache}sansserif_cache."
        },
        "7": {
            "figure_path": "2403.09054v2_figure_7.png",
            "caption": "Figure 5: The effect of damping on the model quality for Cerebras-GPT-6.7B model with 50% \ud835\uddaa\ud835\uddb5\ud835\uddaa\ud835\uddb5\\mathsf{KV}sansserif_KV \ud835\uddbc\ud835\uddba\ud835\uddbc\ud835\uddc1\ud835\uddbe\ud835\uddbc\ud835\uddba\ud835\uddbc\ud835\uddc1\ud835\uddbe\\mathsf{cache}sansserif_cache reduction. Even after damping the score function to counteract the excess attention score, one does not achieve the model quality of the full attention model."
        },
        "8": {
            "figure_path": "2403.09054v2_figure_8.png",
            "caption": "Figure 6: The overview of \ud835\uddaa\ud835\uddbe\ud835\uddd2\ud835\uddbf\ud835\uddc8\ud835\uddcb\ud835\uddc6\ud835\uddbe\ud835\uddcb\ud835\uddaa\ud835\uddbe\ud835\uddd2\ud835\uddbf\ud835\uddc8\ud835\uddcb\ud835\uddc6\ud835\uddbe\ud835\uddcb\\mathsf{Keyformer}sansserif_Keyformer: \\Circled[fill color=black,inner color=white,]1 Initial decoding step involves token generation with n\ud835\udc5bnitalic_n tokens in \ud835\uddaa\ud835\uddb5\ud835\uddaa\ud835\uddb5\\mathsf{KV}sansserif_KV \ud835\uddbc\ud835\uddba\ud835\uddbc\ud835\uddc1\ud835\uddbe\ud835\uddbc\ud835\uddba\ud835\uddbc\ud835\uddc1\ud835\uddbe\\mathsf{cache}sansserif_cache. \ud835\uddaa\ud835\uddbe\ud835\uddd2\ud835\uddbf\ud835\uddc8\ud835\uddcb\ud835\uddc6\ud835\uddbe\ud835\uddcb\ud835\uddaa\ud835\uddbe\ud835\uddd2\ud835\uddbf\ud835\uddc8\ud835\uddcb\ud835\uddc6\ud835\uddbe\ud835\uddcb\\mathsf{Keyformer}sansserif_Keyformer induces noise for \ud835\uddc4\ud835\uddbe\ud835\uddd2\ud835\uddc4\ud835\uddbe\ud835\uddd2\\mathsf{key}sansserif_key \ud835\uddcd\ud835\uddc8\ud835\uddc4\ud835\uddbe\ud835\uddc7\ud835\uddcc\ud835\uddcd\ud835\uddc8\ud835\uddc4\ud835\uddbe\ud835\uddc7\ud835\uddcc\\mathsf{tokens}sansserif_tokens identification, selecting w\ud835\udc64witalic_w tokens from the recent window and (k\u2212w)\ud835\udc58\ud835\udc64(k-w)( italic_k - italic_w ) tokens from the remaining (n\u2212w)\ud835\udc5b\ud835\udc64(n-w)( italic_n - italic_w ) to maintain (k)\ud835\udc58(k)( italic_k ) tokens in \ud835\uddaa\ud835\uddb5\ud835\uddaa\ud835\uddb5\\mathsf{KV}sansserif_KV \ud835\uddbc\ud835\uddba\ud835\uddbc\ud835\uddc1\ud835\uddbe\ud835\uddbc\ud835\uddba\ud835\uddbc\ud835\uddc1\ud835\uddbe\\mathsf{cache}sansserif_cache. \\Circled[fill color=black,inner color=white,]2 Subsequent decoding step uses the reduced \ud835\uddaa\ud835\uddb5\ud835\uddaa\ud835\uddb5\\mathsf{KV}sansserif_KV \ud835\uddbc\ud835\uddba\ud835\uddbc\ud835\uddc1\ud835\uddbe\ud835\uddbc\ud835\uddba\ud835\uddbc\ud835\uddc1\ud835\uddbe\\mathsf{cache}sansserif_cache from the previous iteration. \\Circled[fill color=black,inner color=white,]3 The design of \ud835\uddaa\ud835\uddbe\ud835\uddd2\ud835\uddbf\ud835\uddc8\ud835\uddcb\ud835\uddc6\ud835\uddbe\ud835\uddcb\ud835\uddaa\ud835\uddbe\ud835\uddd2\ud835\uddbf\ud835\uddc8\ud835\uddcb\ud835\uddc6\ud835\uddbe\ud835\uddcb\\mathsf{Keyformer}sansserif_Keyformer from the perspective of a single decoder layer involves taking unnormalized logits from Q\u2062KT\ud835\udc44superscript\ud835\udc3e\ud835\udc47QK^{T}italic_Q italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT and introducing \u2019Gumbel\u2019 noise. This is done using a Gumbel-based probability distribution and helps address the issue of \ud835\uddc4\ud835\uddbe\ud835\uddd2\ud835\uddc4\ud835\uddbe\ud835\uddd2\\mathsf{key}sansserif_key \ud835\uddcd\ud835\uddc8\ud835\uddc4\ud835\uddbe\ud835\uddc7\ud835\uddcc\ud835\uddcd\ud835\uddc8\ud835\uddc4\ud835\uddbe\ud835\uddc7\ud835\uddcc\\mathsf{tokens}sansserif_tokens being discarded. The score function (f\u03b8subscript\ud835\udc53\ud835\udf03f_{\\theta}italic_f start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT) accumulates over decoding steps for each layer and head."
        },
        "9": {
            "figure_path": "2403.09054v2_figure_9.png",
            "caption": "Figure 7: The accuracy comparison involves Full Attention, Window Attention, H22{}_{2}start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPTO, and \ud835\uddaa\ud835\uddbe\ud835\uddd2\ud835\uddbf\ud835\uddc8\ud835\uddcb\ud835\uddc6\ud835\uddbe\ud835\uddcb\ud835\uddaa\ud835\uddbe\ud835\uddd2\ud835\uddbf\ud835\uddc8\ud835\uddcb\ud835\uddc6\ud835\uddbe\ud835\uddcb\\mathsf{Keyformer}sansserif_Keyformer with different \ud835\uddaa\ud835\uddb5\ud835\uddaa\ud835\uddb5\\mathsf{KV}sansserif_KV \ud835\uddbc\ud835\uddba\ud835\uddbc\ud835\uddc1\ud835\uddbe\ud835\uddbc\ud835\uddba\ud835\uddbc\ud835\uddc1\ud835\uddbe\\mathsf{cache}sansserif_cache sizes. The solid black line represents Full Attention without discarding tokens and with a full \ud835\uddaa\ud835\uddb5\ud835\uddaa\ud835\uddb5\\mathsf{KV}sansserif_KV \ud835\uddbc\ud835\uddba\ud835\uddbc\ud835\uddc1\ud835\uddbe\ud835\uddbc\ud835\uddba\ud835\uddbc\ud835\uddc1\ud835\uddbe\\mathsf{cache}sansserif_cache. The red dotted line denotes the 99% accuracy threshold, aligning with the MLPerf guidelines Reddi et al. (2020). Despite using 90% \ud835\uddaa\ud835\uddb5\ud835\uddaa\ud835\uddb5\\mathsf{KV}sansserif_KV \ud835\uddbc\ud835\uddba\ud835\uddbc\ud835\uddc1\ud835\uddbe\ud835\uddbc\ud835\uddba\ud835\uddbc\ud835\uddc1\ud835\uddbe\\mathsf{cache}sansserif_cache, both Window Attention and H22{}_{2}start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPTO fall short of the desired accuracy. In contrast, \ud835\uddaa\ud835\uddbe\ud835\uddd2\ud835\uddbf\ud835\uddc8\ud835\uddcb\ud835\uddc6\ud835\uddbe\ud835\uddcb\ud835\uddaa\ud835\uddbe\ud835\uddd2\ud835\uddbf\ud835\uddc8\ud835\uddcb\ud835\uddc6\ud835\uddbe\ud835\uddcb\\mathsf{Keyformer}sansserif_Keyformer achieves baseline accuracy with only 70% of the \ud835\uddaa\ud835\uddb5\ud835\uddaa\ud835\uddb5\\mathsf{KV}sansserif_KV \ud835\uddbc\ud835\uddba\ud835\uddbc\ud835\uddc1\ud835\uddbe\ud835\uddbc\ud835\uddba\ud835\uddbc\ud835\uddc1\ud835\uddbe\\mathsf{cache}sansserif_cache size."
        },
        "10": {
            "figure_path": "2403.09054v2_figure_10.png",
            "caption": "Figure 8: Evaluating long context summarization using MPT-7B-storywriter for GovReport dataset with 8k sequence length."
        },
        "11": {
            "figure_path": "2403.09054v2_figure_11.png",
            "caption": "Figure 9: Speedup of inference in an iso-accuracy setting for the MPT-7B model. Here, \ud835\uddaa\ud835\uddbe\ud835\uddd2\ud835\uddbf\ud835\uddc8\ud835\uddcb\ud835\uddc6\ud835\uddbe\ud835\uddcb\ud835\uddaa\ud835\uddbe\ud835\uddd2\ud835\uddbf\ud835\uddc8\ud835\uddcb\ud835\uddc6\ud835\uddbe\ud835\uddcb\\mathsf{Keyformer}sansserif_Keyformer reduces \ud835\uddaa\ud835\uddb5\ud835\uddaa\ud835\uddb5\\mathsf{KV}sansserif_KV \ud835\uddbc\ud835\uddba\ud835\uddbc\ud835\uddc1\ud835\uddbe\ud835\uddbc\ud835\uddba\ud835\uddbc\ud835\uddc1\ud835\uddbe\\mathsf{cache}sansserif_cache by 50% and H22{}_{2}start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPTO reduces \ud835\uddaa\ud835\uddb5\ud835\uddaa\ud835\uddb5\\mathsf{KV}sansserif_KV \ud835\uddbc\ud835\uddba\ud835\uddbc\ud835\uddc1\ud835\uddbe\ud835\uddbc\ud835\uddba\ud835\uddbc\ud835\uddc1\ud835\uddbe\\mathsf{cache}sansserif_cache by 10%. H22{}_{2}start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPTO falls short of baseline accuracy with a 50% \ud835\uddaa\ud835\uddb5\ud835\uddaa\ud835\uddb5\\mathsf{KV}sansserif_KV \ud835\uddbc\ud835\uddba\ud835\uddbc\ud835\uddc1\ud835\uddbe\ud835\uddbc\ud835\uddba\ud835\uddbc\ud835\uddc1\ud835\uddbe\\mathsf{cache}sansserif_cache."
        },
        "12": {
            "figure_path": "2403.09054v2_figure_12.png",
            "caption": "Figure 10: Normalized time for \ud835\uddaa\ud835\uddb5\ud835\uddaa\ud835\uddb5\\mathsf{KV}sansserif_KV \ud835\uddbc\ud835\uddba\ud835\uddbc\ud835\uddc1\ud835\uddbe\ud835\uddbc\ud835\uddba\ud835\uddbc\ud835\uddc1\ud835\uddbe\\mathsf{cache}sansserif_cache data movement and Scaled Dot Product (Q\u2062KT)\u2062V\ud835\udc44superscript\ud835\udc3e\ud835\udc47\ud835\udc49(QK^{T})V( italic_Q italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ) italic_V with \ud835\uddaa\ud835\uddbe\ud835\uddd2\ud835\uddbf\ud835\uddc8\ud835\uddcb\ud835\uddc6\ud835\uddbe\ud835\uddcb\ud835\uddaa\ud835\uddbe\ud835\uddd2\ud835\uddbf\ud835\uddc8\ud835\uddcb\ud835\uddc6\ud835\uddbe\ud835\uddcb\\mathsf{Keyformer}sansserif_Keyformer 50% \ud835\uddaa\ud835\uddb5\ud835\uddaa\ud835\uddb5\\mathsf{KV}sansserif_KV \ud835\uddbc\ud835\uddba\ud835\uddbc\ud835\uddc1\ud835\uddbe\ud835\uddbc\ud835\uddba\ud835\uddbc\ud835\uddc1\ud835\uddbe\\mathsf{cache}sansserif_cache reduction along with its score function overhead."
        },
        "13": {
            "figure_path": "2403.09054v2_figure_13.png",
            "caption": "Figure 11: Increase in sparsity with varying threshold percentage."
        },
        "14": {
            "figure_path": "2403.09054v2_figure_14.png",
            "caption": "Figure 12: Varying the recent ratio (w)\ud835\udc64(w)( italic_w ) in \ud835\uddaa\ud835\uddbe\ud835\uddd2\ud835\uddbf\ud835\uddc8\ud835\uddcb\ud835\uddc6\ud835\uddbe\ud835\uddcb\ud835\uddaa\ud835\uddbe\ud835\uddd2\ud835\uddbf\ud835\uddc8\ud835\uddcb\ud835\uddc6\ud835\uddbe\ud835\uddcb\\mathsf{Keyformer}sansserif_Keyformer at 70% \ud835\uddaa\ud835\uddb5\ud835\uddaa\ud835\uddb5\\mathsf{KV}sansserif_KV \ud835\uddbc\ud835\uddba\ud835\uddbc\ud835\uddc1\ud835\uddbe\ud835\uddbc\ud835\uddba\ud835\uddbc\ud835\uddc1\ud835\uddbe\\mathsf{cache}sansserif_cache and its impact on model accuracy for summarization task with CNN/DailyMail dataset."
        },
        "15": {
            "figure_path": "2403.09054v2_figure_15.png",
            "caption": "Figure 13: ROUGE-1 and ROUGE-L scores comparison of Full Attention, Window Attention, H22{}_{2}start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPTO and \ud835\uddaa\ud835\uddbe\ud835\uddd2\ud835\uddbf\ud835\uddc8\ud835\uddcb\ud835\uddc6\ud835\uddbe\ud835\uddcb\ud835\uddaa\ud835\uddbe\ud835\uddd2\ud835\uddbf\ud835\uddc8\ud835\uddcb\ud835\uddc6\ud835\uddbe\ud835\uddcb\\mathsf{Keyformer}sansserif_Keyformer with varying \ud835\uddaa\ud835\uddb5\ud835\uddaa\ud835\uddb5\\mathsf{KV}sansserif_KV \ud835\uddbc\ud835\uddba\ud835\uddbc\ud835\uddc1\ud835\uddbe\ud835\uddbc\ud835\uddba\ud835\uddbc\ud835\uddc1\ud835\uddbe\\mathsf{cache}sansserif_cache size. The solid black line shows Full Attention without discarding any token and full \ud835\uddaa\ud835\uddb5\ud835\uddaa\ud835\uddb5\\mathsf{KV}sansserif_KV \ud835\uddbc\ud835\uddba\ud835\uddbc\ud835\uddc1\ud835\uddbe\ud835\uddbc\ud835\uddba\ud835\uddbc\ud835\uddc1\ud835\uddbe\\mathsf{cache}sansserif_cache. The red dotted line shows 99% accuracy mark as per MLPerf Reddi et al. (2020)."
        },
        "16": {
            "figure_path": "2403.09054v2_figure_16.png",
            "caption": "Figure 14: Attention heat map for GPT-J Wang & Komatsuzaki (2021) model with first 4 layers and heads (denoted as L_<layer>,H_<head>). The x-axis comprises context + text generation, while the y-axis only contains text generation. Empty (white) dots show zero attention score and present inherent sparsity."
        },
        "17": {
            "figure_path": "2403.09054v2_figure_17.png",
            "caption": "Figure 15: Attention heat map for MPT-7B Team et al. (2023) model with first 6 layers and 8 heads (denoted as L_<layer>,H_<head>). The x-axis comprises context + text generation, while the y-axis only contains text generation. Empty (white) dots show zero attention score and present inherent sparsity. The effect of ALiBi Press et al. (2021) can be seen as we move from Head 0 to Head 7."
        },
        "18": {
            "figure_path": "2403.09054v2_figure_18.png",
            "caption": "Figure 16: Varying the temperature parameter (\u03c4)\ud835\udf0f(\\tau)( italic_\u03c4 ) for MPT-7B model for \ud835\uddaa\ud835\uddbe\ud835\uddd2\ud835\uddbf\ud835\uddc8\ud835\uddcb\ud835\uddc6\ud835\uddbe\ud835\uddcb\ud835\uddaa\ud835\uddbe\ud835\uddd2\ud835\uddbf\ud835\uddc8\ud835\uddcb\ud835\uddc6\ud835\uddbe\ud835\uddcb\\mathsf{Keyformer}sansserif_Keyformer and its impact on model accuracy for summarization task with CNN/DailyMail dataset. Sweeping value of \u03c4\ud835\udf0f\\tauitalic_\u03c4 from 1 to 2 across the token generation phase works better in comparison to fixed temperature across the prompt processing and token generation phase."
        }
    },
    "references": [
        {
            "1": {
                "title": "Accelerating recommendation system training by leveraging popular choices.",
                "author": "Adnan, M., Maboud, Y. E., Mahajan, D., and Nair, P. J.",
                "venue": "Proc. VLDB Endow., 15(1):127\u2013140, sep 2021.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Ad-rec: Advanced feature interactions to address covariate-shifts in recommendation networks.",
                "author": "Adnan, M., Maboud, Y. E., Mahajan, D., and Nair, P. J.",
                "venue": "arXiv preprint arXiv:2308.14902, 2023.",
                "url": null
            }
        },
        {
            "3": {
                "title": "Heterogeneous acceleration pipeline for recommendation system training.",
                "author": "Adnan, M., Maboud, Y. E., Mahajan, D., and Nair, P. J.",
                "venue": "In Proceedings of the 51st International Symposium on Computer Architecture (ISCA). ACM, 2024.",
                "url": null
            }
        },
        {
            "4": {
                "title": "Gqa: Training generalized multi-query transformer models from multi-head checkpoints.",
                "author": "Ainslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y., Lebr\u00f3n, F., and Sanghai, S.",
                "venue": "arXiv preprint arXiv:2305.13245, 2023.",
                "url": null
            }
        },
        {
            "5": {
                "title": "Dynamic context pruning for efficient and interpretable autoregressive transformers.",
                "author": "Anagnostidis, S., Pavllo, D., Biggio, L., Noci, L., Lucchi, A., and Hoffmann, T.",
                "venue": "arXiv preprint arXiv:2305.15805, 2023.",
                "url": null
            }
        },
        {
            "6": {
                "title": "Longbench: A bilingual, multitask benchmark for long context understanding.",
                "author": "Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z., Du, Z., Liu, X., Zeng, A., Hou, L., et al.",
                "venue": "arXiv preprint arXiv:2308.14508, 2023.",
                "url": null
            }
        },
        {
            "7": {
                "title": "Longformer: The long-document transformer.",
                "author": "Beltagy, I., Peters, M. E., and Cohan, A.",
                "venue": "arXiv preprint arXiv:2004.05150, 2020.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Piqa: Reasoning about physical commonsense in natural language.",
                "author": "Bisk, Y., Zellers, R., Gao, J., Choi, Y., et al.",
                "venue": "In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp.  7432\u20137439, 2020.",
                "url": null
            }
        },
        {
            "9": {
                "title": "Language models are few-shot learners.",
                "author": "Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.",
                "venue": "Advances in neural information processing systems, 33:1877\u20131901, 2020.",
                "url": null
            }
        },
        {
            "10": {
                "title": "Longlora: Efficient fine-tuning of long-context large language models.",
                "author": "Chen, Y., Qian, S., Tang, H., Lai, X., Liu, Z., Han, S., and Jia, J.",
                "venue": "arXiv:2309.12307, 2023.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Generating long sequences with sparse transformers.",
                "author": "Child, R., Gray, S., Radford, A., and Sutskever, I.",
                "venue": "arXiv preprint arXiv:1904.10509, 2019.",
                "url": null
            }
        },
        {
            "12": {
                "title": "Rethinking attention with performers.",
                "author": "Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., et al.",
                "venue": "arXiv preprint arXiv:2009.14794, 2020.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Generalized gumbel distribution.",
                "author": "Cooray, K.",
                "venue": "Journal of Applied Statistics, 37(1):171\u2013179, 2010.",
                "url": null
            }
        },
        {
            "14": {
                "title": "Flashattention: Fast and memory-efficient exact attention with io-awareness.",
                "author": "Dao, T., Fu, D., Ermon, S., Rudra, A., and R\u00e9, C.",
                "venue": "Advances in Neural Information Processing Systems, 35:16344\u201316359, 2022.",
                "url": null
            }
        },
        {
            "15": {
                "title": "Transformers4rec: Bridging the gap between nlp and sequential/session-based recommendation.",
                "author": "de Souza Pereira Moreira, G., Rabhi, S., Lee, J. M., Ak, R., and Oldridge, E.",
                "venue": "In Proceedings of the 15th ACM Conference on Recommender Systems, pp.  143\u2013153, 2021.",
                "url": null
            }
        },
        {
            "16": {
                "title": "Cerebras-gpt: Open compute-optimal language models trained on the cerebras wafer-scale cluster, 2023.",
                "author": "Dey, N., Gosal, G., Zhiming, Chen, Khachane, H., Marshall, W., Pathria, R., Tom, M., and Hestness, J.",
                "venue": null,
                "url": null
            }
        },
        {
            "17": {
                "title": "Qaq: Quality adaptive quantization for llm kv cache.",
                "author": "Dong, S., Cheng, W., Qin, J., and Wang, W.",
                "venue": "2024.",
                "url": null
            }
        },
        {
            "18": {
                "title": "An image is worth 16x16 words: Transformers for image recognition at scale.",
                "author": "Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.",
                "venue": "arXiv preprint arXiv:2010.11929, 2020.",
                "url": null
            }
        },
        {
            "19": {
                "title": "A framework for few-shot language model evaluation.",
                "author": "Gao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., McDonell, K., Muennighoff, N., et al.",
                "venue": "Version v0. 0.1. Sept, 2021.",
                "url": null
            }
        },
        {
            "20": {
                "title": "Model tells you what to discard: Adaptive kv cache compression for llms.",
                "author": "Ge, S., Zhang, Y., Liu, L., Zhang, M., Han, J., and Gao, J.",
                "venue": "arXiv preprint arXiv:2310.01801, 2024.",
                "url": null
            }
        },
        {
            "21": {
                "title": "Hiclip: Contrastive language-image pretraining with hierarchy-aware attention.",
                "author": "Geng, S., Yuan, J., Tian, Y., Chen, Y., and Zhang, Y.",
                "venue": "arXiv preprint arXiv:2303.02995, 2023.",
                "url": null
            }
        },
        {
            "22": {
                "title": "Power-bert: Accelerating bert inference via progressive word-vector elimination.",
                "author": "Goyal, S., Choudhury, A. R., Raje, S., Chakaravarthy, V., Sabharwal, Y., and Verma, A.",
                "venue": "In International Conference on Machine Learning, pp.  3690\u20133699. PMLR, 2020.",
                "url": null
            }
        },
        {
            "23": {
                "title": "Efficient attentions for long document summarization.",
                "author": "Huang, L., Cao, S., Parulian, N., Ji, H., and Wang, L.",
                "venue": "arXiv preprint arXiv:2104.02112, 2021a.",
                "url": null
            }
        },
        {
            "24": {
                "title": "Efficient attentions for long document summarization.",
                "author": "Huang, L., Cao, S., Parulian, N., Ji, H., and Wang, L.",
                "venue": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp.  1419\u20131436, Online, June 2021b. Association for Computational Linguistics.",
                "url": null
            }
        },
        {
            "25": {
                "title": "Categorical reparameterization with gumbel-softmax.",
                "author": "Jang, E., Gu, S., and Poole, B.",
                "venue": "arXiv preprint arXiv:1611.01144, 2016.",
                "url": null
            }
        },
        {
            "26": {
                "title": "Gear: An efficient kv cache compression recipe for near-lossless generative inference of llm.",
                "author": "Kang, H., Zhang, Q., Kundu, S., Jeong, G., Liu, Z., Krishna, T., and Zhao, T.",
                "venue": "arXiv preprint arXiv:2403.05527, 2024.",
                "url": null
            }
        },
        {
            "27": {
                "title": "Flat: An optimized dataflow for mitigating attention bottlenecks.",
                "author": "Kao, S.-C., Subramanian, S., Agrawal, G., Yazdanbakhsh, A., and Krishna, T.",
                "venue": "In Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2, pp.  295\u2013310, 2023.",
                "url": null
            }
        },
        {
            "28": {
                "title": "Transformers are rnns: Fast autoregressive transformers with linear attention.",
                "author": "Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.",
                "venue": "In International Conference on Machine Learning, pp.  5156\u20135165. PMLR, 2020.",
                "url": null
            }
        },
        {
            "29": {
                "title": "Soda: Million-scale dialogue distillation with social commonsense contextualization.",
                "author": "Kim, H., Hessel, J., Jiang, L., West, P., Lu, X., Yu, Y., Zhou, P., Bras, R. L., Alikhani, M., Kim, G., Sap, M., and Choi, Y.",
                "venue": "ArXiv, abs/2212.10465, 2022.",
                "url": null
            }
        },
        {
            "30": {
                "title": "Kaiser, \u0142l., levskaya, a.: Reformer: The efficient transformer.",
                "author": "Kitaev, N.",
                "venue": "arXiv preprint arXiv:2001.04451, 2020.",
                "url": null
            }
        },
        {
            "31": {
                "title": "Efficient memory management for large language model serving with pagedattention.",
                "author": "Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J. E., Zhang, H., and Stoica, I.",
                "venue": "arXiv preprint arXiv:2309.06180, 2023.",
                "url": null
            }
        },
        {
            "32": {
                "title": "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.",
                "author": "Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L.",
                "venue": "arXiv preprint arXiv:1910.13461, 2019.",
                "url": null
            }
        },
        {
            "33": {
                "title": "How long can context length of open-source llms truly promise?",
                "author": "Li, D., Shao, R., Xie, A., Sheng, Y., Zheng, L., Gonzalez, J., Stoica, I., Ma, X., and Zhang, H.",
                "venue": "In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023.",
                "url": null
            }
        },
        {
            "34": {
                "title": "Rouge: A package for automatic evaluation of summaries.",
                "author": "Lin, C.-Y.",
                "venue": "In Text summarization branches out, pp.  74\u201381, 2004.",
                "url": null
            }
        },
        {
            "35": {
                "title": "Deja vu: Contextual sparsity for efficient llms at inference time.",
                "author": "Liu, Z., Wang, J., Dao, T., Zhou, T., Yuan, B., Song, Z., Shrivastava, A., Zhang, C., Tian, Y., Re, C., et al.",
                "venue": "In International Conference on Machine Learning, pp.  22137\u201322176. PMLR, 2023.",
                "url": null
            }
        },
        {
            "36": {
                "title": "The concrete distribution: A continuous relaxation of discrete random variables.",
                "author": "Maddison, C. J., Mnih, A., and Teh, Y. W.",
                "venue": "arXiv preprint arXiv:1611.00712, 2016.",
                "url": null
            }
        },
        {
            "37": {
                "title": "Can a suit of armor conduct electricity? a new dataset for open book question answering.",
                "author": "Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A.",
                "venue": "arXiv preprint arXiv:1809.02789, 2018.",
                "url": null
            }
        },
        {
            "38": {
                "title": "Landmark attention: Random-access infinite context length for transformers.",
                "author": "Mohtashami, A. and Jaggi, M.",
                "venue": "arXiv preprint arXiv:2305.16300, 2023.",
                "url": null
            }
        },
        {
            "39": {
                "title": "Learning to compress prompts with gist tokens.",
                "author": "Mu, J., Li, X. L., and Goodman, N.",
                "venue": "arXiv preprint arXiv:2304.08467, 2023.",
                "url": null
            }
        },
        {
            "40": {
                "title": "fairseq: A fast, extensible toolkit for sequence modeling.",
                "author": "Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and Auli, M.",
                "venue": "In Ammar, W., Louis, A., and Mostafazadeh, N. (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pp.  48\u201353, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.",
                "url": null
            }
        },
        {
            "41": {
                "title": "Efficiently scaling transformer inference.",
                "author": "Pope, R., Douglas, S., Chowdhery, A., Devlin, J., Bradbury, J., Heek, J., Xiao, K., Agrawal, S., and Dean, J.",
                "venue": "Proceedings of Machine Learning and Systems, 5, 2023.",
                "url": null
            }
        },
        {
            "42": {
                "title": "Train short, test long: Attention with linear biases enables input length extrapolation.",
                "author": "Press, O., Smith, N. A., and Lewis, M.",
                "venue": "arXiv preprint arXiv:2108.12409, 2021.",
                "url": null
            }
        },
        {
            "43": {
                "title": "Exploring the limits of transfer learning with a unified text-to-text transformer.",
                "author": "Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J.",
                "venue": "The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.",
                "url": null
            }
        },
        {
            "44": {
                "title": "Mlperf inference benchmark, 2020.",
                "author": "Reddi, V. J., Cheng, C., Kanter, D., Mattson, P., Schmuelling, G., Wu, C.-J., Anderson, B., Breughe, M., Charlebois, M., Chou, W., Chukka, R., Coleman, C., Davis, S., Deng, P., Diamos, G., Duke, J., Fick, D., Gardner, J. S., Hubara, I., Idgunji, S., Jablin, T. B., Jiao, J., John, T. S., Kanwar, P., Lee, D., Liao, J., Lokhmotov, A., Massa, F., Meng, P., Micikevicius, P., Osborne, C., Pekhimenko, G., Rajan, A. T. R., Sequeira, D., Sirasao, A., Sun, F., Tang, H., Thomson, M., Wei, F., Wu, E., Xu, L., Yamada, K., Yu, B., Yuan, G., Zhong, A., Zhang, P., and Zhou, Y.",
                "venue": null,
                "url": null
            }
        },
        {
            "45": {
                "title": "Choice of plausible alternatives: An evaluation of commonsense causal reasoning.",
                "author": "Roemmele, M., Bejan, C. A., and Gordon, A. S.",
                "venue": "In 2011 AAAI Spring Symposium Series, 2011.",
                "url": null
            }
        },
        {
            "46": {
                "title": "Winogrande: An adversarial winograd schema challenge at scale.",
                "author": "Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y.",
                "venue": "Communications of the ACM, 64(9):99\u2013106, 2021.",
                "url": null
            }
        },
        {
            "47": {
                "title": "Get to the point: Summarization with pointer-generator networks.",
                "author": "See, A., Liu, P. J., and Manning, C. D.",
                "venue": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.  1073\u20131083, Vancouver, Canada, July 2017. Association for Computational Linguistics.",
                "url": null
            }
        },
        {
            "48": {
                "title": "Fast transformer decoding: One write-head is all you need.",
                "author": "Shazeer, N.",
                "venue": "arXiv preprint arXiv:1911.02150, 2019.",
                "url": null
            }
        },
        {
            "49": {
                "title": "Flexgen: High-throughput generative inference of large language models with a single gpu.",
                "author": "Sheng, Y., Zheng, L., Yuan, B., Li, Z., Ryabinin, M., Chen, B., Liang, P., R\u00e9, C., Stoica, I., and Zhang, C.",
                "venue": "In International Conference on Machine Learning, pp.  31094\u201331116. PMLR, 2023.",
                "url": null
            }
        },
        {
            "50": {
                "title": "D\u00e9j\u00e0vu: Kv-cache streaming for fast, fault-tolerant generative llm serving.",
                "author": "Strati, F., Mcallister, S., Phanishayee, A., Tarnawski, J., and Klimovic, A.",
                "venue": "arXiv preprint arXiv:2403.01876, 2024.",
                "url": null
            }
        },
        {
            "51": {
                "title": "Roformer: Enhanced transformer with rotary position embedding, 2022.",
                "author": "Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., and Liu, Y.",
                "venue": null,
                "url": null
            }
        },
        {
            "52": {
                "title": "Adaptive attention span in transformers.",
                "author": "Sukhbaatar, S., Grave, E., Bojanowski, P., and Joulin, A.",
                "venue": "arXiv preprint arXiv:1905.07799, 2019.",
                "url": null
            }
        },
        {
            "53": {
                "title": "Bert4rec: Sequential recommendation with bidirectional encoder representations from transformer.",
                "author": "Sun, F., Liu, J., Wu, J., Pei, C., Lin, X., Ou, W., and Jiang, P.",
                "venue": "In Proceedings of the 28th ACM international conference on information and knowledge management, pp.  1441\u20131450, 2019.",
                "url": null
            }
        },
        {
            "54": {
                "title": "Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023.",
                "author": "Team, M. N. et al.",
                "venue": "URL www. mosaicml. com/blog/mpt-7b. Accessed, pp.  05\u201305, 2023.",
                "url": null
            }
        },
        {
            "55": {
                "title": "Attention is all you need.",
                "author": "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I.",
                "venue": "Advances in neural information processing systems, 30, 2017.",
                "url": null
            }
        },
        {
            "56": {
                "title": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.",
                "author": "Wang, B. and Komatsuzaki, A.",
                "venue": "https://github.com/kingoflolz/mesh-transformer-jax, May 2021.",
                "url": null
            }
        },
        {
            "57": {
                "title": "Spatten: Efficient sparse attention architecture with cascade token and head pruning.",
                "author": "Wang, H., Zhang, Z., and Han, S.",
                "venue": "In 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA), pp.  97\u2013110. IEEE, 2021.",
                "url": null
            }
        },
        {
            "58": {
                "title": "FLuID: Mitigating stragglers in federated learning using invariant dropout.",
                "author": "Wang, I., Nair, P. J., and Mahajan, D.",
                "venue": "In Thirty-seventh Conference on Neural Information Processing Systems, 2023.",
                "url": null
            }
        },
        {
            "59": {
                "title": "Linformer: Self-attention with linear complexity.",
                "author": "Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H.",
                "venue": "arXiv preprint arXiv:2006.04768, 2020.",
                "url": null
            }
        },
        {
            "60": {
                "title": "Overlap communication with dependent computation via decomposition in large deep learning models.",
                "author": "Wang, S., Wei, J., Sabne, A., Davis, A., Ilbeyi, B., Hechtman, B., Chen, D., Murthy, K. S., Maggioni, M., Zhang, Q., et al.",
                "venue": "In Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1, pp.  93\u2013106, 2022.",
                "url": null
            }
        },
        {
            "61": {
                "title": "Huggingface\u2019s transformers: State-of-the-art natural language processing.",
                "author": "Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., et al.",
                "venue": "arXiv preprint arXiv:1910.03771, 2019.",
                "url": null
            }
        },
        {
            "62": {
                "title": "Efficient streaming language models with attention sinks.",
                "author": "Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M.",
                "venue": "arXiv preprint arXiv:2309.17453, 2023.",
                "url": null
            }
        },
        {
            "63": {
                "title": "El-attention: Memory efficient lossless attention for generation.",
                "author": "Yan, Y., Chen, J., Qi, W., Bhendawade, N., Gong, Y., Duan, N., and Zhang, R.",
                "venue": "In International Conference on Machine Learning, pp.  11648\u201311658. PMLR, 2021.",
                "url": null
            }
        },
        {
            "64": {
                "title": "Big bird: Transformers for longer sequences.",
                "author": "Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., et al.",
                "venue": "Advances in neural information processing systems, 33:17283\u201317297, 2020.",
                "url": null
            }
        },
        {
            "65": {
                "title": "H  o: Heavy-hitter oracle for efficient generative inference of large language models.",
                "author": "Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai, R., Song, Z., Tian, Y., R\u00e9, C., Barrett, C., et al.",
                "venue": "arXiv preprint arXiv:2306.14048, 2023.",
                "url": null
            }
        },
        {
            "66": {
                "title": "A survey of large language models.",
                "author": "Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., et al.",
                "venue": "arXiv preprint arXiv:2303.18223, 2023.",
                "url": null
            }
        },
        {
            "67": {
                "title": "Alisa: Accelerating large language model inference via sparsity-aware kv caching.",
                "author": "Zhao, Y., Wu, D., and Wang, J.",
                "venue": "arXiv preprint arXiv:2403.17312, 2024.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.09054v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "5"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.2.1",
            "3.3",
            "3.3.1",
            "3.3.2",
            "3.4"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4",
            "4.4.1",
            "4.4.2",
            "4.4.3",
            "4.4.4",
            "4.4.5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.4",
            "4.4.1",
            "4.4.2",
            "4.4.3",
            "4.4.4",
            "4.4.5"
        ]
    },
    "research_context": {
        "paper_id": "2403.09054v2",
        "paper_title": "1 Introduction",
        "research_background": "### Motivation:\nThe paper is motivated by the significant advancements in deploying Large Language Models (LLMs) across a range of tasks\u2014such as language modeling, image recognition, recommendations, and text generation\u2014thanks to the success of Transformers. However, these models face critical challenges in terms of inference latency and throughput, especially when processing longer input sequences. The cause of these issues is mainly twofold: the computationally expensive attention mechanism, which scales quadratically with input size, and the inefficiency of Key-Value Cache (KV Cache) structures that need to be frequently accessed from off-chip memory. Previous efforts to mitigate these challenges often fall short by either requiring extensive retraining or solely focusing on attention mechanisms without adequately addressing the KV Cache's expandability and latency issues.\n\n### Research Problem:\nThe research problem addressed in this paper is how to develop effective inference-time strategies to reduce the size of the KV Cache without compromising on the accuracy of Large Language Models. The goal is to create a method that allows for low-latency and high-throughput inference even with larger context sequences while maintaining stringent accuracy requirements.\n\n### Relevant Prior Work:\n1. **Success of Transformers:**\n   - Language modeling (Lewis et al. 2019, Brown et al. 2020, Raffel et al. 2020)\n   - Image recognition (Dosovitskiy et al. 2020)\n   - Recommendations (Sun et al. 2019, de Souza Pereira Moreira et al. 2021, Adnan et al. 2023, Zhao et al. 2023)\n\n2. **Challenges in Processing Long Sequences:**\n   - Bai et al. (2023)\n   - Li et al. (2023)\n   - Chen et al. (2023)\n   - Huang et al. (2021a)\n\n3. **Attention Mechanism Overheads:**\n   - Sukhbaatar et al. (2019)\n   - Dao et al. (2022)\n   - Choromanski et al. (2020)\n\n4. **KV Cache and Memory Latency Issues:**\n   - Ott et al. (2019)\n\n5. **Previous Mitigation Efforts:**\n   - Reducing attention mechanism costs (Zaheer et al. 2020, Kitaev 2020, Wang et al. 2020, Beltagy et al. 2020)\n   - System-level optimizations (FlexGen Sheng et al. 2023, Flash Attention Dao et al. 2022, Paged Attention Kwon et al. 2023, multi-dimensional partitioning Pope et al. 2023)\n   - Multi-query and group-query attention (Shazeer 2019, Ainslie et al. 2023)\n\nThe paper introduces a novel approach named **Keyformer** that dynamically reduces the KV Cache size during inference by selectively discarding less significant tokens without sacrificing model accuracy. This enhances overall inference performance, making it suitable for deployment in resource-constrained scenarios while adhering to accuracy constraints as defined by benchmarks like MLPerf.",
        "methodology": "## Methodology\n\nThe proposed method leverages the inherent sparsity within decoder layers by identifying key tokens using a mixture of recent tokens. It adjusts the changes in the score function resulting from discarded tokens by applying regularization to the unnormalized logits for the identification of significant sequences.\n\nKey components and innovations of our method include:\n\n1. **Sparsity Exploitation**: Utilizing the inherent sparsity in decoder layers by focusing on a mixture of recent tokens to perform identification effectively.\n\n2. **Logit Regularization**: Changes in the score function, affected by discarded tokens, are addressed by applying regularization to the unnormalized logits, improving the robustness of token identification.\n\n3. **Gumbel Distribution**: Inspired by the Gumbel distribution (Cooray, 2010), we employ this distribution due to its suitability for characterizing maximum values within a set of samples and its skewness towards initial tokens. This makes it ideal for modeling long sequences, as demonstrated in the application of the Gumbel pdf to unnormalized logits (Equation 5 ###reference_###) and the pdf of logits adjusted with Gumbel addition (Equation 6 ###reference_###). \n\n4. **Statistical Significance**: The Gumbel distribution's relevance is further underscored by the Gumbel limit theorem, which posits that common probability distributions (e.g., normal, exponential, uniform) converge to the Gumbel distribution. This theoretical backing supports its appropriateness for our identification task.\n\n5. **Regularization Distribution**: During inference, when information about discarded tokens is unavailable, selecting a regularization distribution that promotes uniformity after normalization is crucial. The method highlights the importance of uniformity to aid in identification.\n\n6. **Entropy Measurement**: To quantify the spread of probability distributions post-normalization, entropy is employed. Our analysis shows that Gumbel-based logit adjustment fosters a more uniform distribution, indicating its effectiveness as a regularization technique. This is substantiated through entropy calculations as presented in Equation 7 ###reference_### and Equation 8 ###reference_###.\n\nThe combination of these elements forms a robust and theoretically underpinned approach for effective token identification in handling long sequences within decoder layers.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n**Datasets and Models**:\n- **GPT-J Wang & Komatsuzaki (2021)**: Uses Rotary Position Embeddings (RoPE).\n- **Cerebras-GPT Dey et al. (2023)**: Employs learnable position embeddings.\n- **MPT Team et al. (2023)**: Utilizes ALiBi (Attention with Linear Bias) for position encoding. Includes MPT-chat, fine-tuned for dialogue generation, and MPT-storywriter, fine-tuned for long document story writing.\n\n**Tasks**:\n- **Summarization**:\n  - **Datasets**: CNN/DailyMail See et al. (2017) and GovReport Huang et al. (2021b).\n  - Model Usage: GPT-J was specifically fine-tuned for summarization; MPT-storywriter model was employed for generating long documents.\n\n- **Conversation**:\n  - **Dataset**: SODA Kim et al. (2022).\n  - Model Usage: MPT-chat version of MPT model, fine-tuned for dialogue generation.\n\n- **Few-Shot Evaluation Tasks**: PIQA Bisk et al. (2020), Winogrande Sakaguchi et al. (2021), OpenBookQA Mihaylov et al. (2018), and COPA Roemmele et al. (2011).\n\n**Evaluation Metrics and Procedures**:\n- Used Fixed Beam Size: 4 for all evaluations.\n- Accuracy Target: Aim for 99% to 99.9% of Full Attention, considered the gold standard.\n- Comparisons: Compared against Full Attention, Window Attention, and the HO model Zhang et al. (2023).\n- GPU: Evaluations executed on NVIDIA A100 (80GB) GPUs.\n\n**Experimental Results**:\n- **Accuracy**: The accuracy of the new method generally maintained the desired threshold of 99% even with a reduced context size (50%  size), outperforming the HO model, which presented significantly lower accuracy at the same size.\n- **Efficiency**:\n  - Reduced KV Cache: The method achieved a reduction factor of 2.9 in data movement from off-chip GPU HBM.\n  - Scaled Dot Product Optimization: Enhanced computational efficiency by a factor of 1.3 in the attention module\u2019s scaled dot product, specifically for sequences of length 4k.\n- **Performance**:\n  - For long document summarization using the MPT-storywriter model, the system was able to handle content up to 84k tokens with significant improvements in computational efficiency.\n\nOverall, the findings imply that the proposed method maintains high accuracy while significantly reducing the computational burden, particularly in managing long sequences, emphasizing its practical utility and effectiveness."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To investigate how different logit adjustment distributions impact model accuracy and identification.",
            "experiment_process": "The study evaluated three regularization strategies: no logit adjustment, constant logit adjustment, and Gaussian distribution-based logit adjustment. Experiments were conducted on the CNN/Daily Mail dataset for a summarization task with a 60% reduction in a specified parameter. For comparison, the standard Gumbel probability density function (pdf) with a mean of mu and variance of theta, Gaussian pdf with identical mean and variance, and constant value for logit adjustment were used.",
            "result_discussion": "Empirical evidence shows that the Gumbel distribution is effective for logit adjustment due to its skewness towards initial tokens. This suggests that Gumbel-based logit adjustment provides better model accuracy for the summarization task compared to other approaches.",
            "ablation_id": "2403.09054v2.No1"
        },
        {
            "research_objective": "To compare the performance of shared versus per-layer score functions in generative LLMs.",
            "experiment_process": "In generative LLMs with stacked decoder layers, the score function, phi, was either shared across all layers (Shared) or dedicated to each layer (Per-Layer). The study maintained original positional information and consistent KV size to compare accuracy. Per-Layer assigns a unique score function to each decoder layer, while Shared uses one global score function.",
            "result_discussion": "The Per-Layer score function yields better accuracy than the shared score function. This supports the intuition that hierarchical text representations across layers in transformers benefit from having a per-layer score function specific to each layer.",
            "ablation_id": "2403.09054v2.No2"
        },
        {
            "research_objective": "To investigate the impact of original versus new positional information on model accuracy.",
            "experiment_process": "The study explored two approaches: Original Positional (Org Pos) and New Positional (New Pos) information. Org Pos reflected original token positions, while New Pos was based on the new arrangement of tokens. Accuracy was compared under consistent KV size and score function.",
            "result_discussion": "Using original positional information (Org Pos) results in higher accuracy. However, even with new positional information (New Pos), the approach outperforms the state-of-the-art method HO.",
            "ablation_id": "2403.09054v2.No3"
        },
        {
            "research_objective": "To examine the sensitivity of model performance to varying ratios of recent tokens in the KV Cache.",
            "experiment_process": "The study varied the ratio of recent tokens, resulting in different KV caches' sizes and corresponding accuracies. The study focused on the range of 20% to 30% for recent tokens ratio.",
            "result_discussion": "Models perform better when the recent token ratio falls within the 20% to 30% range, supporting the hypothesis that recent tokens are crucial for LLM inference.",
            "ablation_id": "2403.09054v2.No4"
        },
        {
            "research_objective": "To compare the efficacy of the KV Cache method with StreamingLLM's attention sinks approach.",
            "experiment_process": "The comparison maintained a KV Cache size of 60% for both methods. StreamingLLM uses the first four tokens as 'attention sinks' alongside a moving window of recent tokens. Accuracy comparisons were drawn to evaluate how each method handles sequence lengths.",
            "result_discussion": "StreamingLLM struggles with text summarization by relying on only the first four tokens as attention sinks. The KV Cache method demonstrates better performance for summarization tasks.",
            "ablation_id": "2403.09054v2.No5"
        }
    ]
}