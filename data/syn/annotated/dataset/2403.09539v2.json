{
    "title": "Logits of API-Protected LLMs Leak Proprietary Information",
    "abstract": "The commercialization of large language models (LLMs) has led to the common practice of restricting access to proprietary models via a limited API. In this work, we show that, with a conservative assumption about the model architecture, it is possible to learn a surprisingly large amount of non-public information about an API-protected LLM from a small number of API queries (e.g., costing under $1,000 for OpenAI\u2019s gpt-3.5-turbo). Our findings are centered on one key observation: most modern LLMs suffer from a softmax bottleneck, which restricts the model outputs to a linear subspace of the full output space. We exploit this fact to unlock several capabilities: obtaining cheap full-vocabulary outputs, detecting and disambiguating different model updates, identifying the source LLM given a single full LLM output, and even estimating the output layer parameters. Our empirical investigations show the effectiveness of our methods. Lastly, we discuss ways that LLM providers can guard against these attacks, as well as how these capabilities can be viewed as a feature (rather than a bug) by allowing for greater transparency and accountability.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "As large language models (LLMs) have become more capable and valuable, it has become increasingly common for companies and organizations to train closed-source LLMs and make them available only via an API. This setup may foster a false sense of security for LLM providers, who might mistakenly assume that information about their model architecture is private, and that certain types of attacks on their LLM are infeasible. On the flip side, users must seemingly take providers\u2019 word that LLMs only change when the provider publicly announces version updates.\n\nDespite this apparent state of affairs, in reality many LLM APIs reveal much more information about their underlying models than previously thought. In this paper we show how to extract detailed information about LLM parameterization using only common API configurations. Our findings allow LLM clients to hold providers accountable by tracking model changes, recovering hidden prompts, and cheaply obtaining full vocabulary outputs. At the same time, our approach also allows LLM providers to establish unique identities for their models, enabling trust with their users as well as improved accountability.\n\nOur method exploits the low-rank output layer common to most LLM architectures by observing that this layer\u2019s outputs are constrained to a low-dimensional subspace of the full output space. We call this restricted output space the LLM\u2019s image. We can obtain a basis for this image by collecting a small number of LLM outputs, and we develop novel algorithms that enable us to accomplish this cheaply and quickly for standard LLM APIs. Obtaining the LLM image allows us to explore several capabilities that cover a broad set of applications, and empirically verify several of them.\n\nWe empirically show the effectiveness of using LLM images as unique signatures that can be used to identify outputs from a model with high accuracy, a useful property for API LLM accountability. The sensitivity of these signatures to slight changes in the LLM parameters also makes them suitable for inferring granular information about model parameter updates. Considering several proposals to mitigate this vulnerability, we find no obvious fix to prevent obtaining LLM images without dramatically altering the LLM architecture. While providers may choose to alter the API to hide this information, the relevant API features have valuable and safe use cases for the LLM clients, who may rely on access to features like logit bias.\n\nThough our findings may be viewed as a warning to LLM providers to carefully consider the consequences of their LLM architectures and APIs, we prefer to view our findings as a potential feature that LLM providers may choose to keep in order to better maintain trust with their customers by allowing outside observers to audit their model."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "LLM outputs are restricted to a low-dimensional linear space",
            "text": "###figure_2### Consider the architecture of a typical LLM, depicted in Figure 2  ###reference_###.\nA Transformer with embedding size  outputs a low-dimensional contextualized embedding  (or simply embedding).\nProjecting the embedding onto  via the linear map defined by the LLM\u2019s softmax matrix , we obtain the logits .\nBecause  is in , its rank (i.e., number of linearly independent columns) is at most .\nThe rank of a matrix corresponds to the dimension of the image of the linear map it defines, i.e., the vector space comprising the set of possible outputs of the function.\nIn other words, if the linear map  is defined as ,\nthen \u2019s image  is a -dimensional subspace of .\nThus, the LLM\u2019s logits will always lie on the -dimensional111\nMore accurately, the logits will always lie on an at-most--dimensional subspace. For convenience, we assume full-rank matrices, and thus a -dimensional subspace.\n subspace of .\n\n###figure_3### ###figure_4### ###figure_5### ###figure_6### We now turn our attention to the LLM\u2019s final output: the next-token distribution .\nDue to the softmax function, this is a valid probability distribution over  items,\nmeaning it is a -tuple of real numbers between  and  whose sum is .\nThe set of valid probability distributions over  items\nis referred to as the -simplex, or .\nPerhaps surprisingly,  is also a valid vector space\n(albeit under a non-standard definition of addition and scalar multiplication)\nand is isomorphic to a -dimensional vector subspace of  (Aitchison, 1982  ###reference_b1###; Leinster, 2016  ###reference_b12###).\nIn particular, it is isomorphic to the hyperplane  that is perpendicular to the all-ones vector , as illustrated in Figure 3  ###reference_###.\nThe softmax function is thus an isomorphism \nwhose inverse mapping  is the center log ratio transform (clr),\nwhich is defined as\nObserve also that the function\n\nprojects  linearly onto the nearest point in .\nBy this linearity,\nand the fact that  is a -dimensional subspace of ,\nwe can observe that  and  are also a -dimensional vector subspaces of  and  respectively.\nInterpreted, this means that the LLM\u2019s outputs\noccupy -dimensional subspaces of the logit space , probability space , and .\nWe call these subspaces the image of the LLM on each given space.\nA natural consequence of the low-dimensional output space is that any collection of  linearly independent LLM outputs  form a basis for the image of the model,\ni.e., all LLM outputs can be expressed as a unique linear combination of these  outputs (Yang et al., 2018  ###reference_b20###; Finlayson et al., 2023  ###reference_b5###).\nThe rest of this paper discusses the implications and applications of this mathematical fact for API-protected LLMs,\nstarting with methods for finding the LLM image given a restrictive API,\nthen using the LLM image for various purposes."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Fast, full outputs from API-protected LLMs",
            "text": "There are several uses for full-vocab LLM outputs,\nhowever most LLM APIs do not return full outputs.\nThis is likely because full outputs are large and expensive to send over an API,\nbut could also be to prevent abuse of APIs,\nsuch as using these outputs to distill models (Hinton et al., 2015  ###reference_b8###; Mukherjee and Awadallah, 2019  ###reference_b15###; Hsieh et al., 2023  ###reference_b9###)\nor discover proprietary information (see Section 4  ###reference_###).\nIn their paper, Morris et al. (2023  ###reference_b14###) give an algorithm\nfor recovering these full outputs from restricted APIs\nby taking advantage of a common API option that allows users to add bias to the logits for specific tokens.\nThe algorithm they describe requires  calls to the API top obtain one full output with precision .\nWe give an improvement that theoretically obtains full outputs in  API calls\nfor APIs that return the log-probability of the top- tokens.\nWe find that this improved algorithm suffers from numerical instability,\nand give a numerially stable algorithm that obtains full outputs in  API calls.\nNext, we give a practical algorithm for dealing with stochastic APIs\nthat randomly choose outputs from a set of  possible outputs.\nThis algorithm allows the collection of full outputs in less than  API calls on average.\nFinally, we reduce the number of API calls in all of the above algorithms from  to \nby adding in a preprocessing step to find the low-dimensional image of the LLM.\nThis speedup makes obtaining full LLM outputs up to 100 times faster and cheaper, depending on the LLM.\nTable 2  ###reference_### gives an overview of our algorithms with back-of-the envelope cost estimates for a specific LLM."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Fast, full outputs from APIs with logprobs",
            "text": "Our goal is to recover a full-vocabulary next-token distribution  from an API-protected LLM.\nWe will assume that the API accepts a prompt on which to condition the distribution,\nas well as a list of up to  tokens and a bias term  to add to the logits of the listed tokens before applying the softmax function.\nThe API returns a record with the  most likely tokens and their probabilities from the biased distribution.\nFor instance, querying the API with  maximally biased tokens,\nwhich without loss of generality we will identify as tokens ,\nyields the top- most probable tokens from the biased distribution \nwhere\nand  is the LLM\u2019s logit output for the given prompt.\nAssuming that the logit difference between any two tokens is never greater than ,\nthese top- biased probabilities will be .\nFor each of these biased probabilites , we can solve for the unbiased probability as\n(proof in the Appendix A.1  ###reference_###).\nThus, for each API call, we can obtain the unbiased probability of  tokens,\nand obtain the full distribution in  API calls."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Numerically stable full outputs from APIs",
            "text": "In practice, the algorithm described in Section 3.1  ###reference_### suffers from severe numerical instability,\nwhich can be attributed to the fast-growing exponential term ,\nand the term  which quickly approaches 1.\nWe can eliminate the instability by sacrificing some speed and using a different strategy to solve for the unbiased probabilities.\nWithout loss of generality, let  be the maximum unbiased token probability.\nThis can be obtained by querying the API once with no bias.\nIf we then query the API and apply maximum bias to only tokens ,\nthen the API will yield  and .\nWe can then solve for the unbiased probabilities of the  tokens\n(proof in Appendix A.2  ###reference_###).\nBy finding  unbiased token probabilities with every API call,\nwe obtain the full output in  calls total."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Full outputs from stochastic APIs",
            "text": "Each of the above algorithms assume that the API is deterministic,\ni.e., the same query will always return the same output.\nThis is not always the case.\nFor instance, we find that OpenAI\u2019s LLM APIs are stochastic.\nWhile this would seem to doom any attempt at obtaining full outputs from the LLM,\nwe find that certain types of stochasticity can be dealt with.\nIn particular, we model OpenAI\u2019s stochastic behavior as a collection of  outputs \nfrom which the API randomly returns from.\nThis might be the result of multiple instances of the LLM being run on different hardware which results in slightly different outputs.\nWhichever instance the API returns from determines which of the  outputs we get.\nIn order to determine which of the outputs the API returned from,\nassume without loss of generality that  is the second highest token probability for all ,\nand observe that\nfor all outputs  and unbiased outputs  where tokens  and  are not biased (proof in Appendix A.3  ###reference_###).\nTherefore, by biasing only  tokens for each call,\nthe API will return  as well as  and  for some ,\nand we can determine which output  the result comes from by using  as an identifier.\nThus, after an average of  calls to the API we can collect the full set of probabilities for one of the outputs."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Ultra-fast full outputs using the LLM image",
            "text": "So far, the dominating factor in each algorithm\u2019s runtime is .\nWe now introduce a preprocessing step that takes advantage of the low-dimensional LLM output space to obtain  versions of all the above algorithms.\nSince  for many modern language models, this modification can result in multiple orders of magnitude speedups, depending on the LLM.\nFor instance, the speedup for an LLM like pythia-70m would be .\nThe key to this algorithm is the observation from Section 2  ###reference_### that  linearly independent outputs from the API\nconstitute a basis for the whole output space (since the LLM\u2019s image has dimension ).\nWe can therefore collect these outputs\nas a preprocessing step in  API calls using any of the above algorithms and  unique prompts,\nand then use these to reconstruct the full LLM output after only  queries for each subsequent output.\nTo get a new full output , use any of the above algorithms to obtain .\nNext, we will use the additive log ratio () transform,\nwhich an isomorphism  and is defined as\nto transform the columns of  and  into vectors in ,\nthough since we only know the first  values of ,\nwe can only obtain the first  values of .\nBecause the alr transform is an isomorphism, we have that the columns of\nform a basis for a -dimensional vector subspace of ,\nand  lies within this subspace.\nTherefore, there is some  such that .\nTo solve for ,\nall that is required is to find the unique solution to the first  rows of this system of linear equations\nAfter finding ,\nwe can reconstruct the full LLM output \nwhere the inverse alr function is defined as\nThus we can retrieve  in only  API queries.\nThis  speedup makes any method that relies on full model outputs significantly cheaper. This includes model stealing (Tram\u00e8r et al., 2016  ###reference_b18###) which attempts to learn a model that exactly replicates the behavior of a target model."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Discovering the embedding size of API-protected LLMs",
            "text": "Without knowing the exact architectural details, other than assuming the generic output layer described in Figure 2, it is possible to infer characteristics of an API-protected language model from its outputs alone. This is due to the fact that the model outputs occupy a -dimensional subspace of , and therefore collecting linearly independent outputs from the LLM will form a basis for the LLM\u2019s image, meaning that all subsequent model outputs will be a linear combination of the first outputs. Assuming that model outputs are full rank, we can discover the dimensionality by collecting outputs one at a time until the number of linearly independent outputs in the collection stops increasing. In practice, we find that this full-rank assumption tends to hold well enough when over-collecting outputs.\n\nTo determine the dimension of a space spanned by model outputs, we analyze the singular values of a matrix of these outputs. A matrix with linearly independent columns will have non-zero singular values. We can therefore plot the singular values of the matrix and observe the index at which the magnitude of the singular values drops to zero. We will loosely refer to this matrix as the logit matrix, though these are technically the logits projected onto the nearest point. In practice, due to precision issues, the magnitudes may not drop all the way to zero.\n\nTo validate our method, we collect next-token distributions (conditioned on unique, 1-token prompts) from several open-source LLMs from the Pythia family (Biderman et al., 2023). We use our stochastically robust algorithm to collect nearly 6,000 next-token distribution outputs from gpt-3.5-turbo, a popular API-protected LLM whose architectural features are not publicly disclosed.\n\nWe find that the singular values for these outputs provide insights into the model's architecture. This predicted dimensionality is somewhat abnormal, as LLM characteristics are traditionally set to powers of two (or sums of powers of two). If this were the case for gpt-3.5-turbo, it would be reasonable to guess a typical configuration. We predict that any abnormal outputs due to errors (whether in our own code or OpenAI\u2019s) could affect observations.\n\nExtrapolating further and assuming that gpt-3.5-turbo has a similar architecture to most transformer-based LLMs, it is likely that the number of parameters in gpt-3.5-turbo is around 7 billion. This estimation is based on the fact that most known transformer-based LLMs with similar configurations have around this number of parameters. Any other parameter count would result in either abnormally narrow or wide neural networks which are not known to have favorable performance.\n\nAn important exception to this pattern would be the increasingly popular \u201cmixture-of-experts\u201d architecture, which tends to have many more parameters per dimension. Previous estimates of gpt-3.5-turbo\u2019s parameter count have generally exceeded 7B (Singh et al., 2023), although with updates and decreasing cost of inference, it is possible that its size and architecture have changed over time. Our method can be used to monitor these updates over time, alerting end-users when LLM providers make architectural changes to their model, specifically updates to the parameter count."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Identifying LLMs from a single output",
            "text": "###figure_8### The image of two different LLMs, even different checkpoints from the same LLM,\nare largely disjoint.\nAs shown in Figure 5  ###reference_###,\nthe logit output from a late-training checkpoint of Pythia-70M lies uniquely in the image of the checkpoint,\nand not in the image of the preceding or following checkpoints.\nWe also verify that the output does not lie in the image of any checkpoints from a similar LLM trained on deduplicated data,\nnor any checkpoint from a larger LLM trained on the same data.\nThis suggests that the image of a LLM is highly sensitive to small parameter changes.\nIntuitively this makes sense, since the output of one model will only be in the image of another model in the extremely unlikely event that it happens to lie on the low-dimensional () intersection of the models\u2019 images.\nMathematically the dimension of the models\u2019 images\u2019 intersection is small since the intersection \nof two subspaces  and  that are not subsets of one another\nhas dimension ,\nwhich implies that \n(since ).\nThus, it is possible to determine precisely which LLM produced a particular output,\nusing only API access to a set of LLMs and without knowing the exact inputs to the model.\nIn this way, the model\u2019s image acts as a signature for the model, i.e., a unique model identifier.\nThis finding has implications for LLM provider accountability.\nFor instance, if provider  suspects provider  of stealing their proprietary model,\nprovider  could check provider \u2019s model signature to see if it matches their own.\nIt would be extremely unlikely that the signatures match if the LLMs are not the same.\nSimilarly, if a provider claimed to be serving a proprietary model while in actuality attempting to profit off of an open-source model with a non-commercial license,\nan auditor again could discover this by checking the LLM signature.\nThis test is somewhat one-sided, however, since a clever provider may do a small amount of additional fine-tuning to change their stolen LLM\u2019s image."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Detecting and disambiguating LLM updates",
            "text": "Another practical application of highly sensitive LLM images\nis to detect and distinguish minor and major model updates to API-protected LLMs.\nUsing the above test, we can tell whether or not the LLM image remains the same, even if the logit outputs change.\nThis phenomenon would correspond to a partial model update,\nwhere some part of the model changes,\nbut the softmax matrix remains the same.\nAn example of this would be if the LLM has a hidden prefix added to all prompts and this hidden prefix changes: the model\u2019s outputs will change but the image will remain the same.\nTable 3  ###reference_### gives an overview for how to interpret various combinations of detectable API changes,\nThis information is useful for monitoring unannounced changes in API LLMs.\nInterpretation\nNo update\nHidden prompt change or partial finetune with frozen output layer\nLoRA update\nFull finetune"
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Detecting LoRA updates",
            "text": "We speculate that it is possible to gain even higher granularity information on LLM updates of certain types, such as LoRA (Hu et al., 2022  ###reference_b10###) updates.\nLoRA is a popular parameter-efficient fine-tuning method which adjusts model weights with a low-rank update \nwhere  and \nso that the softmax matrix  becomes .\nWe speculate that it is possible to detect these types of updates\nby collecting LLM outputs before () and after () the update\nand decomposing them as  and \nwhere \nsuch that .\nIf such a decomposition is found,\nthen it appears likely that the weights received a low-rank update of rank .\nWe leave it to future work to determine whether finding such a decomposition is sufficient to conclude that a LoRA update took place,\nas well as to find an efficient algorithm for finding such a decomposition."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "More applications",
            "text": "Access to the LLM\u2019s image can lead to many other capabilities, some of which we discuss below.\nWe leave further investigation of these methods for future work."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Finding unargmaxable tokens",
            "text": "Due to the low-rank constraints on LLM outputs, it is possible that some tokens become unargmaxable (Demeter et al., 2020  ###reference_b4###; Grivas et al., 2022  ###reference_b7###),\ni.e., there is a token  such that the constraints disallow any output  with .\nThis happens when the LLM\u2019s embedding representation of  lies within the convex hull of the other tokens\u2019 embeddings.\nPreviously, finding unargmaxable tokens appeared to require full access to the softmax matrix .\nInterestingly, we find that it is possible to identify unargmaxable tokens using only the LLM\u2019s image,\nwhich our method is able to recover.\nThis technique allows API customers to find quirks in LLM behavior\nsuch as tokens that the model is unable to output (at least under greedy decoding)."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Recovering the softmax matrix from outputs",
            "text": "Since our method gives us access to the image of the output layer, and not the output layer parameters,\nwe investigate the how one might reconstruct the output later parameters, either exactly or approximately.\nWe hypothesize that LLM embeddings generally lie near the surface of a hypersphere in  with a small radius .\nWe see evidence of this in the fact that the Pythia LLM embedding norms are all small and roughly normally distributed, as shown in Figure 6  ###reference_###.\n\n###figure_9### Assuming this holds for any LLM,\nwe can attempt to recover  up to a rotation\nby simplifying the assumption into a constraint that all embeddings must have a magnitude of 1.\nThen, given a matrix  of model logits,\nwe can find  (up to a rotation) by finding a decomposition \nsuch that for all , .\nThis solution may also be approximated by finding the singular value decomposition  of ,\nthough it is likely that all rows of this  will have magnitude less than  and they are not guaranteed to be normally distributed."
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "Improved LLM inversion",
            "text": "Morris et al.  ###reference_b14###\u2019s (2023  ###reference_b14###) recent approach to recovering hidden prompts\n(i.e., an additional prefix added to LLM input, not shown to the user)\nuses full LLM API outputs to train an \u201cinversion\u201d model that generates the prefix conditioned on the LLM\u2019s full logprob vector .\nIn addition to our algorithm from Section 3.4  ###reference_### making this much cheaper to implement,\nwe also propose a methodological change to their procedure.\nIn particular, to deal with the size mismatch between the LLM\u2019s vocabulary size \nand the inversion model\u2019s embedding size ,\nMorris et al. (2023  ###reference_b14###) pad and reshape the vector into a -length sequence of embeddings in .\nThis transformation is somewhat unnatural, and requires that the inversion model only be conditioned on a single output.\nWe propose instead to take advantage of our knowledge of the LLM\u2019s image\nto obtain a linearly lossless representation of the LLM output in ,\nwhich is much closer to the inversion model size,\nthen use an MLP to project this representation onto  to feed into the inversion model.\nFormally, after obtaining  and  from the API, we use the unique solution  to  as the input to the inversion model.\nThis modification has an additional advantage: instead of conditioning on a single output ,\nthe LLM can be used to generate a short sequence of outputs \nwhich can be fed into the inversion model as  where .\nWe leave the implementation and evaluation of this proposed modification for future work.\nThis technique can be applied generally to any method that takes LLM outputs as input."
        },
        {
            "section_id": "6.4",
            "parent_section_id": "6",
            "section_name": "Basis-aware sampling",
            "text": "In a recent paper, Finlayson et al. (2023  ###reference_b5###) propose a decoding algorithm\nthat avoids type-I sampling errors\nby identifying tokens that must have non-zero true probability.\nImportantly, this method relies on knowing the basis of the LLM\u2019s output space,\nand is therefore only available for LLMs whose image is known.\nOur proposed approach for finding the image of API-protected LLMs makes this decoding algorithm possible for API LLMs."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Mitigations",
            "text": "We consider a three proposals that LLM providers may take to guard against our methods.\nThe first proposal is to remove API access to logprobs.\nThis, however is theoretically insufficient,\nas Morris et al. (2023  ###reference_b14###) show that it is possible to obtain full outputs using only information about the biased argmax token,\nalbeit inefficiently in  API calls.\nRegardless of the theoretical result, providers may rely on the extreme inefficiency of the algorithm to protect the LLM.\nThis appears to be the approach OpenAI took after learning about this vulnerability from Carlini et al. (2024  ###reference_b3###),\nby always returning the top- unbiased logprobs.\nOur new proposed algorithm, however, brings the number of queries down to a more feasible  API calls once the initial work of finding the LLM image has finished,\nweakening the argument that the expensiveness of the algorithm is sufficient to disregard our technique.\nThe next proposal is to remove API access to logit bias.\nThis would be an effective way to protect the LLM, since there are no known methods to recover full outputs from such an API.\nHowever, the logit bias interface is presumably useful for many clients who might be disappointed by its shutdown.\nLastly, we consider alternative LLM architectures that do not suffer from a softmax bottleneck.\nThere are several such proposed architectures with good performance. Though this is the most expensive of the proposed defenses, due to the requirement of training a new LLM,\nit would have the beneficial side effect of also treating other tokenization issues that plague large-vocabulary LLMs (e.g., Itzhak and Levy, 2022  ###reference_b11###).\nA transition to softmax-bottleneck-free LLMs would fully prevent our attack, since the model\u2019s image would be the full output space."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "Overall, the potential applications of our methods can have great impact in building trust between the LLM API users and the providers; at the same time, none are particularly harmful to LLM providers.\nFor instance, consider discovering the embedding size of the LLM, which is often non-public information in proprietary LLMs.\nHowever, access to this hyperparameter does not enable a competitor to fully recover the parameters of the LLM\u2019s softmax matrix or boost performance of their own model; several other factors such as training data mix, human feedback and several other engineering choices are still critical and hidden.\nEven using model outputs to steal hidden prompts (see Section 6.3  ###reference_###) is unlikely to have detrimental effects,\nas hidden prompt leakage is a known vulnerability\nand best practice dictates that no private information should be contained in these system prompts (Greshake et al., 2023  ###reference_b6###; Liu et al., 2023  ###reference_b13###).\nWe find that the most dangerous consequence of our findings might simply be that model stealing methods that rely on full outputs get cheaper by a factor of , which in the case of gpt-3.5-turbo amounts to about .\nOn the other hand, allowing LLM API users to detect model changes builds trust between LLM providers and their customers,\nand leads to greater accountability and transparency for the providers.\nOur method can be used to implement efficient protocols for model auditing without exposing the model parameters or detailed configuration information, which may help model safety and privacy protection of personalized, proprietary models.\nWe therefore take the position that our proposed methods and findings do not necessitate a change in LLM API best practices,\nbut rather expand the tools available to API customers,\nwhile warning LLM providers of the information their APIs expose."
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "Simultaneous discovery",
            "text": "In a remarkable case of simultaneous discovery,\nCarlini et al. (2024  ###reference_b3###) propose a very similar approach to ours for gaining insight into API-protected LLMs.\nHere we review a some interesting interactions between our work and theirs.\nFirst, we give an algorithm for obtaining full outputs in  API calls, while their algorithm corresponds to our  algorithm.\nThis has little impact on our final result, since our  algorithm suffers from numerical instability, but it is an interesting theoretical result nonetheless.\nOn the flip side, Carlini et al.  ###reference_b3### propose an improved logprob-free method for getting full outputs from an API that takes advantage of parallel queries.\nThis useful method is actually complementary to our  algorithm, since they can be combined to yield an even better algorithm for obtaining full outputs.\nNext, our experiments with OpenAI\u2019s API were fraught with issues of stochasticity: the API would return different results for the same query, leading us to develop the stochastically robust full output algorithm.\nMeanwhile, our colleagues did not appear to encounter such issues, perhaps because they had access to a more stable API endpoint than ours.\nLastly, Carlini et al.  ###reference_b3### focus mostly on defenses and mitigations against attacks, while our own work focuses more on understanding the capabilities such attacks provide once an LLM image has been obtained.\nThese approaches complement each other, since our work provides additional motivation for why (or why not) such defenses should be implemented."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A Proofs",
            "text": "This appendix contains derivations for the equations used to solve for unbiased token probabilities given biased LLM outputs."
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S1.T1\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>An overview of our proposed applications that exploit LLM images.</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S1.T1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.1\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\" id=\"S1.T1.1.1.1.1\" style=\"width:346.9pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S1.T1.1.1.1.1.1\"><span class=\"ltx_text\" id=\"S1.T1.1.1.1.1.1.1\" style=\"font-size:90%;\">Application</span></p>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S1.T1.1.1.1.2\"><span class=\"ltx_text\" id=\"S1.T1.1.1.1.2.1\" style=\"font-size:90%;\">Section</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S1.T1.1.2.1\">\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S1.T1.1.2.1.1\" style=\"width:346.9pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S1.T1.1.2.1.1.1\"><span class=\"ltx_text\" id=\"S1.T1.1.2.1.1.1.1\" style=\"font-size:90%;\">Efficiently extract full LLM outputs.</span></p>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S1.T1.1.2.1.2\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.09539v2#S3\" style=\"font-size:90%;\" title=\"3 Fast, full outputs from API-protected LLMs \u2023 Logits of API-Protected LLMs Leak Proprietary Information\"><span class=\"ltx_text ltx_ref_tag\">3</span></a></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.3.2\">\n<td class=\"ltx_td ltx_align_justify\" id=\"S1.T1.1.3.2.1\" style=\"width:346.9pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S1.T1.1.3.2.1.1\"><span class=\"ltx_text\" id=\"S1.T1.1.3.2.1.1.1\" style=\"font-size:90%;\">Find the embedding size of an LLM and guess its parameter count.</span></p>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S1.T1.1.3.2.2\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.09539v2#S4\" style=\"font-size:90%;\" title=\"4 Discovering the embedding size of API-protected LLMs \u2023 Logits of API-Protected LLMs Leak Proprietary Information\"><span class=\"ltx_text ltx_ref_tag\">4</span></a></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.4.3\">\n<td class=\"ltx_td ltx_align_justify\" id=\"S1.T1.1.4.3.1\" style=\"width:346.9pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S1.T1.1.4.3.1.1\"><span class=\"ltx_text\" id=\"S1.T1.1.4.3.1.1.1\" style=\"font-size:90%;\">Identify which LLM produced a given output.</span></p>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S1.T1.1.4.3.2\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.09539v2#S5\" style=\"font-size:90%;\" title=\"5 Identifying LLMs from a single output \u2023 Logits of API-Protected LLMs Leak Proprietary Information\"><span class=\"ltx_text ltx_ref_tag\">5</span></a></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.5.4\">\n<td class=\"ltx_td ltx_align_justify\" id=\"S1.T1.1.5.4.1\" style=\"width:346.9pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S1.T1.1.5.4.1.1\"><span class=\"ltx_text\" id=\"S1.T1.1.5.4.1.1.1\" style=\"font-size:90%;\">Detect when and what type of LLM updates occur.</span></p>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S1.T1.1.5.4.2\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.09539v2#S5.SS1\" style=\"font-size:90%;\" title=\"5.1 Detecting and disambiguating LLM updates \u2023 5 Identifying LLMs from a single output \u2023 Logits of API-Protected LLMs Leak Proprietary Information\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.6.5\">\n<td class=\"ltx_td ltx_align_justify\" id=\"S1.T1.1.6.5.1\" style=\"width:346.9pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S1.T1.1.6.5.1.1\"><span class=\"ltx_text\" id=\"S1.T1.1.6.5.1.1.1\" style=\"font-size:90%;\">Find tokenization bugs (unargmaxable tokens).</span></p>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S1.T1.1.6.5.2\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.09539v2#S6.SS1\" style=\"font-size:90%;\" title=\"6.1 Finding unargmaxable tokens \u2023 6 More applications \u2023 Logits of API-Protected LLMs Leak Proprietary Information\"><span class=\"ltx_text ltx_ref_tag\">6.1</span></a></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.7.6\">\n<td class=\"ltx_td ltx_align_justify\" id=\"S1.T1.1.7.6.1\" style=\"width:346.9pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S1.T1.1.7.6.1.1\"><span class=\"ltx_text\" id=\"S1.T1.1.7.6.1.1.1\" style=\"font-size:90%;\">Approximately reconstruct the LLM\u2019s softmax matrix.</span></p>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S1.T1.1.7.6.2\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.09539v2#S6.SS2\" style=\"font-size:90%;\" title=\"6.2 Recovering the softmax matrix from outputs \u2023 6 More applications \u2023 Logits of API-Protected LLMs Leak Proprietary Information\"><span class=\"ltx_text ltx_ref_tag\">6.2</span></a></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.8.7\">\n<td class=\"ltx_td ltx_align_justify\" id=\"S1.T1.1.8.7.1\" style=\"width:346.9pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S1.T1.1.8.7.1.1\"><span class=\"ltx_text\" id=\"S1.T1.1.8.7.1.1.1\" style=\"font-size:90%;\">Cheaply and accurately reconstruct \u201chidden prompts\u201d.</span></p>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S1.T1.1.8.7.2\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.09539v2#S6.SS3\" style=\"font-size:90%;\" title=\"6.3 Improved LLM inversion \u2023 6 More applications \u2023 Logits of API-Protected LLMs Leak Proprietary Information\"><span class=\"ltx_text ltx_ref_tag\">6.3</span></a></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.9.8\">\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S1.T1.1.9.8.1\" style=\"width:346.9pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S1.T1.1.9.8.1.1\"><span class=\"ltx_text\" id=\"S1.T1.1.9.8.1.1.1\" style=\"font-size:90%;\">Implement basis-aware decoding algorithms.</span></p>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S1.T1.1.9.8.2\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.09539v2#S6.SS4\" style=\"font-size:90%;\" title=\"6.4 Basis-aware sampling \u2023 6 More applications \u2023 Logits of API-Protected LLMs Leak Proprietary Information\"><span class=\"ltx_text ltx_ref_tag\">6.4</span></a></td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 1: An overview of our proposed applications that exploit LLM images."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T2\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>\nA summary of our proposed algorithms, with estimates for the number of API calls required per output, and the price of acquiring the model image.\nEstimates are based on a <span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T2.21.1\">gpt-3.5-turbo</span>-like API LLM with , , , , , and\u00a0.\nNote that the  algorithm cannot be used to obtain the LLM image,\nsince it relies on having LLM image as a preprocessing step.\n</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T2.19\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T2.19.6.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S3.T2.19.6.1.1\"><span class=\"ltx_text\" id=\"S3.T2.19.6.1.1.1\" style=\"font-size:90%;\">Algorithm</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S3.T2.19.6.1.2\"><span class=\"ltx_text\" id=\"S3.T2.19.6.1.2.1\" style=\"font-size:90%;\">Complexity</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.19.6.1.3\"><span class=\"ltx_text\" id=\"S3.T2.19.6.1.3.1\" style=\"font-size:90%;\">API calls per output</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.19.6.1.4\"><span class=\"ltx_text\" id=\"S3.T2.19.6.1.4.1\" style=\"font-size:90%;\">Image price</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T2.15.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T2.15.1.2\"><span class=\"ltx_text\" id=\"S3.T2.15.1.2.1\" style=\"font-size:90%;\">Logprob-free</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T2.15.1.1\"></th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T2.15.1.3\"><span class=\"ltx_text\" id=\"S3.T2.15.1.3.1\" style=\"font-size:90%;\">800K</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T2.15.1.4\"><span class=\"ltx_text\" id=\"S3.T2.15.1.4.1\" style=\"font-size:90%;\">$16,384</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.16.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.16.2.2\"><span class=\"ltx_text\" id=\"S3.T2.16.2.2.1\" style=\"font-size:90%;\">With logprobs</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.16.2.1\"></th>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T2.16.2.3\"><span class=\"ltx_text\" id=\"S3.T2.16.2.3.1\" style=\"font-size:90%;\">20K</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T2.16.2.4\"><span class=\"ltx_text\" id=\"S3.T2.16.2.4.1\" style=\"font-size:90%;\">$410</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.17.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.17.3.2\"><span class=\"ltx_text\" id=\"S3.T2.17.3.2.1\" style=\"font-size:90%;\">Numerically stable</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.17.3.1\"></th>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T2.17.3.3\"><span class=\"ltx_text\" id=\"S3.T2.17.3.3.1\" style=\"font-size:90%;\">25K</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T2.17.3.4\"><span class=\"ltx_text\" id=\"S3.T2.17.3.4.1\" style=\"font-size:90%;\">$512</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.18.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.18.4.2\"><span class=\"ltx_text\" id=\"S3.T2.18.4.2.1\" style=\"font-size:90%;\">Stochastically robust</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.18.4.1\"></th>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T2.18.4.3\"><span class=\"ltx_text\" id=\"S3.T2.18.4.3.1\" style=\"font-size:90%;\">133K</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T2.18.4.4\"><span class=\"ltx_text\" id=\"S3.T2.18.4.4.1\" style=\"font-size:90%;\">$2,724</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.19.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S3.T2.19.5.2\"><span class=\"ltx_text\" id=\"S3.T2.19.5.2.1\" style=\"font-size:90%;\">LLM Image</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S3.T2.19.5.1\"></th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T2.19.5.3\"><span class=\"ltx_text\" id=\"S3.T2.19.5.3.1\" style=\"font-size:90%;\">1K\u201332K</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T2.19.5.4\"><span class=\"ltx_text\" id=\"S3.T2.19.5.4.1\" style=\"font-size:90%;\">-</span></td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 2: \nA summary of our proposed algorithms, with estimates for the number of API calls required per output, and the price of acquiring the model image.\nEstimates are based on a gpt-3.5-turbo-like API LLM with , , , , , and\u00a0.\nNote that the  algorithm cannot be used to obtain the LLM image,\nsince it relies on having LLM image as a preprocessing step.\n"
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T3\">\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Implications of image/logit changes.</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S5.T3.3\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T3.3.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" id=\"S5.T3.3.1.1.1\"><span class=\"ltx_text\" id=\"S5.T3.3.1.1.1.1\" style=\"font-size:90%;\">Change</span></th>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" id=\"S5.T3.3.1.1.2\" style=\"width:216.8pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T3.3.1.1.2.1\"><span class=\"ltx_text\" id=\"S5.T3.3.1.1.2.1.1\" style=\"font-size:90%;\">Interpretation</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.3.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T3.3.2.2.1\"><span class=\"ltx_text\" id=\"S5.T3.3.2.2.1.1\" style=\"font-size:90%;\">No logit change, no image change</span></th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T3.3.2.2.2\" style=\"width:216.8pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T3.3.2.2.2.1\"><span class=\"ltx_text\" id=\"S5.T3.3.2.2.2.1.1\" style=\"font-size:90%;\">No update</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.3.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T3.3.3.3.1\"><span class=\"ltx_text\" id=\"S5.T3.3.3.3.1.1\" style=\"font-size:90%;\">Logit change, no image change</span></th>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T3.3.3.3.2\" style=\"width:216.8pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T3.3.3.3.2.1\"><span class=\"ltx_text\" id=\"S5.T3.3.3.3.2.1.1\" style=\"font-size:90%;\">Hidden prompt change or partial finetune with frozen output layer</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.3.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T3.3.4.4.1\">\n<span class=\"ltx_text\" id=\"S5.T3.3.4.4.1.1\" style=\"font-size:90%;\">Low-rank image change (See \u00a7\u00a0</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.09539v2#S5.SS2\" style=\"font-size:90%;\" title=\"5.2 Detecting LoRA updates \u2023 5 Identifying LLMs from a single output \u2023 Logits of API-Protected LLMs Leak Proprietary Information\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a><span class=\"ltx_text\" id=\"S5.T3.3.4.4.1.2\" style=\"font-size:90%;\">)</span>\n</th>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T3.3.4.4.2\" style=\"width:216.8pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T3.3.4.4.2.1\"><span class=\"ltx_text\" id=\"S5.T3.3.4.4.2.1.1\" style=\"font-size:90%;\">LoRA update</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.3.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S5.T3.3.5.5.1\"><span class=\"ltx_text\" id=\"S5.T3.3.5.5.1.1\" style=\"font-size:90%;\">Image change</span></th>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S5.T3.3.5.5.2\" style=\"width:216.8pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T3.3.5.5.2.1\"><span class=\"ltx_text\" id=\"S5.T3.3.5.5.2.1.1\" style=\"font-size:90%;\">Full finetune</span></p>\n</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 3: Implications of image/logit changes."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.09539v2_figure_1.png",
            "caption": "Figure 1: \nLLM outputs are constrained to a low-dimensional subspace of the full output space.\nWe can use this fact to glean information about API-protected LLMs by analyzing their outputs.\nHere we show how a toy LLM\u2019s low-dimensional embeddings in \u211ddsuperscript\u211d\ud835\udc51\\mathbb{R}^{d}blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT\n(illustrated here as a 1-D space)\nare transformed linearly into logits in \u211ddsuperscript\u211d\ud835\udc51\\mathbb{R}^{d}blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT (here, a 3D space) via the softmax matrix \ud835\udc7e\ud835\udc7e\\boldsymbol{W}bold_italic_W.\nThe resulting outputs lie within a (d=1\ud835\udc511d=1italic_d = 1)-dimensional subspace of the output space.\nThe softmax function also preserves this low-dimensionality when transforming the logits into probabilities distributions on \u0394vsubscript\u0394\ud835\udc63\\Delta_{v}roman_\u0394 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT.\nWe call this low-dimensional subspace the image of the model.\nWe can obtain a basis for the image of an API-protected LLM by collecting d\ud835\udc51ditalic_d of its outputs.\nThe LLM\u2019s image can reveal non-public information, such as the LLM\u2019s embedding size,\nbut it can also be used for accountability,\nsuch as verifying which LLM an API is serving."
        },
        "2": {
            "figure_path": "2403.09539v2_figure_2.png",
            "caption": "Figure 2: \nA typical language model architecture.\nAfter the input its processed by a neural network, usually a transformer (Vaswani et al., 2017), into a low-dimensional embedding \ud835\udc89\ud835\udc89\\boldsymbol{h}bold_italic_h,\nit is multiplied by the softmax matrix \ud835\udc7e\ud835\udc7e\\boldsymbol{W}bold_italic_W,\nprojecting it linearly from \u211ddsuperscript\u211d\ud835\udc51\\mathbb{R}^{d}blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT onto \u211dvsuperscript\u211d\ud835\udc63\\mathbb{R}^{v}blackboard_R start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT\nto obtain the logit vector \u2113bold-\u2113\\boldsymbol{\\ell}bold_\u2113.\nThe softmax function is then applied to the logit vector\nto obtain a valid probability distribution \ud835\udc91\ud835\udc91\\boldsymbol{p}bold_italic_p\nover next-token candidates."
        },
        "3": {
            "figure_path": "2403.09539v2_figure_3.png",
            "caption": "Figure 3: \nPoints in the logit space \u211dvsuperscript\u211d\ud835\udc63\\mathbb{R}^{v}blackboard_R start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT (far left) are mapped via the softmax function to points (probability distributions) on the simplex \u0394vsubscript\u0394\ud835\udc63\\Delta_{v}roman_\u0394 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT (middle left).\nCrucially, the softmax maps all points that lie on the same diagonal (shown as points of the same color) to the same probability distribution.\nFor numerical stability, these values are often stored as log-probabilities (middle right).\nThe clr transform returns probability distributions to points to a subspace Uvsubscript\ud835\udc48\ud835\udc63U_{v}italic_U start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT of the logit space (far right).\nThe softmax function and clr transform are inverses of one another, and form an isomorphism between Uvsubscript\ud835\udc48\ud835\udc63U_{v}italic_U start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT and \u0394vsubscript\u0394\ud835\udc63\\Delta_{v}roman_\u0394 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT."
        },
        "4": {
            "figure_path": "2403.09539v2_figure_4.png",
            "caption": "Figure 3: \nPoints in the logit space \u211dvsuperscript\u211d\ud835\udc63\\mathbb{R}^{v}blackboard_R start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT (far left) are mapped via the softmax function to points (probability distributions) on the simplex \u0394vsubscript\u0394\ud835\udc63\\Delta_{v}roman_\u0394 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT (middle left).\nCrucially, the softmax maps all points that lie on the same diagonal (shown as points of the same color) to the same probability distribution.\nFor numerical stability, these values are often stored as log-probabilities (middle right).\nThe clr transform returns probability distributions to points to a subspace Uvsubscript\ud835\udc48\ud835\udc63U_{v}italic_U start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT of the logit space (far right).\nThe softmax function and clr transform are inverses of one another, and form an isomorphism between Uvsubscript\ud835\udc48\ud835\udc63U_{v}italic_U start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT and \u0394vsubscript\u0394\ud835\udc63\\Delta_{v}roman_\u0394 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT."
        },
        "5": {
            "figure_path": "2403.09539v2_figure_5.png",
            "caption": "Figure 3: \nPoints in the logit space \u211dvsuperscript\u211d\ud835\udc63\\mathbb{R}^{v}blackboard_R start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT (far left) are mapped via the softmax function to points (probability distributions) on the simplex \u0394vsubscript\u0394\ud835\udc63\\Delta_{v}roman_\u0394 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT (middle left).\nCrucially, the softmax maps all points that lie on the same diagonal (shown as points of the same color) to the same probability distribution.\nFor numerical stability, these values are often stored as log-probabilities (middle right).\nThe clr transform returns probability distributions to points to a subspace Uvsubscript\ud835\udc48\ud835\udc63U_{v}italic_U start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT of the logit space (far right).\nThe softmax function and clr transform are inverses of one another, and form an isomorphism between Uvsubscript\ud835\udc48\ud835\udc63U_{v}italic_U start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT and \u0394vsubscript\u0394\ud835\udc63\\Delta_{v}roman_\u0394 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT."
        },
        "6": {
            "figure_path": "2403.09539v2_figure_6.png",
            "caption": "Figure 3: \nPoints in the logit space \u211dvsuperscript\u211d\ud835\udc63\\mathbb{R}^{v}blackboard_R start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT (far left) are mapped via the softmax function to points (probability distributions) on the simplex \u0394vsubscript\u0394\ud835\udc63\\Delta_{v}roman_\u0394 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT (middle left).\nCrucially, the softmax maps all points that lie on the same diagonal (shown as points of the same color) to the same probability distribution.\nFor numerical stability, these values are often stored as log-probabilities (middle right).\nThe clr transform returns probability distributions to points to a subspace Uvsubscript\ud835\udc48\ud835\udc63U_{v}italic_U start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT of the logit space (far right).\nThe softmax function and clr transform are inverses of one another, and form an isomorphism between Uvsubscript\ud835\udc48\ud835\udc63U_{v}italic_U start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT and \u0394vsubscript\u0394\ud835\udc63\\Delta_{v}roman_\u0394 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT."
        },
        "7": {
            "figure_path": "2403.09539v2_figure_7.png",
            "caption": "Figure 4: \nThe singular values of outputs from LLMs with various known and unknown embedding sizes d\ud835\udc51ditalic_d.\nFor each model with known embedding size,\nthere is a clear drop in magnitude at singular value index d\ud835\udc51ditalic_d,\nindicating the embedding size of the model.\nUsing this observation, we can guess the embedding size of gpt-3.5-turbo."
        },
        "8": {
            "figure_path": "2403.09539v2_figure_8.png",
            "caption": "Figure 5: \nResiduals of the least-squares solution of\n\ud835\udc7e\u2062\ud835\udc99=\u2113\ud835\udc7e\ud835\udc99bold-\u2113\\boldsymbol{W}\\boldsymbol{x}=\\boldsymbol{\\ell}bold_italic_W bold_italic_x = bold_\u2113\nfor an output \u2113bold-\u2113\\boldsymbol{\\ell}bold_\u2113 from a pythia-70m checkpoint (training step 120K),\nand softmax matrices \ud835\udc7e\ud835\udc7e\\boldsymbol{W}bold_italic_W at various training steps for pythia-70m, and pythia-70m-deduped, and pythia-160m.\nLow/high residual values indicate that the output \u2113bold-\u2113\\boldsymbol{\\ell}bold_\u2113 is/is not in a model\u2019s image.\nResiduals for pythia-70m decrease as the checkpoints near the checkpoint that generated the output,\nbut remain high.\nNote that this test works even if the softmax matrices \ud835\udc7e\ud835\udc7e\\boldsymbol{W}bold_italic_W are substituted with model outputs \ud835\udc73\ud835\udc73\\boldsymbol{L}bold_italic_L."
        },
        "9": {
            "figure_path": "2403.09539v2_figure_9.png",
            "caption": "Figure 6: Softmax matrix row magnitudes (here from pythia-70m) are small and are distributed approximately normally within a narrow range."
        }
    },
    "references": [
        {
            "1": {
                "title": "The statistical analysis of compositional data.",
                "author": "J. Aitchison.",
                "venue": "Journal of the Royal Statistical Society: Series B\n(Methodological), 44(2):139\u2013160, 1982.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Pythia: A suite for analyzing large language models across training\nand scaling.",
                "author": "Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley,\nKyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit,\nUSVSN Sai Prashanth, Edward Raff, et al.",
                "venue": "In International Conference on Machine Learning, pages\n2397\u20132430. PMLR, 2023.",
                "url": null
            }
        },
        {
            "3": {
                "title": "Stealing part of a production language model, 2024.",
                "author": "Nicholas Carlini, Daniel Paleka, Krishnamurthy Dj Dvijotham, Thomas Steinke,\nJonathan Hayase, A. Feder Cooper, Katherine Lee, Matthew Jagielski, Milad\nNasr, Arthur Conmy, Eric Wallace, David Rolnick, and Florian Tram\u00e8r.",
                "venue": null,
                "url": null
            }
        },
        {
            "4": {
                "title": "Stolen probability: A structural weakness of neural language models.",
                "author": "David Demeter, Gregory Kimmel, and Doug Downey.",
                "venue": "In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault,\neditors, Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 2191\u20132197, Online, July 2020. Association\nfor Computational Linguistics.",
                "url": null
            }
        },
        {
            "5": {
                "title": "Closing the curious case of neural text degeneration.",
                "author": "Matthew Finlayson, John Hewitt, Alexander Koller, Swabha Swayamdipta, and\nAshish Sabharwal.",
                "venue": "ArXiv, abs/2310.01693, 2023.",
                "url": null
            }
        },
        {
            "6": {
                "title": "Not what you\u2019ve signed up for: Compromising real-world llm-integrated\napplications with indirect prompt injection.",
                "author": "Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten\nHolz, and Mario Fritz.",
                "venue": "Proceedings of the 16th ACM Workshop on Artificial Intelligence\nand Security, 2023.",
                "url": null
            }
        },
        {
            "7": {
                "title": "Low-rank softmax can have unargmaxable classes in theory but rarely\nin practice.",
                "author": "Andreas Grivas, Nikolay Bogoychev, and Adam Lopez.",
                "venue": "In Proceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pages 6738\u20136758,\nDublin, Ireland, May 2022. Association for Computational Linguistics.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Distilling the knowledge in a neural network.",
                "author": "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.",
                "venue": "In Proc. of NeurIPS, 2015.",
                "url": null
            }
        },
        {
            "9": {
                "title": "Distilling step-by-step! outperforming larger language models with\nless training data and smaller model sizes.",
                "author": "Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii,\nAlexander J. Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister.",
                "venue": "ArXiv, abs/2305.02301, 2023.",
                "url": null
            }
        },
        {
            "10": {
                "title": "LoRA: Low-rank adaptation of large language models.",
                "author": "Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\nWang, Lu Wang, and Weizhu Chen.",
                "venue": "In International Conference on Learning Representations, 2022.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Models in a spelling bee: Language models implicitly learn the\ncharacter composition of tokens.",
                "author": "Itay Itzhak and Omer Levy.",
                "venue": "In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir\nMeza Ruiz, editors, Proceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, pages 5061\u20135068, Seattle, United States, July 2022.\nAssociation for Computational Linguistics.",
                "url": null
            }
        },
        {
            "12": {
                "title": "How the simplex is a vector space.",
                "author": "Tom Leinster.",
                "venue": "https://golem.ph.utexas.edu/category/2016/06/how_the_simplex_is_a_vector_sp.html,\n2016.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Prompt injection attack against llm-integrated applications.",
                "author": "Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Tianwei Zhang, Yepang Liu, Haoyu\nWang, Yanhong Zheng, and Yang Liu.",
                "venue": "ArXiv, abs/2306.05499, 2023.",
                "url": null
            }
        },
        {
            "14": {
                "title": "Language model inversion.",
                "author": "John X. Morris, Wenting Zhao, Justin T Chiu, Vitaly Shmatikov, and Alexander M.\nRush.",
                "venue": "ArXiv, abs/2311.13647, 2023.",
                "url": null
            }
        },
        {
            "15": {
                "title": "Distilling transformers into simple neural networks with unlabeled\ntransfer data.",
                "author": "Subhabrata Mukherjee and Ahmed Hassan Awadallah.",
                "venue": "ArXiv, abs/1910.01769, 2019.",
                "url": null
            }
        },
        {
            "16": {
                "title": "Gpt-4 technical report, 2024.",
                "author": "OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,\nFlorencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, et al.",
                "venue": null,
                "url": null
            }
        },
        {
            "17": {
                "title": "Codefusion: A pre-trained diffusion model for code generation, 2023.",
                "author": "Mukul Singh, Jos\u00e9 Cambronero, Sumit Gulwani, Vu Le, Carina Negreanu, and Gust\nVerbruggen.",
                "venue": null,
                "url": null
            }
        },
        {
            "18": {
                "title": "Stealing machine learning models via prediction apis.",
                "author": "Florian Tram\u00e8r, Fan Zhang, Ari Juels, Michael K. Reiter, and Thomas\nRistenpart.",
                "venue": "In USENIX Security Symposium, 2016.",
                "url": null
            }
        },
        {
            "19": {
                "title": "Attention is all you need.",
                "author": "Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin.",
                "venue": "In Neural Information Processing Systems, 2017.",
                "url": null
            }
        },
        {
            "20": {
                "title": "Breaking the softmax bottleneck: A high-rank RNN language model.",
                "author": "Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W. Cohen.",
                "venue": "In ICLR, 2018.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.09539v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.4"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "5",
            "9"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "5.1",
            "5.2"
        ]
    },
    "research_context": {
        "paper_id": "2403.09539v2",
        "paper_title": "Logits of API-Protected LLMs Leak Proprietary Information",
        "research_background": "### Motivation:\nThe paper is motivated by the growing use of closed-source large language models (LLMs) provided solely through APIs, a trend driven by the increasing capabilities and values of these models. Providers, such as OpenAI, operate under the assumption that this API-based approach secures information about their model architecture, making certain attacks unfeasible. However, this setup also leaves users dependent on providers for updates and changes, potentially fostering mistrust.\n\n### Research Problem:\nThe primary research problem tackled in this paper is the hidden information leakage from LLMs accessed through APIs. Despite the general belief that these APIs keep model details private, the paper demonstrates that detailed proprietary information about LLM parameterization can be extracted using common API configurations. The research aims to uncover the extent of this leakage and explore its implications, both as a vulnerability and a potential feature for accountability and trust between LLM providers and users.\n\n### Relevant Prior Work:\nThe paper references prior work in which companies like OpenAI have implemented closed-source LLMs accessible only via APIs (OpenAI et al., 2024). This indicates a foundational movement toward protecting model architectures while offering high utility through API interfaces. However, the specifics of how much information can be inadvertently revealed through these APIs, and the potential consequences thereof, have not been extensively explored prior to this study. The paper builds on this understanding by not only identifying this leakage but also explaining its mechanisms and suggesting practical repercussions for accountability and mitigations.",
        "methodology": "The proposed methodology focuses on extracting full-vocab outputs from Large Language Model (LLM) API-protected systems, which generally do not return complete outputs due to the high cost and potential for misuse. Here's a detailed breakdown of the key components and innovations of the proposed method:\n\n1. **Original Algorithm by Morris et al. (2023)**:\n   - **Base Concept**: Morris et al. suggest leveraging an API feature that allows users to bias the logits for specific tokens, effectively manipulating the responses to recover full outputs.\n   - **Performance**: This method requires \\( O(V) \\) API calls to obtain one complete output with a certain precision, where \\( V \\) is the vocabulary size.\n\n2. **Improved Algorithm**:\n   - **Optimization**: The proposed enhancement theoretically reduces the number of required API calls to \\( O(T \\log V) \\) for APIs that provide the log-probabilities of the top- \\( T \\) tokens.\n   - **Challenge**: Despite the reduction in API calls, this improvement faces numerical instability issues.\n\n3. **Numerically Stable Algorithm**:\n   - **Stabilization**: To address the numerical instability, the authors introduce a stable algorithm that still achieves full output recovery in \\( O(T \\log V) \\) API calls.\n\n4. **Algorithm for Stochastic APIs**:\n   - **Handling Stochasticity**: For APIs that randomly select outputs from a possible set of \\( S \\) outputs, a practical algorithm is presented.\n   - **Efficiency**: This method can collect full outputs in fewer than \\( O(S \\log V) \\) API calls on average.\n\n5. **Preprocessing Step**:\n   - **Dimensionality Reduction**: A preprocessing step is added to identify the low-dimensional image of the LLM, significantly reducing the API call requirement.\n   - **Enhanced Performance**: This step lowers the requirement from \\( O(T \\log V) \\) to \\( O(T \\log d) \\) API calls, where \\( d \\) is the reduced dimension.\n   - **Cost and Speed Improvements**: This preprocessing step makes the retrieval process up to 100 times faster and cheaper, depending on the specific LLM used.\n\nOverall, the presented methodology innovates by enhancing the efficiency of extracting full-vocab outputs from protected LLM APIs through theoretical improvements, addressing numerical stability, offering practical solutions for stochastic APIs, and implementing a preprocessing step for significant speedup and cost reduction.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n1. **Objective**:\n   The primary aim of the experiment is to infer the embedding size of an API-protected language model (LLM) from its outputs alone. \n\n2. **Assumptions**:\n   - The model outputs a distribution that occupies a -dimensional subspace.\n   - By collecting \\( d+1 \\) or more linearly independent outputs, a basis for the model's output space can be formed.\n   - The output matrix will have \\( d \\) non-zero singular values if \\( d \\) outputs are linearly independent.\n\n3. **Method**:\n   - Collect next-token distributions from the LLM, conditioned on unique 1-token prompts.\n   - Calculate the singular values of the resulting output matrix.\n   - Observe the index where the magnitude of singular values drops to zero, indicating the embedding size.\n\n4. **Datasets**:\n   - Next-token distributions (outputs) from various open-source LLMs in the Pythia family.\n   - Nearly 6,000 next-token distribution outputs from GPT-3.5-turbo (API-protected model).\n\n5. **Procedure**:\n   - For Pythia family models: Collect outputs and calculate singular values, observing the index drop which corresponds to the known embedding sizes (512 to 1024).\n   - For GPT-3.5-turbo: Collect nearly 6,000 outputs and perform the same singular value analysis to infer the embedding size.\n\n### Evaluation Metrics\n\n- **Singular Value Threshold**: The effectiveness of the method is evaluated by how sharply the singular values drop to zero at a specific index, which should correspond to the embedding size.\n- **Consistency with Expected Embedding Sizes**: Comparison of inferred embedding sizes against known values (for Pythia models) and against typical values for similar models (for GPT-3.5-turbo).\n\n### Main Experimental Results\n\n1. **Pythia Family Models**:\n   - Singular values of the output matrix drop dramatically at indices corresponding exactly to the known embedding sizes (512, 1024, etc.).\n   - This validated the effectiveness of the method for open-source models.\n\n2. **GPT-3.5-turbo**:\n   - Singular values drop dramatically between index 4,600 and 4,650.\n   - This suggests the embedding size is at most in this range, with a plausible exact size of 4096 (given traditional power of two settings and the specific architecture).\n   - Based on inferred embedding size (4096), it is estimated that GPT-3.5-turbo has around 7 billion parameters.\n\n### Conclusion\n\n- The proposed method for inferring the embedding size from model outputs is validated to be effective on both open-source and API-protected models.\n- For GPT-3.5-turbo, the inferred embedding size aligns closely with expectations given known properties of transformer-based models.\n- This method can be potentially used to monitor updates and changes in LLM architectures over time by end-users."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Infer the embedding size of an API-protected language model (LLM) from its outputs alone, leveraging the linear subspace property of model outputs.",
            "experiment_process": "The method involves collecting next-token distribution outputs from a series of prompts until the number of linearly independent outputs no longer increases, identifying the embedding size. Singular value decomposition (SVD) is used to determine the number of non-zero singular values in the logit matrix, indicative of the model\u2019s embedding size. The process was validated using several open-source LLMs from the Pythia family, followed by applying the method to GPT-3.5-turbo, collecting nearly 6,000 next-token distribution outputs.",
            "result_discussion": "Singular values of the outputs for the Pythia models dropped at indices corresponding to their known embedding sizes. For GPT-3.5-turbo, singular values dropped dramatically between indices 4,600 and 4,650, suggesting an embedding size within this range. The most likely embedding size is inferred to be around 4,096. The method's precision is affected by potential abnormal outputs. The estimated number of parameters for GPT-3.5-turbo is around 7 billion, aligning with known transformer-based models. The method offers tools for continuous monitoring of model updates and architectural changes.",
            "ablation_id": "2403.09539v2.No1"
        },
        {
            "research_objective": "Detect and differentiate between minor and major updates to API-protected LLMs.",
            "experiment_process": "The experimental setup involves comparing the LLM image (formed by the linear subspace of the logit outputs) of the model before and after updates. This process helps determine if the logit outputs remain in the same subspace despite changes, indicating partial updates like hidden prompt changes. The study refers to Table 3 for interpretation of combinations of detectable API changes.",
            "result_discussion": "The study categorizes detectable changes into no update, hidden prompt change or partial fine-tuning with a frozen output layer, LoRA update, and full fine-tuning. The ability to differentiate these changes aids in monitoring unannounced API updates and ensuring the integrity of the LLM\u2019s performance and transparency.",
            "ablation_id": "2403.09539v2.No2"
        },
        {
            "research_objective": "Speculate on detecting LoRA updates and gaining granular information on LLM updates.",
            "experiment_process": "The hypothesis involves collecting outputs from the LLM before and after a LoRA update and decomposing the differences to identify a low-rank update. It suggests using a decomposition approach where the differences indicate a rank indicative of LoRA adjustments. The process includes formulating a matrix to find matrix products that signify a LoRA update.",
            "result_discussion": "It is proposed that such decomposition, if possible, signifies a LoRA update. The study highlights this method as a future work direction, indicating potential for additional research to devise a more efficient detection algorithm and confirm the sufficiency of the decomposition in concluding a LoRA update.",
            "ablation_id": "2403.09539v2.No3"
        }
    ]
}