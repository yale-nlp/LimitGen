{
    "title": "Aspect-oriented Consumer Health Answer Summarization",
    "abstract": "Community Question-Answering (CQA) forums have revolutionized how people seek information, especially those related to their healthcare needs, placing their trust in the collective wisdom of the public. However, there can be several answers in response to a single query, which makes it hard to grasp the key information related to the specific health concern. Typically, CQA forums feature a single top-voted answer as a representative summary for each query. However, a single answer overlooks the alternative solutions and other information frequently offered in other responses. Our research focuses on aspect-based summarization of health answers to address this limitation. Summarization of responses under different aspects such as suggestions, information, personal experiences, and questions can enhance the usability of the platforms. We formalize a multi-stage annotation guideline and contribute a unique dataset comprising aspect-based human-written health answer summaries. We build an automated multi-faceted answer summarization pipeline with this dataset based on task-specific fine-tuning of several state-of-the-art models. The pipeline leverages question similarity to retrieve relevant answer sentences, subsequently classifying them into the appropriate aspect type. Following this, we employ several recent abstractive summarization models to generate aspect-based summaries. Finally, we present a comprehensive human analysis and find that our summaries rank high in capturing relevant content and a wide range of solutions. \n\nKeywords:\u2009Corpus (Creation, Annotation, etc.), Natural Language Generation, Summarization",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "Over the last decade, Community Question Answering (CQA) forums such as Stack Exchange and Reddit have become popular platforms for individuals to seek information. An estimated 79% of Internet users search for health information online, and 74% of such users turn to social media-based platforms facilitating peer-to-peer healthcare. Even though these platforms are not necessarily frequented by experts, people often turn to them for their health-related inquiries for reasons such as easily and freely available information, avoiding medical jargon, inhibition in discussing sensitive personal information in person, distrust in modern medicine, and learning from first-hand experiences of others. In addition, access to medical experts may be limited and may be prohibitively expensive. However, being an informal setup with minimal regulations, the pertinent information becomes entangled in lengthy, convoluted responses plagued by redundancy and irrelevant content such as sarcasm, humor, or off-topic discussions. These observations indicate a need for answer summarization in the community health question-answering domain, which directly impacts the well-being of people.\n\nSeveral works on CQA answer summarization focus on a single best-voted answer or multi-QA pair summarization, pairing a single answer with a query. However, in line with multi-answer summarization works, we find that a single answer need not be the unique correct answer. Additionally, structured presentation of the information into aspect-driven summaries can be more useful for the end users. We identify four essential answer aspects in our data\u2014(Clarificatory) Questions, Information, Suggestions, and (Personal) Experiences. We are motivated by the observations that the users are interested in separating facts (information) from opinions (suggestion, experiences), suggestions from information, extracting personal experiences, and identifying unresolved discussions, one way of which is identifying clarificatory questions.\n\nConsider the example in Figure 1, where part of Answer 1 contains Information on the cause of the problem, part of it shares a Personal Experience concerning the query, and part of it raises a Clarificatory Question to obtain information about the precise context of the problem. Answer 2 contains another first-hand experience, and answers 4 and 5 provide some Suggestion to alleviate the problem. Answer 3 is an insult, while part of Answer 1 (in black) does not add any information. Therefore, both of these are irrelevant.\n\nWe contribute:\n(1) CHA-Summ: Consumer Health Answers Multi-Aspect Summarization dataset\u2014the first summarization dataset catering to health answers in the CQA forums. The data is constructed using a multi-step annotation framework. We perform sentence-level annotations for relevance classification and aspect classification and finally provide human-written answer summaries across the four aspects.\n(2) We build an automated multi-step summarization framework based on several pre-trained transformer-based models which have enabled the generation of concise, human-like summaries. Our pipeline selects answer sentences relevant to the query, classifies them into an aspect category, and generates aspect-based abstractive summaries.\n(3) We perform a comprehensive human evaluation of system-generated summaries and find that our summaries cover a wide range of relevant information under appropriate aspect types and identify key areas for future research."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Related Works",
            "text": "Multi-answer summarization can be modeled as multi-document query-focused summarization (Dang, 2005  ###reference_b9###; Daum\u00e9 III and Marcu, 2006  ###reference_b10###). This approach eliminates extraneous answer sentences (documents) not aligned with the query. Typically, such systems produce a single summary, assuming a lack of structure. We venture beyond this and create categorical summaries across different pre-defined aspect types in the source answers. Previous works in aspect-based summarization define fine-grained aspects in specific domains, such as product attributes from the reviews in the e-commerce domain (Gerani et al., 2019  ###reference_b15###; Yang et al., 2018  ###reference_b46###), or discussion topics in news articles (Frermann and Klementiev, 2019  ###reference_b14###), or define aspects over multiple domains (Hayashi et al., 2021  ###reference_b16###). Our work focuses on pre-determined aspects in health-related answers on CQA forums.\nThe existing works catering to summarization in the biomedical domain focus on single-document summarization of clinical records (Zhang et al., 2020c  ###reference_b54###; MacAvaney et al., 2019  ###reference_b28###; Hu et al., 2022  ###reference_b19###), biomedical literature (Zhang et al., 2020a  ###reference_b51###), multi-document summarization of clinical trials (Wallace et al., 2021  ###reference_b38###), biomedical literature (DeYoung et al., 2021  ###reference_b11###; Cohan et al., 2018  ###reference_b8###), expert-sourced medical answers (Abacha and Demner-Fushman, 2019  ###reference_b1###; Zhang et al., 2020b  ###reference_b52###), etc. In contrast to expert-sourced answers, the information on CQA forums is noisy and characterized by typos, grammatical oversights, and informal language. There is limited work at the intersection of noisy CQA forums and the biomedical domain. Here Savery et al. (2020  ###reference_b35###) introduce the task of medical question-driven answer summarization, and several works focus on question summarization (Yadav and Caragea, 2022  ###reference_b42###; Yadav et al., 2022  ###reference_b45###, 2021  ###reference_b44###). We add to this literature by venturing beyond question summarization to the task of health answer summarization in the CQA domain, which can help address inequity in access to health-related information."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   Data Creation",
            "text": "To identify the aspect categories, we begin with question types defined in (Yadav et al., 2023). After analyzing the answer sentences in our data, we find the following categories to be present\u2014Information, Causes, Testing, Treatment, Symptom, Diagnosis, Drug, Complication, Side-effects, and Duration. Following (Bhattacharya et al., 2022), we further add the categories Suggestion, as people may provide alternatives to modern medicine, Experience for personal accounts shared by users, and Question that seek further clarification. We also find the Emotional Support category for sentences such as \u201cyou got this!\u201d, \u201cdon\u2019t worry\u201d. However, we omit this aspect as such sentences are highly repetitive and irrelevant to information summarization. Moreover, very few people are soliciting emotional support from these forums (Kami\u0144ski et al., 2020). We also find some of the other answer categories to be quite rare. These are Causes, Testing, Symptom, Diagnosis, Complication, Side-effects, and Duration. We merge these with the broader Information category. We also merge the rare categories Drug and Treatment suggestions under the broad aspect Suggestion. For example, \u201cHave ginger tea and Benadryl to soothe cough.\u201d. Our final four aspect types are \u2014Suggestion, Experience, Information, and Question. We define these in Table 1. We pre-process the answer sentences and perform sentence tokenization. Then, two annotators with excellent English proficiency and a medical informatics background write aspect-based answer summaries in the following phases.\n\nIdentify relevant answer sentences: Public platforms might encourage a very informal setup, and people often post humorous replies or digress to some other subject in their answers. For instance, given the question \u2018What could potentially happen after having a stroke?\u2019, the answer sentence \u2019You can even look at some older people and say here is someone who is 20% dead\u2019 does not add any meaningful contribution. Further, as discussed before, the sentences that provide emotional support, although relevant to the question, are irrelevant for summarization. Therefore, we classify such answer sentences as irrelevant to the question and answers directly related to the question are classified as relevant.\n\nIn this round, we classify all the relevant sentences into one of our final four aspects. For example, in Figure 1, one answer sentence suggests using Chloreseptic spray as a remedy while another shares a personal Experience having the same reaction with sugar-free gums.\n\nIn this round, we write abstractive answer summaries for each of the four categories as applicable. Note that a particular aspect might not appear in every thread.\n\nTo compute inter-annotator agreement (IAA) for the first phase, we randomly sample 200 question-sentence pairs and annotate them for relevant sentence identification. We obtain a Cohen\u2019s Kappa of 0.64. For the classification task, we sample and annotate 200 relevant sentences into appropriate aspects and achieve a Cohen\u2019s Kappa of 0.63. Both the scores indicate substantial agreement."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   Analysis",
            "text": "We report the final number of observations for each aspect type in Table 2. The number of aspect categories in an observation ranges from 1 to 4, with 2 being the average. We also find that five threads have no irrelevant sentences among the provided answers.\n\nWe show the reduced word count distribution or the inverse compression ratio in Figure 2. When comparing the number of words in the original answer to those in the final summaries combined across aspect types, we observe a reduction to 38% on average, with a standard deviation of 0.15%. Figure 6 reports reduction across health categories. The categories with the highest reduction are Respiratory Diseases, Men\u2019s Health, and Diet and Fitness. While those with the least reduction are Other-General Healthcare, Other-Health & Beauty, and Alternative Medicine. In our annotation pipeline, we observe a reduction to 73% of the original length on average (with a standard deviation of 18%) just by weeding out irrelevant sentences. In five threads, even though all answer sentences are identified as relevant, we still achieve some compression while summarizing them.\n\nWe also analyze the reduction across each aspect type by taking the ratio of the number of words in the final aspect-specific summary to the number of words in the relevant sentences classified under that aspect. The biggest average reduction is for the Suggestion to 55%, and the smallest average reduction is for Question to 80%, and this is also the most sparse class. In a few examples, a particular aspect summary is longer than the relevant sentence cluster in that category. E.g. \u201c6 wks to heal.\u201d is summarized to a complete sentence \u201cIt takes 6 weeks to heal.\u201d. \u201cI\u2019m trying YOU on a Diet Dr. Oz.\u201d to \u201cI\u2019m trying \"YOU on a Diet\" by Dr. Oz\u201d. \u201cThere is no medicine to \"kill\" hair.\u201d to \u201cThere is no medicine to remove hair permanently.\u201d\n\nFinally, we shuffle our data and split it into train-val-test at the thread level (or final summaries level) in the ratio 60:20:20 stratified by health categories."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Summarization Pipeline",
            "text": "To benchmark our dataset,\nwe devise the following pipeline to generate aspect-based summaries of health answers from a community QA forum.\nGiven a natural language health query  and a number () of answer sentences  provided by different users, we learn the following mapping:\n(1)  where  if the sentence  is relevant to . \n(2) , mapping all relevant sentences to the most appropriate aspect type . \n(3) Generate aspect-based summaries ,\nWe hypothesize query and relevant answers to be semantically more similar than query and irrelevant sentences. We use Sentence-BERT (SBERT) (Reimers and Gurevych, 2019  ###reference_b34###) embeddings and compute the cosine similarity between a question and an answer sentence. The model is pre-trained with sentence-similarity relation as the objective. We experiment with several pre-trained variants and find that \u2018all-mpnet-base-v2\u2019 embeddings provide the best separation between the relevant and irrelevant labels on the training set. Finally, we train a logistic regression (LR) model to classify cosine similarity scores into one of the relevant/irrelevant labels. We use balanced class weights and 10-fold CV with grid search to choose the regularization parameter and report the evaluation results on the test split.\nFine-tune a Sentence BERT (SBERT) using triplet loss. We construct 17919 triplets  from the training set by using a question () as an anchor and all possible combinations of relevant () and irrelevant () answer sentences to that question as positive and negative samples respectively.777We exclude four training and one validation set threads with no irrelevant sentence for triplet creation. We warm start with all-mpnet-base-v2 for 1 epoch and fine-tune the model for 10 epochs. We save the best model using triplet evaluation on the validation set. Finally, we compute the cosine similarity between the embeddings, train an LR model as above, and report the results on test set.\nIn this experiment, we fine-tune the BERT variants for sequence classification by combining question and answer pairs with a separator token. We fine-tune the model for 5 epochs and save the model with the best validation loss. We experiment with BERT-base-cased, BioMedRoBERTa-base, and RoBERTa-base. We use a learning rate of 2e-5 with weight decay by 0.01 fraction after 100 warmup steps.\nWe begin with a zero-shot pipeline (ZS) using a prompt and predict paradigm (Yin et al., 2019  ###reference_b48###) based on NLI. This is a popular approach for zero-shot inference from a large pre-trained model. Here, we prepare a prompting template, where each candidate label is posed as a hypothesis and the sequence to classify is posed as the premise in the format: <cls>Premise<sep>Hypothesis<sep>. For example: <cls>If you have fallen arches you may want to get arch supports for your shoes<sep>This example is treatment<sep>.\nScores for entailment are passed through a softmax classifier to obtain target classification. We choose the joeddav/bart-large-mnli-yahoo-answers model trained on multiNLI (MNLI) dataset (Williams et al., 2018  ###reference_b41###) and fine-tuned on the Yahoo!Answers topic classification. We use the following candidate labels\u2014\u2018informative\u2019, \u2018information\u2019, \u2018cause\u2019, \u2018question\u2019, \u2018interrogative\u2019, \u2018suggestion\u2019, \u2018imperative\u2019, \u2018instruction\u2019, \u2018command\u2019, \u2018personal experience\u2019, \u2018experience\u2019, \u2018personal\u2019. If the output label is \u2018informative\u2019, \u2018information\u2019, or \u2018cause\u2019, we classify the sentence under Information; if it is \u2018question\u2019 or \u2018interrogative\u2019 we classify it as Question; if \u2018suggestion\u2019, \u2018imperative\u2019, \u2018instruction\u2019, \u2018command\u2019, as Suggestion; otherwise as Experience.\nWe acquire certain task-specific linguistic heuristics that can potentially improve over the simple zero-shot baseline.\n(1) Personal Pronouns (PP): We observe that the Experience category often consists of sentences with personal pronouns. E.g. \u2018My mother had orthodontia done in her 50s with no problems.\u2019\n(2) Grammatical Moods (GM): the other aspect classes have a close correspondence with the grammatical moods in the English language. The sentences corresponding to Suggestion have an \u2018imperative\u2019 mood. E.g. \u2018Try essential oils\u2019. Those from Information category have an \u2018indicative\u2019 mood. E.g. \u2018Thyroid cancer is very slow growing\u2019. Those of type Question have an \u2018interrogative\u2019 mood. E.g. \u2018Are you low on vitamin B12?\u2019.\nWe propose the following variants to leverage these heuristics:\nZS+PP: Here we leverage only the first heuristic. We follow the same procedure as the baseline zero-shot approach except when the output of zero-shot classification is \u2018informative\u2019, \u2018information\u2019, or \u2018cause\u2019, we first check if the sentence contains personal pronouns (PP) and if so, classify it as Experience otherwise we classify it as Information.\nGM Classifier\nWe train a feature-based grammatical moods (GM) model to classify each sentence into an aspect. The GM model is a Logistic Regression classifier trained using 10-fold CV and balanced class weights with the following features:\n(a) Moods Probabilities: To obtain these, we train an initial moods classifier by augmenting input with Parts-Of-Speech (POS) tags, following the best model from Walters  ###reference_b39###. The classifier is a Convolutional Neural Network (CNN) (LeCun et al., 1989  ###reference_b23###) model trained with a dataset comprising interrogative sentences from SQuAD 2.0 (Rajpurkar et al., 2018  ###reference_b33###) and interrogatives, imperatives and indicative sentences from Speech Act Annotated Dialogues (SPAADIA) corpus (Leech and Weisser, 2013  ###reference_b24###) along with additional human crafted imperative sentences. This data is then split into training and test sets in the ratio 80:20. The model achieves an impressive 98.94% macro-F1 score over the 20% test split. The details of the training process are provided in Appendix E.2  ###reference_###.\nFor inference on our dataset, we process each sentence in our data using this model and obtain imperative, interrogative, and indicative probabilities. \n(b) Personal Pronoun (PP): a binary feature marked true if the sentence contains a personal pronoun and false otherwise.\n(c) Question Features: We use several additional binary features related to question identification\u2014question_mark (true if the sentence ends in a \u2018?\u2019), do_pattern (true if the sentence contains patterns such as \u201cdo i\u201d, \u201care there\u201d, \u201ctell me more\u201d, etc.), helping_verb (true if the sentence starts with helping verbs such as \u2018is\u2019, \u2018are\u2019, \u2018will\u2019, etc.).888The complete list for do_patterns and helping_verbs is provided in Appendix E.3  ###reference_###.\nFinally, we also fine-tune RoBERTa for multi-class classification of an answer sentence into one of the four aspects. We use the roberta-base and fine-tune the model for 10 epochs while saving the model checkpoint with minimum validation loss. We start with a learning rate of 5e-5 for 50 warm-up steps and then a gradual weight decay by a factor of 0.01."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1.   Relevant sentence Selection:",
            "text": "For this subtask, our goal is to find all answer sentences that are relevant to the query. We experiment with several transformer-based language models for sentence-pair classification:\nWe hypothesize query and relevant answers to be semantically more similar than query and irrelevant sentences. We use Sentence-BERT (SBERT) (Reimers and Gurevych, 2019  ###reference_b34###  ###reference_b34###) embeddings and compute the cosine similarity between a question and an answer sentence. The model is pre-trained with sentence-similarity relation as the objective. We experiment with several pre-trained variants and find that \u2018all-mpnet-base-v2\u2019 embeddings provide the best separation between the relevant and irrelevant labels on the training set. Finally, we train a logistic regression (LR) model to classify cosine similarity scores into one of the relevant/irrelevant labels. We use balanced class weights and 10-fold CV with grid search to choose the regularization parameter and report the evaluation results on the test split.\nFine-tune a Sentence BERT (SBERT) using triplet loss. We construct 17919 triplets  from the training set by using a question () as an anchor and all possible combinations of relevant () and irrelevant () answer sentences to that question as positive and negative samples respectively.777We exclude four training and one validation set threads with no irrelevant sentence for triplet creation. We warm start with all-mpnet-base-v2 for 1 epoch and fine-tune the model for 10 epochs. We save the best model using triplet evaluation on the validation set. Finally, we compute the cosine similarity between the embeddings, train an LR model as above, and report the results on test set.\nIn this experiment, we fine-tune the BERT variants for sequence classification by combining question and answer pairs with a separator token. We fine-tune the model for 5 epochs and save the model with the best validation loss. We experiment with BERT-base-cased, BioMedRoBERTa-base, and RoBERTa-base. We use a learning rate of 2e-5 with weight decay by 0.01 fraction after 100 warmup steps."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2.   Aspect Classification:",
            "text": "In this step, we classify the retrieved sentences under different aspects.\nWe begin with a zero-shot pipeline (ZS) using a prompt and predict paradigm (Yin et al., 2019  ###reference_b48###  ###reference_b48###) based on NLI. This is a popular approach for zero-shot inference from a large pre-trained model. Here, we prepare a prompting template, where each candidate label is posed as a hypothesis and the sequence to classify is posed as the premise in the format: <cls>Premise<sep>Hypothesis<sep>. For example: <cls>If you have fallen arches you may want to get arch supports for your shoes<sep>This example is treatment<sep>.\nScores for entailment are passed through a softmax classifier to obtain target classification. We choose the joeddav/bart-large-mnli-yahoo-answers model trained on multiNLI (MNLI) dataset (Williams et al., 2018  ###reference_b41###  ###reference_b41###) and fine-tuned on the Yahoo!Answers topic classification. We use the following candidate labels\u2014\u2018informative\u2019, \u2018information\u2019, \u2018cause\u2019, \u2018question\u2019, \u2018interrogative\u2019, \u2018suggestion\u2019, \u2018imperative\u2019, \u2018instruction\u2019, \u2018command\u2019, \u2018personal experience\u2019, \u2018experience\u2019, \u2018personal\u2019. If the output label is \u2018informative\u2019, \u2018information\u2019, or \u2018cause\u2019, we classify the sentence under Information; if it is \u2018question\u2019 or \u2018interrogative\u2019 we classify it as Question; if \u2018suggestion\u2019, \u2018imperative\u2019, \u2018instruction\u2019, \u2018command\u2019, as Suggestion; otherwise as Experience.\nWe acquire certain task-specific linguistic heuristics that can potentially improve over the simple zero-shot baseline.\n(1) Personal Pronouns (PP): We observe that the Experience category often consists of sentences with personal pronouns. E.g. \u2018My mother had orthodontia done in her 50s with no problems.\u2019\n(2) Grammatical Moods (GM): the other aspect classes have a close correspondence with the grammatical moods in the English language. The sentences corresponding to Suggestion have an \u2018imperative\u2019 mood. E.g. \u2018Try essential oils\u2019. Those from Information category have an \u2018indicative\u2019 mood. E.g. \u2018Thyroid cancer is very slow growing\u2019. Those of type Question have an \u2018interrogative\u2019 mood. E.g. \u2018Are you low on vitamin B12?\u2019.\nWe propose the following variants to leverage these heuristics:\nZS+PP: Here we leverage only the first heuristic. We follow the same procedure as the baseline zero-shot approach except when the output of zero-shot classification is \u2018informative\u2019, \u2018information\u2019, or \u2018cause\u2019, we first check if the sentence contains personal pronouns (PP) and if so, classify it as Experience otherwise we classify it as Information.\nGM Classifier\nWe train a feature-based grammatical moods (GM) model to classify each sentence into an aspect. The GM model is a Logistic Regression classifier trained using 10-fold CV and balanced class weights with the following features:\n(a) Moods Probabilities: To obtain these, we train an initial moods classifier by augmenting input with Parts-Of-Speech (POS) tags, following the best model from Walters  ###reference_b39###  ###reference_b39###. The classifier is a Convolutional Neural Network (CNN) (LeCun et al., 1989  ###reference_b23###  ###reference_b23###) model trained with a dataset comprising interrogative sentences from SQuAD 2.0 (Rajpurkar et al., 2018  ###reference_b33###  ###reference_b33###) and interrogatives, imperatives and indicative sentences from Speech Act Annotated Dialogues (SPAADIA) corpus (Leech and Weisser, 2013  ###reference_b24###  ###reference_b24###) along with additional human crafted imperative sentences. This data is then split into training and test sets in the ratio 80:20. The model achieves an impressive 98.94% macro-F1 score over the 20% test split. The details of the training process are provided in Appendix E.2  ###reference_###  ###reference_###.\nFor inference on our dataset, we process each sentence in our data using this model and obtain imperative, interrogative, and indicative probabilities. \n(b) Personal Pronoun (PP): a binary feature marked true if the sentence contains a personal pronoun and false otherwise.\n(c) Question Features: We use several additional binary features related to question identification\u2014question_mark (true if the sentence ends in a \u2018?\u2019), do_pattern (true if the sentence contains patterns such as \u201cdo i\u201d, \u201care there\u201d, \u201ctell me more\u201d, etc.), helping_verb (true if the sentence starts with helping verbs such as \u2018is\u2019, \u2018are\u2019, \u2018will\u2019, etc.).888The complete list for do_patterns and helping_verbs is provided in Appendix E.3  ###reference_###  ###reference_###.\nFinally, we also fine-tune RoBERTa for multi-class classification of an answer sentence into one of the four aspects. We use the roberta-base and fine-tune the model for 10 epochs while saving the model checkpoint with minimum validation loss. We start with a learning rate of 5e-5 for 50 warm-up steps and then a gradual weight decay by a factor of 0.01."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "4.3.   Aspect-based Answer Summarization:",
            "text": "In this step, we first chunk together all the relevant sentences across a particular aspect class in the order they were in the source answers. Then, we summarize them using transformer-based state-of-the-art abstractive summarization models. In particular, we experiment with BART (Lewis et al., 2020  ###reference_b25###), which uses a denoising auto-encoder trained to regenerate arbitrarily corrupted text.\nT5 (Raffel et al., 2020  ###reference_b32###), a model that generalizes text-to-text transfer learning to a variety of NLP tasks, trained by randomly corrupting text spans. Prophetnet (Qi et al., 2020  ###reference_b31###) trained with a novel self-supervised objective of future n-gram prediction, along with a new n-stream self-attention mechanism.\nAnd, Pegasus (Zhang et al., 2020a  ###reference_b51###), an encoder-decoder architecture trained with a novel self-supervised objective of Gap Sentence Generation by masking full sentences instead of random tokens or text spans, designed especially for abstractive summarization.\nWe save a separate model for each aspect type. We fine-tune the models under the following two scenarios999See Appendix E.4  ###reference_### for implementation details.:\nAns+ft: We use the source answer as the input to fine-tune the model and to obtain the final summaries. We want to test if the models can identify the answer categories by looking at the target summary for the respective aspect.\nPipeline+ft: We use the gold annotations of the relevant sentences under a particular aspect class (over-extractive) for fine-tuning the model. These are the outputs of our second round of annotations. At the time of inference, we use system-generated relevant sentences in a particular category from the test set as the input. These are the output from the second step of our pipeline ."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Result Analysis",
            "text": "The results from experiments on this subtask are reported in Table 3  ###reference_###. While the cosine-similarity classifier using the off-the-shelf Sentence BERT model does quite well, fine-tuning it provides an improvement of more than 1%, and fine-tuning a RoBERTa model provides more than 2% improvement in the macro-F1 score.\nWe report the benchmarking results for this task in Table 4  ###reference_###. We find that an NLI-based zero-shot (ZS) classifier\u2019s performance over Suggestion and Experience classes is quite poor. Combining this classifier with personal pronoun heuristic in ZS+PP improves the  score for Experience category by 20% and provides an improvement in the macro- by around 5.5%. Leveraging both personal pronoun and grammatical mood heuristics in the GM classifier gives an overall improvement of almost 24% over the ZS baseline. Finally, we observe that fine-tuning RoBERTa surpasses even this and gives us a remarkable 37% improvement over the ZS baseline.\nFor end-to-end aspect identification, we first take the relevant sentences classified by RoBERTa-ftr for relevant sentence classification and assign the aspect type assigned by RoBERTa-ftf to these. While we assign \u2019none\u2019 to all sentences classified as irrelevant. The overall combined macro-average  score is 59.39%. We provide the confusion matrix for this end-to-end task in Figure 3  ###reference_###. We note that most errors are cascaded from the relevant sentence selection module.\nApproach\nSuggestion\nExperience\nInformation\nQuestion\nAvg\n\nP\nR\n\nP\nR\n\nP\nR\n\nP\nR\n\n\n\nZS\n88.24\n6.70\n12.45\n23.62\n37.50\n28.99\n41.38\n72.56\n52.70\n50.00\n33.33\n40.00\n33.53\n\nZS+PP\n88.24\n06.70\n12.45\n34.20\n82.50\n48.35\n47.27\n68.37\n55.89\n50.00\n33.33\n40.00\n39.17\n\nGM\n72.73\n42.86\n53.93\n62.77\n73.75\n67.82\n59.31\n80.00\n68.12\n33.33\n50.00\n40.00\n57.47\n\nRoBERTa-ftf\n71.21\n83.93\n77.05\n79.45\n72.50\n75.82\n81.97\n69.77\n75.38\n60.00\n50.00\n54.55\n70.70\n\nObservations\n224\n80\n215\n6\n525\n###figure_3### Considering that only the aspects that appear in original answers would have a corresponding gold summary, we pair together the system and gold summaries for an aspect and evaluate them using ROUGE scores (Lin, 2004  ###reference_b26###) reported in Table 5  ###reference_###. We observe that across all model variants, our pipeline, in general, provides significant improvements in comparison to the respective Ans+ft variants. The improvement can be as high as 31% in terms of ROUGE-L, for example by fine-tuning Pegasus over Experience category. Averaging across models and the aspect types (excluding Question), there\u2019s an improvement of around 10% in terms of ROUGE-1 and ROUGE-L, and 7% in terms of ROUGE-2 from the pipeline strategy over Ans+ft strategy. We also find that fine-tuning using the source answers is not an effective strategy for the sparse Question category where none of the models in the second panel produce any summaries while looking at the bottom panel of the Table, BART and Pegasus perform exceptionally well using the pipeline strategy. We also consider the standard best answer summary for comparison and find that although it is similar in performance to Ans+ft variants over the Suggestion and Information categories and better on the Question category, it is quite poor in the Experience category and worse than Pipeline+ft for all categories. Overall, the BART model fine-tuned using the pipeline strategy has strong performance across all categories.\nModel\nSuggestion\nExperience\nInformation\nQuestion\n\nR1\nR2\nRL\nR1\nR2\nRL\nR1\nR2\nRL\nR1\nR2\nRL\n\n\nBest Ans\n0.32\n0.29\n0.27\n0.16\n0.08\n0.12\n0.35\n0.32\n0.31\n0.16\n0.02\n0.10\n\n\n\n\n\n\n\nAns + ft\n\n\nBART\n0.33\n0.27\n0.29\n0.39\n0.41\n0.37\n0.36\n0.32\n0.32\n0.0\n0.0\n0.0\n\nT5\n0.34\n0.20\n0.28\n0.37\n0.38\n0.34\n0.37\n0.31\n0.32\n0.0\n0.0\n0.0\n\nPegasus\n0.34\n0.26\n0.26\n0.24\n0.19\n0.18\n0.36\n0.27\n0.28\n0.0\n0.0\n0.0\n\nProphetnet\n0.23\n0.14\n0.19\n0.38\n0.40\n0.36\n0.42\n0.35\n0.38\n0.0\n0.0\n0.0\n\n\n\n\n\nPipeline + ft\n\n\nBART\n0.44\n0.31\n0.37\n0.52\n0.48\n0.49\n0.45\n0.36\n0.40\n0.72\n0.62\n0.72\n\nT5\n0.36\n0.22\n0.28\n0.51\n0.45\n0.47\n0.42\n0.35\n0.38\n0.66\n0.48\n0.66\n\nPegasus\n0.41\n0.35\n0.34\n0.52\n0.45\n0.49\n0.39\n0.31\n0.34\n0.79\n0.63\n0.79\n\nProphetnet\n0.43\n0.32\n0.37\n0.53\n0.44\n0.48\n0.38\n0.32\n0.34\n0.42\n0.24\n0.42"
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "5.1.   Experimental Results",
            "text": "The results from experiments on this subtask are reported in Table 3  ###reference_###  ###reference_###. While the cosine-similarity classifier using the off-the-shelf Sentence BERT model does quite well, fine-tuning it provides an improvement of more than 1%, and fine-tuning a RoBERTa model provides more than 2% improvement in the macro-F1 score.\nWe report the benchmarking results for this task in Table 4  ###reference_###  ###reference_###. We find that an NLI-based zero-shot (ZS) classifier\u2019s performance over Suggestion and Experience classes is quite poor. Combining this classifier with personal pronoun heuristic in ZS+PP improves the  score for Experience category by 20% and provides an improvement in the macro- by around 5.5%. Leveraging both personal pronoun and grammatical mood heuristics in the GM classifier gives an overall improvement of almost 24% over the ZS baseline. Finally, we observe that fine-tuning RoBERTa surpasses even this and gives us a remarkable 37% improvement over the ZS baseline.\nFor end-to-end aspect identification, we first take the relevant sentences classified by RoBERTa-ftr for relevant sentence classification and assign the aspect type assigned by RoBERTa-ftf to these. While we assign \u2019none\u2019 to all sentences classified as irrelevant. The overall combined macro-average  score is 59.39%. We provide the confusion matrix for this end-to-end task in Figure 3  ###reference_###  ###reference_###. We note that most errors are cascaded from the relevant sentence selection module.\nApproach\nSuggestion\nExperience\nInformation\nQuestion\nAvg\n\nP\nR\n\nP\nR\n\nP\nR\n\nP\nR\n\n\n\nZS\n88.24\n6.70\n12.45\n23.62\n37.50\n28.99\n41.38\n72.56\n52.70\n50.00\n33.33\n40.00\n33.53\n\nZS+PP\n88.24\n06.70\n12.45\n34.20\n82.50\n48.35\n47.27\n68.37\n55.89\n50.00\n33.33\n40.00\n39.17\n\nGM\n72.73\n42.86\n53.93\n62.77\n73.75\n67.82\n59.31\n80.00\n68.12\n33.33\n50.00\n40.00\n57.47\n\nRoBERTa-ftf\n71.21\n83.93\n77.05\n79.45\n72.50\n75.82\n81.97\n69.77\n75.38\n60.00\n50.00\n54.55\n70.70\n\nObservations\n224\n80\n215\n6\n525\n###figure_4### Considering that only the aspects that appear in original answers would have a corresponding gold summary, we pair together the system and gold summaries for an aspect and evaluate them using ROUGE scores (Lin, 2004  ###reference_b26###  ###reference_b26###) reported in Table 5  ###reference_###  ###reference_###. We observe that across all model variants, our pipeline, in general, provides significant improvements in comparison to the respective Ans+ft variants. The improvement can be as high as 31% in terms of ROUGE-L, for example by fine-tuning Pegasus over Experience category. Averaging across models and the aspect types (excluding Question), there\u2019s an improvement of around 10% in terms of ROUGE-1 and ROUGE-L, and 7% in terms of ROUGE-2 from the pipeline strategy over Ans+ft strategy. We also find that fine-tuning using the source answers is not an effective strategy for the sparse Question category where none of the models in the second panel produce any summaries while looking at the bottom panel of the Table, BART and Pegasus perform exceptionally well using the pipeline strategy. We also consider the standard best answer summary for comparison and find that although it is similar in performance to Ans+ft variants over the Suggestion and Information categories and better on the Question category, it is quite poor in the Experience category and worse than Pipeline+ft for all categories. Overall, the BART model fine-tuned using the pipeline strategy has strong performance across all categories.\nModel\nSuggestion\nExperience\nInformation\nQuestion\n\nR1\nR2\nRL\nR1\nR2\nRL\nR1\nR2\nRL\nR1\nR2\nRL\n\n\nBest Ans\n0.32\n0.29\n0.27\n0.16\n0.08\n0.12\n0.35\n0.32\n0.31\n0.16\n0.02\n0.10\n\n\n\n\n\n\n\nAns + ft\n\n\nBART\n0.33\n0.27\n0.29\n0.39\n0.41\n0.37\n0.36\n0.32\n0.32\n0.0\n0.0\n0.0\n\nT5\n0.34\n0.20\n0.28\n0.37\n0.38\n0.34\n0.37\n0.31\n0.32\n0.0\n0.0\n0.0\n\nPegasus\n0.34\n0.26\n0.26\n0.24\n0.19\n0.18\n0.36\n0.27\n0.28\n0.0\n0.0\n0.0\n\nProphetnet\n0.23\n0.14\n0.19\n0.38\n0.40\n0.36\n0.42\n0.35\n0.38\n0.0\n0.0\n0.0\n\n\n\n\n\nPipeline + ft\n\n\nBART\n0.44\n0.31\n0.37\n0.52\n0.48\n0.49\n0.45\n0.36\n0.40\n0.72\n0.62\n0.72\n\nT5\n0.36\n0.22\n0.28\n0.51\n0.45\n0.47\n0.42\n0.35\n0.38\n0.66\n0.48\n0.66\n\nPegasus\n0.41\n0.35\n0.34\n0.52\n0.45\n0.49\n0.39\n0.31\n0.34\n0.79\n0.63\n0.79\n\nProphetnet\n0.43\n0.32\n0.37\n0.53\n0.44\n0.48\n0.38\n0.32\n0.34\n0.42\n0.24\n0.42"
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "5.2.   Human Evaluation of Summaries",
            "text": "Following (Fabbri et al., 2021  ###reference_b12###; Zhan et al., 2022  ###reference_b50###), we conduct a human evaluation of the system-generated summaries from BART for both Ans+ft and Pipeline+ft strategies to assess the quality of summaries holistically.101010We provide an example of question-answers, gold standard and system-generated summaries from these two BART variants in Appendix F  ###reference_###. For each aspect category, we randomly sample 2\u20133 summaries from the test set where that category is present in both the gold and the system summaries using both model variants. Three of the authors, having a background in text-summarization research, annotate 21 summaries on a Likert scale from 1 to 5 (1 being lowest) along the following dimensions: \nCoherence\u2014How well-organized are all the sentences collectively? \nConsistency\u2014is the summary logically implied (entailed) by the source? Summaries that contain hallucinated facts are penalized.\nFluency\u2014how well written is each sentence. Formatting, capitalization, or grammatical errors degrading the readability are penalized. \nRelevance\u2014The summary should include only relevant and non-redundant information from the source answers. We also penalize sentences irrelevant to the summary corresponding to a particular aspect. For example, an Experience may not be relevant for the Suggestion summary\nCoverage\u2014we add this dimension to assess how well a summary covers a particular aspect from the answers (if present).\nWe report the results in Table 6  ###reference_###. We note that the models have high coherence in general. We also observe that the summaries are quite extractive, which is why the models are scoring high in consistency. However, there is a slight drop for the Ans+ft version. Here, the model adds negation to one of the answers in the source document. Further inspection reveals that this hallucination might be grounded in real-world knowledge. We also note a lack of fluency, more so in the Pipeline+ft version. This, too, can be attributed to the extractive nature of generated summaries, where typos and grammatical disfluencies in the source text get transferred to the summaries as well. We can see this in Figure 4  ###reference_### where the misspelled \"ouside\" and \"snezze\" are copied from the source answers to the summaries. Finally, Pipeline+ft summaries rank high in relevance and coverage in comparison to the Ans+ft strategy, as exemplified in Figure 4  ###reference_###. This highlights the strengths of the pipeline in weeding out irrelevant sentences and preserving pertinent information at the same time. We quantify the disagreements between the annotators in terms of standard deviations across the scores and find high deviations in the average relevance scores. However, all annotators consistently score the pipeline summaries higher on relevance. We also see in Figure 4  ###reference_### how the best answer lacks coverage of an important aspect which is Suggestion. This is addressed in the pipeline-generated summaries in this example.\n###figure_5###"
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Conclusion",
            "text": "Using a multi-stage annotation framework, we introduce a novel problem of aspect-based health answer summarization from a community question-answering platform. This can be especially useful in low healthcare access settings for people seeking answers to common concerns or support from other users who might have experienced similar problems without being lost in a flood of answers on the public forums. We hypothesize that aspect-driven summaries can also improve performance on downstream question-answering tasks by separating information and suggestions from questions and subjective experiences. We provide an automated summarization pipeline built on our dataset using the latest state-of-the-art transformer-based models. The human evaluation demonstrates that our pipeline significantly improves over directly summarizing source answers. We also note that our linguistics heuristics-informed feature-based model offers a good baseline for aspect type classification. Future work can leverage these heuristics to construct a larger silver standard dataset to pre-train the aspect classifier before fine-tuning it on the gold annotations. In the future, our dataset can also be further augmented with factuality scores for each sentence with the help of an expert-sourced website. The summaries could then be rewritten to flag the sentences in Information and Suggestion categories with low expert confidence."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7.   Bibliographical References",
            "text": ""
        }
    ],
    "url": "http://arxiv.org/html/2405.06295v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.1",
            "5.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "5.1"
        ]
    },
    "research_context": {
        "paper_id": "2405.06295v1",
        "paper_title": "Aspect-oriented Consumer Health Answer Summarization",
        "research_background": "### Motivation\nThe paper is motivated by several observations within the realm of Community Question Answering (CQA) forums such as Yahoo! Answers, Stack Exchange, and Reddit, where users frequently seek health-related information. Individuals often turn to these forums because of the easy and free availability of information, ability to avoid medical jargon, preference for discussing sensitive issues anonymously, distrust in modern medicine, and interest in learning from others' experiences. Despite these advantages, the informal nature of these forums results in lengthy and convoluted responses filled with redundant or irrelevant content such as sarcasm, humor, or off-topic discussions. This setting highlights the necessity for effective answer summarization in the community health question-answering domain.\n\n### Research Problem\nThe core research problem addressed in the paper is the development of efficient summarization techniques tailored for consumer health answers in CQA forums. The challenge is to distill lengthy, multi-part responses into concise, informative, and aspect-driven summaries. The paper seeks to classify and extract relevant information from multiple answers, structured into different aspects: Clarificatory Questions, Information, Suggestions, and Personal Experiences. Such structured summarization aims to enhance the utility of the information for users and mitigate the impact of irrelevant or redundant content.\n\n### Relevant Prior Work\nThe paper builds upon several strands of prior work:\n1. **Answer Summarization in CQA Forums**:\n   - Previous research has primarily focused either on summarizing single best-voted answers (Chowdhury and Chakraborty, 2019; Chowdhury et al., 2021) or on summarizing multi-QA pairs with each query paired with a single answer (Hsu et al., 2022). \n   - In contrast, this paper aligns with multi-answer summarization efforts (Liu et al., 2008; Fabbri et al., 2022), recognizing that multiple answers can provide different yet complementary pieces of information.\n   \n2. **Aspect-Driven Summarization**:\n   - Existing studies underscore the importance of organizing information into aspect-driven summaries for better user comprehension (Tauchmann et al., 2018; Frermann and Klementiev, 2019).\n\n3. **User Preferences**:\n   - Research shows that users prefer to distinguish factual information (information) from opinions (suggestions, experiences) (Yu and Hatzivassiloglou, 2003; Negi and Buitelaar, 2015), value extracting personal experiences (Hedegaard and Simonsen, 2013), and seek the identification of unresolved discussions (Kim and Kang, 2014).\n\nThe paper builds on these prior studies by proposing a novel dataset (CHA-Summ) and an automated summarization framework that leverages pre-trained transformer-based models to generate concise, human-like summaries tailored to the specified aspects.",
        "methodology": "Aspect-oriented Consumer Health Answer Summarization Methodology:\n\n**Overview:**\nThe paper proposes a detailed pipeline to generate aspect-based summaries from community question and answer (QA) forums, specifically focusing on health queries. The methodology involves multiple steps, including sentence relevance classification, aspect mapping, and summary generation. The effectiveness of the proposed methods is benchmarked using a specific dataset.\n\n**Step 1: Relevance Classification**\n- **Mapping Function 1 (\\( f_1 \\) ):** Detects whether a given sentence (\\( s_j \\)) is relevant to the natural language health query (\\( Q \\)). The hypothesis is that relevant answers are more semantically similar to the query compared to irrelevant sentences.\n  - Utilizes **Sentence-BERT (SBERT)** embeddings to compute cosine similarity between the question and answer sentences.\n  - **Logistic Regression Model:** Trains an LR model to classify sentences based on cosine similarity scores, using balanced class weights and 10-fold cross-validation (CV) with grid search to select the regularization parameter.\n\n**Step 2: Aspect Classification**\n- **Mapping Function 2 (\\( f_2 \\) ):** Maps relevant sentences to the most appropriate aspect type.\n  - **Fine-tuning SBERT:** Utilizes triplet loss to further fine-tune SBERT for distinguishing relevant sentences from irrelevant ones by constructing 17,919 triplets from the training set and excluding threads with no irrelevant sentence for triplet creation.\n  - Post fine-tuning, cosine similarity embeddings are used with a logistic regression model to classify sentences.\n\n**Step 3: Sequence Classification with BERT Variants**\n- Fine-tunes BERT variants for sequence classification by combining question-answer pairs with a separator token.\n  - Experiments with **BERT-base-cased, BioMedRoBERTa-base, and RoBERTa-base**.\n  - Uses a learning rate of 2e-5 with a weight decay of 0.01 after 100 warmup steps, and the model is fine-tuned for 5 epochs.\n\n**Step 4: Zero-shot Classification**\n- **Zero-shot Pipeline (ZS):** Uses a prompt and predict paradigm based on Natural Language Inference (NLI).\n  - Prepares prompting templates representing candidate labels as hypotheses and the sequence to classify as the premise.\n  - Employs **joeddav/bart-large-mnli-yahoo-answers model**, which is trained on the MNLI dataset and fine-tuned on the Yahoo! Answers topic classification.\n  - Maps classifications into broader categories: Information, Question, Suggestion, and Experience.\n\n**Step 5: Enhanced Heuristics**\n- Leverages task-specific linguistic heuristics to improve over the zero-shot baseline:\n  - **Personal Pronouns (PP):** Recognizes sentences with personal pronouns as Experience.\n  - **Grammatical Moods (GM):** Maps sentence aspects based on grammatical moods\u2014imperative for Suggestions, indicative for Information, and interrogative for Questions.\n\n**Variants Combining Heuristics:**\n- **ZS+PP:** Uses PP heuristic in conjunction with zero-shot approach to classify sentences.\n- **GM Classifier:** A logistic regression classifier trained to distinguish between aspect categories using:\n  - **Moods Probabilities:** Derived from a CNN model trained on several datasets (e.g., SQuAD 2.0, SPAADIA corpus).\n  - **Personal Pronoun (PP) feature:** Binary marking for sentences with personal pronouns.\n  - **Question Features:** Includes binary indicators for question-related features such as question marks, specific patterns, and starting helping verbs.\n\n**Step 6: Multi-class Classification with RoBERTa**\n- Fine-tunes **RoBERTa** for multi-class classification.\n  - Uses the roberta-base model.\n\nOverall, the methodology encompasses leveraging pre-trained language models (like SBERT and BERT) fine-tuned with specific loss functions and additional heuristics to build a robust system for aspect-oriented summarization of health-related QA forum answers.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\n#### Datasets:\nThe experiment focuses on sentence classification and summarization tasks for consumer health answers. The exact dataset details (e.g., size, source) are not given, but it is evident that the data include various aspects such as Suggestion, Experience, Information, and Question.\n\n#### Baselines:\n1. **ZS (Zero-Shot Classifier)**: Uses natural language inference (NLI).\n2. **ZS+PP (Zero-Shot with Personal Pronoun heuristic)**: Combines zero-shot classifier with a personal pronoun heuristic.\n3. **GM (Grammatical Mood classifier)**: Uses personal pronoun and grammatical mood heuristics.\n4. **RoBERTa-ftf**: Fine-tuned RoBERTa model for sentence classification.\n5. **Best Ans**: Standard best answer summary.\n6. **Ans+ft**: Directly fine-tuned models like BART, T5, Pegasus, and Prophetnet on the dataset.\n\n#### Evaluation Metrics:\nThe experiment uses several evaluation metrics:\n- **Precision (P)**\n- **Recall (R)**\n- **Macro-F1 score**: Evaluates overall performance considering all classes.\n- **ROUGE scores (ROUGE-1, ROUGE-2, ROUGE-L)**: Commonly used for summarization tasks to measure overlap between system-generated and reference summaries.\n\n#### Main Experiment Results:\n\n1. **Sentence Classification**:\n   - **RoBERTa-ftf** achieves the highest performance, showing a 37% improvement over the ZS baseline.\n   - The combined macro-average score for end-to-end aspect identification using RoBERTa is 59.39%.\n\n2. **Summarization**:\n   - Across different categories (\"Suggestion\", \"Experience\", \"Information\", \"Question\"), the fine-tuning of summarization models using a pipeline strategy significantly improves ROUGE scores.\n   - Specifically, **Pegasus** fine-tuned over the Experience category shows up to a 31% improvement in ROUGE-L.\n   - For sparse Question categories, models in the second panel (Ans+ft) fail to produce summaries, whereas models using the pipeline strategy (BART and Pegasus in particular) show strong performance.\n   - **BART** fine-tuned using the pipeline strategy exhibits consistently high ROUGE scores across all categories.\n\nThese results indicate that the proposed strategies, especially the pipeline approach with fine-tuning, are effective in improving both the aspect-oriented classification and summarization tasks within the consumer health answers domain."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "The objective of this study is to evaluate and improve the performance of different models for selecting relevant sentences from health-related community QA forums in response to user queries.",
            "experiment_process": "The process involves using Sentence-BERT (SBERT) embeddings to compute the cosine similarity between a question and an answer sentence. The model is pre-trained with a sentence-similarity objective. Various pre-trained SBERT variants were tested, and 'all-mpnet-base-v2' provided the best separation between relevant and irrelevant sentences. A logistic regression (LR) model was then trained on cosine similarity scores with balanced class weights and 10-fold cross-validation (CV). Additionally, SBERT was fine-tuned using triplet loss with 17919 triplets constructed from the training set. The fine-tuning process involved warming up with all-mpnet-base-v2 for 1 epoch and refining for 10 epochs. Another mode involved fine-tuning BERT variants (BERT-base-cased, BioMedRoBERTa-base, and RoBERTa-base) for sequence classification, combining question-answer pairs with a separator token, trained for 5 epochs with a learning rate of 2e-5 and 0.01 weight decay after 100 warmup steps.",
            "result_discussion": "The experiments reported improvements in performance metrics. The cosine-similarity classifier using off-the-shelf SBERT showed good performance, but fine-tuning improved it by more than 1%. Fine-tuning RoBERTa improved the macro-F1 score by more than 2%.",
            "ablation_id": "2405.06295v1.No1"
        },
        {
            "research_objective": "This study aims to classify retrieved sentences into different health-related aspects, such as suggestions, experiences, and questions, to generate aspect-based summaries from QA forums.",
            "experiment_process": "The experiment started with a zero-shot pipeline (ZS) using a prompting template based on NLI. The joeddav/bart-large-mnli-yahoo-answers model was utilized with candidate labels like 'informative', 'information', 'question', 'suggestion', and 'experience'. The study also used linguistic heuristics related to personal pronouns (PP) and grammatical moods (GM). Variants like ZS+PP employed PP heuristics, while GM Classifier used features such as Moods Probabilities from a CNN model trained on several datasets, Personal Pronoun binary feature, and question identification features. Additionally, RoBERTa was fine-tuned for multi-class classification into one of the four aspects.",
            "result_discussion": "The results revealed that the zero-shot classifier performed poorly over Suggestion and Experience classes. Combining the classifier with personal pronoun heuristics improved scores for the Experience category by 20%, and leveraging both PP and GM heuristics in the GM classifier improved the macro-average score by almost 24%. Fine-tuning RoBERTa showed a remarkable 37% improvement over the zero-shot baseline.",
            "ablation_id": "2405.06295v1.No2"
        },
        {
            "research_objective": "The goal of this study is to generate summaries based on different aspects of health-related answers by leveraging state-of-the-art abstractive summarization models.",
            "experiment_process": "The process involved chunking relevant sentences across a particular aspect class and summarizing them using BART, T5, Prophetnet, and Pegasus transformer-based models. Two scenarios were considered: Ans+ft (using the source answer as input and testing if models could identify the answer categories) and Pipeline+ft (utilizing the gold annotations of relevant sentences for fine-tuning and system-generated sentences during inference). Fine-tuning was done for 10 epochs, with model checkpoints saved at minimal validation loss.",
            "result_discussion": "The results indicated significant improvements using the pipeline strategy over Ans+ft, notably a 31% improvement in ROUGE-L scores with fine-tuned Pegasus over the Experience category. Averaging across models, there was a 10% improvement in ROUGE-1 and ROUGE-L, and 7% in ROUGE-2 using the pipeline strategy. BART and Pegasus models performed exceptionally well under the pipeline strategy, surpassing the standard best answer summaries in most categories.",
            "ablation_id": "2405.06295v1.No3"
        }
    ]
}