{
    "title": "Predicting Learning Performance with Large Language Models: A Study in Adult Literacy",
    "abstract": "Intelligent Tutoring Systems (ITSs) have significantly enhanced adult literacy training, a key factor for societal participation, employment opportunities, and lifelong learning. Our study investigates the application of advanced AI models, including Large Language Models (LLMs) like GPT-4, for predicting learning performance in adult literacy programs in ITSs. This research is motivated by the potential of LLMs to predict learning performance based on its inherent reasoning and computational capabilities. Our findings show that GPT-4 presents competitive predictive abilities with traditional machine learning methods such as Bayesian Knowledge Tracing, Performance Factor Analysis, Sparse Factor Analysis Lite (SPARFA-Lite), tensor factorization and eXtreme Gradient Boosting (XGBoost). While XGBoost (trained on local machine) outperforms GPT-4 in predictive accuracy, GPT-4-selected XGBoost and its subsequent tuning on the GPT-4 platform demonstrates superior performance compared to local machine execution. Moreover, our investigation into hyper-parameter tuning by GPT-4 versus grid-search suggests comparable performance, albeit with less stability in the automated approach, using XGBoost as the case study. Our study contributes to the field by highlighting the potential of integrating LLMs with traditional machine learning models to enhance predictive accuracy and personalize adult literacy education, setting a foundation for future research in applying LLMs within ITSs.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Adult literacy education in reading comprehension empowers individuals to participate actively in society, access better job opportunities, and engage in lifelong learning. Effective literacy education programs are tailored to meet the diverse needs of adult learners, using strategies to enhance reading skills, comprehension, and critical analysis of texts. The success of these programs often depends on accurate assessment and continuous improvement of personalized instructions to cater to learner needs. Predicting learning performance is crucial as it allows for the early identification of individuals needing additional support, enabling targeted interventions to enhance reading comprehension and literacy skills.\n\nIntelligent Tutoring Systems (ITSs) personalize instruction in adult literacy by tracking and assessing learning progress, facilitating adaptation to better meet learner needs. A key component of ITSs is predicting learning performance through machine learning models, which enable personalized instruction. This process involves analyzing historical data, including learners\u2019 correctness in problem-solving attempts. Advanced machine learning models, using fine-grained data and natural language related to the problem-solving context, can improve learner performance prediction and the personalization of instruction. Despite this potential, the role of advanced AI models, like multimodal machine learning models and large language models (LLMs), in predicting learning performance remains underexplored.\n\nRecent advancements in AI models, notably LLMs, have shown remarkable predictive capabilities in areas such as mathematical reasoning and time series forecasting. These successes illustrate the potential of LLMs in understanding patterns relevant to learner modeling tasks. In education, prior research has demonstrated the potential of LLMs in predictive analysis, like predicting learning performance in computer science education and identifying at-risk learners. Despite these advances, the use of LLMs to enhance predictive analytics within ITSs is still in the early stages. Motivated by the proven effectiveness of LLMs in educational prediction tasks, our study examines the potential of LLMs, specifically GPT-4, in comparison with traditional methods such as Bayesian Knowledge Tracing (BKT), Performance Factor Analysis (PFA), Sparse Factor Analysis Lite (SPARFA-Lite), tensor factorization, and eXtreme Gradient Boosting (XGBoost), for predicting learner performance in adult literacy education. Our study investigates two Research Questions:\n\nRQ1: How effectively can the GPT-4 model, through specific prompting strategies, predict learning performance in adult literacy programs compared to existing benchmark models?\nRQ2: How can GPT-4 augment traditional human-led efforts in enhancing the prediction accuracy of learning performance in adult literacy lessons?\n\nThe study utilized reading comprehension datasets, including attributes like learner ID, questions, attempts, and learners\u2019 performance scores for each lesson. To answer RQ1, we employed widely-used models including BKT, PFA, SPARFA-Lite, and XGBoost, comparing them with the GPT-4 model. Model performance was assessed through five-fold cross-validation, revealing that the XGBoost model outperformed GPT-4 in predicting learning performance. Interestingly, when prompted, GPT-4 itself recommended XGBoost for predicting learning performance. Executing the GPT-4-selected XGBoost model on the GPT-4 platform provided superior results compared to running it on a local machine. In addressing RQ2, we evaluated the tuning of hyper-parameters by GPT-4 against manual tuning. Specifically, we prompted GPT-4 to optimize an XGBoost model for predicting learning performance. Simultaneously, we manually adjusted the XGBoost model\u2019s hyper-parameters through grid-search on a local machine for comparison. Our findings indicated that while the GPT-4-tuned hyper-parameters achieved performance comparable to manually tuned models, they showed less stability than those optimized through manual grid search."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Adult Literacy Education in Intelligent Tutoring Systems",
            "text": "Adult literacy education has long advocated computer-aided adaptive learning technologies for their capacity to provide personalized and cost-effective educational solutions. These technologies are commonly applied in Intelligent Tutoring Systems (ITSs). ITSs can offer personalized tutoring and adaptive instructions tailored to the individual learner, dynamically adjusting the difficulty levels and contents of lessons based on the learner\u2019s responses to questions and tasks. For instance, utilizing web-based applications, ITSs deploy computer-based agents to deliver customized reading materials and learning tasks, ensuring support is readily available for learners facing challenges. The significant impact of ITSs lies in their ability to create an adaptive learning environment that supports and responds to individual educational needs, thereby empowering learners to advance at their own pace. Furthermore, ITSs tackle the issue of scarce human tutoring and classroom resources for adult learners by utilizing these systems to improve reading comprehension skills."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Learning Performance Prediction",
            "text": "Learning performance prediction is an important task in the field of ITS in education. By understanding the learner\u2019s performance, ITS can accurately assess learning states and offer tailored instructions to support learners throughout the learning process, particularly when they encounter difficulties with questions, face early risks of failure, or experience wheel-spinning.\n\nThe predictive task for learning performance utilizes historical records to predict future performance on questions, incorporating data from multiple attempts. Driven by the need for high accuracy in learning performance prediction, many previous works employed machine learning methods including BKT, PFA, SPARFA-Lite, and tensor factorization. Widely recognized predictive models such as BKT and PFA leverage Bayesian networks and logistic regression for learner performance prediction, respectively. BKT outlines four probabilistic parameters: \u201cknown\u201d (initial or prior knowledge), \u201cslip\u201d (incorrectly answering despite knowing the skill), \u201cguess\u201d (correctly answering without knowing the skill), and \u201clearn\u201d (mastering a skill in subsequent practices). PFA, on the other hand, includes parameters that account for prior successes and failures in answering questions, skill difficulty reflecting the inherent challenge of the skill, and individual learning rates indicating how fast the learner improves in mastering knowledge. Both methods have been utilized for predicting learning performance owing to their stability, strong predictive performance, and explainability.\n\nSPARFA-Lite utilizes quantized matrix completion to predict learner performance in knowledge tracing, representing the probability of answering questions successfully based on three factors: 1) the learner\u2019s understanding of latent concepts, 2) the relationship between questions and concepts, and 3) the inherent difficulty of each question. The tensor factorization method structurally represents learner knowledge in a three-dimensional space, incorporating critical factors such as learners, questions, and attempts to influence learning progress. This approach calculates probability estimates for learner performance using mathematical tensor factorization."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Large Language Models in Education",
            "text": "LLMs, pre-trained on massive amounts of data, enable them to generate human-like text, answer questions, and perform reasoning tasks with unprecedented accuracy [46, 47]. LLMs like ChatGPT have demonstrated remarkable advancements in AI, driving revolutionary shifts in education applications through enhancing instructional feedback [48, 49, 50], boosting student engagement [51], and offering personalized learning experiences [52].\n\nHowever, the applications of LLMs in enhancing predictive analytics within ITSs remain in their early stages. Liu et al.\u2019s investigation [53] on ChatGPT\u2019s effectiveness in logical reasoning, particularly in making prediction-based inferences for multiple-choice reading comprehension and natural language inference tasks, highlights its adeptness at complex educational reasoning challenges. Liu et al. [18] have incorporated ChatGPT for open-ended knowledge tracing in computer science education, enabling enhanced prediction of code snippets for open-ended response analysis. Susnjak [19] has attempted to integrate ChatGPT with machine learning models, enabling advanced predictive analytics to assist at-risk learners through evidence-based remedial recommendations. These cases highlight the advanced predictive capabilities of LLMs or their collaboration with machine learning models for predictive tasks, inspiring further exploration of LLMs\u2019 potential in advancing educational predictive applications."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methods",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Dataset",
            "text": "Our study was granted ethical approval with the Institutional Review Board (IRB) number: H15257. The selected lessons for our analysis include \u201cPersuasive Text\u201d (Lesson 1), \u201cCause and Effect\u201d (Lesson 2), \u201cProblems and Solution\u201d (Lesson 3). Table 1 presents the basic statistics about the dataset on learner performance, detailing information about the learners, questions, and attempts for each lesson."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "The Proposed LLM-based Prediction Method",
            "text": "We developed a LLM-based framework to trace and predict learner performance. This framework includes three procedures: 1) encoding for converting numerical value to contextual prompts, 2) the LLM component for analyzing these prompts and executing predictions, and 3) decoding for outputting the prediction information and assessment along with interpretations.\n\nLearning performance on question-answering tasks was recorded as binary data (labeled as correct or incorrect), to reflect the learner\u2019s success or failure in answering the questions. The records also captured the number of attempts made by the learner. For instance, the performance of the learner on the question during their attempt is recorded as 1 for a correct answer and 0 for an incorrect one (this setting applies to the training dataset, whereas, for the testing dataset, performance data are omitted to enable future prediction). Our objective is to input these performance data into LLM to identify data patterns and latent learning features such as initial knowledge level and learning rate, aiming to predict the learner\u2019s future performance based on the learner\u2019s historical attempts.\n\nEncoding. Learning performance in binary indicator variables representing correct or incorrect responses can be compiled into a contextual prompt by integrating numerical data with textual explanations. For example, the entry could be represented as \u201cThe current learner attempted to answer the question titled as \u2018\u2026\u2019 on their attempt. Their performance was observed as 1 or 0\u201d. Essential considerations for this encoding process include: (a) Numerical to Text Conversion: This involves the embedding of numerical value along with the question contents and answers into a narrative or textual format. (b) Contextual Integration: This aspect involves incorporating information related to the lesson topic, content, and questions, along with knowledge gained from reading comprehension, to enrich understanding of both question and lesson material. This enrichment process also facilitates tailoring and assessing reading comprehension skills of the learner in the learning process.\n\nLLM Component. The contextual prompt serves as input into the LLM component for data analysis and modeling, aimed at predicting learner performance on new or repeated question attempts. Two primary prediction strategies are encompassed in this component: 1) leveraging the inherent reasoning, understanding, and computational capabilities of the GPT-4 model, e.g., the heuristic-based approach; and 2) utilizing available machine learning models, which are automatically selected and fine-tuned by the GPT-4 model for predictive tasks. Through extensive analysis of trial experiments, the following assumptions are included:\n(a) GPT-4 can be pre-trained in predictive tasks, (b) GPT-4 possesses the capability to extract domain-specific knowledge from distinct questions and attempts, (c) GPT-4 can uncover latent learning features within contextual performance data, enabling the evaluation of learners\u2019 reading comprehension skills, and (d) Information inferred by GPT-4, alongside other computational models, can be leveraged to discern trends, patterns, and predict learner learning performance.\n\nDecoding. In the decoding phase, predictive outcomes regarding learning performance are produced through a mechanism that employs either heuristic approaches or machine learning models, which GPT-4 automatically selects and implements. These predictions draw on an analysis of the learning performance distribution integrated in previously mentioned contextual prompts. For instance, it reformat output information into a structured format, such as \u201c{\u2018learner ID\u2019:\u2026, \u2018Question ID\u2019:\u2026, \u2018Attempt\u2019:\u2026, \u2018Prediction\u2019:\u2026, \u2018Assessment\u2019:\u2026}\u201d, efficiently conveying the prediction details. This procedure incrementally increases the dimensionality of the data until it aligns with the original input size of the test dataset."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Baseline Methods",
            "text": "This study will employ machine learning models specifically for predicting learning performance in education [54]. Below is a concise overview of each selected method.\n\nBayesian Knowledge Tracing (BKT) is a computational model used to track and predict learners\u2019 knowledge acquisition over time within educational software, particularly within ITSs [21, 55, 56]. Fundamentally, BKT is based on the principles of Bayesian probability which estimates the likelihood that a learner has mastered a particular skill or concept at various points throughout the learning process, adjusting these estimates in response to the learner\u2019s performance on related tasks or questions [55]. According to [11], the classic BKT\u2019s key parameters include the initial probability of mastering the skill, the probability of acquiring knowledge by transforming from the unmastered state on one skill to master state, the probability of making incorrect answer responses by slipping in mastered state on a skill, and the probability of making correct answer responses by guessing in unmastered state on a skill. BKT advanced this framework by incorporating customized parameters for each learner and each skill into its modeling and predictions [55]. By updating these parameters using Bayesian inferences based on each learner\u2019s responses to questions over time, BKT refines its estimates of a learner\u2019s knowledge state at a particular time step as the learner responds to questions [11, 55].\n\nPerformance Factor Analysis (PFA) utilizes logistic regression to predict the learner\u2019s performance on the questions by incorporating factors on individual learning ability, skill-related features (e.g., difficulty), and the learner\u2019s previous success and failures [12, 22, 43, 57, 58]. Many studies have established PFA as a competitive approach in predicting learner performance, acknowledging the importance of individual differences across skills and learners [12, 42, 59]. In our research, we have further refined the PFA model to better account for the variability among individual learners.\n\nSparse Factor Analysis Lite (SPARFA-Lite), a variant of Sparse Factor Analysis (SPARFA), employs a matrix completion technique to analyze quantified, graded learner performance on questions [23]. This model offers improved efficiency in automatically exploring the number of Knowledge Components for predicting learner performance compared with the traditional Sparse Factor Analysis model [23].\n\nTensor Factorization is a method that decomposes a three-dimensional tensor (representing dimensions of learners, questions, and attempts) into a factor matrix for learners and latent features, and a factor tensor that encompasses dimensions of latent features, questions, and attempts [24, 60, 37, 61]. The latent feature dimensions obtained through tensor decomposition capture learner-specific characteristics such as learning abilities and personalities [60]. The factor tensor models the knowledge space related to learner interactions with questions. Our prior studies have demonstrated its significant efficacy in predicting learner performance, particularly within the adult literacy domain [37, 61]. Here, the rank-based constraint was applied to regulate the factorization computing [24].\n\neXtreme Gradient Boosting (XGBoost) is an algorithm that has become an effective model for knowledge tracing, significantly enhancing prediction performance [26, 62]. At its core, XGBoost constructs an ensemble of decision trees in a sequential manner [25, 63]. In this process, each subsequent tree is specifically trained to address and correct the residuals or errors made by the preceding tree, effectively enhancing the model\u2019s predictive accuracy over iterations. Guided by a gradient descent algorithm, XGBoost optimizes a predefined loss function, systematically reducing prediction errors. Its capability to interpret input features, such as unique learners, questions, and attempts in our study, enables an in-depth understanding of model predictions, enhancing transparency and trustworthiness in predictive analytics."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Evaluation",
            "text": "We employed the recognized quantitative metric Root Mean Square Error (RMSE), which aligned with peer studies [21, 12, 42, 64, 65]. RMSE provides a measure of the square root of the average squared differences between predicted and actual values [66]. Additionally, we conducted a five-fold cross-validation to obtain RMSE values for comparative analysis. In our LLM-based prediction method, specifically utilizing GPT-4, we allocated four out of five folds for training to enable the model to learn from historical data through contextual prompts. The remaining fold was used for testing, to evaluate the accuracy of predictions made by the trained GPT-4."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Results",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Results on RQ1",
            "text": "We investigated the comparison of prediction performance between LLM-based models and Baseline Methods. The result is shown in Table 2  ###reference_### which presents the RMSE values of all models across three CSAL lessons, with lower value indicating better performance in prediction. It should be noted that all the RMSE scores were obtained from models after adjusting their hyper-parameters.\nSpecifically, the XGBoost (selected by GPT-4) showcases an enhanced application, leveraging GPT-4\u2019s strengths in reasoning, computation, and automatic tuning to refine outcomes. Aware of the potential for errors or instability in GPT-4\u2019s predictions, we established the reliability of our RMSE by conducting seven repeated prediction runs for both the standard GPT-4 and the GPT-4 enhanced with XGBoost, across each lesson. The outstanding predictive accuracy of the XGBoost (selected by GPT-4) model led us to perform an extensive manual grid search, documented in Table 2  ###reference_###. This process entailed evaluating 1,296 combinations of hyper-parameters, including number of trees, learning rate, maximum tree depth, training instance subsample ratio, column subsample ratio per tree, minimum loss reduction for further partitioning, and the minimum sum of instance weight required in a child node. This rigorous hyper-parameter tuning was aimed at further enhancing the model\u2019s performance for each lesson, as detailed in Table 3  ###reference_###.\nTable 2  ###reference_### presents the RMSE values of all models across three CSAL lessons, with lower value indicating better performance in prediction. The RMSE value of GPT-4, as shown in Table 2  ###reference_###, surpasses that of most other models, demonstrating only higher values when compared to XGBoost in Lessons 1 and Lesson 2. Notably, the RMSE value of XGBoost (selected by GPT-4) is the lowest among all six models across the three lessons. This demonstrates the substantial enhancement in predicting learning performance achieved through the integration of GPT-4 and XGBoost. Additionally, the XGBoost model outperforms other traditional knowledge tracing models, including Individualized BKT, PFA, SPARFALite, and Tensor Factorization, in the reading comprehension data. As for the standard errors for the RMSE values, lower values indicate less variability in the estimated RMSE values, thereby suggesting greater confidence in the accuracy of the predictions. The standard errors for all RMSE values related to GPT-4 and XGBoost (as chosen by GPT-4) fall within the range of [0.004, 0.009], indicating a relatively moderate variability in the prediction outcomes across all models."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Results on RQ2",
            "text": "Lesson 1\n0.435\n0.422\n0.053\n0.398\nLesson 2\n0.376\n0.361\n0.033\n0.351\nLesson 3\n0.398\n0.382\n0.036\n0.381\nLesson 1\n0.433\n0.426\n0.017\n0.412\nLesson 2\n0.391\n0.384\n0.020\n0.366\nLesson 3\n0.396\n0.394\n0.010\n0.384\nTable 3  ###reference_### displays a comparative analysis of RMSE values from two hyperparameter tuning approaches for the XGBoost model: one selected by GPT-4 and the other via manual grid search. GPT-4 consistently yields lower minimum and median RMSE values across all three lessons compared to the manual method. However, GPT-4\u2019s method results in a lower mean RMSE value only for Lesson 2. The standard deviation values from GPT-4 are larger than those from the manual grid search, indicating a wider variability in RMSE outcomes. Additionally, the minimum and maximum range of values obtained through GPT-4\u2019s method exceed those from the manual approach, suggesting a greater spread in the performance results."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussions",
            "text": ""
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Efficient LLM-based Method for Predicting Learning Performance",
            "text": "Our study highlights the capabilities of GPT-4 in predicting learning performance in ITSs. We provide an in-depth examination of our experimental results regarding the application of GPT-4 for predicting learning performance within the CSAL AutoTutor datasets. Two exact predictive strategies, one leveraging the inherent heuristic reasoning method and the other utilizing available machine learning models, are implemented by GPT-4.\nWhen employing its heuristic-based reasoning approach, GPT-4 takes into account factors such as the perceived difficulty of questions and their attempt frequency. This approach does not rely on a fixed algorithm but uses logical reasoning to analyze historical performance data. It assumes that questions deemed more difficult are less likely to be answered correctly on the first try. Furthermore, if learners make multiple attempts on certain questions, GPT-4 interprets this as a sign of struggle with the material, leading to a more conservative performance prediction.\nAt the same time, enhancing GPT-4\u2019s predictive accuracy significantly involves incorporating reading comprehension materials, questions, and additional background information to craft context-specific prompts. By understanding the learning content and the questions\u2019 context, GPT-4 can offer explanations and leverage its vast knowledge base more effectively. This contextually enriched reasoning allows GPT-4 to outperform traditional learning performance prediction methods, such as BKT, PFA, SPARFA-Lite, and Tensor Factorization. The result is not just more accurate predictions, but also insights that are directly relevant and tailored to the specific learning scenario. This makes GPT-4 an invaluable tool for educators seeking to understand and improve student learning performance.\nWhen utilizing available machine learning models, GPT-4 demonstrates its ability by recommending and applying a range of machine learning models tailored to the specific needs of the data. Among these models are logistic regression, random forest, gradient boosting machine, and XGBoost. GPT-4\u2019s unique self-programming ability enables it to autonomously test these models and select the most effective one based on performance metrics from validation results. Through this process, XGBoost is identified as the most suitable model for predicting learning outcomes, leading to a novel approach in our experiments that combines the strengths of GPT-4 with XGBoost, referred to as GPT-4 with selected XGBoost. This approach remains adaptable, with GPT-4 continuously seeking to refine and enhance its choice of models. The fusion of GPT-4\u2019s capabilities with advanced machine learning techniques broadens its application scope, pushing the boundaries of what can be achieved in computational tasks. This not only showcases GPT-4\u2019s potential for complex problem-solving but also highlights its role in driving forward the evolution of ITSs."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Prompt Strategy for Predicting Learning Performance",
            "text": "In this study, the prompt engineering plays a crucial role. The foundational framework of our prompts encompasses encoding for the contextual representation of numerical values and decoding to facilitate LLM-based understanding, reasoning, and analysis in the generation of predictive outputs. This approach allows the LLM to seamlessly integrate all processes, from data input to final prediction. It employs self-search and self-optimization for refining prompt engineering, alongside semantic compiling techniques for processing learning performance data. The Chain-of-Thought prompt strategy [47  ###reference_bx47###] is employed to generate GPT-4 output that illustrates model reasoning and its interpretative process. By activating specific prompts within GPT-4, we guide it to more effectively analyze and interpret learner learning performance data. This method not only improves the transparency of the AI\u2019s decision-making process but also enhances the precision and relevance of its predictive capabilities.\nSpecifically, the Chain-of-Thought prompt strategy systematically maps out the reasoning steps necessary for predicting learning performance, employing a sequence of precisely tailored prompt compositions to ensure effective execution of each steps. These compositions encompass: (a) Presentation of Learning Materials: Share the learning materials and associated comprehension questions to establish a basis for analysis. (b) Contextual Transcriptions of Learning Performance Data: Provide a detailed contextual representation of the learning performance data. (c) Analysis Request: Clearly articulate the request for data analysis, specifying the desired insights or outcomes. (d) Method Selection: GPT-4 suggests appropriate analytical or machine learning methods based on the project needs. (e) Model Development: Assistance in developing a machine model, e.g., XGBoost, including training and validation across dataset folds. (f) Performance Evaluation: Calculation and presentation of validation outcomes, such as RMSE, for each fold. (g) Configuration Disclosure: Detailed sharing of the model\u2019s configuration settings for transparency and reproducibility. (h) Skill Assessment: Discussion on assessing learners\u2019 reading comprehension skills based on their performance data. (i) Optimization: Guidance on fine-tuning the model\u2019s hyperparameters for improved predictive performance. (j) Iterative Feedback: Continuous exchange for clarification, refinement, and further analysis based on user inputs and GPT-4\u2019s suggestions. For a comprehensive overview and detailed instructions, please refer to Appendix A  ###reference_1###."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "Although the present study highlights the potential of LLMs in enhancing predictive accuracy of learning performance prediction, it also identifies certain limitations. Future work remains in strengthening the connections between specific reading comprehension knowledge and skills and the reasoning process. There is a need to explore how these connections can be utilized to refine prompts and enhance predictive effectiveness. Specifically, constraints related to the fine-tuning of LLM-based platforms or APIs may hinder the optimization of models tailed for our dataset. Additionally, limitations in executing deep learning models restrict the application of advanced techniques such as Deep Knowledge Tracing (DKT) [67  ###reference_bx67###], Self-Attentive Knowledge Tracing (SAKT) [68  ###reference_bx68###], Dynamic Key-Value Memory Networks (DKVMN) [69  ###reference_bx69###], which may further improve the predictive accuracy."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Future Directions",
            "text": ""
        },
        {
            "section_id": "7.1",
            "parent_section_id": "7",
            "section_name": "LLMs for Knowledge Tracing in Learner Model",
            "text": "The potential of LLMs for knowledge tracing relies on at least two key aspects: firstly, their capability in identifying knowledge components [70  ###reference_bx70###, 71  ###reference_bx71###], which encapsulate the prerequisite knowledge for proficiently addressing specific questions with the given context; and secondly, their integration with machine learning models (self-selected by LLMs or external), which is further bolstered by LLMs\u2019 inherent interpretability, facilitating cohesive reasoning, assessment, and predictive capabilities concerning the learner performance. Further research in this direction holds significant promise for advancing our understanding and application of knowledge tracing methodologies based on LLMs."
        },
        {
            "section_id": "7.2",
            "parent_section_id": "7",
            "section_name": "LLM-based Trace of Learners\u2019 Learning for Intelligent Tutoring Systems",
            "text": "The present study\u2019s finding motivate future research into LLMs to augment and complement modeling of learner learning and dynamic learning states within ITSs. The endeavor involves utilizing diverse data types, including numerical, textual, and even multimodal inputs, to construct a comprehensive learner model. Drawing from LLMs like ChatGPT, Llama, and Gemini, along with various machine learning methods, future research could provide effective real-time prediction of learner learning. By integrating insights from LLMs and machine learning, this approach enhances the pedagogical component of Intelligent Tutoring Systems, enabling more precise instructional strategies and feedback mechanisms. Specifically, the present use of LLMs for learner modeling could be used in nascent applications for tutoring through Expectation-Misconception Tailored (EMT) conversation styles in adult literacy [6  ###reference_bx6###, 35  ###reference_bx35###, 72  ###reference_bx72###] or applications of LLMs to automatically generate peer-tutoring dialog[49  ###reference_bx49###]. Improving learner modeling through these applications could enable more personalized and effective pedagogical strategies and feedback for learners."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "The present study investigates the use of LLMs, specifically GPT-4, in predicting learner\u2019s learning performance in the context of adult literacy in Intelligent Tutoring Systems. We developed an LLM-based prediction method that integrates the encoding of numerical learning performance data into a contextual prompt, conducting data analysis and prediction within LLM component, and decoding the output to obtain predicted learning performance data. Compared to traditional machine learning methods, such as BKT, PFA, SPARFA-Lite and Tensor Factorization, our LLM-based method achieves higher predictive accuracy when incorporating with XGBoost (selected by GPT-4 in our study). Although XGBoost running on a local machine initially surpasses GPT-4 in accuracy, the optimization of XGBoost parameters selected by GPT-4 and fine-tuned within the LLM environment exhibits enhanced performance over local execution. Our examination of hyperparameter tuning by GPT-4 versus manual grid search reveals similar outcomes, yet the GPT-4 method introduces a degree of variability. Our findings underscore the potential of merging LLMs with established machine learning frameworks to boost personalization and efficacy in adult literacy education. This work lays the groundwork for future inquiries into the integration of LLMs in ITS environments, demonstrating the increased practicality of employing LLMs for learner performance prediction in AI-enhanced educational contexts."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix t0",
            "parent_section_id": null,
            "section_name": "Appendix A: Example Prompt Strategy for Predicting Learning Performance",
            "text": "User: Hello ChatGPT, I am working on a project related to adult literacy and have a dataset from the Center for the Study of Adult Literacy (CSAL) AutoTutor lessons. This dataset contains responses from adult learners engaged in reading comprehension activities. I seek your expertise in data mining and analysis to uncover insights and predict learning outcomes.\nI\u2019d be delighted to help you with your project on adult literacy. To get started and provide you with the most accurate assistance, I\u2019ll need some additional information about your dataset and your specific goals. Here are a few questions to guide our next steps:\n\n\n\u2022\n\nDataset Overview: Could you provide an overview of the dataset? For instance, what are the features (columns) in your dataset, and what types of data do they contain (numerical, categorical, text, etc.)?\n\n\u2022\n\nLearning Performance: What specific learning performance are you interested in predicting? For example, are you looking to predict overall improvement in literacy levels, comprehension scores after lessons, or something else?\n\n\u2022\n\nAnalysis Goals: Beyond predicting learning performance, are there any specific insights or patterns you\u2019re hoping to uncover? This might include identifying the most challenging lessons, understanding which factors contribute most to literacy improvement, or anything else relevant to your project.\nOnce you provide more details, we can discuss potential data mining techniques and analytical approaches that could be beneficial for your project \u2026\nUser: Here is the information for one CSAL lesson. The topic of the lesson resolves around Problems and Solution for testing adult reading comprehension. The reading content is titled \u2019Minor Burns\u2019. We assume the difficulty level of this lesson and questions as Medium.\nThe reading material for the current lesson is in following:{Reading Material}. All the questions and corresponding answers are as follows: {Questions}. Could you provide a mapping of the relationship between the knowledge required for each question, based on the lesson content, and the reading skills necessary to comprehend and master this knowledge?\nGPT-4: To map the relationship between the knowledge required for each question based on the \"Minor Burns\" lesson content, we\u2019ll examine the questions and identify the specific pieces of information or skills needed to answer them correctly. This process involves linking each question to the relevant parts of the text and categorizing the type of knowledge or cognitive skill they test, such as factual knowledge, comprehension, application, or analysis. Knowledge Mapping:\n\n\n\u2022\n\nQuestion 1: What is the topic of this text? \nKnowledge Required: Factual knowledge about the text\u2019s main topic. \nRelated Text Content: The entire reading is focused on minor burns. \nSkill Type: Identification of main idea.\n\n\n \nThis mapping highlights that successfully answering the questions requires a mix of factual knowledge, comprehension, application, analysis, and evaluation skills. Each question is tied to specific content within the \"Minor Burns\" lesson, demanding learners to engage with the material at different cognitive levels \u2026\nUser: This dataset is organized into five distinct sections, each comprising a training and a testing dataset. These datasets feature four key columns: learner ID, Question ID, Attempts, and Obs (actual performance). Within the Obs column, a performance score of 0 signifies a completely incorrect answer, while a score of 1 denotes a completely correct answer, in relation to the previously mentioned questions and their respective answers. All historical performance data are encapsulated within the training dataset, whereas data awaiting prediction are contained within the testing dataset. The task involves utilizing the training dataset to fine-tune the GPT-4 model, applying the testing dataset for validation, and predicting learner performance as reflected in the Obs column. \nNotes for consideration:\n\n\n1.\n\nI should rely on my analytical and computational skills for making these predictions.\n\n2.\n\nIt\u2019s requested that I calculate a continuous likelihood score between 0 and 1 for each learner\u2019s chance of correctly answering a question, based on the learner ID and Question ID. This score should directly correlate with the likelihood of a correct or incorrect answer, adhering to the criteria outlined in the dataset\u2019s description.\n\n\n{Add \u201cAvoid using the machine learning models.\" for GPT-4 model.}\n{Attached Dataset Files}\nGPT-4: To begin analyzing and predicting learner performance using the provided datasets, I\u2019ll start by examining one of the training datasets to understand its structure and content. This initial analysis will help me determine the appropriate data preprocessing steps and model training approach\u2026 \n{Compile Dataset Files}\n\n\u22ee\nGiven the nature of the data (categorical IDs for learners and questions without additional metadata), the model will need to capture patterns based on historical performance metrics associated with specific learners and questions.\n\nFor predicting learner performance on reading comprehension questions based on the dataset provided, we can explore a variety of methods. These can range from heuristic approaches to sophisticated machine learning models. {Suggest Methods} \nLet\u2019s outline potential methods within these categories:\n\n\n1.\n\nHeuristic Methods \u2026\n\n2.\n\nMachine Learning Models \u2026\n\n\n\u22eeFor simplicity and efficiency, we\u2019ll begin with a model that doesn\u2019t require extensive preprocessing of these categorical features. A gradient boosting machine, such as XGBoost, can handle numerical categorical variables directly and is well-suited for this task due to its ability to model complex relationships and provide probability scores for outcomes. \n    \u22ee\n{Alternatively, GPT-4 is capable of autonomously selecting the optimal prediction method, with logistic regression and XGBoost being the most commonly utilized approaches based on our experiments.}\nThis is one optional step:\n{User: Request to verify dataset information and details to ensure GPT-4 comprehends the data and can perform fundamental statistical analyses. For instance, calculate the number of students, the number of questions, and attempts, including the distribution and frequency of these elements, among others. }\n{GPT-4: Showcase its understanding and analytical findings in response to your request \u2026}\nUser: Could you assist in employing XGBoost regression for model development and prediction tasks across the five folds? I would appreciate if you could also provide the validation outcomes, specifically Root Mean Squared Error (RMSE), for each fold. Additionally, could you detail the specific configuration of the XGBoost algorithm utilized in this analysis?\n\n{If you opt to not use machine learning models and instead rely on GPT-4\u2019s intrinsic reasoning and computational abilities for prediction, you can request GPT-4 to proceed using its heuristic approach in formulating responses.}\nGPT 4: Using a very simplified XGBoost regression model for the first fold, we successfully completed the training and evaluation process within the computational constraints. Here are the updated results for all five folds in following:\n\n\n\u2022\n\nFold 1:\n\n\u2022\n\nFold 2:\n\n\u2022\n\nFold 3:\n\n\n\u2022\n\nFold 4:\n\n\u2022\n\nFold 5:\n\n\n\u22ee\nUser: Can you help me present the model\nsetting?\nGPT-4: For the evaluations performed across the five folds, the XGBoost regression model was configured with the following settings to balance the execution time and predictive performance within the operational constraints:\n\n\n\u2022\n\nn_estimators:\u2026\n\n\u2022\n\nmax_depth:\u2026\n\n\u2022\n\nlearning_rate:\n\n\n\u22ee\nUser: Is there any reading comprehension skills for answering the questions? Can you help me assess the learners reading comprehension skills?\nGPT-4: To assess a learner\u2019s reading comprehension skills based on the performance data provided, I will first need to analyze the dataset for patterns in their responses to questions that we can infer are related to reading comprehension. Given the structure of the dataset, I can attempt to identify: \u2026\n{Fine-tuning Model} \nUser: Could you assist in optimizing the model by fine-tuning its hyperparameters to achieve enhanced predictive performance?\nGPT-4: Optimizing an XGBoost model involves tuning various hyperparameters to find the best combination that minimizes prediction error on your validation set. The most common hyperparameters to adjust include: \u2026 \nLet\u2019s start by selecting a set of hyperparameters to tune for the XGBoost model\u2026\nUser: {Other prompts that you need}\nGPT-4: {Other responses}"
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T1\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Dataset from the CSAL AutoTutor lessons</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.1\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S3.T1.1.1.1.1\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.1.1.1.1.1\">Dataset</p>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S3.T1.1.1.1.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.1.1.1.2.1\">Lesson Name</p>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T1.1.1.1.3\"># Learners</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T1.1.1.1.4\"># Questions</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T1.1.1.1.5\">Max. Attempt</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.2.1\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.1.2.1.1\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.1.2.1.1.1\">Lesson 1</p>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.1.2.1.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.1.2.1.2.1\">Persuasive Text</p>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.2.1.3\">66</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.2.1.4\">8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.2.1.5\">9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.3.2\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_row\" id=\"S3.T1.1.3.2.1\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.1.3.2.1.1\">Lesson 2</p>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_row\" id=\"S3.T1.1.3.2.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.1.3.2.2.1\">Cause and Effect</p>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.3.2.3\">68</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.3.2.4\">9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.3.2.5\">9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.4.3\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_bb\" id=\"S3.T1.1.4.3.1\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.1.4.3.1.1\">Lesson 3</p>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_bb\" id=\"S3.T1.1.4.3.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.1.4.3.2.1\">Problems and Solution</p>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.1.4.3.3\">86</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.1.4.3.4\">11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.1.4.3.5\">5</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 1: Dataset from the CSAL AutoTutor lessons"
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T2\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Comparison of model performance using RMSE with the standard error from five-folds cross validation</figcaption>\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T2.22\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1\">\n<th class=\"ltx_td ltx_nopad ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S4.T2.1.1.1\"><svg height=\"19.07\" overflow=\"visible\" version=\"1.1\" width=\"91.56\"><g transform=\"translate(0,19.07) scale(1,-1)\"><path d=\"M 0,19.07 91.56,0\" stroke=\"black\" stroke-width=\"0.4\"></path><g class=\"ltx_svg_fog\" transform=\"translate(0,0)\"><g transform=\"translate(0,9.61) scale(1, -1)\"><foreignobject height=\"9.61\" overflow=\"visible\" width=\"43.13\">\n<span class=\"ltx_inline-block\" id=\"S4.T2.1.1.1.pic1.1.1\">\n<span class=\"ltx_inline-block ltx_align_left\" id=\"S4.T2.1.1.1.pic1.1.1.1\">\n<span class=\"ltx_p\" id=\"S4.T2.1.1.1.pic1.1.1.1.1\">Models</span>\n</span>\n</span></foreignobject></g></g><g class=\"ltx_svg_fog\" transform=\"translate(45.78,9.61)\"><g transform=\"translate(0,9.46) scale(1, -1)\"><foreignobject height=\"9.46\" overflow=\"visible\" width=\"45.78\">\n<span class=\"ltx_inline-block\" id=\"S4.T2.1.1.1.pic1.2.1\">\n<span class=\"ltx_inline-block ltx_align_right\" id=\"S4.T2.1.1.1.pic1.2.1.1\">\n<span class=\"ltx_p\" id=\"S4.T2.1.1.1.pic1.2.1.1.1\">Lessons</span>\n</span>\n</span></foreignobject></g></g></g></svg></th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T2.1.1.2\">\n<table class=\"ltx_tabular ltx_align_top\" id=\"S4.T2.1.1.2.1\">\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.2.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.2.1.1.1\">Lesson 1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.2.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.2.1.2.1\">(RMSE)</td>\n</tr>\n</table>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T2.1.1.3\">\n<table class=\"ltx_tabular ltx_align_top\" id=\"S4.T2.1.1.3.1\">\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.3.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.3.1.1.1\">Lesson 2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.3.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.3.1.2.1\">(RMSE)</td>\n</tr>\n</table>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T2.1.1.4\">\n<table class=\"ltx_tabular ltx_align_top\" id=\"S4.T2.1.1.4.1\">\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.4.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.4.1.1.1\">Lesson 3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.4.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.4.1.2.1\">(RMSE)</td>\n</tr>\n</table>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T2.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T2.4.4.4\">BKT</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S4.T2.2.2.1\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T2.2.2.1.1.1\"></p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S4.T2.3.3.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T2.3.3.2.1.1\"></p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S4.T2.4.4.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T2.4.4.3.1.1\"></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.7.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T2.7.7.4\">PFA</th>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T2.5.5.1\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T2.5.5.1.1.1\"></p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T2.6.6.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T2.6.6.2.1.1\"></p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T2.7.7.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T2.7.7.3.1.1\"></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.10.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T2.10.10.4\">SPARFA-Lite</th>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T2.8.8.1\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T2.8.8.1.1.1\"></p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T2.9.9.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T2.9.9.2.1.1\"></p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T2.10.10.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T2.10.10.3.1.1\"></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.13.13\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T2.13.13.4\">Tensor Factorization</th>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T2.11.11.1\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T2.11.11.1.1.1\"></p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T2.12.12.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T2.12.12.2.1.1\"></p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T2.13.13.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T2.13.13.3.1.1\"></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.16.16\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T2.16.16.4\">XGBoost</th>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T2.14.14.1\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T2.14.14.1.1.1\"></p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T2.15.15.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T2.15.15.2.1.1\"></p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T2.16.16.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T2.16.16.3.1.1\"></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.19.19\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T2.19.19.4\">GPT-4</th>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T2.17.17.1\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T2.17.17.1.1.1\"></p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T2.18.18.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T2.18.18.2.1.1\"></p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T2.19.19.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T2.19.19.3.1.1\"></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.22.22\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S4.T2.22.22.4\">XGBoost (selected by GPT-4)</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S4.T2.20.20.1\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T2.20.20.1.1.1\"></p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S4.T2.21.21.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T2.21.21.2.1.1\"></p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S4.T2.22.22.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T2.22.22.3.1.1\"></p>\n</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 2: Comparison of model performance using RMSE with the standard error from five-folds cross validation"
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T3\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Comparison of RMSE of hyper-parameter tuning methods for XGBoost (Note: the Std. is the abbreviation of standard deviation).</figcaption>\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T3.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.1\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.1.1.1.1\" style=\"width:105.3pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S4.T3.1.1.1.1.1\">Methods</span></th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.1.1.1.2\" style=\"width:39.3pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center ltx_align_top\" id=\"S4.T3.1.1.1.2.1\">Lessons</span></th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.1.1.1.3\" style=\"width:39.3pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center ltx_align_top\" id=\"S4.T3.1.1.1.3.1\">Mean</span></th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.1.1.1.4\" style=\"width:39.3pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center ltx_align_top\" id=\"S4.T3.1.1.1.4.1\">Median</span></th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.1.1.1.5\" style=\"width:39.3pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center ltx_align_top\" id=\"S4.T3.1.1.1.5.1\">Std.</span></th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.1.1.1.6\" style=\"width:39.3pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center ltx_align_top\" id=\"S4.T3.1.1.1.6.1\">Min.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.1.1.1.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.7.1\">Max.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T3.1.2.1\">\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S4.T3.1.2.1.1\" rowspan=\"3\" style=\"width:105.3pt;\"><span class=\"ltx_text ltx_align_top\" id=\"S4.T3.1.2.1.1.1\">\n<span class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T3.1.2.1.1.1.1\">\n<span class=\"ltx_tr\" id=\"S4.T3.1.2.1.1.1.1.1\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T3.1.2.1.1.1.1.1.1\">Hyper-parameters tuning</span></span>\n<span class=\"ltx_tr\" id=\"S4.T3.1.2.1.1.1.1.2\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T3.1.2.1.1.1.1.2.1\">by GPT-4</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S4.T3.1.2.1.2\" style=\"width:39.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.2.1.2.1\">Lesson 1</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S4.T3.1.2.1.3\" style=\"width:39.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.2.1.3.1\">0.435</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S4.T3.1.2.1.4\" style=\"width:39.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.2.1.4.1\">0.422</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S4.T3.1.2.1.5\" style=\"width:39.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.2.1.5.1\">0.053</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S4.T3.1.2.1.6\" style=\"width:39.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.2.1.6.1\">0.398</p>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.2.1.7\">0.552</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.3.2\">\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T3.1.3.2.1\" style=\"width:39.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.3.2.1.1\">Lesson 2</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T3.1.3.2.2\" style=\"width:39.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.3.2.2.1\">0.376</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T3.1.3.2.3\" style=\"width:39.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.3.2.3.1\">0.361</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T3.1.3.2.4\" style=\"width:39.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.3.2.4.1\">0.033</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T3.1.3.2.5\" style=\"width:39.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.3.2.5.1\">0.351</p>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.3.2.6\">0.444</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.4.3\">\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T3.1.4.3.1\" style=\"width:39.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.4.3.1.1\">Lesson 3</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T3.1.4.3.2\" style=\"width:39.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.4.3.2.1\">0.398</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T3.1.4.3.3\" style=\"width:39.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.4.3.3.1\">0.382</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T3.1.4.3.4\" style=\"width:39.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.4.3.4.1\">0.036</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T3.1.4.3.5\" style=\"width:39.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.4.3.5.1\">0.381</p>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.4.3.6\">0.480</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.5.4\">\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_t\" id=\"S4.T3.1.5.4.1\" rowspan=\"3\" style=\"width:105.3pt;\"><span class=\"ltx_text ltx_align_top\" id=\"S4.T3.1.5.4.1.1\">\n<span class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T3.1.5.4.1.1.1\">\n<span class=\"ltx_tr\" id=\"S4.T3.1.5.4.1.1.1.1\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T3.1.5.4.1.1.1.1.1\">Hyper-parameters tuning</span></span>\n<span class=\"ltx_tr\" id=\"S4.T3.1.5.4.1.1.1.2\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T3.1.5.4.1.1.1.2.1\">by manual grid search</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S4.T3.1.5.4.2\" style=\"width:39.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.5.4.2.1\">Lesson 1</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S4.T3.1.5.4.3\" style=\"width:39.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.5.4.3.1\">0.433</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S4.T3.1.5.4.4\" style=\"width:39.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.5.4.4.1\">0.426</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S4.T3.1.5.4.5\" style=\"width:39.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.5.4.5.1\">0.017</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S4.T3.1.5.4.6\" style=\"width:39.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.5.4.6.1\">0.412</p>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.5.4.7\">0.484</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.6.5\">\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T3.1.6.5.1\" style=\"width:39.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.6.5.1.1\">Lesson 2</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T3.1.6.5.2\" style=\"width:39.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.6.5.2.1\">0.391</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T3.1.6.5.3\" style=\"width:39.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.6.5.3.1\">0.384</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T3.1.6.5.4\" style=\"width:39.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.6.5.4.1\">0.020</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T3.1.6.5.5\" style=\"width:39.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.6.5.5.1\">0.366</p>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.6.5.6\">0.423</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.7.6\">\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S4.T3.1.7.6.1\" style=\"width:39.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.7.6.1.1\">Lesson 3</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S4.T3.1.7.6.2\" style=\"width:39.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.7.6.2.1\">0.396</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S4.T3.1.7.6.3\" style=\"width:39.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.7.6.3.1\">0.394</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S4.T3.1.7.6.4\" style=\"width:39.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.7.6.4.1\">0.010</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S4.T3.1.7.6.5\" style=\"width:39.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.7.6.5.1\">0.384</p>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.1.7.6.6\">0.433</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 3: Comparison of RMSE of hyper-parameter tuning methods for XGBoost (Note: the Std. is the abbreviation of standard deviation)."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.14668v1_figure_1.png",
            "caption": "Figure 1: The interface of AutoTutor for the Center for the Study of Adult Literacy."
        },
        "2": {
            "figure_path": "2403.14668v1_figure_2.png",
            "caption": "Figure 2: LLM-based prediction framework for learner learning performance."
        }
    },
    "references": [
        {
            "1": {
                "title": "\u201cImproving adult literacy instruction: Options for practice and research\u201d",
                "author": "National Research Council",
                "venue": "National Academies Press, 2012",
                "url": null
            }
        },
        {
            "2": {
                "title": "\u201cLiteracy, lives and learning\u201d",
                "author": "David Barton et al.",
                "venue": "Routledge, 2012",
                "url": null
            }
        },
        {
            "3": {
                "title": "\u201cReading comprehension\u201d",
                "author": "Gary Woolley and Gary Woolley",
                "venue": "Springer, 2011",
                "url": null
            }
        },
        {
            "4": {
                "title": "\u201cDiagnostic Assessment of Adults\u2019 Reading Deficiencies in an Intelligent Tutoring System.\u201d",
                "author": "Genghu Shi et al.",
                "venue": "In ITS Workshops, 2018, pp. 105\u2013112",
                "url": null
            }
        },
        {
            "5": {
                "title": "\u201cA conversation-based intelligent tutoring system benefits adult readers with Low literacy skills\u201d",
                "author": "Ying Fang et al.",
                "venue": "In Adaptive Instructional Systems: First International Conference, AIS 2019, Held as Part of the 21st HCI International Conference, HCII 2019, Orlando, FL, USA, July 26\u201331, 2019, Proceedings 21, 2019, pp. 604\u2013614",
                "url": null
            }
        },
        {
            "6": {
                "title": "\u201cAutoTutor: A tutor with dialogue in natural language\u201d",
                "author": "Arthur C Graesser et al.",
                "venue": "In Behavior Research Methods, Instruments, & Computers 36",
                "url": null
            }
        },
        {
            "7": {
                "title": "\u201cAutoTutor and family: A review of 17 years of natural language tutoring\u201d",
                "author": "Benjamin D Nye, Arthur C Graesser and Xiangen Hu",
                "venue": "In International Journal of Artificial Intelligence in Education 24",
                "url": null
            }
        },
        {
            "8": {
                "title": "\u201cUsing AutoTutor to track performance and engagement in a reading comprehension intervention for adult literacy students\u201d",
                "author": "Arthur C Graesser, Daphne Greenberg, Jan C Frijters and Amani Talwar",
                "venue": "In Revista Signos. Estudios de Ling\u00fc\u00edstica 54.107, 2021",
                "url": null
            }
        },
        {
            "9": {
                "title": "\u201cIntelligent tutoring systems\u201d",
                "author": "Albert T Corbett, Kenneth R Koedinger and John R Anderson",
                "venue": "In Handbook of human-computer interaction",
                "url": null
            }
        },
        {
            "10": {
                "title": "\u201cIntelligent tutoring systems.\u201d",
                "author": "Arthur C Graesser, Mark W Conley and Andrew Olney",
                "venue": "American Psychological Association, 2012",
                "url": null
            }
        },
        {
            "11": {
                "title": "\u201cKnowledge tracing: Modeling the acquisition of procedural knowledge\u201d",
                "author": "Albert T Corbett and John R Anderson",
                "venue": "In User modeling and user-adapted interaction 4",
                "url": null
            }
        },
        {
            "12": {
                "title": "\u201cLogistic knowledge tracing: A constrained framework for learner modeling\u201d",
                "author": "Philip I Pavlik, Luke G Eglington and Leigh M Harrell-Williams",
                "venue": "In IEEE Transactions on Learning Technologies 14.5",
                "url": null
            }
        },
        {
            "13": {
                "title": "\u201cMathprompter: Mathematical reasoning using large language models\u201d",
                "author": "Shima Imani, Liang Du and Harsh Shrivastava",
                "venue": "In arXiv preprint arXiv:2303.05398, 2023",
                "url": null
            }
        },
        {
            "14": {
                "title": "\u201cLarge Language Models for Mathematical Reasoning: Progresses and Challenges\u201d",
                "author": "Janice Ahn et al.",
                "venue": "In arXiv preprint arXiv:2402.00157, 2024",
                "url": null
            }
        },
        {
            "15": {
                "title": "\u201cTime-llm: Time series forecasting by reprogramming large language models\u201d",
                "author": "Ming Jin et al.",
                "venue": "In arXiv preprint arXiv:2310.01728, 2023",
                "url": null
            }
        },
        {
            "16": {
                "title": "\u201cLarge language models are zero-shot time series forecasters\u201d",
                "author": "Nate Gruver, Marc Finzi, Shikai Qiu and Andrew Gordon Wilson",
                "venue": "In arXiv preprint arXiv:2310.07820, 2023",
                "url": null
            }
        },
        {
            "17": {
                "title": "\u201cLarge Language Models for Time Series: A Survey\u201d",
                "author": "Xiyuan Zhang, Ranak Roy Chowdhury, Rajesh K Gupta and Jingbo Shang",
                "venue": "In arXiv preprint arXiv:2402.01801, 2024",
                "url": null
            }
        },
        {
            "18": {
                "title": "\u201cOpen-ended knowledge tracing for computer science education\u201d",
                "author": "Naiming Liu, Zichao Wang, Richard Baraniuk and Andrew Lan",
                "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, 2022, pp. 3849\u20133862",
                "url": null
            }
        },
        {
            "19": {
                "title": "\u201cBeyond Predictive Learning Analytics Modelling and onto Explainable Artificial Intelligence with Prescriptive Analytics and ChatGPT\u201d",
                "author": "Teo Susnjak",
                "venue": "In International Journal of Artificial Intelligence in Education",
                "url": null
            }
        },
        {
            "20": {
                "title": "\u201cGpt-4 technical report\u201d",
                "author": "Josh Achiam et al.",
                "venue": "In arXiv preprint arXiv:2303.08774, 2023",
                "url": null
            }
        },
        {
            "21": {
                "title": "\u201cIndividualized bayesian knowledge tracing models\u201d",
                "author": "Michael V Yudelson, Kenneth R Koedinger and Geoffrey J Gordon",
                "venue": "In Artificial Intelligence in Education: 16th International Conference, AIED 2013, Memphis, TN, USA, July 9-13, 2013. Proceedings 16, 2013, pp. 171\u2013180",
                "url": null
            }
        },
        {
            "22": {
                "title": "\u201cPerformance Factors Analysis\u2013A New Alternative to Knowledge Tracing.\u201d",
                "author": "Philip I Pavlik Jr, Hao Cen and Kenneth R Koedinger",
                "venue": "In Online Submission",
                "url": null
            }
        },
        {
            "23": {
                "title": "\u201cQuantized matrix completion for personalized learning\u201d",
                "author": "Andrew S Lan, Christoph Studer and Richard G Baraniuk",
                "venue": "In arXiv preprint arXiv:1412.5968, 2014",
                "url": null
            }
        },
        {
            "24": {
                "title": "\u201cRank-based tensor factorization for student performance prediction\u201d",
                "author": "Thanh-Nam Doan and Shaghayegh Sahebi",
                "venue": "In 12th International Conference on Educational Data Mining (EDM), 2019",
                "url": null
            }
        },
        {
            "25": {
                "title": "\u201cXgboost: extreme gradient boosting\u201d",
                "author": "Tianqi Chen et al.",
                "venue": "In R package version 0.4-2 1.4, 2015, pp. 1\u20134",
                "url": null
            }
        },
        {
            "26": {
                "title": "\u201cEnhancing the prediction of student performance based on the machine learning XGBoost algorithm\u201d",
                "author": "Amal Asselman, Mohamed Khaldi and Souhaib Aammou",
                "venue": "In Interactive Learning Environments 31.6",
                "url": null
            }
        },
        {
            "27": {
                "title": "\u201cReading comprehension lessons in AutoTutor for the Center for the Study of Adult Literacy\u201d",
                "author": "Arthur C Graesser et al.",
                "venue": "In Adaptive educational technologies for literacy instruction",
                "url": null
            }
        },
        {
            "28": {
                "title": "\u201cEducational technologies that support reading comprehension for adults who have low literacy skills\u201d",
                "author": "Arthur C Graesser, Daphne Greenberg, Andrew Olney and Maureen W Lovett",
                "venue": "In The Wiley handbook of adult literacy",
                "url": null
            }
        },
        {
            "29": {
                "title": "\u201cTwo heads may be better than one: Learning from computer agents in conversational trialogues\u201d",
                "author": "Arthur C Graesser, Carol M Forsyth and Blair A Lehman",
                "venue": "In Teachers College Record 119.3",
                "url": null
            }
        },
        {
            "30": {
                "title": "\u201cPatterns of adults with low literacy skills interacting with an intelligent tutoring system\u201d",
                "author": "Ying Fang et al.",
                "venue": "In International Journal of Artificial Intelligence in Education",
                "url": null
            }
        },
        {
            "31": {
                "title": "\u201cClustering the Learning Patterns of Adults with Low Literacy Skills Interacting with an Intelligent Tutoring System.\u201d",
                "author": "Ying Fang et al.",
                "venue": "In Grantee Submission",
                "url": null
            }
        },
        {
            "32": {
                "title": "\u201cUsing an adaptive intelligent tutoring system to promote learning affordances for adults with low literacy skills\u201d",
                "author": "Anne Lippert, Jessica Gatewood, Zhiqiang Cai and Arthur C Graesser",
                "venue": "In Adaptive Instructional Systems: First International Conference, AIS 2019, Held as Part of the 21st HCI International Conference, HCII 2019, Orlando, FL, USA, July 26\u201331, 2019, Proceedings 21, 2019, pp. 327\u2013339",
                "url": null
            }
        },
        {
            "33": {
                "title": "\u201cTechnology use and integration in adult education and literacy classrooms\u201d, 2019",
                "author": "Glenda Lynn Rose, Chih-Wei Wang, Amanda Sainz and Suresh Joshi",
                "venue": null,
                "url": null
            }
        },
        {
            "34": {
                "title": "\u201cAssessment with computer agents that engage in conversational dialogues and trialogues with learners\u201d",
                "author": "Arthur C Graesser, Zhiqiang Cai, Brent Morgan and Lijia Wang",
                "venue": "In Computers in Human Behavior 76",
                "url": null
            }
        },
        {
            "35": {
                "title": "\u201cComputerized Learning Environments That Incorporate Research in Discourse Psychology, Cognitive Science, and Computational Linguistics.\u201d",
                "author": "Arthur C Graesser, Xiangen Hu and Danielle S McNamara",
                "venue": "American Psychological Association, 2005",
                "url": null
            }
        },
        {
            "36": {
                "title": "\u201cExploring an intelligent tutoring system as a conversation-based assessment tool for reading comprehension\u201d",
                "author": "Genghu Shi et al.",
                "venue": "In Behaviormetrika 45",
                "url": null
            }
        },
        {
            "37": {
                "title": "\u201cExploring the Individual Differences in Multidimensional Evolution of Knowledge States of Learners\u201d",
                "author": "Liang Zhang et al.",
                "venue": "In International Conference on Human-Computer Interaction, 2023, pp. 265\u2013284",
                "url": null
            }
        },
        {
            "38": {
                "title": "\u201cWheel-spinning: Students who fail to master a skill\u201d",
                "author": "Joseph E Beck and Yue Gong",
                "venue": "In Artificial Intelligence in Education: 16th International Conference, AIED 2013, Memphis, TN, USA, July 9-13, 2013. Proceedings 16, 2013, pp. 431\u2013440",
                "url": null
            }
        },
        {
            "39": {
                "title": "\u201cStudent modeling in intelligent tutoring systems\u201d, 2014",
                "author": "Yue Gong",
                "venue": null,
                "url": null
            }
        },
        {
            "40": {
                "title": "\u201cA review of recent advances in learner and skill modeling in intelligent learning environments\u201d",
                "author": "Michel C Desmarais and Ryan SJ d Baker",
                "venue": "In User Modeling and User-Adapted Interaction 22",
                "url": null
            }
        },
        {
            "41": {
                "title": "\u201cA Review of Learner Models Used in Intelligent Tutoring Systems\u201d",
                "author": "Philip I Pavlik Jr, Keith Brawner, Andrew Olney and Antonija Mitrovic",
                "venue": "In Design Recommendations for Intelligent Tutoring Systems: Volume 1-Learner Modeling 1",
                "url": null
            }
        },
        {
            "42": {
                "title": "\u201cAutomated Search Improves Logistic Knowledge Tracing, Surpassing Deep Learning in Accuracy and Explainability\u201d",
                "author": "Philip Irvin Pavlik Jr and Luke G Eglington",
                "venue": "In Journal of Educational Data Mining 15.3, 2023, pp. 58\u201386",
                "url": null
            }
        },
        {
            "43": {
                "title": "\u201cInstructional Factors Analysis: A Cognitive Model For Multiple Instructional Interventions.\u201d",
                "author": "Min Chi et al.",
                "venue": "In EDM 2011",
                "url": null
            }
        },
        {
            "44": {
                "title": "\u201cBayesian knowledge tracing, logistic models, and beyond: an overview of learner modeling techniques\u201d",
                "author": "Radek Pel\u00e1nek",
                "venue": "In User Modeling and User-Adapted Interaction 27",
                "url": null
            }
        },
        {
            "45": {
                "title": "\u201cSparse factor analysis for learning and content analytics\u201d",
                "author": "Andrew S Lan, Andrew E Waters, Christoph Studer and Richard G Baraniuk",
                "venue": "In arXiv preprint arXiv:1303.5685, 2013",
                "url": null
            }
        },
        {
            "46": {
                "title": "\u201cTowards reasoning in large language models: A survey\u201d",
                "author": "Jie Huang and Kevin Chen-Chuan Chang",
                "venue": "In arXiv preprint arXiv:2212.10403, 2022",
                "url": null
            }
        },
        {
            "47": {
                "title": "\u201cChain-of-thought prompting elicits reasoning in large language models\u201d",
                "author": "Jason Wei et al.",
                "venue": "In Advances in Neural Information Processing Systems 35, 2022, pp. 24824\u201324837",
                "url": null
            }
        },
        {
            "48": {
                "title": "\u201cCan large language models provide feedback to students? A case study on ChatGPT\u201d",
                "author": "Wei Dai et al.",
                "venue": "In 2023 IEEE International Conference on Advanced Learning Technologies (ICALT), 2023, pp. 323\u2013325",
                "url": null
            }
        },
        {
            "49": {
                "title": "\u201cRuffle&Riley: Towards the Automated Induction of Conversational Tutoring Systems\u201d",
                "author": "Robin Schmucker, Meng Xia, Amos Azaria and Tom Mitchell",
                "venue": "In arXiv preprint arXiv:2310.01420, 2023",
                "url": null
            }
        },
        {
            "50": {
                "title": "\u201cImproving Assessment of Tutoring Practices using Retrieval-Augmented Generation\u201d",
                "author": "Jionghao Lin et al.",
                "venue": "In arXiv preprint arXiv:2402.14594, 2024",
                "url": null
            }
        },
        {
            "51": {
                "title": "\u201cLarge Language Model-Driven Classroom Flipping: Empowering Student-Centric Peer Questioning with Flipped Interaction\u201d",
                "author": "Chee Wei Tan",
                "venue": "In arXiv preprint arXiv:2311.14708, 2023",
                "url": null
            }
        },
        {
            "52": {
                "title": "\u201cEvaluating reading comprehension exercises generated by LLMs: A showcase of ChatGPT in education applications\u201d",
                "author": "Changrong Xiao et al.",
                "venue": "In Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023), 2023, pp. 610\u2013625",
                "url": null
            }
        },
        {
            "53": {
                "title": "\u201cEvaluating the logical reasoning ability of chatgpt and gpt-4\u201d",
                "author": "Hanmeng Liu et al.",
                "venue": "In arXiv preprint arXiv:2304.03439, 2023",
                "url": null
            }
        },
        {
            "54": {
                "title": "\u201cKnowledge tracing: A survey\u201d",
                "author": "Ghodai Abdelrahman, Qing Wang and Bernardo Nunes",
                "venue": "In ACM Computing Surveys 55.11",
                "url": null
            }
        },
        {
            "55": {
                "title": "\u201cModeling individualization in a bayesian networks implementation of knowledge tracing\u201d",
                "author": "Zachary A Pardos and Neil T Heffernan",
                "venue": "In International conference on user modeling, adaptation, and personalization, 2010, pp. 255\u2013266",
                "url": null
            }
        },
        {
            "56": {
                "title": "\u201cKT-IDEM: Introducing item difficulty to the knowledge tracing model\u201d",
                "author": "Zachary A Pardos and Neil T Heffernan",
                "venue": "In User Modeling, Adaption and Personalization: 19th International Conference, UMAP 2011, Girona, Spain, July 11-15, 2011. Proceedings 19, 2011, pp. 243\u2013254",
                "url": null
            }
        },
        {
            "57": {
                "title": "\u201cUser modeling\u2013a notoriously black art\u201d",
                "author": "Michael Yudelson, Philip I Pavlik and Kenneth R Koedinger",
                "venue": "In User Modeling, Adaption and Personalization: 19th International Conference, UMAP 2011, Girona, Spain, July 11-15, 2011. Proceedings 19, 2011, pp. 317\u2013328",
                "url": null
            }
        },
        {
            "58": {
                "title": "\u201cHow to optimize student learning using student models that adapt rapidly to individual differences\u201d",
                "author": "Luke G Eglington and Philip I Pavlik Jr",
                "venue": "In International Journal of Artificial Intelligence in Education",
                "url": null
            }
        },
        {
            "59": {
                "title": "\u201cComparing knowledge tracing and performance factor analysis by using multiple model fitting procedures\u201d",
                "author": "Yue Gong, Joseph E Beck and Neil T Heffernan",
                "venue": "In Intelligent Tutoring Systems: 10th International Conference, ITS 2010, Pittsburgh, PA, USA, June 14-18, 2010, Proceedings, Part I 10, 2010, pp. 35\u201344",
                "url": null
            }
        },
        {
            "60": {
                "title": "\u201cKnowledge tracing for complex problem solving: Granular rank-based tensor factorization\u201d",
                "author": "Chunpai Wang et al.",
                "venue": "In Proceedings of the 29th ACM conference on user modeling, adaptation and personalization, 2021, pp. 179\u2013188",
                "url": null
            }
        },
        {
            "61": {
                "title": "\u201c3DG: A Framework for Using Generative AI for Handling Sparse Learner Performance Data From Intelligent Tutoring Systems\u201d",
                "author": "Liang Zhang et al.",
                "venue": "In arXiv preprint arXiv:2402.01746, 2024",
                "url": null
            }
        },
        {
            "62": {
                "title": "\u201cAn XGBoost-Based Knowledge Tracing Modelf\u201d",
                "author": "Wei Su et al.",
                "venue": "In International Journal of Computational Intelligence Systems 16.1",
                "url": null
            }
        },
        {
            "63": {
                "title": "\u201cXgboost: A scalable tree boosting system\u201d",
                "author": "Tianqi Chen and Carlos Guestrin",
                "venue": "In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining, 2016, pp. 785\u2013794",
                "url": null
            }
        },
        {
            "64": {
                "title": "\u201cGoing deeper with deep knowledge tracing.\u201d",
                "author": "Xiaolu Xiong, Siyuan Zhao, Eric G Van Inwegen and Joseph E Beck",
                "venue": "In International Educational Data Mining Society",
                "url": null
            }
        },
        {
            "65": {
                "title": "\u201cWhen is deep learning the best approach to knowledge tracing?\u201d",
                "author": "Theophile Gervet, Ken Koedinger, Jeff Schneider and Tom Mitchell",
                "venue": "In Journal of Educational Data Mining 12.3, 2020, pp. 31\u201354",
                "url": null
            }
        },
        {
            "66": {
                "title": "\u201cAnother look at measures of forecast accuracy\u201d",
                "author": "Rob J Hyndman and Anne B Koehler",
                "venue": "In International journal of forecasting 22.4",
                "url": null
            }
        },
        {
            "67": {
                "title": "\u201cDeep knowledge tracing\u201d",
                "author": "Chris Piech et al.",
                "venue": "In Advances in neural information processing systems 28, 2015",
                "url": null
            }
        },
        {
            "68": {
                "title": "\u201cA self-attentive model for knowledge tracing\u201d",
                "author": "Shalini Pandey and George Karypis",
                "venue": "In arXiv preprint arXiv:1907.06837, 2019",
                "url": null
            }
        },
        {
            "69": {
                "title": "\u201cDynamic key-value memory networks for knowledge tracing\u201d",
                "author": "Jiani Zhang, Xingjian Shi, Irwin King and Dit-Yan Yeung",
                "venue": "In Proceedings of the 26th international conference on World Wide Web, 2017, pp. 765\u2013774",
                "url": null
            }
        },
        {
            "70": {
                "title": "\u201cThe Knowledge-Learning-Instruction framework: Bridging the science-practice chasm to enhance robust student learning\u201d",
                "author": "Kenneth R Koedinger, Albert T Corbett and Charles Perfetti",
                "venue": "In Cognitive science 36.5",
                "url": null
            }
        },
        {
            "71": {
                "title": "\u201cAutomatic Domain Model Creation and Improvement.\u201d",
                "author": "Philip I Pavlik Jr, Luke G Eglington and Liang Zhang",
                "venue": "In Grantee Submission",
                "url": null
            }
        },
        {
            "72": {
                "title": "\u201cChatgpt in the generalized intelligent framework for tutoring\u201d",
                "author": "Faruk Ahmed, Keith Shubeck and Xiangen Hu",
                "venue": "In Proceedings of the 11th Annual Generalized Intelligent Framework for Tutoring (GIFT) Users Symposium (GIFTSym11), 2023, pp. 109",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.14668v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2.1",
            "2.2",
            "2.3"
        ],
        "methodology_sections": [
            "3.2",
            "3.4"
        ],
        "main_experiment_and_results_sections": [
            "3.1",
            "3.3",
            "4.1",
            "4.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "1",
            "4.1",
            "4.2",
            "5.1",
            "5.2",
            "8"
        ]
    },
    "research_context": {
        "paper_id": "2403.14668v1",
        "paper_title": "Predicting Learning Performance with Large Language Models: A Study in Adult Literacy",
        "research_background": "This paper investigates the potential of large language models (LLMs), specifically GPT-4, in predicting learning performance within adult literacy education programs. The motivation for this research stems from the critical role of adult literacy in empowering individuals and the closely related need for personalized instruction in literacy education. Accurate performance prediction is vital as it allows early identification of learners who may need additional support, enabling targeted interventions to enhance their reading comprehension and literacy skills.\n\n**Research Problem:**\nThe primary research problem addressed in this paper is the under-explored application of advanced AI models, such as LLMs, in predicting learning performance within Intelligent Tutoring Systems (ITSs) in the context of adult literacy education. The study aims to evaluate GPT-4's prediction capabilities concerning literacy education compared to traditional models like Bayesian Knowledge Tracing (BKT), Performance Factor Analysis (PFA), Sparse Factor Analysis Lite (SPARFA-Lite), tensor factorization, and eXtreme Gradient Boosting (XGBoost).\n\n**Relevant Prior Work:**\n1. **Literacy Education:**\n   - Prior works have highlighted the significance of effective literacy education, which addresses diverse needs and improves reading skills and comprehension [1, 2, 3, 4].\n   - The need for accurate assessment and personalized instruction to meet learner needs is emphasized in existing literature [4, 5].\n\n2. **Intelligent Tutoring Systems (ITSs):**\n   - ITSs are noted for their capability to personalize instruction by tracking and assessing learning progress [6, 7, 8].\n   - The prediction of learning performance using machine learning models within ITSs has been considered essential for personalizing instruction [9, 10].\n\n3. **Machine Learning Models:**\n   - Traditional models like BKT, PFA, SPARFA-Lite, tensor factorization, and XGBoost have been utilized to predict learning performance [11, 12].\n   - Historical data, including learners\u2019 correctness in problem-solving attempts, has been a focal point for modeling [11, 12].\n\n4. **Advancements in AI:**\n   - LLMs have shown remarkable capabilities in various predictive tasks, including mathematical reasoning and time series forecasting [13, 14, 15, 16, 17].\n   - In educational domains, LLMs have been leveraged for predicting learning performance and identifying at-risk learners [18, 19].\n   - Despite these advances, the use of LLMs to enhance predictive analytics within ITSs is still at an early stage.\n\n5. **GPT-4 Application:**\n   - The study explores GPT-4\u2019s potential, contrasting it with traditional benchmark models for predicting learning performance [20].\n   - The investigation includes how GPT-4 can augment human-led efforts to improve prediction accuracy through hyper-parameter tuning and model optimization.\n\nBy addressing these areas, this paper contributes to the field by evaluating the potential of GPT-4 in improving learner performance prediction in adult literacy programs, thereby informing the design of more effective ITSs.",
        "methodology": "### Methodology Description ###\n\n**Framework Overview:** \nWe developed a Large Language Model (LLM)-based framework to trace and predict learner performance in adult literacy. This framework encompasses three major procedures:\n1. **Encoding:**\n    - Converting numerical data into contextual prompts.\n2. **LLM Component:**\n    - Analyzing these prompts and making predictions.\n3. **Decoding:**\n    - Outputting the prediction information and assessment with interpretations.\n\n**Learning Performance Data:**\nLearner performance data on question-answering tasks are recorded as binary entries (1 for correct, 0 for incorrect), reflecting the learner\u2019s success or failure. Additionally, the number of attempts a learner makes is documented. For example, the performance on a specific question during a particular attempt is recorded (1 for a correct answer, 0 for an incorrect one). While the training dataset captures detailed performance records, the testing dataset omits this information to facilitate future performance predictions. The goal is to input these performance data into the LLM to detect data patterns and latent learning features, such as initial knowledge level and learning rate, for predicting future performance based on historical attempts.\n\n**Encoding:**\nBinary performance indicators are transformed into contextual prompts integrating numerical data with textual explanations:\n- **Numerical to Text Conversion:**\n  - Embedding numerical values alongside question contents and answers into a narrative format.\n- **Contextual Integration:**\n  - Incorporating lesson topics, content, and question details. This also involves embedding knowledge from reading comprehension activities to provide a richer context, aiding the assessment of the learner\u2019s comprehension skills.\n\n**LLM Component:**\nThe contextual prompts are input into the LLM for analysis and predictive modeling. Two primary prediction strategies are used:\n1. Leveraging the data reasoning, understanding, and computational capabilities of the GPT-4 model.\n2. Utilizing automatically selected and fine-tuned machine learning models by GPT-4 for prediction tasks.\n\nSeveral assumptions inform this component:\n(a) GPT-4 can be pre-trained on predictive tasks.\n(b) GPT-4 can extract domain-specific knowledge from various questions and attempts.\n(c) GPT-4 can identify latent learning features within contextual performance data, thus evaluating learners\u2019 reading comprehension skills.\n(d) GPT-4 inferred information, alongside other computational models, can identify trends, patterns, and predict learner performance.\n\n**Decoding:**\nThe decoding phase outputs predictive outcomes about learning performance. This is executed using heuristic approaches or machine learning models automatically selected by GPT-4. Predictions are presented in a structured format, such as:\n`{\u2018learner ID\u2019:\u2026, \u2018Question ID\u2019:\u2026, \u2018Attempt\u2019:\u2026, \u2018Prediction\u2019:\u2026, \u2018Assessment\u2019:\u2026}`,\nincreasing the dimensionality of the data to match the original input size of the test dataset. This structured output efficiently conveys prediction details based on the learning performance distribution from contextual prompts.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n**Dataset**:\nWe employed datasets from AutoTutor lessons developed for the Center for the Study of Adult Literacy (CSAL). These lessons are publicly accessible online and involve a trialogue interaction mode with two computer agents (a tutor agent and a virtual peer agent) to help human learners improve their reading comprehension skills through multiple-choice questions. Specifically, our analysis focused on three lessons:\n- Lesson 1: Persuasive Text\n- Lesson 2: Cause and Effect\n- Lesson 3: Problems and Solution\n\n**Baselines**:\nThe document does not explicitly mention the specific baseline models used in the experiment. Typically, in studies involving predictive learning performance, common baselines might include traditional machine learning models (e.g., linear regression, decision trees) or simpler algorithms for comparison against more complex models like large language models (LLMs). One might assume that such approaches could be employed unless otherwise mentioned.\n\n**Evaluation Metrics**:\nAgain, while not explicitly specified in the provided text, common metrics for evaluating predictive models in educational settings typically include accuracy, precision, recall, and F1-score for classification tasks (predicting correct/incorrect responses), or metrics like RMSE (Root Mean Squared Error) or MAE (Mean Absolute Error) for regression tasks (predicting scores or learning gains). The metrics would assess how well the model can predict learner performance based on the data.\n\n### Main Experimental Results\n\nThe original text provided does not include explicit results from the main experiment. To outline main experimental results accurately, typical findings might include:\n- The performance comparison of the predictive model to baseline models.\n- Specific statistics or values of the evaluation metrics.\n- Insights or implications derived from the predictive performance\u2014such as whether the large language model significantly outperforms traditional baselines in predicting learner performance.\n\nHowever, without the explicit results from the provided document, a precise summary cannot be given."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To investigate the effectiveness of predicting learning performance in adult literacy programs using GPT-4 in comparison with traditional machine learning methods.",
            "experiment_process": "The study employed reading comprehension datasets from the ITS, AutoTutor, developed for the Center for the Study of Adult Literacy (CSAL). The datasets comprise learner ID, questions, attempts, and performance scores for each lesson. The predictive capabilities of GPT-4 were compared with traditional models like Bayesian Knowledge Tracing (BKT), Performance Factor Analysis (PFA), Sparse Factor Analysis Lite (SPARFA-Lite), Tensor Factorization, and eXtreme Gradient Boosting (XGBoost) using five-fold cross-validation. For hyper-parameter tuning, GPT-4's performance was juxtaposed against a manual grid-search method focusing on XGBoost model optimization.",
            "result_discussion": "The findings revealed that XGBoost outperformed GPT-4 in predictive accuracy. Nonetheless, running XGBoost on the GPT-4 platform yielded better results than local machine execution. Hyper-parameter tuning by GPT-4 achieved comparable performance to manual tuning but showed less stability. The study highlighted the potential of combining GPT-4 with traditional machine learning models for enhanced predictive accuracy in adult literacy education.",
            "ablation_id": "2403.14668v1.No1"
        },
        {
            "research_objective": "To compare the prediction performance of LLM-based models with traditional baseline methods.",
            "experiment_process": "The prediction performances of various models were evaluated through their RMSE values across three CSAL lessons. All models had their hyper-parameters adjusted before the evaluations. 7 repeated prediction runs were conducted for both standard GPT-4 and GPT-4 enhanced with XGBoost for each lesson to establish RMSE reliability.",
            "result_discussion": "GPT-4 showed lower RMSE values than most other models, except for XGBoost which had the lowest RMSE values across all lessons. The integration of GPT-4 with XGBoost significantly improved prediction performance. The standard errors for RMSE values related to GPT-4 and XGBoost fell within a moderate range, suggesting a relative consistency in prediction accuracy.",
            "ablation_id": "2403.14668v1.No2"
        },
        {
            "research_objective": "To examine the difference in prediction outcomes between hyper-parameter tuning approaches for XGBoost: GPT-4 selected versus manual grid search.",
            "experiment_process": "Two hyper-parameter tuning approaches were compared: one selected by GPT-4 and the other via a manual grid search. Both approaches were applied to XGBoost across three lessons, and the results were evaluated by comparing RMSE values.",
            "result_discussion": "GPT-4 tuning demonstrated consistently lower minimum and median RMSE values across all lessons compared to manual tuning. However, the manual method resulted in less variability and tighter performance spreads. GPT-4's approach showed a wider range of performance results, suggesting potential benefits but also greater variability.",
            "ablation_id": "2403.14668v1.No3"
        },
        {
            "research_objective": "To explore the effectiveness of GPT-4\u2019s heuristic reasoning and machine learning model selection in predicting learning performance.",
            "experiment_process": "GPT-4 employed two predictive strategies: one using its heuristic reasoning based on question difficulty and attempt frequency; the other utilizing machine learning models. GPT-4\u2019s heuristic approach depended on logical reasoning to interpret historical performance data, while the machine learning approach involved selecting appropriate models like logistic regression, random forest, gradient boosting machine, and XGBoost, testing, and validating them.",
            "result_discussion": "GPT-4's heuristic reasoning, combined with contextually enriched reasoning from learning materials, improved prediction accuracy compared to traditional models like BKT, PFA, SPARFA-Lite, and Tensor Factorization. Using machine learning methods, GPT-4 identified and optimized XGBoost as the best model, demonstrating adaptability and performance enhancement through continuous refinement.",
            "ablation_id": "2403.14668v1.No4"
        },
        {
            "research_objective": "To evaluate the impact of prompt engineering on GPT-4's prediction capabilities for learning performance.",
            "experiment_process": "A prompt strategy leveraging the Chain-of-Thought method was employed to guide GPT-4 in analyzing and interpreting learner data. The approach involved a sequence of precisely tailored prompts covering learning material presentation, contextual transcriptions, analysis requests, method suggestions, model development, performance evaluation, configuration disclosure, skill assessment, optimization, and iterative feedback.",
            "result_discussion": "The Chain-of-Thought prompt strategy improved the transparency of GPT-4's decision-making process, enhancing the precision and relevance of its predictive capabilities. Through systematic prompt compositions, the method ensured effective execution, resulting in improved prediction accuracies and detailed insights relevant to specific learning scenarios.",
            "ablation_id": "2403.14668v1.No5"
        }
    ]
}