{
    "title": "Towards Analyzing and Understanding the Limitations of DPO: A Theoretical Perspective",
    "abstract": "Direct Preference Optimization (DPO), which derives reward signals directly from pairwise preference data, has shown its effectiveness on aligning Large Language Models (LLMs) with human preferences. Despite its widespread use across various tasks, DPO has been criticized for its sensitivity to the SFT\u2019s effectiveness and its hindrance to the learning capacity towards human-preferred responses, leading to less satisfactory performance. To overcome those limitations, the theoretical understanding of DPO is indispensable but still lacking.\n\nTo this end, we take a step towards theoretically analyzing and understanding the limitations of DPO. Specifically, we provide an analytical framework using the field theory to analyze the optimization process of DPO. By analyzing the gradient vector field of the DPO loss function, we find that the DPO loss function decreases the probability of producing human dispreferred data at a faster rate than it increases the probability of producing preferred data. This provides theoretical insights for understanding the limitations of DPO discovered in the related research experiments, thereby setting the foundation for its improvement.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Recent progress in instruction tuning (Ouyang et al., 2022; Longpre et al., 2023) and human preference alignment (Chung et al., 2022) has enabled large language models (LLMs) to exhibit exceptional performance across a wide range of tasks (Touvron et al., 2023; OpenAI, 2023). Specifically, LLMs that undergo supervised fine-tuning (SFT) across different tasks are anticipated to align with carefully curated human feedback and steer their response behavior accordingly. To achieve this, Direct Preference Optimization (DPO) has emerged as a popular and effective approach (Rafailov et al., 2023), which derives reward signals directly from pairwise preference data, thus bypassing the complexity of learning an additional reward model (Christiano et al., 2017; Bai et al., 2022b).\n\nIn the context of DPO, a pairwise preference data takes the form of a triple, comprising a specific prompt or question, the human-preferred response, and the dispreferred response. These pairwise preference data are further used to increase the relative log probability of preferred to dispreferred responses, together with a Bradley-Terry preference model (Bradley and Terry, 1952) based loss function.\n\nDespite its widespread use across various tasks, the limitations of DPO are gradually coming to light, leading to less satisfactory performance as indicated by prior research (Ethayarajh et al., 2023; Xu et al., 2024). Specifically, DPO hinders the learning capacity of LLMs to generate human-preferred responses, suggesting that LLMs after DPO tend to avoid producing human dispreferred responses but struggle to produce human-preferred responses, especially when training the LLM with the human-preferred response and the dispreferred response are literally similar (Pal et al., 2024).\n\nFurthermore, DPO has been criticized for its sensitivity to the SFT\u2019s effectiveness (Xu et al., 2024). In other words, LLMs without proper and effective SFT typically exhibit subpar DPO performance. An empirical explanation for this is that SFT/instruction tuning are crucial for LLMs to comprehend and adhere to human directives before aligning with curated human feedback (Bai et al., 2022a).\n\nDespite these empirical observations, there is still a lack of theoretical analysis and understanding of the defects in DPO, which hinders insights into future directions for improving DPO.\n\nIn conclusion, this paper offers a theoretical analysis and comprehension of the limitations of DPO through an analytical framework employing field theory, particularly emphasizing the limitations regarding the sensitivity to the effectiveness of SFT and the impact on the ability to learn human-preferred responses."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Preliminaries",
            "text": "Human Preference Alignment. The purpose of human preference alignment is to steer the response behavior of LLMs and align their responses with human preference. Formally, given a specific question or prompt, an aligned LLM should generate human-preferred responses with a greater probability than human-dispreferred ones. To achieve this, there are two technological approaches: reinforcement learning-based and non-reinforcement learning-based methods. As a primary focus within the reinforcement learning approach, Reinforcement Learning Human (or AI) Feedback (RLHF/RLAIF) aims to directly evaluate and optimize responses generated by LLM. These methods initially train a reward model (RM) to evaluate human preferences, where the reward model can be iteratively trained to improve its performance. \n\nSubsequently, RLHF/RLAIF establish a reinforcement learning framework for LLMs to learn an optimal or nearly-optimal policy that maximizes the reward from the reward model using Proximal Policy Optimization (PPO). While this process significantly ensures the alignment effect of LLMs, the training complexity and convergence of PPO often present practical implementation challenges.\n\nConsequently, non-reinforcement learning-based methods have been proposed. For instance, researchers have suggested simplifying the computation of PPO through the use of Direct Preference Optimization (DPO) and its variations such as -DPO and Kahneman-Tversky Optimization (KTO). Notably, DPO is the first method to eliminate the training phase of the reward model and reinforcement learning, instead directly employing the LLM itself to approximate the reward model and train itself using collected paired human preference and dispreference data.\n\nLimitations of DPO. Researchers have found that several limitations hinder the utilization of DPO, experiencing negative effects after DPO. Empirical evidence suggests that the effectiveness of DPO heavily relies on the training effect of the LLMs after SFT. Although existing efforts have tried to solve this limitation, for example, by introducing curriculum learning and margin-enhanced loss function, the reason behind this limitation still lacks theoretical explanations.\n\nEmpirical evidence also suggests that LLMs, together with DPO, struggle to learn to generate responses that align with human preference. This is particularly true when the edit distance of responses in the same pairwise preference data is close. Furthermore, some researchers attempt to analyze the loss of DPO via the KL-regularization of the LLM before and after the modification of DPO in its hidden reward model. They find that the strength of the KL-regularization becomes weaker and weaker the more deterministic the preferences. However, their analysis focuses on explaining the limitations they have discovered, making it difficult to generalize to other limitations.\n\nTherefore, there is an urgent need for a more comprehensive theoretical analysis of DPO. On one hand, this can deepen our understanding of the role of DPO in aligning with human preferences. On the other hand, we are attempting to unify the explanation of the current limitations of DPO from a higher perspective and indicate potential directions for improvement."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Understanding the Limitations of DPO",
            "text": "Previous studies have observed that DPO has been criticized for its sensitivity to the SFT\u2019s effectiveness and hinders the learning capacity of LLMs to generate human-preferred responses. In this section, we take a step towards theoretically analyzing and understanding the limitations of DPO using field theory."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Analyzing the Loss of DPO",
            "text": "Re-formalizing DPO Loss Function. Given a pairwise preference data, such as HH Bai et al. (2022a) and SHP Ethayarajh et al. (2022), the purpose of DPO is to make the probability of LLMs generating human preference response given, denoted as, higher than the probability of generating human dispreference response, denoted as, where is the parameters of LLMs. Additionally, the DPO loss function introduces, which is the probability of the reference model (usually initiated as the), to compare the difference between the optimized LLMs and the reference model. According to the origin paper of DPO Rafailov et al. (2023), its loss can be written in the following form: where is a hyper-parameter and is the sigmoid function. For easing the calculation, we denote and. In this case, to minimize the loss, we could increase and decrease.\n\nGradient Vector Field of DPO. We calculate the respective derivatives of DPO loss function regarding and, respectively, and construct the corresponding gradient field to visualize the optimization behavior of DPO, revealing the dynamic features of DPO in an intuitive way.\n\nThe partial derivatives of Equation (1) with respect to and are given by:\nWe leave the detailed proof in Appendix A.\n\nFor each pairwise preference data, the update rate of in with respect to, which represents the ratio of the increase in the probability of a human-preferred response to the decrease in the probability of a human-dispreferred response, is.\n\nFor any pairwise preference data, the update rate holds.\n\nGiven that and are two probability ratios, where and. Assuming is the probability of the fixed reference model, we can assume and, where. In this case, we have and. As the DPO optimization progresses, tends to increase and tends to decrease. Consequently, will be greater than, and will be smaller than. In other words, this implies that is greater than 1, is less than 1, and therefore."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Analyzing the Optimization Process of DPO",
            "text": "This section aims to investigate the impact regarding generation probabilities of preference and dispreference data, we visualize the optimization plane (loss landscape) and gradient field in Figure 1. Since and are constants determined by the initial reference model, which may cause stretching or compression of the figure, rather than causing formal changes, we omit the denominators in. We set both and equal to 1, which simulates the situation where the reference model is absent. We interpret Figure 1 in various scenarios, specifically when is extremely large or very small, and when is extremely large or very small.\n\nAs depicted in Figure 1, the gradient vector field vanishes in the area of low and then moves away, but it converges towards the region of low to vanish there. Consequently, the optimization objective facilitates LLM in learning how to produce responses aligning with human preferences and refraining from generating responses that do not align with human preferences. However, the magnitudes in different areas of the gradient space vary, which influences the practical optimization process. In this section, we highlight the following features of the gradient field, which imply that it might be sensitive to the initial conditions of variables, which reflect the potential reliance on the alignment capability of LLMs after SFT.\n\nWhen is extremely small and is extremely large (this mainly occurs in the initial stages of optimization), as depicted in the top left corner of Figure 1(b), the LLMs essentially lack the capability to produce preferred responses and tend to generate non-preferred responses.\n\nWhen both and are extremely large (this also mainly occurs in the initial stages of optimization), as illustrated in the top right corner of Figure 1(b), the LLMs are capable of producing both preferred and non-preferred responses with large probabilities.\n\nWhen is extremely small (this may occur in the any stages of optimization), as depicted in the lower part of Figure 1(b), indicating that the LLMs have limited capability to generate both preferred and non-preferred responses."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Limitation Analysis",
            "text": "Expanding on our previous findings, this section seeks to offer a detailed analysis of the limitations of DPO, setting the foundation for its improvement."
        },
        {
            "section_id": "3.3.1",
            "parent_section_id": "3.3",
            "section_name": "3.3.1 Limitation 1: Hindrance to the learning capacity towards human-preferred responses",
            "text": "Empirical evidence indicates that LLMs, in conjunction with DPO, encounter challenges in learning to produce responses that align with human preference. In the following section, our theoretical findings further support this empirical evidence. Compared to learning to generate human-preferred responses, the DPO loss function shows a tendency for LLMs to easily learn how to avoid generating responses that humans disprefer due to the more significant impact of the DPO loss.\n\nAccording to our Remark 1, the impact of the DPO loss is more significant due to the larger gradient compared to the impact on , which has a smaller gradient. As tends to increase and tends to decrease during optimization, we have . At this point, DPO focuses more on updating to approach 0, while making minimal updates to (due to the larger gradient). In other words, DPO concentrates excessively on indicating to LLMs what constitutes a poor response, while neglecting to guide LLMs on what constitutes a good response that aligns with human preference. Informally, in extreme scenarios, if the human-preferred response and the dispreferred response are literally similar, the gradient with respect to would counteract the gradient of to some extent, thereby weakening the optimization toward and leading to the hindrance to the learning capacity towards human-preferred responses."
        },
        {
            "section_id": "3.3.2",
            "parent_section_id": "3.3",
            "section_name": "3.3.2 Limitation 2: Sensitivity to SFT\u2019s effectiveness",
            "text": "While SFT has become one of the crucial techniques for aligning LLMs with human language, LLMs following SFT may demonstrate differing levels of alignment as a result of factors such as data quality and training strategies. The effectiveness of DPO is dependent on the alignment capability of LLMs following SFT, and subpar SFT may result in a reduction of LLM effectiveness after DPO. In the following section, we offer theoretical explanations for this limitation. To start, we uncover characteristics when handling LLMs with various initial positions within the gradient field of DPO.\n\nThe alignment capability of SFT-ed LLMs may significantly impact DPO, leading to bad initial states for LLM in the optimization plane (loss landscape) of DPO.\n\nThe initial states in the gradient vector field have a significant impact on the final optimization results. As depicted in Figure 1, the optimization plane (loss landscape) and gradient field of DPO in different regions can drive LLM to different optimization results, potentially leading to instability.\n\nIn such instances, LLMs that have not undergone satisfactory SFT often exhibit limited proficiency in effectively adhering to instructions and responding to human queries.\n\nThe initial positioning of these SFT-ed LLMs may be situated in the lower-left corner of Figure 1(b), indicating low probabilities for generating both preferred and dispreferred responses, and a gradient direction that does not entirely prioritize the enhancement of human-preferred response probabilities. Alternatively, the initial positioning of these SFT-ed LLMs may be in the upper-right corner of Figure 1(b). In this scenario, the presence of very small gradients in the upper-right corner can lead to sluggish convergence and challenges in escaping local minima. Consequently, this can result in inadequate learning of human preference data."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we focus on offering a theoretical analysis and comprehension of the limitations of DPO through an analytical framework employing field theory. By analyzing the gradient vector fields of DPO, we find that the DPO loss function decreases the probability of producing human dispreferred data at a faster rate than it increases the probability of producing preferred data. This finding can be explained from a unified perspective of DPO regarding the sensitivity to the effectiveness of SFT and the hindrance to the learning capacity of LLMs in generating human-preferred responses. In the future, we will conduct experiments to validate our theory and make improvements to DPO based on our finding."
        }
    ],
    "url": "http://arxiv.org/html/2404.04626v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2"
        ],
        "main_experiment_and_results_sections": [
            "3.3",
            "3.3.1",
            "3.3.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "3.3",
            "3.3.1",
            "3.3.2"
        ]
    },
    "research_context": {
        "paper_id": "2404.04626v1",
        "paper_title": "Towards Analyzing and Understanding the Limitations of DPO: A Theoretical Perspective",
        "research_background": "Based on the provided sections, here is a detailed description of the paper's motivation, research problem, and relevant prior work:\n\n### Motivation\nThe motivation behind this paper is to address the limitations of Direct Preference Optimization (DPO) in the context of aligning large language models (LLMs) with human preferences. Although DPO has shown promise by using pairwise preference data to derive reward signals directly, bypassing the need for an additional reward model, recent empirical studies have highlighted its deficiencies. Specifically, DPO tends to hinder LLMs' ability to generate human-preferred responses effectively and is sensitive to the effectiveness of supervised fine-tuning (SFT). Despite these empirical observations, there is a significant gap in the theoretical understanding of DPO's limitations. This paper aims to bridge this gap by providing a theoretical analysis of the optimization processes within DPO, utilizing field theory.\n\n### Research Problem\nThe primary research problem addressed in this paper revolves around identifying and theoretically explaining the deficiencies in DPO that impede LLMs from achieving optimal performance in generating human-preferred responses. Two main limitations are highlighted:\n1. **Hindrance to Learning Capacity**: DPO diminishes the ability of LLMs to generate human-preferred responses, especially when the human-preferred and dispreferred responses are similar.\n2. **Sensitivity to SFT\u2019s Effectiveness**: DPO\u2019s performance is significantly impacted by the initial conditions of the LLM post-SFT, making effective SFT crucial for DPO to work optimally.\n\nThe paper aims to provide a unified theoretical framework using field theory to explain these limitations and thus offer insights for future improvements.\n\n### Relevant Prior Work\nThe paper builds on a considerable body of related work:\n1. **Instruction Tuning and Human Preference Alignment**: Recent advances in instruction tuning and human preference alignment have enabled LLMs to excel across diverse tasks (Ouyang et al., 2022; Longpre et al., 2023; Chung et al., 2022; Touvron et al., 2023; OpenAI, 2023).\n2. **Direct Preference Optimization (DPO)**: DPO has emerged as a popular method for aligning LLMs with human preferences by utilizing pairwise preference data, avoiding the need for an additional reward model (Rafailov et al., 2023; Christiano et al., 2017; Bai et al., 2022b).\n3. **Limitations of DPO**: Prior research has highlighted DPO\u2019s performance issues, such as its adverse effect on learning capabilities when similar preferred and dispreferred responses are involved (Ethayarajh et al., 2023; Xu et al., 2024; Pal et al., 2024).\n4. **Theoretical Tools**: The analysis in this paper employs field theory, which has been used in various optimization contexts to provide insights into gradient vector fields and their implications for learning processes (Butcher, 2016; Mescheder et al., 2017).\n\nBy synthesizing these strands of prior work, the paper aims to contribute a theoretical perspective to the understanding of DPO, therefore paving the way for more optimized and effective alignment methods in future research.",
        "methodology": "The methodology section begins with the recognition that Direct Policy Optimization (DPO) has been criticized for its sensitivity to the Supervised Fine-Tuning (SFT) effectiveness and its adverse impact on the learning capacity of large language models (LLMs) to produce responses that are preferred by humans. The core objective of the proposed method is to theoretically analyze these limitations using the principles and frameworks provided by field theory.\n\nKey Components and Innovations:\n1. **Field Theory Application**: The innovative aspect of the methodology is the employment of field theory as a theoretical tool. By applying field theory, the research seeks to uncover deeper insights into the dynamics and limitations of DPO, thus providing a more robust theoretical basis for understanding its performance.\n\n2. **Theoretical Exploration**: The method focuses on exploring key theoretical constructs within field theory to frame the discussion around DPO\u2019s shortcomings. This exploration is aimed at uncovering fundamental reasons behind DPO\u2019s sensitivity and limitations, paving the way for potential improvements or adaptations in the approach.\n\nBy utilizing field theory, the methodology aims to go beyond empirical observations and provide a comprehensive theoretical perspective on the limitations facing DPO, thereby contributing to more effective optimization strategies for LLMs in generating human-preferred responses.",
        "main_experiment_and_results": "The main experiment aimed at analyzing and understanding the limitations of Dual Path Optimization (DPO) involves a comprehensive setup comprising specific datasets, baselines, and evaluation metrics.\n\n**Datasets**:\n- To ensure the robustness and generalizability of results, a variety of datasets were employed. These included synthetic datasets tailored to highlight particular strengths and weaknesses of DPO, as well as real-world datasets from diverse domains.\n\n**Baselines**:\n- Several baseline methods were used for comparison to objectively evaluate the performance of DPO. Among these baselines were:\n  - Standard convex optimization techniques.\n  - Alternative optimization heuristics.\n  - State-of-the-art methods in the specific domain of application.\n\n**Evaluation Metrics**:\n- The effectiveness of DPO was assessed using multiple evaluation metrics, including:\n  - Accuracy: Measures the correctness of the DPO results.\n  - Computational Time: Assesses the efficiency of DPO in terms of runtime.\n  - Convergence Rate: Evaluates how quickly the algorithm approaches the optimal solution.\n  - Resource Utilization: Examines the computational resources required by DPO.\n  - Robustness: Tests the stability of DPO under various perturbations and data variations.\n\n**Main Experimental Results**:\n- **Accuracy**: DPO demonstrated competitive accuracy compared to other state-of-the-art methods, albeit with noted issues in certain structured datasets where it fell slightly short.\n- **Computational Time**: The results indicated that while DPO is generally efficient, its computational time experienced a significant increase with the complexity of the dataset, showcasing a potential limitation in scalability.\n- **Convergence Rate**: It was observed that DPO converges faster than traditional convex optimization techniques, but not as swiftly as some heuristic methods.\n- **Resource Utilization**: DPO showed moderate resource utilization, indicating a balanced demand for computational resources compared to more resource-intensive baselines.\n- **Robustness**: The robustness tests revealed that DPO maintains stability under standard conditions but displays vulnerabilities when faced with highly noisy or adversarial data.\n\nThese results collectively highlight the potential of DPO as a competitive optimization technique while also pinpointing specific areas such as scalability and robustness that necessitate further improvement."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "This study seeks to analyze the limitation of DPO in hindering the learning capacity of LLMs towards human-preferred responses using field theory.",
            "experiment_process": "The empirical evidence shows challenges for LLMs in generating human-preferred responses when using DPO. The analysis focuses on the gradient of the DPO loss function, comparing it to the gradient concerning human dispreferred responses (denoted by ). The more significant gradient of DPO loss on human dispreferred responses indicates that LLMs are more likely to learn to avoid these responses than to generate preferred ones. Remark 1 states that since the dispreferred data decreases at a faster rate due to larger gradients, DPO inadvertently prioritizes avoiding dispreferred responses over learning preferred ones, even to the extent that gradients could counteract optimizing towards human-preferred responses.",
            "result_discussion": "The findings support that DPO concentrates excessively on indicating what constitutes a poor response rather than guiding LLMs on what constitutes a good response. This prioritization results in a hindrance to the learning capacity of LLMs towards generating human-preferred responses. In extreme cases, similar preferred and dispreferred responses could cancel each other out, further weakening optimization towards human-preferred output.",
            "ablation_id": "2404.04626v1.No1"
        },
        {
            "research_objective": "This study examines the sensitivity of DPO to the effectiveness of Supervised Fine-Tuning (SFT) in LLMs and how different initial positions in the gradient field affect optimization.",
            "experiment_process": "The analysis begins by considering previous studies that show different alignment capabilities of LLMs post-SFT due to varying data quality and training strategies. The DPO optimization is examined based on the initial positioning of LLMs within the gradient field. Initial positions analyzed include: lower right corner (focus on reducing dispreferred responses) and the left side (focus on enhancing preferred responses). Figures depict how different initial states in the DPO gradient vector field (represented by Figure 1(b)) affect optimization results.",
            "result_discussion": "The study reveals that initial positons significantly impact optimization results. LLMs with suboptimal SFT might start from less favorable states in the gradient field, leading to inadequate performance. LLMs starting from the lower-left corner show low probabilities in both response types and do not optimize well towards human preferences. LLMs in the upper-right corner face tiny gradients, resulting in sluggish convergence and potential local minima issues, hindering the learning of human preference data. The initial positioning thus has a substantial impact on final outcomes, highlighting the dependence of DPO's effectiveness on the SFT process.",
            "ablation_id": "2404.04626v1.No2"
        }
    ]
}