{
    "title": "Khayyam Challenge (PersianMMLU): Is Your LLM Truly Wise to The Persian Language?",
    "abstract": "Evaluating Large Language Models (LLMs) is challenging due to their generative nature, necessitating precise evaluation methodologies. Additionally, non-English LLM evaluation lags behind English, resulting in the absence or weakness of LLMs for many languages.\n\nIn response to this necessity, we introduce Khayyam Challenge (also known as PersianMMLU), a meticulously curated collection comprising 20,192 four-choice questions sourced from 38 diverse tasks extracted from Persian examinations, spanning a wide spectrum of subjects, complexities, and ages. The primary objective of the Khayyam Challenge is to facilitate the rigorous evaluation of LLMs that support the Persian language. Distinctive features of the Khayyam Challenge are (i) its comprehensive coverage of various topics, including literary comprehension, mathematics, sciences, logic, intelligence testing, etc aimed at assessing different facets of LLMs such as language comprehension, reasoning, and information retrieval across various educational stages, from lower primary school to upper secondary school (ii) its inclusion of rich metadata such as human response rates, difficulty levels, and descriptive answers (iii) its utilization of new data to avoid data contamination issues prevalent in existing frameworks (iv) its use of original, non-translated data tailored for Persian speakers, ensuring the framework is free from translation challenges and errors while encompassing cultural nuances (v) its inherent scalability for future data updates and evaluations without requiring special human effort. Previous works lacked an evaluation framework that combined all of these features into a single comprehensive benchmark. Furthermore, we evaluate a wide range of existing LLMs that support the Persian language, with statistical analyses and interpretations of their outputs. We believe that the Khayyam Challenge will improve advancements in LLMs for the Persian language by highlighting the existing limitations of current models, while also enhancing the precision and depth of evaluations on LLMs, even within the English language context.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large Language Models (LLMs) have recently revolutionized applications of machine intelligence (Hong et al., 2024; Wu et al., 2023; Thirunavukarasu et al., 2023; Glukhov et al., 2023). The rapid deployment of these models within industrial and public sector solutions has made evaluating their capabilities an imperative task (Guo et al., 2023). \n\nTo address critical aspects of language understanding and bridge the gap between the knowledge that models observed during pretraining and the measures of success, the MMLU benchmark was introduced (Hendrycks et al., 2021). It assesses the ability of LLMs across a diverse set containing 57 subjects.\n\nAlthough automatic translation efforts like MMLU can bridge the gap in evaluation resources for languages other than English (Achiam et al., 2023), the cultural specificity of these questions limits their applicability across different languages and cultures. This is particularly true for Persian, a language with its own rich culture and literature, where direct translation of English MMLU content may not be suitable for certain culture-specific subjects. Additionally, our investigation reveals significant complications in automatic translation, particularly in specialized domains like Physics. Despite advancements in machine translation technology, accurately conveying precise meaning in certain domains remains a hard task. For instance, the GPT-3.5 automatic translation often fails to include essential units such as \"microjoules\" in translations, leading to inaccuracies. Also, loss of information can occur in translation due to the existence of Persian words that lack exact equivalents in English. For example, in Persian, there are distinct terms for paternal aunt, maternal aunt, paternal uncle, and maternal uncle, while in English, only \u201caunt\u201d and \u201cuncle\u201d are used. This underscores the inherent difficulty in translating domain-specific terminology accurately. For such reasons, some studies have evaluated LLMs on original non-English datasets rather than the translated ones. For instance, Li et al. (2023) has introduced a Chinese dataset across 67 topics and indicated that current models struggle to achieve accuracies above a certain threshold. Also, Zhang et al. (2023) introduced a multilingual and multimodal dataset, and showed that multilingual text processing hardly achieves over certain accuracy.\n\nMotivated by the mentioned issues, we propose the Khayyam Challenge, also referred to as PersianMMLU, a benchmark designed to analyze the performance of LLMs in Persian and evaluate their knowledge and abilities comprehensively. Named in honor of the famed Persian polymath Omar Khayyam, the Khayyam Challenge embodies the multidimensional nature of Persian language understanding.\n\nThis benchmark covers 38 subjects, including Mathematics and Physics, which require reasoning and computational ability, to humanities and social sciences, demanding nuanced understanding and cultural sensitivity. Unlike the previous Persian datasets such as ParsiNLU (Khashabi et al., 2021), our benchmark includes more diverse topics in addition to different educational stages.\n\nUnlike PersianQA (Ayoubi, 2021) and PQuAD (Darvishi et al., 2023), which are extractive datasets where models are tasked with extracting answers from given paragraphs and questions, our benchmark offers a more comprehensive evaluation of LLMs. This is because the task of answer extraction alone may not sufficiently assess the models\u2019 overall language understanding and reasoning capabilities.\n\nOur proposed dataset contains \u201cIran\u2019s national university entrance\u201d, and Kanoon Farhangi Amoozesh (Cultural Educational Center), wherein questions are not only verified and validated by experts in each subject but also accompanied by metadata for each question. This metadata includes the difficulty level, a descriptive answer, educational stage, subject, and the specific topic of the question. Through the Khayyam Challenge, we aim to provide a holistic evaluation framework that reflects the diverse linguistic and cognitive challenges inherent in processing Persian text across various domains.\n\nIn our evaluations, we assessed several state-of-the-art language models, including GPT-3.5, GPT-4 (OpenAI, 2023), Aya (\u00dcst\u00fcn et al., 2024), PersianMind (Rostami et al., 2024), mGPT (Shliazhko et al., 2022), mT0 (Muennighoff et al., 2022), Claude3-haiku (Anthropic, 2024), and XVERSE111https://github.com/xverse-ai, all purportedly equipped with some level of understanding of the Persian language. Our findings indicate that while most of these models struggle to grasp Persian nuances, particularly evident in domains such as Calculus, Logic, and Geometry where accurate comprehension is essential, some exhibit comparatively better performance in contexts reliant on contextual understanding, such as Economics, Psychology, and Social studies. Notably, GPT-4 showcased relatively improved performance across multiple domains. However, there remains a clear imperative need for further enhancements across all models, especially in technical disciplines like"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Large Language Models",
            "text": "Over the past few years, there has been a significant improvement in the performance of language models. This progress has been observed in line with the scaling law (Kaplan et al., 2020), thanks to the increasing size of training datasets, enhanced processing power, and new evolved model architectures. The continuing process of scaling the models resulted in LLMs like GPT-3 (Brown et al., 2020), GPT-4 (OpenAI, 2023), Claude3, mT0 (Muennighoff et al., 2022), XVERSE, Aya (\u00dcst\u00fcn et al., 2024), etc. Even though AI models are highly capable of solving various tasks, they continue to encounter difficulties when it comes to real-world problems that, for example, require strong reasoning abilities or complex mathematical calculations (Chang et al., 2024; Zhong et al., 2023). Therefore, we need to assess the effectiveness of these models in solving high-level tasks. This enables us to identify the weak points of the models and work towards improving them in the future. Despite some of the recent LLMs being multilingual, studies indicate that their effectiveness is not as pronounced in non-Latin or low-resource languages as it is in English (Zhang et al., 2023). Consequently, it is essential to assess multilingual LLMs on tasks that employ languages other than English."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Evaluation of LLMs",
            "text": "Several benchmarks have been developed to assess the performance of LLMs. One of the most significant benchmarks is MMLU (Hendrycks et al., 2021), which evaluates language models for answering multiple-choice questions in 57 different tasks, but only in English. M3Exam (Zhang et al., 2023) introduces a multilingual, multimodal, and multilevel benchmark for evaluating LLMs, including more than 12K multiple-choice questions from 9 languages (excluding Persian) at three educational stages. AGIEval (Zhong et al., 2023) is another benchmark that assesses the performance of LLMs on human-centric standardized exams in English and Chinese languages to measure their ability in human-level tasks.\n\nThere have been a few benchmarks built to assess language models on the Persian language, including PersianQA (Ayoubi, 2021), ParsiNLU (Khashabi et al., 2021), and PQuAD (Darvishi et al., 2023), in which some of their features are compared in Table 2. PersianQA and PQuAD present extractive datasets where models are asked to extract answers from given paragraphs. While this task can gauge models\u2019 reading comprehension skills, it may not effectively evaluate general capabilities and inherent knowledge of models. ParsiNLU evaluates language models based on 14,500 questions from six language understanding tasks, including multiple-choice QA (MCQA), sentiment analysis, and more (Table 2). However, the questions in this benchmark fail to reach human-level complexity, thus inadequately assessing important skills of LLMs, including complex reasoning, needed for solving higher educational stages questions.\n\nMoreover, ParsiNLU covers only three subject tasks in MCQA and lacks adequate metadata, such as question difficulty levels. This deficiency further restricts our capacity to evaluate the model\u2019s proficiency across specific tasks and different difficulty levels. To address this gap, we introduce Khayyam Challenge, which features multiple-choice questions sourced from high-standard exams, and contains rich metadata to evaluate the innate knowledge and human-like skills of LLMs across different difficulty levels and educational stages."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Data",
            "text": "The Khayyam Challenge presents a robust dataset aimed at enhancing the evaluation of LLMs that support Persian, particularly in the context of multiple-choice question answering. This dataset encompasses a diverse range of subjects, reflecting a comprehensive approach to assessing various cognitive abilities including language comprehension, reasoning, and knowledge recall across different educational stages.\nThe educational system in Iran, from which this dataset draws, is structured into 12 years of schooling divided into segments: 6 years of primary education and 6 years of secondary education. Primary education is split into lower primary school (LPS) for the first 3 years, followed by upper primary school (UPS) for the next 3 years. Secondary education is similarly divided, with lower secondary school (LSS) encompassing the first 3 years, and upper secondary school (USS) comprising the final 3 years."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Data construction",
            "text": "The dataset originates from the \u201cPellekan Yadgiri (Learning Ladder)\u201d website222https://peleyad.com/  ###reference_peleyad.com/###, a part of the Kanoon Farhangi Amoozesh333https://en.kanoon.ir/  ###reference_en.kanoon.ir/### (Cultural Educational Center), a renowned private educational institution in Iran. Since conducting its first examination in 1993, Kanoon has been at the forefront of educational innovation, serving a nationwide network of 450,000 students. The center is highly regarded for its comprehensive range of educational services, notably its facilitation of creating and administering smart, customized tests and providing standardized solutions for exercises through the Pellekan Yadgiri platform. This initiative aims to enable educators to design and administer quality, customized tests effortlessly and to provide students with insightful feedback to enhance their learning process.\nThe quality of the questions in the dataset is exceptionally high, a testament to the institution\u2019s reputable standing in the educational sector. The ongoing development of new questions for various educational stages and subjects allows for the continuous expansion and updating of the dataset, reducing the risks of data contamination and erosion. A subset of the questions also includes items from the national university entrance examination in Iran, adding to the dataset\u2019s authenticity and relevance."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Metadata characteristics",
            "text": "The Khayyam Challenge is enriched with valuable metadata that elevates its utility beyond a simple aggregation of questions. This metadata includes:\n###table_1### ###figure_1### ###figure_2### Educational Stage: Specifies the educational stage for which the question is intended (LPS, UPS, LSS, USS), allowing for the assessment of appropriateness and difficulty relative to the expected knowledge base at each stage of education.\nDifficulty Level: Each question is classified into one of five distinct difficulty levels: easy, relatively easy, medium, relatively difficult, and difficult. This nuanced categorization allows a detailed analysis of question difficulty and examinee performance.\nDescriptive Answers: In addition to the correct answer, our dataset provides a detailed explanation for each question. This is crucial for understanding the reasoning behind the correct answer, facilitating a deeper comprehension of the question.\nTrap: Human experts have identified if a question contains a \u201dtrap\u201d choice\u2014an incorrect answer that might be easily mistaken for the correct one. These questions are referred to as \u201dtrapped questions\u201d and are generally more challenging, with the majority classified as difficult. This helps in understanding common misconceptions and the effectiveness of question design in truly testing knowledge and reasoning abilities.\nHuman Performance: This metric quantifies the percentage of students answering a question correctly.\nSpecific Topic: Questions are meticulously categorized into detailed subjects, such as \u201dMathematics \u00bf Discrete Maths \u00bf Combinatorics.\u201d This detailed classification enables targeted analysis of exam content and provides insights into the distribution and depth of questions across various subjects.\nYear: Indicates the year when the question was designed, which can provide insights into the evolution of question complexity and educational standards over time.\nThe inclusion of this metadata is not merely for augmentative purposes; it serves a critical role in enabling comprehensive analyses that can benefit educators, researchers, and developers of educational technologies. Specifically, it allows for the comparison of performance between human examinees and LLMs on specific topics under varying difficulty levels. By assessing whether LLMs fall for the traps as humans or how they approach questions requiring complex thought processes, we can gain valuable insights into the capabilities and limitations of current AI technologies in educational contexts.\nMoreover, the presence of descriptive answers supports the development of more sophisticated AI models by facilitating \u201dchain of thought\u201d processing, where the model learns to approach a problem step-by-step, mirroring human problem-solving methods. This not only enhances the model\u2019s problem-solving skills but also its ability to explain its reasoning in a manner that is understandable to humans."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Data statistics",
            "text": "The dataset contains 20,192 multiple-choice questions across 38 tasks, spanning subject areas like humanities, mathematics, natural science, and social science, along with elements of intelligence testing. These questions necessitate a blend of knowledge and reasoning. Additionally, the dataset includes 15,933 questions with human performance data, excluding Iran\u2019s national university entrance exam questions, and features 3,531 trapped questions. Figure 1  ###reference_###-(a) depicts the allocation of questions among the main categories and their respective tasks. Figure 1  ###reference_###-(b) outlines the distribution of questions based on their levels of difficulty. For more detailed information about the data, refer to Appendix A  ###reference_###."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Key features",
            "text": "The Khayyam Challenge Dataset stands out for several reasons:\nComprehensive Coverage: It spans a broad spectrum of subjects from literary comprehension to logic and intelligence testing, catering to different stages of education. This diversity makes it a versatile tool for assessing language models\u2019 capabilities across various domains.\nRich Metadata: The inclusion of detailed question metadata enhances the dataset\u2019s utility for nuanced analysis and model evaluation, providing valuable context for each question.\nNew Data Utilization: By incorporating questions never before used in research, the dataset avoids common data contamination issues, offering a fresh challenge to language models.\nOriginal, Non-Translated Content: Focused on the Persian language, the dataset eliminates translation errors and incorporates cultural nuances, making it uniquely valuable for related linguistic and cultural studies.\nScalability: The dataset\u2019s design and sourcing methodology ensure its adaptability and expandability, allowing for straightforward updates and extensions without substantial human intervention."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "###figure_3###"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Settings",
            "text": "We evaluated our benchmark using nine distinct models: GPT-4, GPT-3.5, Aya, XVERSE-13B, XVERSE-7B, PersianMind, mT0XL, mGPT, and Claude 3/Haiku. Detailed descriptions of each model can be found in Appendix B  ###reference_###. Additionally, we benchmarked a standardized prompt-0 5  ###reference_### template on the entire dataset and two other templates, prompt-1 6  ###reference_### and 7  ###reference_###, on a subset of 1000 samples from the dataset. We also conducted Chain-of-Thought (CoT) 8  ###reference_### on a subset of 1000 samples requiring CoT, such as mathematical questions. To ensure consistency and fairness throughout our experiments, we kept all model hyperparameters at their default values. Furthermore, we set the temperature parameter to zero for all models and did not impose any maximum limit on the number of tokens, allowing models the freedom to conduct any type of inference they desired."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Answer extraction methods",
            "text": "We employed three distinct techniques to extract answers: Regex, Single Token Probability, and Full Answer Probability, and conducted an evaluation of their individual accuracies. In the Regex method, we developed detailed Regex functions tailored to each model, which is available in our framework, to accurately capture the desired choices. In cases where Regex failed to identify the correct option, we utilized a pre-trained model to generate embeddings for each choice within the question, selecting the most similar one as the correct response. Furthermore, in our analysis, we considered human accuracy by examining our comprehensive metadata containing response percentages for each question choice. We chose the answer selected by humans if the combined total of the remaining three options was lower.\nIn the Single Token Probability approach, applied to models providing token probabilities, we calculated the softmax probability of tokens and selected the most probable tokens from 1, 2, 3, and 4 in Persian as the response. As for the Full Answer Probability method as we can see at formula 1  ###reference_###, we tokenized the text of each choice, computed the sum of the logarithm of token probabilities for each option, standardized it to the length of each option, and chose the most likely option as the answer.\nOur analysis shows that the Regex method has the highest accuracy, whereas the full prob method demonstrated a significant decrease in accuracy compared to the other two approaches."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Impact of translation quality",
            "text": "We aim to assess the impact of translation from state-of-the-art (SOTA) translation models on performance. To do so, we selected a set of sample questions from the dataset and translated them with the assistance of field experts, as well as using off-the-shelf translation models. Following this, we evaluated the models\u2019 performance on both sets of samples: those translated with expert assistance and those translated by models alone. Our findings revealed a notable decrease in performance when translated with the help of off-the-shelf models, highlighting the need for a new dataset that does not depend solely on translated data."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Limitations of few-shot approach",
            "text": "We have developed a benchmark code for our dataset and used the zero-shot and CoT methods to calculate accuracy. Previous studies (Li et al., 2023  ###reference_b14###; Zhong et al., 2023  ###reference_b23###; Zhang et al., 2023  ###reference_b22###) shows that using few shots on instruction-tuned models does not enhance accuracy and may even may decrease it. Therefore, we did not measure accuracy using few-shot techniques."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Results and discussions",
            "text": ""
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Results across all models",
            "text": "The evaluation results of zero-shot method for all models across five main categories and three choice-extraction methods (regex+, single token probability, and full answer probability), as well as human performance, are presented in the Table 3  ###reference_###, and appendix Tables 8  ###reference_###, 7  ###reference_###. The more comprehensive results on all 38 tasks are reported in the appendix (Tables 9  ###reference_###, 10  ###reference_###, 11  ###reference_###). Also the results of CoT and its comparison with zero-shot for GPT-3.5 using regex+ method across three main categories on a subset of dataset with 1000 questions are presented in appendix Table 6  ###reference_###. These results yield the following key findings:\nUtilizing Regex method for answer extraction (Table 3  ###reference_###) results in highest model performance compared to Single Token Probability (Table 8  ###reference_###) and Full Answer Probability (Table 7  ###reference_###) due to its more accurate and comprehensive choice extraction procedure. In the rest of the results section, we compare the accuracy of models using the Regex method.\nGPT-4 outperforms all other models in all five main categories, with an average accuracy of 8 percent higher than Claude3-haiku, the second-best performing model.\nAya, an open-source model, performs comparably or even better than GPT-3.5, a closed-source model, in 8 tasks including Sociology USS and Theology LSS. This demonstrates the convergence of open-source models\u2019 capabilities towards closed-source models.\nAlthough PersianMind, a 7B Persian-English LLM, is trained and fine-tuned on 2 billion Persian tokens (Rostami et al., 2024  ###reference_b17###), its performance is weaker than mT0XL, a multilingual 3.7B LLM.\nThe performance gap between the best-performing model, GPT-4, and human averages around 35%. In subjects like mathematics, this gap widens to 50% accuracy for GPT-4. This exhibits a real challenge of current LLMs in solving human-level questions, especially in complex mathematical questions that need high-level mathematical calculation and reasoning skills.\nThe models exhibit weaker performance in mathematics and natural science main categories compared to humanities and social sciences. This indicates their weaker performance on questions requiring high reasoning skills, compared to those mainly reliant on models\u2019 inherent knowledge. This underscores the necessity for enhancing the models\u2019 reasoning ability in the Persian language.\nThe CoT has improved the performance of GPT-3.5 in Mathematics questions by 10%, but decreased the performance in Humanities questions by 8%."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Accuracy trends",
            "text": "We demonstrated the accuracy of the models on all three difficulty levels of questions for different educational stages and question publication years in Figure 9  ###reference_### and Figure 10  ###reference_### in the Appendix E  ###reference_###, respectively. Key findings from these figures include:\nMost model accuracies decline with the increasing publication year for questions with medium and easy difficulty levels, while human performance remains consistent. This suggests that humans may have adapted to the evolving questions\u2019 difficulty over time, whereas models have not.\nThere exists a notable performance gap between difficult and medium questions in human performance, indicating that difficulty level has a stronger impact on human performance compared to LLMs. Since humans assigned difficulty levels to questions, this gap may stem from differing perceptions of difficulty between humans and LLMs.\nGPT-4 outperforms humans in difficult questions within four years, as well as during the initial two educational stages (LPS and UPS). This result suggests that modern LLMs may excel at analyzing difficult questions compared to humans."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Selected choice distribution",
            "text": "Figures 12  ###reference_###, 13  ###reference_###, 14  ###reference_### in the Appendix E  ###reference_### depicts the selected choice distributions of various models. It reveals that despite GPT-3.5 having the lowest count of unanswered questions, GPT-4 surpasses it in accuracy, indicating a more refined understanding despite its higher non-response rate. Comparatively, GPT-4 exhibits a more uniform distribution of choice selection, aligning closely with the Ground Truth and showing less bias than its predecessor, GPT-3.5, which tends towards selecting the second and third choices. High-performance models like GPT-4 demonstrate a closer alignment with Ground Truth distribution, indicating a lower bias level and potentially higher utility in applications."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Trap analysis",
            "text": "Table 15  ###reference_### in the Appendix F  ###reference_### compares the performance of models and humans on trapped questions, using an x/y format where x represents overall accuracy and y the accuracy on trapped questions. The data shows that while traps often mislead students, leading to nearly random performance, models like GPT-4 exhibit only a slight drop in accuracy when faced with these traps. However, this drop in accuracy is most pronounced in the Social Sciences and Humanities. Notably, GPT-4 outperforms humans across all main categories in handling trapped questions, indicating its robustness against misleading choices and affirming the different perspectives of difficulty between humans and AI models."
        },
        {
            "section_id": "5.5",
            "parent_section_id": "5",
            "section_name": "Difficulty levels analysis",
            "text": "To explore how humans and LLMs perceive the difficulty of questions, we examined the accuracy of different models across three difficulty levels. Although our dataset included five labels ranging from easy to difficult to denote question difficulty, we combined the two relatively easy and relatively difficult labels into \u201ceasy\u201d and \u201cdifficult\u201d, respectively, to ensure a more balanced distribution of question difficulty. Our experiment revealed a consistent trend: as question difficulty increased, both human and model answering accuracy decreased (See Tables 12  ###reference_###, 13  ###reference_###, 14  ###reference_### in the Appendix E  ###reference_###).\nNotably, in analytical and knowledge-based topics such as the humanities category, the GPT-4 model demonstrated superior performance compared to humans in tackling difficult questions. Conversely, models exhibiting more human-like performance, such as GPT-4, revealed that humans significantly outperformed them on easier questions.\nFor additional statistical results, refer to Appendix G  ###reference_###."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusions",
            "text": "We introduced Khayyam Challenge, also known as PersianMMLU, as the first framework for assessing LLMs in the Persian language across various tasks, difficulty levels, and educational stages. This framework includes comprehensive metadata such as human performance, difficulty levels, and traps. Our assessment encompassed examining the performance of current LLMs on these datasets, evaluating their ability to extract answers (probabilistic and rule-based paradigms), and considering various aspects highlighted in the metadata. Our findings revealed that while LLMs demonstrated relatively satisfactory performance in question-solving tasks (especially on GPT-4), they still significantly lagged behind human performance, particularly in tasks necessitating reasoning. Moreover, analysis of metadata concerning difficulty levels and trapped questions unveiled notable discrepancies between model and human behavior, suggesting fundamental differences in learning approaches. This underscores the necessity for adaptations in LLMs training methodologies to achieve human-like proficiency. For future works, we aim to develop an LLM that bridges the performance gap between existing open-source models and GPT-4."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Data Restriction",
            "text": "This dataset is distributed under a Creative Commons No Derivatives (CC ND) license, prohibiting the creation of derivative works. It is designated exclusively for non-commercial, academic research to prevent conflicts with Kanoon Farhangi Amoozesh\u2019s (Cultural Educational Center) educational activities. Researchers are obligated to adhere to this condition, ensuring their utilization of the dataset remains confined to academic research purposes and respects the restrictions of the CC ND license."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Acknowledgement",
            "text": "The authors would like to express their sincere gratitude to Pellekan Yadgiri (Learning Ladder), a sub-organization of Kanoon Farhangi Amoozesh (Cultural Educational Center), for providing the valuable dataset and related metadata that was crucial for this research. We are particularly grateful to Masoud Tajfard for his kind assistance in facilitating the acquisition of this data, which was permitted for research purposes. The dataset and metadata made available by Pellekan Yadgiri were instrumental in enabling the analyses and findings presented in this work."
        }
    ],
    "url": "http://arxiv.org/html/2404.06644v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.4"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4",
            "5",
            "5.1",
            "5.2",
            "5.3",
            "5.4",
            "5.5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ]
    },
    "research_context": {
        "paper_id": "2404.06644v1",
        "paper_title": "Khayyam Challenge (PersianMMLU): Is Your LLM Truly Wise to The Persian Language?",
        "research_background": "### Motivation\nLarge Language Models (LLMs) have significantly impacted machine intelligence applications, leading to wide deployments in industrial and public sectors. Consequently, it has become crucial to evaluate these models' capabilities accurately. Existing benchmarks like the MMLU have been instrumental in assessing LLMs across diverse subjects but are limited primarily to English. When it comes to non-English languages, cultural specifics and translation inaccuracies pose significant challenges. This concern is especially pertinent for Persian, a language rich in culture and literature that requires nuanced understanding beyond mere translations.\n\n### Research Problem\nThe primary research problem addressed in this paper is the inadequacy of current benchmarks, such as MMLU, in evaluating LLMs' understanding of the Persian language. These challenges include cultural specificity, translation inaccuracies, and the difficulties inherent in rendering domain-specific terms accurately in Persian. The paper aims to introduce a new benchmark, the Khayyam Challenge (PersianMMLU), designed specifically for the Persian language. This includes a comprehensive evaluation across various subjects, educational stages, and specialized domains, with questions verified and validated by subject matter experts.\n\n### Relevant Prior Work\n1. **MMLU Benchmark (Hendrycks et al., 2021)**: Designed to assess LLMs across a wide array of subjects, primarily in English.\n2. **Challenges in Automatic Translations**: Studies revealing limitations in translating domain-specific terms and culturally specific content, including GPT-3.5's translation issues.\n3. **Non-English Benchmarks**:\n   - **Chinese Dataset (Li et al., 2023)**: Evaluated LLMs on 67 topics, highlighting their struggles in achieving high accuracy.\n   - **Multilingual and Multimodal Dataset (Zhang et al., 2023)**: Demonstrated the difficulties in achieving accuracy in multilingual text processing.\n4. **Previous Persian Datasets**:\n   - **ParsiNLU (Khashabi et al., 2021)**: A Persian dataset focusing on natural language understanding.\n   - **ParSQuAD (Abadani et al., 2021)**: A Persian QA dataset based on SQuAD but lacking cultural and linguistic nuances.\n   - **PersianQA (Ayoubi, 2021) and PQuAD (Darvishi et al., 2023)**: Extractive QA datasets unfit for assessing broader language understanding and reasoning skills.\n\n### Novelty and Contributions\nThe Khayyam Challenge aims to fill the identified gaps by:\n- Offering a benchmark originally constructed in Persian, incorporating nuanced semantics and cultural intricacies.\n- Covering a wide array of subjects from the natural sciences to humanities, demanding both computational ability and cultural sensitivity.\n- Providing comprehensive metadata for each question to facilitate deeper evaluations.\n\nThis benchmark aims to provide a holistic evaluation framework reflecting the diverse linguistic and cognitive challenges inherent in processing Persian text across various domains.",
        "methodology": "### Methodology\n\nThe Khayyam Challenge proposes a comprehensive and meticulously curated dataset specifically designed to assess the effectiveness of large language models (LLMs) in the Persian language, with a particular focus on their proficiency in handling multiple-choice question answering tasks. The innovation of this dataset lies in its ability to cover a wide array of subjects, thereby providing a holistic means of evaluating various cognitive skills intrinsic to effective language models. These skills include, but are not limited to, language comprehension, reasoning, and knowledge recall. \n\n#### Key Components:\n\n1. **Diverse Range of Subjects:** The dataset includes questions from a broad spectrum of subjects, ensuring a well-rounded evaluation of the LLMs' capabilities.\n   \n2. **Multiple-Cognitive Abilities Assessment:** It is designed to evaluate different cognitive functions such as:\n    - **Language Comprehension:** Understanding and interpreting text.\n    - **Reasoning:** Logical thinking and problem-solving.\n    - **Knowledge Recall:** Reproducing information learned over various stages of education.\n\n3. **Educational Segments:** The dataset is structured to reflect the various stages of the formal educational system in Iran. This system is segmented as follows:\n   - **Primary Education:**\n     - **Lower Primary School (LPS):** The first 3 years of schooling.\n     - **Upper Primary School (UPS):** Subsequent 3 years following LPS.\n   - **Secondary Education:**\n     - **Lower Secondary School (LSS):** The initial 3 years of secondary education.\n     - **Upper Secondary School (USS):** The final 3 years culminating in high school graduation.\n\n#### Innovations:\n\n1. **Comprehensive Representation of Educational System:** By mirroring the structure of Iran\u2019s educational system, the dataset provides nuanced levels of difficulty corresponding to the different stages of education.\n\n2. **Holistic Evaluation:** Unlike other datasets that might focus purely on language comprehension or isolated cognitive tasks, the Khayyam Challenge's approach ensures a more well-rounded evaluation across multiple cognitive dimensions.\n\n3. **Contextualized Learning Stages:** The division of questions according to specific educational stages (LPS, UPS, LSS, USS) allows for targeted evaluation of an LLM's ability to grasp and apply knowledge at progressively complex levels.\n\nIn summary, the Khayyam Challenge dataset stands out due to its detailed and structured approach that not only aligns with the educational stages in Iran but also extensively covers different subject matters. This makes it a valuable resource for thoroughly testing the Persian language proficiency of large language models in a diverse and comprehensive manner.",
        "main_experiment_and_results": "Sure, here is a concise description of the main experiment setup and results:\n\n**Main Experiment Setup:**\n\n- **Datasets:** This experiment utilizes the PersianMMLU dataset, which is specifically designed to test the Persian language comprehension abilities of language models. PersianMMLU includes a range of questions across various subjects.\n\n- **Baselines:** Several baseline models are employed to evaluate the performance in the Persian language:\n  - **GPT-3 and GPT-4:** These models are developed by OpenAI and are widely used benchmark language models.\n  - **BERT-Based Models:** Models such as ParsBERT, a pre-trained BERT model specialized for the Persian language.\n\n- **Evaluation Metrics:** The primary evaluation metric for this experiment is accuracy. This is calculated as the percentage of correctly answered questions out of the total number of questions in the dataset.\n\n**Main Experimental Results:**\n\nThe main experimental results demonstrate the performance of different models on the PersianMMLU dataset. The results indicate that:\n\n- GPT-4 achieves the highest accuracy among the evaluated models, outperforming GPT-3.\n- ParsBERT and other BERT-based models also show competitive performance but do not surpass the GPT-4 model.\n- There is a clear distinction in performance metrics, with advanced transformer models like GPT-4 demonstrating superior understanding and language comprehension abilities compared to older or less specialized models.\n\nThe results suggest that GPT-4 exhibits significant advancements in Persian language processing, confirming its robustness across a diverse array of topics within the dataset."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the performance of different Large Language Models (LLMs) on the Khayyam Challenge (PersianMMLU) dataset, focusing on various prompt templates and Chain-of-Thought (CoT) processes.",
            "experiment_process": "The evaluation involved nine distinct models: GPT-4, GPT-3.5, Aya, XVERSE-13B, XVERSE-7B, PersianMind, mT0XL, mGPT, and Claude 3/Haiku. A standardized prompt-0 template was used on the entire dataset, while prompt-1, prompt-7, and CoT were applied to subsets of 1000 samples. All models had their hyperparameters at default values and a temperature parameter set to zero without any token limit to ensure consistency.",
            "result_discussion": "The study does not explicitly mention individual model performances or comparative results. The focus on consistency in hyperparameters and the use of subsets for specific templates suggests an effort to identify optimal model settings and processes for the Persian language dataset.",
            "ablation_id": "2404.06644v1.No1"
        },
        {
            "research_objective": "To compare the accuracy of different answer extraction techniques (Regex, Single Token Probability, and Full Answer Probability) for evaluating LLMs in the Khayyam Challenge",
            "experiment_process": "Three techniques were used: Regex, Single Token Probability, and Full Answer Probability. The Regex method involved tailored functions for each model, falling back on embeddings when Regex failed. Single Token Probability calculated the softmax probability of tokens to select answers. Full Answer Probability computed the sum of logarithm token probabilities, standardized to option length. Human accuracy metadata was also considered by selecting the majority human choice when applicable.",
            "result_discussion": "The Regex method demonstrated the highest accuracy among the three techniques, while the Full Answer Probability method showed a significant decrease in accuracy compared to others.",
            "ablation_id": "2404.06644v1.No2"
        },
        {
            "research_objective": "To evaluate the impact of translation quality on the performance of LLMs in the Khayyam Challenge.",
            "experiment_process": "Sample questions from the dataset were translated using experts and off-the-shelf models. LLM performance was then assessed on both sets of translated samples.",
            "result_discussion": "Performance decreased significantly when samples were translated by off-the-shelf models, highlighting the need for a dataset not dependent on translated data.",
            "ablation_id": "2404.06644v1.No3"
        },
        {
            "research_objective": "To investigate the limitations of few-shot learning approaches on the LLMs' performance for the Khayyam Challenge.",
            "experiment_process": "The zero-shot and Chain-of-Thought (CoT) methods were used to calculate accuracy. Previous studies indicated that few-shot learning on instruction-tuned models might not improve and could even decrease accuracy.",
            "result_discussion": "Few-shot techniques were avoided due to prior evidence suggesting a potential decrease in accuracy. The benchmark focused on zero-shot and CoT methods to ensure accurate evaluations.",
            "ablation_id": "2404.06644v1.No4"
        }
    ]
}