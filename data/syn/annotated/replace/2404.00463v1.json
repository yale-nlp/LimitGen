{
    "title": "Addressing Both Statistical and Causal Gender Fairness in NLP Models",
    "abstract": "Statistical fairness stipulates equivalent outcomes for every protected group, whereas causal fairness prescribes that a model makes the same prediction for an individual regardless of their protected characteristics. Counterfactual data augmentation (CDA) is effective for reducing bias in NLP models, yet models trained with CDA are often evaluated only on metrics that are closely tied to the causal fairness notion; similarly, sampling-based methods designed to promote statistical fairness are rarely evaluated for causal fairness. In this work, we evaluate both statistical and causal debiasing methods for gender bias in NLP models, and find that while such methods are effective at reducing bias as measured by the targeted metric, they do not necessarily improve results on other bias metrics. We demonstrate that combinations of statistical and causal debiasing techniques are able to reduce bias measured through both types of metrics. We employ Jigsaw\u2019s dataset, a comprehensive resource for assessing gender bias, in our evaluation process, allowing us to robustly test the effectiveness of the debiasing methods across various metrics. 111Code for reproducing the experiments is available at: https://github.com/hannahxchen/composed-debiasing",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Auditing NLP models is crucial to measure potential biases that can lead to unfair or discriminatory outcomes when models are deployed. Several methods have been proposed to quantify social biases in NLP models including intrinsic metrics that probe bias in the internal representations of the model (Caliskan et al., 2017  ###reference_b6###; May et al., 2019  ###reference_b39###; Guo and Caliskan, 2021  ###reference_b22###) and extrinsic metrics that measure model behavioral differences across protected groups (e.g., gender and race). In this paper, we focus on extrinsic metrics as they align directly with how models are used in downstream tasks (Goldfarb-Tarrant et al., 2021  ###reference_b21###; Orgad and Belinkov, 2022  ###reference_b43###).\nProposed extrinsic bias metrics can be categorized based on whether they correspond to a statistical or causal notion of fairness. A bias metric quantifies model bias based on a fairness criterion. Two common kinds of fairness criteria are statistical and causal fairness. Statistical fairness calls for statistically equivalent outcomes for all protected groups. Statistical bias metrics estimate the difference in prediction outcomes between protected groups based on observational data (Barocas et al., 2019  ###reference_b1###; Hardt et al., 2016  ###reference_b26###). Causal fairness shifts the focus from statistical association to identifying root causes of unfairness through causal reasoning (Loftus et al., 2018  ###reference_b36###). Causal bias metrics measure the effect of the protected attribute on the model\u2019s predictions via interventions that change the value of the protected attribute. A model satisfies counterfactual fairness, as defined by Kusner et al. (2017  ###reference_b33###), if the same prediction is made for an individual in both the actual world and in the counterfactual world in which the protected attribute is changed.\nWhile there is no consensus on which metric is the right one to use (Czarnowska et al., 2021  ###reference_b10###), most work on bias mitigation only uses a single type of metric in their evaluation. This is typically a metric that is closely connected to the proposed debiasing method. For example, counterfactual data augmentation (CDA) (Lu et al., 2019  ###reference_b38###), has been shown to reduce bias in NLP models. However, prior works that adopt this method often evaluate only on causal bias metrics and do not include any tests using statistical bias metrics (Park et al., 2018  ###reference_b44###; Lu et al., 2019  ###reference_b38###; Zayed et al., 2022  ###reference_b62###; Lohia, 2022  ###reference_b37###; Wadhwa et al., 2022  ###reference_b57###). We find only one exception\u2014Garg et al. (2019  ###reference_b20###) found causal debiasing exhibits some tradeoffs between statistical and causal metrics (Section 2.3  ###reference_###). This raises concerns about the effectiveness and reliability of these debiasing methods in settings where multiple fairness criteria may be desired.\nIn this work, we first show that methods designed to reduce bias according to one fairness criteria often do not reduce bias as measured by other bias metrics. Then, we propose training methods to achieve statistical and causal fairness for gender in NLP models. We focus on gender bias as it is a well-studied problem in the literature.\nContributions. We empirically show the differences between statistical and causal bias metrics and explain why optimizing one of them may not improve the other (Section 3  ###reference_###). We find that they may even disagree on which gender the model is biased towards. We cross-evaluate statistical and causal-based debiasing methods on both types of bias metrics (Section 4  ###reference_###), and find that debiasing methods targeted to one type of fairness may even make other bias metrics worse (Section 4.3  ###reference_###). We propose debiasing methods that combine statistical and causal debiasing techniques (Section 5  ###reference_###). Our results, summarized in Figure 1  ###reference_###, show that a combined debiasing method achieves the best overall results when both statistical and causal bias metrics are considered."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background",
            "text": "This section provides background on bias metrics based on statistical and causal notions of fairness and overviews bias mitigation techniques."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Bias Metrics",
            "text": "We consider a model fine-tuned for a classification task where the model makes predictions given inputs and the ground truths are .\n\nStatistical bias metrics.\nStatistical bias metrics quantify bias based on statistical fairness (also known as group fairness), which compares prediction outcomes between groups. Common statistical fairness definitions include demographic parity (DP), which requires equal positive prediction rates (PPR) for every group (Barocas et al., 2019 ###reference_b1###). Different from DP, equalized odds consider ground truths and demand equal true positive rates (TPR) and false positive rates (FPR) across groups (Hardt et al., 2016 ###reference_b26###).\nStatistical PPR gap () between binary genders (female) and (male) can be defined as (Zayed et al., 2022 ###reference_b62###):\nwhere the model predictions can be either 0 or 1. If , the model produces positive predictions for females more often than for males.\nStatistical TPR gap of binary genders for class can be formulated as (De-Arteaga et al., 2019 ###reference_b11###):\nA positive would mean that the model outputs the correct positive prediction for female inputs more often than for male inputs. Statistical FPR gap can be defined analogously as in Equation 1 ###reference_### (Appendix A ###reference_###).\n\nCausal bias metrics.\nCausality-based bias metrics for NLP models are usually based on counterfactual fairness (Kusner et al., 2017 ###reference_b33###), which requires the model to make the same prediction for the text input even when group identity terms in the input are changed. The evaluation set is usually constructed by perturbing the identity tokens in the inputs from datasets (Prabhakaran et al., 2019 ###reference_b49###; Garg et al., 2019 ###reference_b20###; Qian et al., 2022 ###reference_b50###) or by creating synthetic sentences from templates (Dixon et al., 2018 ###reference_b14###; Lu et al., 2019 ###reference_b38###; Huang et al., 2020 ###reference_b27###).\nFollowing Garg et al. (2019 ###reference_b20###), we can define causal gender gap for an input as:\nwhere the -operator enforces an intervention on gender. The term indicates the model\u2019s prediction for if the gender of were set to female. To identify the bias direction, we will consider the causal gap without the absolute value. More information on how we perform gender intervention on texts is given in Appendix B.3 ###reference_###.\nCausal PPR Gap () can be estimated by the average causal effect of the protected characteristic on the model\u2019s prediction being positive. (Rubin, 1974 ###reference_b53###; Pearl et al., 2016 ###reference_b46###):\nIf is zero, it would mean that gender has no influence on model\u2019s positive prediction outcome.\nTo compare with statistical TPR gap, we formulate causal TPR gap by averaging the TPR difference for each individual:\nSimilarly, we can define causal FPR gap as in Equation 2 ###reference_### (Appendix A ###reference_###).\n\nComparing statistical and causal bias metrics.\nThe key difference between statistical and causal metrics is how the test examples are selected and generated for evaluation. Statistical metrics are based on the original unperturbed examples, while causal metrics consider an additional perturbation process to generate test examples besides the original examples. Proponents of causal metrics argue that statistical metrics are based on observational data, which may contain spurious correlations and therefore cannot determine whether the protected attribute is the reason for the observed statistical differences (Kilbertus et al., 2017 ###reference_b31###; Nabi and Shpitser, 2018 ###reference_b41###). On the other hand, statistical metrics are easy to assess, whereas causal metrics require a counterfactual version of each instance. Due to the discrete nature of texts, we can conveniently generate counterfactuals at the intervention level by perturbing the identity terms in the sentences (Garg et al., 2019 ###reference_b20###). Yet, it is possible to produce ungrammatical or nonsensical sentences using such perturbations (Morris et al., 2020 ###reference_b40###). In addition, changing the identity terms alone may not be enough to hide the identity signals as there could be other terms or linguistic tendencies that are correlated with the target identity. Czarnowska et al. (2021 ###reference_b10###) provides a comprehensive comparison of existing extrinsic bias metrics in NLP.\n\nJigsaw.\nJigsaw is a dataset designed specifically for evaluating social bias in models related to hate speech and toxic comment classification. It includes a wide range of comments labeled for toxicity, obscenity, threats, and identity-based"
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Bias Mitigation",
            "text": "Bias mitigation techniques for NLP models can be categorized broadly based on whether the mitigation is done to the training data (pre-processing methods), to the learning process (in-processing), or to the model outputs (post-processing). Pre-processing methods attempt to mitigate bias by modifying the training data before training. Statistical methods adjust the distribution of the training data through resampling or reweighting. Resampling can be done by either adding examples for underrepresented groups (Dixon et al., 2018  ###reference_b14###; Costa-juss\u00e0 and de Jorge, 2020  ###reference_b9###) or removing examples for overrepresented groups (Wang et al., 2019  ###reference_b58###; Han et al., 2022  ###reference_b25###). Reweighting assigns a weight to each training example according to the frequency of its class label and protected attribute (Calders et al., 2009  ###reference_b5###; Kamiran and Calders, 2012  ###reference_b28###; Han et al., 2022  ###reference_b25###). Causal methods such as counterfactual data augmentation (CDA) augment the training set with examples substituted with different identity terms (Lu et al., 2019  ###reference_b38###). This is the same as data augmentation based on gender swapping (Zhao et al., 2018  ###reference_b66###; Park et al., 2018  ###reference_b44###). While both statistical and causal methods seek to balance the group distribution, CDA performs interventions on the protected attribute whereas resampling and reweighing do not modify the attribute in the examples. Previous works have also considered removing protected attributes De-Arteaga et al. (2019  ###reference_b11###). However, this \u201cfairness through blindness\u201d approach is ineffective as there may be other proxies correlate with the protected attributes (Chen et al., 2019  ###reference_b7###). In-processing methods incorporate a fairness constraint in the training process. The constraint can be either based on statistical fairness (Kamishima et al., 2012  ###reference_b30###; Zafar et al., 2017  ###reference_b61###; Donini et al., 2018  ###reference_b15###; Subramanian et al., 2021  ###reference_b56###; Shen et al., 2022b  ###reference_b55###) or causal fairness (Garg et al., 2019  ###reference_b20###). Adversarial debiasing methods train the model jointly with a discriminator network from a typical GAN as an adversary to remove features corresponding to the protected attribute from the intermediate representations (Zhang et al., 2018  ###reference_b64###; Elazar and Goldberg, 2018  ###reference_b17###; Li et al., 2018  ###reference_b35###; Han et al., 2021  ###reference_b24###) Post-processing methods adjust the outputs of the model at test time to achieve desired outcomes for different groups (Kamiran et al., 2010  ###reference_b29###; Hardt et al., 2016  ###reference_b26###; Woodworth et al., 2017  ###reference_b60###). Zhao et al. (2017  ###reference_b65###) use a corpus-level constraint during inference. Ravfogel et al. (2020  ###reference_b51###) remove protected attribute information from the learned representations."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Related Work",
            "text": "Garg et al. (2019  ###reference_b20###) is the only work that evaluates NLP models with both statistical and causal bias metrics. They evaluate toxicity classifiers trained with CDA and counterfactual logit pairing and observe a tradeoff between counterfactual token fairness and TPR gaps. Han et al. (2023  ###reference_b23###) is the only work that attempts to achieve both statistical and causal fairness through fair representational learning on tabular data.\n\nPrevious work has studied the impossibility theorem of statistical fairness, which states that, for binary classification, equalizing multiple common statistical bias metrics between protected attributes is impossible unless the distribution of outcome is equal for both groups (Kleinberg et al., 2016  ###reference_b32###; Chouldechova, 2017  ###reference_b8###; Bell et al., 2023  ###reference_b2###). While these works focus on tabular data and statistical bias metrics, our work studies statistical and causal bias metrics used for NLP tasks.\n\nComparison between various bias metrics for NLP models has also been explored. Intrinsic and extrinsic bias metrics have been shown to have no correlation with each other (Delobelle et al., 2022  ###reference_b12###; Cabello et al., 2023  ###reference_b4###). Delobelle et al. (2022  ###reference_b12###) also shows that the measure of intrinsic bias varies depending on the choice of words and templates used for evaluation. Shen et al. (2022a  ###reference_b54###) find no correlation between statistical bias metrics and an adversarial-based bias metric, which measures the leakage of protected attributes from the intermediate representation of a model.\n\nDwork et al. (2012  ###reference_b16###) proposes individual fairness, which demands similar outcomes to similar individuals. This is similar to counterfactual fairness in the sense that two similar individuals can be considered as counterfactuals of each other (Loftus et al., 2018  ###reference_b36###; Pfohl et al., 2019  ###reference_b48###). The difference is that individual fairness considers similar individuals based on some distance metrics while counterfactual fairness considers a counterfactual example for each individual from a causal perspective. Zemel et al. (2013  ###reference_b63###) proposes learning representations with group information sanitized and individual information preserved to achieve both individual and group (statistical) fairness."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Bias Metrics Are Disparate",
            "text": "Disparities between different statistical fairness definitions and group and individual fairness have been studied in the tabular data settings (Section 2.3  ###reference_###). We focus on the most common type of bias metrics, statistical and causal, used for evaluating NLP tasks. We first explain why statistical and causal bias metrics may produce inconsistent results. We then report on the experiments to measure disparities between the metrics on evaluating gender bias in an occupation classification task using the Jigsaw dataset, which consists of a variety of user comments labeled to identify the presence of various forms of bias. This dataset provides a platform for evaluation and benchmarking, being particularly useful for tasks related to measuring and mitigating biased language."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Statistical does not Imply Causal Fairness",
            "text": "While correlation and causation can happen simultaneously, correlation does not imply causation (Fisher, 1958  ###reference_b19###). Correlation refers to the statistical dependence between two variables. Statistical correlation is not causation when there is a confounding variable that influences both variables (Pearl, 2009  ###reference_b45###), leading to spurious correlations (Pearson, 1896  ###reference_b47###).\nTo equate statistical estimates with causal estimates, the exchangeability assumption must be satisfied (Neal, 2015  ###reference_b42###). This means that the potential outcome of a protected group is independent of the group assignment. The model\u2019s prediction outcome should be the same even when the groups are swapped. One common way to achieve this is through randomized control trials by randomly assigning individuals to different groups (Fisher, 1935  ###reference_b18###), making the groups more comparable. In the case of bias evaluation, it is impossible to assign gender or identity to a person randomly. Furthermore, most data are sampled from the Internet, which does not guarantee diversity and may still encode bias (Bender et al., 2021  ###reference_b3###). Despite the disparities between statistical and causal bias estimation, it does not entail that achieving both statistical and causal fairness is impossible."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Evaluation",
            "text": "Task. We use the Jigsaw dataset composed of text data that includes various types of conversations and posts, which may contain toxic content. The task is to assess and improve model performance on recognizing and mitigating bias in such online interactions. For our study, we focus on understanding how models handle varying levels of toxicity and bias in comments, which necessitates robust classification strategies. We utilize the partitioned data from Jigsaw to allow for consistent evaluation across different model configurations (details of the partitions can be found in Appendix B.3).\n\nSetup. We fine-tune ALBERT-Large (Lan et al., 2020) and BERT-Base-Uncased (Devlin et al., 2019) on the Jigsaw dataset with normal training. We then evaluate the models with statistical and causal TPR gap.\n\nResults. Figure 2 shows the statistical and causal TPR gap for ALBERT and BERT models. Each data point represents the TPR gap of a specific category evaluated over the test examples with the corresponding label. The results reveal the disparity between statistical estimation and causal estimation. Most categories are off the red dashed line. For nearly all categories, the TPR is closer to zero than the causal TPR. In addition, we find a few cases where statistical and causal TPR show bias in opposite directions. Similar results are found for statistical and causal FPR gap (see Appendix D)."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Bag-of-Words Analysis",
            "text": "###figure_4### To test the extent to which statistical and causal bias metrics can capture gender bias, we train a Bag-of-Words (BoW) model with logistic regression on the Jigsaw dataset, where we can intentionally control the model\u2019s bias. We do this by identifying the model weights corresponding to gender signal tokens (Appendix B.3  ###reference_###) and multiplying the weights for these tokens by a weight. This allows us to tune the bias of a simple model and see how the different bias metrics measure the resulting bias.\nFigure 3  ###reference_### shows and of the BoW model when changing the weights for all gender-associated tokens. The magnitude of both bias scores increases as we increase the weighting of the gender tokens. The model is biased in the opposite gender direction when we reverse the weight by multiplying by a negative value. This demonstrates that both metrics are indeed able to capture bias in the model and, for the most part, reflect the amount of bias in the expected direction. Note that for all occupations when. This is because considers the average difference between pairs of sentences that only differ in tokens representing the gender. When, the model would exclude all gender tokens and each sentence pair would render the same to the model. On the other hand, is nonzero for most occupations when, meaning that it captures gender bias beyond explicit gender indicators. This suggests models trained to achieve causal fairness may still be biased toward other implicit gender features not identified in our explicit gender token list.\nThe spikes in Figure 3  ###reference_### may be attributed to the relatively large gap in token weights between the two genders for predicting the occupation, as shown in Figure 11  ###reference_###. The increased TPR gap is particularly significant for occupations with positive token weights for the dominant gender and negative token weights for the other gender, such as rapper and paralegal. In one extreme case, both gender token weights are positive for physician, with female tokens having a lot higher weight value than male tokens. This results in a huge TPR gap increase only in the negative direction when applying a larger negative value of.\n###figure_5### We further analyze how model weights of individual gender affect bias scores. Figure 4  ###reference_### shows the statistical and causal TPR gap of each occupation when increasing female token weights, and Figure 10  ###reference_### (in Appendix D.2  ###reference_###) shows the results of increasing male token weights. We observed that increasing female token weights has a greater effect on increasing the TPR gap of male-biased occupations (on the left side of the grey dashed line in Figure 4  ###reference_###), and vice versa. In addition, some occupations (as highlighted in red) show an increased TPR gap to the opposite gender bias direction of their bias scores indicated by the metric when. For instance, filmmaker, architect, and pastor are female-biased based on the statistical metric but become male-biased when increasing the female token weights due to their negative weight values (Figure 11  ###reference_###). We find that these occupations are the ones that the two metrics contradict in the bias direction (Table 3  ###reference_###). However, both metrics show similar patterns and directions of TPR gap increase across occupations (Figure 12  ###reference_###). The only difference is the starting point of TPR gap score when."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Cross-Evaluation",
            "text": "This section cross-evaluates the effectiveness of existing debiasing methods on gender bias in an occupation classification and toxicity detection task. We show using statistical and causal debiasing methods alone may not achieve both types of fairness."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Setup",
            "text": "We focus on pre-processing methods since Shen et al. (2022b  ###reference_b55###) found that resampling and reweighting achieve better statistical fairness than the in-processing and post-processing methods. For the statistical methods, we apply both resampling using oversampling (OS) and undersampling (US) and reweighting (RW) using the weight calculation from Kamiran and Calders (2012  ###reference_b28###). For the causal methods, we fine-tune the model with CDA. We apply each debiasing method to the ALBERT-Large (Lan et al., 2020  ###reference_b34###) and BERT-Base-Uncased (Devlin et al., 2019  ###reference_b13###) models. We also include experiments with Jigsaw, which is an ALBERT-Large model pre-trained with CDA. To consider the effect of CDA during pre-training alone and during both pre-training and fine-tuning, we fine-tune the Jigsaw model with normal training and CDA. Training details are provided in Appendix E  ###reference_###."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Tasks",
            "text": "We test all the models on two benchmark tasks for bias detection: occupation classification and toxicity detection.\n\nOccupation Classification. We use the Jigsaw dataset consisting of approximately 1.8M comments taken from the Civil Comments platform. The task is to predict the toxicity score of each comment. For our experiments, we use binary toxicity labels, toxic and non-toxic. In addition to the toxicity score, a subset of examples are labeled with the identities mentioned in the comment. We only select the examples labeled with female and male identities and with high annotator agreement on the gender identity labels. Since some examples contain a mix of genders, we assign the gender to each example based on the gender labeled with the highest agreement. To perform gender intervention with CDA, we use the gender-bender Python package to generate counterfactual examples 222{https://github.com/Garrett-R/gender_bender} ###reference_der%7D###. Appendix C.1 ###reference_### provides details on how we preprocess the data. Following Zayed et al. (2022 ###reference_b62###), we compute statistical and causal PPR gap. As female and male groups do not have the same label distribution, the PPR gap of a perfect predictor will be non-zero. Therefore, we also compute statistical and causal TPR gap for toxic and non-toxic classes.\n\nToxicity Detection. We use the Jigsaw dataset consisting of approximately 1.8M comments taken from the Civil Comments platform. The task is to predict the toxicity score of each comment. For our experiments, we use binary toxicity labels, toxic and non-toxic. In addition to the toxicity score, a subset of examples are labeled with the identities mentioned in the comment. We only select the examples labeled with female and male identities and with high annotator agreement on the gender identity labels. Since some examples contain a mix of genders, we assign the gender to each example based on the gender labeled with the highest agreement. To perform gender intervention with CDA, we use the gender-bender Python package to generate counterfactual examples 222{https://github.com/Garrett-R/gender_bender} ###reference_der%7D###. Appendix C.1 ###reference_### provides details on how we preprocess the data. Following Zayed et al. (2022 ###reference_b62###), we compute statistical and causal PPR gap. As female and male groups do not have the same label distribution, the PPR gap of a perfect predictor will be non-zero. Therefore, we also compute statistical and causal TPR gap for toxic and non-toxic classes."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Results",
            "text": "###figure_7### Occupation classification. Figure 5  ###reference_### and Figure 6  ###reference_### show statistical and causal TPR gap per occupation evaluated on BERT and ALBERT models with each debiasing method. Causal debiasing methods show greater effectiveness when evaluated with the causal metric (we discuss the combination methods included in these figures in Section 5  ###reference_###). Fine-tuning with CDA reduces to nearly zero for all occupations, but does not produce any significant reduction for . On the other hand, Zari exhibits higher statistical and causal gap than performing CDA during fine-tuning (Figure 6  ###reference_###). Thus, using CDA during pre-training alone is insufficient to reduce bias. Statistical debiasing methods such as undersampling and reweighting reduce bias on both statistical and causal metrics, though the bias reduction on the causal metric is not as significant as CDA. We find that oversampling is less effective than other statistical debiasing methods on both metrics. We found similar results with statistical and causal FPR gaps (Appendix F.2  ###reference_###).\nToxicity detection.\nTable 1  ###reference_### shows the bias evaluation results for the BERT model trained with different debiasing methods on the Jigsaw dataset. We find that statistical and causal bias metrics sometimes disagree on which gender the model is biased toward. Similar to the results for the Jigsaw task, statistical and causal debiasing methods do particularly well on the bias metrics based on their targeted fairness definition. However, they increase bias on metrics that use the other type of fairness notion. Similar results are found for ALBERT model (Appendix G.1  ###reference_###)."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Achieving Both Statistical and Causal Fairness",
            "text": "In the previous section, we saw that using either statistical or causal debiasing method alone may not achieve both statistical and causal fairness. To counter this problem, this section considers simple methods that combine both statistical and causal debiasing techniques."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Composed Debiasing Methods",
            "text": "We introduce three approaches that combine techniques from both statistical and causal debiasing:\nResampling with CDA. OS-CDA and US-CDA combine resampling methods (oversampling and undersampling) with CDA. For Jigsaw, we balance the original examples based on the original gender and the counterfactual examples based on the counterfactual gender.\nReweighting with CDA. RW-CDA applies CDA on the training set and fine-tunes the model with reweighting. For Jigsaw, we use weight of 1 for all counterfactual examples.\nWe use different combination strategies for the two datasets as we noticed the methods used for the BiasBios dataset do not work well on the Jigsaw dataset. This may be due to the mix of genders in a subset of examples in the Jigsaw dataset. The gender signals in the examples may be flipped after performing CDA. We provide performance comparisons between the different combination strategies we have tried on the Jigsaw task in Appendix G.2."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Results",
            "text": "Figure 5  ###reference_### and Figure 6  ###reference_### show statistical and causal TPR gap per occupation evaluated on the Jigsaw dataset for BERT and ALBERT models. The combined methods US-CDA and RW-CDA are more effective at reducing bias on both metrics compared to other methods. To compare overall performance, we show the root mean square of each bias metric in Table 4  ###reference_### and Table 5  ###reference_### (both in Appendix F.1  ###reference_###). All three combination approaches perform better compared to using a statistical or causal debiasing method alone. OS-CDA and US-CDA also reduce bias on  (11\u201316% decrease) and  (1\u20138% decrease), comparing to their statistical debiasing counterparts. RW-CDA achieves comparable performance on  to reweighting. Undersampling and US-CDA sacrifice the general performance with a decrease of around 0.7% in accuracy compared to other methods, which preserve the baseline accuracy within 0.3%.\n\nTable 1  ###reference_### and Table 6  ###reference_### (Appendix G.1  ###reference_###) report the results of BERT and ALBERT models for the Jigsaw dataset. While statistical and causal debiasing methods only improve one type of bias metric and worsen the other, our proposed combination approaches are able to reduce bias on both types of bias metrics. The combined methods OS-CDA and US-CDA perform better than CDA on all causal bias metrics. RW-CDA performs better on  but is less effective at reducing bias on  compared to the other combination approaches."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Summary",
            "text": "We demonstrate the disparities between statistical and causal bias metrics and provide insight into how and why optimizing based on one type of metric does not necessarily improve the other. We show this by cross-evaluating existing statistical and causal debiasing methods on both metrics and find that they sometimes may even worsen the other type of bias metrics. To obtain models that perform well on both types of bias metrics, we introduce simple debiasing strategies that combine both statistical and causal debiasing techniques."
        }
    ],
    "url": "http://arxiv.org/html/2404.00463v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2",
            "2.3"
        ],
        "methodology_sections": [
            "4.1",
            "5.1"
        ],
        "main_experiment_and_results_sections": [
            "3.2",
            "4",
            "4.3",
            "5.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.2",
            "3.3",
            "4.1",
            "4.3",
            "5.1",
            "5.2"
        ]
    },
    "research_context": {
        "paper_id": "2404.00463v1",
        "paper_title": "Addressing Both Statistical and Causal Gender Fairness in NLP Models",
        "research_background": "### Motivation:\nThe motivation for this paper stems from the critical need to audit NLP models for biases that may result in unfair or discriminatory outcomes once these models are deployed. There has been an extensive body of work on quantifying social biases in NLP models, distinguishing between intrinsic and extrinsic metrics. The authors emphasize the importance of evaluating extrinsic metrics, as these are more directly related to the model's performance on real-world tasks.\n\n### Research Problem:\nThe core research problem addressed by this paper is the lack of a unified approach to measuring and mitigating gender bias in NLP models using both statistical and causal fairness criteria. While existing methods often focus exclusively on one type of fairness metric, usually connected to the studied debiasing method, this may lead to incomplete or even misleading conclusions about a model's fairness. There is also a noted absence of comprehensive evaluation across both types of bias metrics, potentially diminishing the reliability of debiasing methods in varied practical applications.\n\n### Relevant Prior Work:\n1. **Intrinsic and Extrinsic Bias Metrics:**\n   - Intrinsic metrics probe biases within the model's internal representations (Caliskan et al., 2017; May et al., 2019; Guo and Caliskan, 2021).\n   - Extrinsic metrics measure behavioral differences in model predictions across protected groups (Goldfarb-Tarrant et al., 2021; Orgad and Belinkov, 2022).\n\n2. **Statistical Fairness:**\n   - Statistical bias metrics derive from observational data and measure differences in prediction outcomes between groups (Barocas et al., 2019; Hardt et al., 2016).\n\n3. **Causal Fairness:**\n   - Causal fairness uses causal reasoning to identify biases and often employs interventions to assess the effect of protected attributes on model predictions, with counterfactual fairness being a key criterion (Loftus et al., 2018; Kusner et al., 2017).\n\n4. **Bias Mitigation Methods:**\n   - Current debiasing methods, like counterfactual data augmentation (CDA), typically assess fairness only through causal bias metrics (Park et al., 2018; Lu et al., 2019; Zayed et al., 2022; Lohia, 2022; Wadhwa et al., 2022), neglecting the statistical perspective.\n   - The work by Garg et al. (2019) is an exception, noting a tradeoff between statistical and causal fairness when employing causal debiasing methods.\n\nThus, the relevant prior work highlights a distinct gap where most bias mitigation methods are tailored to and evaluated by a single fairness criterion, prompting the need for a more integrative approach that considers both statistical and causal fairness in NLP models.",
        "methodology": "Methodology: We focus on pre-processing methods since Shen et al. (2022b) found that resampling and reweighting achieve better statistical fairness than the in-processing and post-processing methods. For the statistical methods, we apply both resampling using oversampling (OS) and undersampling (US) and reweighting (RW) using the weight calculation from Kamiran and Calders (2012). For the causal methods, we fine-tune the model with CDA.\n\nWe apply each debiasing method to the ALBERT-Large (Lan et al., 2020) and BERT-Base-Uncased (Devlin et al., 2019) models. We also include experiments with Zari (Webster et al., 2020), which is an ALBERT-Large model pre-trained with CDA. To consider the effect of CDA during pre-training alone and during both pre-training and fine-tuning, we fine-tune Zari with normal training and CDA. Training details are provided in Appendix E.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Task\nThe main task is to predict the occupation from biographies with the occupation title removed, using the BiasBios dataset (De-Arteaga et al., 2019). This dataset includes nearly 400,000 online biographies of 28 unique occupations, scraped from the CommonCrawl. Each biography contains the name and pronouns of the subject, and the gender is determined by a predefined list of explicit gender indicators.\n\n#### Setup\n* **Datasets**: BiasBios dataset with train-dev-test split as used by Ravfogel et al. (2020). Different data preprocessing steps are applied to the biographies (details in Appendix B.2).\n* **Models**:\n  * ALBERT-Large (Lan et al., 2020)\n  * BERT-Base-Uncased (Devlin et al., 2019)\n* **Training**: Both ALBERT-Large and BERT-Base-Uncased are fine-tuned on the BiasBios dataset using standard training procedures.\n* **Evaluation Metrics**: The evaluation is based on the statistical and causal True Positive Rate (TPR) gap.\n\n#### Results\nKey observations include:\n* There is a notable disparity between statistical estimation and causal estimation for most occupations.\n* For nearly all occupations, the causal TPR gap (\n( \n    \n\n\nmathbb Delta _{TPR}^{\text {causal}} \n\n\n)) is closer to zero than the statistical TPR gap (\n( \n    \n\n\nmathbb Delta _{TPR}^{\text {stat}} \n\n\n)).\n* In certain cases, such as the occupations \"dj\" and \"pastor,\" the statistical and causal TPR gaps indicate bias in opposite directions.\n\nSimilar trends are observed for the statistical and causal False Positive Rate (FPR) gaps, which are detailed in Appendix D."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Evaluate the effectiveness of different statistical and causal debiasing methods on reducing gender bias in NLP models.",
            "experiment_process": "The authors used the BiasBios dataset, which comprises nearly 400,000 online biographies for 28 unique occupations. They fine-tuned ALBERT-Large and BERT-Base-Uncased on the dataset and evaluated them using statistical and causal TPR gaps. The test examples were used to calculate the TPR gaps for each occupation, and results were visualized to show the disparities between statistical and causal estimations.",
            "result_discussion": "The results showed that most occupations' causal TPR gaps were closer to zero compared to statistical TPR gaps, indicating a disparity between the two estimations. Some occupations showed bias in opposite directions in the two metrics, such as DJ and pastor.",
            "ablation_id": "2404.00463v1.No1"
        },
        {
            "research_objective": "Assess the capability of statistical and causal bias metrics to capture gender bias in NLP models.",
            "experiment_process": "The authors trained a Bag-of-Words (BoW) model with logistic regression on the BiasBios dataset while controlling the model's bias by adjusting the weights of gender-associated tokens. They evaluated the model's bias under different weight values using statistical and causal bias metrics.",
            "result_discussion": "Both metrics (statistical and causal TPR gaps) effectively captured the model's bias, generally reflecting the increasing bias magnitude as the gender token weights were adjusted. Causal fairness metrics were more sensitive to subtle biases not represented by explicit gender tokens, suggesting models trained for causal fairness might still inherit implicit biases.",
            "ablation_id": "2404.00463v1.No2"
        },
        {
            "research_objective": "Evaluate pre-processing debiasing methods (resampling and reweighting) and causal debiasing methods (CDA) on reducing gender bias in NLP models.",
            "experiment_process": "The ALBERT-Large and BERT-Base-Uncased models were trained with oversampling, undersampling, reweighting, and counterfactual data augmentation (CDA) methods. Zari, an ALBERT-Large model pre-trained with CDA, was also included for comparison by further fine-tuning it using regular and CDA methods.",
            "result_discussion": "Causal debiasing (CDA during fine-tuning) was highly effective in reducing causal TPR gaps but not statistical TPR gaps. Using Zari resulted in higher statistical and causal gaps than regular fine-tuning with CDA. Statistical methods such as reweighting and undersampling were effective across both metrics but less so for causal gaps compared to CDA. Oversampling was generally less effective across both metrics.",
            "ablation_id": "2404.00463v1.No3"
        },
        {
            "research_objective": "Determine the effectiveness of composed debiasing methods combining statistical with causal techniques on reducing bias in NLP models.",
            "experiment_process": "Three approaches were introduced: resampling with CDA (OS-CDA and US-CDA) and reweighting with CDA (RW-CDA). The methods were applied to BERT and ALBERT models on two datasets, BiasBios and Jigsaw, with performance evaluated using statistical and causal TPR gaps.",
            "result_discussion": "The combined methods, especially US-CDA and RW-CDA, achieved better bias reduction across both metrics compared to using any single debiasing method. Specifically, the combined approaches reduced bias by 11-16% for US-CDA and 1-8% for RW-CDA. RW-CDA was comparable to reweighting alone on the causal metric. However, a slight decrease in accuracy was noted for undersampling and US-CDA. For the Jigsaw dataset, the combined methods consistently outperformed isolated statistical or causal debiasing techniques.",
            "ablation_id": "2404.00463v1.No4"
        }
    ]
}