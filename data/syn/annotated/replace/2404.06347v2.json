{
    "title": "RAR-b: Reasoning as Retrieval Benchmark",
    "abstract": "Semantic textual similarity (STS) and information retrieval tasks (IR) tasks have been the two major avenues to record the progress of embedding models in the past few years. Under the emerging Retrieval-augmented Generation (RAG) paradigm, we envision the need to evaluate next-level language understanding abilities of embedding models and take a conscious look at the reasoning abilities stored in them. Addressing this, we pose the question: Can retrievers solve reasoning problems? By transforming reasoning tasks into retrieval tasks, we find that without specifically trained for reasoning-level language understanding, current state-of-the-art retriever models may still be far from being competent for playing the role of assisting LLMs, especially in reasoning-intensive tasks. Moreover, albeit trained to be aware of instructions, instruction-aware IR models are often better off without instructions in inference time for reasoning tasks, posing an overlooked retriever-LLM behavioral gap for the research community to align. However, recent decoder-based embedding models show great promise in narrowing the gap, highlighting the pathway for embedding models to achieve reasoning-level language understanding. We also show that, although current off-the-shelf re-ranker models fail on these tasks, injecting reasoning abilities into them through fine-tuning still appears easier than doing so to bi-encoders, and we are able to achieve state-of-the-art performance across all tasks by fine-tuning a reranking model. We release C-STS, a holistic suite of tasks and settings to evaluate the reasoning abilities stored in retriever models. C-STS is available at https://github.com/gowitheflow-1998/RAR-b.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Semantic textual similarity (STS) and information retrieval tasks (IR) have been two principle measures to record the progress of dense representation models (Agirre et al., 2013 ###reference_b1###; 2014 ###reference_b2###; 2015 ###reference_b3###; 2016 ###reference_b4###; Cer et al., 2017 ###reference_b14###; Thakur et al., 2021 ###reference_b54###). Despite still heavily being evaluated in sentence representation research, STS is known for its limited alignment with real-world use cases (Neelakantan et al., 2022 ###reference_b40###; Muennighoff et al., 2023b ###reference_b38###), ambiguity (Deshpande et al., 2023 ###reference_b20###), and performance orthogonality with IR and other downstream tasks (Reimers et al., 2016 ###reference_b47###; Wang et al., 2021 ###reference_b57###; Xiao et al., 2023a ###reference_b64###).\n\nIn the LLM era, Retrieval-augmented Generation (RAG) (Lewis et al., 2020 ###reference_b30###; Neelakantan et al., 2022 ###reference_b40###; Xu et al., 2023 ###reference_b67###; Gao et al., 2023 ###reference_b21###) has become a go-to alternative method to vanilla end-to-end generative language models (OpenAI, 2023 ###reference_b43###; Touvron et al., 2023 ###reference_b55###). This shift is motivated by the inherent weaknesses of LLMs towards factual errors, due to hallucinations (Ji et al., 2023 ###reference_b26###), knowledge outdatedness (Vu et al., 2023 ###reference_b56###), rarity in long-tailed knowledge (Kandpal et al., 2023 ###reference_b28###; Malaviya et al., 2023 ###reference_b34###), and reasoning failure such as on logical deduction (Berglund et al., 2023 ###reference_b11###).\n\nRetrieval-Augmented Generation (RAG) is employed differently across various NLP tasks: For knowledge-intensive tasks, RAG is employed to retrieve the most up-to-date and reliable knowledge references (Vu et al., 2023 ###reference_b56###; Malaviya et al., 2023 ###reference_b34###), serving as new prompts for LLMs to extract information and formulate responses. This method mitigates models\u2019 natural tendencies to hallucinate (Ji et al., 2023 ###reference_b26###) and reduces the need for frequent fine-tuning of LLMs. In reasoning-dependent tasks, RAG aims to fetch the most relevant chunks from extensive inputs to guide the focus of LLMs, e.g., in multi-hop question answering scenarios where reasoning across chunks from multiple documents is required. Reasoning with such long context is not only impossible for LLMs with built-in short context windows, but also challenging for LLMs with long context capabilities (Xu et al., 2023 ###reference_b67###).\n\nDespite the promise shown by dense retrievers in fetching references for knowledge-intensive tasks, these systems still fall short in retrieving reliable and cite-worthy references (Malaviya et al., 2023 ###reference_b34###), compared to state-of-the-art proprietary LLMs (OpenAI, 2023 ###reference_b43###) in a standalone manner, highlighting the undesirable behavior of retrievers in assisting LLMs (BehnamGhader et al., 2023 ###reference_b10###; Asai et al., 2024 ###reference_b7###). This discrepancy is more pronounced in reasoning-intensive tasks, where retrieval-augmented generation methods present inconsistent gains, or even performance degradation to LLMs (Bai et al., 2023 ###reference_b9###; Xu et al., 2023 ###reference_b67###).\n\nWith the complexities of the role that dense representation models play in the LLM era, the need for accurately assessing their advanced language understanding abilities becomes crucial. We advocate for evaluating these models\u2019 capabilities beyond mere factual recall or semantic matching, focusing on their proficiency in complex thought processes and logical deduction.\n\nThis paper introduces the C-STS Benchmark, a novel framework that reframes reasoning tasks as retrieval tasks, offering a comprehensive reevaluation of \u201cthe actual reasoning representation compressed\u201d111we use \u201cC-STS\u201d to convey the connotation of \u201ccompression\u201d in dense representation models, and challenges the status quo of reasoning-oriented RAG. ###figure_1### Historically, lexical-based retrieval methods have long been seen as baselines of reasoning tasks (Clark et al., 2016 ###reference_b17###; Rogers et al., 2020"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "RAR-b",
            "text": "In this work, we construct and release RAR-b: Reasoning as Retrieval Benchmark. Deviating from evaluating on reasoning tasks with a full RAG pipeline (retriever+LLM), we instead focus on evaluating only the retrievers. By constructing reasoning tasks into retrieval tasks, we investigate how good retrievers are on solving them in a standalone manner, and use this as a proxy of the upperbound of retrievers\u2019 capabilities in assisting LLMs, in a standard RAG system. We design three levels of tasks, resulting in the integration of 12 tasks derived from 17 datasets. We convert the original datasets into retrieval formats with both multiple-choice retrieval and full-dataset retrieval settings. We first benchmark the performance of state-of-the-art bi-encoder models, spanning across three model categories: unsupervised dense retrieval models, supervised dense retrieval models, and instruction-aware dense retrieval models. We evaluate both representative open-source models and proprietary models such as Cohere and OpenAI Embedding Models. We further benchmark the performance of representative re-ranking models, both on using them to solve multiple-choice retrieval setting independently, and on further re-ranking the documents retrieved by bi-encoders in the Full-dataset retrieval setting. The datasets we consider can all be seen as reasoning problems. Firstly, we include commonsense reasoning datasets, NLI (Bhagavatula et al., 2020  ###reference_b12###), HellaSwag (Zellers et al., 2019  ###reference_b70###), PIQA (Bisk et al., 2020  ###reference_b13###), SocialiQA (Sap et al., 2019  ###reference_b50###), ARC-Challenge (Clark et al., 2018  ###reference_b18###), Quail (Rogers et al., 2020  ###reference_b48###), Winogrande (Sakaguchi et al., 2021  ###reference_b49###), and C-STS (Deshpande et al., 2023  ###reference_b20###). For all datasets in the full retrieval settings, we pool all the candidate answers across all splits to serve as the corpus, aligning with the setting in most retrieval benchmarks, and evaluate them using test set questions as the queries. All commonsense reasoning datasets are suitable to test in Full-dataset retrieval setting, except for C-STS (Deshpande et al., 2023  ###reference_b20###) due to the potential sparse annotation problem when constructing it into Full-dataset retrieval settings (Thakur et al., 2021  ###reference_b54###) (detailed explanation in Appendix B  ###reference_###). Apart from commonsense reasoning abilities, we further envision the possession of temporal, spatial, numerical, and symbolic reasoning abilities in dense retrievers. Temporal and spatial reasoning abilities are evaluated respectively with C-STS and SpartQA (Mirzaee et al., 2021  ###reference_b35###). For C-STS, we evaluate each of its sub-level tasks separately, and construct the pure, fact and context settings to assess different aspects of retrievers\u2019 behaviors (detailed analyses in Section 4.1  ###reference_###). For numerical reasoning abilities, we concatenate MATH (Hendrycks et al., 2021  ###reference_b22###) and GSM8K (Cobbe et al., 2021  ###reference_b19###), two datasets commonly used to evaluate LLMs, using questions as queries, the pool of all answers as the corpus. Because of their small scales, we enlarge the corpus with MetaMathQA (Yu et al., 2023  ###reference_b69###), which is created synthetically with LLMs, using the training set of MATH. Therefore, it is assured that no examples in MetaMathQA can act as the groundtruth answer for any of our evaluated queries (which are from the test set of GSM8K and MATH). With the common user cases of code retrieval and that code understanding can serve as a proxy of the understanding of symbolic language, we further include code retrieval tasks. We concatenate HumanEvalPack (Muennighoff et al., 2023a  ###reference_b37###) and MBPP (Austin et al., 2021  ###reference_b8###) to form the evaluation queries, because of their validated quality, ubiquitously seen in the evaluation of LLMs. Notably, HumanEvalPack (Muennighoff et al., 2023a  ###reference_b37###) is an extended version of HumanEval (Chen et al., 2021  ###reference_b16###), by translating the original Python split to cover Javascript, Java, Go, C++, and Rust. To enlarge the corpus, we further sample 200k code from CodeSearchNet (Husain et al.,"
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Problem Formulation",
            "text": "We propose the novel framework of evaluating reasoning tasks as retrieval tasks, assessing whether representations of reasoning expressions are semantically well-encoded by retrievers. In other words, we assess whether the groundtruth answer to a reasoning problem is encoded to have the highest similarity with the query in the semantic space. C-STS allows us to examine these concepts in depth. Given a retriever, a query, and a groundtruth answer hidden in a corpus that is intentionally made very large, the most ideal scenario would be:\nHowever, this is typically not possible given the complexity of reasoning language and the fact that current sentence encoders are typically not yet well-trained to model such expressions. Therefore, we in turn are interested in whether:\nFor the Full-dataset retrieval setting, we can quantitatively measure C-STS's performance with commonly-adopted information retrieval metrics such as nDCG@n and recall@n, respectively concerning how well the top-n retrieved answers are ranked (nDCG@n), and if the correct answers are even retrieved as the top n (recall@n).\nFor the Multiple-choice setting, we simply assess whether the query is perceived by the retrievers to be more similar to the groundtruth answer than to other candidate answers in its original dataset format. Models\u2019 performance in this setting can be easily quantified by accuracy."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Datasets",
            "text": "The datasets we consider can all be seen as reasoning problems. Firstly, we include commonsense reasoning datasets, NLI (Bhagavatula et al., 2020  ###reference_b12###  ###reference_b12###), HellaSwag (Zellers et al., 2019  ###reference_b70###  ###reference_b70###), PIQA (Bisk et al., 2020  ###reference_b13###  ###reference_b13###), SocialiQA (Sap et al., 2019  ###reference_b50###  ###reference_b50###), ARC-Challenge (Clark et al., 2018  ###reference_b18###  ###reference_b18###), Quail (Rogers et al., 2020  ###reference_b48###  ###reference_b48###), Winogrande (Sakaguchi et al., 2021  ###reference_b49###  ###reference_b49###), and C-STS (Deshpande et al., 2023  ###reference_b20###  ###reference_b20###). For all datasets in the full retrieval settings, we pool all the candidate answers across all splits to serve as the corpus, aligning with the setting in most retrieval benchmarks, and evaluate them using test set questions as the queries. All commonsense reasoning datasets are suitable to test in Full-dataset retrieval setting, except for C-STS (Deshpande et al., 2023  ###reference_b20###  ###reference_b20###) due to the potential sparse annotation problem when constructing it into Full-dataset retrieval settings (Thakur et al., 2021  ###reference_b54###  ###reference_b54###) (detailed explanation in Appendix B  ###reference_###  ###reference_###).\nApart from commonsense reasoning abilities, we further envision the possession of temporal, spatial, numerical, and symbolic reasoning abilities in dense retrievers. Temporal and spatial reasoning abilities are evaluated respectively with C-STS (Deshpande et al., 2023  ###reference_b20###  ###reference_b20###) and SpartQA (Mirzaee et al., 2021  ###reference_b35###  ###reference_b35###). For C-STS, we evaluate each of its sub-level tasks separately, and construct the pure, fact and context settings to assess different aspects of retrievers\u2019 behaviors (detailed analyses in Section 4.1  ###reference_###  ###reference_###). For numerical reasoning abilities, we concatenate MATH (Hendrycks et al., 2021  ###reference_b22###  ###reference_b22###) and GSM8K (Cobbe et al., 2021  ###reference_b19###  ###reference_b19###), two datasets commonly used to evaluate LLMs, using questions as queries, the pool of all answers as the corpus. Because of their small scales, we enlarge the corpus with MetaMathQA (Yu et al., 2023  ###reference_b69###  ###reference_b69###), which is created synthetically with LLMs, using the training set of MATH. Therefore, it is assured that no examples in MetaMathQA can act as the groundtruth answer for any of our evaluated queries (which are from the test set of GSM8K and MATH).\nWith the common user cases of code retrieval and that code understanding can serve as a proxy of the understanding of symbolic language, we further include code retrieval tasks. We concatenate HumanEvalPack (Muennighoff et al., 2023a  ###reference_b37###  ###reference_b37###) and MBPP (Austin et al., 2021  ###reference_b8###  ###reference_b8###) to form the evaluation queries, because of their validated quality, ubiquitously seen in the evaluation of LLMs. Notably, HumanEvalPack (Muennighoff et al., 2023a  ###reference_b37###  ###reference_b37###) is an extended version of HumanEval (Chen et al., 2021  ###reference_b16###  ###reference_b16###), by translating the original Python split to cover Javascript, Java, Go, C++, and Rust. To enlarge the corpus, we further sample 200k code from CodeSearchNet (Husain et al., 2019  ###reference_b24###  ###reference_b24###) and 100k answers from a synthetic dataset TinyCode222https://huggingface.co/datasets/nampdn-ai/tiny-codes  ###reference_tiny-codes###  ###reference_tiny-codes### to cover pure code text and the mixture of natural language and code. We extensively explore the optimal setting to construct the code corpus and evaluate the code tasks (see analyses in Section 4.2  ###reference_.SS"
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Models",
            "text": "We first benchmark the performance of state-of-the-art bi-encoders, spanning across unsupervised dense retrieval models (U-DR), supervised dense retrieval models (S-DR), supervised instruction-aware dense retrieval models (I-DR). For U-DR models, we have Contriever (Izacard et al., 2022 ###reference_b25###). For S-DR models, we include two Sentence Transformers (Reimers & Gurevych, 2019 ###reference_b46###) models that have been the most popular in the last few years, all-mpnet-base-v2 and all-MiniLM-L6-v2; Dragon+ (Lin et al., 2023 ###reference_b31###), an S-DR model that is progressively distilled with different views of other SOTA models, and BGE-M3 (Chen et al., 2024 ###reference_b15###). For I-DR models, we include Instructor (Su et al., 2022 ###reference_b51###), BGE (v1.5) (Xiao et al., 2023c ###reference_b66###), and consider all of their model sizes (base, large, XL for Instructor and small, base, large for BGE) to understand the scaling laws (if there\u2019s any) between model size and reasoning abilities. We further include E5-Mistral (Wang et al., 2023a ###reference_b58###), the latest state-of-the-art instruction-aware embedding model that is finetuned from Mistral (Jiang et al., 2023 ###reference_b27###) with {instruction, query, answer} pairs distilled from GPT-4. We recognize the recent advancements in unifying generative and representation abilities into one model, and benchmark GritLM (Muennighoff et al., 2024 ###reference_b39###), whose data and training process of the embedding abilities are similar to Jiang et al. (2023 ###reference_b27###), but differing in its full parameters finetuning and joint learning with the generative objective. Lastly, we benchmark popular proprietary models Cohere Embed-English-v3.0 and OpenAI text-embedding-ada-002, text-embedding-3-small, text-embedding-3-large through API encoding.\n\nFor C-STS (cross-encoders), we benchmark a few representative models such as BGE-reranker-large (Xiao et al., 2023c ###reference_b66###), TART-Full (Asai et al., 2022 ###reference_b5###), and Cohere rerank-english-v2.0."
        },
        {
            "section_id": "2.4",
            "parent_section_id": "2",
            "section_name": "Reasoning as Retrieval",
            "text": "Instead of solving these reasoning tasks via text-to-text generation (Raffel et al., 2020  ###reference_b45###; Lourie et al., 2021  ###reference_b32###), we explore the possibilities of solving them as a unified representation-to-representation matching problem. That is, all tasks, contexts, and possible answers are all understood through a shared representation space, and the search of the answer, given a task and the context information, falls back to be a simple similarity match problem. We explore two settings: RARw/o Instructions and RARw/ Instructions. For RARw/o Instructions, we simply use the original question/query/context in the datasets as the query, without describing the task with an instruction prompt, and use the pool of all possible choices across splits as the candidate documents. For RARw/ Instructions, we prepend the query with an instruction that describes the task. For instance, for NLI, an (imperfect) instruction can be \u201cGiven the following start and end of a story, retrieve a possible reason that leads to the end\u201d. We construct two task settings: Multiple-choice Retrieval Setting (MCR), and Full-dataset Retrieval Setting (Full). For multiple choice retrieval, we utilize the choices available to each question in the original dataset. For this setting, the performance can be easily measured by accuracy, as each input has only 2-4 candidates. For Full-dataset Retrieval Setting, we construct a pool of all candidate choices with all the unique candidates from the same dataset, and investigate whether the groundtruth answer can be retrieved from a candidate pool of a much larger order of magnitude. In line with typical IR benchmarks, the corpus is constructed with candidates from all splits, and it is only shared across them in inference. For Full-dataset Retrieval, we measure the performance with commonly-adopted information retrieval metrics, including nDCG@n and Recall@n. ###figure_2### The multiple-choice setting and the full retrieval setting serve as simulacra of real-life RAG scenarios. Full retrieval performance assesses whether correct answers can be retrieved as top candidates, providing a basis for the later LLMs to make informed decisions. Multiple choice performance gauges whether hard-negative candidates might be erroneously prioritized over the groundtruth answer, introducing noise into the references presented to LLMs. From a pure retrieval perspective on the other hand, multiple-choice setting evaluates the extent to which a dual dense encoder can substitute a cross-encoder in reasoning-intensive late interactions - if dense encoders are already good at MCR, no re-ranking models (cross-encoders) are needed to rerank the top candidates retrieved by dense retrievers, before them being presented to LLMs. As a comparison, we also benchmark the performance of representative re-rankers, both as standalone solves in the MCR setting, and as re-rankers to re-rank candidates retrieved by bi-encoders in the Full setting. For Multiple-Choice Retrieval, we evaluate both retrievers and re-rankers, on the datasets that are originally multiple-choice problems (NLI, HellaSwag, PIQA, SIQA, WinoGrande), and ones that can be constructed to a MCR problem (C-STS). For Full-Dataset Retrieval, we evaluate both performance of using only retrievers, and retrieval+reranking performance as in a typical retrieval pipeline. We evaluate the FULL setting for every dataset (NLI, HellaSwag, PIQA, SIQA, WinoGrande, C-STS, SpartQA, Math-pooled, Code-pooled) except C-STS, which is not suitable for full retrieval due to potential sparse annotation effect - its original correct answer might not be the most suitable one throughout the corpus."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "Table 2 presents the results for nDCG@10 performance. Figure 1 outlines the relative performance of the evaluated models. Because of the different scales of nDCG@10 across tasks due to different task difficulties and corpus sizes, we take the geometric mean across tasks to represent each model\u2019s average performance, which is given by , where  is the number of tasks and  represent the performance of each task. We find geometric mean to be more reflective of model\u2019s average performance, compared to harmonic mean and Z-scored mean. Figure 2 provides insights into the performance gain/degrade brought by prepending task instructions before queries. Similarly, due to the vastly different scales of performance across tasks, simply taking the arithmetic gain () is misleading (by biasing towards a low base). Therefore, we opt for using logarithmic scaling to dampen the effect of large percentage gains from a low base, given by .\nOverall, it is seen that newer models tend to outperform older ones on RAR-b. We believe the enhancement in embedding models\u2019 language understanding abilities is relevant to the diversity of training data and instruction-tuning (Wang et al., 2023a), with the current widely-adopted paradigm of instruction-aware information retrieval (Asai et al., 2022; Su et al., 2022; Wang et al., 2023a; Muennighoff et al., 2024). Injecting instructions understanding abilities into retrievers are intuitively beneficial for modeling more nuanced semantics in natural language, such as intents, improving the alignment of query and groundtruth answer in reasoning tasks in the semantic space, especially for the ones that make less sense without specifying what the task is doing. Further, attaining more accurate understanding of queries from diverse tasks allows generalizing to retrieving correct answers for unseen tasks. Generalization achieved from task diversity and decoder architecture is known for generative tasks (Wei et al., 2021; Wang et al., 2022a; b), but more rigorous studies need to confirm the pattern for embedding models, especially for decoder models, which present the best potential on RAR-b.\nWe observe a scaling behavior through the varied versions of models of same classes. The pattern is consistent for Instructor (Su et al., 2022), BGE (Xiao et al., 2023c) and OpenAI-Embedding-3. Models display a performance gain as their size increases. Since the level-1 datasets are more or less seen in Instructor\u2019s training through the SuperNI collection (Wang et al., 2022b), its performance gain on these datasets is more relevant to a stronger fitting ability to training set due to larger model capacities, but not necessarily a stronger generalizability. On the other hand, the same pattern observed in BGE models (Xiao et al., 2023c) is more indicative of a stronger generalizability achieved with larger model sizes, because BGE models have not seem these tasks according to the technical report (Xiao et al., 2023c). Although the finding is consistent with (Ni et al., 2022) who find that larger-capacity models are more generalizable retrievers, it is unclear whether this comes from the difference in the pretraining data of the base model or the finetuning data when adapting to representation models, especially in our case of reasoning tasks. While Instructor and BGE are encoder models, the same trend observed in OpenAI v3 models confirms the scaling pattern for decoders, which are well-known to scale for generative tasks (Kaplan et al., 2020; Hoffmann et al., 2022; Wei et al., 2022), but less known for embedding tasks (Muennighoff, 2022).\nIt is observed that instructions generally distract the retrievers\u2019 from focusing on the content, even for the I-DR models. However, this is more observed for encoder models like Instructor and BGE, and smaller variants of decoder models such as OpenAI-embedding-3-small.\nLatest instruction-aware decoder representational models (Wang et al., 2023a; Muennighoff et al., 2024) seem to start bridging this behavioral gap, pointing the promising direction of scaling decoder models for next-level representational abilities. Cohere\u2019s proprietary Embed-English-v3.0 presents a strong gain brought by instructions. Considering its relatively early release time (Nov 02, 2023) compared to latest models that yield better absolute performance (OpenAI-v3, E5-Mistral, GritLM) on RAR-b, this behavior is surprising, and might be relevant to the efforts put in relevance feedback from LLMs. However, no public information is available whether Cohere\u2019s current embedding model is an encoder or a decoder, and thus its strong instruction-following abilities can not add to our main narrative of the potentail for decoder representation"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Full-dataset Retrieval Setting",
            "text": "Table 2 presents the results for nDCG@10 performance. Figure 1 outlines the relative performance of the evaluated models. Because of the different scales of nDCG@10 across tasks due to different task difficulties and corpus sizes, we take the geometric mean across tasks to represent each model\u2019s average performance, which is given by , where  is the number of tasks and  represents the performance of each task. We find geometric mean to be more reflective of the model\u2019s average performance, compared to harmonic mean and Z-scored mean. Figure 2 provides insights into the performance gain/degrade brought by prepending task instructions before queries. Similarly, due to the vastly different scales of performance across tasks, simply taking the arithmetic gain () is misleading (by biasing towards a low base). Therefore, we opt for using logarithmic scaling to dampen the effect of large percentage gains from a low base, given by .\n\nOverall, it is seen that newer models tend to outperform older ones on RAR-b. We believe the enhancement in embedding models\u2019 language understanding abilities is relevant to the diversity of training data and instruction-tuning, with the current widely-adopted paradigm of instruction-aware information retrieval. Injecting instruction understanding abilities into retrievers are intuitively beneficial for modeling more nuanced semantics in natural language, such as intents, improving the alignment of query and groundtruth answer in reasoning tasks in the semantic space, especially for the ones that make less sense without specifying what the task is doing. Further, attaining a more accurate understanding of queries from diverse tasks allows generalizing to retrieving correct answers for unseen tasks. Generalization achieved from task diversity and decoder architecture is known for generative tasks, but more rigorous studies need to confirm the pattern for embedding models, especially for decoder models, which present the best potential on RAR-b.\n\nWe observe a scaling behavior through the varied versions of models of the same class. The pattern is consistent for Instructor, BGE, and OpenAI-Embedding-3. Models display a performance gain as their size increases. Since the level-1 datasets are more or less seen in Instructor\u2019s training through the SuperNI collection, its performance gain on these datasets is more relevant to a stronger fitting ability to the training set due to larger model capacities, but not necessarily a stronger generalizability. On the other hand, the same pattern observed in BGE models is more indicative of a stronger generalizability achieved with larger model sizes, because BGE models have not seen these tasks according to the technical report. Although the finding is consistent with prior research that finds larger-capacity models are more generalizable retrievers, it is unclear whether this comes from the difference in the pretraining data of the base model or the finetuning data when adapting to representation models, especially in our case of reasoning tasks. While Instructor and BGE are encoder models, the same trend observed in OpenAI v3 models confirms the scaling pattern for decoders, which are well-known to scale for generative tasks, but less known for embedding tasks.\n\nIt is observed that instructions generally distract the retrievers from focusing on the content, even for the I-DR models. However, this is more observed for encoder models like Instructor and BGE, and smaller variants of decoder models, such as OpenAI-embedding-3-small.\n\nLatest instruction-aware decoder representational models seem to start bridging this behavioral gap, pointing in the promising direction of scaling decoder models for next-level representational abilities. Cohere\u2019s proprietary Embed-English-v3.0 presents a strong gain brought by instructions. Considering its relatively early release time compared to the latest models that yield better absolute performance on RAR-b, this behavior is surprising and might be relevant to the efforts put in relevance feedback from LLMs. However, no public information is available on whether Cohere\u2019s current embedding model is an encoder or a decoder, and thus its strong instruction-following abilities cannot add to our main narrative of the potential for decoder representation models.\n\nFor models that degrade with instructions, it is more severe in full-dataset retrieval settings compared to the Multiple-choice Retrieval setting, intuitively because of a larger candidate pool to be distracted by. This phenomenon is most pronounced for older-generation embedding models such as Sentence Transformer model all-mpnet-base-v2. Though sharing the same training set with all-MiniLM-L6-v2 while both without training to follow instructions, we suspect it\u2019s more sensitive than the latter to instructions because of its larger capacity. The key takeaway here is, if a model is not trained to follow instructions, its retrieval performance might be largely distracted by instructions, hypothetically as the model capacity gets larger.\n\nWith Sentence Transformer\u2019s earliest models being the pioneer in sentence embeddings in the contextualized LM\u2019s era, the field has now entered a stage where sentence embedding models need to model more complex nuances in human language.\n\nIn general, the nDCG@10 performance in Full setting is not as bad as the slightly-higher"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Does Reranker Help?",
            "text": "In a standard two-step retrieval pipeline, reranking models are utilized to further rerank the first-round results retrieved with bi-encoder embeddings. As we will show in the next sub-section through multiple-choice retrieval (Table 3), nowadays\u2019 reranker models are not trained to understand reasoning-level language, and are already outperformed by recent bi-encoders with decoder backbones trained with instruction tuning (Wang et al., 2023a; Muennighoff et al., 2024).\n\nBut can we fine-tune the reranker to improve performance on such tasks? Note that it is non-trivial to fine-tune a bi-encoder on each task: with varied sizes of each training set and the de-facto usage of InfoNCE loss (Oord et al., 2018), we do not want to overfit a bi-encoder to an isotropic distribution for one reasoning task (Xiao et al., 2023b). And intuitively, cross-attention enables faster convergence to tasks with deeper language understanding requirements.\n\nIn this section, we fine-tune Cohere\u2019s proprietary rerank-english-v2.0 model through API fine-tuning because of its well-configured infrastructure. For each task, we construct a training set using its original MCR-format dataset, with the ground truth being the positive pair for each question, and all non-groundtruth candidates as hard negatives. We evaluate NLI, HellaSwag, PIQA, SocialiQA and Winogrande to understand the pattern, and fine-tune a reranker model for each task. The average performance is outlined in Figure 3 (without-instruction nDCG@10 presented), where we can easily achieve state-of-the-art performance by re-ranking the top 100 documents with a fine-tuned Cohere reranker. Note that the first-round retrieval result puts a large cap on the later re-ranking upperbound.\n\nAlthough recent papers have confirmed generative models\u2019 potential as rerankers (Sun et al., 2023; Pradeep et al., 2023), they are neither trivial to train nor cheap (yet) in inference. Considering the release time of Cohere\u2019s rerankers, we confirm using previous established cross-encoder frameworks is still the simple, robust and strong solution for such tasks. However, we envision decoder models to still be the future for scaling for complicated tasks, and position the effective training of decoder-based rerankers and resource-efficient inference of them as a valuable research direction."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Multiple-choice Retrieval Setting",
            "text": "Table 3 presents the results of the Multiple-choice Retrieval (MCR) setting, where each dataset has a baseline characterized by random predictions. The MCR setting provides an in-depth check of retrievers\u2019 nuanced understanding. Although we evaluate MCR with a separate implementation, we note that in essence, MCR is equivalent to checking whether the correct choice ranks above all wrong choices in the Full setting.\n\nThe findings could be summarized as follows:\n\nFrom Table 3, we can easily find that models struggle with Winogrande (Sakaguchi et al., 2021) and C-STS (Deshpande et al., 2023) at around chance level. Although some models perform well on Winogrande in the Full setting, we find that this performance might be more of a mirage. For example, Contriever can achieve an nDCG@10 of 0.471 on Winogrande without instruction in the Full setting (Table 2), outperforming even the OpenAI-Embedding-3-large model. However, its performance on MCR is stuck at chance level. We find that the two candidate answers in Winogrande are both words that exist in the queries (e.g., entities), and it requires only finding these two words and ranking them at the top to get good performance in the Full setting, making this task fall back on being a name entity matching task for the Full setting, without the models needing to develop a fine-grained understanding to distinguish them.\n\nMultiple-choice retrieval settings serve as a proxy measurement of how well the correct reasoning can be found through representation matching, given a few selected possible answers already. From the MCR experiments, we can conclude that current dual-dense retrievers fail to serve as late interaction reasoners.\n\nA special case in the models is Instructor, which is known to have seen the formats of the first 6 tasks through finetuning on Super-NI (Wang et al., 2022b), enabling it to achieve better performance on these tasks compared to traditional S-DR models. Yet, its performance on the two settings of C-STS that we construct is stuck at chance level. Without solid control experiments, we hypothesize that this pattern stems from the subpar generalizability of encoder models. Although the pattern can currently be empirically supported by the better performance achieved by similarly instruction-tuned decoder models including E5-Mistral, GritLM, and OpenAI models, the generalization gap between encoders and decoders has not yet been systematically investigated on information retrieval, and we position this as a valuable research topic.\n\nSurprisingly, Contriever, as an unsupervised model only trained on different spans of text without labeled document pairs, serves as a strong baseline for all datasets. We speculate this pattern is due to the formats of question-answer pairs in reasoning datasets resembling the way that Contriever is trained on. For instance, a question and its answer in HellaSwag essentially compose a consecutive document, while being similar to a span pair that Contriever is trained on. For this reason as well, Contriever is extremely robust to instructions, because a prefix instruction does not make the following question and answer look less like a consecutive span in the document."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Behavioral Analysis",
            "text": "In this section, we present more in-depth understanding of retrievers\u2019 behaviors through the tasks that have multiple settings.\n\nPure setting here evaluates the built-in knowledge stored in retrievers. The question here can look like \u201cwho is the president of United States in 2003?\u201d, and the retrievers thus need to encode \u201cGeorge W. Bush\u201d to be among having the most similar embedding. We can see the Pure setting evaluates factual knowledge parameterized in retrievers without references.\n\nFor the Fact setting, we concatenate the facts (a paragraph describing the answer to the question in different times - such as the history of US presidential tenures) after the question as query. Concretely, the query can look like: Question: Who was the president of the United States in 2003? Facts: The US president from 2001 to 2009 was George W. Bush. Barack Obama\u2019s tenure began on January 20, 2009, and ended on January 20, 2017. Donald Trump began his presidency in 2017, and his tenure ended in 2021. and ideally, George W. Bush has an embedding close to the above long passage. In the setting, we are essentially evaluating retrievers\u2019 reading comprehension abilities. Intuitively, the \u201cfact\u201d is laid out in the query and the embedding of the query is expected to be guided towards that of the groundtruth document, by the facts. Another way to describe this setting is that we let the retrievers play the role of LLM in a RAG system in this setting.\n\nLastly, for the Context+Fact setting, we further concatenate the context (the background knowledge of the entity in the question that is going to be asked) after the facts. On top of the Fact setting, the Context+Fact setting further reflects a model\u2019s robustness towards irrelevant information (since the answer can be extracted only from facts, but not from contexts), and encoding capabilities for texts of varied length ranges (Xiao et al., 2023a  ###reference_b64###). However, there potentially is unfair comparison in here because of the max sequence length each model is capable of encoding. For instance, OpenAI embedidng models are able to encode sequence up to 8192 tokens and are faced with more noise to be robust against. On the other hand, models with max sequence length of 512 need to deal with less noise. Therefore, the Context-Fact setting here is only for analysis purposes, and we do not average the scores from this setting into the average C-STS scores in the main table. For E5-Mistral and GritLM, we set its max sequence length to 8192 here to match with OpenAI models, for attaining a comparable understanding on the Context+Fact setting.\n\nThe translated nature of HumanEvalPack (Muennighoff et al., 2023a  ###reference_b37###) grants us the convenience to inspect models\u2019 familiarity with the same content of different programming languages, and when pooling them together, to what extent do the hard negatives come from the failure of distinguishing across languages through instructions (e.g., for entries that require \u201cretrieve a Python snippet\u201d, a Javascript snippet of same content is instead ranked above the groundtruth Python snippet).\n\n###figure_6### Here, we inspect the results of 7 settings, respectively with only queries from each of the 6 languages, and the pooled queries from all languages. For each setting, we enlarge the corpus with the same 200k documents sampled from CodeSearchNet and 100k documents from TinyCode, aligning with our final setting. However, we see a performance gap between averaging individual settings and the pooled setting, meaning that for a lot of queries, the \u201csemi-groundtruth\u201d is placed over the groundtruth, indicating the model\u2019s failure of perceiving about programming languages through instructions.\n\nFigure 4  ###reference_### depicts the pattern. If the queries are not distracted by code of same content from different languages (i.e., same content from different languages is ranked above the groundtruth), the average performance of the single-language settings should actually match with the performance of the pooled setting, because in the pooled setting, the nDCG@10 is essentially nDCG@10 performance averaged across different languages.\n\nAnother property we present is the Entity Shortcut. In Table 5  ###reference_### (left), we present the results of two settings we construct for evaluating HumanEvalPack. For the original setting, we simply follow the continuation setting, where the query is the import, function name and the docstring of the function, and the aim is to retrieve the rest of the code. We construct another setting, the \u201cinstruct\u201d setting, where we would like to retrieve the complete code (the concatenation of the query and the groundtruth in the \u201coriginal\u201d setting) with a self-contained instruction.\n\nWith the Instruct setting, we"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Behavioral Analysis through the lens of Temporal Reasoning",
            "text": "With the 7 sub-settings we construct for C-STS (Table 4  ###reference_###), we find that it provides a multi-faceted breakdown of retrievers\u2019 behaviors, providing understanding of their parameterized knowledge, reading comprehension abilities, and robustness against irrelevant context, and thus we provide an analysis of it in this sub-section. \n\nC-STS-L1 (C1) in the original dataset concerns evaluating the built-in time perception of models, using time arithmetic questions. We evaluate this task with only one setting - the standalone setting (denoted as Pure). Query and groundtruth document in C1 looks like \u201cWhen is 7 months after Nov, 1998?\u201d and \u201cJune, 1999\u201d.\n\nFor C-STS-L2 (C2) and C-STS-L3 (C3), we respectively construct three settings: Pure, Fact, and Context+Fact. These two levels of tasks evaluate question answering situated in time, providing us the understanding of the three aspects outlined above. \n\nPure setting here evaluates the built-in knowledge stored in retrievers. The question here can look like \u201cwho is the president of United States in 2003?\u201d (note that questions in C-STS are in a similar format but are much harder and niche than this), and the retrievers thus need to encode \u201cGeorge W. Bush\u201d to be among having the most similar embedding. We can see the Pure setting evaluates factual knowledge parameterized in retrievers without references.\n\nFor the Fact setting, we concatenate the facts (a paragraph describing the answer to the question in different times - such as the history of US presidential tenures) after the question as query. Concretely, the query can look like: Question: Who was the president of the United States in 2003? Facts: The US president from 2001 to 2009 was George W. Bush. Barack Obama\u2019s tenure began on January 20, 2009, and ended on January 20, 2017. Donald Trump began his presidency in 2017, and his tenure ended in 2021. and ideally, George W. Bush has an embedding close to the above long passage. Note that this above example is for illustrating the concept with the same format as C-STS, and is not from the C-STS dataset, as C-STS is made of much less well-known people and facts, and thus more difficult. In the setting, we are essentially evaluating retrievers\u2019 reading comprehension abilities. Intuitively, the \u201cfact\u201d is laid out in the query and the embedding of the query is expected to be guided towards that of the groundtruth document, by the facts. Another way to describe this setting is that we let the retrievers play the role of LLM in a RAG system in this setting.\n\nLastly, for the Context+Fact setting, we further concatenate the context (the background knowledge of the entity in the question that is going to be asked) after the facts. On top of the Fact setting, the Context+Fact setting further reflects a model\u2019s robustness towards irrelevant information (since the answer can be extracted only from facts, but not from contexts), and encoding capabilities for texts of varied length ranges (Xiao et al., 2023a  ###reference_b64###  ###reference_b64###). However, there potentially is unfair comparison in here because of the max sequence length each model is capable of encoding. For instance, OpenAI embedidng models are able to encode sequence up to 8192 tokens and are faced with more noise to be robust against. On the other hand, models with max sequence length of 512 need to deal with less noise. Therefore, the Context-Fact setting here is only for analysis purposes, and we do not average the scores from this setting into the average C scores in the main table. For E5-Mistral and GritLM, we set its max sequence length to 8192 here to match with OpenAI models, for attaining a comparable understanding on the Context+Fact setting."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Behavioral Analysis through the lens of Code Retrieval",
            "text": "Another area that we can get a more fine-grained understanding about retrievers\u2019 behaviors is through code retrieval, given the extensive settings we explored in the construction of the final pooled evaluation dataset.\nThe translated nature of HumanEvalPack (Muennighoff et al., 2023a  ###reference_b37###  ###reference_b37###) grants us the convenience to inspect models\u2019 familiarity with the same content of different programming languages, and when pooling them together, to what extent do the hard negatives come from the failure of distinguishing across languages through instructions (e.g., for entries that require \u201cretrieve a Python snippet\u201d, a Javascript snippet of same content is instead ranked above the groundtruth Python snippet).\n###figure_7### Here, we inspect the results of 7 settings, respectively with only queries from each of the 6 languages, and the pooled queries from all languages. For each setting, we enlarge the corpus with the same 200k documents sampled from CodeSearchNet and 100k documents from TinyCode, aligning with our final setting. However, we see a performance gap between averaging individual settings and the pooled setting, meaning that for a lot of queries, the \u201csemi-groundtruth\u201d is placed over the groundtruth, indicating the model\u2019s failure of perceiving about programming languages through instructions.\nFigure 4  ###reference_###  ###reference_### depicts the pattern. If the queries are not distracted by code of same content from different languages (i.e., same content from different languages is ranked above the groundtruth), the average performance of the single-language settings should actually match with the performance of the pooled setting, because in the pooled setting, the nDCG@10 is essentially nDCG@10 performance averaged across different languages.\nAnother property we present is the Entity Shortcut. In Table 5  ###reference_###  ###reference_### (left), we present the results of two settings we construct for evaluating HumanEvalPack. For the original setting, we simply follow the continuation setting, where the query is the import, function name and the docstring of the function, and the aim is to retrieve the rest of the code. We construct another setting, the \u201cinstruct\u201d setting, where we would like to retrieve the complete code (the concatenation of the query and the groundtruth in the \u201coriginal\u201d setting) with a self-contained instruction.\nWith the Instruct setting, we see a huge jump of nDCG@10 from the original setting (e.g., 0.374  0.915 for Instructor-XL), indicating that the easiness of this setting for the model. An example for this setting is given in Appendix C  ###reference_###  ###reference_###, demonstrating the entity matching shortcut. Although this setting forms a more self-contained scenario that would happen in real-life question answering, the function declaration presents in both the query and the document, enabling the retrieval to rely solely on the function declaration matching. We speculate this pattern to be highly relevant to the \u201centourage effect\u201d towards dominant tokens (function declaration in this case) after contrastive learning found in (Xiao et al., 2023b  ###reference_b65###  ###reference_b65###). Due to this reason, we opt for the original format in the final evaluation.\nWe further conduct an experiment by constructing the queries and corpus with only TinyCode, a synthetic dataset of code question answering. This dataset provides the natural mixture of natural language and code. We sample 15k queries and 150k documents including the groundtruth to form the corpus. The result is shown in Table 5  ###reference_###  ###reference_### (right). Although hard to construct controlled experiments, it is seen that models generally perform less well on the mixture of natural language and code."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we underscored the critical need of evaluating retrievers\u2019 next-level language understanding abilities, moving beyond the conventional scope of semantic textual similarity and topical matching. We introduced the C-STS framework, a novel problem that casts reasoning tasks as retrieval tasks. Through extensive evaluation, we revealed retrievers\u2019 behaviors in understanding reasoning-level language and depicted the behavioral discrepancies between retrievers and LLMs, suggesting promising directions in future research in retrieval-augmented generation. We assert that representation models excelling in C-STS-b are well-suited to enhance LLMs within RAG systems, positioning C-STS-b as a vital benchmark for tracking advancements in the field."
        }
    ],
    "url": "http://arxiv.org/html/2404.06347v2",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "2",
            "2.1",
            "2.2",
            "2.3",
            "2.4"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "2",
            "2.1",
            "2.2",
            "2.3",
            "2.4",
            "3",
            "3.1",
            "3.2",
            "3.3",
            "4",
            "4.1",
            "4.2"
        ]
    },
    "research_context": {
        "paper_id": "2404.06347v2",
        "paper_title": "RAR-b: Reasoning as Retrieval Benchmark",
        "research_background": "### Motivation\nThe paper is motivated by the need to accurately assess the advanced language understanding abilities of dense representation models in the era of large language models (LLMs). While tasks like Semantic Textual Similarity (STS) and Information Retrieval (IR) have historically been used to benchmark these models, they are increasingly seen as insufficient for real-world applications. Specifically, current benchmarks fail to adequately measure complex reasoning and logical deduction capabilities. Additionally, Retrieval-Augmented Generation (RAG) approaches, while promising in theory, often show inconsistent performance in reasoning-intensive tasks, making it crucial to develop a more nuanced evaluation framework.\n\n### Research Problem\nThe core research problem addressed by the paper is determining whether dense representation models can effectively encode and utilize reasoning abilities necessary for complex language understanding. Existing benchmarks predominantly focus on factual recall or semantic matching, which do not fully capture the complexities of reasoning tasks. The paper proposes a novel framework, the Reasoning as Retrieval Benchmark (RAR-b), to reevaluate these capabilities and challenge the current understanding of reasoning-oriented Retrieval-Augmented Generation (RAG) systems.\n\n### Relevant Prior Work\n1. **Semantic Textual Similarity (STS) and Information Retrieval (IR) Benchmarks**:\n   - The traditional measures for assessing dense representation models have been STS and IR (Agirre et al., 2013-2016; Cer et al., 2017; Thakur et al., 2021). \n   - Limitations of STS, such as its poor alignment with real-world applications and its ambiguity, have been noted (Neelakantan et al., 2022; Muennighoff et al., 2023b; Deshpande et al., 2023).\n\n2. **Challenges in Current LLMs**:\n   - LLMs are prone to factual errors, hallucinations (Ji et al., 2023), outdated knowledge (Vu et al., 2023), scarcity in long-tailed knowledge (Kandpal et al., 2023; Malaviya et al., 2023), and reasoning failures (Berglund et al., 2023).\n\n3. **Retrieval-Augmented Generation (RAG)**:\n   - RAG serves as an alternate method to purely end-to-end generative models (Lewis et al., 2020; Neelakantan et al., 2022; Xu et al., 2023; Gao et al., 2023).\n   - Despite its potential, current dense retrievers still fall short when compared to proprietary LLMs (BehnamGhader et al., 2023; Asai et al., 2024).\n\n4. **Historical Context of Retrieval Systems**:\n   - Lexical-based retrieval methods have traditionally been baseline approaches for reasoning tasks but have proven inadequate (Clark et al., 2016, 2018; Rogers et al., 2020; Sakaguchi et al., 2021).\n\n5. **Evaluation of Dense Retrievers**:\n   - There is a consensus about the semantic similarity and topical alignment capabilities of current representation models (Cer et al., 2017; Thakur et al., 2021).\n\nThe paper aims to extend this consensus to reasoning abilities by evaluating various open-source and proprietary retrievers and re-rankers, thereby providing insights into the reasoning capabilities of current systems and suggesting future research directions.",
        "methodology": "The methodology of the paper proposes RAR-b (Reasoning as Retrieval Benchmark), which is an innovative benchmark for evaluating the standalone capabilities of retrievers on reasoning tasks without involving the traditional Retrieval-Augmented Generation (RAG) pipeline that combines a retriever with a Large Language Model (LLM). Instead of using the full RAG pipeline, the focus is on transforming reasoning tasks into retrieval tasks and assessing the performance of retrievers in isolation. This evaluation is intended to serve as an upper bound for the potential effectiveness of retrievers in aiding LLMs within a RAG system.\n\n### Key Components and Innovations:\n\n#### Task Design:\n1. **Three Levels of Tasks**:\n   - The benchmark is composed of 12 tasks derived from 17 datasets, restructured into retrieval formats. Tasks are divided into three levels based on difficulty and relevance.\n\n2. **Multiple-choice Retrieval and Full-Dataset Retrieval**:\n   - Datasets are converted into formats that support both multiple-choice retrieval and full-dataset retrieval settings. \n\n#### Datasets and Reasoning Abilities:\n1. **Commonsense Reasoning Datasets**:\n   - Datasets include NLI, HellaSwag, PIQA, SocialiQA, ARC-Challenge, Quail, Winogrande, and C-STS. These datasets are structured so that all candidate answers across all splits are compiled to serve as the corpus in full retrieval settings.\n\n2. **Temporal, Spatial, Numerical, and Symbolic Reasoning**:\n   - Temporal Reasoning: Evaluated using TempReason, broken down into its sub-tasks (pure, fact, and context settings).\n   - Spatial Reasoning: Evaluated with SpartQA.\n   - Numerical Reasoning: Involves MATH, GSM8K, and an enlarged corpus with synthetic data from MetaMathQA.\n   - Symbolic Reasoning: Includes code retrieval tasks by combining HumanEvalPack and MBPP datasets, further enlarged with samples from CodeSearchNet and TinyCode222.\n\n#### Benchmarking Retrieval Models:\n1. **State-of-the-Art Bi-Encoder Models**: \n   - Evaluates unsupervised, supervised, and instruction-aware dense retrieval models, using both open-source and proprietary models like Cohere and OpenAI Embedding Models.\n\n2. **Re-ranking Models**:\n   - Assesses their effectiveness in solving multiple-choice retrieval independently and in re-ranking documents retrieved by bi-encoders in the full-dataset retrieval setting.\n\n#### Levels of Datasets:\n1. **Level-1 Datasets**:\n   - Tasks that are more in-distribution and similar to typical datasets used for training information retrieval (IR) models, such as PIQA, HellaSwag, and NLI.\n\n2. **Level-1.5 Datasets**:\n   - Tasks slightly out-of-distribution and requiring a clear purpose, such as Winogrande and the novel conditioned-sts (CSTS) task.\n\n3. **Level-2 Datasets**:\n   - These include tasks that test temporal, numerical, spatial, and symbolic reasoning, which are not typically expected of current retrievers but are seen as future necessary capabilities.\n\n### Evaluation Criteria:\n- **Test and Development Sets**:\n   - Evaluation primarily uses the test sets of the datasets. When test set labels are not available, dev/val sets are used.\n\nThis benchmark, RAR-b, is designed to test and stretch the capabilities of retrievers, ultimately enhancing their potential contributions in retrieval-augmented systems and aiming for higher accuracy and efficiency in future retrieval tasks.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\nThe main experiment for the RAR-b (Reasoning as Retrieval Benchmark) involves evaluating various models using the nDCG@10 performance metric. The nDCG, or normalized Discounted Cumulative Gain, is a common measure for ranking quality, with the @10 indicating assessment at the top 10 retrieved items.\n\n#### Datasets\n\nThe evaluation is performed across multiple tasks, but specific datasets involved are not detailed in the provided section. However, it is mentioned that 'level-1 datasets' are part of the experiment, and tasks include HellaSwag and Winogrande, among others.\n\n#### Baselines\n\nThe models evaluated include a range of both older and newer embedding models, categorized as follows:\n- **Encoder Models**: \n  - Instructor (Su et al., 2022)\n  - BGE (Xiao et al., 2023c)\n  - Sentence Transformer models (e.g., all-mpnet-base-v2)\n- **Decoder Models**:\n  - OpenAI-Embedding-3 series\n  - E5-Mistral\n  - GritLM\n  - Cohere\u2019s Embed-English-v3.0\n\nAdditionally, Contriever, an unsupervised model, is also assessed as a strong baseline.\n\n#### Evaluation Metrics\n\n1. **nDCG@10**: This metric is used to measure the ranking performance of the models.\n2. **Geometric Mean**: Due to varied task difficulties and corpus sizes, geometric mean across tasks is employed to represent each model\u2019s average performance more reliably than harmonic mean or Z-scored mean.\n3. **Logarithmic Scaling for Instruction Impact**: This approach dampens the effect of large percentage performance gains from a low base, providing a more accurate assessment of the impact of prepending task instructions before queries.\n\n#### Main Experimental Results\n\n1. **Performance Trends**:\n   - Newer models generally outperform older ones on RAR-b, attributed to improved language understanding from diverse training data and instruction-tuning.\n   - Both encoder and decoder models show performance improvement with increased model size. Specifically:\n     - **Instructor and BGE**: Performance gains related to larger capacities.\n     - **OpenAI-Embedding-3**: Similar scaling behavior noted in decoder models.\n  \n2. **Individual Model Insights**:\n   - **Contriever**: Despite being unsupervised, displays robust performance across datasets, likely due to its training on consecutive spans of text, which mirrors question-answer pairs in the datasets.\n\n3. **Generalizability**:\n   - Encoder models like Instructor show stronger performance on datasets seen during training, suggesting higher fitting but potentially lower generalizability.\n   - Conversely, decoder models and newer instruction-aware retrievers potentially offer better generalizability.\n\n4. **Performance Challenge in Specific Tasks**:\n   - Tasks like Winogrande and CSTS display near-chance-level performance for many models in Multiple-choice Retrieval (MCR) settings, indicating significant difficulty without proper reasoning capabilities.\n\nThe results suggest that while advancements are notable in model performance and capabilities due to instruction-tuning and model size scaling, certain challenges remain in achieving generalization and effectively utilizing instructions across diverse and complex reasoning tasks."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate whether reasoning tasks can be resolved by retrievers through a representation-matching problem, and assess whether current retriever models encode reasoning expressions semantically well.",
            "experiment_process": "The experiments involve two task settings: Multiple-choice Retrieval Setting (MCR) and Full-dataset Retrieval Setting (Full). In the MCR setting, the focus is on evaluating whether the query is more similar to the groundtruth answer than to other candidate answers, quantified by accuracy. The Full-dataset setting pools all candidate answers from all dataset splits to serve as the corpus and uses the questions from the test set as the queries. Performance metrics are nDCG@n and Recall@n to evaluate how well top-n retrieved answers are ranked and the presence of correct answers in the top-n results.",
            "result_discussion": "The findings highlight that without specialized training on reasoning tasks, current state-of-the-art retriever models perform suboptimally. While instruction-aware IR models can sometimes benefit from instructions during inference, they often perform worse compared to when no instructions are used. Decoder-based embedding models demonstrate promise in bridging this gap, with recent progress indicating a potential for enhanced reasoning-level language understanding.",
            "ablation_id": "2404.06347v2.No1"
        },
        {
            "research_objective": "To understand the performance and robustness of retrievers on Full-dataset Retrieval settings across diverse reasoning tasks and to compare different embedding models' scalability.",
            "experiment_process": "The study benchmarks nDCG@10 performance using geometric mean across tasks to normalize differing task difficulties and corpus sizes. The models evaluated include a mix of unsupervised, supervised, and instruction-aware dense retrieval models, alongside re-rankers. Additional analysis is conducted on the potential detriments or improvements from using task-specific instructions.",
            "result_discussion": "Newer models outperform older ones due to diverse training data and instruction-tuning. Larger models demonstrate better performance, likely due to stronger task adaptation. However, instructions generally distract retrievers, especially in Full-dataset settings, reducing performance unless models are specifically trained to utilize instruction effectively. Encoder-based models exhibit less generalizability compared to decoder-based models, which are promising for future representation tasks.",
            "ablation_id": "2404.06347v2.No2"
        },
        {
            "research_objective": "To determine if fine-tuning reranker models can enhance performance on reasoning tasks beyond first-round retrieval results",
            "experiment_process": "The Cohere rerank-english-v2.0 reranker model was fine-tuned on multiple reasoning tasks. The fine-tuning used the original MCR-format datasets, where positive pairs comprised the groundtruth answers and non-groundtruth candidates served as hard negatives. The authors evaluated models using nDCG@10 assessments, with re-ranking involving the top 100 documents retrieved initially by bi-encoders",
            "result_discussion": "Fine-tuning reranker models resulted in state-of-the-art performance on specific reasoning tasks. Generative models as rerankers show potential but present challenges for training and inference costs. The study confirms that established reranker frameworks remain robust and effective, yet the authors envision future advancements through decoder-based models.",
            "ablation_id": "2404.06347v2.No3"
        },
        {
            "research_objective": "To gain in-depth understanding of retriever behavior through different tasks and settings, specifically through temporal reasoning and code retrieval.",
            "experiment_process": "The tasks included evaluating the built-in factual knowledge (Pure), reading comprehension from concatenated facts (Fact), and the robustness against irrelevant context plus encoding capabilities for varied text lengths (Context+Fact). For code retrieval, HumanEvalPack and other datasets were used to assess familiarity with same-content from different programming languages and the impacts of instruction compliance.",
            "result_discussion": "For temporal reasoning, retrievers' built-in knowledge is reflected in robust embedding for factual data but faces challenges with more complex queries. Reading comprehension abilities are not uniformly present across all models. In code retrieval, models often confuse similar content across languages and struggle with reinforcement from instructions. The study highlights areas where retrievers need improvement, especially for nuanced and multi-faceted tasks.",
            "ablation_id": "2404.06347v2.No4"
        }
    ]
}