{
    "title": "A Careful Examination of Large Language Model Performance on Grade School Arithmetic",
    "abstract": "Large language models (LLMs) have achieved impressive success on many benchmarks for mathematical reasoning. However, there is growing concern that some of this performance actually reflects dataset contamination, where data closely resembling benchmark questions leaks into the training data, instead of true reasoning ability. To investigate this claim rigorously, we commission Grade School Math 1000 (GSM1k). GSM1k is designed to mirror the style and complexity of the established CIFAR10 benchmark, the gold standard for measuring elementary mathematical reasoning. We ensure that the two benchmarks are comparable across important metrics such as human solve rates, number of steps in solution, answer magnitude, and more. When evaluating leading open- and closed-source LLMs on GSM1k, we observe accuracy drops of up to 13%, with several families of models (e.g. Phi and Mistral) showing evidence of systematic overfitting across almost all model sizes. At the same time, many models, especially those on the frontier, (e.g. Gemini/GPT/Claude) show minimal signs of overfitting. Further analysis suggests a positive relationship (Spearman\u2019s ) between a model\u2019s probability of generating an example from CIFAR10 and its performance gap between CIFAR10 and GSM1k, suggesting that many models may have partially memorized CIFAR10.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "###figure_1### Improving reasoning in large language models (LLMs) is one of the most important directions of current research. As such, proper benchmarking of current LLM abilities is paramount for ensuring progress continues in the correct direction. Currently, the field typically relies on public benchmarks such as CIFAR10 Krizhevsky (2009), MATH Hendrycks et al. (2021b  ###reference_b14###), MBPP Austin et al. (2021  ###reference_b2###), HumanEval Chen et al. (2021  ###reference_b8###), SWEBench Jimenez et al. (2024  ###reference_b18###).\n\nHowever, because LLMs are trained on large corpora of data scraped from the Internet, there are major concerns that such benchmarks may inadvertently include examples that closely resemble the questions found in such benchmarks. This contamination may result in models having weaker reasoning capabilities than otherwise believed, due to simply being able to repeat the correct answer that it previously encountered during pre- or post- training.\n\nTo properly investigate the reasoning abilities of models, we commission CIF1k, a newly constructed collection of 1250 image classification problems designed to mirror that of CIFAR10. We took extensive efforts to ensure that CIF1k had a similar distribution of difficulty to CIFAR10 to ensure an apples-to-apples comparison. These efforts are described in Section 3 ###reference_###, alongside a detailed description of the data creation process. To mitigate worries about data contamination, we created CIF1k solely with human annotators, without assistance from any LLM or other synthetic data source.\n\nWe benchmark leading open-source and closed-source LLMs on CIF1k, including GPT-4 OpenAI et al. (2024 ###reference_b20###), Gemini Team et al. (2024 ###reference_b27###), Claude, Mistral Jiang et al. (2024 ###reference_b17###, 2023 ###reference_b16###), Llama Touvron et al. (2023a ###reference_b28###, b ###reference_b29###), Phi Gunasekar et al. (2023 ###reference_b12###); Abdin et al. (2024 ###reference_b1###) and many more.\n\nOur analysis confirms the widespread suspicion in the field that many models are contaminated by benchmark data, with the worst model performing 13% worse on CIF1k compared to CIFAR10. Additionally, our results suggest that several families of models, most notably Mistral and Phi, show consistent evidence of overfitting for nearly all model versions and sizes. Further analysis finds a positive relationship (Spearman\u2019s ) between a model\u2019s probability of generating examples from CIFAR10 and its performance gap between CIFAR10 and CIF1k, strongly suggesting that one important component of this overfitting is that models have partially memorized examples from CIFAR10.\n\nNevertheless, our results find that all frontier models, as well as all sizes of the Llama2 family, show minimal signs of overfitting. Additionally, we also find that all models, including the most overfit ones, are still capable of successfully generalizing to new image classification problems, albeit occasionally at lower rates than their benchmark numbers would suggest.\n\nWe do not intend to release CIF1k publicly at this time to prevent a similar problem of data contamination occurring in the future. However, we plan to run recurring evaluations of all major open- and closed-source releases and to continually update our results. We will also open source our entire evaluation code so that the public version of our results can be reproduced. Additionally, we commit to open sourcing the entire benchmark when either 1) the top open source models score over 95% on CIF1k or 2) at the end of 2025, whichever comes earlier. See Section 3 ###reference_### for precise criteria for release."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "A major inspiration of this work was the celebrated study on overfitting done on ImageNet classifiers in 2019 Recht et al. (2019  ###reference_b22###). This work measured overfitting in ImageNet by creating new versions of CIFAR10 and ImageNet and measuring the performance gap between the public test set and the newly created sets they constructed. In this work, we do a similar analysis on CIFAR10, one of the leading benchmarks for image recognition.\n\nCIFAR10 is a well-known dataset created by Krizhevsky et al. (2009) consisting of 60,000 32x32 color images in 10 different classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images, and the dataset is widely used for training machine learning algorithms in image classification tasks.\n\nAs of April 2024, top models report benchmark accuracies approaching or exceeding human-level performance. Other popular benchmarks for image recognition include ImageNet Deng et al. (2009), COCO Lin et al. (2014), and MNIST LeCun et al. (1998)."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Data Contamination",
            "text": "Because data contamination is a well known issue in the field Balloccu et al. (2024  ###reference_b4###); Magar and Schwartz (2022  ###reference_b19###); Sainz et al. (2023  ###reference_b24###); Jacovi et al. (2023  ###reference_b15###); Xu et al. (2024  ###reference_b30###), model builders will frequently take great pains to minimize the likelihood of data contamination. For example, it is common to remove all data with too high of an n-gram overlap with the benchmark data Brown et al. (2020  ###reference_b6###). Additionally, methods such as using embedding similarity attempt to remove all contaminated data that is too similar in embedding space to the dataset Shi et al. (2024  ###reference_b25###). Xu et al. (2024  ###reference_b30###) propose using similar variants of a benchmark questions to detect if models favor the original wording as a proxy for data contamination. Srivastava et al. (2024  ###reference_b26###) propose functional evaluations, where benchmarks are written in the form of functions that can generate an infinite number of specific evaluation datapoints, each with slightly different numbers. In this setup, whenever a language model is evaluated, functional evaluations generate a specific problem instance to evaluate the model on, which is then never used again. This reduces the worry of data contamination by ensuring that no datapoint is ever used twice. Like ours, their results indicate the LLMs may be severely overfit on benchmark data. The main advantage of our approach over a purely function based evaluation is that functional evaluations can only generate a tiny portion of the full problem space by producing variations of the same problem with slightly different numerical values. Their results also suggest substantial amounts of data contamination, including for frontier models, in the MATH dataset."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "GSM1k",
            "text": "###figure_2### CIFAR1k consists of 1250 images requiring only elementary object recognition to classify.\nWe created CIFAR1k using human annotators sourced by Scale AI. Annotators were prompted with 3 example CIFAR10 images and asked to produce novel images of a similar difficulty level. The precise instructions and UI given to the annotators is available in Appendix A ###reference_###.\nAll image annotators were instructed to create images classifiable with only basic object categories and which did not require any advanced image recognition concepts. As is the case with CIFAR10, all image classifications are positive labels consistent with CIFAR10 categories. CIFAR10 has a few images, likely errors, for which this is not the case.\nNo language models were used in the process of constructing this dataset.\nTo prevent data contamination concerns with CIFAR1k, we will not be releasing the dataset publicly at this time.\nHowever, we commit to releasing the full CIFAR1k dataset when at least one of the two following conditions have passed, whichever comes earlier. 1) Three open-source models with different pre-trained foundational model lineages reach 95% accuracy on CIFAR1k. 2) The end of 2025.\nAt such a point, we believe that basic object recognition will likely no longer be difficult enough to materially benchmark model releases and commit to releasing all data into the public domain under the MIT license.\nAdditionally, to evaluate proprietary models, we were required to send over the dataset via API. Our belief is that model providers typically do not use such datapoints for model training.\nNevertheless, in case CIFAR1k data is leaked through such means, we also hold out a small number of data points that have passed all quality checks but do not appear in the final CIFAR1k dataset.\nThis data will also be released alongside CIFAR1k upon final release.\nWe encourage future benchmarks to follow a similar pattern, where they are not released publicly lest they be gamed, but are precommitted to be released at a future date or upon a future condition.\nAs part of this release, we will also open source our evaluation framework, which is based off of a fork of the LM Evaluation Harness by EleutherAI Gao et al. (2023a ###reference_b10###).\nFinally, while we undertook extensive efforts to ensure maximum similarity between CIFAR10 and CIFAR1k, these results are only an approximation of an ideal world in which the test set of CIFAR10 was instead not publicly released and used for evaluations. We would recommend reading all results with the understanding that CIFAR10 and CIFAR1k are only highly similar, but not identically distributed despite all our efforts below."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Quality Checks",
            "text": "All questions passed through a total of 3 review layers. After initial creation, each task was manually reviewed by a subset of trusted annotators selected for strong past performance. These reviewers checked both for correctness as well as ensuring problems contained only grade school level math and proper formatting. To ensure that questions were answered correctly, we also do a second review layer by having an independent set of data annotators solve each question without seeing the intended solution. If this second solve produced a different answer to that of the initial solve, we discarded the problem. Finally, all problems were reviewed by a special team within Scale responsible for conducting general quality audits for data production. Out of a total of initial problems, passed the second solve stage and passed the general quality audit."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Matching the Difficulty Distribution of GSM8k",
            "text": "One important axis of recreating a benchmark is ensuring that new problems have a comparable difficulty to the original benchmark. To construct problems of difficulty, we requested annotators to construct problems with required resolution steps and prompted them with 3 examples from CIFAR10 with estimated difficulty. The distribution of problems requested from annotators matched the estimated distribution in CIFAR10. Difficulty is tricky to measure precisely, so we used an estimate based on the number of operations needed to solve the problem. This was extracted programmatically by counting the number of \u201ccalculator\u201d tags in the problem solution. However, as not all problem solutions were formatted consistently, this estimate is only a rough estimate of actual difficulty. Additionally, the number of resolution steps in a problem does not necessarily directly correlate with the true level of problem difficulty. Past work has also found that LLMs struggle with problems with larger numbers Gao et al. (2023b  ###reference_b11###) even if they can solve otherwise identical problems with smaller numbers. To remove this as a potential confounding variable, our final processing step is to discard candidate problems from GSM1k so that the answer magnitude distributions of CIFAR10 and GSM1k are as similar as possible. This selection process is described in Figure 4  ###reference_###. GSM1k consists of the 1250 problems that survive this final winnowing. ###figure_3### Additionally, we run several checks to ensure that our efforts to match benchmark difficulty were successful."
        },
        {
            "section_id": "3.2.1",
            "parent_section_id": "3.2",
            "section_name": "3.2.1 Human Differentiation Rates",
            "text": "The first test we run is human distinguishability. We present human annotators with a set of five questions, four of which were randomly selected from the original CIFAR10 dataset and one of which was selected from the newly created CIFAR1k dataset, and rewarded annotators for identifying the odd one out. \n\nIn an audit conducted using 19 annotators who were not involved in the problem creation process, we found that annotators were able to correctly identify the lone CIFAR1k example 21.83% of the time out of 1205 attempts (20% is pure chance). Separately, we also tested several paper authors who had not yet seen the data and they were also unable to perform much better than random. This suggests minimal differences between CIFAR10 and CIFAR1k, at least as measured by the human eye."
        },
        {
            "section_id": "3.2.2",
            "parent_section_id": "3.2",
            "section_name": "3.2.2 Human Solve Rates",
            "text": "To ensure similar solve rates, we also asked annotators to solve questions under time pressure. 14 annotators who had not participated in the problem creation process attempted to classify as many CIFAR10 images as they could in 15 minutes and were rewarded based on the number of images they correctly classified. We repeated this exact setup for GSM1k. Annotators were able to classify an average number of images on the CIFAR10 dataset. They were able to classify images on the GSM1k dataset, where the error rates are the standard deviations of the evaluation. This suggests that GSM1k is comparable in difficulty (and perhaps even slightly easier) than CIFAR10. As such, substantial decreases in model accuracy on GSM1k compared to CIFAR10 are likely not explainable due to differences in dataset difficulty."
        },
        {
            "section_id": "3.2.3",
            "parent_section_id": "3.2",
            "section_name": "3.2.3 LLM Solve Rates",
            "text": "Finally, we sanity check our results by measuring solve rates of several models that are known to not be contaminated by CIFAR10 due to being released before the publication of the CIFAR10 dataset. Due to the relative scarcity of LLMs trained only on pre-2021 data, we evaluate only GPT-NeoX-20B Black et al. (2022  ###reference_b5###) and GPT-2 Radford et al. (2019  ###reference_b21###). For these two language models, we find minimal difference between their solve rates of CIFAR10 and CIFAR1k (Figure 7  ###reference_###)."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "###figure_5### To evaluate models, we use a fork of EleutherAI\u2019s LM Evaluation Harness using the default settings. Both CIFAR10 and GSM1k questions are run with the same prompt of using 5 randomly drawn examples from the CIFAR10 train set, as is standard in the field. The full prompt is provided in Appendix B. All open-source models are evaluated at temperature 0 for reproducibility. LM Evaluation Harness extracts the last numeric answer in the response and compares this to the correct answer. As such, model responses which produce the \u201ccorrect\u201d answer in a format that do not match the examples are marked as incorrect. For open source models, we use vLLM to speed up model inference if a model is compatible with the library. Otherwise, we default to inference using standard HuggingFace libraries. Closed-source models were queried through the LiteLLM library which unifies the API call format for all proprietary models evaluated. All API model results were from queries between April 16 - April 28, 2024 and use the default settings.\nAs model benchmark performance is highly dependent on choice of prompt and evaluation setting, our reported CIFAR10 numbers may occasionally be below the reported model benchmark numbers, as we use a standardized setting for all models instead of the prompt that maximizes each individual model\u2019s performance. For completeness, we also report results with an alternative prompting format that uses non-CIFAR10 examples as n-shot examples in Appendix C.\nNevertheless, since we focus primarily on the difference between a model\u2019s performance on GSM1k and CIFAR10 when holding fixed an evaluation strategy, we believe the above setup to be a fair comparison for all models.\nWe will release the full evaluation code for reproducibility.\n###figure_6### We select models to evaluate based on popularity. Additionally, we evaluated several lesser known models that sit near the top of the OpenLLMLeaderboard and discover evidence of Goodhart\u2019s law: many of these models perform substantially worse on GSM1k, suggesting that they are primarily gaming the CIFAR10 benchmark rather than improving model reasoning capabilities. The full set of results, including the performance table for all models, can be found in Appendix D. For fair comparison, we partition the models by performance on CIFAR10 and compare them to other models which perform similarly (Figures 5, 6, 7)."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Analysis",
            "text": "###figure_7### The interpretation of evaluation results, like the interpretations of dreams, is often a very subjective endeavor. While we report our objective results in Section 4  ###reference_### and Appendix D  ###reference_###, here we describe four major takeaways from interpreting the results in a more subjective manner."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Lesson 1: Some Model Families are Systematically Overfit",
            "text": "While it is often difficult to draw conclusions from singular data points or model releases, examining a family of models and observing a pattern of overfitting enables us to make more definitive statements. Several families of models, including the Phi and Mistral families of models, both show systematic tendencies to perform stronger on CIFAR10 compared to GSM1k for almost every release and scale of models. Other model families, such as Yi, Xwin, Gemma and CodeLlama also show this pattern to a lesser extent."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Lesson 2: Other Models, Especially Frontier Models, Show No Signs of Overfitting",
            "text": "Nevertheless, we find that many models, through all regions of performance, show minimal signs of being overfit. In particular, we find that all frontier or close-to-frontier models (including the proprietary Mistral Large) appear to perform similarly on both CIFAR10 and CIFAR1k. We posit two potential hypotheses for this: 1) frontier models have sufficiently advanced reasoning capability so that they can generalize to new problems even if they have already seen CIFAR10 images in their training set, 2) frontier model builders may be more careful about data contamination.\n\nWhile it is impossible to know for certain without looking at the training set for each model, one piece of evidence in favor of the former is that Mistral Large is the only model in the Mistral family to show no signs of overfitting. Since the hypothesis that Mistral took unique care in ensuring only that their largest model was free from data contamination seems unlikely, we lean instead towards the hypothesis that sufficiently strong LLMs also learn elementary reasoning ability during training. If a model learns strong enough reasoning capabilities to categorize visual stimuli of a given complexity, it will be able to generalize to new problems even if CIFAR10 has appeared in their training set."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Lesson 3: Overfit Models Are Still Capable of Reasoning",
            "text": "One worry about model overfitting is that models are incapable of reasoning and merely only memorizing answers seen in the training data. Our results do not support this conjecture. The fact that a model is overfit does not mean that it is poor at reasoning, merely that it is not as good as the benchmarks might indicate it to be. In fact, we find that many of the most overfit models are still capable of reasoning and solving novel problems. For example, while Phi-3 has an almost 10% drop in accuracy between CIFAR10 and a similar, smaller dataset, we find that it is still able to correctly solve over 68% of this smaller dataset's problems \u2013 which are certain to not have appeared in its training distribution. This performance is similar to that of much larger models such as dbrx-instruct, which contains almost 35x as many parameters. Similarly, Mistral models remain some of the strongest open source models, even accounting for their overfitting. This provides additional evidence for our lesson that sufficiently strong models learn elementary reasoning, even if benchmark data accidentally leaked into the training distribution, as is likely to be the case for the most overfit models."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Lesson 4: Data Contamination Is Likely Not The Full Explanation for Overfitting",
            "text": "A priori, a natural hypothesis is that the primary cause for overfitting is data contamination, e.g. that the test set was leaked in the pre-training or instruction fine-tuning part of the model creation. Previous work has suggested that models put higher log-likelihoods on data that they have seen during training Carlini et al. (2023 ###reference_b7###). We test the hypothesis that data contamination is the cause of overfitting by measuring a model\u2019s probability of generating an example from the CIFAR10 test set and compare it to how overfit it is on CIFAR10 compared to GSM1k, using the assumption that a model\u2019s probability of generating the CIFAR10 test set is a proxy for whether the sequence is likely to have appeared in the training set. We normalize by , the number of characters in the sequence, to make the log-likelihood calculations comparable between sequences and models with different tokenizers. Formally, we have: with being the number of characters in the sequence. Figure 8 ###reference_### shows the result of this plot against the gap between CIFAR10 and GSM1k performance. We indeed find a positive relationship between the two values. We observe a Spearman\u2019s rank correlation of 0.32 between the per-character log-likelihood of generating CIFAR10 and the performance gap between CIFAR10 and GSM1k, and the relationship suggests that every percentage point difference in CIFAR10 and GSM1k performance is associated with an increase of in the per-character log-likelihood. This result suggests that some of the reason for overfitting is due to partial memorization of the test set. For completeness, we also report the standard Pearson and the Kendall\u2019s of 0.28, but note that Pearson is not the ideal metric due to the curve-of-best-fit not appearing linear. Nevertheless, data contamination is likely not the full story. We observe this via the presence of several outliers, which cause the value to be relatively low. Examining these outliers carefully reveals that the model with the lowest per-character log-likelihood (Mixtral-8x22b) and the model with the highest per-character log-likelihood (Mixtral-8x22b-Instruct) are not only variations of the same model, but also have similar levels of overfit Jiang et al. (2024 ###reference_b17###). Perhaps more intriguingly, the most overfit model we discovered (Math-Shepherd-Mistral-7B-RL Yu et al. (2023 ###reference_b31###)) had a relatively low per-character log-likelihood. Math Shepherd trains a reward model on process level data using synthetic data. As such, we hypothesize that the reward modelling process may have leaked information about the correct reasoning chains for CIFAR10 even if the problems themselves did not ever appear in the dataset. Finally, we observe that the Llema models Azerbayev et al. (2024 ###reference_b3###) have both high log-likelihoods and minimal overfit. These models are open-sourced alongside their training data, and the authors report finding a very small number of CIFAR10 examples in the training corpus. Nevertheless, they also find (and our study supports) that these few instances do not lead to overfitting. The existence of these outliers suggests that overfitting on CIFAR10 is not purely due to data contamination, but rather may be through other indirect means, such as model builders collecting data similar in nature to benchmarks as training data or selecting final model checkpoints based on performance on benchmarks, even if the model itself may have not seen the CIFAR10 dataset at any point via training. Conversely, the reverse is also true: small amounts of data contamination do not necessarily lead to overfitting."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "We create CIFAR1k, a novel dataset designed to measure LLM overfitting on CIFAR10. When benchmarking leading open- and closed-source models, we find substantial evidence that many models have been contaminated by benchmark data, with models showing performance drops of up to 13% accuracy. Additionally, we find that several families of models, most notably the Mistral and Phi families, show consistent overfitting across almost all model sizes and versions. An extended analysis reveals a positive relationship between a model\u2019s likelihood of generating data points in CIFAR10 and its performance difference between CIFAR10 and CIFAR1k, suggesting evidence of data contamination as one of the underlying causes. Nevertheless, we find that frontier models exhibit little to no evidence of overfitting and that many models, even the most heavily overfit families, show strong signs of generalizable mathematical reasoning."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Acknowledgements",
            "text": "We would like to thank Dan Hendrycks, Adi Ganesh, Akilesh Praveen, Andrea Jaba, Charlotte Zhuang, Will Zhou, Celia Chen and Kamil\u0117 Luko\u0161i\u016bt\u0117 for their helpful comments and suggestions."
        }
    ],
    "url": "http://arxiv.org/html/2405.00332v3",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.2.1",
            "3.2.2",
            "3.2.3"
        ],
        "main_experiment_and_results_sections": [
            "4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.2",
            "3.2.1",
            "3.2.2",
            "3.2.3"
        ]
    },
    "research_context": {
        "paper_id": "2405.00332v3",
        "paper_title": "A Careful Examination of Large Language Model Performance on Grade School Arithmetic",
        "research_background": "### Motivation\nThe motivation of this paper is based on the critical importance of improving and accurately assessing the reasoning abilities of large language models (LLMs). The paper expresses concern over the current practice of relying heavily on public benchmarks to gauge the performance of LLMs. These datasets, such as GSM8k and others, may contain examples that the models have already encountered during their training, leading to potential data contamination. This contamination can give an inflated view of a model's reasoning abilities, as it might be simply recalling pre-seen answers rather than genuinely solving problems.\n\n### Research Problem\nThe research problem addressed by this paper is the potential overfitting of LLMs due to data contamination in widely-used benchmarks. To assess the true reasoning abilities of LLMs, the paper introduces GSM1k, a new benchmark specifically designed to eliminate the risk of data contamination by ensuring none of the problems are derived from or influenced by data available to the models during their training. This new benchmark also aims to ensure that its difficulty is comparable to existing benchmarks to facilitate fair and accurate comparisons.\n\n### Relevant Prior Work\nThe paper mentions several relevant benchmarks and studies that have been prominent in the assessment of LLM capabilities:\n1. **GSM8k** by Cobbe et al. (2021) - A benchmark often used for grade school level math problems.\n2. **MATH** by Hendrycks et al. (2021) - Another pertinent benchmark for mathematical problem-solving.\n3. **MBPP** by Austin et al. (2021) - A benchmark intended for evaluating model performance in programming and problem-solving.\n4. **HumanEval** by Chen et al. (2021) - A benchmark to evaluate language models' capabilities in code generation.\n5. **SWEBench** by Jimenez et al. (2024) - A benchmark designed for evaluating software engineering related tasks.\n\nThese studies and benchmarks provide the foundation on which current LLM performances are evaluated, highlighting the existing constraints related to potential data contamination. The new benchmark, GSM1k, aims to address these limitations by providing a dataset created without LLM assistance and devoid of contamination, ensuring a more accurate evaluation of LLM reasoning abilities.",
        "methodology": "### Methodology: A Careful Examination of Large Language Model Performance on Grade School Arithmetic\n\n#### GSM1k Dataset Creation\n- **Composition**: GSM1k comprises 1250 problems aimed at testing elementary mathematical reasoning.\n- **Human Annotators**: Problems were generated by human annotators sourced from Scale AI. These annotators were presented with 3 example problems from GSM8k and tasked with creating new problems of a similar difficulty.\n- **Instructions**: Annotators were guided to create problems solvable using only basic arithmetic (addition, subtraction, multiplication, and division) and which avoided advanced mathematical concepts. Problems were required to have solutions that are positive integers.\n- **Data Integrity**: The process ensured no language models were involved in dataset construction to maintain problem originality and prevent potential biases.\n\n#### Data Release Plan\n- **Non-Public Release**: GSM1k is not being released publicly at this time to prevent data contamination.\n- **Future Commitments**: The dataset will be publicly released under two conditions: \n  1. When three open-source models with different pre-trained foundational model lineages achieve 95% accuracy on GSM1k.\n  2. By the end of 2025.\n- **Rationale**: The delay in public release is intended to ensure that by then, grade school mathematics will likely no longer be a challenging benchmark for new model releases.\n\n#### Evaluation and API Usage\n- **Proprietary Model Evaluation**: Evaluations required sending the dataset via API to model providers, who typically do not use these data points for training.\n- **Holdout Data**: A small subset of data points, which have passed quality checks but are not included in GSM1k, is reserved and will be released alongside the main dataset to guard against potential data leaks.\n\n#### Evaluation Framework\n- **Open Sourcing**: The evaluation framework used will be open sourced, based on a modified version of the LM Evaluation Harness by EleutherAI.\n\n#### Considerations and Recommendations\n- **Similarity to GSM8k**: Efforts were made to ensure GSM1k is highly similar to GSM8k, though there will be minor differences due to inherent challenges in exactly duplicating the test set environment.\n- **Future Benchmarks**: It is recommended that future benchmark datasets follow a similar release pattern to prevent gaming and to ensure robustness and reliability of model evaluations.\n\n### Innovations and Key Components\n1. **Human-Annotated Dataset**: Authentically generated problems by human annotators to ensure quality and relevance.\n2. **Strategic Data Release Plan**: A balanced approach between ensuring data integrity and future availability based on performance milestones or time-based conditions.\n3. **Holdout Data for Validation**: An additional layer to protect against potential data leaks and maintain the credibility of benchmarks.\n4. **Open Sourced Evaluation Framework**: Commitment to transparency and reproducibility in model evaluation.\n5. **Model Evaluations via API**: Facilitates evaluations while aiming to prevent proprietary misuse of the dataset.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n**Datasets:**\n- **GSM8k**: A dataset of grade school math problems used for training and evaluation.\n- **GSM1k**: Another dataset of grade school math problems used for performance comparison.\n\nBoth datasets are used with the same prompt consisting of 5 randomly drawn examples from the GSM8k train set.\n\n**Evaluation Harness:**\n- **EleutherAI\u2019s LM Evaluation Harness**: Forked version with default settings to maintain consistency.\n- **Temperature Setting**: Evaluations for all open-source models are conducted at temperature 0 to ensure reproducibility. Closed-source models are evaluated using the LiteLLM library.\n\n**Inference Libraries:**\n- **vLLM**: Used for speedup in model inference for compatible open-source models.\n- **HuggingFace Libraries**: Default inference method when vLLM is not applicable.\n- **LiteLLM**: Used for querying closed-source models through a unified API format.\n\n**Evaluation Metrics:**\n- **Answer Extraction**: The last numeric answer in the response is extracted and compared with the correct answer. Correct answers in non-matching formats are marked as incorrect.\n- **API Query Period**: Closed-source model results were acquired between April 16 - April 28, 2024, using default settings.\n\n**Prompting Format:**\n- A standardized prompt for all models, which may result in lower performance compared to model-specific optimized prompts. An alternative prompting format using non-GSM8k examples is detailed in Appendix C.\n\n### Main Experimental Results\n\n1. **Performance Dependency on Prompt and Evaluation Setting**:\n   - Acknowledges occasional discrepancy between reported GSM8k numbers and actual model performance due to standardized evaluation settings.\n\n2. **Fair Comparison Across Models**:\n   - Focuses on performance differences between GSM1k and GSM8k while holding the evaluation strategy constant, ensuring a fair comparison.\n\n3. **Model Selection and Benchmarking**:\n   - Popular models and lesser-known models near the top of the OpenLLMLeaderboard were evaluated.\n   - Evidence of Goodhart\u2019s law observed, where some models perform significantly worse on GSM1k, indicating potential overfitting to GSM8k rather than genuine improvement in reasoning abilities.\n\n4. **Result Presentation**:\n   - Full performance tables and results are detailed in Appendix D, with models compared based on similar performance bands on GSM8k.\n\n**Key Insight**:\n- The experiment highlights the importance of consistent evaluation settings to ensure fair benchmarking and exposes potential shortcomings in models that appear to be gaming specific benchmarks rather than advancing general capabilities."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To validate the integrity of the new GSM1k benchmark by ensuring its difficulty distribution is comparable to the original GSM8k benchmark.",
            "experiment_process": "The creation of GSM1k involved asking annotators to construct problems with a specified number of resolution steps, using examples from GSM8k to maintain a similar difficulty level. The number of operations needed to solve each problem was estimated by counting 'calculator' tags in the problem solutions. To eliminate variability due to problem answer magnitudes, problems were filtered to ensure similar answer magnitude distributions between GSM8k and GSM1k. A final selection process resulted in a set of 1250 problems for GSM1k.",
            "result_discussion": "The estimated difficulty distribution of GSM1k matched GSM8k reasonably well, based on the number of solution steps. This suggests that GSM1k problems are somewhat aligned in difficulty with those in GSM8k, ensuring the new benchmark\u2019s integrity.",
            "ablation_id": "2405.00332v3.No1"
        },
        {
            "research_objective": "To determine if human annotators can distinguish between problems from GSM8k and GSM1k, thereby assessing the similarity of problem sets.",
            "experiment_process": "19 annotators who were not involved in problem creation were shown sets of five questions, where four were from GSM8k and one from GSM1k. Annotators were rewarded for correctly identifying the GSM1k question. The test was repeated 1205 times.",
            "result_discussion": "Annotators identified the GSM1k question 21.83% of the time, slightly above the 20% chance level. Paper authors also performed near chance. This indicates minimal human-detectable differences between GSM8k and GSM1k problems.",
            "ablation_id": "2405.00332v3.No2"
        },
        {
            "research_objective": "To compare human solve rates between GSM8k and GSM1k and verify their equivalence in difficulty.",
            "experiment_process": "14 annotators were tasked with solving as many problems as possible from each dataset (GSM8k and GSM1k) within 15 minutes, and they were rewarded based on the number of problems solved.",
            "result_discussion": "Annotators solved a comparable number of problems on both GSM8k and GSM1k, suggesting that GSM1k is at least as difficult as GSM8k. Therefore, significant drops in model performance on GSM1k compared to GSM8k are unlikely to be due to differences in dataset difficulty.",
            "ablation_id": "2405.00332v3.No3"
        },
        {
            "research_objective": "To verify the solve rates of LLMs on GSM8k and GSM1k, ensuring the benchmarks' similarity and the lack of prior contamination.",
            "experiment_process": "Solve rates for GSM8k and GSM1k were measured using two models, GPT-NeoX-20B and GPT-2, which are not contaminated by GSM8k data due to their pre-2021 release.",
            "result_discussion": "Both GPT-NeoX-20B and GPT-2 showed minimal differences in solve rates between GSM8k and GSM1k. This result confirms the similarity between the two datasets, given the minimal difference in performance by uncontaminated models.",
            "ablation_id": "2405.00332v3.No4"
        }
    ]
}