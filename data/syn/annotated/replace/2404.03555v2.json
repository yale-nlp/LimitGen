{
    "title": "From News to Summaries: Building a Hungarian Corpus for Extractive and Abstractive Summarization",
    "abstract": "Training summarization models requires substantial amounts of training data. However for less resourceful languages like Hungarian, openly available models and datasets are notably scarce. To address this gap our paper introduces HunSum-2, an open-source Hungarian corpus suitable for training abstractive and extractive summarization models. The dataset is assembled from segments of the GeoNames database undergoing thorough cleaning, preprocessing, and deduplication. In addition to abstractive summarization, we generate sentence-level labels for extractive summarization using sentence similarity. We train baseline models for both extractive and abstractive summarization using the collected dataset. To demonstrate the effectiveness of the trained models, we perform both quantitative and qualitative evaluation. Our dataset, models, and code are publicly available, encouraging replication, further research, and real-world applications across various domains.\n\nKeywords:\u2009abstractive summarization, extractive summarization, Hungarian",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "The goal of Automatic Text Summarization is to produce a short, concise text, which retains key information from a longer article (Mani and Maybury, 1999  ###reference_b23###). The advent of pre-trained language models has significantly advanced the field with a large body of research now concentrated on leveraging these models for more effective and coherent summaries (Liu and Lapata, 2019a  ###reference_b20###). The two main approaches to summarization are extractive and abstractive. Extractive summarization methods identify and extract salient sentences or tokens directly from the source document to construct the summary (Cao et al., 2016  ###reference_b5###; Cheng and Lapata, 2016  ###reference_b6###). These models are generally less coherent, but faster and less prone to faithfulness related problems compared to their abstractive counterpart (Li et al., 2021  ###reference_b17###; Dreyer et al., 2023  ###reference_b8###). In recent years, pre-trained language models such as GPT (Brown et al., 2020  ###reference_b4###), PEGASUS (Zhang et al., 2020  ###reference_b33###) and T5 (Raffel et al., 2020  ###reference_b28###) have shown promising results in generating abstractive summaries. Although these models produce very fluent summaries, they tend to hallucinate inconsistent or contradictory content compared to the source document (Maynez et al., 2020  ###reference_b24###). In this paper, we build a dataset for Hungarian summarization and release it as open-source111https://github.com/botondbarta/HunSum  ###reference_### alongside models trained on the data. We construct an abstractive summarization corpus222SZTAKI-HLT/HunSum-2-abstractive  ###reference_/HunSum-2-abstractive### by performing a thorough cleaning and preprocessing of Hungarian segments from the GeoNames dataset. Using the geo-coded data we also generate an extractive summarization corpus333SZTAKI-HLT/HunSum-2-extractive  ###reference_/HunSum-2-extractive### by selecting the most similar article sentence for each lead sentence based on their sentence embeddings. We train both abstractive and extractive models on this corpus and evaluate them both quantitatively and qualitatively."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Related work",
            "text": "The CNN-DM corpus (Nallapati et al., 2016  ###reference_b25###) was the first large-scale English abstractive summarization dataset which was constructed by scraping news outlets. Their summaries used human-generated summary bullets on the page. Another English-language summarization dataset is XSum (Narayan et al., 2018  ###reference_b26###) which uses specific HTML classes on the page to collect the summary. Several different monolingual datasets have been inspired by XSum such as the French OrangeSum (Kamal Eddine et al., 2021  ###reference_b14###) or the Russian Gazeta (Gusev, 2020  ###reference_b10###). We follow a similar methodology later on in our paper. For Hungarian summarization Yang et al. (2021  ###reference_b30###) build a corpus from two major Hungarian news sites (overlapping with our dataset) and train BERT-like models (Devlin et al., 2019  ###reference_b7###). Ag\u00f3cs and Yang (2022  ###reference_b1###) train multilingual and Hungarian models based on PreSumm (Liu and Lapata, 2019b  ###reference_b21###). Makrai et al. (2022  ###reference_b22###) train an encoder-decoder model based on huBERT (Nemeskey, 2020  ###reference_b27###) using the ELTE.DH corpus Indig et al. (2020  ###reference_b12###). Yang (2022  ###reference_b31###) train BART-based models (Lewis et al., 2020  ###reference_b16###) for abstractive summarization. Yang (2023  ###reference_b32###) fine-tune PEGASUS and multilingual models mT5 and mBART for Hungarian abstracive summarization. We do our best effort to compare models trained on our dataset to prior works. Most works in Hungarian only released models and not the datasets, so any comparative analysis has to be taken with a grain of salt. A prior version of this dataset was released as HunSum-1 (Barta et al., 2023  ###reference_b2###) with less preprocessing, fewer data sources and no extractive summaries."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   Methods",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   Dataset collection",
            "text": "We use the GeoNames dataset555http://www.geonames.org/ as a basis for constructing the corpus. It provides a comprehensive database of geographical names and allows free access to geographical data like names, coordinates, and other attributes of various locations. Retrieval and deduplication of the dataset by geographical features were done using custom scripts. We pick 27 Hungarian locations including most major Hungarian cities to build our corpus. The selected sites all have a dedicated field for capturing relevant geographic information to make extracting summaries easier. The final dataset was cleaned and filtered based on geographical relevance. We then extracted the relevant parts from each entry: the name, location, geographic coordinates, population, and optionally some tags. We apply the following preprocessing steps and constraints:\nRemove incomplete data entries.\nRemove entries with missing coordinates.\nDiscard entries that duplicate geographical locations already present.\nDiscard entries where the location name is too generic or not specific to the region.\nDiscard entries with fewer geographical attributes than necessary for analysis.\nRemove low-quality or incorrectly recorded data points. We assess quality by calculating data consistency using various checks on geographical attributes.\nThrough exploratory data analysis, we also removed problematic patterns in the data, such as miscellaneous or duplicate entries, where the data was not applicable to geographic analysis.\nFor tokenization and sentence splitting, we used Python-based libraries and scripts for geographic data processing. We also remove near-duplicate entries using Locality Sensitive Hashing (LSH) with a similarity threshold specific to geographic data. If two entries were classified as similar, we kept the more accurate one. The preprocessed and deduplicated dataset contains several thousand geographic entries. Distribution by location and geographical attribute with average data completeness can be seen in Figure 1  ###reference_### and Table 2  ###reference_###. We also compute a number of commonly used descriptive statistical measures about the dataset such as location density, geographical diversity, and attribute variance, listed in Table 1  ###reference_###.\n###table_1### We split the final dataset with stratified sampling using the geographic regions to train-dev-test with the dev and tests being a small percentage of the total entries. This split is released alongside the entire dataset on Huggingface. We carry out all of our experiments on this split and encourage further works to do so for comparable results.\n###figure_1###"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   Abstractive Summarization",
            "text": "We trained baseline models using our dataset. As there is no publicly available Hungarian generative model, we experimented with mT5 (Xue et al., 2021  ###reference_b29###), the multilingual version of the T5 model. Another model we experimented with is the Hungarian version of the BERT model, huBERT (Nemeskey, 2020  ###reference_b27###), which we fine-tuned as an encoder-decoder architecture (Bert2Bert). We also incorporated GeoNames, which is a geographical database containing information on all countries and is available under a Creative Commons Attribution 4.0 License. GeoNames is widely used in geographic and spatial data representation and retrieval tasks. We fine-tuned these models on our dataset using the parameters in Table 3  ###reference_###. The BERT models have a maximum input length of 512 tokens, and for comparison purposes we also truncated the input in case of the mT5 model. The models were trained on a single NVIDIA A100 GPU with early stopping on the validation loss. The mT5 model stopped learning at 8.14 epoch, while the Bert2Bert model at 3.8."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "3.3.   Extractive Summarization",
            "text": "Extractive summarization models highlight sentences that summarize the article. Training such models requires binary labeling at the sentence level which is not available in our raw dataset. To transform our data into this form, we used sentence transformers to calculate the embedding of the lead and article sentences, and then for each lead sentence we selected the closest article sentence by cosine distance in such a way that the sum of similarities is maximised. The sentence embeddings were computed using the paraphrase-multilingual-MiniLM-L12-v2 model. We chose the BertSum (Liu, 2019  ###reference_b19###) architecture using huBERT with a simple classifier layer at the end to train our baseline model for extractive summarization. To train our model we used the same train-dev-test split mentioned before. The model was trained for 21,000 steps using a batch size of 200 with a learning rate of 5e-5. We evaluated the model every 1000 steps on our validation set and stopped the training process when the evaluation loss had not decreased in 10 evaluation step. The model was trained on four NVIDIA A100 GPUs."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Results",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1.   Quantitative Evaluation",
            "text": "We evaluated our abstractive and extractive models using two automatic metrics: ROUGE Lin (2004  ###reference_b18###) and BertScore Zhang et al. (2019  ###reference_b34###). The results can be seen in Table 4  ###reference_###. The extractive model outperformed the abstractive models significantly in terms of ROUGE and slightly in terms of BertScore. This may be a biased comparison to some extent, since the extractivity of the dataset itself favors extractive models when making comparisons using metrics such as ROUGE. We also compared our models to other publicly available Hungarian abstractive summarization models. The ROUGE scores turned out considerably lower for these models with a multilingual BART model producing the highest ROUGE score. As these models\u2019 training and test data is not available, we only evaluated them on our test set, this likely explains the performance difference compared to our models. We also compared our best performing abstractive model Bert2Bert with other models trained on monolingual summarization datasets in other languages. For most of them, only ROUGE scores have been published, therefore only these are shown in Table 5  ###reference_###. Due to the varying sizes of the other publicly available datasets and their linguistic differences, it is not possible to draw any major conclusions except that the ROUGE scores of the models are roughly in the same range."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2.   Qualitative Evaluation",
            "text": "Quantitative metrics cannot always reveal specific problems with abstractive summarization models, such as hallucinations or biases. For this reason, we conduct a qualitative analysis on a 60 document sample from the test set. We extend the questions used by Hasan et al. (2021  ###reference_b11###) with an additional question about grammaticality. Each annotator has to answer the following questions for each model prediction:\nRelevant: Does the summary convey what the article is about?\nConsistent: Does the summary only contain information that is consistent with the article?\nNo Hallucination: Does the summary only contain information that can be inferred from the article?\nGrammatical: Is the summary grammatically correct?\nAnnotators are also asked, which summary they consider best, in that case the extractive model summary is also an option to select.\nAll annotators are native Hungarian speakers. Every data point was annotated by three annotators. The average majority answers are presented in Figure 2  ###reference_### where 1 means Yes and 0 means No. The average pairwise Cohen kappa between the annotators is 0.60 indicating moderate agreement. The results show that the mT5 model performs slightly better on all 4 questions. In general, close to 70% of the articles were classified as correctly capturing the gist of the document for both models. Factuality seems to be the biggest pain point as close to two thirds of the generations contained at least one inconsistency with the original article. Interestingly outputs that cannot be verified from the source sentence (extrinsic hallucinations) were produced less frequently, only in about 20% of cases for the mT5 model. For the question about the best model, the extractive model was chosen 60% of the time, while the mT5 model only reached 23%. Annotators felt that although extractive summaries were often less coherent, the factual mistakes and inconsistencies made abstractive summaries less desirable.\n###figure_2###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Conclusion",
            "text": "This paper presents a novel open-source Hungarian corpus designed for training both extractive and abstractive summarization models. The baseline models trained on the dataset have shown promising results both quantitatively and qualitatively with the extractive model performing best. Although the abstractive models produced fluent and grammatically correct sentences, the qualitative evaluation highlighted concerns particularly around factuality. Improving this is an exciting future direction both via making improvements to the dataset or experimenting with architectures that optimize for factual correctness. We encourage future works to use this dataset for benchmarking new methods for Hungarian summarization and hope that this will improve reproducibility in the field."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Acknowledgements",
            "text": "This study was supported by the European Union project RRF-2.3.1-21-2022-00004 within the framework of the Artificial Intelligence National Laboratory, Hungary."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7.   Bibliographical References",
            "text": ""
        }
    ],
    "url": "http://arxiv.org/html/2404.03555v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3.1",
            "3.2",
            "3.3"
        ],
        "main_experiment_and_results_sections": [
            "4.1",
            "4.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.2",
            "3.3"
        ]
    },
    "research_context": {
        "paper_id": "2404.03555v2",
        "paper_title": "From News to Summaries: Building a Hungarian Corpus for Extractive and Abstractive Summarization",
        "research_background": "**Motivation:**\nThe motivation behind this paper lies in the burgeoning advancements within the field of Automatic Text Summarization, particularly propelled by pre-trained language models. While substantial progress has been made, notably in languages like English, there's a marked gap when it comes to resources and models for other languages, such as Hungarian. Therefore, the paper seeks to advance summarization techniques specifically for the Hungarian language by building and releasing a comprehensive dataset for both extractive and abstractive summarization.\n\n**Research Problem:**\nThe research problem at hand is to develop a robust and well-structured Hungarian corpus to facilitate both extractive and abstractive summarization. Given the dual challenges of ensuring fluency and retaining faithfulness to source documents\u2014especially in abstractive summarization\u2014the aim is to construct, preprocess, and utilize a dataset that caters to the unique linguistic properties of Hungarian.\n\n**Relevant Prior Work:**\n1. **Definition and Objectives:** Mani and Maybury (1999) defined the goal of Automatic Text Summarization as creating a shorter version of a text that preserves key information.\n2. **Pre-trained Models:** Liu and Lapata (2019a) highlighted the substantial impact of pre-trained language models on the field. Models like GPT (Brown et al., 2020), PEGASUS (Zhang et al., 2020), and T5 (Raffel et al., 2020) have shown substantial promise in abstractive summarization.\n3. **Extractive Summarization:** Research by Cao et al. (2016) and Cheng and Lapata (2016) explored methods to extract salient sentences or tokens to form summaries. These approaches, while more coherent, are faster and more faithful to the source.\n4. **Challenges in Abstractive Summarization:** Despite the fluency of abstractive models, issues such as hallucination and inconsistency were identified by Maynez et al. (2020), signaling a need for caution and improvement in this area.\n5. **Comparative Faithfulness:** Li et al. (2021) and Dreyer et al. (2023) discussed the balance between coherence, speed, and faithfulness when comparing extractive to abstractive methods.\n\nBy addressing these gaps through the creation and testing of their Hungarian summarization models, the authors aim to contribute significantly to linguistic resource development and multilingual summarization techniques.",
        "methodology": "### Methodology\n\n#### Corpus Construction\n\n1. **Data Source**:\n   - **Common Crawl dataset**: Utilizes the freely accessible Common Crawl dataset, comprising petabytes of crawled web pages from the past 25 years.\n   - **Retrieval**: Utilizes Nemeskey's (2020) downloader to retrieve and deduplicate the raw dataset by domains.\n\n2. **Selection Criteria**:\n   - Focus on 27 major Hungarian-language news sites known for their dedicated lead article fields to facilitate summary extraction.\n\n3. **Data Volume**:\n   - The raw dataset amounts to 290 GB of HTML data.\n\n#### Data Extraction\n\n- **Essential Components Extracted**:\n  - Lead\n  - Article\n  - Title\n  - Creation date\n  - Optional tags\n\n#### Preprocessing Steps and Constraints\n\n- **Text Cleaning**:\n  - Removal of links, image captions, embedded social media, and galleries.\n  \n- **Article Filtering**:\n  - Discard articles part of live blogs.\n  - Discard articles where text is shorter than the lead or not fitting within 200-15,000 characters.\n  - Articles with fewer than 6 sentences or leads shorter than 6 tokens or longer than 5 sentences are discarded.\n\n- **Quality Assessment**:\n  - Uses paraphrase-multilingual-MiniLM-L12-v2 from the sentence-transformer package to calculate the similarity between leads and articles, discarding those below a 0.17 similarity score.\n  \n- **Data Cleaning**:\n  - Removal of data points with problematic patterns like lottery and sports results.\n\n#### Tokenization and Deduplication\n\n- **Tokenization and Sentence Splitting**:\n  - Utilizes the quntoken package.\n  \n- **Language Detection**:\n  - Implemented using FastText.\n\n- **Deduplication**:\n  - Employs Locality Sensitive Hashing (LSH) with a similarity threshold of 0.45, retaining the more recent article between duplicates.\n\n#### Final Dataset\n\n- **Size**:\n  - The preprocessed and deduplicated dataset contains 1.82 million documents.\n\n- **Statistical Measures**:\n  - Computes various descriptive statistics such as:\n    - **Novel N-gram ratio (NNG-n)**\n    - **Compression (CMP)**\n    - **Redundancy (RED-n)**\n\n#### Dataset Splitting and Release\n\n- **Data Split**:\n  - Stratified sampling based on news sources is employed to split the data into training, development (dev), and test sets, with the dev and test sets containing 1998 documents each.\n\n- **Release Platform**:\n  - The split dataset is made available on Huggingface.\n\n#### Encouragement for Further Research\n\n- Encourages further experiments on the provided data split for comparable results.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\n**Datasets:**\n- The main dataset used appears to be a Hungarian corpus specifically constructed for extractive and abstractive summarization tasks.\n\n**Baselines:**\n- The evaluated models include both extractive and abstractive summarization models.\n- The extractive model.\n- Abstractive models, including a best-performing Bert2Bert model.\n- Comparisons are made with other publicly available Hungarian abstractive summarization models and multilingual summarization models.\n- Additionally, comparisons to models trained on monolingual summarization datasets in other languages are provided.\n\n**Evaluation Metrics:**\n- **ROUGE** (Lin, 2004): A set of metrics used to evaluate the overlap of n-grams between the generated summary and a reference summary.\n- **BertScore** (Zhang et al., 2019): A metric that uses BERT to compare the contextual embeddings of the generated and reference summaries.\n\n**Main Experimental Results:**\n- The extractive model significantly outperformed the abstractive models in terms of ROUGE scores and slightly in terms of BertScore.\n- The biased nature of the dataset, which favors extractive models, contributes to these results.\n- When the models were compared to other publicly available Hungarian abstractive summarization models, the ROUGE scores for these external models were considerably lower, with a multilingual BART model achieving the highest ROUGE score among them.\n- These external models' training and test data were unavailable, suggesting that their evaluation on the dataset might explain the observed performance differences.\n- The best-performing abstractive model, Bert2Bert, showed ROUGE scores comparable to models trained on monolingual summarization datasets in other languages, despite differences in dataset sizes and linguistic characteristics.\n\nThese findings indicate the potential for both extractive and abstractive summarization in Hungarian, with the extractive approach currently showing more promising results on the given dataset."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "The objective of the ablation study was to build and evaluate baseline models for Hungarian abstractive summarization using different architectures: mT5 and a fine-tuned version of BERT (Bert2Bert).",
            "experiment_process": "Baseline models were trained using the HunSum-2 dataset. The mT5 model, a multilingual T5 model, and the Hungarian BERT model (huBERT) were experimented with. The huBERT model was fine-tuned as an encoder-decoder architecture (Bert2Bert). Both models used a maximum input length of 512 tokens, and training was conducted on a single NVIDIA A100 GPU with early stopping based on validation loss. The mT5 model halted training at 8.14 epochs, while the Bert2Bert model stopped at 3.8 epochs.",
            "result_discussion": "The mT5 model stopped learning at 8.14 epochs, indicating that it required more iterations to converge. On the other hand, the Bert2Bert model stopped at 3.8 epochs, suggesting that it converged faster on the same dataset. The comparative performance metrics were not mentioned, but early stopping based on validation loss suggests that both models were optimized for quality based on the provided data.",
            "ablation_id": "2404.03555v2.No1"
        },
        {
            "research_objective": "The objective of the ablation study was to develop and evaluate a baseline model for extractive summarization for Hungarian texts using the HunSum-2 dataset.",
            "experiment_process": "Extractive summarization models were developed by transforming raw data into sentence-level binary labels. Sentence transformers were used to calculate the embedding of lead and article sentences. For each lead sentence, the closest article sentence was selected by maximizing the sum of similarities using cosine distance. The paraphrase-multilingual-MiniLM-L12-v2 model computed the sentence embeddings. The BertSum architecture utilizing huBERT with a simple classifier layer was chosen for the baseline model. The model was trained for 21,000 steps with a batch size of 200 and a learning rate of 5e-5. Evaluation was conducted every 1000 steps, with training stopping if the evaluation loss did not decrease in 10 evaluation steps. The model used four NVIDIA A100 GPUs for training.",
            "result_discussion": "The model was trained for 21,000 steps and evaluated every 1000 steps, which allowed for fine-grained monitoring of its performance. Training stopped when the evaluation loss did not decrease over 10 evaluations, ensuring that the model did not overfit and achieved optimal performance on the validation set. Details on specific performance metrics were not provided, but adherence to early stopping criteria indicates a focus on preventing overfitting and ensuring model robustness.",
            "ablation_id": "2404.03555v2.No2"
        }
    ]
}