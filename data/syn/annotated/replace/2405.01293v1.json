{
    "title": "Low-resource speech recognition and dialect identification of Irish in a multi-task framework",
    "abstract": "This paper explores the use of Hybrid CTC/Attention encoder-decoder models trained with Intermediate CTC (InterCTC) for Irish (Gaelic) low-resource speech recognition (ASR) and dialect identification (DID). Results are compared to the current best performing models trained for ASR (TDNN-HMM) and DID (ECAPA-TDNN). An optimal InterCTC setting is initially established using a Conformer encoder. This setting is then used to train a model with an E-branchformer encoder and the performance of both architectures are compared. A multi-task fine-tuning approach is adopted for language model (LM) shallow fusion. The experiments yielded an improvement in DID accuracy of 10.8% relative to a baseline ECAPA-TDNN, and WER performance approaching the TDNN-HMM model. This multi-task approach emerges as a promising strategy for Irish low-resource ASR and DID.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Irish (Gaelic) is a highly inflected, minority language indigenous to the island of Ireland. A difficulty in developing speech technology for the language is the high degree of dialect variation across different speaker communities. There are three major native dialects of Irish, namely Ulster (Ul), Connaught (Co) and Munster (Mu), along with many sub-varieties. The L1 Irish speaking communities are found in limited remote areas called Gaeltachta\u00ed. However, there is a considerable number of L2 speakers as Irish is also taught as a compulsory subject in primary and secondary schools. This paper focuses on the native dialects and the data used is overwhelmingly of L1 speakers.\nIrish, being a low-resource language, poses a tough challenge for automatic speech recognition (ASR). Limited data makes it hard to create accurate and adaptable ASR models for Irish, impacting their effectiveness. Additionally, the fact that there is no single standard spoken variety of Irish adds to the complexity: it is therefore crucial to identify these dialects to optimise the ASR process. As part of the ABAIR initiative, which is developing speech technologies for the Irish language, a TDNN-HMM ASR system \u00c9ist [1  ###reference_b1###] has been developed and is available for public use111www.abair.ie.\nPrevious work established that Irish dialect bias in ASR could not be adequately mitigated through corpus balancing alone [2  ###reference_b2###]. Spoken dialect identification (DID) of Irish was explored in [3  ###reference_b3###], where an ECAPA-TDNN model achieved a classification accuracy of 73%, surpassing the performance of wav2vec 2.0 XLS-R [4  ###reference_b4###] after finetuning. Further to this result, the output logits of a text-based classifier were fused with the acoustic classifier which improved classification accuracy to 76%, demonstrating the importance of dialect features for classification in both the acoustic and the text domain. This paper aims to explore innovative approaches that could improve both ASR and DID performance in a multi-task setting.\nMulti-task training is a promising strategy that uses shared knowledge across related tasks. It has been shown to be an effective method for language identification in multilingual ASR systems [5  ###reference_b5###]. Recognizing its potential to enhance model adaptability, this paper explores Intermediate CTC, which has been used for multilingual speech recognition [6  ###reference_b6###], for multi-task Irish speech recognition and dialect identification."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background",
            "text": "Spoken language identification (LID) is the process of automatically identifying the language of a speaker from speech. Classical i-vectors [7 ###reference_b7###, 8 ###reference_b8###] were the state-of-the-art for LID before the introduction of acoustic embeddings extracted from DNNs, namely x-vectors [9 ###reference_b9###]. Phonetically-aware acoustic features from acoustic models pretrained for ASR have been shown to outperform classical acoustic features for LID. Phonetic features extracted from a phone-discriminative model, initially trained as an ASR acoustic model, were explored in [10 ###reference_b10###], and led to an improvement over conventional acoustic features. Similarly, in [11 ###reference_b11###], ASR-based phone posteriorgram features were used for accent identification of English, and improved performance over filter-bank features. In [12 ###reference_b12###] a Conformer model was initially trained for ASR with CTC, and subsequently used as a feature extractor, which yielded comparable and in some cases better acoustic features than classical multilingual bottleneck features for LID, without the need for phone-alignment information to train the feature extractor. The winners of the 2021 Oriental Language Recognition challenge [13 ###reference_b13###] pretrained an encoder-decoder U2++ Conformer model with ASR, before finetuning the encoder component for LID, demonstrating that conditioning an LID model to be phonetically-aware is a strong strategy [14 ###reference_b14###].\n[15 ###reference_b15###] explored jointly training an RNN-T model for language identification and ASR. It was found that the best jointly trained model in their study surpassed the performance of monolingual ASR by 6.4\u20139.2% WER, and surpassed the LID system with a reduction in error rate of 53.9\u201356.1%.\nHowever, identifying the dialects within a single language is likely to be a more difficult task than the identification of completely separate languages. [16 ###reference_b16###] investigates many configurations of combining ASR and DID, but most notably, their Japanese multi-dialect ASR system, which models ASR and DID in a multi-task set-up, outperforms the baseline system. Extending the vocabulary of a grapheme-based multi-dialect E2E ASR model for English to include a label for the dialect of a speaker is explored in [17 ###reference_b17###], and this multi-task approach was found to outperform models trained on single dialects.\nIntermediate CTC (InterCTC) was originally proposed as a regularisation technique for deep encoders by incorporating the CTC loss of the intermediate layers as part of a multi-task objective [18 ###reference_b18###, 19 ###reference_b19###]. It has been proven to be effective for the joint modelling of ASR and a speech classification task, as in [6 ###reference_b6###] for multilingual speech recognition and language identification, achieving state-of-the-art results. Similarly, this method also achieved state-of-the-art Aphasia speech recognition and detection performance in [20 ###reference_b20###].\nIn this paper, we explore the use of a hybrid CTC/Attention-based encoder-decoder model for Irish dialect identification and speech recognition. We explore the usefulness of InterCTC for both of these tasks by introducing an auxiliary task to predict dialect. The assignment of the auxiliary task to different encoder layers is varied systematically to find the optimum setting. Improvements to the encoder architecture are investigated, and shallow fusion with a transformer language model (LM) fine-tuned for multi-task DID and ASR shallow fusion is tested."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Data",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Acoustic data",
            "text": "The data used in these experiments has either been recorded by the ABAIR project or drawn from external sources and are summarised in Table 1. The recordings made in-house are divided into two corpora: Oxford English Dictionary and Synthesis. The Oxford English Dictionary data collection platform was created with speech recognition development in mind. Dialect-appropriate textual prompts were selected or crafted for recording. The dataset of 45.7h and 369 speakers includes crowdsourced recordings where participants recorded utterances using their own devices in varied recording environments as well as live recordings where the authors had control over the microphones used and the acoustic environment. The meta-data pertaining to the linguistic background of speakers is also collected, enabling selection of L1 vs L2 speakers. The Synthesis corpus of 25.6h is comprised of recordings of 5 L1 speakers used to create the ABAIR synthetic voices. The audiobooks used are from different sources: recordings made at home by two Mu Irish speakers reading the books Mo Sg\u00e9al F\u00e9in and An tOile\u00e1nach were used. A collection of stories published by Cois Life as well as a collection of short stories from \u00c9abhl\u00f3id were used as well. A spontaneous speech corpus of broadcast material named Corpas na Cainte Beo provided by Foras na Gaeilge\u2019s New English-Irish Dictionary project, which is tagged with dialect information, is also used. This corpus was provided to ABAIR without audio-text alignment. The alignment procedure employed is detailed in Section 3.1.1."
        },
        {
            "section_id": "3.1.1",
            "parent_section_id": "3.1",
            "section_name": "3.1.1 Alignment of Spontaneous Speech Corpus",
            "text": "The alignment of the spontaneous speech corpus was done in two stages, firstly using CTC-Segmentation as described in [21  ###reference_b21###] using the ESPnet toolkit [22  ###reference_b22###]. CTC-Segmentation utilizes CTC log-posteriors to determine utterance timings in the audio given a ground-truth text. Initially, a forward pass is executed, wherein transition probabilities are mapped into a trellis diagram of the ground-truth token sequence across all time steps. The algorithm backtracks from the most probable timing of the last token and finds the most probable path through the trellis diagram. A confidence score is computed for each utterance, based on per-token probabilities within the trellis. A Conformer encoder [23  ###reference_b23###] which uses XLS-R 300M as the frontend with the CTC objective is trained on all available Irish data and used for alignment. CTC-segmentation requires inference over the entire audio sequence of a file, which when using Transformer-based encoders with quadratic memory-based complexity, can be an issue for longer files. Following the implementation in [24  ###reference_b24###], files above 600s are partitioned into smaller segments of audio. Inference is calculated on these segments and the CTC-posteriors of the segments are subsequently concatenated. As splitting audio abruptly can lead to distortions, an overlap of 1s is used and these overlap posteriors are later discarded for scoring.\nThe resulting alignment contained speech segments longer than the conventional maximum of 20s for ASR. To split these files into segments 20s in length, Kaldi-based scripts are used. A biased LM is created according to the transcripts of the input data and together with a TDNN-HMM acoustic model, the data is split into reasonable chunks222kaldi/egs/wsj/s5/steps/cleanup/segment_long_utterances_nnet3.sh. An additional step was taken to ensure the resulting splits match their transcripts by removing bad portions or to make minor modifications to allow for disfluencies or repetitions333kaldi/egs/wsj/s5/steps/cleanup/clean_and_segment_data_nnet3.sh. Implementation details for both of these scripts can be found in [25  ###reference_b25###]. 200h out of a total 320h were successfully aligned using this process."
        },
        {
            "section_id": "3.1.2",
            "parent_section_id": "3.1",
            "section_name": "3.1.2 Construction of train, validation and test sets",
            "text": "The train set contains 290h in total, and the validation and test sets contain 1.7h and 3.5h respectively. These were constructed such that there was no overlap of speakers or utterance texts between the sets. As this paper is focusing on ASR and dialect identification of L1 speakers of Irish, the Oxford English Dictionary corpus is the most appropriate set to use to construct the validation and test sets. As noted, specific meta-data relating to the linguistic background of each speaker was collected and therefore, including only L1 speakers of Irish in the test sets can be done easily. Also, the authors had control over the text prompts used when recording and could ensure their dialect appropriateness. One issue with this corpus which pertains to Ulster speakers particularly, is that only a limited set of 3000 recording prompts were used for Ul recordings. Therefore, there is less variability of utterance texts in the Ul portion of this corpus, compared to Co and Mu. Avoiding overlap of texts would necessitate a large portion of Ul data to be discarded from any set. To avoid this, a portion 0.2h of Ul data from the Audiobook collection was added to the test set. Details of the validation and test sets are presented in Table 2 ###reference_###."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Text corpora for shallow-fusion experiment",
            "text": "The transformer language model used in the shallow-fusion experiment is trained in two stages. Firstly, the model is trained with text-only data, and then it is fine-tuned with corpora containing dialect information. In the first stage, two corpora were used: i) Paracrawl, which is the Irish part of the ga-en pairs from ParaCrawl v7 [26  ###reference_b26###]; and ii) ConLL17, the Irish data from the CoNLL 2017 Shared Task on Universal Dependancy Parsing [27  ###reference_b27###]. As these corpora were scraped from the web, they contain many symbols and characters from different languages. There is only a limited amount of textual data available on the web in the Irish language and therefore, the two corpora contain some overlap. To overcome these issues, an aggressive multi-stage cleaning process was employed: i) a conventional text-cleaning script for Irish was used, which transliterates certain characters and removes unnecessary punctuation symbols. After this, any sentence that contained symbols outside of the limited character set of Irish was removed. The corpus was then sorted and uniqued on a sentence level to ensure that overlap between the two corpora is resolved. This resulted in a corpus of 58m words, a reduction of almost 50% of the combined size of the Paracrawl and CoNLL 2017 and is used to pretrain the transformer LM used for shallow fusion.\nThe second collection of texts that were used include dialect information at the sentence level, which is generally not available in Irish corpora. One useful source that is tagged with dialect is the Historical Irish Corpus of the Royal Irish Academy. This corpus consists of texts written between 1900-50, before the introduction of the standard written form, so morphosyntactic and lexical markers of dialect are salient in these texts (for more details, see [3  ###reference_b3###]). Alongside this corpus, the transcripts from the entire Spontaneous Speech Corpus were used, as well as the transcripts from the training set. See for Table 3  ###reference_### for information regarding these corpora."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Method",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Hybrid CTC/Attention",
            "text": "Hybrid CTC/Attention-based encoder-decoder models [28  ###reference_b28###] utilize two principal techniques in ASR, namely Connectionist Temporal Classification CTC [29  ###reference_b29###] and the attention mechanism. The process can be summarized as follows:\nThe encoder, denoted as a function , transforms an acoustic sequence  into a series of embeddings , where  represents the length of the input sequence, and  is the length of the encoded embeddings.\nThese embeddings  can optionally be converted into text using CTC, generating a preliminary textual output .\nThe decoder, utilizing an auto-regressive approach and denoted by , predicts the text output  given the acoustic embeddings .\nDuring training, the model is optimized using the weighted sum of the CTC loss and the decoder loss, with a weight set to 0.3 . The final hypothesis is generated during inference by jointly decoding the output of the encoder and decoder using beam search. This hybrid approach effectively combines the robust alignment capabilities of CTC with the contextual sensitivity of the attention mechanism to improve ASR performance."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Intermediate CTC",
            "text": "Intermediate CTC (InterCTC) [18  ###reference_b18###] was introduced as a regularization technique for deep encoder networks and to facilitate multi-task learning [6  ###reference_b6###, 20  ###reference_b20###]. A CTC module is applied to the output of an intermediate encoder layer with index . Self-conditioned CTC is also applied, where subsequent encoder layers incorporate these intermediate predictions into their input. The rearranged Equation 1 is expressed as follows:\nwhere\n represents the encoder layers, and  is the latent sequence of the InterCTC target sequence . The functions NRM( ) and LIN( ) correspond to a normalization layer and a linear layer, respectively. The InterCTC loss is the negative log likelihood of generating :\nThe selection of  depends on the task. During training, the intermediate layer is optimized to accurately predict  by integrating  into the loss function:\nHere, the InterCTC weight  serves as a hyper-parameter. The updated overall loss function is derived by inserting Equation 9  ###reference_### into Equation 5:\nIt is noteworthy that CTC can be applied to multiple encoder layers with different target sequences for each. Where the target sequence differs from the decoder output, this is considered as an auxiliary task. The average of all InterCTC losses is used as ."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Dialect identification as an auxiliary task",
            "text": "Using the above model structure, dialect is captured by a combination of two methods. Firstly, it is captured explicitly in the text output sequence, by prepending a dialect tag to the text to be predicted:\nThe vocabulary of the ASR model is extended to include dialect tags in the following way:\nThis approach jointly models the ASR task and the speech classification task effectively.\nThe second method to capture dialect is to use a DID InterCTC objective, where the ground truth of an utterance is the dialect tag of the speaker. During inference, the dialect of an utterance is predicted using InterCTC greedy search."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Model structure",
            "text": "The model training in these experiments adopts the hybrid CTC/Attention-based encoder-decoder framework. Two encoder architectures, namely the Conformer and the E-branchformer [30  ###reference_b30###], are investigated. These encoders were proposed as improvements to the transformer encoder architecture, which captures global acoustic contextual information using the attention mechanism. The Conformer in addition to utilizing convolutional layers to capture local contexts, which is particularly important for speech, also makes use of the attention mechanism to capture global information. On the other hand, the E-branchformer has superseded the Conformer encoder as the state-of-the-art encoder, by capturing the local and global contexts in parallel branches before merging the outputs. For the front-end module, a self-supervised learning model (SSL) was chosen, namely the multilingual XLS-R 300M [4  ###reference_b4###], which is trained on 436k hours of data from 128 languages. Despite the absence of Irish in this multilingual training set, this model was deemed to be more appropriate for Irish speech recognition and the dialect disambiguation than other acoustic SSL models typically trained monolingually on English data. All models trained use a transformer decoder."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": ""
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Setup",
            "text": "In these experiments, the hybrid CTC/Attention-based encoder-decoder structure is explored for jointly modelling Irish speech recognition and dialect identification, using the ESPnet toolkit [22  ###reference_b22###]. See Section 4.3 for details on how dialect is incorporated. Accuracy is used to report DID performance, as the test set is quite balanced with respect to the number of utterances per dialect. Throughout all experiments, the XLS-R 300M was used as a front-end module for acoustic feature extraction. All encoder-decoder models are trained with a transformer decoder with 6 blocks, each having 2048 hidden units and 4 attention heads. Three different encoders are tested in these experiments, a Conformer encoder and a small and large E-branchformer encoder. The Conformer encoder has 12 blocks, each having 2048 hidden units and 4 attention heads, and Conformer-based models trained here have 113m trainable parameters. The small E-branchformer model has 45.8m trainable parameters, with 12 blocks, each with 1048 linear units, an output size of 256 and 4 attention heads. The larger model has 130m trainable parameters and uses 2048 linear units and an output size of 256, while the remaining parameters are kept the same. The cgMLP module of the E-branchformer encoders has 3072 units and the convolution kernel size is 31. Speed perturbation with warping factors of 0.9, 1.0 and 1.1 as well as Spectral Augment are used to augment the training data.\nA modular TDNN-HMM ASR model is trained with speed perturbation and spectral augment as a comparison for ASR performance and an ECAPA-TDNN model is trained with speed perturbation and added noise and reverberation as a comparison for DID performance."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "ASR baseline models",
            "text": "Despite the advance of End-to-End ASR models, the hybrid TDNN-HMM is still a robust model for low-resource languages and was still the best performing model for Irish [31  ###reference_b31###]. A TDNN-HMM model was trained as the baseline to compare with the CTC/Attention encoder-decoder architecture, following the same set-up as [31  ###reference_b31###]. A 4-gram LM is trained using both the training and fine-tuning text corpora listed in Section 3.2  ###reference_### and Table 3  ###reference_###.\nTo evaluate both the effectiveness of InterCTC on DID and ASR performance and the impact of multi-task learning on ASR performance, three Conformer-based models are trained and compared with the hybrid TDNN-HMM baseline. The first Conformer-based model, which will be referred to as Conformer (ASR) and is the second row in Table 4  ###reference_###, is trained for ASR only with an ASR InterCTC objective in layers 3, 6 and 9. This model is designed to compare the capability of the model architecture on the ASR task with TDNN-HMM on the ASR task. The second Conformer model, which will be referred to as Conformer (multi-task) and is the third row in Table 4  ###reference_###, is trained for multi-task ASR and DID with InterCTC DID and ASR objectives defined for encoder layers 3, 6 and 9.\nThe third model Conformer (no InterCTC), see the fourth row in Table 4  ###reference_###, is trained for multi-task ASR and DID without InterCTC.\nTable 4  ###reference_### presents the ASR performance of these models. The TDNN-HMM baseline outperforms the best performing Conformer model, specifically Conformer (ASR), by 16.5% relative WER. Conformer (multi-task) performs worse than Conformer (ASR) by 13% relative, suggesting that jointly modelling DID and ASR leads to ASR performance degradation. Conformer (no InterCTC) performs less well, showing that the addition of InterCTC leads to WER improvements."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "DID baseline models",
            "text": "Previous work [3  ###reference_b3###] demonstrates that the ECAPA-TDNN performs well for Irish DID, outperforming wav2vec 2.0, hence an ECAPA-TDNN model was trained using SpeechBrain [32  ###reference_b32###] following the same setup as [3  ###reference_b3###], serving as the baseline model for the DID task. The embedding model is initialised from an ECAPA-TDNN trained for language identification using the VoxLingua107 corpus [33  ###reference_b33###]. The model is trained using the Additive Angular Margin loss [34  ###reference_b34###] and speed perturbation, adding noise and reverberation are applied as data augmentation.\nIn Table 5  ###reference_###, the DID performance of the ECAPA-TDNN is compared with a Conformer (multi-task) and a Conformer (no InterCTC). Both Conformer models outperform the ECAPA-TDNN model by a wide margin. Interestingly, InterCTC with the multi-task objective assigned to layers 3, 6 and 9 does not lead to a gain in DID accuracy compared to the model trained without InterCTC."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Experiment 1",
            "text": "In this experiment, assigning InterCTC objectives to layers of the Conformer encoder is explored. The assignment of the Multi-task and DID InterCTC objectives, detailed in Section 4.3, to the encoder layers is varied systematically. An additional model trained to perform both ASR and DID without InterCTC is included here for comparison.\nTable 6  ###reference_### shows the results of the InterCTC configurations for DID accuracy and WER. Row 1 shows the performance of baseline Conformer (no InterCTC), and row 7 shows the performance for baseline Conformer (multi-task). As mentioned before, the Conformer (no InterCTC) model achieved a DID accuracy of 79.7%. However, in comparison with the models trained with InterCTC, its ASR performance is relatively poor at 18.9% WER, suggesting that InterCTC is most helpful for ASR performance as opposed to the speech classification task of DID. Surprisingly, the models trained with DID InterCTC objectives only, as in rows 2-4, performed worse in DID accuracy than the baseline on row 7, which was trained with multi-task CTC objectives. However, the ASR performance of the these models (rows 2-4) varies considerably: row 2 trained with DID InterCTC objective in layers 3, 6 and 9 garnered the best WER (16.6%) in this experiment; row 4, where the only InterCTC objective was assigned to layer 3, achieved the worst WER in the experiment (19.4%). The best performing system (row 6) is trained with the DID objective in layer 3 and the multi-task objective in layers 6 and 9. This model achieved the highest DID accuracy in this experiment, a boost of 10.8% relative to the ECAPA-TDNN baseline. The same model also achieved the second lowest WER among the models trained for both ASR and DID, with 16.7% WER. This model is chosen as the best configuration of InterCTC objectives for joint DID and ASR modelling for Experiments 2 and 3."
        },
        {
            "section_id": "5.5",
            "parent_section_id": "5",
            "section_name": "Experiment 2",
            "text": "The E-branchformer architecture using the best InterCTC configuration established in Experiment 1 is explored to compare the performance of the Conformer and the E-branchformer encoder. Two models are trained using the E-branchformer encoder - a smaller version (45.8m trainable parameters) and a larger version (130m trainable parameters). For further details see Section 5.1  ###reference_###.\nThe best configuration (row 6 in Table 6  ###reference_###) of Experiment 1 is selected to compare the performance of Conformer and E-branchformer encoders in this experiment. Results are presented in Table 7  ###reference_###. E-branchformer Small is the best performing model in terms of DID, with an accuracy of 81.4%, an improvement of 1.1% absolute. The model\u2019s WER performance however is worse, suggesting that the smaller number of trainable parameters more strongly affects WER performance than it does DID performance. E-branchformer Large performs slightly worse than the Conformer model in terms of DID, but it has a relative WER performance gain of 6%."
        },
        {
            "section_id": "5.6",
            "parent_section_id": "5",
            "section_name": "Experiment 3",
            "text": "This experiment explores the usefulness of multi-task shallow fusion. A transformer language model was initially trained with a larger corpus of Irish text and fine-tuned on a smaller dialect-tagged corpus of Irish text for multi-task DID and ASR shallow fusion. For details of the corpora used, see Table 3  ###reference_###. A grid-search found 0.3 to be the optimal weight for shallow fusion.\nThe results for LM shallow fusion can also be seen in Table 7  ###reference_###. A slight improvement in DID classification accuracy of 0.8% was gained through shallow fusion, but its most significant contribution is, unsurprisingly, a reduction of WER by 2.2%."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Discussion and Conclusions",
            "text": "The hybrid CTC/Attention-based encoder-decoder architecture emerges as a good strategy for Irish multi-task ASR and DID. The best results were obtained from the E-branchformer Large model with the optimal InterCTC setting (established in Experiment 1) and multi-task LM shallow fusion, where the LM was fine-tuned with dialect labels prepended to the text. That the inclusion of LM shallow fusion improves DID accuracy slightly is an encouraging finding. Perhaps with more available dialect-tagged text corpora, such an approach could garner further improvement. The present DID results are promising in that they surpass the previous best model trained (ECAPA-TDNN). The best DID accuracy obtained here is 81.5% compared to 73% obtained in [3  ###reference_b3###]. The overall contribution of InterCTC to DID accuracy is slight, as can be seen by comparing row 1 in Table 6  ###reference_### with the other rows. Only in one case (row 6) does the inclusion of InterCTC yield a DID improvement. This differs from what was found in [6  ###reference_b6###] and [20  ###reference_b20###], where InterCTC improved the speech classification accuracy more considerably. As regards WER, training for multi-task DID and ASR does lead to some performance degradation. However, when the InterCTC objectives are optimally set, the WER degradation is greatly lessened. While the ASR performance of these E2E models is not better than the modular TDNN-HMM model, the gap between the two approaches is reduced a great deal and this approach promises to be a fruitful avenue for continued exploration."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Acknowledgements",
            "text": "This work is part of the ABAIR initiative, which is supported by the Department of Tourism, Culture, Arts, the Gaeltacht, Sport and Media, with funding from the National Lottery, as part of the 20-year Strategy for Irish. The authors gratefully acknowledge the speakers and publishers that have contributed to this work."
        }
    ],
    "url": "http://arxiv.org/html/2405.01293v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ],
        "main_experiment_and_results_sections": [
            "5.1",
            "5.2",
            "5.3",
            "5.4",
            "5.5",
            "5.6"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "5.4",
            "5.5",
            "5.6"
        ]
    },
    "research_context": {
        "paper_id": "2405.01293v1",
        "paper_title": "Low-resource speech recognition and dialect identification of Irish in a multi-task framework",
        "research_background": "The paper \"Low-resource speech recognition and dialect identification of Irish in a multi-task framework\" is motivated by the challenges associated with developing speech technology for the Irish language, a highly inflected and low-resource minority language spoken in Ireland. The high degree of dialectal variation across different native speaker communities\u2014specifically Ulster, Connaught, and Munster dialects\u2014compounds the difficulty of creating accurate and adaptable automatic speech recognition (ASR) systems. Moreover, the lack of a standard spoken variety heightens the necessity for dialect identification to optimize the ASR process.\n\nThe primary research problem addressed in this paper revolves around improving the performance of ASR systems for Irish and enhancing dialect identification (DID) in a multi-task learning framework. This includes tackling the insufficiently mitigated dialect bias and limited data resources that negatively affect the adaptability and accuracy of existing ASR models like the TDNN-HMM ASR system \u00c9ist, developed under the ABAIR initiative.\n\nThe paper builds upon relevant prior work including:\n1. The development and public deployment of the \u00c9ist ASR system for Irish by the ABAIR initiative.\n2. Findings that dialect bias in Irish ASR cannot be sufficiently mitigated through corpus balancing alone [2].\n3. Previous research on spoken dialect identification for Irish, where an ECAPA-TDNN model showed superior classification accuracy compared to wav2vec 2.0 XLS-R [3, 4]. This research also highlighted the benefits of combining acoustic and text-based classifiers.\n4. The potential of multi-task training, particularly Intermediate CTC, to improve language identification and model adaptability in multilingual ASR systems [5, 6].\n\nThe current study aims to leverage these insights and innovative multi-task training approaches to advance both ASR and DID for the Irish language.",
        "methodology": "The methodology adopted in the paper focuses on a hybrid CTC/Attention-based encoder-decoder model for low-resource speech recognition and dialect identification of Irish. The key components and innovations of the proposed method can be described as follows:\n\n1. **Model Structure**:\n   - The model integrates two principal techniques in Automatic Speech Recognition (ASR): Connectionist Temporal Classification (CTC) and the attention mechanism.\n   - The encoder, denoted by a function, is responsible for transforming an acoustic sequence into a series of embeddings. Here, represents the length of the input sequence, and denotes the length of the encoded embeddings.\n\n2. **Encoder Operation**:\n   - The encoder processes the acoustic sequence to produce embeddings, subsequently allowing these embeddings to be converted into text using CTC. This step generates an initial textual output.\n\n3. **Decoder Operation**:\n   - The decoder functions in an auto-regressive manner, leveraging the embeddings to predict the final text output, denoted as . This process uses the contextual sequences provided by the encoder embeddings.\n\n4. **Loss Optimization**:\n   - During training, the model is optimized using a combination of losses: the CTC loss and the decoder loss. A weighted sum is employed with the CTC loss weighted at 0.3.\n   \n5. **Inference Process**:\n   - The final output hypothesis is generated during the inference phase by jointly decoding the outputs of both the encoder and decoder using beam search.\n   - This hybrid approach merges the robust alignment capabilities inherent in the CTC method with the attention mechanism's ability to understand context, enhancing the performance of ASR tasks.\n\nBy combining these elements, the proposed method leverages the strengths of both CTC and attention mechanisms, allowing for improved performance in recognizing speech and identifying dialects, even within low-resource languages like Irish.",
        "main_experiment_and_results": "### Main Experiment Setup\n\nThe main experiment assesses the hybrid Connectionist Temporal Classification (CTC) and Attention-based encoder-decoder system for simultaneous Irish speech recognition and dialect identification. The ESPnet toolkit is utilized for this setup.\n\n**Datasets and Preprocessing:**\n- **Test Set:** Designed to be balanced with respect to the number of utterances per dialect.\n- **Data Augmentation:** Speed perturbation with warping factors of 0.9, 1.0, and 1.1, and Spectral Augment techniques are employed to enhance the training data.\n\n**Acoustic Feature Extraction:**\n- **Front-end Module:** XLS-R 300M is used for extracting acoustic features.\n\n**Model Components:**\n- **Decoder:** Transformer decoder with 6 blocks, each possessing 2048 hidden units and 4 attention heads.\n\n**Baselines:**\n- For Automatic Speech Recognition (ASR): A modular TDNN-HMM ASR model using speed perturbation and spectral augment.\n- For Dialect Identification (DID): An ECAPA-TDNN model with speed perturbation and added noise and reverberation.\n\n**Evaluation Metrics:**\n- **ASR Performance:** Metrics not explicitly mentioned, but typically measured using Word Error Rate (WER) or similar.\n- **DID Performance:** Accuracy, given the balanced nature of the test set.\n\n### Main Experimental Results:\n\nThe experimental results are primarily distinguished by the performance of the various encoder-decoder configurations in both the ASR and DID tasks. However, specific quantitative results (e.g., accuracy percentages, WER) are not provided in the setup description. The outcomes demonstrate the model's capability to handle both tasks simultaneously with a particular focus on the efficiency and trainable parameter sizes of different encoder configurations. The results likely illustrate the trade-offs between model complexity (parameter count) and performance on the dual tasks.\n\nNote: For a detailed understanding of specific numerical results or comparative performance figures, one would typically refer to the results section of the research paper, which isn't included in this summary."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Exploring the assignment of InterCTC objectives to layers of the Conformer encoder to improve ASR and DID performance.",
            "experiment_process": "The experiment varied the assignment of Multi-task and DID InterCTC objectives to the encoder layers systematically. An additional model trained to perform both ASR and DID without InterCTC was included for comparison. The performance in terms of DID accuracy and WER was detailed in Table 6, which showed various configurations of InterCTC objectives applied to different layers (3, 6, and 9).",
            "result_discussion": "The Conformer without InterCTC achieved DID accuracy of 79.7% but had poor ASR performance at 18.9% WER. Models trained with DID InterCTC objectives only performed worse in DID accuracy but exhibited a range in ASR performance (best WER of 16.6%). The best performing system, trained with DID objective in layer 3 and multi-task objectives in layers 6 and 9, achieved the highest DID accuracy and second lowest WER (16.7%), making it the best configuration for joint DID and ASR modeling for subsequent experiments.",
            "ablation_id": "2405.01293v1.No1"
        },
        {
            "research_objective": "Comparing the performance of Conformer and E-branchformer encoders using the best InterCTC configuration established in Experiment 1.",
            "experiment_process": "Two models were trained using the E-branchformer encoder: a smaller version (45.8m trainable parameters) and a larger version (130m trainable parameters). The best configuration from Experiment 1 (DID objective in layer 3 and multi-task objectives in layers 6 and 9) was used for comparison. Results were presented in Table 7.",
            "result_discussion": "The E-branchformer Small model achieved the best DID accuracy (81.4%), an improvement of 1.1% absolute, but had worse WER performance. The E-branchformer Large model performed slightly worse than the Conformer in DID accuracy but showed a relative WER performance gain of 6%.",
            "ablation_id": "2405.01293v1.No2"
        },
        {
            "research_objective": "Exploring the usefulness of multi-task shallow fusion for improving DID and ASR performance.",
            "experiment_process": "A transformer language model was initially trained with a larger corpus of Irish text and fine-tuned on a smaller dialect-tagged corpus for multi-task DID and ASR shallow fusion. A grid-search determined 0.3 to be the optimal weight for shallow fusion. The results were compared in Table 7.",
            "result_discussion": "Shallow fusion resulted in a slight improvement in DID classification accuracy (0.8%) but significantly reduced WER by 2.2%.",
            "ablation_id": "2405.01293v1.No3"
        }
    ]
}