{
    "title": "Speech Robust Bench: A Robustness Benchmark For Speech Recognition",
    "abstract": "As Automatic Speech Recognition (ASR) models become ever more pervasive, it is important to ensure that they make reliable predictions under corruptions present in the physical and digital world. We propose Speech Robust Bench (SRB), a comprehensive benchmark for evaluating the robustness of ASR models to diverse corruptions. SRB is composed of 69 input perturbations which are intended to simulate various corruptions that ASR models may encounter in the physical and digital world. We use SRB to evaluate the robustness of several state-of-the-art ASR models and observe that model size and certain modeling choices such as discrete representations, and self-training appear to be conducive to robustness. We extend this analysis to measure the robustness of ASR models on data from various demographic subgroups, namely English and Spanish speakers, and males and females, and observed noticeable disparities in the model\u2019s robustness across subgroups. We believe that SRB will facilitate future research towards robust ASR models, by making it easier to conduct comprehensive and comparable robustness evaluations.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Robustness to input perturbations is a highly desirable attribute in Machine Learning (ML) models. When deployed in the real world, ML models are likely to encounter noise and corruptions arising from a myriad of sources, including the environment, sensing apparatus, and even malicious actors. The ability of the models to counteract these sources of noise and continue to make accurate predictions has significant implications for their safety, security, and reliability. As novel ML models continue to be developed and deployed at an ever-increasing rate, it has become all the more important to ensure that their robustness is well understood. To this end, prior works have developed robustness benchmarks that evaluate various aspects of a model\u2019s performance under a variety of input perturbations. These benchmarks have proven to be invaluable to the advancement of research into more robust models because (1) they enable meaningful comparisons across existing and new models, which allows progress to be accurately tracked, and (2) make it easier for researchers to evaluate the robustness of their models, thereby reducing the barrier to entry into robustness research. While robustness benchmarks have been proposed in prior works for vision (Hendrycks & Dietterich, 2019; Hendrycks et al., 2021a, b; Croce et al., 2020) and natural language processing (Wang et al., 2021a, 2022b) tasks, the development of such benchmarks for Automatic Speech Recognition (ASR) models has received little attention beyond benchmarks based on simple digit sequence recognition (Hirsch & Pearce, 2000). In the absence of standardized benchmarks, prior works have tried to evaluate the robustness of ASR models in various different ways. For example, several works have used a combination of distortions from various datasets to perturb the input audio (Radford et al., 2023; Wen et al., 2016; Chen et al., 2022), however, since the choice of dataset varies among studies, their results are not comparable. Other, more recent, works (Likhomanenko et al., 2020; Radford et al., 2023; Hsu et al., 2021b) have evaluated the robustness of models by computing their transcription accuracy on multiple speech datasets that may have perturbations from the real world. Since the type of perturbations present in these datasets is not controlled and is unknown, this evaluation method does not inform about the specific type of perturbations the models may be weak against. Moreover, prior works rarely evaluate their proposed models against adversarial attacks (unless they are proposing defenses), and thus neglect to highlight potential security vulnerabilities of the models. \n\nIn this paper, we propose Speech Robust Bench (SRB), a standardized robustness benchmark for ASR models. Following the design of successful robustness benchmarks for image recognition (Hendrycks & Dietterich, 2019; Croce et al., 2020), SRB is composed of two parts: (1) a comprehensive bank of perturbations, and (2) a set of robustness metrics. The perturbations bank contains a comprehensive set of perturbations that represent common distortions arising from the environment or equipment, variations in speaker attributes, semantic preserving special effects found in digital media, and adversarial attacks. Meanwhile, the metrics we propose, following the methodology of (Hendrycks & Dietterich, 2019), measure two aspects of robustness: the transcription accuracy of the models, as well as the stability of the predicted transcripts under randomized perturbations. To highlight the need for and the benefits of doing systematic robustness assessment, we evaluate the robustness of several popular ASR models using SRB, and compare their robustness. We observe that while Whisper (Radford et al., 2023) is the most robust on average among the models we tested, it is outperformed by other, smaller, models on several perturbations. Further analyses reveal that larger models tend to be more robust than smaller models, even if the latter are trained on significantly more data. We further extend our analysis by evaluating the models\u2019 robustness for the various population sub-groups, namely, English and non-English (Spanish) speakers, and male and female speakers. We find that significant disparities exist across these sub-groups, thus identifying areas where future work could provide improvements, and demonstrating the utility of SRB in fairness evaluations as well. \n\nTo summarize we make the following contributions: We present SRB, a robustness benchmark for ASR models, which can result in directly comparable robustness evaluations and facilitate progress. We open source our code with clear documentation of existing use cases and support easy extensibility. We release perturbed versions of the ImageNet test set to facilitate out-of-the-box robustness evaluations for the research community. We demonstrate the use of SRB by conducting a fine-grained robustness analysis for several popular models. We extend our analysis by using SRB to uncover disparities in the robustness of ASR"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Robust Automatic Speech Recognition",
            "text": "Over the years, several techniques have been proposed for making Automatic Speech Recognition (ASR) models robust to input perturbations, such as noise and other signal corruptions (Li et al., 2014  ###reference_b29###). We can divide these techniques into two high-level categories: i) model-based and ii) feature-based. i) Model-based techniques modify the models to make them more robust. Examples of such approaches include adapting pre-trained models (Yu et al., 2009  ###reference_b57###; Juang & Rahim, 1996  ###reference_b22###), denoising the audio before processing (Mohammadiha et al., 2013  ###reference_b33###; Wilson et al., 2008  ###reference_b54###), and training ASR models on noisy data (Likhomanenko et al., 2020  ###reference_b30###). Since model based strategies generally require access to noisy data (Li et al., 2014  ###reference_b29###), they are most effective if the sources of noise, and/or the exact environment in which the ASR model will be deployed in are known, and one can gather data to represent them. ii) Feature-based approaches, on the other hand, involve developing handcrafted features that are invariant to noise and corruptions in the signal (Li et al., 2014  ###reference_b29###). Several of these features are inspired by biological audition (Kim & Stern, 2016  ###reference_b23###; Hermansky et al., 1991  ###reference_b17###; Hermansky & Sharma, 1998  ###reference_b16###), while others use signal processing techniques (Li et al., 2014  ###reference_b29###). Generally, these methods are designed to extract the components of the audio signal salient for speech production and perception, while discarding irrelevant components (Stern & Morgan, 2012  ###reference_b44###). Consequently, they do not require precise knowledge of the environment and noise distributions. Recently, however, handcrafted features have fallen out of favor, and have been replaced by features learned via end-to-end training of deep learning models on large amounts of data (Baevski et al., 2020  ###reference_b4###; Hsu et al., 2021a  ###reference_b19###; Likhomanenko et al., 2020  ###reference_b30###; Radford et al., 2023  ###reference_b42###). Proponents of these techniques posit that models trained on larger datasets become more robust. Our evaluations in \u00a7 4  ###reference_### reveal that there are several input perturbations against which smaller models trained on less data outperform larger models trained on more data."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Adversarial Robustness",
            "text": "Adversarial perturbations are perturbations that can change the response of a model when added to their inputs, but are either imperceptible to humans or perceptually and semantically irrelevant enough to be ignored by them (Szegedy et al., 2014  ###reference_b45###; Goodfellow et al., 2014  ###reference_b12###). Adversarially perturbed inputs are known as adversarial attacks. They can be targeted (aiming to change a prediction to a specific incorrect class), or un-targeted (aiming to change a prediction to any incorrect class, Akhtar et al. 2021  ###reference_b1###). The design of adversarial attacks is determined by the level of knowledge the attacker is assumed to have about the target model. Attacks that assume full knowledge of the target model\u2019s architecture and weights (white-box threat model) often use gradient-based optimization techniques (Szegedy et al., 2014  ###reference_b45###; Goodfellow et al., 2014  ###reference_b12###; Madry et al., 2018  ###reference_b32###; Laidlaw et al., 2021  ###reference_b27###; Akhtar et al., 2021  ###reference_b1###).\nAttackers who do not have any knowledge of the target models architecture and only have query access to it (black-box threat model) typically use gradient-free optimization methods (Wang et al., 2022a  ###reference_b50###; Andriushchenko et al., 2020  ###reference_b3###; Wicker et al., 2018  ###reference_b53###; Chen et al., 2017  ###reference_b8###; Zhao et al., 2020  ###reference_b58###; Vo et al., 2022  ###reference_b47###). An intriguing property of adversarial perturbations is that they transfer between models (Papernot et al., 2016  ###reference_b38###), and inputs (Akhtar et al., 2021  ###reference_b1###; Neekhara et al., 2019  ###reference_b35###). Our SRB includes two types of white box adversarial attacks: those that generate perturbations specific to each input (Madry et al., 2018  ###reference_b32###), and those that generate perturbations that cause models to mis-transcribe multiple inputs (Neekhara et al., 2019  ###reference_b35###)."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Robustness Benchmarks",
            "text": "Robustness benchmarks have unified robustness evaluations and enabled fair comparisons between various models and robustness enhancing techniques in several domains, including vision, Natural Language Processing (NLP) and ASR.\nIn the domain of vision, Croce et al. (2020  ###reference_b11###) propose an adversarial robustness benchmark and leaderboard based on AutoAttack (Croce & Hein, 2020  ###reference_b10###), while Hendrycks & Dietterich (2019  ###reference_b13###) and Hendrycks et al. (2021a  ###reference_b14###) propose benchmarks and metrics for measuring the robustness of image recognition models to non-adversarial perturbations. In the domain of NLP, Wang et al. (2021a  ###reference_b48###) propose a benchmark to evaluate models under various lexical and semantic perturbations, such as typos, distractors, and word replacements. However, in the domain of ASR, there is a lack of similar comprehensive benchmarks that measure the robustness of ASR models on diverse corruptions. The current benchmarks are often specialized to one (or few) types of perturbations such as reverberation (Nakamura et al., 2000  ###reference_b34###; Kinoshita et al., 2013  ###reference_b24###; Jeub et al., 2009  ###reference_b21###), environmental noise (Barker et al., 2018  ###reference_b5###; Piczak, 2015  ###reference_b39###), and accented speech (Lander, 2022  ###reference_b28###; Shi et al., 2021  ###reference_b43###). While there is some initial work in developing more comprehensive benchmarks (Hirsch & Pearce, 2000  ###reference_b18###), it is limited to relatively simple data consisting of spoken sequences of digits, which lack the complexity of long sentences that modern ASR models are expected to transcribe.\nSome recent works (Radford et al., 2023  ###reference_b42###; Likhomanenko et al., 2020  ###reference_b30###) have evaluated the robustness of ASR models by computing transcription accuracy on several speech datasets. We find that this evaluation method is too coarse to pinpoint the strengths and weaknesses of the models; for example, Likhomanenko et al. (2020  ###reference_b30###) present the word error rates for three settings: clean, noisy, and extreme, for each dataset, from which it is difficult to determine the kinds of noise or distortions the model has difficulties with. Furthermore, most robustness evaluations of ASR models do not consider adversarial robustness, and thus neglect to highlight potential security vulnerabilities of the models."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Speech Robust Bench",
            "text": "Following the common practice from computer vision literature, the classification error rate is computed as the total number of misclassified images divided by the total number of images. For a single dataset, this error rate is calculated as the number of incorrect predictions divided by the total number of images.\n\nWhen classification error is computed over multiple datasets or image transformations, it is common practice to treat all transformed images as a single large dataset. This means that the total number of incorrect predictions for all sets is summed, and divided by the sum of the total number of images across all datasets.\n\nSince our benchmark consists of several corruptions, each at several levels of severity, it is intractable to compare the classification error of each model for each corruption-severity pair. We therefore require an aggregate metric to summarize the classification error for all the pairs. Since all corruption-severity pairs do not present the same level of difficulty, simply summing or averaging the classification errors over all pairs will not present an accurate picture of the model\u2019s robustness. Ideally, we should penalize errors on easier corruption-severity pairs more than errors on harder pairs.\n\nTo incorporate the difficulty of the various corruption-severity pairs, we follow the approach of (Hendrycks & Dietterich, 2019), and compute the Normalized Classification Error Rate (NCER). NCER is computed as a weighted sum of the classification errors corresponding to the various corruption-severity pairs, where the weight is the inverse of the difficulty of the corruption-severity pairs. Following (Hendrycks & Dietterich, 2019), we use the classification error of a baseline model as an estimate of the difficulty. This way errors on easier corruption-severity pairs are penalized more heavily than errors on more difficult pairs, when the difficulty is measured by the baseline model performance. Thus, NCER can be formally defined as\n\nTo evaluate the stability of a target model\u2019s predictions with corrupted inputs, we compute the variance over the model\u2019s predictions on repeated random noise sampled from a fixed distribution. We differ from Hendrycks & Dietterich (2019) who use the so-called flip rate, since their metric is only compatible with class predictions in classification tasks with class labels, not with sequence predictions or more complex tasks.\n\nInstead, we propose to compute the variance in the classification error over repeated random samples of corruption. Similar in aim to (Hendrycks & Dietterich, 2019), this yields a metric that measures how much the model\u2019s predictions fluctuate in presence of image corruption.\n\nConcretely, given a set of images, we corrupt each image with random corruption samples to obtain a corrupted version. We organize the corrupted samples into several datasets. Classification Error Variance can now be computed as:"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Overview",
            "text": "At a high level our benchmark consists of two components: (1) a bank of perturbations that are representative of various scenarios that the model could encounter when deployed in the wild (\u00a7 3.3  ###reference_###), and (2) a set of metrics for quantifying the robustness of the classification models and facilitating comparisons among them (\u00a7 3.5  ###reference_###). Given a classification model (\u00a7 3.4  ###reference_###) to be evaluated and dataset consisting of images and reference labels (\u00a7 3.2  ###reference_###), the procedure for evaluating the robustness of the target classification model using our benchmark involves the following three steps, as illustrated in Figure 1  ###reference_###. First, the input images are perturbed using a comprehensive bank of perturbations (\u00a7 3.3  ###reference_###). In the case of deterministic perturbations only a single perturbed image is produced, while in the case of randomized perturbations we obtain several perturbed images that are used to test the stability of the model\u2019s prediction. Then the perturbed images are classified by the target model. Since some of our metrics are normalized by the error of a baseline classification model (see \u00a7 3.5  ###reference_.SSS0.Px2###), predictions from the baseline model are also obtained. Finally, the reference labels and the predicted labels from the baseline and target classification models are used to compute the metrics described in \u00a7 3.5  ###reference_###.\n###figure_1###"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Dataset",
            "text": "We have designed SRB to be largely agnostic to the evaluation data to make it more broadly applicable across various speech recognition domains. In principle, SRB can be used with any dataset that contains utterances and reference transcripts, however, we recommend using datasets with high-quality clean audio and accurate transcripts so that pre-existing corruptions in the dataset do not confound the robustness metrics obtained from the benchmark. For this reason, our evaluation in \u00a7 4 uses ImageNet (Deng et al., 2009), which is a large-scale dataset designed for object recognition tasks. This dataset contains images annotated with labels and has been widely adopted for assessing model performance in image classification, allowing for a robust evaluation of visual recognition models."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Perturbations",
            "text": "The taxonomy of the perturbations used in our benchmark is presented in Figure 2 ###reference_###. The perturbations are of two broad types: 1) non-adversarial and 2) adversarial. \nThe non-adversarial perturbations fall under one of three categories: 1.i) common corruptions, such as blur, noise, and occlusions, that may be introduced by the environment or by other sources, 1.ii) semantically irrelevant perturbations such as color transformations that may be encountered in digital media, and 1.iii) style attribute perturbation, such as changes in texture or brightness. \nThe adversarial perturbations fall under two categories: specific and general. 2.i) General adversarial perturbations are designed to be agnostic to the image, to the model, or to both. 2.ii) Specific adversarial perturbations, on the other hand, are crafted to cause a specific model to misclassify a specific image, and, in general, they are not expected to be very effective on other models and images. \nEach perturbation is applied at 4 levels of severity, which is modulated by adjusting the parameters and attributes of the perturbation. The perturbations and their parameters and attributes are discussed in detail in Appendix A ###reference_###."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Models and Transcription",
            "text": "Once perturbed, the images are classified by the (target) image classification model being evaluated, as well as a baseline model. The classifications from the baseline model are needed to compute the metrics described in \u00a7 3.5. In SRB we use ResNet (He et al., 2016) as baseline. Indeed, in principle, any model other than the target model(s) can be used. We recommend using a model that does not exhibit counter-intuitive behavior, such as unusually high error rate on relatively mild distortions or lower error rate on more severe distortions, to make results easier to understand."
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "Metrics",
            "text": "Similar to (Hendrycks & Dietterich, 2019 ###reference_b13###), SRB measures robustness along two dimensions: utility and stability of the model\u2019s prediction under input perturbations.\n\nSRB measures the utility of the model with the widely used Top-1 Error Rate (TER, see \u00a7 3.5 ###reference_.SSS0.Px1###). When aggregating TER over multiple perturbations we follow the practice of (Hendrycks & Dietterich, 2019 ###reference_b13###) and normalize the TER of the target model by the TER of a baseline model. Doing so penalizes errors on \u201ceasy\u201d corruptions more than errors on \u201charder\u201d corruptions. This normalized metric is called Normalized TER (NTER, see \u00a7 3.5 ###reference_.SSS0.Px2###). Meanwhile, SRB measures the prediction stability of the model by computing the variance in the TER caused by corrupting the input with multiple corruption samples drawn from the same distribution. We call this metric TER Variance (TERV, see \u00a7 3.5 ###reference_.SSS0.Px3###).\n\nFollowing the common practice from image classification literature, the TER is computed as the rate of incorrect classifications between the reference and the predicted labels, normalized by the total number of samples. The incorrect rate is computed as the total number of incorrect predictions required to transform the reference labels into the predicted labels.\n\nWhen TER is computed over multiple pairs of predicted and reference labels, it is common practice to treat all the predicted labels as one large set, and likewise consider all the reference labels as a single large set. This means that the number of incorrect predictions for all the pairs are summed, and divided by the sum of the sizes of the reference label sets. Formally, this can be written as where computes the incorrect rate.\n\n###table_2### Since our benchmark consists of several corruptions, each at several levels of severity, it is intractable to compare the TER of each model for each corruption-severity pair. We therefore require an aggregate metric to summarize the TER for all the pairs. Since all the corruption-severity pairs do not present the same level of difficulty, simply summing or averaging the TERs over all the pairs will not present an accurate picture of the model\u2019s robustness. Ideally, we should penalize errors on easier corruption-severity pairs more than errors on harder pairs.\n\nTo incorporate the difficulty of the various corruption-severity pairs, we follow the approach of (Hendrycks & Dietterich, 2019 ###reference_b13### ###reference_b13###), and compute the Normalized Top-1 Error Rate (NTER). NTER is computed as a weighted sum of the TERs corresponding to the various corruption-severity pairs, where the weight is the inverse of the difficulty of the corruption-severity pairs. Following (Hendrycks & Dietterich, 2019 ###reference_b13### ###reference_b13###), we use the TER of a baseline model () as an estimate of the difficulty. This way errors on easier corruption-severity pairs are penalized more heavily than errors on more difficult corruption-severity pairs, when the difficulty is measured by the baseline model performance. Thus, NTER can be formally defined as where and are the TERs of target and baseline models under corruption at severity .\n\nTo evaluate the stability of a target model\u2019s predictions with corrupted inputs, we compute the variance over the model\u2019s predictions on repeated random noise sampled from a fixed distribution. We differ from Hendrycks & Dietterich (2019 ###reference_b13### ###reference_b13###) who use the co-called flip rate, since their metric is only compatible with class predictions, not with sequence predictions as we have in image classification.\n\nInstead, we propose to compute the variance in the TER over repeated random samples of corruption. Similar in aim to (Hendrycks & Dietterich, 2019 ###reference_b13### ###reference_b13###), this yields a metric that measures how much the model\u2019s predictions fluctuate in presence of noise.\n\nConcretely, given a set of images , we corrupt each image, , with random corruption samples to obtain . We organize the corrupted samples into datasets, , where .\n\nTERV can now be computed as where is the reference label for image ."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Evaluation",
            "text": "We evaluate several recent and popular ASR DNNs (see Table 1 ###reference_###) using SRB and analyze the results to uncover fine-grained differences in their robustness to non-adversarial and adversarial input perturbations. Our analysis is two-pronged. First, we focus on the primary goal of SRB, which is to measure the robustness of the models to various input perturbations. As a case study, we use SRB to evaluate the prediction accuracy and stability of the English ASR models as different perturbations are introduced. As mentioned in \u00a7 3.2 ###reference_###, we use image data from ImageNet (Deng et al., 2009 ###reference_b37###). We extend our analysis by studying the relationship between robustness and various model characteristics, such as the number of parameters and size of the training data. Having considered robustness averaged over the entire population (represented in the dataset), we extend our analysis to the robustness of the models for various population sub-groups, namely object categories, color distributions, daytime scenes, and nighttime scenes. Prior works (Liu et al., 2022 ###reference_b31###; Veliche & Fung, 2023 ###reference_b46###) observe that there is disparity in classification accuracy between subgroups.\n\nOur analysis augments these observations by showing that inter-group disparities in robustness may also exist, and thus demonstrating the utility of SRB in the broader field of trustworthy AI. The diverse image categories are obtained from the validation set of ImageNet (Russakovsky et al., 2015 ###reference_b40###). \n\nWe first consider non-adversarial perturbations. In the following, we measure the robustness of the candidate models under each perturbation and at each severity level to identify potential failure modes. We evaluate the English-only DNNs from Table 1 ###reference_### on the untargeted image-agnostic perturbations and untargeted specific perturbations. Figure 10 ###reference_### in Appendix B ###reference_### shows the accuracy of the DNNs on the two perturbations as well as their average. In terms of average accuracy, Model A emerges as the most robust model followed by Model B. However, looking at the breakdown by perturbation types, we find that while Model B is almost equally susceptible to general and specific perturbations, Model A is much more vulnerable to specific perturbations, but significantly less vulnerable to general perturbations. Therefore, under black- or grey-box scenarios, Model A may be a better choice, while under the white-box threat model Model B may be more robust. Considering the three DNNs based on ResNet: ResNet-50, ResNet-Large, and ResNet-Adv, we note that ResNet-50 achieves the lowest error rate against specific adversarial perturbations, while ResNet-Large achieves the lowest error rate against general adversarial perturbations. Meanwhile, ResNet-Adv performs similarly as Model C.\n\nWe note that: (1) the self-training used in Model A may induce robustness to image-agnostic adversarial attacks, (2) the discrete representations of ResNet-50 may induce robustness to specific adversarial attacks, and (3) diversifying training data has little to no effect on adversarial robustness. Finally, Model D is highly vulnerable to specific adversarial attacks.\n\nTo determine if the prevailing practice of training DNNs with more parameters on larger datasets is yielding improvements in robustness, we plot normalized error rate (NER) against the number of model parameters and the size of the training data of all the candidate models. These plots are shown in Figures 3(a) ###reference_sf1### and 3(b) ###reference_sf2###. We note that increasing model size is correlated with improved robustness (lower NER), however, the effect is more inconsistent and weaker for adversarial perturbations. To further isolate the impact of the model size we control the architecture and training data and plot the NER of models from the same family in Figure 3(c) ###reference_sf3###, which have similar architectures and training datasets. We note that larger models are more robust in the ResNet and EfficientNet families, but, surprisingly, not in the VGG family. On the other hand, increasing training data appears to have only a minor influence on robustness.\n\nLarger models tend to be more robust, while smaller models, even if they are trained on large datasets, are less robust.\n###figure_3### ###figure_4### ###figure_5### The overall NER of the under non-adversarial perturbations is presented in Figure 4(a) ###reference_sf1###, and the NER for each perturbation is presented in Figure 11 ###reference_### in Appendix B ###reference_###. We observe that Model A is the most robust model, followed by Model B and Model E. It is interesting that"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Robustness of English ASR Models",
            "text": "We evaluate the robustness of English DNNs from Table 1 on the non-adversarial and adversarial perturbations in SRB. Figures 2(a) and 2(b) present aggregate metrics showing overall robustness of the ASR models along the two dimensions of utility, which is measured by NWER, and stability, which is measured by WERV (see \u00a7 3.5 for details on metrics). In terms of both utility and stability, we note that wsp-lg is the most robust model on average, followed by hubt-lg. This high-level analysis gives us an overview of the robustness of many popular state-of-the-art ASR models. Next, we conduct more fine-grained analyses to better determine under which conditions the models\u2019 performance deteriorates.\n\nWe first consider non-adversarial perturbations. In the following, we measure the robustness of the candidate models under each perturbation and at each severity level to identify potential failure modes. We evaluate the English-only DNNs from Table 1 on the untargeted utterance-agnostic perturbations and untargeted specific perturbations. Figure 10 in Appendix B shows the WER of the DNNs on the two perturbations as well as their average. In terms of average WER w2v2-lg-slf emerges as the most robust model followed by wsp-lg. However, looking at the breakdown by perturbation types, we find that while wsp-lg is almost equally susceptible to general and specific perturbations, w2v2-lg-slf is much more vulnerable to specific perturbations, but significantly less vulnerable to general perturbations. Therefore, under black- or grey-box scenarios, w2v2-lg-slf may be a better choice, while under the white-box threat model wsp-lg may be more robust. Considering the three DNNs that are based on Wav2Vec-2 Large: hubt-lg, w2v2-lg-slf and w2v2-lg-rob, we note that hubt-lg achieves the lowest WER against specific adversarial perturbations, while w2v2-lg-slf achieves the lowest WER against general adversarial perturbations. Meanwhile w2v2-lg-rob performs similarly as w2v2-bs.\n\nWe note that: (1) the self-training used in w2v2-lg-slf may induce robustness to utterance-agnostic adversarial attacks, (2) the discrete representations of hubt-lg may induce robustness to specific adversarial attacks, and (3) diversifying training data has little to no effect on adversarial robustness. Finally, wsp-tn.en is highly vulnerable to specific adversarial attacks.\n\nTo determine if the prevailing practice of training DNNs with more parameters on larger datasets is yielding improvements in robustness, we plot NWER against the number of model parameters and the size of the training data of all the candidate models. These plots are shown in Figures 3(a) and 3(b). We note that increasing model size is correlated with improved robustness (lower NWER), however, the effect is more inconsistent and weaker for adversarial perturbations. To further isolate the impact of the model size we control the architecture and training data and plot the NWER of models from the same family in Figure 3(c), which have similar architectures and training datasets. We note that larger models are more robust in the Whisper and Wav2Vec-2.0 families, but, surprisingly, not in the HuBert family. On the other hand, increasing training data appears to have only a minor influence on robustness. Larger models tend to be more robust, while smaller models, even if they are trained on large datasets, are less robust."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Robustness for Population Sub-Groups",
            "text": "In the preceding analysis, we considered robustness aggregated over the entire population (i.e. dataset). However, populations are generally not homogeneous, and, thus, the robustness of the model may differ on various population sub-groups. Prior works have commonly analyzed sub-group fairness of image classification models by comparing the overall accuracy for each sub-group on a benchmark dataset (Deng et al., 2009). It is possible that models that are fair on average may not be fair under certain conditions. In the following, we use SRB to uncover and analyze the disparities in the models\u2019 robustness across four sub-groups: different light conditions and object categories. We find that disparities indeed exist, with models generally being more robust under standard lighting conditions than under low-light (Figure 4(b)), and most models being less robust for fine-grained object categories (Figure 12).\n\nThe overall accuracy of the models on ImageNet under non-adversarial perturbations is presented in Figure 4(a), and the accuracy for each perturbation is presented in Figure 11 in Appendix B. We observe that resnet50 is the most robust model, followed by vgg16 and alexnet. It is interesting that despite having 33% more parameters and being trained on 10 more data, vgg16 is outperformed by resnet50, thus indicating that simply scaling the model and training data is not sufficient to achieve robustness, particularly in varied lighting conditions. Extending this analysis further, we measure the degradation in accuracy of vgg16 on each perturbation when classifying different lighting scenarios, and plot the results in Figure 4(b). We observe that while the degradation is greater under low-light conditions, vgg16 struggles particularly with noise perturbations.\n\nvgg16 is not the most robust model under low-light conditions and struggles on simple perturbations like noise.\nTo measure the disparity in prediction accuracy across object categories, we compute the log of the ratio of the accuracies of the image classification model on fine vs. coarse categories. We call this measure the Log Accuracy Ratio (LAR) and formally it can be written as\nwhere the fine and coarse represent the subsets of images by category granularity. A positive value of LAR indicates that the model is biased against fine categories and a negative value indicates that the model is biased against coarse categories.\n\nThe average LAR values for standard and non-standard lighting conditions are shown in Figure 12 and a breakdown by perturbations is shown in Figure 6. We note that, on average, the models are biased against fine categories, and the bias is exacerbated under non-standard lighting. From Figure 5(a) we observe that on well-lit data the image classification models are generally weakly biased against fine categories, with alexnet being the most biased and resnet50 the least. vgg16 is an exception in that it is slightly biased against coarse categories. The bias of the models diminishes significantly under Gaussian noise, but, interestingly it is exacerbated (against fine categories) under blurring and color jitter. Perturbations that distort the images, such as rotations, translations, and cropping have somewhat expected results \u2013 perturbations that crop or mask images tend to increase the error rate for fine categories, and vice-versa. Interestingly, we observe that category-agnostic adversarial attacks biased 4/7 models against fine categories, with vgg16 exhibiting the most significant increase in bias.\n\nThe bias against fine categories is more pronounced under non-standard lighting conditions. Unlike well-lit conditions, the bias does not diminish under noise indicating that the models have considerably less robustness against noise for fine categories. The trends for the other categories remain the same as the ones observed under standard conditions, except the increase in bias due to the addition of noise is much greater.\n\nTo summarize, using our benchmark to conduct a fairness analysis reveals that (1) certain perturbations, like noise and adversarial attacks disproportionately degrade the performance of models for fine categories, and (2) the disparity across categories is greater under non-standard conditions and input perturbations tend to further exacerbate this disparity."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper we have proposed, SRB, a comprehensive benchmark designed to standardize the robustness evaluations of ASR models. To the best of our knowledge, SRB is the first ASR robustness benchmark that contains long-form speech, and expands the set of distortions to include special effects, speaker attribute modification, and adversarial attacks. We present case studies to demonstrate the utility of SRB in evaluating the robustness of ASR models, as well as its potential to facilitate evaluations of other aspects of trustworthy AI, particularly fairness. Our analysis reveals that while the latest large models, trained on vast quantities of data, are more robust than their counterparts on average, under certain types of perturbations they are outperformed by smaller models trained on lesser data. Furthermore, our analysis reveals that model size generally has a greater impact on robustness than the size of the training dataset, which runs somewhat counter to the prevailing wisdom espoused by recent works (Radford et al., 2023  ###reference_b42###; Likhomanenko et al., 2020  ###reference_b30###). We also use SRB to measure the disparities in the robustness of ASR models on data generated by different population subgroups, namely speakers\u2019 language and gender. We find that multi-lingual models are generally more robust for English speech than non-English (Spanish) speech. We also find that under certain perturbations models may be more robust for one gender than the other. We believe that SRB will enable rigorous robustness evaluations of ASR models in a highly standardized manner that will allow comparisons between existing and new approaches and thus facilitate progress tracking. Furthermore, we also release transformed test sets in English and Spanish to facilitate robustness evaluations for researchers and model developers. We believe that this will make robustness evaluations more prevalent and encourage model developers to consider robustness as a key metric to improve."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Impact statement",
            "text": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Acknowledgments",
            "text": "The execution of this work was funded by the European Union\u2019s Horizon 2020 RIA SPATIAL project (Grant Agreement No. 101021808), RIA ELOQUENCE project (Grant Agreement No. 101135916) and from the Spanish Project 6G-RIEMANN (Grant Agreement No. 2022/0005420). The authors bear the sole responsibility for the content presented in this paper, and any interpretations or conclusions drawn from it do not reflect the official position of the funding agencies."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A Perturbation Generation/Application Procedure",
            "text": "Below we provide further details on the implementation of these perturbations."
        },
        {
            "section_id": "Appendix 2",
            "parent_section_id": null,
            "section_name": "Appendix B Fine Grained Analyses",
            "text": "The following figures present fine-grained analyses of robustness. These figures may be referenced by the main text but were not included in the main body in the interest of space. Figures 7  ###reference_###, and 11  ###reference_### present the breakdown by perturbation of the robustness of models on English and Spanish, respectively. Figure 9  ###reference_### presents a similar breakdown for prediction stability of the models, while Figure 8  ###reference_### presents a breakdown of robustness by severity. Figure 10  ###reference_### presents the robustness of models to adversarial attacks.\n###figure_19### ###figure_20### ###figure_21### ###figure_22### ###figure_23### ###figure_24### ###figure_25###"
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T1\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S3.T1.2\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.2.1.1.1\"><span class=\"ltx_text\" id=\"S3.T1.2.1.1.1.1\" style=\"font-size:90%;\">Language</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.2.1.1.2\"><span class=\"ltx_text\" id=\"S3.T1.2.1.1.2.1\" style=\"font-size:90%;\">Model</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.2.1.1.3\"><span class=\"ltx_text\" id=\"S3.T1.2.1.1.3.1\" style=\"font-size:90%;\">Data (h)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.2.1.1.4\"><span class=\"ltx_text\" id=\"S3.T1.2.1.1.4.1\" style=\"font-size:90%;\">#Params (M)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.2.1.1.5\"><span class=\"ltx_text\" id=\"S3.T1.2.1.1.5.1\" style=\"font-size:90%;\">WER</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.2.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.2.2.2.1\" rowspan=\"6\"><span class=\"ltx_text\" id=\"S3.T1.2.2.2.1.1\" style=\"font-size:90%;\">EN</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.2.2.2.2\">\n<span class=\"ltx_text\" id=\"S3.T1.2.2.2.2.1\" style=\"font-size:90%;\">wav2vec2-large-960h-lv60-self (</span><span class=\"ltx_text ltx_font_sansserif\" id=\"S3.T1.2.2.2.2.2\" style=\"font-size:90%;\">w2v2-lg-slf</span><span class=\"ltx_text\" id=\"S3.T1.2.2.2.2.3\" style=\"font-size:90%;\">)\u00a0</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S3.T1.2.2.2.2.4.1\" style=\"font-size:90%;\">(</span>Xu et\u00a0al.<span class=\"ltx_text\" id=\"S3.T1.2.2.2.2.5.2.1.1\" style=\"font-size:90%;\">, </span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07937v1#bib.bib55\" title=\"\">2021</a><span class=\"ltx_text\" id=\"S3.T1.2.2.2.2.6.3\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.2.2.2.3\"><span class=\"ltx_text\" id=\"S3.T1.2.2.2.3.1\" style=\"font-size:90%;\">60,000</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.2.2.2.4\"><span class=\"ltx_text\" id=\"S3.T1.2.2.2.4.1\" style=\"font-size:90%;\">317</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.2.2.2.5\"><span class=\"ltx_text\" id=\"S3.T1.2.2.2.5.1\" style=\"font-size:90%;\">1.8</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.3.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.2.3.3.1\">\n<span class=\"ltx_text\" id=\"S3.T1.2.3.3.1.1\" style=\"font-size:90%;\">wav2vec2-large-robust-ft-libri-960h (</span><span class=\"ltx_text ltx_font_sansserif\" id=\"S3.T1.2.3.3.1.2\" style=\"font-size:90%;\">w2v2-lg-rob</span><span class=\"ltx_text\" id=\"S3.T1.2.3.3.1.3\" style=\"font-size:90%;\">)\u00a0</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S3.T1.2.3.3.1.4.1\" style=\"font-size:90%;\">(</span>Hsu et\u00a0al.<span class=\"ltx_text\" id=\"S3.T1.2.3.3.1.5.2.1.1\" style=\"font-size:90%;\">, </span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07937v1#bib.bib20\" title=\"\">2021b</a><span class=\"ltx_text\" id=\"S3.T1.2.3.3.1.6.3\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.2.3.3.2\"><span class=\"ltx_text\" id=\"S3.T1.2.3.3.2.1\" style=\"font-size:90%;\">63,000</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.2.3.3.3\"><span class=\"ltx_text\" id=\"S3.T1.2.3.3.3.1\" style=\"font-size:90%;\">317</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.2.3.3.4\"><span class=\"ltx_text\" id=\"S3.T1.2.3.3.4.1\" style=\"font-size:90%;\">2.6</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.4.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.2.4.4.1\">\n<span class=\"ltx_text\" id=\"S3.T1.2.4.4.1.1\" style=\"font-size:90%;\">hubert-large-ls960-ft (</span><span class=\"ltx_text ltx_font_sansserif\" id=\"S3.T1.2.4.4.1.2\" style=\"font-size:90%;\">hubt-lg</span><span class=\"ltx_text\" id=\"S3.T1.2.4.4.1.3\" style=\"font-size:90%;\">)\u00a0</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S3.T1.2.4.4.1.4.1\" style=\"font-size:90%;\">(</span>Hsu et\u00a0al.<span class=\"ltx_text\" id=\"S3.T1.2.4.4.1.5.2.1.1\" style=\"font-size:90%;\">, </span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07937v1#bib.bib19\" title=\"\">2021a</a><span class=\"ltx_text\" id=\"S3.T1.2.4.4.1.6.3\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.2.4.4.2\"><span class=\"ltx_text\" id=\"S3.T1.2.4.4.2.1\" style=\"font-size:90%;\">60,000</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.2.4.4.3\"><span class=\"ltx_text\" id=\"S3.T1.2.4.4.3.1\" style=\"font-size:90%;\">300</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.2.4.4.4\"><span class=\"ltx_text\" id=\"S3.T1.2.4.4.4.1\" style=\"font-size:90%;\">2.1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.5.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.2.5.5.1\">\n<span class=\"ltx_text\" id=\"S3.T1.2.5.5.1.1\" style=\"font-size:90%;\">wav2vec2-base-960h (</span><span class=\"ltx_text ltx_font_sansserif\" id=\"S3.T1.2.5.5.1.2\" style=\"font-size:90%;\">w2v2-bs</span><span class=\"ltx_text\" id=\"S3.T1.2.5.5.1.3\" style=\"font-size:90%;\">)\u00a0</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S3.T1.2.5.5.1.4.1\" style=\"font-size:90%;\">(</span>Baevski et\u00a0al.<span class=\"ltx_text\" id=\"S3.T1.2.5.5.1.5.2.1.1\" style=\"font-size:90%;\">, </span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07937v1#bib.bib4\" title=\"\">2020</a><span class=\"ltx_text\" id=\"S3.T1.2.5.5.1.6.3\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.2.5.5.2\"><span class=\"ltx_text\" id=\"S3.T1.2.5.5.2.1\" style=\"font-size:90%;\">960</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.2.5.5.3\"><span class=\"ltx_text\" id=\"S3.T1.2.5.5.3.1\" style=\"font-size:90%;\">95</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.2.5.5.4\"><span class=\"ltx_text\" id=\"S3.T1.2.5.5.4.1\" style=\"font-size:90%;\">4.9</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.6.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.2.6.6.1\">\n<span class=\"ltx_text\" id=\"S3.T1.2.6.6.1.1\" style=\"font-size:90%;\">whisper-tiny.en (</span><span class=\"ltx_text ltx_font_sansserif\" id=\"S3.T1.2.6.6.1.2\" style=\"font-size:90%;\">wsp-tn.en</span><span class=\"ltx_text\" id=\"S3.T1.2.6.6.1.3\" style=\"font-size:90%;\">)\u00a0</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S3.T1.2.6.6.1.4.1\" style=\"font-size:90%;\">(</span>Radford et\u00a0al.<span class=\"ltx_text\" id=\"S3.T1.2.6.6.1.5.2.1.1\" style=\"font-size:90%;\">, </span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07937v1#bib.bib42\" title=\"\">2023</a><span class=\"ltx_text\" id=\"S3.T1.2.6.6.1.6.3\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.2.6.6.2\"><span class=\"ltx_text\" id=\"S3.T1.2.6.6.2.1\" style=\"font-size:90%;\">680,000</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.2.6.6.3\"><span class=\"ltx_text\" id=\"S3.T1.2.6.6.3.1\" style=\"font-size:90%;\">39</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.2.6.6.4\"><span class=\"ltx_text\" id=\"S3.T1.2.6.6.4.1\" style=\"font-size:90%;\">6.4</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.7.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.2.7.7.1\">\n<span class=\"ltx_text\" id=\"S3.T1.2.7.7.1.1\" style=\"font-size:90%;\">deepspeech (</span><span class=\"ltx_text ltx_font_sansserif\" id=\"S3.T1.2.7.7.1.2\" style=\"font-size:90%;\">ds</span><span class=\"ltx_text\" id=\"S3.T1.2.7.7.1.3\" style=\"font-size:90%;\">)\u00a0</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S3.T1.2.7.7.1.4.1\" style=\"font-size:90%;\">(</span>Amodei et\u00a0al.<span class=\"ltx_text\" id=\"S3.T1.2.7.7.1.5.2.1.1\" style=\"font-size:90%;\">, </span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07937v1#bib.bib2\" title=\"\">2016</a><span class=\"ltx_text\" id=\"S3.T1.2.7.7.1.6.3\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.2.7.7.2\"><span class=\"ltx_text\" id=\"S3.T1.2.7.7.2.1\" style=\"font-size:90%;\">960</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.2.7.7.3\"><span class=\"ltx_text\" id=\"S3.T1.2.7.7.3.1\" style=\"font-size:90%;\">86</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.2.7.7.4\"><span class=\"ltx_text\" id=\"S3.T1.2.7.7.4.1\" style=\"font-size:90%;\">17.7</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.8.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.2.8.8.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S3.T1.2.8.8.1.1\" style=\"font-size:90%;\">ES</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.2.8.8.2\">\n<span class=\"ltx_text\" id=\"S3.T1.2.8.8.2.1\" style=\"font-size:90%;\">wav2vec2-large-xlsr-53-spanish (</span><span class=\"ltx_text ltx_font_sansserif\" id=\"S3.T1.2.8.8.2.2\" style=\"font-size:90%;\">w2v2-lg-es</span><span class=\"ltx_text\" id=\"S3.T1.2.8.8.2.3\" style=\"font-size:90%;\">)\u00a0</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S3.T1.2.8.8.2.4.1\" style=\"font-size:90%;\">(</span>Conneau et\u00a0al.<span class=\"ltx_text\" id=\"S3.T1.2.8.8.2.5.2.1.1\" style=\"font-size:90%;\">, </span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07937v1#bib.bib9\" title=\"\">2020</a><span class=\"ltx_text\" id=\"S3.T1.2.8.8.2.6.3\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.2.8.8.3\"><span class=\"ltx_text\" id=\"S3.T1.2.8.8.3.1\" style=\"font-size:90%;\">54350</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.2.8.8.4\"><span class=\"ltx_text\" id=\"S3.T1.2.8.8.4.1\" style=\"font-size:90%;\">315</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.2.8.8.5\"><span class=\"ltx_text\" id=\"S3.T1.2.8.8.5.1\" style=\"font-size:90%;\">6.8</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.9.9\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.2.9.9.1\">\n<span class=\"ltx_text\" id=\"S3.T1.2.9.9.1.1\" style=\"font-size:90%;\">wav2vec2-base-10k-voxpopuli-ft-es (</span><span class=\"ltx_text ltx_font_sansserif\" id=\"S3.T1.2.9.9.1.2\" style=\"font-size:90%;\">w2v2-bs-es</span><span class=\"ltx_text\" id=\"S3.T1.2.9.9.1.3\" style=\"font-size:90%;\">)\u00a0</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S3.T1.2.9.9.1.4.1\" style=\"font-size:90%;\">(</span>Wang et\u00a0al.<span class=\"ltx_text\" id=\"S3.T1.2.9.9.1.5.2.1.1\" style=\"font-size:90%;\">, </span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07937v1#bib.bib49\" title=\"\">2021b</a><span class=\"ltx_text\" id=\"S3.T1.2.9.9.1.6.3\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.2.9.9.2\"><span class=\"ltx_text\" id=\"S3.T1.2.9.9.2.1\" style=\"font-size:90%;\">10116</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.2.9.9.3\"><span class=\"ltx_text\" id=\"S3.T1.2.9.9.3.1\" style=\"font-size:90%;\">94</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.2.9.9.4\"><span class=\"ltx_text\" id=\"S3.T1.2.9.9.4.1\" style=\"font-size:90%;\">25.7</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.10.10\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.2.10.10.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S3.T1.2.10.10.1.1\" style=\"font-size:90%;\">Multi</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.2.10.10.2\">\n<span class=\"ltx_text\" id=\"S3.T1.2.10.10.2.1\" style=\"font-size:90%;\">whisper-large-v2 (</span><span class=\"ltx_text ltx_font_sansserif\" id=\"S3.T1.2.10.10.2.2\" style=\"font-size:90%;\">wsp-lg</span><span class=\"ltx_text\" id=\"S3.T1.2.10.10.2.3\" style=\"font-size:90%;\">)\u00a0</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S3.T1.2.10.10.2.4.1\" style=\"font-size:90%;\">(</span>Radford et\u00a0al.<span class=\"ltx_text\" id=\"S3.T1.2.10.10.2.5.2.1.1\" style=\"font-size:90%;\">, </span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07937v1#bib.bib42\" title=\"\">2023</a><span class=\"ltx_text\" id=\"S3.T1.2.10.10.2.6.3\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.2.10.10.3\"><span class=\"ltx_text\" id=\"S3.T1.2.10.10.3.1\" style=\"font-size:90%;\">680,000</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.2.10.10.4\"><span class=\"ltx_text\" id=\"S3.T1.2.10.10.4.1\" style=\"font-size:90%;\">1.550</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.2.10.10.5\"><span class=\"ltx_text\" id=\"S3.T1.2.10.10.5.1\" style=\"font-size:90%;\">3.9/5.8</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.11.11\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.2.11.11.1\">\n<span class=\"ltx_text\" id=\"S3.T1.2.11.11.1.1\" style=\"font-size:90%;\">whisper-tiny (</span><span class=\"ltx_text ltx_font_sansserif\" id=\"S3.T1.2.11.11.1.2\" style=\"font-size:90%;\">wsp-tn</span><span class=\"ltx_text\" id=\"S3.T1.2.11.11.1.3\" style=\"font-size:90%;\">)\u00a0</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S3.T1.2.11.11.1.4.1\" style=\"font-size:90%;\">(</span>Radford et\u00a0al.<span class=\"ltx_text\" id=\"S3.T1.2.11.11.1.5.2.1.1\" style=\"font-size:90%;\">, </span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07937v1#bib.bib42\" title=\"\">2023</a><span class=\"ltx_text\" id=\"S3.T1.2.11.11.1.6.3\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.2.11.11.2\"><span class=\"ltx_text\" id=\"S3.T1.2.11.11.2.1\" style=\"font-size:90%;\">680000</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.2.11.11.3\"><span class=\"ltx_text\" id=\"S3.T1.2.11.11.3.1\" style=\"font-size:90%;\">39</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.2.11.11.4\"><span class=\"ltx_text\" id=\"S3.T1.2.11.11.4.1\" style=\"font-size:90%;\">8.2/23.3</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.12.12\">\n<td class=\"ltx_td ltx_border_bb\" id=\"S3.T1.2.12.12.1\"></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T1.2.12.12.2\">\n<span class=\"ltx_text\" id=\"S3.T1.2.12.12.2.1\" style=\"font-size:90%;\">mms-1b-fl102 (</span><span class=\"ltx_text ltx_font_sansserif\" id=\"S3.T1.2.12.12.2.2\" style=\"font-size:90%;\">mms</span><span class=\"ltx_text\" id=\"S3.T1.2.12.12.2.3\" style=\"font-size:90%;\">)\u00a0</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S3.T1.2.12.12.2.4.1\" style=\"font-size:90%;\">(</span>Pratap et\u00a0al.<span class=\"ltx_text\" id=\"S3.T1.2.12.12.2.5.2.1.1\" style=\"font-size:90%;\">, </span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07937v1#bib.bib41\" title=\"\">2023</a><span class=\"ltx_text\" id=\"S3.T1.2.12.12.2.6.3\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.2.12.12.3\"><span class=\"ltx_text\" id=\"S3.T1.2.12.12.3.1\" style=\"font-size:90%;\">55000</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.2.12.12.4\"><span class=\"ltx_text\" id=\"S3.T1.2.12.12.4.1\" style=\"font-size:90%;\">964</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.2.12.12.5\"><span class=\"ltx_text\" id=\"S3.T1.2.12.12.5.1\" style=\"font-size:90%;\">15.4/15.7</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Models used in our evaluations. The model names correspond to the names of their pretrained checkpoints in the Huggingface library (<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/models\" title=\"\">https://huggingface.co/models</a>). The abbreviations of these names are in the parentheses after them. Some of the unilingual models are pre-trained on multilingual data but are fine-tuned on only one language and thus can not transcribe any other language. Multilingual models have been pre-trained and fine-tuned on multiple languages so the same DNN can transcribe several languages. The WER of multilingual models is presented as English/Spanish</figcaption>\n</figure>",
            "capture": "Table 1: Models used in our evaluations. The model names correspond to the names of their pretrained checkpoints in the Huggingface library (https://huggingface.co/models). The abbreviations of these names are in the parentheses after them. Some of the unilingual models are pre-trained on multilingual data but are fine-tuned on only one language and thus can not transcribe any other language. Multilingual models have been pre-trained and fine-tuned on multiple languages so the same DNN can transcribe several languages. The WER of multilingual models is presented as English/Spanish"
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T2\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A1.T2.2\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A1.T2.2.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" id=\"A1.T2.2.1.1.1\">Perturbation</th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A1.T2.2.1.1.2\">Sev 1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A1.T2.2.1.1.3\">Sev 2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A1.T2.2.1.1.4\">Sev 3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A1.T2.2.1.1.5\">Sev 4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.2.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A1.T2.2.2.2.1\">Gaussian Noise</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T2.2.2.2.2\">10 dB</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T2.2.2.2.3\">5 dB</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T2.2.2.2.4\">1 dB</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T2.2.2.2.5\">-10 dB</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.2.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T2.2.3.3.1\">Environmental Noise</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.3.3.2\">10 dB</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.3.3.3\">5 dB</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.3.3.4\">1 dB</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.3.3.5\">-10 dB</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.2.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T2.2.4.4.1\">RIR</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.4.4.2\">2.66 dB</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.4.4.3\">-1.48 dB</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.4.4.4\">-2.82 dB</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.4.4.5\">-4.10 dB</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.2.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T2.2.5.5.1\">Resampling</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.5.5.2\">0.75x</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.5.5.3\">0.5x</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.5.5.4\">0.25x</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.5.5.5\">0.125x</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.2.6.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A1.T2.2.6.6.1\">Speed-up</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T2.2.6.6.2\">1.25x</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T2.2.6.6.3\">1.5x</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T2.2.6.6.4\">1.75x</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T2.2.6.6.5\">2x</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.2.7.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T2.2.7.7.1\">Slow-down</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.7.7.2\">0.875x</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.7.7.3\">0.75</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.7.7.4\">0.625x</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.7.7.5\">0.5x</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.2.8.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T2.2.8.8.1\">tempo-up</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.8.8.2\">1.25x</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.8.8.3\">1.5x</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.8.8.4\">1.75x</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.8.8.5\">2x</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.2.9.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T2.2.9.9.1\">tempo-down</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.9.9.2\">0.875x</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.9.9.3\">0.75</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.9.9.4\">0.625x</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.9.9.5\">0.5x</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.2.10.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T2.2.10.10.1\">Pitch Step-up</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.10.10.2\">0.25 oct</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.10.10.3\">0.5 oct</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.10.10.4\">0.75 oct</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.10.10.5\">1 oct</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.2.11.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T2.2.11.11.1\">Pitch Step-down</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.11.11.2\">0.25 oct</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.11.11.3\">0.5 oct</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.11.11.4\">0.75 oct</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.11.11.5\">1 oct</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.2.12.12\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T2.2.12.12.1\">Gain (factor)</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.12.12.2\">10x</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.12.12.3\">20x</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.12.12.4\">30x</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.12.12.5\">40x</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.2.13.13\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T2.2.13.13.1\">Echo (delay)</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.13.13.2\">125 ms</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.13.13.3\">250 ms</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.13.13.4\">500 ms</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.13.13.5\">1000 ms</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.2.14.14\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T2.2.14.14.1\">Phaser (decay)</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.14.14.2\">0.3 s</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.14.14.3\">0.5 s</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.14.14.4\">0.7 s</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.14.14.5\">0.9 s</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.2.15.15\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T2.2.15.15.1\">Low-pass filter</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.15.15.2\">4 kHz</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.15.15.3\">2833 kHz</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.15.15.4\">1666 kHz</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.15.15.5\">500 kHz</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.2.16.16\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T2.2.16.16.1\">High-pass filter</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.16.16.2\">500 kHz</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.16.16.3\">1333 kHz</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.16.16.4\">2166 kHz</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.2.16.16.5\">3000 kHz</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.2.17.17\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A1.T2.2.17.17.1\">Voice Conversion</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"4\" id=\"A1.T2.2.17.17.2\">N/A</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.2.18.18\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A1.T2.2.18.18.1\">PGD Attack</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T2.2.18.18.2\">40 dB</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T2.2.18.18.3\">30 dB</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T2.2.18.18.4\">20 dB</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T2.2.18.18.5\">10dB</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.2.19.19\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"A1.T2.2.19.19.1\">Utterance Agnostic Attack</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A1.T2.2.19.19.2\">40 dB</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A1.T2.2.19.19.3\">30 dB</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A1.T2.2.19.19.4\">20 dB</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A1.T2.2.19.19.5\">10dB</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"A1.T2.3.1.1\" style=\"font-size:90%;\">Table 2</span>: </span><span class=\"ltx_text\" id=\"A1.T2.4.2\" style=\"font-size:90%;\">The parameters defining the various severity levels of the perturbations used in the proposed benchmark.</span></figcaption>\n</figure>",
            "capture": "Table 2: The parameters defining the various severity levels of the perturbations used in the proposed benchmark."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.07937v1_figure_1.png",
            "caption": "Figure 1: An illustration of the processes involved in using our benchmark to evaluate the robustness of ASR models."
        },
        "2": {
            "figure_path": "2403.07937v1_figure_2.png",
            "caption": "Figure 2: Taxonomy of perturbations currently used in SRB."
        },
        "3": {
            "figure_path": "2403.07937v1_figure_3.png",
            "caption": "(a)"
        },
        "4": {
            "figure_path": "2403.07937v1_figure_4.png",
            "caption": "(b)"
        },
        "5": {
            "figure_path": "2403.07937v1_figure_5.png",
            "caption": "(a)"
        },
        "6": {
            "figure_path": "2403.07937v1_figure_6.png",
            "caption": "(b)"
        },
        "7": {
            "figure_path": "2403.07937v1_figure_7.png",
            "caption": "(c)"
        },
        "8": {
            "figure_path": "2403.07937v1_figure_8.png",
            "caption": "(a)"
        },
        "9": {
            "figure_path": "2403.07937v1_figure_9.png",
            "caption": "(b)"
        },
        "10": {
            "figure_path": "2403.07937v1_figure_10.png",
            "caption": "(a) English"
        },
        "11": {
            "figure_path": "2403.07937v1_figure_11.png",
            "caption": "(b) Spanish"
        },
        "12": {
            "figure_path": "2403.07937v1_figure_12.png",
            "caption": "Figure 7: NWER of the models on different augmentations, averaged over all severities."
        },
        "13": {
            "figure_path": "2403.07937v1_figure_13.png",
            "caption": "Figure 8: NWER as the severity of the augmentation is increased."
        },
        "14": {
            "figure_path": "2403.07937v1_figure_14.png",
            "caption": "Figure 9: The WERV of various models under Gaussian noise and environmental noise of SNR 10 dB"
        },
        "15": {
            "figure_path": "2403.07937v1_figure_15.png",
            "caption": "Figure 10: The WER of the various models under general (blue) and specific (orange) adversarial perturbations. The green markers and line show the average WER on both perturbation types."
        },
        "16": {
            "figure_path": "2403.07937v1_figure_16.png",
            "caption": "Figure 11: The NWER of Spanish only and Multi-Lingual models on Spanish utterances under various perturbations."
        },
        "17": {
            "figure_path": "2403.07937v1_figure_17.png",
            "caption": "(a)"
        },
        "18": {
            "figure_path": "2403.07937v1_figure_18.png",
            "caption": "(b)"
        }
    },
    "references": [
        {
            "1": {
                "title": "Advances in adversarial attacks and defenses in computer vision: A survey.",
                "author": "Akhtar, N., Mian, A., Kardan, N., and Shah, M.",
                "venue": "IEEE Access, 9:155161\u2013155196, 2021.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Deep speech 2: End-to-end speech recognition in english and mandarin.",
                "author": "Amodei, D., Ananthanarayanan, S., Anubhai, R., Bai, J., Battenberg, E., Case, C., Casper, J., Catanzaro, B., Cheng, Q., Chen, G., et al.",
                "venue": "In International conference on machine learning, pp.  173\u2013182. PMLR, 2016.",
                "url": null
            }
        },
        {
            "3": {
                "title": "Square attack: a query-efficient black-box adversarial attack via random search.",
                "author": "Andriushchenko, M., Croce, F., Flammarion, N., and Hein, M.",
                "venue": "In European Conference on Computer Vision, pp.  484\u2013501. Springer, 2020.",
                "url": null
            }
        },
        {
            "4": {
                "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations.",
                "author": "Baevski, A., Zhou, Y., Mohamed, A., and Auli, M.",
                "venue": "Advances in neural information processing systems, 33:12449\u201312460, 2020.",
                "url": null
            }
        },
        {
            "5": {
                "title": "The fifth \u2019chime\u2019 speech separation and recognition challenge: Dataset, task and baselines.",
                "author": "Barker, J., Watanabe, S., Vincent, E., and Trmal, J.",
                "venue": "In Proc. Interspeech 2018, pp.  1561\u20131565, 2018.",
                "url": null
            }
        },
        {
            "6": {
                "title": "Yourtts: Towards zero-shot multi-speaker tts and zero-shot voice conversion for everyone.",
                "author": "Casanova, E., Weber, J., Shulby, C. D., Junior, A. C., G\u00f6lge, E., and Ponti, M. A.",
                "venue": "In International Conference on Machine Learning, pp.  2709\u20132720. PMLR, 2022.",
                "url": null
            }
        },
        {
            "7": {
                "title": "Noise-robust speech recognition with 10 minutes unparalleled in-domain data.",
                "author": "Chen, C., Hou, N., Hu, Y., Shirol, S., and Chng, E. S.",
                "venue": "In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp.  4298\u20134302. IEEE, 2022.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models.",
                "author": "Chen, P.-Y., Zhang, H., Sharma, Y., Yi, J., and Hsieh, C.-J.",
                "venue": "In Proceedings of the 10th ACM workshop on artificial intelligence and security, pp.  15\u201326, 2017.",
                "url": null
            }
        },
        {
            "9": {
                "title": "Unsupervised cross-lingual representation learning for speech recognition.",
                "author": "Conneau, A., Baevski, A., Collobert, R., Mohamed, A., and Auli, M.",
                "venue": "arXiv preprint arXiv:2006.13979, 2020.",
                "url": null
            }
        },
        {
            "10": {
                "title": "Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks.",
                "author": "Croce, F. and Hein, M.",
                "venue": "In International conference on machine learning, pp.  2206\u20132216. PMLR, 2020.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Robustbench: a standardized adversarial robustness benchmark.",
                "author": "Croce, F., Andriushchenko, M., Sehwag, V., Debenedetti, E., Flammarion, N., Chiang, M., Mittal, P., and Hein, M.",
                "venue": "arXiv preprint arXiv:2010.09670, 2020.",
                "url": null
            }
        },
        {
            "12": {
                "title": "Explaining and harnessing adversarial examples, 2014.",
                "author": "Goodfellow, I. J., Shlens, J., and Szegedy, C.",
                "venue": null,
                "url": null
            }
        },
        {
            "13": {
                "title": "Benchmarking neural network robustness to common corruptions and perturbations.",
                "author": "Hendrycks, D. and Dietterich, T.",
                "venue": "arXiv preprint arXiv:1903.12261, 2019.",
                "url": null
            }
        },
        {
            "14": {
                "title": "The many faces of robustness: A critical analysis of out-of-distribution generalization.",
                "author": "Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M., et al.",
                "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.  8340\u20138349, 2021a.",
                "url": null
            }
        },
        {
            "15": {
                "title": "Natural adversarial examples.",
                "author": "Hendrycks, D., Zhao, K., Basart, S., Steinhardt, J., and Song, D.",
                "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.  15262\u201315271, 2021b.",
                "url": null
            }
        },
        {
            "16": {
                "title": "Traps-classifiers of temporal patterns.",
                "author": "Hermansky, H. and Sharma, S.",
                "venue": "In Fifth International Conference on Spoken Language Processing, 1998.",
                "url": null
            }
        },
        {
            "17": {
                "title": "Rasta-plp speech analysis.",
                "author": "Hermansky, H., Morgan, N., Bayya, A., and Kohn, P.",
                "venue": "In Proc. IEEE Int\u2019l Conf. Acoustics, speech and signal processing, volume 1, pp.  121\u2013124, 1991.",
                "url": null
            }
        },
        {
            "18": {
                "title": "The aurora experimental framework for the performance evaluation of speech recognition systems under noisy conditions.",
                "author": "Hirsch, H.-G. and Pearce, D.",
                "venue": "In ASR2000-Automatic speech recognition: challenges for the new Millenium ISCA tutorial and research workshop (ITRW), 2000.",
                "url": null
            }
        },
        {
            "19": {
                "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units.",
                "author": "Hsu, W.-N., Bolte, B., Tsai, Y.-H. H., Lakhotia, K., Salakhutdinov, R., and Mohamed, A.",
                "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:3451\u20133460, 2021a.",
                "url": null
            }
        },
        {
            "20": {
                "title": "Robust wav2vec 2.0: Analyzing domain shift in self-supervised pre-training.",
                "author": "Hsu, W.-N., Sriram, A., Baevski, A., Likhomanenko, T., Xu, Q., Pratap, V., Kahn, J., Lee, A., Collobert, R., Synnaeve, G., et al.",
                "venue": "arXiv preprint arXiv:2104.01027, 2021b.",
                "url": null
            }
        },
        {
            "21": {
                "title": "A binaural room impulse response database for the evaluation of dereverberation algorithms.",
                "author": "Jeub, M., Schafer, M., and Vary, P.",
                "venue": "In 2009 16th International Conference on Digital Signal Processing, pp.  1\u20135. IEEE, 2009.",
                "url": null
            }
        },
        {
            "22": {
                "title": "Signal bias removal by maximum likelihood estimation for robust telephone speech recognition.",
                "author": "Juang, B.-H. and Rahim, M. G.",
                "venue": "IEEE Transactions on Speech and Audio Processing, 4(1):19, 1996.",
                "url": null
            }
        },
        {
            "23": {
                "title": "Power-normalized cepstral coefficients (pncc) for robust speech recognition.",
                "author": "Kim, C. and Stern, R. M.",
                "venue": "IEEE/ACM Transactions on audio, speech, and language processing, 24(7):1315\u20131329, 2016.",
                "url": null
            }
        },
        {
            "24": {
                "title": "The reverb challenge: A common evaluation framework for dereverberation and recognition of reverberant speech.",
                "author": "Kinoshita, K., Delcroix, M., Yoshioka, T., Nakatani, T., Habets, E., Haeb-Umbach, R., Leutnant, V., Sehr, A., Kellermann, W., Maas, R., et al.",
                "venue": "In 2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, pp.  1\u20134. IEEE, 2013.",
                "url": null
            }
        },
        {
            "25": {
                "title": "A study on data augmentation of reverberant speech for robust speech recognition.",
                "author": "Ko, T., Peddinti, V., Povey, D., Seltzer, M. L., and Khudanpur, S.",
                "venue": "In 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp.  5220\u20135224. IEEE, 2017.",
                "url": null
            }
        },
        {
            "26": {
                "title": "Racial disparities in automated speech recognition.",
                "author": "Koenecke, A., Nam, A., Lake, E., Nudell, J., Quartey, M., Mengesha, Z., Toups, C., Rickford, J. R., Jurafsky, D., and Goel, S.",
                "venue": "Proceedings of the National Academy of Sciences, 117(14):7684\u20137689, 2020.",
                "url": null
            }
        },
        {
            "27": {
                "title": "Perceptual adversarial robustness: Defense against unseen threat models.",
                "author": "Laidlaw, C., Singla, S., and Feizi, S.",
                "venue": "In International Conference on Learning Representations, 2021.",
                "url": null
            }
        },
        {
            "28": {
                "title": "CSLU: Foreign Accented English Release 1.2, 2022.",
                "author": "Lander, T.",
                "venue": "URL https://doi.org/10.5683/SP2/K7EQTE.",
                "url": null
            }
        },
        {
            "29": {
                "title": "An overview of noise-robust automatic speech recognition.",
                "author": "Li, J., Deng, L., Gong, Y., and Haeb-Umbach, R.",
                "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 22(4):745\u2013777, 2014.",
                "url": null
            }
        },
        {
            "30": {
                "title": "Rethinking evaluation in asr: Are our models robust enough?",
                "author": "Likhomanenko, T., Xu, Q., Pratap, V., Tomasello, P., Kahn, J., Avidov, G., Collobert, R., and Synnaeve, G.",
                "venue": "arXiv preprint arXiv:2010.11745, 2020.",
                "url": null
            }
        },
        {
            "31": {
                "title": "Towards measuring fairness in speech recognition: Casual conversations dataset transcriptions.",
                "author": "Liu, C., Picheny, M., Sar\u0131, L., Chitkara, P., Xiao, A., Zhang, X., Chou, M., Alvarado, A., Hazirbas, C., and Saraf, Y.",
                "venue": "In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp.  6162\u20136166. IEEE, 2022.",
                "url": null
            }
        },
        {
            "32": {
                "title": "Towards deep learning models resistant to adversarial attacks.",
                "author": "Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A.",
                "venue": "In International Conference on Learning Representations, 2018.",
                "url": null
            }
        },
        {
            "33": {
                "title": "Supervised and unsupervised speech enhancement using nonnegative matrix factorization.",
                "author": "Mohammadiha, N., Smaragdis, P., and Leijon, A.",
                "venue": "IEEE Transactions on audio, speech, and language processing, 21(10):2140\u20132151, 2013.",
                "url": null
            }
        },
        {
            "34": {
                "title": "Acoustical sound database in real environments for sound scene understanding and hands-free speech recognition.",
                "author": "Nakamura, S., Hiyane, K., Asano, F., Nishiura, T., and Yamada, T.",
                "venue": "In LREC, pp.  965\u2013968, 2000.",
                "url": null
            }
        },
        {
            "35": {
                "title": "Universal adversarial perturbations for speech recognition systems.",
                "author": "Neekhara, P., Hussain, S., Pandey, P., Dubnov, S., McAuley, J., and Koushanfar, F.",
                "venue": "arXiv preprint arXiv:1905.03828, 2019.",
                "url": null
            }
        },
        {
            "36": {
                "title": "Recent improvements of asr models in the face of adversarial attacks.",
                "author": "Olivier, R. and Raj, B.",
                "venue": "Interspeech, 2022.",
                "url": null
            }
        },
        {
            "37": {
                "title": "Librispeech: an asr corpus based on public domain audio books.",
                "author": "Panayotov, V., Chen, G., Povey, D., and Khudanpur, S.",
                "venue": "In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), pp.  5206\u20135210. IEEE, 2015.",
                "url": null
            }
        },
        {
            "38": {
                "title": "Transferability in machine learning: from phenomena to black-box attacks using adversarial samples.",
                "author": "Papernot, N., McDaniel, P., and Goodfellow, I.",
                "venue": "arXiv preprint arXiv:1605.07277, 2016.",
                "url": null
            }
        },
        {
            "39": {
                "title": "ESC: Dataset for Environmental Sound Classification.",
                "author": "Piczak, K. J.",
                "venue": "In Proceedings of the 23rd Annual ACM Conference on Multimedia, pp.  1015\u20131018. ACM Press, 2015.",
                "url": null
            }
        },
        {
            "40": {
                "title": "Mls: A large-scale multilingual dataset for speech research.",
                "author": "Pratap, V., Xu, Q., Sriram, A., Synnaeve, G., and Collobert, R.",
                "venue": "arXiv preprint arXiv:2012.03411, 2020.",
                "url": null
            }
        },
        {
            "41": {
                "title": "Scaling speech technology to 1,000+ languages.",
                "author": "Pratap, V., Tjandra, A., Shi, B., Tomasello, P., Babu, A., Kundu, S., Elkahky, A., Ni, Z., Vyas, A., Fazel-Zarandi, M., et al.",
                "venue": "arXiv preprint arXiv:2305.13516, 2023.",
                "url": null
            }
        },
        {
            "42": {
                "title": "Robust speech recognition via large-scale weak supervision.",
                "author": "Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., and Sutskever, I.",
                "venue": "In International Conference on Machine Learning, pp.  28492\u201328518. PMLR, 2023.",
                "url": null
            }
        },
        {
            "43": {
                "title": "The accented english speech recognition challenge 2020: open datasets, tracks, baselines, results and methods.",
                "author": "Shi, X., Yu, F., Lu, Y., Liang, Y., Feng, Q., Wang, D., Qian, Y., and Xie, L.",
                "venue": "In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp.  6918\u20136922. IEEE, 2021.",
                "url": null
            }
        },
        {
            "44": {
                "title": "Hearing is believing: Biologically inspired methods for robust automatic speech recognition.",
                "author": "Stern, R. M. and Morgan, N.",
                "venue": "IEEE signal processing magazine, 29(6):34\u201343, 2012.",
                "url": null
            }
        },
        {
            "45": {
                "title": "Intriguing properties of neural networks.",
                "author": "Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., and Fergus, R.",
                "venue": "In ICLR, 2014.",
                "url": null
            }
        },
        {
            "46": {
                "title": "Improving fairness and robustness in end-to-end speech recognition through unsupervised clustering.",
                "author": "Veliche, I.-E. and Fung, P.",
                "venue": "In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp.  1\u20135. IEEE, 2023.",
                "url": null
            }
        },
        {
            "47": {
                "title": "Query efficient decision based sparse attacks against black-box deep learning models.",
                "author": "Vo, V. Q., Abbasnejad, E., and Ranasinghe, D. C.",
                "venue": "arXiv preprint arXiv:2202.00091, 2022.",
                "url": null
            }
        },
        {
            "48": {
                "title": "Adversarial glue: A multi-task benchmark for robustness evaluation of language models.",
                "author": "Wang, B., Xu, C., Wang, S., Gan, Z., Cheng, Y., Gao, J., Awadallah, A. H., and Li, B.",
                "venue": "arXiv preprint arXiv:2111.02840, 2021a.",
                "url": null
            }
        },
        {
            "49": {
                "title": "Voxpopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation.",
                "author": "Wang, C., Riviere, M., Lee, A., Wu, A., Talnikar, C., Haziza, D., Williamson, M., Pino, J., and Dupoux, E.",
                "venue": "arXiv preprint arXiv:2101.00390, 2021b.",
                "url": null
            }
        },
        {
            "50": {
                "title": "Black-box adversarial attacks on deep neural networks: A survey.",
                "author": "Wang, C., Zhang, M., Zhao, J., and Kuang, X.",
                "venue": "In 2022 4th International Conference on Data Intelligence and Security (ICDIS), pp.  88\u201393. IEEE, 2022a.",
                "url": null
            }
        },
        {
            "51": {
                "title": "Measure and improve robustness in nlp models: A survey.",
                "author": "Wang, X., Wang, H., and Yang, D.",
                "venue": "In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp.  4569\u20134586, 2022b.",
                "url": null
            }
        },
        {
            "52": {
                "title": "Learning structured sparsity in deep neural networks.",
                "author": "Wen, W., Wu, C., Wang, Y., Chen, Y., and Li, H.",
                "venue": "In NIPS, 2016.",
                "url": null
            }
        },
        {
            "53": {
                "title": "Feature-guided black-box safety testing of deep neural networks.",
                "author": "Wicker, M., Huang, X., and Kwiatkowska, M.",
                "venue": "In Tools and Algorithms for the Construction and Analysis of Systems: 24th International Conference, TACAS 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings, Part I 24, pp.  408\u2013426. Springer, 2018.",
                "url": null
            }
        },
        {
            "54": {
                "title": "Speech denoising using nonnegative matrix factorization with priors.",
                "author": "Wilson, K. W., Raj, B., Smaragdis, P., and Divakaran, A.",
                "venue": "In 2008 IEEE International Conference on Acoustics, Speech and Signal Processing, pp.  4029\u20134032. IEEE, 2008.",
                "url": null
            }
        },
        {
            "55": {
                "title": "Self-training and pre-training are complementary for speech recognition.",
                "author": "Xu, Q., Baevski, A., Likhomanenko, T., Tomasello, P., Conneau, A., Collobert, R., Synnaeve, G., and Auli, M.",
                "venue": "In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp.  3030\u20133034. IEEE, 2021.",
                "url": null
            }
        },
        {
            "56": {
                "title": "CSTR VCTK Corpus: English multi-speaker corpus for CSTR voice cloning toolkit (version 0.92), 2019.",
                "author": "Yamagishi, J., Veaux, C., and MacDonald, K.",
                "venue": null,
                "url": null
            }
        },
        {
            "57": {
                "title": "Unsupervised adaptation with discriminative mapping transforms.",
                "author": "Yu, K., Gales, M., and Woodland, P. C.",
                "venue": "IEEE Transactions on Audio, Speech, and Language Processing, 17(4):714\u2013723, 2009.",
                "url": null
            }
        },
        {
            "58": {
                "title": "Towards query-efficient black-box adversary with zeroth-order natural gradient descent.",
                "author": "Zhao, P., Chen, P.-Y., Wang, S., and Lin, X.",
                "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence, pp.  6909\u20136916, 2020.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.07937v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2",
            "2.3"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.4",
            "3.5"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "4.1",
            "4.2"
        ]
    },
    "research_context": {
        "paper_id": "2403.07937v1",
        "paper_title": "Speech Robust Bench: A Robustness Benchmark For Speech Recognition",
        "research_background": "## Motivation\nThe primary motivation for this paper stems from the need for robustness in Machine Learning (ML) models, particularly in Automatic Speech Recognition (ASR) systems, when faced with input perturbations. In real-world contexts, these perturbations can arise from environmental noise, equipment variations, or even malicious attacks. Robustness ensures that these models can continue to make accurate predictions despite such challenges, which is crucial for their safety, security, and reliability.\n\n## Research Problem\nThe research problem addressed by the paper is the lack of standardized benchmarks for evaluating the robustness of ASR models. While similar benchmarks have been valuable in the fields of image recognition and natural language processing, ASR has seen little attention beyond basic benchmarks like digit sequence recognition. This lack of standardization makes it difficult to compare results across studies and understand the specific weaknesses of different models under various perturbations.\n\n## Relevant Prior Work\nThe paper draws on several pieces of prior work:\n1. **Vision Benchmarks**: There are established robustness benchmarks for vision tasks (Hendrycks & Dietterich, 2019; Hendrycks et al., 2021; Croce et al., 2020), which have helped in tracking progress and enabling easier comparisons across models.\n2. **Natural Language Processing Benchmarks**: Robustness benchmarks have also been developed for natural language processing tasks (Wang et al., 2021, 2022).\n3. **ASR Robustness Assessments**: Previous efforts to evaluate ASR robustness have involved using a mix of distortions from various datasets (Radford et al., 2023; Wen et al., 2016; Chen et al., 2022) or computing transcription accuracy on multiple speech datasets with real-world perturbations (Likhomanenko et al., 2020; Hsu et al., 2021). However, these approaches lack standardized datasets and controlled perturbations, making results difficult to compare.\n4. **Adversarial Attacks**: The field has given limited attention to evaluating ASR models against adversarial attacks unless the intention was to propose defenses, leaving potential security risks unaddressed.\n\n## Contributions\nIn response to the identified gaps, this paper introduces SRB (Speech Robust Bench), a standardized robustness benchmark for ASR models. SRB comprises:\n1. A comprehensive bank of perturbations, including environmental distortions, speaker attribute variations, semantic-preserving special effects found in digital media, and adversarial attacks.\n2. Robustness metrics that evaluate not only transcription accuracy but also the stability of predicted transcripts under randomized perturbations.\n\nAdditionally, the paper showcases the use of SRB by conducting robustness assessments of popular ASR models, revealing disparities among them and across different population sub-groups. These insights underscore the benchmark's utility for both robustness and fairness evaluations in trustworthy AI research.",
        "methodology": "The methodology section outlines a robust benchmarking framework for evaluating Automatic Speech Recognition (ASR) systems under various corrupted conditions. Here\u2019s the proposed method or model, with an emphasis on its key components and innovations:\n\n1. **Word Error Rate (WER) Calculation:**\n   The Word Error Rate (WER) is used as the primary metric for ASR performance. Typically, WER is defined as the word-level edit distance between the predicted and reference transcripts, normalized by the length of the reference transcript. The edit distance includes the number of word substitutions, deletions, and additions needed to match the predicted transcript with the reference.\n   \n   - When dealing with multiple transcript pairs, the predicted and reference transcripts are concatenated into one long text segment each. The edit distances for all pairs are summed and divided by the total length of the reference transcripts. Formally:\n     \n     \\(\\text{WER} = \\frac{\\sum \\text{edit distance}}{\\sum \\text{length of reference transcripts}}\\)\n\n2. **Normalized Word Error Rate (NWER):**\n   Given multiple corruption scenarios and varying levels of corruption severity, simply averaging WERs might not accurately reflect a model's robustness due to differing difficulty levels of these pairs.\n   \n   - The NWER addresses this by weighting the WER of each corruption-severity pair inversely to its difficulty. The difficulty is estimated based on the performance of a baseline model.\n   - This ensures that errors on easier corruption-severity pairs are penalized more than errors on more difficult pairs.\n   - Formally, NWER is defined as:\n     \n     \\[\\text{NWER} = \\sum_{c,s} \\frac{\\text{WER}_{target}(c, s)}{\\text{WER}_{baseline}(c, s)}\\]\n\n3. **Stability Metric - WER Variance (WERV):**\n   The stability of an ASR model\u2019s predictions under noise is measured by the variance in its WER over repeated random samples of corruption.\n   \n   - Since Hendrycks & Dietterich (2019) used flip rate for classification tasks (incompatible with sequence predictions in ASR), this new metric evaluates the variance in WER instead.\n   - For a set of utterances, each utterance \\(\\mathbf{x}\\) is corrupted with multiple random corruption samples, grouped into multiple datasets.\n   - WERV is computed using:\n     \n     \\[\\text{WERV} = \\text{Var}(\\text{WER}_{target}(\\mathbf{x}, \\mathbf{x_i, c}))\\]\n   \n   where \\(\\mathbf{x_i}\\) represents the corrupted sample for utterance \\(\\mathbf{x}\\) under corruption \\(c\\) and severity \\(s\\).\n\nThis methodology provides a comprehensive framework for evaluating and benchmarking the robustness of ASR models, factoring both error rates across different corrupt conditions and the stability of predictions under such conditions.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Experiment Setup:\n1. **Objective**: Evaluate the robustness of recent and popular ASR DNNs to non-adversarial and adversarial perturbations using SRB.\n2. **Datasets**:\n   - **English Speech**: Librispeech (Panayotov et al., 2015).\n   - **Spanish Speech**: Spanish test set from Multi-Lingual Librispeech (Pratap et al., 2020).\n3. **Baselines**:\n   - Multiple ASR models including:\n     - **w2v2-lg-slf** (Wav2Vec-2 Large self-trained)\n     - **wsp-lg**\n     - **w2v2-lg-rob**\n     - **hubt-lg**\n     - **w2v2-bs** (Wav2Vec-2 Base)\n     - **w2v2-lg-es** (Spanish model)\n     - **mms**\n     - **wsp-tn.en**\n4. **Evaluation Metrics**:\n   - **Word Error Rate (WER)**: Measures transcription accuracy.\n   - **Normalized Word Error Rate (NWER)**: Used to assess robustness, especially in the presence of perturbations.\n   - **Log WER Ratio (LWERR)**: Measures disparity in prediction accuracy across genders.\n\n#### Main Experimental Results:\n\n1. **Overall Robustness**:\n   - **w2v2-lg-slf** is the most robust model on average WER.\n   - **wsp-lg** follows, but it is equally susceptible to general and specific perturbations.\n   - **wsp-tn.en** is highly vulnerable to specific adversarial attacks.\n\n2. **Model Characteristics and Robustness**:\n   - Larger models tend to be more robust, though less consistently so against adversarial perturbations.\n   - Increasing training data has minimal impact on robustness.\n   - Self-training may induce robustness to utterance-agnostic adversarial attacks.\n   - For Spanish ASR models, **w2v2-lg-es** is the most robust, outperforming **wsp-lg** despite the latter's larger size and training data.\n\n3. **Impact of Specific Perturbations**:\n   - **wsp-lg** struggles with Spanish, especially on Room Impulse Response (RIR) perturbations.\n\n4. **Fairness Analysis**:\n   - On clean data, English models are slightly biased against female speakers; models like **ds** are the most biased, while **hubt-lg** is the least.\n   - Bias is reduced under noise perturbations but increases with room impulse response and phasing.\n   - The bias against female speakers is more pronounced in Spanish ASR models and is exacerbated by noise, particularly room impulse response.\n   - Adversarial attacks also contribute to gender bias in predictions.\n\n#### Summary:\n- The study underscores that model size positively correlates with robustness to perturbations, especially for the Whisper and Wav2Vec-2.0 families, although specific architectural features such as self-training and discrete representation also play a role.\n- There is a noticeable bias in robustness against female speakers in both English and Spanish models, with certain perturbations disproportionately degrading performance for female speakers.\n- Simply scaling the model size and training data is not sufficient for achieving robustness, especially in a multilingual context."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Measure the robustness of different ASR models to non-adversarial and adversarial input perturbations.",
            "experiment_process": "Evaluated several recent and popular ASR DNNs using SRB. Perturbations included non-adversarial and adversarial (both untargeted utterance-agnostic and untargeted specific). Experimental groups featured various ASR models, including w2v2-lg-slf, wsp-lg, hubt-lg, w2v2-lg-rob, etc. Used datasets, mainly English speech from Librispeech, and evaluation metrics such as WER (Word Error Rate) and NWER (Normalized WER). Analyzed the robustness against perturbations for specific models and also measured robustness based on model characteristics, such as the number of parameters and size of the training data.",
            "result_discussion": "w2v2-lg-slf emerged as the most robust model regarding average WER, while hubt-lg achieved the lowest WER against specific adversarial perturbations. Larger models are generally more robust, especially within Whisper and Wav2Vec-2.0 families, though less so for the HuBert family. Increasing training data had a minor influence on robustness. The self-training method used in w2v2-lg-slf and discrete representations in hubt-lg contributed to their robustness against certain adversarial attacks. Diversifying training data had minimal effects on adversarial robustness.",
            "ablation_id": "2403.07937v1.No1"
        },
        {
            "research_objective": "Evaluate the robustness of English-only ASR DNNs and identify conditions under which models\u2019 performance deteriorates.",
            "experiment_process": "Analyzed robustness of English ASR models from Table 1 on non-adversarial and adversarial perturbations in SRB. Aggregate metrics (NWER and WERV) were presented to measure robustness in terms of utility and stability. Detailed analyses on performances deteriorating under different perturbations and severity levels were conducted.",
            "result_discussion": "wsp-lg emerged as the most robust model on average, followed by hubt-lg. w2v2-lg-slf performed best against general perturbations but was more vulnerable to specific perturbations. Larger models within Whisper and Wav2Vec-2.0 families showed more robustness, while training on larger datasets showed limited influence on robustness. The trends were inconsistent across the HuBert family. Increasing model parameters improved average robustness even though there were inconsistencies observed regarding adversarial problems.",
            "ablation_id": "2403.07937v1.No2"
        },
        {
            "research_objective": "Analyze disparities in robustness across different population sub-groups (English/Spanish speakers, males/females).",
            "experiment_process": "Extended robustness analysis to various demographic subgroups using SRB. Evaluated disparities in robustness by comparing NWER and WER degradation across subgroups. Used Spanish speech from Multi-Lingual Librispeech. Measured biases by calculating the Log WER Ratio (LWERR) for gender-specific analyses.",
            "result_discussion": "Disparities were observed, with multi-lingual models generally more robust for English than Spanish. w2v2-lg-es was the most robust for Spanish but wsp-lg faced difficulties with simple perturbations like RIR. Models were more biased against females, with bias levels increasing in Spanish models. Various perturbations like room impulse response disproportionately degraded performance for female speakers. The analysis revealed that certain perturbations exacerbate gender-based robustness disparities, particularly in Spanish models.",
            "ablation_id": "2403.07937v1.No3"
        }
    ]
}