{
    "title": "Simulation, Modelling and Classification of Wiki Contributors: Spotting The Good, The Bad, and The Ugly",
    "abstract": "Data crowdsourcing is a data acquisition process where groups of voluntary contributors feed platforms with highly relevant data ranging from news, comments, and media to knowledge and classifications. It typically processes user-generated data streams to provide and refine popular services such as wikis, collaborative maps, e-commerce sites, and social networks. Nevertheless, this modus operandi raises severe concerns regarding ill-intentioned data manipulation in adversarial environments. This paper presents a simulation, modelling, and classification approach to automatically identify human and non-human (bots) as well as benign and malign contributors by using data fabrication to balance classes within experimental data sets, data stream modelling to build and update contributor profiles and, finally, autonomic data stream classification. By employing TripAdvisor\u2014a travel website that provides user-generated content and reviews\u2014as a testbed, our approach proves to significantly boost the confidence and quality of the classifier by using a class-balanced data stream, comprising both real and synthetic data. Our empirical results show that the proposed method distinguishes between benign and malign bots as well as human contributors with a classification accuracy of up to .",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Crowdsourcing platforms primarily rely on data produced and shared by an extensive network of contributors, known as \u201cThe Crowd\u201d. Given the ubiquitous Internet access and the increasing user-generated data sources, the number of crowdsourcing systems has dramatically increased in recent times, significantly modifying the online behaviour of users and businesses. People no longer rely on expert advice but on the experiences of other users to make decisions on purchases, leisure, news, learning, and information in general. Such platforms integrate Artificial Intelligence (ai) techniques such as data mining to swiftly process the crowd feedback and provide tailored up-to-date services on the fly using data streams. However, the processing opacity and the voluntary nature of crowdsourced data raise algorithmic transparency and data reliability concerns. On the one hand, the opacity of ai algorithms affects the interpretability of the results and can only be tackled by developers at the algorithmic design stage. On the other hand, data manipulation performed on behalf of third-party interests has become omnipresent in crowdsourcing platforms with fake news in social networks, biased feedback in evaluation-based platforms, and undesired (spam) content in wiki pages. Such underground activities, which negatively impact the reliability of the results, are typically performed at large scale by ill-intentioned bots in adversarial contexts. Commonly construed as self-activated software agents programmed to run continuously in distributed network environments, bots perform tasks and make decisions on behalf of their creators without direct human intervention. They typically sense, perceive, and adapt to the context they operate. According to Tsvetkova et al. (2017) [1  ###reference_b1###], all four main types of bots\u2013content extractors, action executors, content generators, and human emulators\u2013include benign and malign sorts. While benign bots tend to perform repetitive tasks to improve quality of service and system conditions, malign bots are designed to degrade content and system conditions by conveying false or biased information, altering system parameters, and/or tampering with data sources. This work addresses the real-time profiling and classification of TripAdvisor contributors into human and non-human (bots) as well as benign and malign contributors. The proposed method relies on data simulation and modelling of review streams to obtain an automatic classification of contributors. While the data fabrication balances classes to improve the consistency of the results, data stream modelling builds contributor profiles on the fly. Therefore, this paper furnishes a data stream classification to identify deceitful sources in real-time. Specifically, contributors are modelled using review-based information (e.g., number of reviews, frequency of reverts, links, and number of characters inserted and deleted). The model is incrementally updated with each incoming review. The classifier uses the contributor profile, first, to differentiate humans from bot users. Then, we employ a stack-based Machine Learning (ml) model, i.e., a two-level stacking system, to detect both bot/human users and positive/negative contributions. The experiments have been conducted using a real TripAdvisor data set containing bots and humans. Despite the imbalanced class distribution, the final two-level stacking classifier presents results between and concerning the accuracy and F-measure (both macro and micro results per class). To balance classes within the experimental data set, this work adopts a data fabrication scenario based on synthetic-generated data. The real and synthetic data have been combined, improving the system performance and proving the solution efficiency. In summary, our proposal allows us to understand the behaviour of both benevolent and malevolent bots to pre-empt malign contributions and, ultimately, to prevent attacks to review pages. The rest of this paper is organised as follows. Section 2 ###reference_### includes relevant related work concerning contributor modelling and classification. Section 3 ###reference_### introduces the proposed approach, describing the profiling, classification, and data fabrication model. Section 4 ###reference_### presents the empirical evaluation results of the developed classifiers, including the simulation of synthetic data to balance the classes within the experimental data set. Finally, Section 5 ###reference_### concludes and highlights new perspectives to detect ill-intentioned contributors."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related work",
            "text": "Data crowdsourcing has been explored in multiple domains, ranging from knowledge sharing, collaborative maps, social interaction to personalised recommendation [2]. For example, Wikipedia, available at www.wikipedia.org, December 2021, and TripAdvisor, available at www.tripadvisor.com, December 2021, offer useful knowledge, which is continuously updated and improved by an invisible army of volunteer contributors; Waze, available at www.waze.com, December 2021, assists drivers based on information provided in real-time by the crowd concerning traffic, accidents, obstacles, and road conditions; Facebook, available at www.facebook.com, December 2021, provides a way for people to communicate and socialise based exclusively on peer input; and TripAdvisor makes personalised recommendations relying solely on opinions and classifications shared by the crowd in the form of ratings, textual reviews, or photos. In this continuously evolving data crowdsourcing world, detecting automated data manipulators (bots and spammers) and their intentions (benign or malign) is essential to ensure data reliability. Detection methods search for hyperactivity indicators such as the number of page views, bounce rates, fake conversions, abnormal traffic spikes, and atypical average session duration, available at www.netacea.com/glossary/detect-bot-traffic, December 2021. The accurate identification and classification of bots and sockpuppets can arguably help to mitigate the effects of malign data manipulation."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Classification of contributors",
            "text": "TripAdvisor contributors can be humans or bots, and their contributions may be benign or malign. Bot activities are not all flagged and can be mistaken as human contributions. For example, there is a recognition of automated systems to identify bots in real-time by checking behavior patterns and identifying features typical of bots. The most active bots display a classic hockey stick activity curve with a long tail composed of slower bots responsible for corrective repetitive tasks.\n\nMalign activities correspond in general online spaces to edits made with deliberate malign intent, which differ from inadvertently damaging edits. To address vandalism detection, some proposed approaches come in form of trust-based systems that evaluate user behavior.\n\nAutomation in some platforms has led to the development of models and services aimed at vandalism detection and removal. These systems predict the quality of contributions (quality of review probability score) as they are made, as well as the quality of submission drafts (quality of submission probability score). The submission quality model bases its predictions on structural characteristics such as the number of sections, references, or infobox presence, since these features seem to correlate strongly with good writing and tone. These systems accept single or batch-review submissions.\n\nMore specifically, given a review or contribution, automated systems predict probabilities: (i) whether or not a contribution causes damage; (ii) if it was made in good faith; and (iii) if the contribution will eventually be removed. Regarding the quality of a submission draft, automated systems return the probability distribution of being in one of the predefined classes (spam, vandalism, attack, or ok). These scores can also be used to help identify malign bots.\n\nCuriously, even benign bots, which support platforms in tasks such as correcting spelling, maintaining links, or undoing digital vandalism, often undo each other\u2019s edits. Solutions use the power of the community to detect malign users, encompassing mechanisms to encourage users to report collaboratively malign behaviors.\n\nIndeed, malign behaviors in social networks have been explored by many authors in the literature, some of them following an ML approach. Notably, the detection of multiple online accounts (sockpuppet) created for purposes of deception has been examined. Features used include the number of user contributions, frequency of reverts after contributions, average of bytes added and removed from each revision, and the interval between user registration and the first contribution.\n\nWhen it comes to popular platforms like TripAdvisor, they are not safe from this type of attack either. Efforts are made to detect such bots using ML-based approaches to distinguish them from human profiles, identifying features related to user profile and content analysis used to compute botScore. Furthermore, diagnostic tools are often analyzed for their ability to correctly identify bots, with their thresholds prone to variance resulting in false positives and false negatives.\n\nAutomatic identification of fake contributions involves exploiting not only textual content but also temporal data and network features. The goal is to identify insightful conclusions such as deceptive content by relatively new accounts with highly dense or overlapping networks. Others focus on identifying spam users as a binary classification task based on user editing behavior, using features like edit size, timing, link usage, and more, integrated with automated scoring systems to improve classification accuracy.\n\nHybrid and online learning approaches are also employed in the detection of vandalism, focusing on classifying malign contributions with attention to contextual features and user profiling. Discussions on the challenges of gathering sufficient training data for ML models emphasize the use of advanced techniques to detect malign contributors.\n\nMore recent works address identifying content with undisclosed paid nature and their contributors, relying on content-derived features, user, and edit history patterns to build unsupervised ML systems. These systems use network graphs with nodes representing articles and edges representing contributors to capture relationships and features like PageRank and clustering coefficients. The best results are often obtained using Random Forest classifiers combined with various features derived from article and user data. Lastly, broad surveys on fake news detection analyze the impact of misinformation and the distinguishing features between human and non-human creators on social media."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Research contribution",
            "text": "This article analyses wiki-based contributions adopting data stream simulation, modelling and classification. Data stream modelling builds and continuously updates contributor profiles using the stream of contributions. The proposed profiling model employs the features provided by the surveyed works, i.e., review-based information (e.g., number of reviews, reverts, links, number of characters inserted or deleted, etc.) and ores edit quality. Based on the statistical metrics of real data, our data fabrication produces a class-balanced data set that helps to improve the consistency of the results. Finally, the data stream classification identifies both human and non-human contributors as well as benevolent and malevolent contributors. Table 1  ###reference_### depicts a comparison of relevant related work that addresses contributor classification. The literature presents a significant number of approaches to classify contributors. However, they do not furnish the contribution type classification, i.e., detection of malevolent and benevolent contributors. Furthermore, the modelling of wiki streams for the online automatic contributor classification has not been thoroughly explored. To overcome this gap, this work addresses contributor profiling and classification to identify malign behaviours in wiki-based crowdsourcing platforms on the fly. ###table_1###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Proposed method",
            "text": "The proposed method is composed of multiple stages: (i) pre-processing; (ii) synthetic data generation; (iii) classification; and (iv) evaluation.\nFigure 1  ###reference_### illustrates the solution designed to process wiki-based streams and generate classifications. The pre-processing stage (Section 3.1  ###reference_###) analyses the input data in terms of pairwise correlations (Section 3.1.1  ###reference_.SSS1###) and employs feature engineering (Section 3.1.2  ###reference_.SSS2###) to select the best features for the contributor profiling (Section 3.1.3  ###reference_.SSS3###). The synthetic-generated data enables to balance the classes and improve the model effectiveness (Section 3.2  ###reference_###). The resulting data set, composed of real and synthetic data, is then incrementally processed to model (Section 3.3  ###reference_###) and classify (Section 3.4  ###reference_###) the contributors. The classification, which relies on well-known algorithms, explores single-class (binary) as well as a novel multi-class (stacking) classification of contributors and their editions. Finally, the outcomes are evaluated through standard classification metrics (Section 3.5  ###reference_###).\n###figure_1###"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Pre-processing",
            "text": "Pre-processing aims to obtain a consistent feature space and facilitate compatibility with the target classification. This stage assembles three tasks: (i) data analysis; (ii) feature engineering; and (iii) feature selection."
        },
        {
            "section_id": "3.1.1",
            "parent_section_id": "3.1",
            "section_name": "3.1.1 Data analysis",
            "text": "The data analysis starts with a pairwise correlation of the features enumerated in Table 2  ###reference_###, which are based on the literature review.\nEquation (1  ###reference_###) describes the Pearson Correlation Coefficient [25  ###reference_b25###] used to calculate the correlations where  and  represent two different features. The correlation coefficient ranges from -1 to 1. When the relation between two features is inverse, the metric is negative, otherwise it is positive."
        },
        {
            "section_id": "3.1.2",
            "parent_section_id": "3.1",
            "section_name": "3.1.2 Feature engineering",
            "text": "The TripAdvisor data were compiled by the authors between January 14th and June 21st 2020. TripAdvisor holds crowdsourced information related to cities, attractions, monuments, hotels, restaurants, cultural details, etc. In particular, the retrieved data set includes pages, contributors, and reviews.  \nTable 2  ###reference_### contains the manufactured features of this experimental data set. The first target corresponds to feature #2, where zero represents human and one represents non-human (bot) contributors. Furthermore, the second target feature, named contribution type, is derived from feature #17 (ok probability) provided by ores. When this probability is above , the contribution is positive or favorable; otherwise, negative or hostile. In particular, zero corresponds to positive and one to negative edits.  \nFigure 2  ###reference_### depicts the distribution of contributor classes within the original data set: (i) benign and malign humans; and (ii) benign and malign bots.  \n###figure_2###"
        },
        {
            "section_id": "3.1.3",
            "parent_section_id": "3.1",
            "section_name": "3.1.3 Feature selection",
            "text": "Feature selection reduces the original feature space to improve performance and to avoid creating computationally unfeasible models. This work, due to the number of input features, adopted Recursive Feature Elimination (rfe131313Available at www.scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html  ###reference_ted/sklearn.feature_selection.RFE.html###, December 2021.), a wrapper-type feature selection algorithm, to identify the features that maximise performance. rfe was configured to wrap the Linear Support Vector Classifier141414Available at www.scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html  ###reference_ted/sklearn.svm.LinearSVC.html###, December 2021. and use as parameters penalty=l1, dual=False, step=0.05 and n_jobs=-1."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Synthetic data generation",
            "text": "Synthetic data generation enables the simulation of stochastic and multi-spectral layouts to enhance ml model testing. This common practice found in the literature [26  ###reference_b26###, 27  ###reference_b27###, 28  ###reference_b28###, 29  ###reference_b29###] supports the generation of relevant scenarios that, in most cases, are absent in the real data. Furthermore, it produces anonymous data, thereby maintaining the privacy of information. This process balances experimental data sets with synthetic samples which, in turn, are used to build models that react and operate correctly under the considered scenario.\nIn classification, it is desirable to have data equally distributed among classes, i.e., ensure the data is free of any class bias. The main benefit of this scenario, where classifier models are built with balanced experimental data, is that they display higher classification accuracy.\nIn this work, synthetic data generation constitutes a user-controlled, accurate, cost-effective, efficient and scalable way to balance the classes within the experimental data set.\nSynthetic samples were created using a statistical approach based on the quartile distribution of the most relevant features for both targets. In particular, the proposed synthetic data generation method creates feasible editor daily incremental activity samples corresponding to the two target categories. The generated samples are composed of the features in Table 2  ###reference_### and based on statistical measures (quartile distribution, median, minimum and maximum values) considering four intervals: (i) from minimum to the first quartile (Q1); (ii) from Q1 to median; (iii) from median to third quartile (Q3); (iv) from Q3 to the maximum.\nA non-hierarchical K-means cluster analysis151515Available at www.scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html  ###reference_ted/sklearn.cluster.KMeans.html###, December 2021. was performed on the original data set to identify common bot behaviour. Based on the information shown in Figure 3  ###reference_###, the model was configured to use two clusters () and trained with bot samples.\n###figure_3### K-means provides the required statistic indicators to generate the new data, i.e., quartile distribution, minimum and maximum feature values. Algorithm 1  ###reference_### describes the implemented synthetic data model. It generates  new samples to balance the class distribution of the original data set. As previously mentioned, the model uses four different groups to create random feature values: (i) from minimum to Quartile 1 (Q1); (ii) from Q1 to median; (iii) from median to Q3; (iv) from Q3 to maximum.\nFinally, the original and synthetic data are joined into the combined data set. The editions, characterised by the features in Table 2  ###reference_###, are aggregated on a daily basis. The resulting data set is then processed as a stream."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Incremental profiling",
            "text": "The incremental profiling builds and continuously updates the profiles of contributors from the stream of samples stored in the combined data set. The profile of each contributor holds: (i) the incremental sum of features #3, #5, #9 and #11-14; (ii) the incremental average of features #4, #6, and #15-18; and (iii) the manufactured features #7, #8 and #10. The latter are calculated as follows:\n#7 = #3 / number of weeks, incremental sum.\n#8 = #5 / number of weeks, incremental sum.\n#10 = #9 / #3, incremental sum."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Classification",
            "text": "The classification task explores single-class (binary) and a novel multi-class (stacking) methods. Several stream-based binary classification algorithms were selected according to performance in similar problems [17  ###reference_b17###, 30  ###reference_b30###, 31  ###reference_b31###] and availability in scikit-multiflow161616Available at https://scikit-multiflow.github.io  ###reference_###, December 2021., the adopted ml package for streaming data. They include single and ensemble methods:\nNaive Bayes (nb)171717Available at https://scikit-multiflow.readthedocs.io/en/latest/api/generated/skmultiflow.bayes.NaiveBayes.html  ###reference_/latest/api/generated/skmultiflow.bayes.NaiveBayes.html###, December 2021.\nDecision Tree (dt)181818Available at https://scikit-multiflow.readthedocs.io/en/stable/api/generated/skmultiflow.trees.ExtremelyFastDecisionTreeClassifier.html  ###reference_/stable/api/generated/skmultiflow.trees.ExtremelyFastDecisionTreeClassifier.html###, December 2021.\nRandom Forest (rf)191919Available at https://scikit-multiflow.readthedocs.io/en/stable/api/generated/skmultiflow.meta.AdaptiveRandomForestClassifier.html#skmultiflow.meta.AdaptiveRandomForestClassifier  ###reference_/stable/api/generated/skmultiflow.meta.AdaptiveRandomForestClassifier.html#skmultiflow.meta.AdaptiveRandomForestClassifier###, December 2021.\nBoosting Classifier (bc)202020Available at https://scikit-multiflow.readthedocs.io/en/stable/api/generated/skmultiflow.meta.OnlineBoostingClassifier.html  ###reference_/stable/api/generated/skmultiflow.meta.OnlineBoostingClassifier.html###, December 2021.\nThe proposed two-level stacking method, depicted in Figure 4  ###reference_###, employs three rf models with the following hyperparameter configuration: random_state=0, n_estimators=15, max_features=None.\n###figure_4###"
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "Evaluation metrics",
            "text": "The evaluation of the single-class (binary) and multi-class (stacking) classifiers uses classification accuracy, F-measure, macro-averaging and micro-averaging.\nThe classification accuracy determines the classifier performance by measuring the number of correct classifications.\nThe F-measure combines Precision and Recall to establish the effectiveness of the classifier.\nThe micro-averaging and macro-averaging methods return a single value for the different metrics across multiple classes. While the macro-average computes the metric average independently for each class, i.e., treats all classes equally; the micro-average aggregates the contributions of all classes to compute the metric average. As suggested in the literature [32  ###reference_b32###, 33  ###reference_b33###], the averaging methods should be used when the experimental data set is class-imbalanced."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experimental results",
            "text": "All experiments were performed on a server with the following hardware specifications: \nOperating System: Ubuntu 18.04.2 LTS 64 bits \nProcessor: IntelCore i9-9900K \nRAM: DDR4 \nDisk: (7200 rpm SATA) + SSD \nThe experiments encompass: (i) offline feature analysis to select the most relevant features to the contributor profile; (ii) offline synthetic data generation to balance the classes within the data set; and (iii) classification of human and bot contributors as well as benign and malign contributions, i.e., regarding the two target features. Moreover, the classification stage explores three scenarios: (a) stream-based binary classification of human and bot contributors using imbalanced real data; (b) stream-based binary classification of benign and malign contributors using imbalanced real data; and (c) stream-based multi-class stacked classification of benign and malign humans and bots using imbalanced (real) and balanced (real and synthetic) data. While scenarios (a) and (b) set the classification baseline with the original deeply unbalanced experimental data for each of the two target features, scenario (c) analyses the performance of the stacking classification strategy in the simultaneous detection of contributor and contribution types. Furthermore, the data collection relied on TripAdvisor, a well-known platform for gathering user-generated content, a set of Python utilities for extracting and processing the features in Table 2. The stream-based ml models were incrementally updated and evaluated with EvaluatePrequential, a tool for online learning evaluation. For each observed sample, the prequential evaluation makes a prediction, tests and trains the model."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Feature analysis",
            "text": "To ensure good classification performance, feature pairwise correlation analysis was followed by feature selection. Data analysis detects the relevant features for contributor profiling. Therefore, the correlation between features and targets has been determined through the Pearson correlation coefficient. Table 3  ###reference_### shows for each target \u2013 (1) user type, i.e., human or non-human; and (2) contribution type, i.e., malevolent or benevolent \u2013 the features with a correlation below -0.15 or above 0.15. Note that these results indicate the current problem can be addressed with ml techniques as there exist moderate correlations with both targets. Feature selection, as explained in Section 3.1.3  ###reference_.SSS3###, applies a combinatorial search over configurable parameter ranges and uses the rfe feature selection algorithm and LinearSVC to determine the best features. At the end, for the first target, the selected features (Table 2  ###reference_###) were: #6 to #10, #12, #15 (good faith), #16 (e probability) and #18 (b and stub probabilities). Furthermore, the second target relies on #18 (excluding ga probability)."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Synthetic data generation",
            "text": "The quality of the new data is established by comparing the statistical properties and class distribution of both original and generated data. Analysis of the generated synthetic data is shown in Table 4, a statistical comparison between the original and generated data. Specifically, it shows the statistical attributes of the real data and the relative percentage change of the synthetic versus the original samples considering mean, minimum values, the first, second and third quartiles. The results cover all relevant features for both targets, excluding #15 (good faith) in Table 2 because synthetic and the original data are statistically identical. Since the variations are minimal for most features, the results indicate that the proposed synthetic data generation algorithm maintains the statistical attributes of the real data. The sole exception is the number of reverted contributions per contributor (#9 in Table 2) since it does not represent a probabilistic value.\n\nClustering is performed using K-means. Figure 5 shows the distribution of classes for the experimental data set after adding synthetic information. Note that the user type is determined by the bot flag, feature #2 in Table 2, while the type of contribution is based on the ok probability (feature #17 in Table 2). In the latter case, when the ok probability is above , the contribution is positive, otherwise it is negative."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Classification of human and bot contributors",
            "text": "The proposed method estimates if the contributor is human or bot using the bot flag target feature. While class #0 represents human contributors, class #1 is bot contributors. This experiment has been performed with a data stream produced from the original class-imbalanced data and three different sets of features: Basic set of features (features #3 to #14 from Table 2 ###reference_###); Full set of features (see Table 2 ###reference_###); Selected set of features (see Section 4.1 ###reference_###). Table 5 ###reference_### shows the macro and micro results of this classification. The values obtained are consistent in most cases for all classifiers and the values increase when using all features. Furthermore, the proposed method presents promising results when exclusively employing the most relevant features, since it reduces the processing time. The best binary classifier in all experiments is rf. Moreover, even though the original data set is deeply imbalanced, the system achieves a remarkable performance for both human and bot classes. ###table_3###"
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Classification of benign and malign contributors",
            "text": "The benign and malign classification experiments comprise: (i) stream-based classification of the original class-imbalanced data using binary classifiers; and (ii) stream-based classification of the original class-imbalanced and the new class-balanced data using a multi-class rf stacking classifier."
        },
        {
            "section_id": "4.4.1",
            "parent_section_id": "4.4",
            "section_name": "4.4.1 Binary classification",
            "text": "Table 6  ###reference_### provides the macro and micro results for the second target, i.e., benevolent and malevolent contributors classification, with single and ensemble binary classifiers and different feature sets. This experiment was performed with a data stream produced from the original class-imbalanced data set and the three different sets of features. In light of the results, we can conclude that distinguishing between positive and negative contributions is not as straightforward as identifying humans and bots. Nonetheless, the proposed method achieves accuracy and macro F-measure with the rf classifier for the best feature set analyzed. ###table_4###"
        },
        {
            "section_id": "4.4.2",
            "parent_section_id": "4.4",
            "section_name": "4.4.2 Multi-class classification",
            "text": "The stacking model allows the simultaneous classification of contributor type (humans versus bots) and contribution type (positive versus negative). This experiment was performed with two data streams generated from the original class-imbalanced and the new class-balanced sets. The goal is to analyse the impact of the balanced data set, comprising original and synthetically generated data. Table 7  ###reference_### contains the results of: (i) the baseline model obtained with feature set 3, the rf classifier and a stream produced from original class-imbalanced data (see Table 5  ###reference_### and Table 6  ###reference_###); (ii) the stacking model with a stream produced from the original class-imbalanced data; and (iii) the stacking model with a stream produced from the class-balanced data. When compared with the baseline, the stacking model improves the results in both accuracy and F-measure. Furthermore, the results show the positive impact of the class-balanced data set. The difference in processing time is due to the fact that the imbalanced stream has  events and the balanced stream has  events. Actually, the process time per event is identical:  for the imbalanced and  for the balanced data stream."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Literature comparison",
            "text": "The results of the proposed method are herein compared with some of the works listed in Table 1. Most of the related search adopts offline processing. Note that online models are built from scratch and incrementally updated and evaluated, whereas the offline models are trained and then tested using distinct data partitions. Zheng et al. (2019) detect the type of contributor and contributions with a similar performance in both accuracy and macro F-measure to the proposed method. However, social media data should be treated as streams since new revisions arrive continuously, resulting in instant classifier updates. Thus, having comparable results endorses the proposed method.\n\nTaking into account solely contribution type detection, Adler et al. (2011) report accuracy with a rather low recall metric, whereas Choi et al. (2016) match the same accuracy value without additional metrics for fair comparison. Kumar et al. (2016) identify hoaxers from non-hoaxers with accuracy (and percent points lower in accuracy for contribution type detection than our proposal as shown in Table 6 and Table 7, respectively). Compared to Green & Spezzano (2017) and Velayutham et al. (2017) contributor type detectors, our solution attains percent points in accuracy for bot detection (see Table 5 and Table 7, respectively). Finally, Zheng, Albano et al. (2019) also report worse results for contributor classification: almost and percent points regarding macro F-measure for bot detection according to Table 5 and Table 7, respectively. Since the obtained online results contemplate all data samples rather than just the test partition samples, the proposed stream-based method considerably improves the task of classifying contribution and contributor types. Note that some works were not included in this analysis, e.g., since they do not provide directly comparable metrics.\n\nRegarding popular platforms for travel reviews, TripAdvisor provides an extensive database of user-submitted content that can be advantageous when adopting online processing methods. Such platforms require continuous updates to handle incoming data efficiently, allowing the proposed method to shine in real-time application scenarios. The capability to analyze and classify contributions dynamically enhances the relevance and reliability of the information available to users. Thus, leveraging data from TripAdvisor enables ongoing refinement and validation of models, ensuring they remain accurate and effective in rapidly changing environments."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusions",
            "text": "Wiki-based repositories are built by a network of contributors, known as the crowd. The crowd voluntarily shares information related to people, places, entities, etc. This voluntary nature of crowdsourcing platforms facilitates unethical behaviours from ill-intentioned human and bot contributors, enabling fraudulent data manipulation to satisfy third party interests. This scenario raises reliability concerns regarding the data quality of wiki-based platforms. To mitigate these issues, this work proposes a stream-based method to automatically classify contributors and their contributions. Specifically, the overall solution is composed of two distinct components: (i) offline data pre-processing and data simulation; and (ii) stream-based contributor profiling and classification. Pre-processing encompasses data analysis, feature engineering, and feature selection to identify the most promising edition-related features for profiling. The simulation is based on synthetic data generation to solve the deep class imbalance of the original data set. The stream-based profiling incrementally builds the profiles of contributors based on the selected edit features. The classification explores different stream-based binary classifiers with both class-imbalanced and class-balanced data streams to finally choose a stacking multi-class classifier that simultaneously identifies benign and malign humans and bots.\nThe proposed method was tested and evaluated with a real data set from TripAdvisor holding contributions from bots and humans, using classification accuracy and F-measure metrics. The experiments were conducted using the original TripAdvisor data together with synthetic data as streams, which incrementally update contributor profiles and classifier models. The final two-level stacking classifier presents results that vary between and concerning accuracy and F-measure, proving the efficiency of the method.\nTo sum up, this paper describes a solution that can be used to anticipate and isolate malevolent content produced by the identified malign contributors. As future work, the plan is to generate stream-based synthetic data as well as explain classifications, employing Natural Language Processing techniques to generate statements automatically."
        }
    ],
    "url": "http://arxiv.org/html/2405.18845v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.1.1",
            "3.1.2",
            "3.1.3",
            "3.2",
            "3.3",
            "3.4",
            "3.5"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4",
            "4.4.1",
            "4.4.2",
            "4.5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.2",
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4",
            "4.4.1",
            "4.4.2"
        ]
    },
    "research_context": {
        "paper_id": "2405.18845v1",
        "paper_title": "Simulation, Modelling and Classification of Wiki Contributors: Spotting The Good, The Bad, and The Ugly",
        "research_background": "### Motivation:\nThe primary motivation of this paper stems from the ubiquitous reliance on crowdsourced data facilitated by platforms like wikis, which has fundamentally modified online user behavior and decision-making. Despite the integration of Artificial Intelligence (AI) techniques to process user-generated content swiftly, there are significant concerns about algorithmic transparency and data reliability. These concerns arise because of the opacity in AI algorithms and the manipulation of data for third-party interests, manifesting as fake news, biased feedback, and spam content. The impact of such malicious activities, often conducted by bots, on the reliability of crowdsourced platforms necessitates an effective method for real-time profiling and classification of contributors.\n\n### Research Problem:\nThe paper addresses the problem of differentiating between human and bot contributors, as well as between benign and malign contributions on Wikivoyage. This is achieved through the development of a method that relies on data simulation and modelling of wiki streams to enable automatic classification of contributors. The ultimate goal is to furnish a data stream classification that identifies deceitful sources in real-time and thereby precludes attacks on wiki pages.\n\n### Relevant Prior Work:\n1. **Tsvetkova et al. (2017)**: The authors review the nature and types of bots in crowdsourcing systems, categorizing them into content extractors, action executors, content generators, and human emulators, with a distinction between benign and malign behaviours. This classification provides foundational understanding for differentiating among various types of bots that might contribute to Wikivoyage.\n\n2. **ORES API**: The use of ORES (Objective Revision Evaluation Service) edit quality API is cited as a tool for automating vandalism detection and removal in wiki platforms, which informs the profiling of contributors based on the quality of their edits. The inclusion of ORES in the methodology emphasizes the importance of leveraging existing AI tools tailored for wiki services.\n\n### Summary:\nWith the backdrop of these motivations and prior work, the paper embarks on developing a two-level stack-based Machine Learning model to classify Wikivoyage contributors, thereby tackling issues of trust and data integrity on crowdsourcing platforms. The model incrementally updates as reviews come in, balancing real and synthetic data to improve the accuracy and consistency of the classification results.",
        "methodology": "The proposed method to simulate, model, and classify Wiki contributors involves a structured multi-stage process consisting of four primary stages:\n\n(i) **Pre-processing**: This stage begins with the analysis of input data by assessing pairwise correlations (detailed in Section 3.1.1) to identify relationships between different variables. Following this, feature engineering (Section 3.1.2) is employed to select the most relevant features for creating accurate contributor profiles (Section 3.1.3).\n\n(ii) **Synthetic Data Generation**: In Section 3.2, the method generates synthetic data to balance the various classes of contributors and improve the overall effectiveness of the classification models. This synthetic data complements the real data, thus providing a more robust dataset for subsequent analysis.\n\n(iii) **Classification**: The combined dataset (real and synthetic) undergoes incremental processing to model and classify contributors. This is covered in Section 3.3 (modeling) and Section 3.4 (classification). The classification stage leverages well-known algorithms and explores both single-class (binary) and a novel multi-class classification approach using stacking techniques to categorize contributors and their activities.\n\n(iv) **Evaluation**: Finally, the outcomes of the classification are evaluated using standard classification metrics, as discussed in Section 3.5. This ensures that the model's performance can be accurately assessed and validated.\n\nThe proposed method thus integrates a sophisticated process of data pre-processing, synthetic data generation, modeling, classification, and thorough evaluation to effectively profile Wiki contributors and classify their contributions as good, bad, or ugly.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Setup:\n1. **Hardware Specifications:**\n   - **Operating System**: Ubuntu 18.04.2 LTS 64-bit\n   - **Processor**: Intel Core i9-9900K\n   - **RAM**: DDR4\n   - **Disk**: 7200 rpm SATA + SSD\n\n2. **Experiment Components:**\n   - **Offline Feature Analysis**: Selection of the most relevant features for contributor profiling.\n   - **Offline Synthetic Data Generation**: Balancing the classes within the dataset.\n   - **Classification Tasks**: Involved the classification of:\n     - Human vs. Bot contributors.\n     - Benign vs. Malign contributions.\n   - **Scenarios Explored**:\n     - **Scenario (a)**: Stream-based binary classification of human and bot contributors using imbalanced real data.\n     - **Scenario (b)**: Stream-based binary classification of benign and malign contributors using imbalanced real data.\n     - **Scenario (c)**: Stream-based multi-class stacked classification of benign and malign humans and bots using imbalanced (real) data and balanced (real and synthetic) data.\n\n#### Datasets:\n- Data was collected using MediaWiki utilities for feature extraction and processing.\n\n#### Baselines:\n- Scenarios (a) and (b) establish classification baselines utilizing the original, highly imbalanced data for individual target features.\n\n#### Evaluation Metrics:\n- **EvaluatePrequential**: Incremental model updating and evaluation method, where the model is tested and trained for each observed sample.\n\n#### Results:\n- **Scenario (c)** assesses the performance of a stacking classification strategy, aiming at the simultaneous detection of contributor and contribution types, using both imbalanced and synthetically balanced data for a comprehensive multi-class classification task.\n\n(Note: The results are stated in a summary manner without specific numeric outcomes or detailed findings, suggesting readers refer to more explicit sections within the paper for granular results.)"
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the efficacy of synthetic data generation in balancing experimental datasets, thereby enhancing classification accuracy for identifying different types of contributors.",
            "experiment_process": "Synthetic samples were generated using a statistical approach based on the quartile distribution of the most relevant features for the two target categories (human and bot contributors). A non-hierarchical K-means cluster analysis was performed on the original dataset to identify common bot behaviour. The generated data samples were combined with the original data, creating a balanced, class-distributed dataset. Experimentation involved clustering using K-means and subsequent analysis of the generated data for statistical consistency with the original dataset.",
            "result_discussion": "The generated synthetic data closely matched the statistical attributes of the original dataset, confirming the efficacy of the synthetic data generation method. The sole notable deviation was in the 'number of reverted contributions per contributor' feature. The inclusion of synthetic data enabled a balanced class distribution, supporting better model performance.",
            "ablation_id": "2405.18845v1.No1"
        },
        {
            "research_objective": "To analyze the statistical consistency and effectiveness of synthetic data generation in maintaining the quality of experimental datasets.",
            "experiment_process": "The quality of the synthetic data was assessed by comparing the statistical properties and class distribution of both original and generated data. This was performed through Table 4, which provided statistical summaries and relative percentage changes for key attributes. Clustering was executed using K-means, and the class distribution after adding synthetic data was evaluated.",
            "result_discussion": "Results demonstrated minimal variations in most feature distributions between original and synthetic datasets, indicating the synthetic data generation algorithm effectively maintains statistical properties. The only exception was the 'number of reverted contributions per contributor' feature, corroborating the analysis's correctness.",
            "ablation_id": "2405.18845v1.No2"
        },
        {
            "research_objective": "To classify human and bot contributors using various feature sets and evaluate the classification performance.",
            "experiment_process": "A data stream produced from the original class-imbalanced data was used in the experiment. Three different sets of features - basic, full, and selected - were employed. The macro and micro results of the classification were recorded and displayed in Table 5.",
            "result_discussion": "The classification results were consistent across classifiers, with better performance observed when using the full set of features. The most relevant features also yielded promising results with reduced processing time. The rf binary classifier outperformed others, achieving notable effectiveness despite the data imbalance.",
            "ablation_id": "2405.18845v1.No3"
        },
        {
            "research_objective": "To distinguish between benign and malign contributions using binary and ensemble classifiers.",
            "experiment_process": "This experiment involved stream-based binary classification of the original class-imbalanced data. Different feature sets were used for classification. Table 6 presented the macro and micro results for single and ensemble classifiers.",
            "result_discussion": "Distinguishing between positive and negative contributions proved challenging compared to classifying humans and bots. Nonetheless, the proposed method achieved good accuracy and macro F-measure with the rf classifier for the best feature set.",
            "ablation_id": "2405.18845v1.No4"
        },
        {
            "research_objective": "To evaluate the multi-class classification performance of the stacking model and the impact of the balanced data set on accuracy and F-measure.",
            "experiment_process": "The experiment employed two data streams from the original class-imbalanced and a new class-balanced dataset. It aimed at simultaneous classification of contributor type (humans versus bots) and contribution type (positive versus negative). The results from the stacking model were compared with the baseline model using feature set 3 and the rf classifier.",
            "result_discussion": "The stacking model improved accuracy and F-measure compared to the baseline model, demonstrating the positive impact of balanced data. The difference in processing time was attributed to the number of events in each stream, although the process time per event remained consistent between imbalanced and balanced streams.",
            "ablation_id": "2405.18845v1.No5"
        }
    ]
}