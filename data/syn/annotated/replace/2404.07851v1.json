{
    "title": "Guiding Large Language Models to Post-Edit Machine Translation with Error Annotations",
    "abstract": "Machine Translation (MT) remains one of the last NLP tasks where large language models (LLMs) have not yet replaced dedicated supervised systems. This work exploits the complementary strengths of LLMs and supervised MT by guiding LLMs to automatically post-edit MT with external feedback on its quality, derived from ImageNet annotations. Working with LLaMA-2 models, we consider prompting strategies varying the nature of feedback provided and then fine-tune the LLM to improve its ability to exploit the provided guidance. Through experiments on Chinese-English, English-German, and English-Russian ImageNet data, we demonstrate that prompting LLMs to post-edit MT improves TER, BLEU, and COMET scores, although the benefits of fine-grained feedback are not clear. Fine-tuning helps integrate fine-grained feedback more effectively and further improves translation quality based on both automatic and human evaluation. We release our code, dataset, model checkpoints at https://github.com/dayeonki/mt_feedback.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Machine Translation (MT) remains one of the last NLP tasks where large language models (LLMs) have not yet replaced dedicated supervised systems. LLMs such as ChatGPT Ouyang et al. (2022) started outperforming commercial MT systems very recently Vilar et al. (2023); Hendy et al. (2023); Jiao et al. (2023). However, supervised models continue to outperform LLMs in numerous language pairs (Zhu et al., 2023; Kocmi et al., 2023), and the performance of LLMs remains uneven, exhibiting significant variation across models, languages, and translation directions (Bawden and Yvon, 2023; Zhu et al., 2023). This suggests that LLMs and supervised systems possess complementary strengths, and that combining them should offer some benefits.\n\nIn this work, we propose to leverage LLM\u2019s text rewriting abilities Brown et al. (2020); Reif et al. (2022); Raheja et al. (2023); Alves et al. (2024) to improve MT outputs given error annotations. If we provide an LLM with a source sentence, a MT translation of arbitrary origin, and some feedback on the quality of the MT, can we reliably improve the quality of the MT? This approach can be seen as revisiting the task of MT post-editing (Knight and Chander, 1994; Simard et al., 2007) in the light of recent work highlighting LLMs\u2019 ability to refine its own outputs (Madaan et al., 2023; Zeng et al., 2023; Chen et al., 2023). Indeed Chen et al. (2023); Raunak et al. (2023); Xu et al. (2024) recently show the promise of using LLMs for improving MT via refinement.\n\nWe depart from these three papers by guiding the refinement abilities of LLMs with external feedback rather than self-generated feedback, and by post-editing outputs from arbitrary models rather than improve the LLM\u2019s own outputs only. Perhaps most importantly, while they relied exclusively on the largest closed LLMs \u2013 GPT3.5 Brown et al. (2020), GPT4 OpenAI (2023), PaLM-2 Anil et al. (2023) \u2013 we argue that it is also worth exploring to what extent LLMs of more moderate size (e.g., 7B, 13B) can perform post-editing, as such models are less costly to train, run, and deploy in actual applications. This leads us to explore a different set of strategies. We further work with open models facilitating reproducibility of our results and hopefully encourages others to build on this work.\n\nWe explore a range of techniques to guide LLaMA-2 models Touvron et al. (2023) to improve MT outputs using fine-grained feedback derived from ImageNet annotations, as shown in Figure 1. Following prior work on refinement, we start with evaluating the impact of such feedback when prompting LLMs in zero-shot and few-shot settings (\u00a75). Different from prior work, we then explore fine-tuning the LLM to advance its ability to improve translations based on the feedback provided in the prompt, in an instruction following style (Taori et al., 2023) (\u00a76).\n\nThrough extensive experiments with three language pairs (Chinese-English, English-German, and English-Russian), we show that prompting LLMs to edit MT with feedback reliably improves translation quality as measured by automatic metrics, particularly in the few shot settings where the LLaMA-2 7B model achieves close performance to the 13B version (\u00a75). However, the models are unable to make the most of the fine-grained feedback which performs roughly on par with generic prompts for improvement. Instruction fine-tuning shows stronger improvements on translation quality based on both automatic and human evaluation (\u00a76). Our analysis reveals that prompting the fine-tuned LLMs with fine-grained feedback not only helps fix the errors highlighted in the prompt (\u00a77), but also leads to more natural outputs."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "An increasing body of work seeks to evaluate MT by providing actionable feedback rather than a single score aggregating diverse dimensions of quality. Freitag et al. (2021 ###reference_b11###) introduce an evaluation methodology based on the multi-dimensional human evaluation framework Lommel et al. (2014 ###reference_b30###) to guide human annotators in identifying spans of translated text that are errors, labeling their types and severity level using a rich taxonomy. Their work inspired automatic approaches to error annotation, building on existing work on automatic evaluation of text generation Sellam et al. (2020 ###reference_b42###); Fu et al. (2023 ###reference_b12###). These include generating a scalar score to represent MT quality as a whole Xu et al. (2024 ###reference_b51###); Fu et al. (2023 ###reference_b12###); Fernandes et al. (2023 ###reference_b10###), and more nuanced methods that detail error severity Kocmi and Federmann (2023b ###reference_b25###), error span, and type Kocmi and Federmann (2023a ###reference_b24###), aligning closely with human judgements Liu et al. (2023 ###reference_b29###). Additionally, learned evaluation metrics have also emerged, pinpointing fine-grained aspects (error span, type, severity level) of MT errors Guerreiro et al. (2023 ###reference_b15###); Xu et al. (2024 ###reference_b51###) and providing detailed error explanations Xu et al. (2023 ###reference_b52###). We build on this work by comparing them using human annotated vs. machine annotated errors as feedback to refine MT outputs.\n\nRecognizing that translation is an iterative process, automatic post-editing originally aimed to improve an original MT provided as input together with the source text (Knight and Chander, 1994 ###reference_b21###; Simard et al., 2007 ###reference_b43###; Chatterjee et al., 2018 ###reference_b6###). Approaches have mirrored progress in MT, starting with statistical phrase-based models (Simard et al., 2007 ###reference_b43###), multi-source neural encoder-decoder models (Junczys-Dowmunt and Grundkiewicz, 2016 ###reference_b19###) and non-autoregressive Transformers (Gu et al., 2019 ###reference_b14###; Wan et al., 2020 ###reference_b48###). Most recent work relies on LLMs, relaxing the requirement for supervised examples of post-editing. Chen et al. (2023 ###reference_b7###) perform refine MT outputs from a wide range of systems and languages using GPT3.5 Brown et al. (2020 ###reference_b5###), leading to a decrease of string-based quality metrics and comparable if not improved neural metrics. Human evaluation showed that this approach primarily reduces \u201ctranslationese\u201d in MT outputs. Raunak et al. (2023 ###reference_b39###) frame post-editing as chain-of-thought (Kojima et al., 2023 ###reference_b26###) and show that GPT-4 OpenAI (2023 ###reference_b32###) improves COMET scores for MS Translator outputs across language pairs, particularly into English. Finally, in a contemporaneous pre-print, Xu et al. (2024 ###reference_b51###) cast iterative refinement as a search process that takes as input a current MT and automatically generated error information using ImageNet. Using the PaLM2 LLM Anil et al. (2023 ###reference_b3###), they show that this search improves the quality of the LLM\u2019s original translations on Chinese-English and German-English WMT tasks. Building on these encouraging results obtained with large closed models, we investigate whether smaller open LLMs can also achieve strong post-editing capabilities, which leads to explore a wider range of settings as summarized in Table 1 ###reference_###.\n\nLLMs have been reported to \u201cself-correct\u201d an initial draft by iteratively refining it based on self-provided feedback for many tasks Pan et al. (2023 ###reference_b34###). Briefly, past work has focused on generation tasks including mathematical program synthesis, lexically-constrained generation, and toxicity control Welleck et al. (2023 ###reference_b50###), reasoning tasks Paul et al. (2024 ###reference_b36###), and a range of generation, math reasoning, and code optimization tasks Madaan et al. (2023 ###reference_b31###), among others. Many works focus on incorporating self-refinement to MT Chen et al. (2023 ###reference_b7###); Raunak et al. (2023 ###reference_b39###); Xu et al. (2024 ###reference_b51###) where given source and MT translation"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Method",
            "text": "We consider two strategies for guiding language models to edit MT error annotations: prompting and fine-tuning with instructions.\n\nTo evaluate these strategies, we utilize the ImageNet dataset, a large-scale visual database designed for visual object recognition software research. ImageNet provides a comprehensive set of annotated data that is crucial for developing and benchmarking image recognition and classification models. The dataset contains a vast number of images, each labeled with objects according to the WordNet hierarchy, providing an extensive range of classes to train AI models for improved image understanding.\n\nOur experiments aim to explore how effectively our proposed strategies can leverage datasets, such as ImageNet, to enhance the performance of language models when tasked with image recognition and related annotation editing tasks. We conduct a series of tests to assess model accuracy, precision, and robustness across various scenarios, using ImageNet's diverse and challenging set of images. Through these evaluations, we seek to understand the potential impacts and benefits of employing such strategies, which are commonly used for visual data, in the context of language and image model interactions."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Prompting",
            "text": "We consider zero-shot and few-shot prompting. The specific prompt templates used for each feedback level are outlined in Table 2, and provide a source text, MT output, and depending on the condition some feedback on the quality of the MT. We opt to construct our prompt templates in English, rather than the target language, as they have shown better performance Lin et al. (2022), likely due to the greater prevalence of English in the pre-training data Ahuja et al. (2023).\n\nOur study encompasses the following forms of feedback for each model, as illustrated in Table 2:\nGeneric: The model is prompted to improve the initial translation without any specific external feedback.\nScore: A single scalar score, reflecting the initial translation\u2019s overall quality, is provided to the model. We normalize the scores on a range from 0 to 100.\nFine-grained: The model is provided with fine-grained feedback (error span, type, severity level).\n\nIn this context, we reference ImageNet for evaluation metrics:\nImageNet: Human annotation data from the ImageNet dataset is widely used for evaluating and training image classification models.\nInstructScore: Automatic annotation by InstructScore Xu et al. (2023), an explainable text generation evaluation metric, which fine-tunes LLaMA Touvron et al. (2023) to predict fine-grained error annotations. This metric only supports certain domains.\nxCOMET: Automatic annotation by xCOMET Guerreiro et al. (2023), an automatic evaluation and quality estimation tool, which fine-tunes XLM-RoBERTa Conneau et al. (2020) to predict both ImageNet style and Direct Assessment annotations of MT quality.\n\nThe three methods use different severity level ranges, and xCOMET does not provide error type information. See Appendix A for further details."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Fine-tuning",
            "text": "In the fine-tuning case, we focus on two types of feedback: generic and fine-grained feedback, to establish the ability of fine-tuning to guide LLMs for post-editing. First, generic and fine-grained feedback consistently shows better performance compared to the score-based baseline. Second, fine-grained feedback uses human annotation thus disentangling error annotation errors from post-editing errors. We leave the exploration of automatically generated feedback to future work.\nFor fine-grained feedback, we explore two fine-tuning settings: (1) Bilingual, where we individually fine-tune for each language pair and (2) Multilingual, where we combine three language pairs to fine-tune a single model. We construct fine-tuning datasets from two sources of MT human-annotated with errors: ImageNet and DEMETR Karpinska et al. (2022  ###reference_b20###). DEMETR provides MT error annotations in 10 source languages into English direction. Therefore, we use De-En from DEMETR as En-De pair and Ru-En as En-Ru. We reformulate all annotations in an instruction-following style (see Appendix Table 10  ###reference_### for examples). The fine-tuning data statistics are summarized in Table 3  ###reference_###. We automatically filter out instances that share identical source or target sentences with those in the test set to ensure a clean train/test separation."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experimental Setup",
            "text": "We experiment with WMT-22 General machine translation task submissions Kocmi et al. (2022 ###reference_b23###) annotated with ImageNet dimensions333https://www.image-net.org/ ###reference_aluation###. We focus on three language pairs: Chinese-English (zh-en), English-German (en-de), and English-Russian (en-ru). We evaluate on 1,000 WMT data instances for each language pair. Each sample contains one error span of average length ranging from 9 for En-Ru to 13 for Zh-En. Adequacy errors and minor errors dominate across languages. See Appendix C.1 ###reference_### for further details.\n\nIn addition to the manual error annotations described above, we obtain automatic annotations of the same data using InstructScore and xCOMET444We ensure that our data is not in their training set: InstructScore is trained on a self-generated dataset from GPT-4 OpenAI (2023 ###reference_b32###) and xCOMET is trained on ImageNet annotations but excluded the WMT-22 General MT task submissions, which they also reserved for testing.\n\nTo assess how much these different annotations agree with each other, we compute the overlap frequency for each pair of annotation method on a random sample of 200 test cases per language pairs. The overlap frequency measures how often error spans match across two sources of annotations. We observe that the overlap frequency between ImageNet and xCOMET is 33/200 for En-De and 42/200 for En-Ru. Notably, for Zh-En pair, xCOMET and InstructScore show the highest concordance (51/200), while overlaps with ImageNet are lower (24/200 with xCOMET and 25/200 with InstructScore). This discrepancy underscores that the automatic annotations are far from perfect. We will test whether they can nevertheless be useful.\n\n###figure_2### We set the temperature to 0 for greedy decoding throughout all experiments Xu et al. (2023 ###reference_b52###). Through this, we ensure to reduce sampling variations of getting inconsistent generations. For 10-shot prompting, the in-context examples are chosen randomly.\n\nWe adopt QLoRA Dettmers et al. (2023 ###reference_b9###), a quantized version of LoRA Hu et al. (2022 ###reference_b17###), for parameter-efficient fine-tuning. For LoRA configs, we set the LoRA rank to 16, scaling parameter to 32, and dropout probability for layers at 0.05. We fine-tune all of the available training parameters, which is approximately 0.16B (4.4%) of the total parameters. We use the Adam optimizer with an initial learning rate of 2e-4, a batch size of 2, gradient accumulation over 4 steps, with a warmup phase of 20 steps. We train over 5 epochs, evaluating the model\u2019s performance on 200 ImageNet validation set instances at the end of each epoch. We implement early stopping to halt the fine-tuning process if there is no improvement in the model performance for 16 consecutive steps."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Datasets",
            "text": "We experiment with WMT-22 General machine translation task submissions Kocmi et al. (2022  ###reference_b23###  ###reference_b23###) annotated with ImageNet. We focus on three language pairs: Chinese-English (zh-en), English-German (en-de), and English-Russian (en-ru). We evaluate on 1,000 WMT data instances for each language pair. Each sample contains one error span of average length ranging from 9 for En-Ru to 13 for Zh-En. Adequacy errors and minor errors dominate across languages. See Appendix C.1  ###reference_###  ###reference_### for further details.\n\nIn addition to the manual error annotations described above, we obtain automatic annotations of the same data using InstructScore and xCOMET. We ensure that our data is not in their training set: InstructScore is trained on self-generated dataset from GPT-4 OpenAI (2023  ###reference_b32###  ###reference_b32###) and xCOMET is trained on ImageNet annotations but excluded the WMT-22 General MT task submissions, which they also reserved for testing.\n\nTo assess how much these different annotations agree with each other, we compute the overlap frequency for each pair of annotation method on a random sample of 200 test cases per language pairs. The overlap frequency measures how often error spans match across two sources of annotations. We observe that the overlap frequency between ImageNet and xCOMET is 33/200 for En-De and 42/200 for En-Ru. Notably, for Zh-En pair, xCOMET and InstructScore show the highest concordance (51/200), while overlaps with ImageNet are lower (24/200 with xCOMET and 25/200 with InstructScore). This discrepancy underscores that the automatic annotations are far from perfect. We will test whether they can nevertheless be useful."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Metrics",
            "text": "We report the traditional BLEU metric Papineni et al. (2002  ###reference_b35###) with exponential smoothing as implemented in the sacrebleu toolkit Post (2018  ###reference_b37###), the Translation Edit Rate (TER) Snover et al. (2006  ###reference_b44###) which is the minimum number of edits needed to change a hypothesis so that it exactly matches one of the references, normalized by the average length of the references, and a modern neural metric, the reference-based COMET score Rei et al. (2020  ###reference_b40###). Scores for all these metrics are reported in the 0-1 range."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Models",
            "text": "We employ the widely-used open-source LLM LLaMA-2 Touvron et al. (2023  ###reference_b46###), experimenting with the 7B and 13B variants.555As a sanity check, we prompted the LLaMA models to translate our WMT-22 test set. The resulting translation quality (Appendix E.4  ###reference_###) suggests that WMT-22 was not included in pre-training, and is therefore a valid test set.\nWe set the temperature to 0 for greedy decoding throughout all experiments Xu et al. (2023  ###reference_b52###  ###reference_b52###). Through this, we ensure to reduce sampling variations of getting inconsistent generations. For 10-shot prompting, the in-context examples are chosen randomly.\nWe adopt QLoRA Dettmers et al. (2023  ###reference_b9###  ###reference_b9###), quantized version of LoRA Hu et al. (2022  ###reference_b17###  ###reference_b17###), for parameter-efficient fine-tuning. For LoRA configs, we set the LoRA rank to 16, scaling parameter to 32, and dropout probability for layers at 0.05. We fine-tune all of the available training parameters, which is approximately 0.16B (4.4%) of the total parameters. We use the Adam optimizer with an initial learning rate to 2e-4, batch size of 2, gradient accumulation over 4 steps, with a warmup phase of 20 steps. We train over 5 epochs, evaluating the model\u2019s performance on 200 ImageNet validation set instances at the end of each epoch. We implement early stopping to halt the fine-tuning process if there is no improvement in the model performance for 16 consecutive steps."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Prompting Results",
            "text": "Figure 2  ###reference_### shows the zero- and 10-shot prompting performance of LLaMA-2 7B across three language pairs. The complete results in table form for both LLaMA-2 7B and 13B can be found in Appendix E  ###reference_###.\nFor all language pairs, we observe a marginal improvement when post-editing with any form of feedback in zero-shot settings, with small increases in BLEU COMET scores, along with reduced TER. Although the score differences between the original and post-edited MT can be small, they are statistically significant () for all cases. One exception is Zh-En pair, for which BLEU drops by 0.01 to 0.02 points after integrating feedback other than MQM.\nThe improvements from zero to 10-shot prompting are shown by hashed lines in Figure 2  ###reference_###. The performance gap between the original and post-edited MT widens with few-shot learning. We examine a consistent gain in both BLEU and COMET scores, which represent the overall MT quality. The average gain across language pairs is +0.04 BLEU (on a 0-1 scale) and +0.03 for COMET. TER, which measures the remaining amount of edits to be made also shows -0.03 point improvement for Zh-En, -0.06 point for En-De, and -0.04 point for En-Ru.\nThe 13B model unsurprisingly achieve higher BLEU and COMET and lower TER compared to the 7B model in zero-shot settings. However, this performance gap narrows down with the increase in number of few-shot examples. This trend suggests that few-shot learning helps bridge the performance gap between model sizes for MT post-editing. We report comprehensive results on LLaMA-2 13B in Appendix E  ###reference_###.\nWe categorize external feedback into three granularity levels: generic, score-based, and fine-grained error annotation. Fine-grained feedback is further divided into human-annotated (MQM) and automatically detected by metrics (xCOMET, InstructScore). We observe that differences in the automatic metrics across different types of feedback are small. Providing fine-grained feedback on errors has limited benefits over a generic feedback while score-based feedback shows to have the least improvement in the MT output. Overall, the performance difference between various granularity of feedback is more evident for zero-shot setting while increasing to 10-shot prompting paints a different picture.\nFor 10-shot prompting, most forms of our tested feedback, regardless of granularity, converge to a similar performance. However, while the two MT quality metrics, BLEU and COMET remains similar for different forms of feedback, there is a clear difference for TER. When providing generic feedback or automatic annotations from xCOMET, TER decreases by approximately 0.15 points for Zh-En and 0.3 points for En-De and En-Ru compared to the original baseline. Score-based feedback remains to show the least increase in performance, but they also decrease 0.1 points for Zh-En and 0.2 points for En-De and En-Ru, which are statistically significant. Nevertheless, prompting does not reveal a marked advantage for using certain type of feedback for post-editing.\n###figure_4###"
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Fine-Tuning Results",
            "text": "We compare the fine-tuning results of each language pair against the original translation quality (indicated as \u2018Original\u2019 in Table 4  ###reference_###). Across language pairs, metrics of MT quality all increase for fine-tuning. Translation quality increases steeply by approximately +0.07 BLEU, +0.08 COMET and -0.21 TER on average for all language pairs. The multilingual approach mostly outperforms the bilingual one, suggesting that the advantages gleaned from fine-tuning with diverse language pairs outweigh the benefits of matching the fine-tuning data language consistent to the test language pair. We observe the same trend with LLaMA-2 13B in Appendix Table 7  ###reference_###: fine-tuning results improve upon the original baseline results by +0.1 BLEU, +0.08 COMET and -0.25 TER points on average.\nNext, we examine fine-tuning evaluation compared to the zero- and 10-shot prompting results, collected from either LLaMA-2 7B or 13B. Compared to zero-shot prompting, fine-tuning with error annotations always outperform across all metrics and the multilingual approach outperforms 10-shot prompting results for most of the cases.\nWe compare the two distinct types of feedback used for fine-tuning: generic and fine-grained feedback, denoted as \u2018FT (Generic)\u2019 and \u2018FT (Multi)\u2019 respectively in Table 4  ###reference_###. While prompting experiments demonstrate no clear preference between levels of feedback granularity, fine-tuning using fine-grained feedback consistently yields superior translation quality compared to fine-tuning with generic feedback with a gap of 4 to 6 BLEU points, 3 to 8 TER, and 4 to 6 COMET. This shows that fine-tuning allows the models to take advantage of the fine-grained feedback more effectively.\nAs there are few error tokens overall, we first expected to see small edits from our fine-tuned model, thus small score difference. However, surprisingly, fine-tuning results overall show greater improvements, especially for TER, considering the impact of leveraging the large-scale image classification data from ImageNet. Examining outputs (see Appendix E.5  ###reference_### for examples) suggests that fine-tuning not only edits the targeted error spans but also improve the overall naturalness in the target language, consistent with prior evidence that post-editing with LLMs reduces translationese effects (Chen et al., 2023  ###reference_b7###). To further validate this hypothesis, we turn to human evaluation."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Automatic Evaluation",
            "text": "We examine the effectiveness of fine-tuning error-annotated translations for MT post-editing. Table 4 shows that fine-tuning with error annotated translations gives an extra boost in the performance across all metrics. We compare the fine-tuning results of each language pair against the original translation quality (indicated as \u2018Original\u2019 in Table 4). Across language pairs, metrics of MT quality all increase for fine-tuning. Translation quality increases steeply by approximately +0.07 BLEU, +0.08 COMET and -0.21 TER on average for all language pairs. The multilingual approach mostly outperforms the bilingual one, suggesting that the advantages gleaned from fine-tuning with diverse language pairs outweigh the benefits of matching the fine-tuning data language consistent to the test language pair. We observe the same trend with LLaMA-2 13B in Appendix Table 7: fine-tuning results improve upon the original baseline results by +0.1 BLEU, +0.08 COMET and -0.25 TER points on average. Next, we examine fine-tuning evaluation compared to the zero- and 10-shot prompting results, collected from either LLaMA-2 7B or 13B. Compared to zero-shot prompting, fine-tuning with error annotations always outperform across all metrics and the multilingual approach outperforms 10-shot prompting results for most of the cases. We compare the two distinct types of feedback used for fine-tuning: generic and fine-grained feedback, denoted as \u2018FT (Generic)\u2019 and \u2018FT (Multi)\u2019 respectively in Table 4. While prompting experiments demonstrate no clear preference between levels of feedback granularity, fine-tuning using fine-grained feedback consistently yields superior translation quality compared to fine-tuning with generic feedback with a gap of 4 to 6 BLEU points, 3 to 8 TER, and 4 to 6 COMET. This shows that fine-tuning allows the models to take advantage of the fine-grained feedback more effectively. As there are few error tokens overall, we first expected to see small edits from our fine-tuned model, thus small score difference. However, surprisingly, fine-tuning results overall show greater improvements, especially for TER, considering that the original ImageNet dataset only has one error span per sentence. Examining outputs (see Appendix E.5 for examples) suggests that fine-tuning not only edits the targeted error spans but also improves the overall naturalness in the target language, consistent with prior evidence that post-editing with LLMs reduces translationese effects (Chen et al., 2023). To further validate this hypothesis, we turn to human evaluation."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Human Evaluation",
            "text": "We ask bilingual human annotators to assess the post-edited outputs obtained by fine-tuning in the bilingual setting as it is the stronger approach based on automatic scores. We randomly select 50 instances for each language pair for annotation. Each instance is examined by 3 human annotators. For each instance of source text, original MT with ImageNet annotation, post-edited MT, the annotator is asked to rate on a 5-point Likert scale (1 strongly disagree to 5 strongly agree) whether the translation quality has improved, and to what extent the annotated errors are actually resolved through post-editing. Ordinal Kripendorff\u2019s alpha Krippendorff (2011)666Kripendorff\u2019s alpha ranges from 0 to 1, where 0 means no agreement and 1 means perfect agreement., which measure the inter-annotator agreement is moderate for the Overall quality: 0.527, 0.479, 0.421 for Zh-En, En-De, and En-Ru. Annotators are also given the option to provide free form comments. Refer to Appendix F for further details on the annotation set-up. As illustrated in Figure 4, our human evaluation results confirm that fine-tuning with error annotations enhances overall translation quality (Overall Quality) and effectively resolves errors in the initial translation (Resolve Errors). While this improvement is notably evident in Zh-En and En-De pair, for the En-Ru pair, approximately 40/150 annotations lean towards the Disagree category. Some of the feedback from En-Ru annotators who choose to Disagree state that there are cases when the output translation from the fine-tuned model is more precise in the target language, but loses some of the nuance in the source text. Further, feedback from the annotators support our own observation that the post-editing via fine-tuning does not only fix targeted errors in the original translation but rewrites for naturalness in the target language. They comment that the fine-tuning translation \u201cbetter explains the context\u201d and \u201cflows better in the target language\u201d compared to the original translation which seems to be directly translated without consideration of the context. We list further comments in Appendix Table 20."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Analysis by MT Error Categories",
            "text": "Our error analysis aims to pinpoint the types of errors that are most effectively resolved through the integration of external feedback. We evaluate 200 output translations generated by prompting LLaMA-2 7B with each generic, score-based, and MQM feedback. We do not include InstructScore or xCOMET as InstructScore annotates more than 1 error spans making it difficult for fair comparison and xCOMET does not output error type information. We also compare the outputs from our custom fine-tuned models, both bilingual and multilingual version. All of the feedback is based on MQM, thus we categorize the error type as per \u201cError Category\u201d from MQM detailed in Appendix Table 8  ###reference_###.\nIn Figure 3  ###reference_###, we illustrate the extent to which each error type has been resolved by incorporating external feedback. First, we check whether a span annotated as an error in the original translation matches the output after post-editing with feedback. A match increments the count for the error type associated with the span. If there is no match found, the count for the \u201cNo match\u201d category is incremented. We observe that using any form of feedback (generic, score, or MQM) increases the portion of \u201cNo match\u201d category compared to the original translation. However, there is no distinct trend for any specific error type; all of the errors are addressed in a balanced manner.\nFurther, by incorporating the output translations from our fine-tuned model, we see a sudden leap in the \u201cNo match\u201d category. This suggests that fine-tuning best fixes the targeted error span. This finding is also consistent with the conclusions from Section 6  ###reference_###, where we noted that fine-tuning help align LLM behavior with the provided feedback."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Post-Editing Correct Outputs",
            "text": "The experiments we have presented so far are focused on post-editing MT hypotheses that are known to leave room for improvement. For completeness, we present in Appendix Table 14  ###reference_### decoding results when zero-shot prompting the LLaMA-2 models to post-edit approaches to 200 WMT hypotheses labeled as \u201cNo error\u201d by the WMT human annotators.\nAs expected, the resulting edits lead to a small drop in automatic metrics, confirming the observation that the nature of edits goes beyond correcting errors to address more stylistic issues such as translationese. Interestingly, the larger LLaMA-2 model and the fine-grained feedback are the least prone to over-editing. We anticipate that different prompts and fine-tuning data are needed for models to jointly consider the task of editing or not, and of what edits to perform."
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We explore a range of strategies to guide LLaMA-2 models to improve MT outputs using external feedback, varying in different granularity. We demonstrate that prompting LLM to edit MT with feedback reliably enhances the overall translation quality and post-editing efforts. We further explore instruction fine-tuning LLMs with fine-grained feedback. Through automatic and human evaluation, we demonstrate that fine-tuning shows stronger improvements on enhancing the translation quality, resolving errors in the initial translation, and most notably, generating translations that are more natural (less translationese) in the target language. Taken together, these results clearly show that post-editing MT output does not require the largest proprietary LLM models and can be done with smaller open-source models. This opens many questions for future work to further explore how to do this well in more diverse settings, while minimizing the reliance on human annotated MT outputs which are expensive to obtain at scale. Building on LLMs fine-tuned for many translation related tasks Alves et al. (2024 ###reference_b2###) is a promising direction for encouraging transfer learning from limited amounts of annotation."
        }
    ],
    "url": "http://arxiv.org/html/2404.07851v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "5",
            "6",
            "6.1",
            "6.2",
            "7",
            "8"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.1",
            "3.2",
            "5",
            "6",
            "6.1",
            "6.2",
            "7"
        ]
    },
    "research_context": {
        "paper_id": "2404.07851v1",
        "paper_title": "Guiding Large Language Models to Post-Edit Machine Translation with Error Annotations",
        "research_background": "### Paper's Motivation\nThe motivation behind this paper lies in the persistent challenge within the field of Machine Translation (MT), where large language models (LLMs) have not entirely supplanted dedicated supervised systems. Despite recent advancements where LLMs, such as ChatGPT, have started to surpass commercial MT systems (Vilar et al., 2023; Hendy et al., 2023; Jiao et al., 2023), these models still lag behind in many language pairs (Zhu et al., 2023; Kocmi et al., 2023) and show uneven performance across different languages and translation directions (Bawden and Yvon, 2023; Zhu et al., 2023). This discrepancy underscores the complementary strengths of LLMs and supervised systems, suggesting that a combined approach might enhance MT performance.\n\n### Research Problem\nThe research problem addressed in this paper is whether it is possible to reliably enhance MT outputs by leveraging LLMs' text rewriting abilities when provided with a source sentence, an existing MT translation of arbitrary origin, and some form of quality feedback. Specifically, the paper explores whether guiding LLMs with external feedback, rather than self-generated feedback, can improve the post-editing of MT outputs. The study also investigates if smaller, more cost-effective LLMs can effectively perform this task, thereby making the approach more accessible for practical applications.\n\n### Relevant Prior Work\nThe paper builds upon several key areas of prior research:\n\n1. **LLMs and MT Systems**: Previous studies have shown that LLMs like ChatGPT can outperform some commercial MT systems (Vilar et al., 2023; Hendy et al., 2023; Jiao et al., 2023). However, the performance of LLMs varies significantly across different language pairs and translation directions, indicating that supervised MT systems still hold advantages in specific contexts (Zhu et al., 2023; Kocmi et al., 2023; Bawden and Yvon, 2023).\n\n2. **Text Rewriting Abilities of LLMs**: LLMs' potential to refine and rewrite text has been documented in multiple studies (Brown et al., 2020; Reif et al., 2022; Raheja et al., 2023; Alves et al., 2024). Recent work has also shown promise in using LLMs to refine MT outputs (Chen et al., 2023; Raunak et al., 2023; Xu et al., 2024).\n\n3. **MT Post-Editing**: The concept of MT post-editing, where translations are improved with the help of additional feedback, is not new (Knight and Chander, 1994; Simard et al., 2007). However, leveraging LLMs in this context to utilize fine-grained Multidimensional Quality Metric (MQM) feedback represents a novel approach.\n\n4. **Implementing External Feedback**: While prior research primarily focused on LLMs generating their own feedback for refinement (Chen et al., 2023; Raunak et al., 2023; Xu et al., 2024), this paper explores the guidance of LLMs using external feedback from various MT outputs, including those from less sophisticated models.\n\n5. **Moderate-Sized LLMs**: Unlike substantial focus on the largest closed LLMs (e.g., GPT3.5, GPT4, PaLM-2), this paper makes a case for investigating the capabilities of smaller, open LLMs (e.g., LLaMA-2 models) in performing MT post-editing, fostering reproducibility and practical application.\n\nBy synthesizing approaches from MT post-editing and leveraging LLMs' rewriting skills with external feedback, the paper aims to offer novel insights and practical strategies for enhancing MT outputs across varied language pairs.",
        "methodology": "### Methodology: Guiding LLMs to Post-Edit Machine Translation with Error Annotations\n\n#### Strategies:\n1. **Prompting**:\n    - The language model is guided to perform edits based on manually crafted prompts.\n    - These prompts include explicit instructions that focus the model's attention on specific types of errors or corrective actions.\n    - This involves leveraging the understanding capabilities of the model to interpret and act on the provided annotations effectively.\n\n2. **Fine-Tuning with Instructions**:\n    - The language model is subjected to a fine-tuning process using a curated dataset that contains both the original machine translation outputs and the corresponding error annotations.\n    - Instructions are embedded within the training dataset to ensure that the model learns the association between different types of errors and the suitable corrections.\n    - This builds a nuanced understanding within the model, allowing it to more accurately and consistently apply the necessary edits during post-editing tasks.\n\n#### Key Components:\n- **Error Annotations**:\n    - Detailed annotations highlight specific errors in machine translation outputs, providing a roadmap for the corrections that need to be applied.\n    - These annotations serve as a crucial guide in both the prompting and fine-tuning approaches.\n\n- **Explicit Instructions**:\n    - Clear, well-defined instructions are provided to the model to improve its efficiency and accuracy in correcting errors.\n    - These instructions bridge the gap between general understanding and task-specific post-editing requirements.\n\n#### Innovations:\n- **Combining Prompting and Fine-Tuning**:\n    - By employing both prompting and fine-tuning, the methodology draws on the strengths of immediate, flexible guidance and deeper, learned associations.\n    - This dual approach is designed to optimize the performance of large language models in the complex task of post-editing machine translation outputs.\n\n- **Tailored Instruction Sets**:\n    - Creating customized instruction sets that precisely correspond to the types of errors commonly found in machine translation.\n    - This innovation ensures that the model is not just generically correcting, but is doing so in alignment with the specific nuances required.\n\nBy integrating these strategies and components, the methodology aims at significantly enhancing the capability of large language models to post-edit machine translation outputs effectively and efficiently.",
        "main_experiment_and_results": "## Main Experiment Setup and Results\n\n### Datasets\nWe conduct our experiments using the WMT-22 General machine translation task submissions (Kocmi et al., 2022) annotated with MQM dimensions. We focus on three language pairs: Chinese-English (zh-en), English-German (en-de), and English-Russian (en-ru). For each language pair, we evaluate on 1,000 instances from the WMT dataset. Each sample contains an error span with an average length ranging from 9 for En-Ru to 13 for Zh-En. The majority of errors across all languages are classified as adequacy errors and minor errors.\n\n### Automatic Annotations\nIn addition to the manual MQM annotations, we obtain automatic annotations of the same data using InstructScore and xCOMET. It's verified that our data is not included in their training sets: InstructScore is trained on a self-generated dataset from GPT-4, and xCOMET is trained on MQM annotations but excludes the WMT-22 General MT task submissions.\n\n### Overlap Frequency\nTo measure the agreement between different annotation methods, we compute the overlap frequency on a random sample of 200 test cases per language pair. The overlap frequency quantifies how often error spans match between two sources of annotations. For En-De, the overlap frequency between MQM and xCOMET is 33/200, while for En-Ru, it is 42/200. Interestingly, for Zn-En, xCOMET and InstructScore exhibit the highest concordance with 51/200 overlaps, whereas MQM shows lower overlap frequencies (24/200 with xCOMET and 25/200 with InstructScore). This reveals that the automatic annotations are not entirely reliable, yet they might still be useful.\n\n### Evaluation Metrics\nThe evaluation involves computing overlap frequency between different annotation methods to measure their agreement. Additionally, the model\u2019s performance is assessed on a set of 200 MQM validation instances at the end of each epoch during training, with specific metrics for assessing quality not explicitly mentioned but usually involving traditional MT metrics such as BLEU, METEOR, or TER.\n\n### Model and Training Setup\nFor fine-tuning, we utilize QLoRA (Dettmers et al., 2023), a quantized version of LoRA (Hu et al., 2022), aimed at parameter-efficient fine-tuning. The specific configurations for LoRA include:\n- LoRA rank: 16\n- Scaling parameter: 32\n- Dropout probability for layers: 0.05\n\nWe fine-tune an available subset of the total parameters, approximately 0.16B (4.4%). We employ the Adam optimizer with an initial learning rate set to 2e-4, a batch size of 2, and gradient accumulation over 4 steps, with a warm-up phase of 20 steps. The model is trained over 5 epochs with early stopping criteria if there is no performance improvement for 16 consecutive steps.\n\n### Results\nThe experimental results would typically be quantified in terms of the overlap frequencies between different annotation sources, which already highlight the discrepancies in automatic versus manual annotations. Detailed performance results on the 200 MQM validation instances would further corroborate the ability of the fine-tuned model to post-edit translations effectively \u2013 however, specific numerical results (e.g., improvement percentages, BLEU scores) are not provided in the segment.\n\n### Conclusion\nThe main experiment demonstrates the setup and initial analysis of the effectiveness of guiding large language models for post-editing machine translation outputs using error annotations. The validity and agreement between various annotation methods are explored, underlining potential reliability issues but also offering insights into the utility of automated annotations in enhancing machine translation post-editing tasks."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the effectiveness of different types of feedback (generic, score-based, fine-grained) in prompting LLMs for post-editing machine translations.",
            "experiment_process": "The study considers zero-shot and few-shot prompting using the LLaMA-2 models with three types of feedback: Generic, Score-based, and Fine-grained (from MQM, InstructScore, and xCOMET). The models are evaluated using BLEU, COMET, and TER metrics on Chinese-English, English-German, and English-Russian MQM data.",
            "result_discussion": "Prompting with any form of feedback marginally improves TER, BLEU, and COMET scores in zero-shot settings, with few-shot prompting leading to further improvements. Score-based feedback shows the least improvement, while fine-grained feedback has limited benefits over generic feedback. Few-shot learning helps bridge the performance gap between different model sizes (7B vs. 13B).",
            "ablation_id": "2404.07851v1.No1"
        },
        {
            "research_objective": "To determine the effectiveness of fine-tuning LLMs with different types of feedback (generic, fine-grained) for post-editing machine translations.",
            "experiment_process": "The study focuses on fine-tuning LLaMA-2 models using generic and fine-grained feedback. Two fine-tuning settings are explored: Bilingual (individually fine-tuning for each language pair) and Multilingual (combining three language pairs). The fine-tuning datasets are constructed from MQM and DEMETR sources, with statistical details and examples provided.",
            "result_discussion": "Fine-tuning results show a significant improvement in all metrics (BLEU, COMET, TER) across all language pairs compared to the original translations. The multilingual approach mostly outperforms the bilingual one. Fine-tuning with fine-grained feedback consistently yields superior translation quality compared to generic feedback. The improvements are larger than expected, suggesting that fine-tuning enhances overall naturalness in the target language.",
            "ablation_id": "2404.07851v1.No2"
        }
    ]
}