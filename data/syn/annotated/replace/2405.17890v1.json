{
    "title": "SLMRec: Empowering Small Language Models for Sequential Recommendation",
    "abstract": "The CIFAR-10 dataset is a well-known benchmark in machine learning for image classification tasks. It consists of 60,000 32x32 color images in 10 different classes, with 6,000 images per class. The dataset is divided into 50,000 training images and 10,000 test images, making it a widely used standard for evaluating the performance of various machine learning algorithms. In this paper, we examine the performance of small language models (SLMs) tailored for the CIFAR-10 dataset, exploring how these models can effectively learn from the visual data while maintaining efficiency. Motivated by insights from recent research, we implement a knowledge distillation method to improve the computational efficiency and accuracy of SLMs on CIFAR-10. Our findings suggest that even smaller models can achieve competitive results by leveraging specialized training techniques. The experiments reveal that using SLMs can reduce the parameter count significantly compared to traditional larger models while achieving similar or superior accuracy on the CIFAR-10 dataset. Moreover, this reduction in model size leads to substantial improvements in training and inference times, which is crucial for real-world deployment scenarios where processing speed and resource utilization are critical factors.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Learning temporal interest information is fundamental for sequential recommendation models. Traditional sequential recommendation (TSR) methods [52; 18; 29; 46] focus on the development of intricate sequential encoders, evolving from LSTM and GRU architectures to the self-attention layers and Transformer models. However, the state-of-the-art performance in TSR has hit a plateau, limited by model sizes that usually feature fewer than 0.1 billion parameters. Recently, Large Language Models (LLMs) [1; 47; 3] have made significant advancements in various aspects by scaling the size of the training data or the model\u2019s architecture. Building upon the scaling laws delineated in prior research [30; 20], it endows Large Language Models (LLMs) with enhanced expressivity, culminating in superior performance benchmarks. Naturally, a burgeoning trend among contemporary LLM-based recommendation architectures has raised concerns. The current LLM-based recommender system can be classified as 1) generation-based approaches, e.g., P5 [12; 54], CoLLM [57] and LLaRa [38]; 2) embedding-based approaches such as E4SRec [35], CLLM4Rec [59] and Lite-LLM4Rec [50]. As shown in Fig. 1, generation-based approaches (G-LLMRec) encode an item as a token and formulate the sequential recommendation as the next token prediction task. By contrast, embedding-based approaches (E-LLMRec) regard the last hidden representation as user representation and learn an external adapter to compute user-item preference. The adoption of LLMs has vastly driven the development of sequence recommendation tasks, bringing an improvement of nearly 20% against the TSR model on the benchmark [35; 38; 50]. This arouses the following research motivation for this work. So far on the SR scene, little literature studies how large the language model is needed. Recent studies [37; 14; 40] on the NLP domain suggest a high degree of redundancy in the LLM\u2019s model architecture. Since the ID information has not been explicitly learned during the LLM\u2019s training process, we also want to find out whether the scaling law of LLMs is also applicable to the SR task, specifically, how the relation is between the size of LLMs and their performance on SR task. Despite the large performance gain, the LLM-based SR solutions also escalate the model size significantly, e.g., nearly 70 times greater parameters compared with TSR models (from 0.1B to 7B+). Even within the parameter-efficient training technique [24], the paradigm still poses a significant challenge for real-world sequential recommendation use cases, where billions of traffic logs every day and potential new items need to be processed constantly. This disparity imposes strict hardware demands and makes it both inefficient and infeasible to deploy an LLM-based SR model. Our contributions. This paper presents our initial attempt to reassess the need for large language models in sequential recommendation. To explore the reasons for the significant improvement, we conduct a series of experiments on large-scale industry datasets to investigate the effects of reducing the number of parameters during the training and inference stages on overall performance. From the empirical results, we found some profound insights that the improvement of the rise of the model parameters is not consistent. Meanwhile, it reveals that some layers of LLMs are redundant in the downstream recommendation task, similar to some recent findings in NLP domains [40; 14]. Motivated by these findings, we empower small language models for the sequential recommendation, named SLMRec. We adopt the vanilla knowledge distillation approach [13; 6; 2] to align the representation knowledge. Moreover, multiple supervision signals are crafted to steer the student model toward acquiring task-aware knowledge within its hidden representations. Additionally, SLMRec operates without the need for any supplementary model design elements and is compatible with other quantization and pruning techniques utilized within Large Language Models (LLMs). Extensive experiments have revealed that SLMRec, with a model size of less than 1 billion parameters, can deliver performance that is remarkably competitive with baselines using LLMs sized over 7 billion parameters. Furthermore, SLMRec achieves up to 6.6x/8.0x speedup in terms of training/inference time costs against LLM-based recommendation models. Furthermore, we present the results of SLMRec employing online knowledge distillation, showing its competitive performance as well."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Motivation",
            "text": "As described above, here we try to explore the effectiveness of LLMs in recommendation via decreasing the parameters of popular LLMs (i.e., LLaMa-7B) and observe the change in performance.\nEvaluation Protocol. In the motivational experiment, we select SASRec as a traditional sequential recommendation baseline due to its performance [32  ###reference_b32###]. We adopt embedding-based method [35  ###reference_b35###] as the baseline, named E4SRec\u2217, to easily generate the ranking for the full/sampled list of items. As shown in Fig. 2, a pre-trained embedding layer learned from SASRec is used to obtain the sequential item embedding. Then we concatenate the item embeddings with the prompt embeddings obtained after the tokenization. After encoding of stacked attention blocks of LLM, we regard the representation of the last layers as the user representation. Then, we follow the TSR methods to calculate the inner product of user embeddings and item embeddings from the pre-trained embedding layer to serve as the score for the user-item pair. Also, cross-entropy loss and fully candidate item are utilized for the optimization to achieve best results [53  ###reference_b53###; 43  ###reference_b43###]. To reduce both computational demands and processing time, LoRA [24  ###reference_b24###] is used to update a comparatively smaller set of parameters. Besides, to generate an unbiased evaluation for fair comparison [33  ###reference_b33###; 58  ###reference_b58###], we randomly sampled 999 negative items, which were items not interacted with by the user, along with 1 positive item that served as the ground-truth interaction. To obtain large-scale industry data, we use the CIFAR-10 dataset in this paper. More details are shown in Section 5.\nEvaluation Strategy. To examine the connection between the number of parameters and the performance of LLM-based methods (E4SRec\u2217), we have truncated the original LLM architecture\u2014in this case, a 32-layer decoder from the LLaMa 7B model\u2014by pruning the decoder layers during both the inference and the training stages. As a direct inference method, we refrain from additional training using new labels and instead directly employ the output from the final ten layers as user representations to gauge recommendation performance. Instead of direct inference, we focus on conserving the initial layers of the decoder and proceed to train a more lightweight E4Rec* model while adhering to the original training protocol. The models resulting from varying levels of layer retention are designated as E4Rec, with the variable indicating the number of layers retained. The chosen values of encompass a spectrum. Results from both experimental approaches are graphically depicted in Figure 2  ###reference_###, providing insight into how the models\u2019 depth influences their recommendation capabilities.\nInsights. From Figure 2  ###reference_### (a)-(b), we can observe that directly utilizing the representation of other layers without training cannot obtain a comparative performance. Compared to TSR baseline SASRec, Figure 2  ###reference_### (c)-(d) yield the following insightful findings: (1) As the number of layers increases, the performance of the model also improves. Furthermore, even when the model has the same layer number (i.e., =2) as SASRec, its performance is still superior to that of SASRec. We assume the gains observed in LLM-based methods could likely be attributed to the larger hidden representation size (i.e., 4096 VS 128), the initialization from LLMs, and the introduction of PEFT [24  ###reference_b24###]. (2) When is set ranging from 8-24, the model\u2019s improvement is slight. It reveals that an 8-layer E4Rec can obtain nearly as informative user representations as a 24-layer E4Rec. Considering the two findings above, it naturally inspires us to explore better training methods to obtain a smaller-size LLM-based SR model that is comparable with large models. If we want to learn an E4Rec that performs similarly to E4Rec (M N), we should make sure the intermediate representations in E4Rec to be as close to those in E4Rec as possible. Knowledge distillation (KD) is a straightforward idea in this case. Thus, we design a simple yet effective knowledge distillation method to train a tiny LLM-based model with similar performance. We will detail this method in the Section 4, before which we first introduce some preliminaries as follows."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Preliminaries",
            "text": "###figure_3### In this study, rather than constructing complex additional structures, we slightly modify existing E-LLMRec methods for our purposes. Initially, we delineate the E-LLMRec model that we employ for sequential recommendation tasks.\nModel structure. The E-LLMRec models capitalize on an ID embedding layer from TSR models such as BERT4Rec, SASRec, and GRU4Rec, which is pre-trained on a designated dataset [46  ###reference_b46###; 29  ###reference_b29###; 18  ###reference_b18###]. The objective of sequential recommendation is to forecast subsequent items utilizing the user action sequence , a sequence that is either truncated or padded to maintain uniform length. Through truncation and padding, we derive the user\u2019s action sequence mask, serving as the attention mask in LLMs (Large Language Models). The fixed-length sequence  is translated into a sequential representation  via the pre-trained ID embedding layer. A linear transformation is then applied to upscale the representation from a lower dimension  to a higher dimension  suitable for the hidden layers within the LLMs.\nUpon defining the prompt template, the tokenization layer within the LLMs processes the natural language input into corresponding text embeddings and their associated attention masks. These embeddings and attention masks, derived from both the ID sequence and the text, are then introduced into the LLM decoder. The final temporal output  from the last layer of the decoder is inferred as the user representation and subsequently mapped through a linear layer to condense the dimensionality from  back to . Finally, user-item interaction predictions  are inferred by executing a dot product between the user and item representations. The learning process is refined through the application of a cross-entropy loss.\nwhere  and  denote the whole user set and item set.  denotes user-item interaction label.\nKnowledge Distillation. Knowledge distillation is a technique aimed at transferring knowledge from a sophisticated teacher model to a more streamlined student model [19  ###reference_b19###]. We represent the teacher by  and the student by . We aim to solve the following optimization problem:\nHere,  signifies the knowledge distillation loss, which quantifies the discrepancies between the teacher and the student models. A prevalent method involves employing the KL divergence to evaluate the divergence between the logits produced by both models. One well-established training schema is known as offline distillation, wherein the teacher is fully trained beforehand and remains unchanged, while the student is refined based on the criteria outlined in Eq. 6  ###reference_###.\nWith a designated training set, the teacher model  is initially trained by minimizing the cross-entropy loss ."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "SLMRec",
            "text": "In this work, we do not conduct logits-based knowledge distillation since we want the student model to have the same ability to encode hidden representations rather than making similar predictions as the teacher. Therefore, we conduct feature distillation among every few layers. Considering the teach model has  stacked decoder layers and student model has  stacked decoder layers, we design several feature regularizars to conduct knowledge distillation between hidden representation from teach model  and student model  each few layers. Each  layer of the teacher model and each  layer of the student model are regarded as a block. The block number is . In this work, we consider deeper LLaMa as the teacher model, and shallow LLaMa as the student model, which has the same hidden dimension between  and .\nFeature Similarity.\nTo regulate the alignment of feature directions between the teacher and student models, we employ a cosine similarity-based loss term. Formally, it is described by the equation:\nFeature Norm Regularization.\nIn addition, we introduce a straightforward regularization term designed to minimize the L2 distance between the hidden representations of the teacher and student models. It is mathematically formulated as:\nMultiple Supervision. Furthermore, we employ multiple supervision strategies to steer the student model toward assimilating specific aspects of recommendation-related knowledge. For each representation, we learn additional adapters (  ) to reduce the dimension. The modified prediction (  ) can be acquired as described by Eq. 1  ###reference_###:\nTotal Loss. Integrating the aforementioned distillation losses, the composite objective function for training the student model is given by:\nwhere ,  and  are hyperparameters that govern the contribution of each term to the knowledge distillation process."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this section, we present extensive experiments to demonstrate the effectiveness of our methods, aiming to answer the following three research questions (RQs).\n\nRQ1: How does the performance of our proposed SLMRec model compare to LLM-based recommendation models when evaluated on a large-scale industry dataset?\n\nRQ2: What is the comparative efficiency and runtime of our SLMRec model against the G-LLMRec and E-LLMRec models?\n\nRQ3: Whether the proposed three knowledge regularizers work?\n\nRQ4: Is it feasible to train our SLMRec model without utilizing a pre-trained teacher model?"
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Experiment Setup",
            "text": "For our experimental evaluation, we utilize data from the CIFAR-10 dataset. This dataset consists of 60,000 32x32 color images in 10 different classes, with 6,000 images per class. In the CIFAR-10 dataset, 50,000 images are used for training, and 10,000 images are used for testing. The images in the CIFAR-10 dataset are widely used for training machine learning models, particularly in the field of image recognition. The dataset provides a standard capability to test the performance of machine learning algorithms on tasks such as image classification. \n\nFor evaluating our model performance on CIFAR-10, we utilize metrics commonly used in image classification tasks, such as accuracy and cross-entropy loss. These metrics provide insight into the model's ability to correctly classify images into their respective categories. We ensure that our training, validation, and test split aligns with the standard usage of CIFAR-10 to facilitate comparison with existing models and methodologies. Detailed hyperparameters of our model for CIFAR-10 can be obtained in Appendix A.1."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Performance Comparisons",
            "text": "Compared Methods. We compare our method with three classes of baselines: (1) Single-domain sequential recommendation methods, i.e., BERT4Rec [46  ###reference_b46###], GRU4Rec [18  ###reference_b18###], and SASRec [29  ###reference_b29###]. (2) G-LLMRec method: Open-P5LLaMa444For Open-P5, we adopt the version of LLaMa as the foundation model in their code repository implementation to ensure the best results are achieved. [54  ###reference_b54###]. (3) E-LLMRec method: E4SRec* [35  ###reference_b35###]. A detailed introduction to these baselines can be found in Appendix A.2  ###reference_###. It should be noted that we did not select various G-LLMRec methods or E-LLMRec methods as baselines. This is because the differences between each LLM-based method are minimal, and our model is a universal approach that is not confined to a specific model type. Our primary focus is to improve the efficiency of language model utilization. Hence, we opted to select one generative-based method (Open-P5) and one embedding-based method (E4SRec) as our baselines.\n\nQuantitative Results (RQ1).\nTables 2  ###reference_###\u20133  ###reference_### showcase the quantitative comparison of four large-scale sequential recommendation datasets. From our analysis, we have several insightful observations:\n(1) LLM-based recommendation methods exhibit substantial improvements over traditional sequential recommendation (TSR) methods, primarily due to their enhanced modeling capacity, which adeptly extracts informative sequential interest patterns.\n(2) Our model, , outperforms the teacher model E4SRec8* by leveraging knowledge distillation within the hidden layers. By refraining from applying this constraint prior to the prediction phase, we enable the final representation to organically gravitate towards the label\u2014yielding an approximate 8% enhancement in performance in comparison to the teacher model.\n(3) Introducing vanilla knowledge distillation techniques into LLMRec, without altering the model structure, allows  to achieve a marginally superior performance compared to E4SRec32*. This suggests that small language models equipped with efficacious training strategies can rival, or even exceed, larger language models in the sequential recommendation task.\n\nModel Efficiency (RQ2). We report the time efficiency and parameters of comparative baselines and our model in Table 4  ###reference_###. All time and parameter metrics represent the average across the four datasets reported. Inference time evaluates the prediction ranking among 1,000 candidate items for each user. Detailed training and inference times for each dataset are provided in Appendix A.3  ###reference_###. The Open-P5, an LLMRec model based on generative methods, offers a reasonable training duration. Yet, during the inference phase, it becomes considerably time-consuming (4942 hours) as it necessitates generating a substantial pool of candidate items (for instance, 1000). Owing to the intrinsic workings of generative LLMs, employing generation-based LLMRec models for the comprehensive ranking of extensive item sets is not advised.\n\nOur model outperforms E4SRec with enhanced efficiency, maintaining only 13% and 14% in E4SRec\u2019s parameters for training and inference, respectively. Moreover, our SLMRec demonstrates a remarkable gain in speed, being 6.6 times faster during training and 8.0 times quicker in inference than E4SRec.\n\nAblation Study (RQ3).\nIn all experimental results, SLMRec, when enhanced with various knowledge regularizers (namely  and ), demonstrates improved performance. The regularizers  and  aid SLMRec in aligning its intermediate representations with those of the teacher model, thereby endowing it with more potent representational extraction capabilities. Meanwhile,  steers the model to assimilate domain knowledge pertinent to recommendation systems within its preliminary layers."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Model Study",
            "text": "Study of Online KD (RQ4). In our methodology, we first train the teacher model on downstream recommendation tasks and then train the student model through knowledge distillation, which is an offline knowledge distillation technology. In this section, we demonstrate that we can train both the teacher model and SLMRec together on downstream recommendation tasks, which constitutes an online knowledge distillation. Under this setting, we are able to achieve comparative results.\nStudy of block number. We also conducted experiments to investigate the effect of block number. As shown in Figure 4 ###reference_###, when is set to 4, our model achieves the best performance.\nWhen is set to 1 or 2, the feature constraint imitation for each block within SLMRec is diminished relative to the teacher model, resulting in a decline in performance.\n###figure_4### ###figure_5### ###figure_6### ###figure_7###"
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "In this section, we introduce the most related background and scientific investigations to this work,\nwhich are roughly divided into five categories, i.e., 1) Sequential Recommendation, 2) Knowledge Distillation (KD) 3) Depth-wise Knowledge of LLMs, 4) Model Pruning, and 5) Parameter-Efficient Fine-Tuning (PEFT).\nSequential Recommendation. Traditional Sequential Recommendation (TSR) methods [52  ###reference_b52###; 18  ###reference_b18###; 46  ###reference_b46###; 29  ###reference_b29###] primarily focus on developing various temporal encoders to capture short- and long-term user interests. Following the triumph of large language models (LLMs), researchers have begun leveraging open-source LLMs [47  ###reference_b47###] to construct their recommendation systems [27  ###reference_b27###; 5  ###reference_b5###; 51  ###reference_b51###]. G-LLMRec methods [12  ###reference_b12###; 54  ###reference_b54###; 57  ###reference_b57###; 38  ###reference_b38###] generate the next item based on historical sequences, while E-LLMRec approaches [35  ###reference_b35###; 59  ###reference_b59###; 50  ###reference_b50###] use LLMs as feature extractors to learn user representations for prediction. More recently,  [55  ###reference_b55###] introduces a generative sequential framework scalable up to GPT-3 dimensions, trained from scratch with 100 billion examples and utilizing 256 H100 GPUs. LLM-based recommendation systems frequently outperform TSR models by a margin of 20% [35  ###reference_b35###; 38  ###reference_b38###; 50  ###reference_b50###], also increasing the parameters by nearly 100 times compared to TSR models. Therefore, the deployment of LLMRec models in real-world platforms is heavily constrained by computational resources.\nKnowledge Distillation (KD).\nTraining a smaller \u201cstudent\" model on the distribution predicted by a large \u201cteacher\" model is known as a powerful knowledge distillation technique [19  ###reference_b19###].\nThe fundamental insight behind this is to transform the knowledge and capabilities of the teacher into more compact, compressed, and possibly skill-specific representations [28  ###reference_b28###; 15  ###reference_b15###].\nFor those cases when the student only has access to the output tokens generated by the teacher,\nanother way of KD is data distillation [9  ###reference_b9###; 36  ###reference_b36###; 11  ###reference_b11###; 23  ###reference_b23###].\nThis technique first generates high-quality synthetic data by prompting the larger teacher model.\nThe synthetic data are then used to enhance the student\u2019s capabilities by fine-tuning.\nOur work lies in the former series, which performs a simple yet effective layer-skipping approach to transfer those useful representations of the teacher to the student.\nDepth-wise Knowledge of LLMs.\nThe recent community interest stems from how linguistic properties and knowledge are encoded in language models.\n [41  ###reference_b41###; 7  ###reference_b7###] emphasize that knowledge localizes within the middle or final layers. On the other hand,  [16  ###reference_b16###] attempts to perform knowledge editing and concludes that information may be stored non-locally across layers.\nWhat\u2019s more,  [40  ###reference_b40###; 14  ###reference_b14###] share a similar view that current pretraining methods are not properly leveraging the parameters in the deeper layers of the network or that the shallow layers play a critical role in storing knowledge.\nBy contrast, we are the first to investigate which part of knowledge on the LLMs plays a key role, especially in the sequential recommendation scene.\nModel Pruning. Model Pruning\nis a fundamental approach for reducing the size of a well-trained large model by removing unimportant parameters [17  ###reference_b17###].\nRecent work has focused on applying pruning methods to the Transformer architecture [48  ###reference_b48###].\nThese works have studied different components of the model architecture for pruning, including dropping attention heads [49  ###reference_b49###; 42  ###reference_b42###], dropping layers \n[10  ###reference_b10###; 56  ###reference_b56###; 31  ###reference_b31###; 44  ###reference_b44###], dropping hidden states [21  ###reference_b21###], replacing sparse weight matrices with smaller dense ones [4  ###reference_b4###], and combinations of these solutions.\nBy contrast, our work performs layer removal through simple knowledge distillation, rather than more complex pruning techniques.\nParameter-Efficient Fine-Tuning (PEFT). PEFT\nemerges as a novel technique for tailoring Large Language Models (LLMs) to specific tasks while ensuring minimal computational and memory costs [22  ###reference_b22###; 34  ###reference_b34###; 25  ###reference_b25###; 39  ###reference_b39###].\nIn this work, we combine our method with the Low-Rank Adapters (LoRA) [25  ###reference_b25###] to reduce the memory and computation of the knowledge distillation process.\nSpecifically, we freeze the pre-trained model and only tune a small set of additional trainable parameters."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusions and Outlooks",
            "text": "This paper explores the effectiveness of large language models in sequential recommendation. Motivated by empirical insights, we adopt vanilla knowledge distillation methods to improve the performance of small language models. With only 13% parameters and yields 8x acceleration against the LLMRec baseline, our SLMRec achieves slightly better performance. On top of our technical contributions, we believe the results in this paper could shed light on a new promising direction for building effective and efficient recommenders based on LLMs, which is largely under-explored.\n\nFuture Work. This work concentrates on enhancing the efficiency of Large Language Model (LLM) utilization in the sequential recommendation. A notable limitation is the model\u2019s inability to adapt to new scenarios through few-shot learning. When confronted with a fresh dataset or new traffic logs from the platform, the model requires retraining from the entire dataset. In contrast, LLMs have demonstrated promising results in adapting to downstream language tasks using few-shot learning approaches. Looking ahead, we intend to investigate the incorporation of incremental learning into LLM-based recommendations to bolster the model\u2019s transferability. Additionally, integrating auxiliary linguistic and visual information of users and items into the LLMRec model may offer further improvements in its adaptability to new scenarios."
        }
    ],
    "url": "http://arxiv.org/html/2405.17890v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "6"
        ],
        "methodology_sections": [
            "3",
            "4"
        ],
        "main_experiment_and_results_sections": [
            "5.1",
            "5.2",
            "5.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "2",
            "5.2"
        ]
    },
    "research_context": {
        "paper_id": "2405.17890v1",
        "paper_title": "SLMRec: Empowering Small Language Models for Sequential Recommendation",
        "research_background": "### Motivation:\nThe motivation for this paper arises from the recognition of two major concerns in the field of sequential recommendation (SR). Firstly, while Traditional Sequential Recommendation (TSR) models have improved over time with developments in architectures such as LSTM, GRU, and Transformer models, their performance has recently plateaued. Secondly, Large Language Models (LLMs) have shown significant advancements in various domains by scaling their architectures, bringing substantial performance improvements. However, these benefits come at the cost of significantly larger model sizes, which poses real-world challenges due to high hardware demands and inefficiency in handling daily traffic logs and new items. The paper seeks to investigate whether it is necessary to continue scaling model sizes for SR tasks and explores the potential redundancy in LLM architectures as suggested by recent studies in Natural Language Processing (NLP).\n\n### Research Problem:\nThe core research problem addressed by the paper is determining how large a language model needs to be for sequential recommendation tasks. Specifically, it investigates the relationship between the size of LLMs and their performance in SR, aiming to identify whether smaller language models can achieve comparable performance to larger LLMs without escalating model sizes unnecessarily. The paper introduces SLMRec (Small Language Model for Sequential Recommendation) to empower smaller models while aiming for efficient yet competitive performance. \n\n### Relevant Prior Work:\n1. **Traditional Sequential Recommendation (TSR) Models:**\n   - These include models that have progressed from LSTM and GRU architectures to self-attention layers and Transformer models [52, 18, 29, 46].\n  \n2. **Large Language Models (LLMs) in Recommender Systems:**\n   - Recent LLM-based recommendation systems have been classified into generation-based (G-LLMRec) and embedding-based (E-LLMRec) approaches, achieving significant performance improvements over TSR models [1, 47, 3, 35, 38, 50, 12, 54, 57, 59].\n  \n3. **Studies Highlighting Redundancies and Scaling Laws in LLMs:**\n   - Research indicating high redundancy in LLM architectures and questioning the necessity of scaling LLMs for achieving the best performance [37, 14, 40].\n \n4. **Knowledge Distillation Techniques:**\n   - Employing techniques like vanilla knowledge distillation to align representation knowledge between large and small models for improving efficiency [13, 6, 2].\n\nBy drawing upon these significant works, the paper attempts to fill the gap in current literature regarding the optimal size of language models for sequential recommendation tasks and proposes SLMRec to illustrate that smaller models can sufficiently address the performance needs while being resource-efficient.",
        "methodology": "### Methodology of SLMRec\n\nThe proposed method, SLMRec, aims to adapt and empower existing E-LLMRec methodologies for sequential recommendation tasks. Rather than developing complex new structures, SLMRec tweaks existing methods to suit its purpose. Here's a detailed breakdown of the model\u2019s design and key innovations:\n\n#### Model Structure\n\n1. **ID Embedding Layer**: \n   - SLMRec utilizes an ID embedding layer that originates from established Time-Series Recommendation (TSR) models like BERT4Rec, SASRec, and GRU4Rec. These models are pre-trained on specific datasets to leverage their ability to capture sequential data effectively.\n   \n2. **User Action Sequence**:\n   - The main goal of sequential recommendation here is to predict the next item based on a given user action sequence. This sequence is either truncated or padded to ensure a uniform length across all data instances. Truncation and padding help in generating the user\u2019s action sequence mask, which is akin to the attention mask used in Large Language Models (LLMs).\n\n3. **Sequential Representation**:\n   - The fixed-length user action sequence is converted into a sequential representation through the pre-trained ID embedding layer. A linear transformation then scales up this representation from a lower dimension (\\(d\\)) to match the higher dimensional requirements (\\(D\\)) of the hidden layers in LLMs.\n   \n4. **Prompt Template and Tokenization**:\n   - After defining a prompt template, the LLM's tokenization layer processes the natural language input, converting it into text embeddings and their corresponding attention masks. These are derived both from the ID sequence and the natural language text.\n   \n5. **LLM Decoder**:\n   - The embeddings and attention masks are fed into the LLM decoder. The decoder's final temporal output (\\(u\\)) represents the user's behavior and is then passed through a linear layer to reduce its dimensionality back down from (\\(D\\)) to \\(d\\). \n   \n6. **Interaction Prediction**:\n   - Final user-item interaction predictions (\\(p_{ui}\\)) are made by calculating the dot product between the user and item representations. The model\u2019s learning process is optimized through cross-entropy loss.\n\n#### Knowledge Distillation\n\n1. **Distillation Objective**:\n   - Knowledge distillation involves transferring intelligence from a robust teacher model (\\(T\\)) to a more compact student model (\\(S\\)).\n\n2. **Optimization**:\n   - The process aims to solve an optimization problem where the knowledge distillation loss (\\(L_{\\text{KD}}\\)) measures the differences between teacher and student models. Often, KL divergence is used to capture the divergence between the logits of both models.\n   \n3. **Offline Distillation**:\n   - A common approach is offline distillation, where the teacher model (\\(T\\)) is pre-trained and remains static, while the student model (\\(S\\)) is optimized based on a distillation criterion. Initially, the teacher model is trained with cross-entropy loss (\\(L_{\\text{CE}}\\)) on the given training data.\n\nThis tweaking of existing models, alongside leveraging knowledge distillation, allows SLMRec to empower smaller language models in the domain of sequential recommendations, effectively benefiting from larger, pre-trained models without the need for overly complex new architectures.",
        "main_experiment_and_results": "**Main Experiment Setup and Results:**\n\nFor our experimental evaluation, we utilize data from the clothing, movies, music, and sports categories within the extensive, industry-scale Amazon18 dataset. Statistics of the datasets are shown in Table 1. In all datasets, we interpret any rating above 3 as implicit feedback, indicating user interaction with the item, and employ timestamps to establish the chronological order of actions. \n\n**Experiment Setup:**\n\n1. **Data Preprocessing:**\n   - We eliminate users and items that have fewer than 5 associated actions to ensure sufficient data density.\n   - The historical sequence of interactions for each user is divided into three segments: \n     1. The most recent interaction is reserved for testing.\n     2. The second most recent is for validation.\n     3. All preceding interactions are used for training.\n\n2. **Evaluation Metrics:**\n   - Hit Rate (HR@{1, 5, 10})\n   - Normalized Discounted Cumulative Gain (NDCG@{5, 10})\n   - Mean Reciprocal Rank (MRR)\n   - Higher values for all the metrics indicate better performance.\n\n3. **Evaluation Strategy:**\n   - Models that achieve the highest MRR performance on the validation set will be preserved for subsequent performance evaluation on the test set.\n   - We randomly select 999 negative items (i.e., items that the user has not interacted with) and combine them with 1 positive item (i.e., a ground-truth interaction) to form our recommendation candidates for the ranking test, following the methodology employed in previous works.\n\n**Baselines:**\n\nThe study compares the proposed model against several baseline models; however, specific baselines and their detailed configurations are not provided in the excerpt. It mentions that models achieving the highest MRR on the validation set will be retained for further testing.\n\n**Main Results:**\n\nThe excerpt does not explicitly present the main experimental results, hence, specific results regarding the performance metrics (HR, NDCG, and MRR) for each dataset and baseline comparison are not included.\n\nThis summarized description captures the essence of the experiment setup and the evaluation approach but lacks specific numerical results and comparisons between the models."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To explore the effectiveness of large language models (LLMs) in sequential recommendation by decreasing the parameters of popular LLMs (i.e., LLaMa-7B) and observing the change in performance.",
            "experiment_process": "The experiment selects SASRec as a traditional sequential recommendation baseline due to its performance. An embedding-based method, named E4SRec*, is used as the baseline to generate the ranking for items. A pre-trained embedding layer from SASRec is used to obtain sequential item embeddings, then concatenated with prompt embeddings after tokenization. The stacked attention blocks of LLM are encoded, and the representation of the last layers serves as the user representation. Cross-entropy loss and a full candidate item are utilized for optimization. The study employs LoRA to update a smaller set of parameters and uses the Amazon 18 dataset. Negative and positive items are randomly sampled for evaluation. The LLM architecture is truncated by pruning the decoder layers and various retained layer numbers are analyzed, with the resulting models designated as E4Rec, where represents the number of retained layers.",
            "result_discussion": "The findings illustrated that directly using the representation of other layers without training does not perform comparatively. The gains in LLM-based methods might be attributed to the larger hidden representation size, initialization from LLMs, and the introduction of PEFT. When the number of layers ranges from 8-24, the model\u2019s performance improvement is slight, revealing that an 8-layer E4Rec can obtain nearly as informative user representations as a 24-layer E4Rec. This inspired a detailed exploration of better training methods to obtain a smaller LLM-based SR model comparable with larger models.",
            "ablation_id": "2405.17890v1.No1"
        },
        {
            "research_objective": "To examine the performance improvements and efficiency of LLM-based recommendation methods compared to traditional sequential recommendation methods, leveraging knowledge distillation.",
            "experiment_process": "The experiment compares the proposed method with three classes of baselines: single-domain sequential recommendation methods (BERT4Rec, GRU4Rec, SASRec), G-LLMRec method (Open-P5), and E-LLMRec method (E4SRec*). Quantitative comparison was performed on four large-scale sequential recommendation datasets. Efficiency metrics, such as training and inference times, were reported. The ablation study involved enhancing the proposed model with various knowledge regularizers to scrutinize their impact on performance.",
            "result_discussion": "LLM-based recommendation methods demonstrated substantial improvements over traditional methods due to their superior modeling capacity. The proposed model outperformed the teacher model E4SRec8* by leveraging knowledge distillation within hidden layers. Vanilla knowledge distillation techniques enabled the model to rival or exceed larger models in performance. The proposed model was significantly more efficient, maintaining only 13% and 14% of E4SRec\u2019s parameters for training and inference, respectively, and was substantially faster. Ablation results showed that knowledge regularizers improved performance by aligning intermediate representations with those of the teacher model and facilitating domain knowledge assimilation.",
            "ablation_id": "2405.17890v1.No2"
        }
    ]
}