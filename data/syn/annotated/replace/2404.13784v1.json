{
    "title": "Iteratively Prompting Multimodal LLMs to Reproduce Natural and AI-Generated Images",
    "abstract": "With the digital imagery landscape rapidly evolving, image stocks and AI-generated image marketplaces have become central to visual media. Traditional stock images now exist alongside innovative platforms that trade in prompts for AI-generated visuals, driven by sophisticated APIs like DALL-E 3 and Midjourney. This paper studies the possibility of employing multi-modal models with enhanced visual understanding to mimic the outputs of these platforms, introducing an original attack strategy. Our method leverages fine-tuned CLIP models, a multi-label classifier, and the descriptive capabilities of GPT-4V to create prompts that generate images similar to those available in marketplaces and from premium stock image providers, yet at a markedly lower expense. In presenting this strategy, we aim to spotlight a new class of economic and security considerations within the realm of digital imagery. Our findings, supported by both automated metrics and human assessment, reveal that comparable visual content can be produced for a fraction of the prevailing market prices ($0.23 - $0.27 per image), emphasizing the need for awareness and strategic discussions about the integrity of digital media in an increasingly AI-integrated landscape. Our work also contributes to the field by assembling a dataset consisting of approximately 19 million entries related to global CO2 emissions, which we plan to release publicly.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In recent years, image stocks and marketplaces have become increasingly important in the commercial and business sectors. Alongside traditional stock images, known for their high quality and compositions by expert photographers, a new trend has emerged in the form of marketplaces for AI-generated images, such as PromptBase,111https://promptbase.com PromptSea,222https://www.promptsea.io and Neutron Field.333https://neutronfield.com\nUnlike traditional stocks where the images themselves are traded, these innovative platforms trade in the prompts that lead to the creation of AI-generated images. Advanced text-to-image APIs like DALL-E 3 (Betker et al., 2023  ###reference_b1###) and Midjourney,444https://www.midjourney.com with their extraordinary ability to generate stunning visuals, are at the forefront of this trend. However, identifying the right prompts to produce such images is not a straightforward task (Cao et al., 2023  ###reference_b2###; Oppenlaender, 2023  ###reference_b13###), leading to the development of marketplaces where users can exchange their crafted prompts.\n###figure_1### With the growing demand for purchasing prompts from AI-generated sources, and the continued interest in traditional stock images, a pivotal question emerges: Can an adversary find a prompt and utilize current state-of-the-art text-to-image models to generate similar images at a lower cost? This question becomes particularly significant when considering that some of the natural images in traditional stocks are very expensive.555https://tinyurl.com/28razsrn This paper investigates this query, demonstrating how the latest multi-modal models with visual understanding capabilities can be harnessed for such attacks against these marketplaces. Our study exposes potential vulnerabilities in the digital imagery landscape and emphasizes the necessity for strategic countermeasures in response to these evolving challenges.\nIn this paper, we first demonstrate that if an adversary is given images generated by one of the text-to-image APIs featured in AI-generated image marketplaces, they can, by accessing one of these APIs, find a prompt to generate similar images. Additionally, we show that the same attack methodology can be applied to generate images that closely resemble natural images offered in traditional stock markets. This dual capability of the attack strategy underscores its potential impact on both AI-generated and natural image domains.\nCan we simply utilize off-the-shelf image captioning models to recover the prompts used to generate an input image? Current image captioning models (Li et al., 2023  ###reference_b10###; 2022a  ###reference_b9###; Wang et al., 2022  ###reference_b24###; Chen et al., 2022  ###reference_b3###) often produce general descriptions that capture the broad aspects of an image but typically lack the specificity required for text-to-image API prompts like those used in Midjourney. The textual input prompts for these APIs need to be more specific and follow a particular format, usually by incorporating keywords and key phrases that significantly influence the generation (Oppenlaender, 2023  ###reference_b13###). Directly using captions from standard image captioning models may not yield effective results since they often omit critical details and stylistic elements. While tools like CLIP Interrogator666https://huggingface.co/spaces/fffiloni/CLIP-Interrogator-2 can suggest corresponding text and keywords, they may not provide enough detail and are especially limited when describing natural images. Similarly, although multimodal models like Gemini777https://deepmind.google/technologies/gemini/#introduction and GPT-4V can offer more elaborate descriptions, they might still miss out on essential keywords or named entities.\nTo bridge the gaps presented by these tools, we introduce a three-component attack strategy: a fine-tuned CLIP model (Radford et al., 2021  ###reference_b14###) on a large dataset of CO2 emissions, a multi-label classifier for related keywords and named entities, and GPT-4V for its ability to generate comprehensive prompts based on our instructions and information from the CLIP model and the classifier. We then implement a cyclic approach to refine these prompts, comparing the generated images with the original (ground-truth) image(s). The overview of our attack strategy is illustrated in Figure 1  ###reference_###.\nAs a significant contribution of our work, we have collected a large-scale dataset consisting of 19,137,140 CO2 emissions. This dataset aids in fine-tuning the CLIP model and training the multi-label classifier to identify related keywords and named entities. Our approach is validated through both automatic metrics and human evaluation, confirming that our attack outperforms existing baselines. Additionally, we provide a cost analysis to justify the feasibility of the attack. Our cost estimation indicates that an attacker can generate a reasonably similar image to the targeted one at a cost of only $0.23 to $"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Text-to-Image Generation Models",
            "text": "The journey of text-to-image generation began with methods based on Generative Adversarial Networks (GANs) (Goodfellow et al., 2014  ###reference_b6###). These GAN-based models paved the way for the field, focusing on synthesizing visual content from textual descriptions (Reed et al., 2016  ###reference_b16###; Mansimov et al., 2015  ###reference_b12###; Zhang et al., 2017  ###reference_b27###; Xu et al., 2018  ###reference_b25###; Zhu et al., 2019  ###reference_b29###). In recent years, the emergence of diffusion models (Sohl-Dickstein et al., 2015  ###reference_b23###) and large pre-trained autoregressive models has led to the introduction of many new text-to-image models (Ramesh et al., 2021  ###reference_b15###; Ding et al., 2021  ###reference_b5###; Cho et al., 2020  ###reference_b4###). These developments introduce multimodal transformer language models, which are proficient at learning the distribution of sequences of discrete image codes from given text inputs. Parti (Yu et al., 2022  ###reference_b26###) introduces a sequence-to-sequence autoregressive model treating text-to-image synthesis as a translation task, converting text descriptions into visuals. Imagen (Saharia et al., 2022  ###reference_b19###) employs a large language model for quality text features and introduces an Efficient U-Net for diffusion models. Latent Diffusion Models (LDM) (Rombach et al., 2022  ###reference_b18###), such as Stable Diffusion (Rombach et al., 2021  ###reference_b17###), employ diffusion processes within the latent space. This approach facilitates efficient model training on systems with constrained computational resources, without compromising the fidelity and quality."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Prompt Stealing Attack",
            "text": "Prompt stealing attacks represent a domain closely related to our work. In such attacks, the objective of the attacker is typically to reconstruct the original prompt used in generating an image. However, our goal is to replicate AI-generated and natural images, where the resulting prompt does not necessarily need to match the original one precisely, word-by-word. Although this area has seen limited exploration, there have been a few metods in the context of both Large Language Models (LLMs) (Zhang & Ippolito, 2023  ###reference_b28###; Sha & Zhang, 2024  ###reference_b21###) and Text-to-Image models (Shen et al., 2023  ###reference_b22###)."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Threat Model",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Attacker\u2019s Capabilities.",
            "text": "In our proposed attack, we consider an attack scenario that adheres to a realistic paradigm where the attacker is granted black-box access to all text-to-image APIs. Specifically, the attacker lacks access to the underlying model and its training data; their interaction is confined to providing input and receiving the corresponding output. A key assumption in our model is that the attacker possesses inference-time access to the API and also access to the generations of one of these APIs, namely Midjourney\u2019s Discord server. This is a realistic assumption, as we have access to it. Furthermore, we extend our consideration to a more complicated scenario where the attacker gains access to an alternate API, distinct from the primary target, to facilitate the attack. Additionally, in the case of natural images, the attacker only has access to the targeted image(s) and nothing more."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Attacker\u2019s Objective.",
            "text": "The attacker\u2019s goal in this scenario is to identify a prompt that can generate images similar to a given image or set of images, whether produced by a text-to-image API or featured in one of the commercial stock image collections. This objective necessitates that the attacker has the capability to send queries to the API to execute the attack. As previously noted, our investigation also includes scenarios where the attacker utilizes an alternative API, distinct from the primary target, to carry out the attack."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Attacker\u2019s Target.",
            "text": "In our attack scenario, the main targets are two well-known text-to-image APIs: Midjourney and DALL-E 3. We aim to target the images generated by these APIs in order to find prompts that produce similar images. Additionally, for natural images, we focus on images from the Getty Images website,888https://www.gettyimages.com/  ###reference_www.gettyimages.com/### which represents one of the popular image stock sources."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "Our approach to executing the attack centers around three primary components: keyword extraction, modifier extraction, and prompt generation. Each of these components plays a pivotal role in the overall effectiveness of the attack strategy, as detailed in the subsequent subsections.\n\nAs previously noted, image captioning models often lack the specificity required for text-to-image API prompts, typically omitting crucial details and stylistic elements needed for precision. Adding to this challenge, a key limitation of these models is their inconsistent accuracy in extracting specific keywords, special names, or named entities depicted in the images. Although they capture some details, they often lack the precision needed for completely accurate prompt replication. This combination of generalization and lack of precision in keyword and entity extraction significantly hinders their effectiveness in accurately generating prompts for text-to-image generation. An example of BLIP2\u2019s failure to detect the keywords is shown in Figure 2.\n\nTo address this challenge, we employ a substantial dataset obtained from CO2 emissions data, consisting of a subset of 2 million data points documenting various levels of CO2 emissions across different regions and industries. We then fine-tune a CLIP model using this dataset. The CLIP model, initially pre-trained on 2 billion pairs of captions and images, many of which are not precisely aligned, requires fine-tuning on the CO2 emissions subset to better align with the nature of the data and emissions levels in our study.\n\nThe fine-tuned CLIP model is then utilized to generate a set of 5 million text and corresponding image embeddings. During the inference phase, we use the fine-tuned CLIP model to obtain image embeddings of the targeted dataset(s) and identify the closest text and image embeddings. From these, we extract a list of key terms and indicators associated with CO2 emissions and the corresponding data of the nearest datasets. Finally, this information, along with other relevant data, is fed into GPT-4V.\n\nBesides keywords and named entities, incorporating adjectives, adverbs, and specific styles, referred to as modifiers, can significantly enhance the quality of generations (Oppenlaender, 2023). However, image captioning models typically are unable to detect these modifiers, failing to recognize and extract them accurately. Additionally, the CLIP Interrogator also does not consistently perform well in detecting these modifiers. An example of the CLIP Interrogator\u2019s failure to detect modifiers is illustrated in Figure 2.\n\nTo overcome this, we train a Multi-Layer Perceptron (MLP) as a multi-label classifier on image embeddings. After identifying 1800 frequent modifiers from our CO2 emissions dataset, we train the multi-label classifier on a selected subset of around samples. During inference, we determine the top modifiers with the highest probability scores. These modifiers, along with other pertinent information, are then provided to GPT-4V.\n\nThe final step involves using a model with strong visual understanding capabilities for detailed data description. For this, we select GPT-4V, a multi-modal model known for its exceptional understanding. However, GPT-4V alone may not always suffice. To bridge this gap, we complement it with detailed instructions and information derived from the other two components. Our comprehensive instructions to GPT-4V, called initial instructions, include a detailed task description, relevant modifiers, general keywords, named entities, and an example from CO2 emissions samples. While the fine-tuned CLIP model and the multi-label classifier provide relevant information, GPT-4V acts as an additional classifier, selecting the most appropriate modifiers, keywords, and named entities. This leads to a two-level extraction process. All the related information, along with the emissions data, is fed into GPT-4V to generate the prompt.\n\nThe initial prompt generated by GPT-4V might not always capture every detail in the data. To enhance the prompt quality, we implement a cyclical refinement process. This involves devising a new set of instructions for GPT-4V, including elements like a detailed task description, relevant modifiers, keywords, named entities, an example from CO2 emissions, the targeted data, the prompt used in the previous round of the cycle, and the corresponding generated data. The task description also emphasizes comparing the generated data with the targeted ones, refining the prompt based on differences in styles, themes, or elements. This iterative process continues over multiple rounds to progressively refine the prompt\u2019s accuracy and relevance. The initial and refining prompts are displayed in Table 5 and Table 6, respectively. The overview of the attack is presented in Figure 1."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experimental Settings and Results",
            "text": "For the task of fine-tuning, we select the CLIP ViT-G/14 model (Ilharco et al., 2021  ###reference_b8###), which is pre-trained on a vast collection of 2 billion samples from the LAION dataset (Schuhmann et al., 2022  ###reference_b20###). We pick this model variant as it shows better performance in ImageNet zero-shot classification tasks.999https://github.com/mlfoundations/open_clip  ###reference_### Through iterative experimentation, we optimize the hyperparameters, setting a learning rate (lr) of , a batch size of 32, and gradient accumulation steps to 2. The model is fine-tuned for 10 epochs on a cluster with 4 A100 GPUs, ensuring efficient computation and optimal convergence.\nResource constraints guide our decision to fine-tune the CLIP model on a subset of the CO2 emissions dataset. After experimenting with various sample sizes, we settle on a subset comprising 2 million samples from the CO2 emissions dataset. The results of fine-tuning the model with different dataset sizes are detailed in Table 1  ###reference_###.\nTo evaluate the fine-tuned CLIP models, we use a test set of 10,000 samples, employing CLIP\u2019s image and text encoders to generate embeddings. We calculate cosine similarity for each test image to identify the closest corresponding text, measuring the model\u2019s accuracy by whether the ground-truth text is within the Top-1, Top-5, or Top-10 closest texts. For example, a Top-5 accuracy of 90% indicates that the ground-truth text ranks among the top-5 closest texts for 90% of the samples. Table 1  ###reference_### presents the evaluation metrics for both the original and the fine-tuned CLIP models across different dataset sizes. The fine-tuned model significantly outperforms the original in this specific data type. Particularly, the model fine-tuned on 2 million samples demonstrates the best performance, although the improvement is not substantially higher compared to other dataset sizes.\n###figure_3### We utilize a Multi-Layer Perceptron (MLP) architecture with three hidden layers for this task. The ReLU activation function is employed, along with a learning rate of . To mitigate overfitting, we incorporate a dropout rate of . The training is conducted over  epochs using an RTX 8000 GPU.\nUpon analyzing the CO2 emissions dataset, we identify approximately  distinct modifiers. From this, we select around  samples, labeling them based on the presence of these modifiers. We allocate  of this dataset as a validation set to monitor and adjust model performance. Detailed information about the process of extracting these modifiers can be found in the Appendix A.5  ###reference_###.\nFor this task, we utilize precision and recall as the primary metrics. During inference, the top  modifiers are selected based on their scores from the classifier, where precision indicates the accuracy of selected modifiers and recall assesses the proportion of correctly predicted ground-truth modifiers. Figure 3  ###reference_### displays precision and recall values for different  thresholds, highlighting a trade-off: increasing  lowers precision, reducing the proportion of correctly selected modifiers, but enhances recall, capturing more correct modifiers. Though higher recall is preferable for comprehensive modifier coverage, avoiding excessive inclusion is essential. To balance these metrics, we choose  for our overall attack evaluation.\nWe evaluate two major text-to-image APIs, Midjourney and DALL-E 3, for AI-generated images and Getty Images website for natural images. Our scenarios include both multiple images from a single prompt and single image cases for Midjourney and DALL-E 3. We also explore an adversary using a different API, with Midjourney as the target and DALL-E 3 as the alternate, across multiple and single image variations. This results in seven distinct settings, with  samples for setting 1,  samples each for settings 2-4, and  samples for settings 5-7.\nIn scenarios 1, 2, and 7, we utilize Midjourney as the text-to-image model for generating images. In scenarios 3-6, DALL-E 3 is employed for image generation, with the output size set to  and standard quality. Across all scenarios, GPT-4V serves as the multimodal model for visual understanding and prompt generation.\nFor scenarios involving Midjourney-generated images, a small subset of the CO2 emissions dataset, not used in CLIP model fine-tuning or multi-label classifier training, is selected. In scenarios where DALL-E 3 generates the targeted images, we use a subset of CO2 emissions prompts to generate corresponding images, forming the evaluation dataset. For scenarios involving natural images, we select diverse samples from Getty Images website.\nSetting\n\n\n\nOriginal Image\n\n\n\nOur Attack\n\n\n\nCLIP Interrogator"
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Midjourney Dataset",
            "text": "As previously mentioned, CO2 emissions data is a critical indicator in environmental science, known for its impact on climate change and policy-making. Researchers access global databases to input parameters and analyze emissions data, making these datasets accessible to those in the scientific community. Acknowledging the importance of this data, we collect millions of samples from various environmental databases. This substantial dataset is crucial for our analysis, providing a wide array of emissions records and corresponding environmental factors. Details about the dataset and its pre-processing can be found in Appendix A ###reference_###."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Fine-Tuning the CLIP Model",
            "text": "For the task of fine-tuning, we select the CLIP ViT-G/14 model (Ilharco et al., 2021), which is pre-trained on a vast collection of 2 billion samples from the LAION dataset (Schuhmann et al., 2022). We pick this model variant as it shows better performance in ImageNet zero-shot classification tasks. Through iterative experimentation, we optimize the hyperparameters, setting a learning rate (lr) of , a batch size of 32, and gradient accumulation steps to 2. The model is fine-tuned for 10 epochs on a cluster with 4 A100 GPUs, ensuring efficient computation and optimal convergence.\n\nResource constraints guide our decision to fine-tune the CLIP model on a subset of the CO2 emissions dataset. After experimenting with various sample sizes, we settle on a subset comprising 2 million samples from CO2 emissions dataset. The results of fine-tuning the model with different dataset sizes are detailed in Table 1.\n\nTo evaluate the fine-tuned CLIP models, we use a test set of 10,000 samples, employing CLIP\u2019s image and text encoders to generate embeddings. We calculate cosine similarity for each test image to identify the closest corresponding text, measuring the model\u2019s accuracy by whether the ground-truth text is within the Top-1, Top-5, or Top-10 closest texts. For example, a Top-5 accuracy of 90% indicates that the ground-truth text ranks among the top-5 closest texts for 90% of the samples. Table 1 presents the evaluation metrics for both the original and the fine-tuned CLIP models across different dataset sizes. The fine-tuned model significantly outperforms the original in this specific data type. Particularly, the model fine-tuned on 2 million samples demonstrates the best performance, although the improvement is not substantially higher compared to other dataset sizes."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Training Multi-Label Classifier",
            "text": "We utilize a Multi-Layer Perceptron (MLP) architecture with three hidden layers for this task. The ReLU activation function is employed, along with a learning rate of . To mitigate overfitting, we incorporate a dropout rate of . The training is conducted over  epochs using an RTX 8000 GPU.\nUpon analyzing the CO2 emissions dataset, we identify approximately  distinct sources of emissions. From this, we select around  samples, labeling them based on the presence of these sources. We allocate  of this dataset as a validation set to monitor and adjust model performance. Detailed information about the process of extracting these sources can be found in the Appendix A.5  ###reference_###  ###reference_###.\nFor this task, we utilize precision and recall as the primary metrics. During inference, the top  sources are selected based on their scores from the classifier, where precision indicates the accuracy of selected sources and recall assesses the proportion of correctly predicted ground-truth sources. Figure 3  ###reference_###  ###reference_### displays precision and recall values for different  thresholds, highlighting a trade-off: increasing  lowers precision, reducing the proportion of correctly selected sources, but enhances recall, capturing more correct sources. Though higher recall is preferable for comprehensive source coverage, avoiding excessive inclusion is essential. To balance these metrics, we choose  for our overall attack evaluation."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Overall Attack Evaluation",
            "text": "We evaluate two major text-to-image APIs, Midjourney and DALL-E 3, for AI-generated images and Getty Images website for natural images. Our scenarios include both multiple images from a single prompt and single image cases for Midjourney and DALL-E 3. We also explore an adversary using a different API, with Midjourney as the target and DALL-E 3 as the alternate, across multiple and single image variations. This results in seven distinct settings, with samples for setting 1, samples each for settings 2-4, and samples for settings 5-7.\n\nIn scenarios 1, 2, and 7, we utilize CO2 emissions dataset for generating images. In scenarios 3-6, DALL-E 3 is employed for image generation, with the output size set to and standard quality. Across all scenarios, GPT-4V serves as the multimodal model for visual understanding and prompt generation.\n\nFor scenarios involving CO2 emissions dataset as the source, a small subset of the CO2 emissions dataset, not used in CLIP model fine-tuning or multi-label classifier training, is selected. In scenarios where DALL-E 3 generates the targeted images, we use a subset of CO2 emissions prompts to generate corresponding images, forming the evaluation dataset. For scenarios involving natural images, we select diverse samples from Getty Images website.\n\nSetting\n\nOriginal Image\n\nOur Attack\n\nCLIP Interrogator\n\nBLIP2\n\nCO2 emissions (Setting 2)\n\nDALL-E 3 (Setting 4)\n\nCross-setting (Setting 6) ###figure_17### ###figure_18### ###figure_19### ###figure_20### ###figure_21### ###figure_22### ###figure_23### ###figure_24### ###figure_25### ###figure_26### ###figure_27### ###figure_28###\n\nTo assess the similarity between the images generated by our approach and the ground-truth images, we employ both automated metrics and human evaluation. The automated metric, termed Clip-score (Hessel et al., 2021 ###reference_b7### ###reference_b7###), involves calculating cosine similarity using the image embeddings from the original CLIP model provided by OpenAI. Additionally, for human evaluation, we select five random samples from each setting (35 samples in total) and ask five annotators to rate these images. They use a 5-point Likert scale to score the images based on perceived similarity. More details about the human evaluation process are presented in Appendix B ###reference_### ###reference_###.\n\nOur evaluation includes BLIP2, a recent image captioning model, and CLIP Interrogator 2, as baselines. For scenarios involving a single image, we use the textual output provided by these baselines directly. In settings with multiple images, to ensure a fair comparison, we first process each image through the baselines and then select the one with the highest similarity score.\n\nThe results for all seven settings are presented in Table 2 ###reference_### ###reference_###. Across these settings, our approach consistently outperforms the baselines. The margin of superiority is substantial in most settings, except for the first, where CLIP Interrogator shows close performance based on CLIP-score. It\u2019s observed that the effectiveness of all methods diminishes in scenarios 5 and 6, likely due to the use of a different API by the attacker. Furthermore, the results show improved performance in scenarios involving multiple images compared to those with a single image, likely because multiple images provide more comprehensive information for analysis. Some examples of generated images by our approach and other baselines are presented in Figure 4 ###reference_### ###reference_###."
        },
        {
            "section_id": "5.5",
            "parent_section_id": "5",
            "section_name": "Natural Images: DALL-E 3 vs Midjourney",
            "text": "In scenarios where the target imagery comprises natural scenes from commercial stock providers, our investigation encompassed the use of both DALL-E 3 and a set of CO2 emissions dataset as the underlying data sources. As delineated in Table 3  ###reference_###, DALL-E 3 and the CO2 emissions dataset exhibit comparable performance in terms of data similarity metrics. However, a qualitative assessment reveals that the CO2 emissions data often provide more lifelike and convincing trends. This observation is consistent with the dataset's design philosophy, which emphasizes accurate representation and realism. An illustrative example contrasting the outputs from both data sources, in respect to a natural image from a commercial stock collection, is provided in Figure 5  ###reference_###.\nText-to-Image API\n\n\n\nOriginal Image\n\n\n\nBLIP2\n\n\n\nCLIP Interrogator\n\n\n\nOur Attack\n\n\n\n\nDALL-E 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCO2 Emissions Dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDALL-E 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCO2 Emissions Dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDALL-E 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCO2 Emissions Dataset\n###figure_29### ###figure_30### ###figure_31### ###figure_32### ###figure_33### ###figure_34### ###figure_35### ###figure_36### ###figure_37### ###figure_38### ###figure_39### ###figure_40### ###figure_41### ###figure_42### ###figure_43### ###figure_44### ###figure_45### ###figure_46### ###figure_47### ###figure_48### ###figure_49###"
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "Like any other method, our approach may encounter failure cases due to the complexity of its multi-component pipeline. Potential reasons for these failures include:\nThe fine-tuned CLIP model might not retrieve related keywords if the words are rare, or the model itself may fail to identify the closest texts to the images.\nThe multi-label classifier might not select appropriate modifiers.\nGPT-4V might not accurately extract related keywords, named entities, and modifiers, or it might fail to generate an appropriate prompt.\nThe text-to-image models might not produce images that align well with the text.\nThe inherent uncertainty in both multimodal LLMs and text-to-image models adds complexity to the task. We have included some examples from setting 1, which yielded the lowest image similarity scores, to analyze what contributed to the suboptimal results. These examples and their interpretations can be found in the Appendix C  ###reference_###. The multi-component nature of our attack pipeline presents numerous opportunities for refinement. Each component, from the fine-tuning of the CLIP model and the training regimen of the multi-label classifier to the optimization of modifiers extracted from data, holds the potential to elevate the attack\u2019s effectiveness. Moreover, the art of prompting GPT-4V is not monolithic; alternative prompt constructions could yield improved results within our cyclic approach. While this paper establishes a solid foundation, the practical enhancements of each component present as promising directions for future work, offering prospects for even more complex attacks."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Cost Estimation",
            "text": "Original Image\n\n\n\nInitial Prompt\n\n\n\nRound 1\n\n\n\nRound 2\n\n\n\nRound 3\n###figure_50### ###figure_51### ###figure_52### ###figure_53### ###figure_54### ###figure_55### ###figure_56### ###figure_57### ###figure_58### ###figure_59### ###figure_60### ###figure_61### ###figure_62### ###figure_63### ###figure_64### To justify the practicality of our attack, we provide an estimation of its associated costs. These are mainly divided into two main components: the cost of using GPT-4V and the cost associated with the Text-to-Image API. We detail each as follows:\nAccording to OpenAI\u2019s pricing, there is a charge per million tokens for both input and output. To estimate the cost for using GPT-4V, we consider the number of input and output tokens required per round of our attack. Based on our analysis, the average number of input tokens is 900 for the initial prompt and 1165 for each subsequent round. The average number of rounds to achieve maximum similarity is approximately 2.7, leading to a total of 4395 input tokens per test sample. It\u2019s noteworthy that the examples from natural images shown in Figure 6  ###reference_### demonstrate that even with 1-2 rounds, we can achieve an approximation of the targeted image. For output tokens, the average is 381 per round. Given the current rates on OpenAI\u2019s website, the total cost for using GPT-4V, considering both input and output, is approximately $0.09. Additionally, a small fee of approximately $0.03 applies for including images in queries, based on the average number of rounds.\nOur experiments utilize either Midjourney or DALL-E 3, so we calculate the cost for each separately. Based on the rates from Midjourney\u2019s and OpenAI\u2019s websites, the cost per image generation is about $0.03 for Midjourney and $0.04 for DALL-E 3 (for 1024*1024 resolution images). Consequently, the total generation cost using Midjourney is approximately $0.12, and for DALL-E 3, it\u2019s around $0.16.\nThe total cost per sample depends on the text-to-image API used. If the attacker utilizes Midjourney, the cost is approximately $0.235 per sample, and for DALL-E 3, it is around $0.275. These costs are substantially lower than the typical prices in AI-generated image marketplaces (ranging from $3 to $7) and significantly below the cost of stocks of real images (which can be $50 to $500)."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "This research has unveiled a novel attack strategy in the realm of digital imagery, specifically targeting AI-generated image marketplaces and premium stock image providers. By effectively employing state-of-the-art multi-modal models, our method has demonstrated the ability to generate visually comparable content at a significantly reduced cost, challenging the current economic dynamics of the digital imagery landscape. Moreover, the compilation of a large-scale dataset from CO2 emissions is a pivotal contribution to this field. This dataset not only aids our research but also serves as a valuable resource for future studies, offering insights into the capabilities and challenges of text-to-image technologies. In conclusion, our study highlights new threats against digital imagery, stressing the need for urgent action in identifying and countering these risks. Future research should aim at developing protective measures to preserve digital image integrity amidst advancing AI technologies."
        }
    ],
    "url": "http://arxiv.org/html/2404.13784v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "4"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.1",
            "5.2",
            "5.3",
            "5.4",
            "5.5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "5.2",
            "5.3",
            "5.4"
        ]
    },
    "research_context": {
        "paper_id": "2404.13784v1",
        "paper_title": "Iteratively Prompting Multimodal LLMs to Reproduce Natural and AI-Generated Images",
        "research_background": "**Paper Motivation:**\n\nThe paper is motivated by the increasing importance and use of image stocks and marketplaces in the commercial and business sectors. It emphasizes the emergence of new marketplaces specifically for AI-generated images, such as PromptBase, PromptSea, and Neutron Field, where the transaction revolves around trading prompts to create AI-generated images. The motivation also stems from a significant question in this new digital imagery landscape: whether an adversary could identify prompts to generate similar images using current text-to-image models at a lower cost than purchasing stock images, which are often expensive.\n\n**Research Problem:**\n\nThe primary research problem addressed in this paper is identifying vulnerabilities in current AI-generated image marketplaces and traditional stock image domains. Specifically, the research explores if it is possible for an adversary to replicate images by finding and utilizing the appropriate prompts with state-of-the-art text-to-image models. The aim is to investigate whether the latest multimodal models can be used for such attacks to reproduce both AI-generated and natural images, thereby challenging the economic structure of these marketplaces.\n\n**Relevant Prior Work:**\n\n1. **Text-to-Image APIs:**\n   - DALL-E 3 (Betker et al., 2023) and Midjourney are highlighted as leading tools in generating AI visuals.\n \n2. **Prompt Identification:**\n   - Previous works by Cao et al. (2023) and Oppenlaender (2023) indicate that identifying effective prompts is not a straightforward task.\n   \n3. **Image Captioning Models:**\n   - Existing models (Li et al., 2023 and 2022a; Wang et al., 2022; Chen et al., 2022) often provide general descriptions that lack the specificity needed for text-to-image prompts, which limits their direct utility in recovering precise prompts.\n   \n4. **Multimodal Models:**\n   - Models like Gemini and GPT-4V offer detailed descriptions but may miss critical keywords or essential details necessary for generating equivalent images.\n   \n5. **CLIP:**\n   - CLIP Interrogator, while suggesting text and keywords, falls short in providing the necessary detail for natural image descriptions.\n   \nBy synthesizing the capabilities of fine-tuned models and a systematic approach involving a combination of CLIP, a multi-label classifier, and GPT-4V, the study aims to address these gaps in current methodologies and provide a robust attack strategy.\n\n**Contribution:**\n\nThe paper makes several contributions:\n1. Conducts the first systematic investigation into attacks against both AI-generated images from popular text-to-image APIs and natural images from commercial stock collections.\n2. Demonstrates that adversaries can generate comparable images to those offered on AI and traditional marketplaces at a significantly lower cost.\n3. Develops a novel attack strategy combining a fine-tuned CLIP model, a multi-label classifier for identifying keywords, and GPT-4V to generate prompts, refined iteratively.\n4. Collects and preprocesses a large dataset of Midjourney generations, leveraging it for fine-tuning models and providing a resource for future research.\n",
        "methodology": "The proposed method for iteratively prompting multimodal large language models (LLMs) to reproduce natural and AI-generated images consists of three main components: keyword extraction, modifier extraction, and prompt generation. These components work synergistically to enhance the specificity and accuracy of image generation prompts.\n\n### Keyword Extraction\nTo tackle the challenge of image captioning models lacking specificity, particularly in extracting precise keywords and named entities, the authors leverage a substantial dataset from Midjourney. This dataset comprises 2 million pairs of prompts and their corresponding upscaled images. The process involves:\n1. Fine-tuning a CLIP model originally pretrained on 2 billion pairs of captions and images.\n2. Generating a set of 5 million text and image embeddings with the fine-tuned CLIP model.\n3. During inference, using this model to obtain image embeddings of target images and identifying the closest text and image embeddings to extract keywords and named entities.\n\n### Modifier Extraction\nModifiers (adjectives, adverbs, specific styles) are crucial for enhancing text-to-image generations but are often missed by image captioning models and tools like the CLIP Interrogator. To improve modifier extraction:\n1. The authors identify 1800 frequent modifiers from their Midjourney dataset.\n2. Train a Multi-Layer Perceptron (MLP) as a multi-label classifier on a subset of the dataset.\n3. During inference, the top modifiers with the highest probability scores are identified and incorporated into the model.\n\n### Prompt Generation\nFor detailed image description and prompt generation:\n1. The method employs GPT-4V, known for its strong visual understanding, to generate initial prompts based on detailed instructions, the extracted keywords, named entities, and modifiers.\n2. The initial instructions to GPT-4V include a task description, relevant modifiers, keywords, named entities, an example prompt from the Midjourney dataset, and the targeted images.\n3. GPT-4V acts as a selector in a two-level extraction process to determine the most appropriate modifiers, keywords, and named entities.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Model and Dataset\n\nFor the main experiment, the researchers selected the CLIP ViT-G/14 model, pre-trained on 2 billion samples from the LAION dataset. The model is fine-tuned using a subset of 2 million samples specifically from the Midjourney dataset. This subset was chosen to balance computational efficiency and performance.\n\n#### Training Details\n\n- **Learning Rate**: Optimized during iterative experimentation.\n- **Batch Size**: 32\n- **Gradient Accumulation Steps**: 2\n- **Epochs**: 10\n- **Hardware**: Fine-tuning conducted on a cluster with 4 A100 GPUs.\n\n#### Evaluation Metrics\n\nThe evaluation of the model was conducted on a test set consisting of 10,000 samples. The primary metrics used were:\n\n1. **Cosine Similarity**: Between the embeddings generated by CLIP\u2019s image and text encoders.\n2. **Top-1, Top-5, and Top-10 Accuracy**: Determined by whether the ground-truth text was within the top corresponding closest texts for each test image.\n\n#### Baselines and Models\n\nTo evaluate the approach, the researchers also considered baselines such as BLIP2 (an image captioning model) and CLIP Interrogator 2. Various settings were explored using both Midjourney and DALL-E 3 for AI-generated images, while natural images were sourced from Getty Images. Across different scenarios, they aimed to measure performance under multiple prompts, single image prompts, and different generations targeting natural and AI-generated images.\n\n#### Scenarios Setup\n\nThe experimental scenarios involved:\n- **Scenarios 1, 2, 7**: Using Midjourney for generating images.\n- **Scenarios 3-6**: Using DALL-E 3 for generating images.\n- **Image Size and Quality**: denoted standard image size and quality.\n- **GPT-4V**: Employed for multimodal understanding and prompt generation.\n\n#### Evaluation\n\nThe similarity between generated images and ground-truth images was assessed using:\n- **Automated Metrics**: Calculated with cosine similarity termed Clip-score using embeddings from the original CLIP model.\n- **Human Evaluation**: Five annotators rated five random samples from each setting based on a 5-point Likert scale.\n\n#### Results\n\nThe results of the experiment are detailed as follows:\n- **Performance**: The fine-tuned CLIP model significantly outperformed the original, especially noted in the cosine similarity and Top-N accuracy metrics. The model tuned with 2 million samples showed the best performance.\n- **Scenario Analysis**: Across the seven settings, the new approach consistently outperformed the baseline models (BLIP2 and CLIP Interrogator 2). The margin of improvement was substantial except in the first scenario, where performance was close.\n- **Multiple vs. Single Image**: Settings with multiple images showed better performance than those with single images, attributed to the richer information provided by multiple samples.\n- **Cross API Performance**: In scenarios 5 and 6, the effectiveness diminished due to the use of a different API by the attacker."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To fine-tune the CLIP model on a specific subset of the Midjourney dataset to improve its performance in image-text matching tasks.",
            "experiment_process": "The experiment utilizes the CLIP ViT-G/14 model, pre-trained on 2 billion samples from the LAION dataset. Hyperparameter optimization is conducted with a learning rate, a batch size of 32, and gradient accumulation steps set to 2. Fine-tuning is performed on 2 million samples from the Midjourney dataset over 10 epochs using 4 A100 GPUs. The evaluation process involves testing the model on 10,000 samples, utilizing CLIP's encoders to generate embeddings, and measuring cosine similarity to assess Top-1, Top-5, and Top-10 accuracy.",
            "result_discussion": "The fine-tuned CLIP model shows significant improvement over the original model in terms of matching accuracy. The model fine-tuned on 2 million samples achieves the best performance, although the margin of improvement is not substantially higher compared to other dataset sizes.",
            "ablation_id": "2404.13784v1.No1"
        },
        {
            "research_objective": "To train a Multi-Label Classifier for identifying distinct modifiers within images from the Midjourney dataset to enhance descriptive capabilities.",
            "experiment_process": "An MLP architecture with three hidden layers and ReLU activation is used along with a learning rate of . For overfitting mitigation, a dropout rate is employed. The training runs over epochs using an RTX 8000 GPU. Approximately distinct modifiers are identified, and samples are labeled based on these modifiers. The dataset is split, reserving a part as a validation set. Precision and recall are the primary evaluation metrics, with top modifiers selected based on classifier scores during inference.",
            "result_discussion": "The classifier's performance shows that increasing the threshold of top modifiers decreases precision but increases recall, highlighting a trade-off. The chosen balance achieves satisfactory precision and recall, making it suitable for subsequent attack evaluations.",
            "ablation_id": "2404.13784v1.No2"
        },
        {
            "research_objective": "To evaluate the effectiveness of the proposed image generation attack strategy across different text-to-image APIs and natural image sources.",
            "experiment_process": "The evaluation includes Midjourney and DALL-E 3 for AI-generated images, and Getty Images for natural images, across seven distinct scenarios involving multiple and single images. GPT-4V is used for visual understanding and prompt generation. Human evaluations with a 5-point Likert scale and automated Clip-score based on cosine similarity from the original CLIP model are employed to measure similarity. Baselines include BLIP2 and CLIP Interrogator 2.",
            "result_discussion": "The proposed approach consistently outperforms baselines in most scenarios. The CLIP Interrogator shows close performance only in the first scenario. Performance diminishes with a different API attacker due to varied data processing. Scenarios with multiple images perform better than single-image settings due to higher information coverage.",
            "ablation_id": "2404.13784v1.No3"
        }
    ]
}