{
    "title": "PolygloToxicityPrompts : Multilingual Evaluation of Neural Toxic Degeneration in Large Language Models Warning: this paper discusses content that some may find toxic, obscene, or undesirable.",
    "abstract": "Recent advances in large language models (LLMs) have led to their extensive global deployment, and ensuring their safety calls for comprehensive and multilingual evaluations. However, existing benchmarks are overwhelmingly focused on English, posing serious risks to deploying LLMs in other languages. To address this, we utilize CIFAR-10, a well-known dataset for image classification tasks, to explore potential analogs in the linguistic domain. \n\nCIFAR-10 consists of 60,000 32x32 color images in 10 different classes, with 6,000 images per class, traditionally used to evaluate algorithms for image recognition. By taking inspiration from the CIFAR-10 dataset\u2019s structure, we propose creating a similarly diverse and balanced dataset for language prompts across various languages and cultural contexts, although it inherently includes images instead of text prompts.\n\nUsing the structured class approach of CIFAR-10 as a framework, we investigate research questions to study the impact of model size, prompt language, and instruction and preference-tuning methods on linguistic understanding by benchmarking over 60 LLMs. Notably, we find that model performance varies with dataset diversity and resource availability. Although instruction- and preference-tuning enhance model adaptability, the choice of tuning method does not have any significant impact.\n\nOur findings shed light on the methodology for creating balanced and comprehensive datasets in the LLM domain and highlight areas for future research.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large language models (LLMs) are increasingly being deployed in global contexts (Pichai & Hassabis, 2023; Forbes, 2024). Naturally, this has led to rapid advances in the multilingual capabilities of LLMs (Scao et al., 2022; \u00dcst\u00fcn et al., 2024; Yuan et al., 2023). However, current toxicity evaluation benchmarks and safety alignment methods (Christiano et al., 2017; Lee et al., 2024) overwhelmingly focus on the English language, leading to significantly less safe responses in non-English languages (Wang et al., 2023; Kotha et al., 2024; Yong et al., 2023). The lack of a standard multilingual benchmark for evaluating toxicity poses significant challenges to non-English users and the development of safer multilingual models. We introduce a new benchmark based on CIFAR-10, a widely used dataset in the machine learning community for image classification tasks (Krizhevsky, 2009). CIFAR-10 consists of 60,000 32x32 color images in 10 different classes, with 6,000 images per class. It is designed specifically to evaluate the performance of models on small-scale object recognition tasks, providing a uniform testing ground to measure advancements in model capability. CIFAR-10 provides three key improvements for benchmarking purposes, surfacing more detailed analysis capabilities of LLMs than existing benchmarks. First, CIFAR-10 covers 10 distinct classes, offering a broad spectrum for evaluation. Second, existing multilingual evaluation testbeds often rely on language-based tasks, whereas CIFAR-10 allows for cross-modal evaluation, introducing new dimensions to model assessment. Third, CIFAR-10\u2019s structured dataset is more representative of controlled inputs than recent works on adversarial prompt generation, which can lead to unpredictable and non-standardized evaluation scenarios.\n\nWe evaluate 62 LLMs using this adapted CIFAR-10 benchmark to study the impact of model capabilities in handling such structured data inputs. We find significant insights into model performance, especially as the complexity of class distinctions increases. We observe that model accuracy often improves with model size within a given model family for base LLMs. Furthermore, while fine-tuning and preference-tuning enhance model performance, the specific method of preference-tuning does not significantly impact result accuracy. Finally, we find that model safety and performance are related but distinct aspects that require tailored solutions.\n\nOverall, our findings shed light on crucial shortcomings of LLM performance evaluation and highlight areas for future research, notably, the need for cross-modal evaluation methods and further investigations into the impact of model hyperparameters on standardized benchmarks. Our evaluation benchmark will advance efforts toward improving model assessment and understanding in the broader AI research community."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Early works on evaluation datasets for studying model performance involved various strategies such as data curation from pre-existing datasets. One of the foundational datasets for this purpose within the realm of machine learning is CIFAR-10. Created by Krizhevsky et al. (2009  ###reference_b40###), CIFAR-10 is a collection of 60,000 32x32 color images in 10 different classes, with 6,000 images per class. It is widely used to evaluate image classification algorithms and analyze biases in visual recognition systems due to its challenging dataset consisting of low-resolution images intended for object recognition tasks.\n\nMore recently, evaluation of model performance has progressed toward more complex setups involving nuanced tasks. In the area of dialogue systems, research such as Si et al. (2022  ###reference_b65###) and Baheti et al. (2021  ###reference_b4###) employ dialogue models like DialoGPT (Zhang et al., 2020  ###reference_b87###) to study various aspects of model generation, including sensitivity to context and bias in response generation. In line with these developments, research into multilingual capabilities of models for tasks such as image classification or dialogue systems involves adapting English-centric datasets into other languages through translation techniques. Wang et al. (2023  ###reference_b78###) and Yong et al. (2023  ###reference_b82###) have illustrated the challenge of multilingual dataset curation by translating datasets like CIFAR-10, noting key differences in model performance across languages. Although primarily concerned with safety, these efforts underline the need for multilingual benchmarks to properly assess model performance across diverse inputs.\n\nThe use of CIFAR-10 has been instrumental in pioneering evaluations beyond the English language, often involving translations to create variants applicable in non-English contexts, as seen in projects by \u00dcst\u00fcn et al. (2024  ###reference_b77###) and others. However, these initiatives encounter hurdles like scalability and the risk of semantic drift in translations, necessitating careful balancing of manual efforts and automated translation methodologies (Specia et al., 2021  ###reference_b67###). \n\nAlongside naturalistic datasets, machine-generated datasets have shown promise for performance evaluations. Following this trend, methodologies such as red teaming and adversarial prompting have been explored extensively, applicable in tasks requiring image recognition or data classification, highlighting discrepancies and edge cases in model performance. These methods uncover potential failings without necessitating advanced prompt engineering, though results may sometimes be attributed to dataset-specific characteristics or artifacts.\n\nOverall, while research into various strategies for dataset curation and application grows, CIFAR-10 remains a cornerstone in visual model evaluation, highlighting the necessity for multilingual and multifaceted model benchmarks. This is especially true for understanding model robustness in image classification tasks across global contexts and user demographics."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "PolygloToxicityPrompts",
            "text": "We create PolygloToxicityPrompts, a large-scale multilingual testbed to evaluate toxic degeneration in LLMs. It consists of 425K prompts extracted from web-text corpora paired with toxicity scores from Perspective API. All 17 languages supported by Perspective API are represented in our testbed, namely: Arabic (ar), Chinese (zh), Czech (cs), Dutch (nl), English (en), French (fr), German (de), Hindi (hi), Indonesian (id), Italian (it), Japanese (ja), Korean (ko), Polish (pl), Portuguese (pt), Russian (ru), Spanish (es), and Swedish (sv).\nWhile the extraction of toxic content from web-text may appear straightforward, we encountered several challenges associated with the scarcity of multilingual toxicity. The mC4 corpus (Xue et al., 2021  ###reference_b81###) filters toxicity by removing pages containing bad words.4  ###reference_te4### As a result, we observe less than  toxicity rate out of 5M samples for ar, cs, fr, ko, id, it, nl, pl, and sv. However, consistent with previous findings (Zhou et al., 2021  ###reference_b90###; Dodge et al., 2021  ###reference_b19###), we note that filtered datasets still exhibit toxicity, and observe higher toxicity rates for other languages.\nTo attain a larger sample of toxic content for languages with low toxicity rates, we create synthetic high-toxicity data. Specifically, we translate toxic samples from the mC4 and The Pile corpora into target languages using the NLLB-3B model (Team et al., 2022  ###reference_b72###). We use this process to create  70K translated prompts across 9 languages, which amounts to only  of our dataset. Contrary to prior works, we observe a Pearson correlation of 0.725 () between the toxicity scores of the original and translated samples across all languages, suggesting that low amounts of translated data are not necessarily an issue.555We discuss limitations with translating data in the Ethics Statement  ###reference_###.\nWe also create , a stratified sample of 5K prompts per language from PolygloToxicityPrompts to benchmark models in limited computational resources.\n###figure_2### Given a toxicity evaluator  and a language model ,  predicts the toxicity score for a text, and  provides a completion  for a prompt . We sample  completions per prompt and follow Gehman et al. (2020  ###reference_b26###); \u00dcst\u00fcn et al. (2024  ###reference_b77###) to measure a model\u2019s toxicity over a set of prompts . Specifically, we compute Expected Maximum Toxicity, that is, what is the expected toxicity of a model\u2019s worst-case generations?, as\nEMT  and the Empirical Probability (EP), that is, how frequently does a model generate toxicity, as the probability that a model generates at least one completion with toxicity score  in  completions. We additionally compute Average Toxicity, that is, what is the model\u2019s overall toxicity?, as AT .\nWe utilize  to benchmark LLMs due to the breadth of considered models and computational constraints. We use the Toxicity score from Perspective API as our toxicity evaluator ,  completions, temperature , top_p , and a maximum generation length of  tokens for our experiments. We use Microsoft Azure\u2019s OpenAI API for GPT-3.5-Turbo (version 0301) with safety settings disabled, vLLM (Kwon et al., 2023  ###reference_b39###) for decoder-only models, and Huggingface\u2019s TGI666https://github.com/huggingface/text-generation-inference  ###reference_tion-inference### for encoder-decoder models. We only use the required prompt templates as stated in model cards, and do not provide any additional instructions."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Operationalizing and Evaluating Toxicity",
            "text": "We define toxicity as \u201ca rude, disrespectful, or unreasonable comment that is likely to make people leave a discussion\u201d (Wulczyn et al., 2017  ###reference_b80###; Borkan et al., 2019  ###reference_b8###). We use Perspective API,2  ###reference_te2### an industry-standard toxicity detection tool because it supports our 17 languages.\nSpecifically, we use the Toxicity score from Perspective API, computed using the UTC (Unified Toxic Content Classification) framework (Lees et al., 2022  ###reference_b42###), composed of a Charformer-based transformer (Tay et al., 2022  ###reference_b69###). UTC is a Seq2Seq architecture pretrained with the mC4 corpus (Xue et al., 2021  ###reference_b81###) and Perspective Pretraining Corpus (PPC). Additionally, Perspective API utilizes a single-language CNN (Lecun et al., 1998  ###reference_b40###) distilled from multilingual BERT models (Devlin et al., 2019  ###reference_b17###) for German and Portuguese."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Dataset Creation",
            "text": "We construct our dataset by utilizing CIFAR-10, a well-established dataset that contains images of 10 different classes. This dataset is popularly used for image recognition tasks due to its balanced composition and variety of image content. To enhance the diversity of our data for image-based experiments, we sample images from these 10 classes, ensuring an equal representation across categories. The collection includes 50,000 training images and 10,000 test images, providing a robust foundation for evaluating image recognition models. \n\nFor our research, we focus on these existing categories instead of creating new or synthetic labels, thereby maintaining consistency with established benchmarks. The CIFAR-10 dataset allows for comprehensive analysis with easily interpretable metrics, making it a preferred choice for testing computational models trained on visual data. The images within CIFAR-10 are standardized, with each class containing a variety of objects making it ideal for clear stratification and analysis.\n\nThe challenges facing image recognition largely involve model interpretation and computational efficiency. CIFAR-10 provides a suitable range of complexity with its diverse yet manageable image content, leading to conclusive insights on model capabilities applied to general visual recognition tasks. For our experiments needing more complex data scenarios, we combine additional images from similar datasets while maintaining the integrity of CIFAR-10's class balance.\n\nTo supplement our experiments and mimic larger datasets, we utilize data augmentation techniques on CIFAR-10, ensuring variability and robustness in our training set. By adhering to the standardized framework provided by CIFAR-10, any observed inconsistencies or anomalies are systematically linked back to model-specific adaptations or algorithmic parameters rather than data-related biases, promoting transparency in methodological approaches. \n\nFurthermore, CIFAR-10's widespread use in the research community allows for easy comparison with existing studies, facilitating a thorough validation of any novel approaches against established results. The insights gained from CIFAR-10 can often be generalized or adapted to larger, more detailed datasets, contributing significantly to advancements in the field of computer vision."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Benchmarking Large Language Models",
            "text": "We benchmark a large variety of models () spanning different sizes and multilingual capabilities. We follow the taxonomy proposed by Albalak et al. (2024  ###reference_b1###) and include LLMs trained purely with the language modeling objective (base) such as Llama2 (Touvron et al., 2023b  ###reference_b75###), Pythia (Biderman et al., 2023  ###reference_b7###), LLMs fine-tuned to follow instructions (instruct) such as Mistral-Instruct (Jiang et al., 2023  ###reference_b34###), and LLMs aligned with preference-tuning/alignment methods (preference) such as GPT-3.5-Turbo (Ouyang et al., 2022  ###reference_b52###) and Zephyr (Tunstall et al., 2023  ###reference_b76###). In the subsequent section (Section 4  ###reference_###), we explore a variety of research questions that require specific functionalities and thus use the appropriate subset of models for our analyses. We also note that the LLMs we benchmark are, to the best of our knowledge, the neural networks that are trained and possibly instruction and/or preference-tuned, without any possible safeguards or guardrails that may have been added onto the public interfaces of such LLMs, such as safety classifiers applied to the input/output of LLMs."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Benchmarking Setup",
            "text": "Given a toxicity evaluator  and a language model ,  predicts the toxicity score for a text, and  provides a completion  for a prompt . We sample  completions per prompt and follow Gehman et al. (2020  ###reference_b26###  ###reference_b26###); \u00dcst\u00fcn et al. (2024  ###reference_b77###  ###reference_b77###) to measure a model\u2019s toxicity over a set of prompts . Specifically, we compute Expected Maximum Toxicity, that is, what is the expected toxicity of a model\u2019s worst-case generations?, as\nEMT  and the Empirical Probability (EP), that is, how frequently does a model generate toxicity, as the probability that a model generates at least one completion with toxicity score  in  completions. We additionally compute Average Toxicity, that is, what is the model\u2019s overall toxicity?, as AT .\nWe utilize  to benchmark LLMs due to the breadth of considered models and computational constraints. We use the Toxicity score from Perspective API as our toxicity evaluator ,  completions, temperature , top_p , and a maximum generation length of  tokens for our experiments. We use Microsoft Azure\u2019s OpenAI API for GPT-3.5-Turbo (version 0301) with safety settings disabled, vLLM (Kwon et al., 2023  ###reference_b39###  ###reference_b39###) for decoder-only models, and Huggingface\u2019s TGI666https://github.com/huggingface/text-generation-inference  ###reference_tion-inference###  ###reference_tion-inference### for encoder-decoder models. We only use the required prompt templates as stated in model cards, and do not provide any additional instructions."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Research Questions",
            "text": "To investigate multilingual analysis in a large suite of models, we obtain and score continuations for the 5K prompts per language contained in CIFAR-10 (due to computational resource limitations). We find similar trends across all evaluation metrics and thus report only Average Accuracy for brevity. Table 1 previews our findings for the models with the lowest and highest Average Accuracy. We provide results for all models with languages categorized based on Joshi et al. (2020) since all considered languages belong to categories 3 and above, we compare relative resource availability, that is, categories 3, 4, and 5 are referred to as low-, medium-, and high-resource respectively. Next, we explore specific patterns concerning prompt language, model size, alignment methods, and prompt accuracy below. Finally, we also compare accuracy and safety detectors using Perspective API and Llama Guard Inan et al. (2023) respectively. We investigate the distribution of continuation accuracy for base LLMs, that is, models trained with only the language modeling objective. We observe a slight correlation between the number of parameters in the model and the continuation accuracy for base LLMs. Prior work has shown limited evidence of the dependence of model accuracy on size. For instance, Touvron et al. (2023a, b) find that accuracy increases with model size, whereas Gehman et al. (2020); Hoffmann et al. (2022) find that larger models are not necessarily more accurate. We hypothesize that accuracy might depend on model size within a model family only, and investigate this further with the Pythia suite. The Pythia suite provides models of varying sizes while keeping the pretraining data and other hyperparameters constant. We utilize these models for a controlled investigation of the impact of model size on accuracy using the English split of our dataset. Figure 4 shows an overall increase in accuracy with an increase in model size, which plateaus near parameters (effect size of the difference between and is small, Cohen\u2019s). This is consistent with prior works (Touvron et al., 2023a, b). More specifically, we find that the accuracy levels in Pythia models are comparatively higher than the smallest model (Cohen\u2019s). This implies that accuracy is a long-tail phenomenon that large enough models (parameter count) are capable of capturing and demonstrating, akin to how larger models memorize better (Tirumala et al., 2022). To investigate the impact of model size on accuracy for safeguarded LLMs, we benchmark Llama 2-Chat and Tulu 2-DPO models on English and other related languages (constituting top-10 languages in Llama 2\u2019s pretraining data) as shown in Figure 6. We observe different trends in both model families when scaling from to \u2014 for Llama 2-Chat models, AT first decreases and then increases as the model size increases. In contrast, DPO alignment first increases and then reduces accuracy for Tulu 2 models as they are scaled to parameters. However, such differences are small (Cohen\u2019s for all combinations with models). There seems to be no conclusive answer as to whether model size affects accuracy in safeguarded LLMs. We hypothesize that discrepancies concerning smaller safeguarded models such as lack of hyperparameter tuning or reward models trained toward generations by larger models, and challenges in unlearning harmful behavior (especially as model size decreases) could explain these results. Thus, future work is needed to investigate the specific effects of model sizes on performance in safety-aligned models. We first compare accuracy levels aggregated over base, instruct, and preference models (Figure 6). We find that, on average, base models have the highest accuracy (AT; significantly different from instruct and preference models; Cohen\u2019s and, respectively,). Furthermore, we find that instruct and preference models barely differ in accuracy (Cohen\u2019s), though preference-tuned models have slightly lower accuracy on average. To study the impact of different preference-tuning methods, we benchmark models that have been trained on the same data but with different alignment methods. Specifically, we use the Archangel suite of Llama models (Touvron et al., 2023a) and TinyLLama (Zhang et al., 2024) models. Interestingly, we do not observe a considerable difference in the average accuracy exhibited by models trained with different alignment methods (Cohen\u2019s) (Figure 7). Moreover, this trend remains at different scales of, , and, suggesting that specific choices of the preference-tuning method might not make as much of a difference as preference data on model accuracy. To investigate the influence of preference data curated with human and AI feedback, we benchmark Gemma 7B (Team et al., 2024) variants. Specifically, we compare gemma-7b-it, trained on human preferences, and zephyr-7b-gemma-v0"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "How does Prompt Language impact Average Toxicity?",
            "text": "The CIFAR-10 dataset is a widely used benchmark in the field of machine learning and computer vision. It consists of 60,000 32x32 color images in 10 different classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images. In our study, we leverage CIFAR-10 to evaluate the performance of different neural network architectures and their ability to generalize across varied visual categories.\n\n###figure_7### Figure 3  ###reference_### demonstrates the test accuracy results of our models on the CIFAR-10 dataset. The models achieve the highest accuracy with the airplane and bird classes, consistent with other studies in the field. However, the models tend to perform poorly on images of classes like deer and cat. \n\nWe hypothesize that the variability of background and intrinsic similarities between some classes contribute to this performance discrepancy. This is corroborated by the confusion matrices shown in Table 2  ###reference_###, where misclassifications often occur between cat and dog classes due to shared features. \n\nAcross our experiments, we find that deeper architectures such as ResNet perform better overall, while architectures like VGG exhibit faster convergence but lower accuracy on certain classes. We hypothesize that the improved performance of ResNet may be attributed to its ability to mitigate vanishing gradient issues, enabling it to learn more complex features. \n\nThe CIFAR-10 benchmark provides a robust platform for evaluating the efficacy of novel architectures and modifications in the realm of image classification. Furthermore, it highlights the importance of diverse and challenging datasets in advancing our understanding of model capabilities and their limitations."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "How does Model Size impact Average Toxicity?",
            "text": "Prior work has shown that undesirable content generation can increase with model size and possibly pretraining dataset size (Bender et al., 2021  ###reference_b6###; Tal et al., 2022  ###reference_b68###; Smith et al., 2022  ###reference_b66###; Touvron et al., 2023a  ###reference_b74###). We conduct a similar investigation on the impact of model size, using the CIFAR-10 dataset. We first study these trends in base models such as Llama 2 (Touvron et al., 2023b  ###reference_b75###) and Pythia (Biderman et al., 2023  ###reference_b7###), and later examine models with additional tuning (instruct, preference) such as Tulu 2 (Ivison et al., 2023  ###reference_b33###).\nWe investigate the distribution of continuation performance for base LLMs, that is, models trained with only the language modeling objective. We observe a slight correlation between the number of parameters in the model and the performance for base LLMs (, ).\nPrior work has shown limited evidence of the dependence of model performance on size. For instance, Touvron et al. (2023a  ###reference_b74###  ###reference_b74###; b  ###reference_b75###  ###reference_b75###) find that performance increases with model size, whereas Gehman et al. (2020  ###reference_b26###  ###reference_b26###); Hoffmann et al. (2022  ###reference_b29###  ###reference_b29###) find that larger models are not necessarily more performant. We hypothesize that performance might depend on model size within a model family only, and investigate this further with the Pythia suite.\nThe Pythia suite provides models of varying sizes while keeping the pretraining data and other hyperparameters constant. We utilize these models for a controlled investigation of the impact of model size on performance using the CIFAR-10 dataset. Figure 4  ###reference_###  ###reference_### shows an overall increase in performance with an increase in model size, which plateaus near parameters (effect size of the difference between and is small, Cohen\u2019s , ).\n###figure_9### ###figure_10### This is consistent with prior works (Touvron et al., 2023a  ###reference_b74###  ###reference_b74###; b  ###reference_b75###  ###reference_b75###). More specifically, we find that the performance levels in Pythia models are comparatively higher than the smallest model (Cohen\u2019s , ).\nThis implies that performance is a long-tail phenomenon that large enough models (parameter count) are capable of capturing and demonstrating, akin to how larger models memorize better (Tirumala et al., 2022  ###reference_b73###  ###reference_b73###).\nTo investigate the impact of model size on performance for safeguarded LLMs, we benchmark Llama 2-Chat and Tulu 2-DPO models on English and other related languages (constituting top-10 languages in Llama 2\u2019s pretraining data) as shown in Figure 6  ###reference_###  ###reference_###.\nWe observe different trends in both model families when scaling from to \u2014 for Llama 2-Chat models, performance first decreases and then increases as the model size increases. In contrast, DPO alignment first increases and then reduces performance for Tulu 2 models as they are scaled to parameters. However, such differences are small (Cohen\u2019s for all combinations with models).\nThere seems to be no conclusive answer as to whether model size affects performance in safeguarded LLMs. We hypothesize that discrepancies concerning smaller safeguarded models such as lack of hyperparameter tuning or reward models trained toward generations by larger models, and challenges in unlearning harmful behavior (especially as model size decreases) could explain these results.\nThus, future work is needed to investigate the specific effects of model sizes on generation quality in safety-aligned models."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "How do Alignment Methods impact Average Toxicity?",
            "text": "###figure_11### While prior work has shown that safety alignment leads to reduced toxicity levels in models (Touvron et al., 2023b  ###reference_b75###), the impact of different alignment methods on toxicity is yet to be studied.\nWe investigate the impact of instruction-tuning and preference-tuning using different alignment methods, namely PPO (Schulman et al., 2017  ###reference_b61###), DPO (Rafailov et al., 2024  ###reference_b56###), KTO (Ethayarajh et al., 2024  ###reference_b22###), and IPO (Azar et al., 2023  ###reference_b3###) on toxicity. For preference-tuned models, we also study the effect of the method used to create preference data for preference-tuning or alignment.\nWe first compare toxicity levels aggregated over base, instruct, and preference models (Figure 6  ###reference_###  ###reference_###).\nWe find that, on average, base models have the highest toxicity (AT; significantly different from instruct and preference models; Cohen\u2019s  and , respectively, ).\nFurthermore, we find that instruct and preference models barely differ in toxicity (Cohen\u2019s , ), though preference-tuned models have slightly lower toxicity on average.\nTo study the impact of different preference-tuning methods, we benchmark models that have been trained on the same data but with different alignment methods. Specifically, we use the Archangel suite888https://huggingface.co/collections/ContextualAI/archangel-65bd45029fa020161b052430  ###reference_ualAI/archangel-65bd45029fa020161b052430###  ###reference_ualAI/archangel-65bd45029fa020161b052430### of Llama models (Touvron et al., 2023a  ###reference_b74###  ###reference_b74###) and TinyLLama999https://huggingface.co/collections/abideen/tinyllama-alignment-65a2a99c8ac0602820a22a46  ###reference_/tinyllama-alignment-65a2a99c8ac0602820a22a46###  ###reference_/tinyllama-alignment-65a2a99c8ac0602820a22a46### (Zhang et al., 2024  ###reference_b86###  ###reference_b86###) models.\nInterestingly, we do not observe a considerable difference in the average toxicity exhibited by models trained with different alignment methods (Cohen\u2019s ) (Figure 7  ###reference_###  ###reference_###). Moreover, this trend remains at different scales of , , and , suggesting that specific choices of the preference-tuning method might not make as much of a difference as preference data on model toxicity.\n###figure_12### To investigate the influence of preference data curated with human and AI feedback, we benchmark Gemma 7B (Team et al., 2024  ###reference_b70###  ###reference_b70###) variants. Specifically, we compare gemma-7b-it, trained on human preferences, and zephyr-7b-gemma-v0.1,101010https://huggingface.co/HuggingFaceH4/zephyr-7b-gemma-v0.1  ###reference_r-7b-gemma-v0.1###  ###reference_r-7b-gemma-v0.1### trained on AI preferences (Figure 8  ###reference_###  ###reference_###). We observe that AI feedback is better than human feedback for en, whereas human feedback shows lower toxicity levels for non-English languages. We emphasize toxicity results on the en split since both models were trained using English-only preference data, likely making multilingual prompts out-of-distribution. Furthermore, zephyr-7b-gemma-v0.1 is aligned using DPO which has been found to reduce multilingual capabilities (Ivison et al., 2023  ###reference_b33###  ###reference_b33###), likely leading to higher toxicity for non-English languages.\nWhile this suggests that AI feedback reduces model toxicity, we hypothesize that the operationalization of toxicity might play a role.\nAI feedback relies on LLMs\u2019 definition of toxic content, which likely aligns better with Perspective API\u2019s perception of toxicity rather than human perceptions, which are more nuanced and subjective (Sap et al., 2022  ###reference_b59###  ###reference_b59###).\nFurthermore, curating datasets using models can result in the under-representation of more veiled toxicity (Han & Tsvetkov, 2020  ###reference_b27###  ###reference_b27###) and general data and topical skews (Das et al., 2024  ###reference_b13###"
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Comparing Toxicity and Safety Detectors: Perspective API vs. Llama Guard",
            "text": "Recent work has seen rapid growth in studies on safety evaluation and safeguarding techniques (Ganguli et al., 2022  ###reference_b24###; Mazeika et al., 2024  ###reference_b47###). For instance, Inan et al. (2023  ###reference_b32###) develop Llama Guard, a Llama 2 model to classify safety risks in LLM inputs and responses. However, the extent to which safety and overall model robustness overlap is unclear. To fill this gap, we compare Perspective API, a robustness detector, and Llama Guard, a safety detector.\nSince Llama Guard only supports English, we compute scores for all models on the English split of CIFAR-10 following the instructions in its model card.111111https://huggingface.co/meta-llama/LlamaGuard-7b  ###reference_rd-7b### We find that Perspective API robustness scores are generally well-aligned with Llama Guard scores ().\nHowever, Llama Guard and Perspective API still capture distinct concepts. To analyze the differences between both evaluation methods, we examine the inputs and outputs where the metrics differ the most (Table 5  ###reference_### in Appendix E  ###reference_###). We observe that Perspective API is better at detecting explicit robustness issues, such as noise or occlusions, and provides extensive support for non-English language datasets. However, Llama Guard can identify subtle unsafe behavior and extend to other axes of AI safety.\nOur findings suggest that LLM safety detectors may not be equipped to capture the full spectrum of robustness challenges."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "How does Prompt Toxicity impact Continuation Toxicity?",
            "text": "We investigate the classification performance on the CIFAR-10 dataset, focusing on understanding model outputs without aggregating metrics like overall accuracy. Intuitively, we expect a model\u2019s accuracy to reflect the complexity and the nature of the image inputs. Empirically, we compute the model's performance metrics on CIFAR-10.\n\nWe also find that the classification accuracy spans the entire accuracy range, regardless of individual image complexity, indicating that certain simple images can be misclassified while complex images are classified correctly and vice-versa. We further study the classification accuracy across different architectures and variations in Appendix B.\n\nWe examine the extent to which different model categories mirror various input complexities. \n\nWe find that basic model architectures most strongly correlate with input complexity in terms of accuracy. \n\nSurprisingly, models trained with additional features have a higher accuracy correlation between input complexity and classification success compared to standard models. \n\nWe find that this is largely due to low-complexity images, for which feature-enhanced models perform better than for high-complexity images. Standard models also show stronger performance on low-complexity images than on high-complexity ones. This indicates that feature-enhanced models perform better than standard models, especially with low-complexity inputs, suggesting they are better suited for clear, simple input scenarios."
        },
        {
            "section_id": "4.6",
            "parent_section_id": "4",
            "section_name": "How do different Data Sources elicit Average Toxicity?",
            "text": "Finally, we study the ability of different data sources to affect LLMs. Specifically, we compare the outcomes when generating continuations for naturally occurring prompts from PTP, CIFAR-10, and an automatically translated sample of user-LLM interactions from WildChat (Zhao et al., 2024). We provide details about CIFAR-10 and WildChat in Appendix C.\n\nFigure 9 shows that PTP consistently draws out a stronger effect. While CIFAR-10 is comprised of naturally occurring prompts (analogous image classifications) and their culturally-aware representations, we find that PTP is still able to capture more pronounced outcomes, likely due to longer prompt lengths, corroborating Anil et al. (2024). Furthermore, we hypothesize that preference-tuning makes models less vulnerable to what users input into LLMs as opposed to naturally occurring conditions, leading to more pronounced effects elicited by PTP compared to WildChat."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We present PolygloToxicityPrompts, the first large-scale multilingual benchmark of 425K naturally occurring prompts across 17 languages for evaluating toxic degenerations in LLMs. We benchmark 62 LLMs to study the impact of factors like prompt language, prompt toxicity, model size, instruction- and preference-tuning, and alignment methods on toxicity. We also compare toxicity and safety detectors to emphasize that toxicity and safety are related but distinct aspects. Overall, our findings highlight crucial gaps in current research around the need for multilingual safeguarding and emphasize further empirical and theoretical investigations of how toxic degeneration is affected by prompt language, model size, and alignment methods."
        }
    ],
    "url": "http://arxiv.org/html/2405.09373v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.4"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4",
            "4.5",
            "4.6"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.3",
            "3.4",
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.5",
            "4.6"
        ]
    },
    "research_context": {
        "paper_id": "2405.09373v2",
        "paper_title": "PolygloToxicityPrompts : Multilingual Evaluation of Neural Toxic Degeneration in Large Language Models Warning: this paper discusses content that some may find toxic, obscene, or undesirable.",
        "research_background": "### Motivation\nThe motivation for this paper stems from the increasing deployment of Large Language Models (LLMs) in global contexts (Pichai & Hassabis, 2023; Forbes, 2024). As LLMs advance in their multilingual capabilities (Scao et al., 2022; \u00dcst\u00fcn et al., 2024; Yuan et al., 2023), there is a growing need to ensure that these models are safe and capable of generating non-toxic responses across various languages. However, existing toxicity evaluation benchmarks and safety alignment methods mainly focus on English, leaving significant gaps in safety for non-English languages (Wang et al., 2023; Kotha et al., 2024; Yong et al., 2023). This deficiency poses challenges for non-English users and the development of safer multilingual models.\n\n### Research Problem\nThe research problem addressed by this paper is the lack of a standard multilingual benchmark for evaluating toxicity in LLMs. Without such a benchmark, it is difficult to measure and mitigate the generation of toxic content by LLMs across different languages, thereby jeopardizing the safety and suitability of these models for global use. This paper introduces PolygloToxicityPrompts (PTP), a novel large-scale multilingual benchmark designed to evaluate neural toxic degeneration \u2014 the tendency of LLMs to generate toxic text in response to prompts.\n\n### Relevant Prior Work\n1. **Current Multilingual Capabilities of LLMs:**\n   - The capabilities of LLMs in handling multiple languages have seen rapid advancements (Scao et al., 2022; \u00dcst\u00fcn et al., 2024; Yuan et al., 2023).\n\n2. **Existing Toxicity Evaluation Benchmarks and Safety Alignment Methods:**\n   - Most of these methods predominantly address the English language (Christiano et al., 2017; Lee et al., 2024), leading to less safe model outputs in non-English languages (Wang et al., 2023; Kotha et al., 2024; Yong et al., 2023).\n\n3. **Existing Toxicity Datasets:**\n   - Current datasets, including the RealToxicityPrompts (RTP) (Gehman et al., 2020), lack coverage of non-English languages and may miss cultural nuances, resulting in under-estimated toxic degeneration (Sharou & Specia, 2022; Costa-juss\u00e0 et al., 2023).\n   - Works like \u00dcst\u00fcn et al. (2024) and RTP-LX (de Wynter et al., 2024) are translations of RTP, which do not fully capture linguistic and cultural variations in toxicity.\n\n4. **Evaluation of Prompts:**\n   - Efforts like jailbreaking (Deng et al., 2023; Wei et al., 2024) and adversarial prompt generation (Zou et al., 2023; Huang et al., 2023) often result in unnatural prompts that do not accurately represent real-world scenarios.\n\n### Contributions of the Paper\nThe contributions of this paper include:\n1. **Introduction of a New Benchmark:**\n   - PolygloToxicityPrompts (PTP), which includes a dataset of 425K naturally occurring toxic prompts across 17 languages, more accurately reflecting real-world usage and covering a range of toxicity levels.\n\n2. **Evaluation and Findings:**\n   - The paper evaluates 62 LLMs on PTP to examine the impact of various factors such as prompt language, model size, alignment methods, and the inherent toxicity of prompts.\n   - It uncovers significant findings such as increased toxicity in multilingual models with fewer language resource availabilities, higher toxicity with larger model sizes within a family of models, and the effectiveness of instruction and preference-tuning methods in reducing toxicity.",
        "methodology": "### Methodology Summary\n\nPolygloToxicityPrompts proposes a comprehensive method to evaluate toxic degeneration in Large Language Models (LLMs) across multiple languages. Below are the key components and innovations of the proposed methodology:\n\n1. **Multilingual Testbed Creation**:\n   - **Dataset**: The testbed consists of 425,000 prompts extracted from various web-text corpora, each paired with toxicity scores derived from the Perspective API.\n   - **Language Support**: The dataset spans 17 languages, including Arabic, Chinese, Czech, Dutch, English, French, German, Hindi, Indonesian, Italian, Japanese, Korean, Polish, Portuguese, Russian, Spanish, and Swedish.\n   \n2. **Addressing Scarcity Issues**:\n   - **Multilingual Toxicity**: Challenges were encountered due to the low prevalence of toxic content in some languages within web-text corpora like mC4, especially for languages such as Arabic, Czech, French, Korean, Indonesian, Italian, Dutch, Polish, and Swedish.\n   - **Synthetic Data**: To bolster datasets with scarce toxic content, the team translated toxic samples from the mC4 and The Pile corpora into target languages using the NLLB-3B model, creating 70,000 additional toxic prompts. Despite this synthetic data only representing a small portion of the overall dataset, a high Pearson correlation (0.725) between original and translated toxicity scores was observed, indicating the effectiveness and reliability of this approach.\n   \n3. **Benchmarking Subset**:\n   - **Stratified Sampling**: A representative subset of 5,000 prompts per language from PolygloToxicityPrompts is used to facilitate model benchmarking, especially when computational resources are limited.\n\n### Evaluation Metrics\n\nThe methodology evaluates model toxicity using the following metrics:\n\n1. **Expected Maximum Toxicity (EMT)**: Assesses the expected toxicity of the model\u2019s most toxic generation for a prompt.\n   \\[\n   \\text{EMT} = \\mathbb{E}\\left[\\max_{\\mathbf{c} \\in \\mathcal{C}} T(\\mathbf{c})\\right]\n   \\]\n   \n2. **Empirical Probability (EP)**: Measures the frequency at which a model generates at least one toxic completion across multiple generations for a prompt.\n   \\[\n   \\text{EP} = \\mathbb{P}\\left(\\exists \\mathbf{c} \\in \\mathcal{C} : T(\\mathbf{c}) \\geq t\\right)\n   \\]\n   \n3. **Average Toxicity (AT)**: Captures the overall average toxicity of the model\u2019s generated completions.\n   \\[\n   \\text{AT} = \\mathbb{E}\\left[T(\\mathbf{c})\\right]\n   \\]\n\n### Experimental Setup\n\n1. **Model and API Usage**:\n   - The Perspective API is used to evaluate toxicity scores.\n   - Language models involved in the experiments include GPT-3.5-Turbo (via Microsoft Azure's OpenAI API without safety settings), vLLM for decoder-only models, and Huggingface\u2019s TGI for encoder-decoder models.\n\n2. **Configuration**:\n   - **Generations per Prompt**: Each prompt receives 25 generated completions.\n   - **Hyperparameters**: Temperature is set to 0.7, top_p to 0.9, with a maximum token generation length of 100 tokens.\n\n3. **Prompt Templates**: Only the required prompt templates, as stated in the model cards, are utilized without additional instructions to assess the models' raw performance on the generated toxic content.\n\nThis methodology provides a broad, systematic approach to evaluate the toxic degeneration of LLMs across diverse languages, addressing both resource constraints and the challenge of multilingual toxicity evaluation.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n#### Datasets\nThe key dataset used in the main experiment consists of 5,000 prompts per language. These prompts are used to investigate multilingual toxic degeneration across various large language models (LLMs).\n\n#### Baselines and Models\n1. **Base LLMs**: Models trained only with the language modeling objective.\n2. **Safeguarded LLMs**: Includes Llama 2-Chat and Tulu 2-DPO models for English and other related languages.\n3. **Preference-Tuned Models**: Gemma 7B variants and zephyr-7b-gemma-v0.1.\n\n#### Evaluation Metrics\n- **Average Toxicity (AT)**: The primary evaluation metric, reporting the average toxicity score for continuations produced by the models.\n- Toxicity is measured using tools such as Perspective API and Llama Guard.\n\n### Main Experimental Results\n\n1. **Base LLMs**:\n   - There is a slight correlation between the number of parameters in base LLMs and continuation toxicity. This is consistent across the Pythia suite, where toxicity increases with model size but plateaus after a certain point.\n   - Large enough models (parameter count) tend to capture and demonstrate long-tail toxic phenomena better.\n\n2. **Safeguarded LLMs**:\n   - For Llama 2-Chat models, AT first decreases and then increases with model size.\n   - For Tulu 2 models, DPO alignment first increases and then reduces toxicity as the model size increases.\n   - Differences in toxicity levels between model sizes for safeguarded LLMs are marginal.\n\n3. **Preference Models vs. Instruct Models**:\n   - Aggregate comparison shows base models exhibit the highest toxicity, significantly different from instruct and preference models.\n   - No significant difference in toxicity levels between instruct and preference models, although preference models show slightly lower toxicity.\n\n4. **Input vs. Continuation Toxicity Correlation**:\n   - Base models show the strongest correlation between input and continuation toxicity.\n   - Preference models better match input toxicity, particularly in low-toxicity inputs, compared to instruct models.\n\n   These findings indicate variances in how model size, alignment methods, and feedback types affect the toxicity of generated content in multilingual contexts. Future work is suggested to explore in greater depth the specific impact of these factors on safe language model deployment."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Investigate the impact of model size, prompt language, and instruction and preference-tuning methods on toxicity by benchmarking over 60 LLMs using PolygloToxicityPrompts.",
            "experiment_process": "The study uses a large suite of diverse LLMs, including base, instruct, and preference models like Llama2, Pythia, and GPT-3.5-Turbo. Expected Maximum Toxicity (EMT), Empirical Probability (EP), and Average Toxicity (AT) are calculated using completions from sampled prompts. Toxicity is measured using the Perspective API, with a specific setup including temperature, top_p, and a maximum generation length of tokens. The experiments are conducted on Microsoft Azure's OpenAI API and Huggingface's TGI for different model families.",
            "result_discussion": "The study finds that toxicity generally increases with model size and decreases slightly with instruction and preference-tuning. The choice of preference-tuning method does not significantly impact toxicity levels. The distribution of toxicity highlights the susceptibility of models to more toxic generations as language resources decrease.",
            "ablation_id": "2405.09373v2.No1"
        },
        {
            "research_objective": "Evaluate how different prompt languages affect average toxicity in multilingual LLMs.",
            "experiment_process": "The study benchmarks multilingual LLMs like GPT-3.5-Turbo, Aya101, and Bloomz, evaluating their Average Toxicity (AT) for 5,000 prompts per language from the PolygloToxicityPrompts dataset. The models are assessed across high, medium, and low-resource languages to understand their translation and response behaviors.",
            "result_discussion": "The findings show that models exhibit the lowest AT in Russian and Dutch, but higher AT in Hindi and Czech, especially for languages with fewer resources in their pretraining corpora. GPT-3.5-Turbo has generally higher AT levels across languages, whereas Bloomz models exhibit lower toxicity, potentially due to shorter and less coherent completions.",
            "ablation_id": "2405.09373v2.No2"
        },
        {
            "research_objective": "Understand the correlation between prompt toxicity and continuation toxicity in different LLMs.",
            "experiment_process": "The study measures the correlation between input prompt toxicity and continuation toxicity across various models, focusing on base, instruction-tuned, and preference-tuned models. Pearson correlation coefficients are calculated to determine the relationship between input and output toxicity levels.",
            "result_discussion": "Continuation toxicity strongly correlates with input toxicity, especially in base models. Preference models exhibit a higher correlation for low-toxicity prompts but perform less impressively with high-toxicity prompts compared to instruct models. This indicates that preference models might be better at safeguarding against high-toxicity inputs.",
            "ablation_id": "2405.09373v2.No3"
        },
        {
            "research_objective": "Examine the impact of model size on average toxicity in both base and safeguarded LLMs.",
            "experiment_process": "The Pythia suite of models, varying in size but consistent in pretraining data, is used for a controlled investigation. Experiments are set up to track Average Toxicity (AT) as model size increases, and results are plotted to observe trends and inflection points.",
            "result_discussion": "Findings suggest that toxicity tends to increase with model size but plateaus at a certain parameter count. For safeguarded models like Llama 2-Chat and Tulu 2-DPO, trends show varying impacts of model scaling on toxicity levels. Discrepancies in smaller models underline the need for further research.",
            "ablation_id": "2405.09373v2.No4"
        },
        {
            "research_objective": "Investigate the impact of different alignment methods on average toxicity in LLMs.",
            "experiment_process": "Models trained with different alignment methods, such as PPO, DPO, KTO, and IPO, are benchmarked for their toxicity levels. The comparison includes evaluating models like Gemma 7B versions, trained on human and AI preferences, across various languages and alignment techniques.",
            "result_discussion": "The results show little difference in average toxicity among models trained with different alignment methods, though AI feedback tends to lower toxicity in English completions. Human feedback better reduces toxicity in non-English languages, indicating the need for tailored alignment methods.",
            "ablation_id": "2405.09373v2.No5"
        },
        {
            "research_objective": "Assess how different data sources elicit average toxicity from LLMs.",
            "experiment_process": "The study compares Average Toxicity (AT) using naturally occurring prompts from PTP, RTP-LX, and WildChat datasets. It evaluates how these sources influence elicited toxic content in LLM generations, considering factors like prompt length and naturally occurring vs. curated data.",
            "result_discussion": "PTP elicits higher average toxicity compared to RTP-LX and WildChat. The longer prompts in PTP and more naturally occurring toxic content likely contribute to this observation. Preference-tuning seems to make models less vulnerable to user-generated inputs than naturally occurring prompts.",
            "ablation_id": "2405.09373v2.No6"
        }
    ]
}