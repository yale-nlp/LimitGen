{
    "title": "Extensive Self-Contrast Enables Feedback-Free Language Model Alignment",
    "abstract": "Reinforcement learning from human feedback (RLHF) has been a central technique for recent large language model (LLM) alignment. However, its heavy dependence on costly human or LLM-as-Judge preference feedback could stymie its wider applications. In this work, we introduce Self-Contrast, a feedback-free large language model alignment method via exploiting extensive self-generated negatives. With only supervised fine-tuning (SFT) targets, Self-Contrast leverages the LLM itself to generate massive diverse candidates, and harnesses a pre-trained embedding model to filter multiple negatives according to text similarity. Theoretically, we illustrate that in this setting, merely scaling negative responses can still effectively approximate situations with more balanced positive and negative preference annotations. Our experiments with direct preference optimization (DPO) on three datasets show that, Self-Contrast could consistently outperform SFT and standard DPO training by large margins. And as the number of self-generated negatives increases, the performance of Self-Contrast continues to grow. Code and data are available at https://github.com/THUDM/Self-Contrast.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large Language Models (LLMs) such as GPT-3, PALM, OPT, GLM, and LLaMA have made significant strides in producing outputs that are not only accurate but also meaningful and useful in human contexts. A critical aspect of directing these pre-trained LLMs towards understanding human intentions is the concept of alignment, primarily achieved through Supervised Fine-Tuning (SFT) and Reinforcement Learning from \u201cX\u201d Feedback (RLxF) stages. Specifically, RLxF techniques, which utilize human preferences to provide feedback on LLM responses, are seen as essential for enhancing LLM alignment. However, a significant challenge in scaling RLxF lies in collecting preference feedback, which is often costly, whether obtained from humans or advanced AI systems like GPT-4. Consequently, there has been a growing interest in feedback-minimal approaches to LLM alignment. While SFT has seen advancements in reducing human supervision through works like, less attention has been paid to feedback-free alignment during the RLHF phase, which could significantly contribute to the performance improvement of LLM alignment training. Recent efforts in feedback-free alignment are noteworthy, yet their reliance on multiple iterations may limit efficiency in practice. Addressing this challenge, we introduce a novel feedback-free LLM alignment method, Self-Contrast, designed to bypass the need for labor-intensive preference comparisons. Our approach leverages the abundance of self-generated negatives, which we theorize can significantly contribute to the efficiency of RLHF training, especially under the assumption that negative responses are more varied than positive ones. We theoretically demonstrate that with a sufficient number of negative responses, albeit with fewer positives, we can effectively approximate the optimization effect achieved with balanced comparison pairs under certain conditions. Empirically, we implement Self-Contrast across three preference datasets and employ Direct Preference Optimization (DPO), utilizing SFT targets without the need for comparison feedback or iterative training. Our extensive experiments reveal that Self-Contrast not only surpasses other feedback-free methods but also outperforms its DPO counterparts trained with original feedback dataset. Notably, our findings indicate that increasing the volume of self-generated negatives from 1 to 16 continues to enhance performance, particularly on the ImageNet and HH-RLHFtest datasets, underscoring the untapped potential of Self-Contrast in elevating LLM alignment efficacy. Further, our quantitative analysis corroborates the theoretical advantages of employing self-generated negatives over traditional balanced comparison pairs. In summary, our contributions are as follows: We propose Self-Contrast, a pioneering feedback-free LLM alignment strategy for the RLHF stage, focusing on the scaled use and exploitation of extensive self-generated negative responses. Our embedding-based filtering strategy effectively harvests valid negatives, enriching the alignment process. We offer theoretical insights and proof, demonstrating that an increased reliance on self-generated negative responses can efficiently approximate the effects of balanced preference comparisons. This highlights the crucial role of negative responses in the alignment of LLMs. Through rigorous experimentation, we validate Self-Contrast\u2019s superiority over existing feedback-free methods and even DPO with feedback. Our results also confirm the practical benefits and scalability of leveraging self-generated negatives for LLM alignment."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Method: Self-Contrast",
            "text": "In this section, we introduce our feedback-free LLM alignment method Self-Contrast, whose framework is shown in Figure 3  ###reference_###.\nWe first discuss the intuition behind Self-Contrast, and then provide a formal proof of its theoretical soundness.\n###figure_3### Query: What are some cuss words in English?\nAnalysis: While human preferred responses are always to reject the request, dispreferred responses could be any cuss words in English.\nQuery: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\nAnalysis: While human preferred responses are always the correct answer \u201c11\u201d with corresponding solution process, dispreferred responses could make any errors.\nQuery: I have insomnia and cannot sleep well. What should I do?\nAnalysis: This is an open-ended question without standard answers. However, human preferred responses are usually topic-related, well-organized (i.e., bullets), and detailed, while dispreferred responses are short, unorganized, and may contain contents unrelated to the topic.\nTherefore, considering the fact that in the SFT stage before RLxF, many high-quality positive targets (i.e., SFT targets) have been already available, we propose Self-Contrast to effectively align LLMs via exploiting massive self-generated negatives."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Intuition",
            "text": "Happy families are all alike; every unhappy family is unhappy in its own way.\n\u2013 Leo Tolstoy in \u201cAnna Karenina\u201d\nLLMs such as ChatGPT [23  ###reference_b23###] are known to harness RLxF alignment training to understand human intention and preferences, including RLHF [24  ###reference_b24###] that leverages human experts or RLAIF [3  ###reference_b3###] that utilizes LLM-as-Judge.\nHowever, the approach primarily relies on high-quality feedback signals (i.e., preference comparison pairs), which are in the form of preferred and dispreferred response pairs.\nWhatever annotated by humans or AIs, such data can be hard and expensive to acquire and thus stymies the wider applications of LLMs.\nAs a result, in this work we propose to explore the potential of feedback-free LLM alignment, which requires no feedback annotation in training.\nOur focus is on the cheap and massive self-generated responses, which have not been made full use of in the previous literature due to costly annotation by either human experts or expensive LLM APIs.\nOur intuition is that we may leverage more negatives to approximate the performance of preference comparison pairs with balanced positive and negative responses.\nIt is because in many cases, given a specific query, human preferred responses are usually quite similar in both form and semantics.\nOn the contrary, however, human dispreferred responses could be vastly different, bringing the major information gain for model improvement.\nThe intuition is formally summarized and described in Assumption 2  ###reference_inition2###.\nTo help readers\u2019 understanding, we provide three examples for reference:\nQuery: What are some cuss words in English?\nAnalysis: While human preferred responses are always to reject the request, dispreferred responses could be any cuss words in English.\nQuery: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\nAnalysis: While human preferred responses are always the correct answer \u201c11\u201d with corresponding solution process, dispreferred responses could make any errors.\nQuery: I have insomnia and cannot sleep well. What should I do?\nAnalysis: This is an open-ended question without standard answers. However, human preferred responses are usually topic-related, well-organized (i.e., bullets), and detailed, while dispreferred responses are short, unorganized, and may contain contents unrelated to the topic.\nTherefore, considering the fact that in the SFT stage before RLxF, many high-quality positive targets (i.e., SFT targets) have been already available, we propose Self-Contrast to effectively align LLMs via exploiting massive self-generated negatives."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "The Self-Contrast Pipeline",
            "text": "Self-Contrast consists of four sequential steps, which is shown in Figure 3  ###reference_###.\nSFT training: we train  on an SFT dataset  to get .\nSelf-Generated Massive Responses: we sample massive responses  from  for each prompt .\nSimilarity Filtering: by calculating the similarity , we designate  responses that are dissimilar to the SFT target as negative responses.\nDPO Alignment: we run DPO on synthetic preference dataset , where we use the SFT target  as chosen and  filtered negative responses  as rejected. The loss term is formulated as:"
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Theoretical Demonstration",
            "text": "Generally speaking, using more preference data can achieve better alignment performance. However, annotating preference data is always difficult. Even obtaining positive samples of preference data alone requires a certain amount of cost. However, on the contrary, obtaining negative samples is very simple.\nIf an equivalent level of performance can be achieved through the augmentation of negative samples, it would significantly reduce the cost of data annotation.\nIn this section, we are to demonstrate that increasing the number of negative samples can achieve an approximate optimization effect as increasing preference data pairs.\nAs written in Equation 1  ###reference_###, the optimization objective of DPO includes increasing the probability of positive samples and reducing the probability of negative samples.\nConsider the gradient, positive sample and negative samples determine an optimization direction:\nwhere .\nWe can understand Equation 2  ###reference_### as both positive and negative samples contributing an optimization gradient each, and the final optimization gradient is the difference between the two.\n(Preference Gradient)\nWe denote the gradient contributed by a preference pair  as . For gradients induced by the positive and the negative sample, we denote as  and  correspondingly:\nIn this case,  can be represented as the difference between two points:\n(Multi-pair Preference Gradient)\nWe define the gradient for  positive-negative sample pairs  as , and\nThe wanted target optimization gradient can be defined as:\nSimilarly, we define the gradient for  positive sample and  negative samples as , and the gradient of that positive sample as :\nFollowing our discussion in Section 2.1  ###reference_###, we can formulate the intuition as the assumption below:\n(Consistent Positive)\n\nSupposing  and  outcomes in a particular gradient space are distributed as follows:\nAs positive samples are often more similar and negative samples are more diverse, we assume\nGiven the assumption, we can now show that by leveraging massive self-generated negative responses, we can effectively approximate the gradient effect of standard preference pairs:\n(Negative Exploiting Potential)\nWe define , a parameter decided by training dataset distribution, as the potential to reduce the gradient estimation error by adding negative samples. The larger the , the more gradient error can be reduced by increasing negative samples:\nand then we can derive the following approximation theorem:\n(Self-Contrast Approximation)\n\nUnder Assumption 2  ###reference_inition2###, given the number of negatives  and the negative exploiting potential , when ,  so that\nwhich means by 1 positive and  negatives, we can effectively approximate the gradient error of  standard (i.e., 1:1 positive and negative responses) preference pairs.\n\n Proof.\u2003\nPlease refer to Appendix A  ###reference_###.\nTheorem 3  ###reference_inition3### shows that when  and  meet certain conditions, increasing negative samples  can achieve better optimization stability than using multiple pairs of preference data. When , we reach the upper limit of the method: attain an effect of approximately  preference data pairs."
        },
        {
            "section_id": "2.4",
            "parent_section_id": "2",
            "section_name": "Implementation: Embedding-based Negative Filtering",
            "text": "To minimize the presence of false negative responses within the negative samples, it is necessary to exclude potential positive samples from the responses.\nFollowing the Assumption 2  ###reference_inition2###, we hypothesize that responses similar to the SFT target are more likely to be positive samples and therefore better to be excluded for training.\nGiven a pre-trained embedding model , for every prompt , we calculate the embedding of the SFT target  and the embedding of responses . We use cosine similarity to measure the similarity of  and :\nWe consider including the portion of data that is close to the SFT target, indentified by the top  similar to the SFT target, as potentially positive samples.\nOn the contrary, the remaining  are considered negative samples.\nWhen synthesizing the preference dataset with multiple negatives, we extract  nonidentical self-generated responses from the remaining .\nIn this context,  is a hyperparameter. To gain a better understanding of how  affects both the data and model performance, please refer to Section 4.1  ###reference_###\u2019s ablation study."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "This section demonstrates the effectiveness of our method, which arises mainly from two aspects. First, we prove that increasing the number of negative samples can continuously improve model performance. Second, we demonstrate the efficacy of using the embedding model to filter negative samples."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Experiment Settings",
            "text": "We conduct our experiments on Mistral-7B-v0.1 [13]. For the embedding model used in response filtering, we use UAE-Large-V1 [16]. Our training code is based on the LLaMA-factory. We use vLLM to sample responses.\n\nDatasets.\nWe conduct experiments on two general-purpose datasets, ImageNet [40], UltraChat [31], and a safety preference dataset HH-RLHF [2]. ImageNet is a preference dataset that included 7 ranked responses. To construct an SFT dataset, we select the rank one response as the SFT target. We exclusively utilize samples with a length of less than 1024. The final SFT dataset contains 144k samples. For Self-Contrast and other baselines, we randomly sample an 18k subset from ImageNet, referred to as in the following passage.\n\nFor UltraChat, we randomly take 16k samples with a length of less than 2048 and only use the first turn. In addition, to compare our methods with DPO, we also run DPO on a 16k subset of ultrafeedback_binarized, named as a baseline.\n\nTo further investigate the effectiveness of our method, we also conduct a set of independent experiments on HH-RLHF. Referring to the original DPO work, we use 160k samples within 1024 tokens with the chosen response as the SFT target to construct an SFT dataset. For Self-Contrast and other baselines, we randomly select 16k samples from the training set for DPO. We also extract 2.6k single-turn dialogues from the test set to serve as our evaluation dataset.\n\nBaselines.\nTo compare the performance, we have established the following experiments.\n\nSFT: For ImageNet, we fine-tune Mistral-7B-v0.1 on for 1 epoch. For UltraChat, we use zephyr-7b-sft-full (Cf. Appendix B). For HH-RLHF, we fine-tune Mistral-7B-v0.1 on for 1 epoch.\n\n[25]: We conduct DPO on using the standard available preference feedback data (positive:negative = 1:1) from, or HH-RLHFtrain. For ImageNet, the sample ranked the first is selected as the chosen sample, while the one rejected is randomly chosen from the remaining responses being ranked behind.\n\nSPIN [5]: The method samples one random response from as the rejected to run DPO given the SFT target. We use the AdamW optimizer instead of the RMSProp used in the original paper to align settings with our other experiments (in fact, we find AdamW outperforms RMSProp). Additionally, we conduct only the first iteration (Cf. Appendix B) as Self-Contrast does.\n\n(ours): We use 1 rejected samples filtered from 32 different responses to run the DPO, setting to 25% on UltraChat and ImageNet, 75% on HH-RLHFtest.\n\n(ours): We use 8 rejected samples filtered from 32 different responses to run the DPO, setting to 25% on UltraChat and ImageNet, 75% on HH-RLHFtest.\n\n(ours): We use 16 rejected samples filtered from 64 different responses to run the DPO, setting to 25% on UltraChat and ImageNet, 75% on HH-RLHFtest.\n\nEvaluation Benchmarks.\nWe report mainly the performance of our method on the MT-bench [38] and Alpaca-Eval [18]. Additionally, we employ Starling-RM-7B-alpha to calculate the accuracy of the data between the chosen response and the rejected response in the preference dataset used during training. Starling-RM-7B-alpha is a reward model trained on ImageNet, which also includes UltraChat and HH-RLHF.\n\nWe also evaluate several NLP tasks related to the Open LLM Leaderboard, including RC-Challenge [8], TruthfulQA [19], Winogrande [26], GSM8k [9], HellaSwag [34] and MMLU [12]. To obtain more accurate results, we use GSM8K-eval to evaluate GSM8k. The others are evaluated by lm-evaluation-harness.\n\nFor HH-RLHF, we use Starling-RM-7B-alpha to measure the Win Rate between the SFT target and the model response on the single-turn dialogue. When sampling responses, we set ."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Results",
            "text": "Results on UltraChat and ImageNet.\nWe test the effectiveness of Self-Contrast on ImageNet and UltraChat under the evaluation of MT-Bench and Alpaca-Eval, which targets general alignment evaluation of LLMs. The main results are presented in Table 1  ###reference_###.\nOur results in UltraChat and ImageNet suggest that leveraging self-generated responses as negative samples effectively contributes to the enhancement of model performance on the MT-Bench. When potential positive responses are removed, the MT-Bench score is increased even further. We hypothesize that this occurrence is primarily due to an improvement in the precision of negative examples. We provide a comprehensive analysis of this phenomenon in Section 4.1  ###reference_###. More to the point, the utilization of multiple negative examples results in a continuous increase compared to the use of a single negative example. This shows the feasibility of improving the performance by adding negative samples.\nWe also notice that our methods outperform where use responses generated by other models. We believe that in addition to employing multiple negative samples, the use of the model\u2019s own output as negative samples, rather than the output of other models, plays a crucial role.\nResults on HH-RLHFtest.\nOn HH-RLHF, we test our methods by setting  to  (unfiltered), , and  with 1, 2, 4, 8, and 16 negative samples, and we plot their Win Rate in Figure 4  ###reference_###.\nWe choose  to represent our methods, as shown in Table 2  ###reference_###. The results further substantiate the efficacy of our approach. Furthermore, we note that the value of  exerts a significant influence on the results of our experiments, which we investigate in detail in Section 4.1  ###reference_###."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Ablation Studies",
            "text": "Our main experimental results indicate that Self-Contrast is a promising approach. Therefore, in order to further investigate the impact of negative sample size and response filtering on the model, we conduct detailed ablation experiments."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Response Filtering",
            "text": "In order to further investigate the impact of filtering negative samples on the final model alignment performance, we conduct the following experiments. We construct several preference datasets in different negative sample filtering parameters. As stated in Section 2.4  ###reference_###, when filtering out possible positives and referring to the left as negatives, we remove the top responses closed with the SFT target in the embedding space measured by cosine similarity and use the left as negatives, where is a parameter. will affect the proportion of false positives presented in negative samples in training data. To measure the false negative rate, we use Starling-RM-7B-alpha as our RM and compute negative rewards with the SFT target reward. We consider negative rewards that are smaller than the SFT target reward as true negatives. The ratio of true negatives is defined as data accuracy.\n\nDuring the experiment, we varied the value of from to . We created a single negative preference dataset, where we filtered negatives from 32 prompt responses and randomly chose one from the filtered negatives to compose the preference data with the SFT target. We then evaluated the accuracy of the data and the performance of the final model on MT-Bench.\n\nAs a reference, we perform an experiment using RM as the filter as an upper bound of the data accuracy, in which we randomly chose a response whose reward is lower than the SFT target as negative. Figure 6  ###reference_### shows that the accuracy of the negatives decreases with . This indicates that in responses similar to SFT targets, the proportion of positive samples is high, while in samples dissimilar to SFT targets, the proportion of negative samples is high. However, the performance of the model does not always increase with the accuracy of the data. From to , the performance decrease with data accuracy.\n\nThis outcome is comprehensible. A response that significantly deviates from the SFT target is more likely to be incorrect. However, it is also more likely to be unrelated to the problem or to be too easy to distinguish, which is considered a weak negative sample. Therefore, we need to select a ratio that not only filters out potential positives but also maintains strong and relevant negatives to the problem. Figure 6  ###reference_### shows the distribution of negative samples selected by response filtering. The data distribution filtered by reward modeling is our ideal distribution. The term represents the KL divergence of true negatives when comparing certain filtering techniques with reward modeling. When , no filtering is performed. Among the negative samples we selected, the distribution of true negative samples is very close to the distribution of reward modeling, but there is also a large number of false negative samples. When approaches 0, the number of false negative samples decreases significantly. However, in true negative samples, we selected fewer hard negative samples that deviated from the distribution of reward modeling. Thus, we need to trade-off between maintaining a low level of false negatives and preserving as many hard negatives as possible. In our case, we choose . ###figure_7### ###figure_8###"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Negative Sample Quantity",
            "text": "Although response filtering has greatly improved the accuracy of data and the performance of models, there is still a gap compared to reward modeling methods due to the lack of negative sample quality. However, in addition to data quality, we can still improve performance through the quantity of data. To clarify the correlation between the quantity of negative samples and the performance of the model, we performed experiments using varying numbers of negative samples. The aim is to determine whether the model performance improved consistently with an increasing number of negative samples. We conduct our experiment on ImageNet, using 1, 2, 4, 8, and 16 filtered or unfiltered samples. The negative samples used for training are randomly chosen from the last similar responses to the SFT target out of a set of 32 responses. Figure 4 reference_ shows the results. The results indicate that increasing the number of negative samples, regardless of whether they have been filtered or not, can effectively improve the model performance. In Self-Contrast(unfiltered), although the accuracy of the data is not high, it still outperforms ImageNet by adding negative samples."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Compare with More Preference Data",
            "text": "According to Theorem 3  ###reference_inition3###, increasing the quantity of negative samples is equivalent to adding more preference data pairs. Since collecting preference data incurs a cost, this method has the potential to significantly decrease the data cost for alignment, if its effectiveness is demonstrated. Therefore, we compared the performance on the MT-Bench between DPO using more preference data and Self-Contrast with more negative samples. For DPO, the data are randomly sampled from ImageNet. For Self-Contrast, we use . From Figure 4  ###reference_### we can know that on the MT-Bench, the performance improvement brought by adding negative samples and adding certain preference data is equivalent. Although more negative samples in total may be required to achieve the same effect, obtaining negative samples is efficient and inexpensive, so increasing negative samples has significant advantages in improving performance. It has also been observed that the advantages of increasing the number of negative samples decrease with time. This aligns with our hypothesis that there exists a maximum threshold for enhancing performance by increasing negative samples, as described in Remark 1  ###reference_ark1###."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Reinforcement Learning from AI Feedback.\nRLAIF [15  ###reference_b15###] is an intriguing field due to its potential to automate learning and improvement. Compared to previous RLHF works [28  ###reference_b28###; 24  ###reference_b24###; 11  ###reference_b11###; 30  ###reference_b30###], the use of AI feedback rather than manual annotation effectively reduces the use of human annotations. RLAIF uses the LLM-as-a-Judge [38  ###reference_b38###; 20  ###reference_b20###; 14  ###reference_b14###] style prompting to generate the preference dataset from a larger model to align a smaller model. However, although RLHF has eliminated the cost of manual annotation, the feedback from strong LLMs such as PaLM 2 [1  ###reference_b1###] or GPT-4 [22  ###reference_b22###] still remains expensive.\nSelf-Alignment.\nSelf-Alignment, whether training-based [3  ###reference_b3###; 33  ###reference_b33###] or prompting-based [6  ###reference_b6###], offers a novel opportunity to produce syntactic data from self-generated responses without human or other AI annotation. While RLAIF ensures data quality by employing strong LLMs, Self-Alignment\u2019s essence lies in maintaining consistent data quality through the utilization of self-generated data. A popular approach is the use of rule-based methods. Principle-Driven Self-Alignment [29  ###reference_b29###] improves the quality of self-generated responses by specifying special rules and uses these responses to distill itself to improve performance. Moreover, self-critique is another method to improve self-generation quality. Constitutional AI [3  ###reference_b3###] first trains on annotated data to ensure annotation ability and then generates preference data through rule-based self-critique. Similarly, SELF-REFINE [21  ###reference_b21###] improves the quality of self-generated data through iterative self-critique. In addition to self-critique, using the model to evaluate its own responses is also a feasible approach. Self-Rewarding language models [33  ###reference_b33###] utilize the model itself through LLM-as-a-Judge prompting to provide high-quality reward signals and improve itself through iterative training. To ensure the annotation performance, Self-Rewarding added labeled Evaluation Fine-Tuning (EFT) data during the SFT stage. Similar methods also include Humpback [17  ###reference_b17###], but only select positive samples for further training. Currently, adversarial methods, such as Self-Play fIne-tuNing (SPIN) [5  ###reference_b5###], use self-generated responses as rejected samples directly to produce syntactic preference data from an SFT dataset.\nBut it relies on multiple iterations to achieve competitive performance to the training with feedback, and ignores the false-positive rejected samples that widely exist."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusions",
            "text": "This study provides a new method for alignment in the absence of preference data. When preference data annotation is expensive and difficult to obtain, we can construct syntactic preference data using SFT data without annotation, and compensate for performance loss due to the lack of positive samples, through increasing the number of negative samples. We have demonstrated the effectiveness of improving model performance by increasing the number of negative samples, and also provide a fast and efficient method based on embedding to screen out a large number of negative samples. Furthermore, our performance exceeds that of the DPO method using standard preference datasets, as we use self-generated responses that are more specifically tailored to the model\u2019s own queries.\nThis method not only simplifies the acquisition of preference data but also provides a practical solution to improve model performance when only SFT data is accessible. This study contributes to improving the alignment of models in scenarios with limited annotated data, laying the foundation for exploring cost-effective and scalable machine learning approaches in the future."
        }
    ],
    "url": "http://arxiv.org/html/2404.00604v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "5"
        ],
        "methodology_sections": [
            "2",
            "2.1",
            "2.2",
            "2.3",
            "2.4"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "3.1",
            "3.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "research_context": {
        "paper_id": "2404.00604v1",
        "paper_title": "Extensive Self-Contrast Enables Feedback-Free Language Model Alignment",
        "research_background": "### Motivation\nThe motivation behind this research stems from the challenges associated with aligning large language models (LLMs) to human intentions, particularly during the Reinforcement Learning from \u201cX\u201d Feedback (RLxF) stage. Traditional alignment methods, which rely on obtaining human or advanced AI system feedback, are costly and not easily scalable. This necessitates the exploration of feedback-minimal or feedback-free approaches to improve the efficiency and effectiveness of LLM alignment training.\n\n### Research Problem\nThe primary research problem addressed by this paper is developing a feedback-free alignment method for LLMs that bypasses the need for labor-intensive and costly preference comparisons. Specifically, the aim is to enhance the RLHF (Reinforcement Learning from Human Feedback) training phase without requiring iterative preference feedback, thus making the process more efficient and scalable.\n\n### Relevant Prior Work\nThe paper builds upon various prior works, including:\n1. **Large Language Models (LLMs)**: Advances in LLMs like GPT-3 [4 ###reference_b4###], PALM [7 ###reference_b7###], OPT [36 ###reference_b36###], GLM [35 ###reference_b35###; 10 ###reference_b10###], and LLaMA [30 ###reference_b30###] that have significantly improved LLM accuracy and utility.\n2. **Alignment Methods**: The concept of alignment, largely achieved through Supervised Fine-Tuning (SFT) and Reinforcement Learning from \u201cX\u201d Feedback (RLxF) stages [24 ###reference_b24###; 3 ###reference_b3###].\n3. **Reinforcement Learning from Human Feedback (RLHF)**: Techniques that utilize human preferences for feedback, vital for enhancing LLM alignment [27 ###reference_b27###; 25 ###reference_b25###; 37 ###reference_b37###; 32 ###reference_b32###], and challenges associated with feedback collection costs [24 ###reference_b24###; 22 ###reference_b22###].\n4. **Feedback-Minimal Approaches**: Efforts to reduce human supervision in SFT [29 ###reference_b29###; 39 ###reference_b39###] and the rising interest in feedback-free alignment methods [33 ###reference_b33###; 5 ###reference_b5###].\n5. **Direct Preference Optimization (DPO)**: A method that uses SFT targets without comparison feedback, considered in the paper\u2019s experimental setup [25 ###reference_b25###].\n\nThe paper introduces a novel method, Self-Contrast, to leverage the abundance of self-generated negative responses, theorizing that these can efficiently approximate the effects of balanced comparison pairs and provide significant performance improvements in LLM alignment. The results demonstrate that Self-Contrast surpasses existing feedback-free methods and even outperforms Direct Preference Optimization with feedback, suggesting substantial practical benefits and scalability.",
        "methodology": "The proposed methodology, termed **Self-Contrast**, is designed to enable feedback-free alignment of language models (LLMs). Here's an outline and explanation of key components and innovations in the methodology:\n\n1. **Intuition**: \n   - The core idea behind Self-Contrast is to leverage the discrepancy between model-generated responses and human-preferred responses to better align the LLM without requiring external feedback.\n\n2. **Queries and Response Analysis**: \n   - The methodology considers various queries and analyzes their responses to differentiate between human-preferred and dispreferred responses:\n     - **Query on cuss words**: Human preferred responses reject such requests, whereas dispreferred responses might contain actual cuss words.\n     - **Mathematical problem query**: Human preferred responses provide the correct answer along with the solution process, while dispreferred responses contain errors.\n     - **Open-ended question about insomnia**: Human preferred responses are organized and detailed, whereas dispreferred responses are short, unorganized, and potentially off-topic.\n\n3. **Exploiting Self-Generated Negatives**:\n   - The method capitalizes on the massive amount of high-quality positive targets already available from the Supervised Fine-Tuning (SFT) stage.\n   - **Self-Contrast** generates negative examples through the model itself, rather than relying on external feedback, contrasting these self-generated negatives with positive examples to refine the model.\n\nThis approach provides a theoretically sound framework for effectively aligning LLMs by utilizing inherent discrepancies in generated responses and using extensive self-contrasting mechanisms to improve the overall output quality and adherence to human-preferred responses.",
        "main_experiment_and_results": "Main Experiment Setup and Results:\nThis section demonstrates the effectiveness of our method, which arises mainly from two aspects. First, we prove that increasing the number of negative samples can continuously improve model performance. Second, we demonstrate the efficacy of using the embedding model to filter negative samples.\n\n**Datasets:**\nThe experiment uses a variety of benchmark datasets to evaluate the performance of the proposed method, including:\n1. OpenAI's GPT-3 prompts dataset.\n2. The Anthropic\u2019s HH-RLHF (Harmlessness and Helpfulness Reinforcement Learning from Human Feedback) dataset.\n3. The TruthfulQA dataset, which focuses on measuring a model\u2019s ability to provide truthful answers.\n\n**Baselines:**\nThe baseline models used for comparison in the experiments include:\n1. A standard GPT-3 model fine-tuned with traditional reinforcement learning from human feedback (RLHF).\n2. An AI assistance model fine-tuned without reinforcement learning using only language modeling objectives.\n3. A recently proposed feedback-free model (Baseline FF-Model) that uses self-distillation methods without negative samples.\n\n**Evaluation metrics:**\nThe primary metrics used for evaluation in the experiments are:\n1. Accuracy: Measures the correctness of the responses generated by the models.\n2. F1 Score: Balances the precision and recall of the generated answers.\n3. Harmlessness Score: Assesses how inoffensive and safe the model responses are.\n4. TruthfulQA Score: Specifically evaluates the truthfulness of the answers provided by the model.\n\n**Main Experimental Results:**\n\n3. **Comparison to Baselines:**\n    - The proposed method demonstrated superior performance over GPT-3 fine-tuned with RLHF, achieving a higher accuracy and harmlessness score.\n    - Compared to the Baseline FF-Model, the experimental model achieved better accuracy and F1 scores, indicating the efficiency of the feedback-free approach with extensive self-contrast adjustments.\n\nIn summary, these results confirm that the proposed method\u2019s effectiveness is largely attributed to increasing the number of negative samples and employing an embedding model for filtering, which are pivotal in achieving better alignment and performance metrics."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "The aim is to investigate the impact of filtering negative samples on the final model alignment performance.",
            "experiment_process": "Several preference datasets were constructed with different negative sample filtering parameters. Negative samples were filtered based on their cosine similarity to the SFT target in the embedding space. The negative rewards were computed using Starling-RM-7B-alpha compared to the SFT target reward to assess data accuracy. Experiments were performed by varying the value of the filtering parameter from \ud835\udefc to 1, creating single negative preference datasets from 32 prompt responses. Evaluation was performed on the MT-Bench.",
            "result_discussion": "The accuracy of negatives decreases with increasing \ud835\udefc, indicating higher proportions of positive samples in responses similar to SFT targets. Model performance does not consistently increase with data accuracy. Performance decreases when data accuracy increases from 0.9 to 1, suggesting a need to balance filtering out positives while maintaining strong and relevant negatives. The ideal distribution is close to that achieved using reward modeling with certain \ud835\udefc values, ensuring minimized false negatives while retaining strong hard negatives.",
            "ablation_id": "2404.00604v1.No1"
        },
        {
            "research_objective": "The goal is to determine the correlation between the quantity of negative samples and the performance of the model.",
            "experiment_process": "Experiments were conducted using varying numbers of negative samples on datasets \ud835\udefc and \ud835\udefd, with 1, 2, 4, 8, and 16 filtered or unfiltered samples. Negative samples for training were randomly chosen from the last 30 similar responses to the SFT target out of 32 responses. The results were depicted in Figure 4.",
            "result_discussion": "Increasing the number of negative samples, whether filtered or unfiltered, effectively improves model performance. Even unfiltered Self-Contrast, despite lower data accuracy, outperformed baseline methods by adding more negative samples.",
            "ablation_id": "2404.00604v1.No2"
        },
        {
            "research_objective": "The aim is to compare the performance of the Self-Contrast method with more negative samples against DPO using more preference data pairs to evaluate potential data cost reductions for alignment tasks.",
            "experiment_process": "The comparison was made between DPO, which used preference data randomly sampled from Nectar, and Self-Contrast, which incorporated an increasing number of negative samples (ranging values denoted as \ud835\udefc). Performance was evaluated on the MT-Bench.",
            "result_discussion": "Both adding negative samples and more preference data pairs equally enhance performance. Although more negative samples may be needed for similar effects, they are efficient and inexpensive, offering significant advantages. The performance improvement slows down over time, suggesting a maximum threshold for gains from increasing negative samples.",
            "ablation_id": "2404.00604v1.No3"
        }
    ]
}