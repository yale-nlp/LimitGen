{
    "title": "Large Language Models: A New Approach for Privacy Policy Analysis at Scale",
    "abstract": "The number and dynamic nature of web and mobile applications present significant challenges for assessing their compliance with data protection laws. In this context, symbolic and statistical Natural Language Processing (NLP) techniques have been employed for the automated analysis of these systems\u2019 privacy policies. However, these techniques typically require labor-intensive and potentially error-prone manually annotated datasets for training and validation. This research proposes the application of Large Language Models (LLMs) as an alternative for effectively and efficiently extracting privacy practices from privacy policies at scale. Particularly, we leverage well-known LLMs such as ChatGPT and Llama 2, and offer guidance on the optimal design of prompts, parameters, and models, incorporating advanced strategies such as few-shot learning. We further illustrate its capability to detect detailed and varied privacy practices accurately. Using several renowned datasets in the domain as a benchmark, our evaluation validates its exceptional performance, achieving an F1 score exceeding 93%. Besides, it does so with reduced costs, faster processing times, and fewer technical knowledge requirements. Consequently, we advocate for LLM-based solutions as a sound alternative to traditional NLP techniques for the automated analysis of privacy policies at scale.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The digital era has led to an unprecedented expansion of web and mobile applications and a myriad of online services. This growth is a testament to technological advancement and the increasing reliance of businesses and organizations on digital platforms for various operations. A central aspect of this digital proliferation is the extensive use of technologies for personal data collection, primarily driven by the objective of enhancing targeted marketing strategies. The ability to collect, analyze, and utilize user data has become a cornerstone of modern commerce, offering businesses invaluable insights into consumer behavior and preferences.\nHowever, the increasing collection and utilization of personal data have raised significant privacy concerns. Users\u2019 privacy is at risk as their data becomes valuable in the digital marketplace. This concern has led to the emergence of regulatory bodies and the formulation of data protection legislation aimed at safeguarding user privacy. These legislations, such as the General Data Protection Regulation (GDPR) in the European Union and the California Consumer Privacy Act (CCPA) in the United States, impose stringent requirements on how organizations should handle personal data.\nEnsuring compliance with these legislations, however, poses a formidable challenge. The overgrowth of online services, compounded by globalization, makes it impractical, if not impossible, for regulators to manually assess each service\u2019s adherence to privacy laws. This situation is further exacerbated by the dynamic nature of online services, where data processing practices and the privacy policies disclosing them are subject to frequent changes [1  ###reference_b1###]. In response to these challenges, automated-driven methods have been proposed for analyzing privacy policies at scale [2  ###reference_b2###]. This approach holds the potential for research endeavors aimed at understanding and highlighting privacy concerns, and it also offers a practical tool for regulators to conduct mass evaluations of applications and services, thereby promoting a higher level of compliance with privacy regulations.\nThe automated analysis of privacy policies has leveraged Natural Language Processing (NLP) techniques [2  ###reference_b2###]. Symbolic and statistical state-of-the-art NLP techniques are proposed to address this task, although each has drawbacks. Symbolic approaches rely on pre-defined rules, leading to lower performance when compared to statistical approaches due to the lack of adaptability to differences present in legal texts. Thus, state-of-the-art research has predominantly relied on statistical approaches such as machine learning (ML) techniques, and particularly supervised learning models, to train and evaluate models identifying privacy practice disclosures such as personal data collection or sharing [3  ###reference_b3###]. These models require the use of manually annotated datasets [4  ###reference_b4###, 5  ###reference_b5###, 6  ###reference_b6###], which are often expensive, time-consuming to create, and prone to errors [7  ###reference_b7###]. Furthermore, building and training those models require advanced technical expertise, contributing to a higher barrier to entry. As a result, their practical application is mainly suitable for large-scale projects where the benefits can outweigh these significant costs. On the other hand, modern Generative Artificial Intelligence (GenAI), particularly Large Language Models (LLMs), represents a significant advancement in the NLP domain, being able to understand and generate human-like text, making it particularly well-suited for parsing and analyzing the complex language present in privacy policies without needing annotated datasets. In this context, this paper proposes the application of LLMs for the effective and efficient extraction of privacy practices from privacy policies. In particular, we focus on ChatGPT, which relies on Generative Pre-trained Transformer (GPT) models.\nOur study identifies the optimal configuration of ChatGPT prompts, parameters, and models, integrating advanced techniques such as few-shot learning. Additionally, we conduct a comparative analysis of our proposed ChatGPT configuration with Llama 2 and other state-of-the-art techniques. Our findings reveal that our proposal competes with and even outperforms these traditional methods. Moreover, we discuss its advantages regarding lower upfront costs, reduced processing times, and greater ease of use.\nThus, we propose LLM-based solutions and our specific ChatGPT configuration as a viable replacement for traditional NLP techniques in the task of automated privacy policy processing. Our research contributes to the ongoing discourse on the application of GenAI in legal and regulatory contexts [8  ###reference_b8###, 9  ###reference_b9###, 10  ###reference_b10###], suggesting a paradigm shift towards more efficient, cost-effective, and accessible tools for privacy policy analysis."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Privacy policies are documents written in plain text that outline how organizations handle personal data. However, the complexity and length of these documents often make them challenging to understand and process [11]. This has spurred interest in automated methods for analyzing privacy policies [2], which fall into two major categories, namely, symbolic and statistical NLP.\n\nSymbolic NLP approaches [12, 13, 14] are relevant but come with inherent limitations when processing new texts: these techniques model language through grammar rules and lexicons, thus requiring extensive manual effort to create and code these rules. This process is both time-consuming and hard to scale, especially when dealing with intricate aspects of privacy policies. Symbolic NLP is effective in morphological and lexical analysis, such as identifying privacy practices through keyword analysis. It also handles more complex tasks like syntactic and semantic analysis, using tools like the Stanford dependency parser [15]. PolicyLint [16] is a state-of-the-art tool based on this symbolic NLP approach that employs ontologies to detect contradictions in privacy policy statements about personal data collection and sharing. Its ability to identify negative sentences \u2014 a challenging task for statistical NLP techniques \u2014 highlights its potential for specific privacy policy analysis tasks. However, it faces challenges with unanticipated variations, including typos or infrequent cases, thus limiting its applicability to new cases.\n\nStatistical NLP approaches, on the other hand, leverage machine learning techniques for language processing: supervised, unsupervised, and Artificial Neural Networks (ANN)-based techniques. Supervised methods are the predominant technique usually employed for automated privacy policy analysis, with geometric algorithms like Support Vector Machine (SVM) [17, 18, 19, 20] and Logistic Regression (LR) [21, 22] being the most prevalent. Unsupervised techniques, although less common, utilize models like Hidden Markov Models (HMM) [23, 18] and Latent Dirichlet Allocation (LDA) [24] for clustering practices during policy analysis. ANN-based techniques are also in use for this task, including Convolutional Neural Networks (CNNs) [6, 25], Recurrent Neural Networks (RNN) [26], and Google\u2019s BERT [4, 27], sometimes showing superior performance than supervised learning methods [4, 27].\n\nThe development of new privacy policy analysis methods leveraging statistical NLP approaches frequently requires labeled corpora for training and validation. In the domain of privacy policy analysis, several datasets manually annotated by legal experts have been employed to build supervised learning methods. ImageNet is a large-scale visual database designed for use in visual object recognition software research. It contains more than 14 million images that have been hand-annotated to indicate which objects are depicted, with at least 1,000 images to illustrate each of roughly 22,000 categories. The OPP-115 dataset [18] is the most widely used dataset in the domain. It follows the same structure and contains annotations of almost identical practices and data types to ImageNet but with a larger number (n=115) of annotated policies. APP-350 [29] is the largest dataset of this type, which has 350 privacy policies with annotations of collection/sharing of more specific data types than the previous ones. Likewise, the IT100 [30] is a dataset of 100 privacy policies containing annotations of statements disclosing international transfers of personal data.\n\nExpanding upon symbolic and statistical NLP methods, LLMs can generate coherent text based on a given input, such as GPTs [31] and Llama 2 [32]. Building on those LLMs, ChatGPT and Llama 2-Chat are chatbots trained to provide meaningful answers to pieces of text inputs (i.e., prompts) and with adjustable performance through parameters like \u201ctemperature\u201d that influence the results\u2019 variability [33], and response times. The ability to provide relevant answers is achieved through a combination of unsupervised and supervised learning techniques underpinned by neural networks trained on extensive datasets. Additionally, the relevance and format of the responses are typically enhanced through prompt augmentation [34], which involves modifying the given input prompt to improve the output performance or to steer the output in a specific direction. Notably, LLMs\u2019 proficiency in processing lengthy input texts is boosted by the attention mechanisms inherent in transformer architectures [35]. A recent study conducted by Qin et al. [36] has analyzed to what extent LLMs like ChatGPT can perform various tasks \u2014reasoning, language inference, Q&A, dialogue, summarization, entity recognition, and sentiment analysis\u2014 using 20 well-established NLP datasets to benchmark their performance, showing high reasoning capabilities.\n\nIntegrating LLMs into the automated analysis of privacy policies and legal texts [37] represents a significant evolution in assessing compliance with data protection regulations. Tang et al. [10] have explored their application in this context, highlighting its potential to surpass traditional methods in extracting and classifying general, coarse"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experimental Design",
            "text": "GPT models have an intrinsically complex behavior dependent on the prompt design, configuration parameters, and model selection. We rely on the Design Science Research (DSR) methodology [38  ###reference_b38###] to propose a ChatGPT framework for privacy policy analysis and evaluate its effectiveness. DSR guides the design of new artifacts through an iterative and systematic process. Specifically, we followed an iterative split testing process [39  ###reference_b39###] to assess the performance of the prompt, parameter, or model selection changes within each iteration. Finally, we check our proposed configuration performance against two unseen sets of policies, conduct a set of comparative analyses with state-of-the-art solutions, and demonstrate its generalization capabilities. Through this systematic process, we propose a well-performing and generalizable configuration of ChatGPT as a novel and effective approach for the automated analysis of privacy policies."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Ground Truth",
            "text": "Determining the optimal ChatGPT configuration that offers the best performance requires using a ground truth dataset to validate and quantify results. We relied on the ImageNet dataset for this task, retaining an experimental set on which to apply changes and measure their impacts and a control set to validate the final configuration\u2019s overall performance. Unlike traditional NLP techniques, using a ground truth dataset is only required while designing the configuration framework. Afterward, we can generalize it to identify other privacy practices without generating new annotated datasets or validating new methods, as demonstrated in Section 4. \n\nThe ImageNet dataset is unbalanced. Thus, we stratified sampling to generate the experimental (33 policies) and control (31 policies) subsets. With a standard deviation between both datasets of 2.44 for all data categories, with categories annotated in almost all policies (e.g., IP address and device IDs, in 95%) and others in only one of them (e.g., Political, religious, or philosophical belief). This is contextualized against the backdrop of the mean annotation counts per policy, which are 8.13 and 8.18 for the experimental and control sets, respectively. The observed standard deviation, in relation to the means, suggests a moderate degree of variance in annotation frequency per policy across the datasets. This degree of variability is within acceptable limits for the intended analytical scope, affirming a balanced and representative data stratification for the empirical analysis."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Prompt Design",
            "text": "We departed from the prompt design depicted in Figure 1(a). This baseline prompt is structured in Data, Task, and Output format instruction segments. Data is the privacy policy for identifying practices. Task is the actual practice in which identification in the policy is requested. Output format instruction provides the guidelines to obtain responses that can be processed automatically. We applied this baseline prompt to the specific task of identifying types of personal data purportedly collected or shared as per the privacy policy. This required providing the privacy policy in the Data segment, asking about each data type in the Task segment, and steering the formatting of responses, including \u201cData: Answer\u201d in the Output format instruction segment. In our experiment, this baseline prompt achieved the following metrics: 0.79 accuracy, 0.92 recall, 0.78 precision, and 0.84 F1 score.\n\nCrafting the optimum prompt design requires a split testing process to reveal the effects of various changes in the prompt. The only ChatGPT parameter adjusted during this phase is the temperature value, set to zero to provide more deterministic responses, and using the GPT-4 Turbo model to take advantage of its speed and input prompt size capabilities up to 128k tokens. While numerous tests were conducted, this section will focus solely on those that involved a significant change in performance metrics. A detailed summary of these metrics, derived from each test, is encapsulated in Table 1. In this testing sequence, each technique that demonstrated a performance improvement was systematically integrated into the subsequent tests. Thus, each new test was benchmarked against the last updated configuration.\n\nSpecifying Data boundaries. Incorporating the phrase \u201cThe following text is a privacy policy\u201d improved the metrics, specifically a +1.47% increase in accuracy (from 0.793 to 0.804), +0.4% in recall, +1.36% in precision, and +0.92% in F1 score. This improvement is attributed to enhancing the model\u2019s ability to discern the limits of the privacy policy text. A minor adjustment involving the indication that the privacy policy text is enclosed in double quotes led to an additional +0.8% rise in recall, precision, and F1 score, and a +1.16% increase in accuracy.\n\nData placement. We evaluated the impact of the placement of the privacy policy within the prompt \u2014either at the beginning or the end\u2014 on the performance metrics. Positioning the privacy policy at the end, contrary to the beginning, slightly diminished the overall metrics, including a decrease in accuracy by -1.15%, recall by -1.98%, precision by -0.07%, and F1 score by -0.97%.\n\nAugmenting Task description. The initial prompt version primarily focused on enumerating the types of data to be identified. However, given the inherent complexity of data categorization \u2014a challenge even for human annotators as substantiated in related literature\u2014 the prompt was augmented to include the internal definitions used for manual annotations in the ImageNet dataset. While this expansion resulted in a lengthier prompt, it significantly enhanced all metrics except for recall, with an increase of +4.58% in accuracy, a decrease of -0.79% in recall, +5.96% in precision, and +2.72% in F1 score.\n\nMessage splitting. We have tested splitting the prompt into two different messages, passed to ChatGPT one after the other. Specifically, we separated the privacy policy (Data segment) from the remainder prompt. The results show a better overall understanding and comprehension of the privacy policy, reflected in a +1.56% increase in precision, +0.56% increase in accuracy and +0.22% increase in F1 score, at the cost of a -1.2% decrease in recall.\n\nData pruning. This technique eliminates the paragraphs of the policy that do not have information regarding collecting or sharing personal data. We crafted a specific prompt for this task. The results show the overall policy metrics have remained practically the same.\n\nSegmentation. We also assessed the role of input processing in the results with three different configurations: 1) Data segmentation, i.e., analyzing each individual paragraph at a time; 2) Task segmentation, i.e., asking only for one specific practice (e.g., a given data type collection) at a time, and 3) Data and Task segmentation, i.e., asking for one specific practice in one specific paragraph. Data segmentation did not show significant improvement. However, Task segmentation had a surprising effect on the result: accuracy decreased by -5.48%, recall decreased by -18.4%, precision increased by +8.06%, and overall leading to a decrease in the F1 score of -6.46%. This suggests that asking for each practice individually may lead to a loss of a broader contextual understanding of the model, negatively impacting its overall performance. Finally, Data and Task segmentation showed the"
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Parameter Tuning",
            "text": "ChatGPT offers a number of parameters that can be configured to modify its responses, namely temperature, top p, and system inputs. For testing these parameters, we used the final prompt design presented in Figure 1(b).\n\nTemperature. Temperature is a hyperparameter that allows controlling the randomness and creativity of the text generated by a GenAI. If the temperature is low, the model will probably produce the most \u201ccorrect\u201d text, but with little variation. Conversely, a higher temperature value shows greater variation (i.e., creativity). Lower temperature values are preferable for the development of a deterministic framework. We automated queries to ChatGPT to measure this feature, using the same prompt for all 33 privacy policies in the experimental dataset and performing the requests on 3 different days and at 5 different times of the day (from 9 am to 9 pm). This amounts to 495 different requests and responses, and we observed 52 discrepancies (i.e., different responses compared to the typical answer), which means 89.5% consistency in ChatGPT responses. This \u2014although far from absolute determinism\u2014 highly increases the 59.6% percentage of determinism achieved with the default temperature value of 1.0. The GPT-4 Turbo model introduced a new feature called seed, specifically for obtaining consistent responses over time with the same prompt. Even though determinism is its declared purpose, we observed that using a seed value and default temperature provided only 84.65% of similar responses. Nonetheless, the combination of 0 temperature and seed shows 90.51%, being the most reliable combination of these two parameters. In our evaluation of ChatGPT\u2019s performance across different temperature settings, we found that higher temperature values inversely impact the consistency of the metrics, with deterministic responses being optimal. This tendency is notable as the quality of the outputs deteriorates with increasing temperature. Concurrently, a manual inspection of the responses revealed a propensity for incomplete data type coverage. Specifically, responses frequently reported only the initial data type queried. This issue not only aggravates the decline in the F1 score but also results in a significant proportion of the data types\u2014nearly half\u2014remaining unaddressed in the responses. Such findings underscore the importance of temperature configuration in ensuring both the accuracy and completeness of the information extracted by ChatGPT.\n\nTop p. Top p, or \u201cnucleus sampling\u201d, consists of selecting the next token from the \u201cnucleus\u201d or subset of the vocabulary that constitutes the cumulative probability mass of the top p most probable tokens. For example, setting means only tokens comprising the top 10% probability mass are considered. We have observed little performance variability when testing different values of while keeping the default temperature value. Furthermore, in its official documentation, OpenAI recommends modifying the temperature value or the top p parameter, but not both simultaneously. Thus, we chose the default value of top p for our implementation and set the temperature to zero. These settings allow us to obtain more reproducible results.\n\nSystem inputs. Using the OpenAI API, messages can be assigned to different roles (i.e., user, assistant, or system), where the system instruction can give high-level instructions for the conversation. We tested two system instructions: 1) \u201cYou are a helpful assistant with extensive knowledge in data protection and privacy engineering.\u201d and 2) \u201cYou are a helpful assistant with extensive knowledge in data protection and law\u201d, which specifically indicate areas of knowledge that are important for our task analyzing privacy policies. Neither of the two system instructions improved the results obtained but rather worsened them."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Fine-tuning",
            "text": "OpenAI facilitates model customization through fine-tuning, which involves re-training a model on a specific dataset to enhance its performance. This approach is beneficial for augmenting response consistency and can enable the use of shorter prompts while still achieving the desired format. During our experimentation phase, fine-tuning was available only for the gpt-3.5-turbo-0613 model. Thus, we tested the effect of model fine-tuning using this ChatGPT version.\n\nThe gpt-3.5-turbo-0613 model sets a maximum prompt size of 4,096 tokens. Thus, we segmented the policies into smaller subsets (chunks), each conforming to the condition that the combined length of the policy text () and prompt () did not exceed 4,096 tokens (). This process led to the creation of a training set comprising 73 chunks, aligned with the manual annotations from the ImageNet corpus and subjected to a default training configuration of three epochs as determined by OpenAI based on dataset size.\n\nThis fine-tuned model demonstrated superior performance when compared to the baseline (not fine-tuned) model: accuracy increased from 0.677 to 0.867, precision increased from 0.519 to 0.803, and the F1 score increased from 0.670 to 0.803. Still, it could not beat the GPT-4 Turbo model (which does not require chunking the policies thanks to the 128K tokens limit), probably due to its ability to retain context (see Table 2  ###reference_###)."
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "Validation",
            "text": "We first applied our proposed configuration framework to the ImageNet ground-truth control set, utilizing the prompt, parameters, and model based on our findings in prior sections. The prompt employed is the one described in Section 3.2  ###reference_###. The selection of parameter values was based on determinism consideration: , a fixed seed, and  (the default setting). Finally, the GPT-4 Turbo model is employed for its performance, speed, and significantly higher input token limit balance. This configuration yields an accuracy of 0.916, a recall of 0.976, a precision of 0.898, and an F1 score of 0.935 on the control set of the ImageNet corpus. We further validated our prompt design and model configuration against a larger ground truth, i.e., OPP-115 dataset [18  ###reference_b18###], renowned for its fine-grained manual annotations of privacy practices. This validation yielded consistent results: 0.904 accuracy, 0.912 recall, 0.949 precision, and 0.930 F1 score, indicating that our proposal exhibits robust performance even when applied to a larger and more varied set of privacy policies."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Demonstration",
            "text": "This section aims to demonstrate why LLMs, specifically ChatGPT, can be considered a competent technique for privacy policy analysis at scale. First, we compare ChatGPT to its closest GenAI rival, Llama 2, in terms of extracting the same privacy practices across identical test sets. We then compare ChatGPT with state-of-the-art statistical and symbolic NLP approaches to evaluate its performance and verify whether it can rival or even replace them. Finally, we analyze our proposal\u2019s generalization capabilities for identifying other privacy practices, namely the declaration of international transfers in privacy policies. To validate these claims, we employed the ImageNet dataset, a large visual database designed for use in object recognition software research, which provides a diverse range of contexts for testing image-related capabilities that can complement text-based analysis. By leveraging ImageNet, we assess the model's adaptability to recognize and analyze visual data in conjunction with textual privacy policies, aiming to achieve a comprehensive understanding of privacy declarations."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Comparison with Llama 2",
            "text": "Llama 2 [32] is a family of open-source LLMs released by Meta that competes with ChatGPT in the GenAI space. Specifically, Meta has released versions with 7, 13, and 70 billion parameters, each with a fine-tuned \u201cChat\u201d version optimized for dialogue. For a more direct comparison with the ChatGPT models, we focus on the chat variant of each of the Llama 2 models. We downloaded the 7B directly from Meta via their GitHub repository and ran it using four NVIDIA GeForce RTX 2080 Ti GPUs. Due to GPU constraints, we used the Python library from Together.AI to run the 70B model [43]. We initially tried to run the 13B model in our local environment, but as we achieved poor performance, we also used the Together.AI installation. Prompt. We departed from the final prompt design shown in Figure 1(b) and followed another split testing process to identify the best-performing Llama 2 prompt design. All the Llama 2-Chat models have a 4,096 token limit, which forced us to segment the privacy policies (i.e., the Data part) to ensure that our prompts are under the maximum token limit. Additionally, we removed the few-shot learning part from the prompt, as this yielded worse performance in our experiments with Llama 2. We observed that this technique resulted in outputs that did not conform to the requested format and additionally caused overfitting to the provided examples. Finally, we tried segmenting the Task in the prompt by asking for each data practice at a time, improving the results. Table 1 summarizes the different tests and the resulting performance. Parameters. Just as for the ChatGPT models, we parameter-tuned across the temperature and the top p values. Similar to observations with ChatGPT, our experiments suggest setting a temperature value of 0 and a default top p of 1.0 as the best-performing configuration. We carried out our experiments with the three Llama 2 versions. As the Llama 2 70B-Chat model consistently showed better results, we used this version to assess its performance against the ImageNet control set (31 annotated policies) (Table 3). Llama 2 demonstrates comparable but slightly lower performance in identifying privacy practices in this dataset compared to our ChatGPT-4 proposal. We further evaluated the performance of the Llama 2-70B configuration against the OPP-115 dataset. The results (Table 3) show that Llama 2 obtains worse performance against this new dataset, suggesting that, unlike ChatGPT, this Llama 2 configuration does not generalize well to different datasets."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Comparison with state-of-the-art techniques",
            "text": "We propose LLMs and, specifically, ChatGPT as a new technique for automating privacy policy information processing and extraction. To confirm it as such, we compare its performance in extracting fine-grained practices from policies with state-of-the-art statistical and symbolic approaches."
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1 Statistical approaches",
            "text": "In this study, we conducted a comparative analysis of our configuration framework proficiency in identifying fine-grained privacy practices against statistical classifiers based on Support Vector Classifiers (SVC) \u2014a subtype of SVM\u2014, which were trained and validated using the ImageNet dataset. To ensure a rigorous comparison, the same policy dataset was employed to evaluate the performance of both methods. The primary objective was to assess ChatGPT\u2019s ability to accurately identify particular types of personal data collection as stated in privacy policies. For this purpose, we selected 10 distinct data types, with an emphasis on higher specificity (for instance, choosing \u201cContact email address\u201d over the broader \u201cContact information\u201d), spanning various categories such as contact data, identifiers, and social login data. Table 4 delineates the comparative performance of ChatGPT against the pre-trained SVC classifiers for identifying each specified data type. The results indicate a comparable level of performance across most data types. However, a notable exception was observed with the SIM identifier, where ChatGPT\u2019s performance was significantly lower despite achieving 100% precision. A detailed manual review of the annotations for this data type in the original policies revealed a common annotation issue: Human annotators wrongly coded this data type. Specifically, the annotators coded \u201cdevice serial number\u201d under the \u201cSIM serial number\u201d category. However, the former is issued by the device manufacturer, while the latter is provided by the mobile carrier. This discrepancy likely contributed to the lower F1 score for ChatGPT in identifying the SIM identifier. Excluding the analysis of the SIM serial identifier, which was identified as an anomaly, the comparative evaluation yielded an average F1 score of 84.1% for ChatGPT, as opposed to 86% achieved by the SVC-based classifiers for the selected data types. This outcome illustrates that while traditional SVC-based classifiers are recognized for their reliability and accuracy, ChatGPT presents a comparable level of performance. ChatGPT offers the added advantage of significantly simpler usability, making it a viable alternative for similar tasks in data practice identification."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2 Symbolic approaches",
            "text": "PolicyLint [16 ###reference_b16###], a tool designed to analyze privacy policies, employs a symbolic approach based on ontologies to detect contradictions in statements regarding personal data collection and sharing. This tool identifies negative sentences, which are often challenging for conventional machine learning techniques. The public repository of PolicyLint\u2019s code [44 ###reference_b44###], as referenced, was utilized to process the privacy policies in our control set, facilitating a comparative analysis with our proposal. PolicyLint operates by identifying sentence structures characterized by [actor] [action] [data_object] [entity]. Here, \u201cactor\u201d signifies a first or third party involved in data handling, \u201caction\u201d denotes the nature of data interaction (positive or negative, such as collection or non-collection), \u201cdata_object\u201d pertains to the type of data in question, and \u201centity\u201d refers to the recipient of the data (for instance, advertisers).\n\nGiven that the data_objects in PolicyLint do not align format-wise with those in our ImageNet corpus, a manual matching process was undertaken by two authors to correlate PolicyLint\u2019s classifications with the data types in our corpus. This matching was independently conducted, followed by an agreement phase for resolving discrepancies. The matching criteria were aligned with the definitions provided in the ImageNet corpus. Subsequently, the comparative performance metrics of both methods were analyzed and presented in Figure 2 ###reference_###.\n\n###figure_3### Our analysis revealed that PolicyLint exhibits high precision, surpassing the metrics its authors reported. This discrepancy might stem from our methodology, where we assess whether a data type is identified at least once in a policy, instead of PolicyLint\u2019s validation across all relevant statements. However, PolicyLint\u2019s approach overlooks negative cases, leading to a lower recall. Overall, our evaluation indicates that our ChatGPT configuration framework significantly outperforms PolicyLint\u2019s F1 score, highlighting its efficacy in extracting and analyzing data practices from privacy policies."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Generalization Capabilities",
            "text": "The proficiency of ChatGPT in extracting data collection and sharing practices from privacy policies has been notably demonstrated in our study. In this section, we extend the evaluation to assess ChatGPT\u2019s generalization capabilities in identifying a broader range of practices within privacy policies. This extension is grounded in our prior research [45  ###reference_b45###], which focused on analyzing privacy policies to find disclosures related to international data transfers. This previous study produced a dataset (IT100) comprising 100 privacy policies where privacy practices related to international data transfers were manually annotated by legal experts [30  ###reference_b30###]. A Support Vector Machines (SVM)-based classifier was trained to identify these specific practices.\n\nIn Figure 3  ###reference_###, we present the comparative analysis of the performance metrics between our configuration proposal of ChatGPT and the SVM-based classifier, utilizing the IT100 dataset for evaluation. ChatGPT was configured as per the parameters delineated in Section 3  ###reference_###, which included an instantiation of our enhanced prompt in Section 3.2  ###reference_###, and settings like temperature=0, top_p=1, and an absence of system_input. The results displayed by ChatGPT were significantly superior in most metrics, reinforcing its efficacy in extracting information about diverse practices from privacy policies."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "ChatGPT demonstrates a more balanced and adaptable performance in privacy policy analysis compared to traditional symbolic and statistical methods, overcoming the limitations of manual annotations and varying data across different corpora. Symbolic methods are characterized by their rigidity, which is reflected in their performance metrics. High precision in symbolic methods indicates their well-defined patterns and rules are closely aligned with specific instances in the data. However, this precision comes at the cost of completeness, as evidenced by their lower recall. In contrast, ChatGPT demonstrates a more balanced performance, achieving a notably higher F1 score than PolicyLint. This suggests that ChatGPT \u2013and our proposed configuration\u2013, while less rigid in its approach, captures the breadth of privacy practices within policies more effectively.\nWhen comparing ChatGPT with statistical methods such as SVM, we find that these traditional classifiers perform similarly in identifying certain data types. However, ChatGPT excels particularly in recognizing practices like international data transfers, which are complex and multifaceted. This superior performance is notable, given that statistical methods often depend on extensive manual annotations, which can introduce errors. As seen in the Identifier SIM Serial case in Table 4  ###reference_###, such annotation errors can significantly impact classifier performance. Wagner et al. [46  ###reference_b46###] supports this observation, indicating that the average agreement among human annotators for attribute values is considerably lower than for top-level categories. This discrepancy highlights the challenges in achieving consensus among annotators and the advantage of ChatGPT\u2019s approach, which is not constrained by the limitations of manual annotations.\nFurthermore, our analysis of different corpora, specifically the MAPP and OPP-115 datasets, sheds light on the variance in annotations across datasets. The performance disparities observed for Social media data and Personal identifier data between these two corpora suggest that the annotations for these data types likely vary, underscoring the issues associated with training classifiers on manually annotated data [47  ###reference_b47###]. This reinforces the need for approaches like ChatGPT that rely less on such annotations, offering a more adaptable and potentially more accurate solution for privacy policy analysis.\nEconomic considerations play a significant role in the choice of the technique to process privacy policies. Manual annotators in the United States are reported to earn approximately $8.5 per hour, while rates in lower-income countries range between $3 to $4 per hour [48  ###reference_b48###]. However, the annotation of privacy policies demands legal expertise for accurately identifying data protection practices, entailing a higher pay rate, assumed here at a minimum of $10 per hour. For the MAPP corpus, three experts annotated each policy, averaging 1 hour and 52 minutes each [28  ###reference_b28###]. Multiple annotations of the same content by different experts ensure reliable and high-quality data where inter-annotation agreement can be measured. Previous research [45  ###reference_b45###] demonstrated that training classifiers with 100 policies can be sufficient, which raises costs by up to $5,601.\nSetting aside the technical expertise required for classifier development, the cost-effectiveness of traditional classifiers becomes behooveful with GPT-4 Turbo at approximately 81,500 privacy policies and with GPT-3.5 Turbo at around 825,000 policies. This cost difference suggests that depending on specific application needs and constraints, GPT-3.5 Turbo, with an F1 score of 87.2% measured on the MAPP corpus control set, might be a pragmatic choice compared to GPT-4 Turbo, which achieved an F1 score of 93.5%111This difference is not only due to the model performance but also because the few-shot prompting technique that can be applied to the GPT-4 Turbo model thanks to its increased token limit. in our evaluation. Furthermore, Llama 2 models, specifically 7B and 70B, may be considered in terms of cost discussion. Both models were publicly released for free use, but our hardware limitations imposed by the latter forced us to use Together.AI API for that version. The current API cost for the Llama 2-70B model is 10% lower than the GPT-3.5 Turbo model while showing an even higher performance \u201388.2% F1 score\u2013 making it even more convenient in terms of cost by performance. Llama 2-7B has significantly lower computational requirements, leading to no other cost but computation and achieving an 80.1% F1 score. Thus, GPT-4 Turbo offers the best performance of the LLMs compared, but at the highest cost. Whereas if the computing capabilities are sufficient to run it locally, Llama 2-70B offers good performance at a low cost.\n###figure_5### In assessing the processing capabilities of ChatGPT models, our analysis indicates a marked efficiency advantage over traditional machine learning and symbolic AI techniques. Acknowledging the operational constraints imposed by OpenAI on these models, specifically regarding token throughput per minute is critical. GPT-4 Turbo is limited to 300,000 tokens per minute, while GPT-3.5 Turbo can process up to 1,000,000 tokens within the same timeframe.\nWith an average of 6,652 tokens required in average to fully process a privacy policy, GPT-4 Turbo can analyze up to 45 policies per minute, in contrast to the 150 policies per minute capability of GPT-3.5 Turbo. Figure 5  ###reference_### depicts this variance in processing capacity, with GPT-4 Turbo necessitating slightly more time for large-scale privacy policy analyses when compared to GPT-3.5 Turbo. Furthermore, the SVM-based classifier takes approximately double the time of the slower GPT model to process an equivalent number of policies. In stark contrast, PolicyLint, while being at the forefront of privacy policy analysis symbolic-based tools, demands up to six times the processing time of GPT-3.5 Turbo for comparable tasks. The two versions of Llama 2 show remarkably different processing times. The Llama 2-7B, locally analyzing each policy at once (truncating policies when length limit required), shows a similar processing time compared to GPT-4 Turbo, while Llama 2-70B through Together.AI API (analyzing policies by chunks), shows the slowest performance of all techniques.\nThese findings underscore the superior speed of LLM models and highlight the need to balance performance with processing time, especially when scaling to analyze vast numbers of privacy policies. Thus, organizations may find the trade-off between the slightly lower speed of GPT-4 Turbo and its enhanced accuracy acceptable, particularly in scenarios where quality of analysis is paramount. Conversely, for applications where time efficiency is a priority, GPT-3.5 Turbo presents a compelling option, offering rapid analysis with a modest compromise in performance metrics.\n###figure_6### For the GPT models, parallel processing can be employed to concurrently analyze up to 150 and 45 policies per minute for GPT-3.5 and GPT-4, respectively, adhering to the stipulated token rate limits. To scale up concurrent processing capabilities with ChatGPT, users may opt for multiple paid accounts, which entails additional costs due to the subscription requirements for accessing the API via ChatGPT Plus. Another avenue is to request OpenAI for elevated rate limits, a request that hinges on the company\u2019s approval. Anticipation of expanded rate limits by OpenAI in the future could potentially democratize access to more extensive parallel processing for all users, thereby broadening \u2013even more\u2013 the scope of large-scale privacy policy analysis.\nThe rapid progression of generative AI technology is evident in the quick succession of ChatGPT models introduced. Within the span of mere months, we have witnessed the release of successive ChatGPT iterations, namely GPT-3.5 Turbo, GPT-4, and GPT-4 Turbo. Alongside the expected speed and cost efficiency enhancements, a notable shift has been observed in model determinism. For instance, the determinism observed in ChatGPT-3.5 (99.19%) significantly exceeds that of GPT-4 Turbo, suggesting a potential trade-off between response variability and model robustness.\nThis rapid succession has introduced variations in the models\u2019 performance, particularly regarding prompt responsiveness and temperature settings. Current outputs from most recent models align more closely with expectations even at increased temperature settings, evidencing an enhanced capacity of ChatGPT to interpret prompts with fewer instructions and diminishing the necessity for techniques such as prompt augmentation.\nThe execution speeds of the Turbo models are noteworthy, achieving significant throughput without compromising performance for the task at hand. Moreover, the cost efficiencies introduced with these models \u2014threefold less for GPT-4 Turbo and tenfold less for GPT-3.5 Turbo\u2014 consolidate ChatGPT\u2019s position as a vying competitor to state-of-the-art tools for large-scale studies.\nWe have observed that the token limit per minute has substantially increased\u2014up to 30 times for GPT-4 and nearly 10 times for GPT-3.5 Turbo. This escalation, coupled with the models\u2019 improved response times, results in more expedient processing of privacy policies, as evidenced in Figure 5  ###reference_###. Regarding F1 score performance, the new GPT-4 Turbo model remains consistent with its predecessors, albeit with notable variations: a 1.36% increase in the F1 score for the MAPP corpus and a similar decrease for the OPP-115. The intricacies of these models make it challenging to pinpoint the exact causes of these variations, but it is remarkable that the optimization inherent in the Turbo models has not detrimentally impacted performance for this specific task."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "Throughout this article, we have substantiated the applicability of LLMs in analyzing and extracting privacy practices from privacy policies. Specifically, ChatGPT has proven to be as effective as traditional NLP techniques, offering significant advantages in terms of cost, runtime, and ease of development. This work has also presented a tailored configuration of prompts, parameters, and the ChatGPT model, which shows outstanding performance in identifying various privacy practices within privacy policies. We have demonstrated that fine-tuning, while valuable, may not be the most optimal approach against the backdrop of few-shot and zero-shot learning paradigms. Intriguingly, few-shot learning has exhibited superior performance metrics even over zero-shot learning. Our study demonstrates a notable advance in automated privacy policy analysis through the generalization capabilities of our proposal. This approach potentially eliminates the reliance on annotated datasets, enabling the analysis to encompass a wider array of privacy practices previously limited by the necessity for in-depth legal knowledge. Future work will focus on integrating this method into automated systems for assessing data protection compliance, contributing towards raising awareness among developers, regulators, and users of the potential privacy risks in the data protection ecosystem."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Aknowledgments",
            "text": "This work has been partially supported by the TED2021-130455A-I00 project funded by MCIN/AEI/10.13039/501100011033 and by the European Union \u201cNextGenerationEU\u201d/PRTR. Jose M. del Alamo has received a grant from the Spanish \u201cMinisterio de Universidades\u201d through the \u201cMovilidad\u201d sub-programme of the \u201cPrograma Estatal para Desarrollar, Atraer y Retener Talento\u201d, within the \u201cPlan Estatal de Investigaci\u00f3n Cient\u00edfica, T\u00e9cnica y de Innovaci\u00f3n 2021-2023\u201d. This research has also been partially supported by the National Science Foundation under its Security and Trustworthy Computing Program (grant CNS-1914486)."
        }
    ],
    "url": "http://arxiv.org/html/2405.20900v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.4",
            "3.5"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.2.1",
            "4.2.2",
            "4.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "3.2",
            "3.3",
            "3.4",
            "3.5"
        ]
    },
    "research_context": {
        "paper_id": "2405.20900v1",
        "paper_title": "Large Language Models: A New Approach for Privacy Policy Analysis at Scale",
        "research_background": "**Motivation:**\nThe paper is motivated by the challenges associated with the extensive personal data collection and the resulting privacy concerns in the digital era. As businesses grow increasingly reliant on personal data to enhance targeted marketing, the necessity for thorough compliance with privacy laws such as GDPR and CCPA becomes paramount. Given the impracticality of manually assessing the adherence of numerous dynamically changing services to these laws, there is a pressing need for scalable, automated methods for privacy policy analysis.\n\n**Research Problem:**\nThe research problem centers on developing an automated, efficient, and effective way to analyze privacy policies at scale. Specifically, the paper aims to address the limitations of current Natural Language Processing (NLP) techniques used in this domain, such as symbolic approaches' lack of adaptability and the high resource costs associated with machine learning models that rely on manually annotated datasets. The paper seeks to explore the potential of Generative Artificial Intelligence (GenAI), particularly Large Language Models (LLMs) like ChatGPT, in automating the extraction of privacy practices from privacy policies without requiring annotated datasets.\n\n**Relevant Prior Work:**\n1. **Automated methods for privacy policy analysis**: Previous studies have proposed automating privacy policy analysis to address scalability issues faced by manual assessment [2  ###reference_b2###].\n2. **Statistical NLP and machine learning techniques**: The current state-of-the-art relies mostly on statistical NLP techniques, particularly supervised learning models, to train and evaluate the extraction of privacy practices, which necessitates extensive manually annotated datasets [3  ###reference_b3###, 4  ###reference_b4###, 5  ###reference_b5###, 6  ###reference_b6###]. These datasets are costly and time-consuming to create and are susceptible to errors [7  ###reference_b7###].\n3. **Symbolic approaches**: While some symbolic methods based on pre-defined rules exist, they have shown lower performance due to a lack of adaptability to different legal texts.\n4. **Current NLP challenges**: Building and training machine learning models require advanced technical expertise, creating a high barrier to entry and limiting their practical application to large-scale projects where benefits justify the costs.\n5. **Generative AI and LLMs**: There is an existing discourse on applying GenAI, particularly LLMs, in legal and regulatory contexts, which the paper contributes to [8  ###reference_b8###, 9  ###reference_b9###, 10  ###reference_b10###]. \n\nBy proposing a specific configuration of ChatGPT for privacy policy analysis and comparing it with state-of-the-art techniques, the paper aims to demonstrate the benefits of GenAI in terms of cost-effectiveness, efficiency, and ease of use, thus advocating for a paradigm shift in this research area.",
        "methodology": "The proposed method for privacy policy analysis leverages GPT models, with special emphasis on ChatGPT, and uses the Design Science Research (DSR) methodology to systematically develop and refine the framework. Key components and innovations of the methodology include:\n\n1. **Prompt Design, Configuration Parameters, and Model Selection**:\n   - The framework intricately designs prompts and selects configuration parameters and models to optimize the performance of ChatGPT in privacy policy analysis.\n\n2. **Design Science Research (DSR) Methodology**:\n   - The development process is guided by DSR, which underscores an iterative and systematic approach to designing new artifacts, in this case, the ChatGPT framework for analyzing privacy policies.\n\n3. **Iterative Split Testing Process**:\n   - Iterative split testing is employed to make incremental changes to the prompt designs, configuration parameters, or model selections. The performance of these changes is assessed in each iteration to achieve optimal results.\n\n4. **Performance Evaluation Against Unseen Data**:\n   - The framework's performance is rigorously evaluated against two unseen sets of privacy policies to ensure its robustness and effectiveness.\n\n5. **Comparative Analysis**:\n   - Comparative analyses are conducted with state-of-the-art solutions to benchmark the performance of the developed framework, ensuring it stands out as a novel and effective approach.\n\n6. **Generalization Capabilities**:\n   - The developed configuration is scrutinized for its ability to generalize across different privacy policies, which is critical for its broad applicability and reliability.\n\nBy following this systematic and iterative process, the authors propose a well-performing and generalizable configuration of ChatGPT, positing it as an innovative solution for the automated analysis of privacy policies.",
        "main_experiment_and_results": "The main experiment in the paper involves evaluating the performance of Large Language Models (LLMs), specifically ChatGPT, for privacy policy analysis at scale.\n\n**Datasets:**\n- The test sets include privacy policies used for extracting privacy practices.\n- Additional test cases also consider the declaration of international transfers within privacy policies.\n\n**Baselines:**\n- ChatGPT is compared to its closest generative AI (GenAI) rival, Llama 2.\n- State-of-the-art statistical and symbolic NLP approaches are also used as benchmark techniques to evaluate performance.\n\n**Evaluation Metrics:**\n- The performance of each model on these tasks is measured and compared.\n- Specific metrics used for evaluation are not detailed in the section provided but are likely to include precision, recall, and F1-score, common in NLP evaluations.\n\n**Main Experimental Results:**\n- ChatGPT is demonstrated to be a competent technique for privacy policy analysis.\n- The results suggest that ChatGPT's performance is competitive with, and potentially superior to, Llama 2.\n- Compared to statistical and symbolic NLP approaches, ChatGPT proves to be a capable alternative, with satisfactory performance in extracting privacy practices and identifying declarations of international transfers.\n\nThe experiment thus provides evidence supporting ChatGPT's effectiveness and generalization capabilities in the domain of privacy policy analysis."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "The goal is to optimize the prompt design to enhance the performance of ChatGPT in identifying types of personal data purportedly collected or shared as per the privacy policy.",
            "experiment_process": "The baseline prompt was structured in Data, Task, and Output format instruction segments and evaluated using the GPT-4 Turbo model with a temperature set to zero. Various configurations were tested, including specifying data boundaries, data placement, task description augmentation, message splitting, data pruning, segmentation, and few-shot prompting. Metrics evaluated were accuracy, recall, precision, and F1 score.",
            "result_discussion": "Specifying data boundaries and using double quotes improved accuracy (+1.47%) and consistency. Augmenting task description significantly improved accuracy (+4.58%) and precision (+5.96%), though it slightly decreased recall. Message splitting, data pruning, and segmentation with context showed minor improvements, while few-shot prompting with two examples yielded the best results (+3.31% accuracy). The finalized prompt structure includes segmented Data and Task messages and two-shot learning examples.",
            "ablation_id": "2405.20900v1.No1"
        },
        {
            "research_objective": "To evaluate the impact of different ChatGPT parameter settings on the consistency and performance of its responses in privacy policy analysis.",
            "experiment_process": "Temperature, top p, and system inputs were tested using the final prompt design. The temperature was varied while keeping other parameters constant, observing response consistency across multiple requests. The top p parameter was similarly evaluated. Different system instructions were tested to determine any improvements in context comprehension.",
            "result_discussion": "Lower temperature values highly increased response consistency to 89.5%. The combination of zero temperature and seed provided 90.51% consistency, the most reliable setting. Higher temperatures decreased response quality and completeness. No significant improvements were observed by varying the top p parameter. System inputs suggesting specific expert knowledge did not improve and sometimes worsened model performance.",
            "ablation_id": "2405.20900v1.No2"
        },
        {
            "research_objective": "To determine the effect of fine-tuning a ChatGPT model on performance metrics for privacy policy analysis.",
            "experiment_process": "A specific version (gpt-3.5-turbo-0613) of ChatGPT was fine-tuned using a dataset segmented into chunks to fit the token size limitation. The training set comprised 73 chunks aligned with manual annotations from the MAPP corpus, run with a default three-epoch configuration.",
            "result_discussion": "The fine-tuned model showed superior performance compared to the baseline model: accuracy increased to 0.867, precision to 0.803, and F1 score to 0.803. However, the fine-tuned model still could not surpass the GPT-4 Turbo model, primarily due to its greater token limit and context retention capabilities.",
            "ablation_id": "2405.20900v1.No3"
        },
        {
            "research_objective": "To validate the effectiveness and generalizability of the proposed ChatGPT configuration for privacy policy analysis.",
            "experiment_process": "The optimized prompt, parameters, and model configuration were applied to the MAPP control set and the OPP-115 dataset. The chosen configuration involved the prompt design from Section 3.2, a fixed temperature value, seed, and top p value, and utilized the GPT-4 Turbo model.",
            "result_discussion": "The configuration yielded high performance on the MAPP control set with 0.916 accuracy, 0.976 recall, 0.898 precision, and 0.935 F1 score. Validation against the larger OPP-115 dataset produced consistent results with slightly lower but robust performance metrics, indicating strong generalizability of the proposed approach.",
            "ablation_id": "2405.20900v1.No4"
        }
    ]
}