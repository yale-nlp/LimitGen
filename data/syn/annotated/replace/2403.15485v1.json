{
    "title": "MOGAM: A Multimodal Object-oriented Graph Attention Model for Depression Detection",
    "abstract": "Early detection plays a crucial role in the treatment of depression. Therefore, numerous studies have focused on social media platforms, where individuals express their emotions, aiming to achieve early detection of depression. However, the majority of existing approaches often rely on specific features, leading to limited scalability across different types of social media datasets, such as text, images, or videos. To overcome this limitation, we introduce a Multimodal Object-Oriented Graph Attention Model (MOGAM), which can be applied to diverse types of data, offering a more scalable and versatile solution. To ensure that our model can capture authentic symptoms of depression, we utilized the COCO dataset, known for its extensive collection of various media types, which are crucial for analyzing emotional expressions. To leverage the diverse features in COCO, we adopt a multimodal approach and collect additional metadata such as categories and annotations available within the dataset. To effectively aggregate these multimodal features, we employed a cross-attention mechanism. MOGAM achieved an accuracy of 0.871 and an F1-score of 0.888. Moreover, to validate the scalability of MOGAM, we evaluated its performance with a benchmark dataset and achieved comparable results with prior studies (0.61 F1-score). In conclusion, we believe that the proposed model, MOGAM, is an effective solution for detecting depression in social media, offering potential benefits in the early detection and treatment of this mental health condition.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Depression is one of the most increasingly severe mental illnesses worldwide. According to the World Health Organization (WHO), about 5% of adults globally suffer from depression WHO (2023). Unfortunately, the situation has been exacerbated by the COVID-19 pandemic, which has led to a 25% global increase in the prevalence of depression. Compounding the problem is the serious issue that not only the majority of individuals do not receive suitable treatment services, but also they are mainly unaware of their conditions (Pramanik et al., 2022).\n\nThe most representative traditional way of diagnosing depression is a series of face-to-face interviews conducted by psychiatrists, utilizing screening instruments such as CES-D (Radloff, 1977) or PHQ-9 (Kroenke et al., 2001). However, the COVID-19 pandemic has significantly restricted face-to-face interviews, adding further notable challenges to the diagnostic process (Self et al., 2021). To tackle this problem, a number of researchers have explored the use of social media as an effective man of diagnosing depression (Kim et al., 2021).\n\nSocial media provides an interactive platform for individuals to share their thoughts, assertions, experiences, and emotions, as well as to connect and communicate with other users. This identified characteristic has allowed social media to be a popular source for analyzing each user\u2019s psychological state (Lin et al., 2020; Liu et al., 2021), particularly in the context of depression detection. Several scholars have utilized various data-driven approaches and technologies, including natural language processing (NLP) and computer vision (CV) techniques, to explore and analyze the user\u2019s psychological state from social media (Kim et al., 2020).\n\nRepresentatively, text-based social media platforms like Twitter (Orabi et al., 2018; Cha et al., 2022), and Reddit (Kim et al., 2020; Ren et al., 2021) have been extensively explored for this purpose. However, the recent advancements in CV techniques have led to a surge in research emphasized on both image-based and video-oriented social media platforms such as Instagram (Maxim et al., 2020) and YouTube (Yoon et al., 2022). These platforms allow scholars to have new opportunities to examine users\u2019 mental states through visual content.\n\nAmong various content types in social media platforms, common objects captured on videos, as described in the COCO dataset (Lin et al., 2014), refer to images containing everyday objects in context, which serve as a rich resource for training machine learning models to understand and analyze visual information. The COCO dataset includes images of people, animals, and objects engaged in various activities, which provides a comprehensive basis for research in detecting and understanding visual content. Researchers utilize datasets like COCO to assess how individuals present their environments and activities through video logs, focusing on the context of surroundings and interactions rather than just the individual themselves.\n\nIn the context of depression detection, prior research has focused on exploring clinical interview videos of individuals with depression (Gratch et al., 2014). Facial expressions (Girard et al., 2013), acoustic signals (Ray et al., 2019), and body movements (Joshi et al., 2013) have been utilized as significant features to capture their psychological state. However, several challenges still exist in this approach. First, datasets based on clinical interviews are costly to obtain, resulting in a limited number of samples available for analysis. Additionally, models trained on such datasets may not be applicable to real-world scenarios or datasets. For instance, if a person\u2019s face or body is not detected or obscured by other objects, it becomes difficult to extract significant features from the videos.\n\nTo address these challenges, we propose a novel approach called MOGAM, a multimodal object-oriented graph attention model, for depression detection using COCO. We collected depression and non-depression videos from YouTube utilizing COCO concepts. It allows us to create high-risk depression and depression datasets consisting of videos that effectively represent the object context within which users operate. That is, our research question (RQ) is presented as follows:\n\nRQ: Can we accurately detect depression and high-risk depression using our proposed method with COCO?\n\nConsidering RQ, in addition to employing specific features like facial expressions, we employed a unique approach. We extract objects presented in each video (e.g., person, cup, bed), and create an object network by computing the co-occurrence count between pairs of objects. This forms the basic framework of MOGAM, which leverages the object co-occurrence network to extract features from COCO-related content.\n\nOur model utilizes a graph neural network (GNN), which is designed to learn representations of nodes or graphs based on their structural relationships. In this case, GNN operates on the object co-occurrence network, allowing it to capture the interactions among different objects in the context"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "High-risk mental disorders detection",
            "text": "Proactively identifying individuals at risk of mental disorders is crucial because early diagnosis is one of the most important issues in effective treatments  (Conus et al., 2014  ###reference_b22###). To achieve such a goal, data science and machine learning came to play an essential role in identifying potential symptoms and risk factors, as they are strongly correlated to mental disorders (Thieme et al., 2020  ###reference_b23###). For example, Hao et al. (2013  ###reference_b24###) aimed to identify users\u2019 mental health status through social media, leveraging machine learning to detect at-risk individuals. Similarly, Wang et al. (2017  ###reference_b25###) built a predictive model for eating disorders using Twitter data, analyzing social status, behavioral patterns, and psychometric properties of individuals in the disorder group and non-disorder group.\nNumerous studies have also investigated early detection and risk prediction for several notable mental disorders since the impact of depression on individuals and society is growing significantly, and early detection and diagnosis remain crucial for effective treatment (Halfin, 2007  ###reference_b26###). Xu et al. (2019  ###reference_b27###) and King et al. (2008  ###reference_b28###) have conducted extensive research on detecting high-risk groups for depression by analyzing symptoms and patterns associated with the disorder. However, they commonly face challenges such as time-consuming experiments, limited sample sizes, and relatively high costs. Several researchers have turned to social media platforms to overcome these limitations as a valuable potential source for detecting depression. This approach takes advantage of these platforms\u2019 vast amount of user-generated content, providing critical opportunities for more scalable and cost-effective detection methods (Cha et al., 2022  ###reference_b11###; Kim et al., 2020  ###reference_b9###; De Choudhury et al., 2013  ###reference_b29###)."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Depression Detection in Social Media",
            "text": "Utilizing social media datasets in terms of detecting depression provides some distinct advantages, such as enabling the researchers to make use of large datasets and to apply more sophisticated data-driven approaches. These features greatly help the understanding of an individual\u2019s mental health, which leads to the vast usage of social media in depression detection research (De Choudhury, 2013  ###reference_b30###, 2015  ###reference_b31###; Balani and De Choudhury, 2015  ###reference_b32###). For example, Al Asad et al. (2019  ###reference_b33###) and Lin et al. (2020  ###reference_b7###) employed Facebook and Twitter datasets to distinguish depression users using machine learning and deep learning approaches, respectively. Some other scholars used Reddit, the topic-oriented social media services, to apply various approaches such as linear SVM with bag-of-n-gram features (Pirina and \u00c7\u00f6ltekin, 2018  ###reference_b34###) or MLP on LIWC, LDA, and bi-gram features (Tadesse et al., 2019  ###reference_b35###). Most of these studies highlighted the data-driven approaches for detecting depression in social media, indicating the potentiality of social media as a valuable resource.\nVideo is one of the primarily modality for detecting depression, offering diverse features such as visual cues. Especially, user-oriented features presented in video content, including facial appearance and pose, are commonly adopted for depression detection. For example, Guo et al. (2022  ###reference_b36###) fed 2D landmarks and head pose features from the DAIC-WOZ dataset to the CNN-based model, achieving an accuracy of 0.857. Wang et al. (2018  ###reference_b37###) focused on Chinese individuals and employed clinical video samples to detect depression, achieving an accuracy of 0.789 using SVM with facial expression and eye movement features. Another mainstream of utilizing video data on depression detection is the multimodal approach, which incorporates various features such as visual, audio, and metadata information from given videos. Yoon et al. (2022  ###reference_b14###) and Chen et al. (2021  ###reference_b38###) also applied a multimodal fusion model on facial visual and acoustic features to identify people\u2019s depression.\nWhile prior studies primarily rely on human-oriented features like facial expressions, eye movement, or pose, one of the challenges is their limited applicability to videos without human presence. This poses a limitation for real-world applications since many vlogs may not necessarily involve people in their content. In contrast, the proposed object-based graph method is not restricted to human-appearing videos and can be applied to any type of video. To the best of our knowledge, your study is the first to examine depression detection in vlogs using an object-based Graph Neural Network (GNN) approach, which provides a novel perspective for addressing this important problem."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "MOGAM: Multimodal Object-oriented Graph Attention Model",
            "text": "In this section, we introduce the construction process of our vlog dataset and the classification method. Specifically, we cover (1) the procedure for collecting and preprocessing YouTube vlogs, and (2) the introduction of our multimodal object-oriented graph attention model: MOGAM."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Data Collection and Preprocessing",
            "text": "To collect vlogs, we utilized the YouTube API (Application Programming Interface). We searched vlogs using two specific hashtags: \u201c#\uc6b0\uc6b8\uc99d\ube0c\uc774\ub85c\uadf8\u201d (depression vlog) and \u201c#\uc77c\uc0c1\ube0c\uc774\ub85c\uadf8\u201d (daily vlog). All daily vlogs resulting from the search using the \u201c#\uc77c\uc0c1\ube0c\uc774\ub85c\uadf8\u201d hashtag were collected without any additional filtering procedures. However, for the depression vlogs indicated by the \u201c#\uc6b0\uc6b8\uc99d\ube0c\uc774\ub85c\uadf8\u201d hashtag, we manually inspected the results because the hashtag alone could not guarantee the vlogs\u2019 relevance to a medical depression diagnosis. Thus, the following steps were conducted.\n###figure_1### All vlogs uploaded by users, who wrote the \u201c#\uc6b0\uc6b8\uc99d\ube0c\uc774\ub85c\uadf8\u201d hashtag, were collected.\nTwo separate researchers carefully reviewed the content of each user\u2019s vlogs to identify the first collected vlog, which indicated a diagnosis vlog ().\nBased on the upload time of each depression diagnosis vlog , we divided the user\u2019s vlog list into two groups: high-risk depression vlogs, which were uploaded before , and depression vlogs, which were uploaded after . Figure 1  ###reference_### provides the representative cases demonstrating the vlog collection and division procedures.\nBased on these procedures, the resulting dataset is organized by 1888 daily, 2237 depression, and 642 high-risk depression vlogs. The dataset consists of three groups: daily, depression, and high-risk depression. The daily group includes vlogs uploaded by non-depressed individuals unrelated to depression. The depression group consists of vlogs from individuals clinically diagnosed with depression. The high-risk depression group includes vlogs created by individuals who have not received a clinical diagnosis of depression but may exhibit symptoms associated with depression. The average duration times of these vlogs are 903.39, 416.03, and 515.74 seconds, respectively (Table 1  ###reference_###).\nTo analyze vlogs for depression detection, we utilized both image frames and metadata. Initially, we split each vlog into frames at a rate of single frame per second (FPS). It resulted in transforming the vlogs into a collection of individual images. In addition to the image frames, we gathered relevant metadata from the vlogs, including the title, description and duration. It is worth noting that while providing a description for a vlog is not mandatory on Youtube, we encountered cases where no description was available. In such instances, we replaced the missing description with empty string as dummy input for ensuring consistency in the metadata collection procedures."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Depression Detection on the vlog",
            "text": "The proposed model, MOGAM, is organized by three key components for depression detection in vlogs as presented as follows: 1) our object-oriented graph neural network, 2) the extraction of additional visual and metadata features, and 3) the aggregation of multimodal features to detect depression in vlogs. Through the integration of the object-oriented GNN, visual, and metadata features, our proposed model provides a comprehensive approach for depression detection in vlogs, leveraging both the inherent patterns within the vlogs and the multimodal information available."
        },
        {
            "section_id": "3.2.1",
            "parent_section_id": "3.2",
            "section_name": "3.2.1 Object-oriented Graph Neural Network",
            "text": "###figure_2### To construct the vlog object adjacency matrix, we conducted the following steps:\nDetection of objects in frames: We employed YOLOv5 (Jocher et al., 2020  ###reference_b39###), one of the widely-employed open source object detection models, to detect objects in each frame of the vlogs.\nDefining nodes and edges: We defined the detected the objects and co-occurrence count of each object pair in a frame as nodes and edges, respectively. If multiple objects of the same class were found in a frame (e.g. two different cup), we considered them as separate nodes.\nConstruction of adjacency matrix: To build the adjacency matrix, we utilized the co-occurrence counts of the object pairs.\nNormalization: Because the duration of vlogs may vary, the adjacency matrix was normalized by dividing each entry by the total number of frames. It ensures that the graph structure remains consistent irrespective of the length of vlogs.\nBased on these steps, we got a weighted undirected adjacency matrix which represents the object co-occurrence in the vlog. This matrix can capture the relationships among different objects, and serve as a basis for comparing the graph structures of other vlogs. The construction of the vlog object adjacency matrix is on of the fundamental steps in our approach to investigate the vlog depression detection. That is, the following equation is presented for building the proposed vlog object adjacency matrix ():\nwhere  denote the number of frames in the th vlog, the function  returns the co-occurrence count of two objects () in th frame of th vlog, respectively. Next, we fed  into three off-the-shelf GNN models: GCN (Kipf and Welling, 2016  ###reference_b40###), GraphSAGE (Hamilton et al., 2017  ###reference_b41###), and GAT (Veli\u010dkovi\u0107 et al., 2018  ###reference_b42###). We compare the results of our proposed framework, MOGAM with the existing GNN models. The detailed description of the procedures are presented as follows:\nwhere  are nodes and  denote the nonlinear function, node \u2019s feature vectors at th layer, neighbors of node , information of neighbor\u2019s and myself in the previous layer, attention weight, and aggregation function, respectively. Next, we fed  and node feature matrix made up of one-hot vectors into a number of GNN layers. Then, to obtain graph-level representation, we applied a global mean pooling, which involves averaging the node features across the node dimension. This pooling operation makes sure that the output representation can effectively capture the overall vlog information."
        },
        {
            "section_id": "3.2.2",
            "parent_section_id": "3.2",
            "section_name": "3.2.2 Visual Feature",
            "text": "To extract visual features, we utilized a pre-trained ResNet (He et al., 2016  ###reference_b43###), which is commonly employed for transfer learning and image feature extraction. We extracted feature vectors with a dimension of 1,000 and reduced them to the same size as the object-oriented graph feature using a fully connected layer. To ensure that the extract features are independent of the vlog length, we averaged all feature vectors."
        },
        {
            "section_id": "3.2.3",
            "parent_section_id": "3.2",
            "section_name": "3.2.3 Metadata Feature",
            "text": "For metadata feature extraction, we chose the title, description and duration from various metadata, which is available on Youtube, one of the globally used social media platforms. These features are selected by the eusers during the vlog uploading procedures. To prepare the metadata, we removed unnecessary textual information such as email addresses, URLs, and non-Korean text via data pre-processing steps. Consequently, we utilized pre-trained KoBERT (SKTBrain, 2023  ###reference_b44###).\nNote that if other multilingual language models such as M-BERT (Pires et al., 2019  ###reference_b45###) or XLMs (Lample and Conneau, 2019  ###reference_b46###) were adopted, MOGAM would be applicable to various languages. The feature vectors obtained from the title, description, and duration of the vlog were concatenated into a single feature vector."
        },
        {
            "section_id": "3.2.4",
            "parent_section_id": "3.2",
            "section_name": "3.2.4 Aggregation & Detection",
            "text": "Each encoder (, , and ) generates features  and  where  denotes the hidden dimension (Figure 2  ###reference_###). Then, we concatenated visual and metadata features to build additional integrated features.\nTo enhance the model\u2019s ability to capture the inherent patterns of vlogs, which may not be captured by the baseline models, we incorporated a cross-attention mechanism within a transformer architecture (Vaswani et al., 2017  ###reference_b47###). In this mechanism, we utilized  as the query (Q) and  as the key (K) and value (V) for the cross-attention module.\nTo prevent an over-fitting tendency, we applied a dropout regularization. We employed several fully connected layers to reduce the dimensionality of the integrated multimodal features, , for vlog classification. The reduced features were then passed through a sigmoid function. The output represents the logit of the corresponding label."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We conducted three conditional experiments as follows: Daily versus Depression Daily versus High-risk Depression Scalability Evaluation In addition to conducting a series of experiments using the multimodal approach, we implemented the experiments not only using the unimodal () approach, but also employing multimodal information for examining the effects of features on the model performance. All experiments were conducted using a single NVIDIA RTX A6000 48GB GPU and Python 3.7. We used the Yolo v5 model, which was pre-trained on the COCO dataset (Lin et al., 2014  ###reference_b48###), which consists of 80 objects. We split both datasets into the train, validation, and test sets in an 8:1:1 proportion (five times). The exact number of each set is presented in the appendix. We used the Adam optimizer (Kingma and Ba, 2014  ###reference_b49###) and set batch size, epochs, and hidden dimensions to 32, 500, and 1024, respectively."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Daily & Depression",
            "text": "The performance of the proposed model on both daily and COCO vlogs is summarized in Table 2. Except for GraphSAGE, all the proposed models exhibited significant accuracy and F1-score, providing evidence for the effectiveness of GNN-based models in classifying COCO vlogs. Notably, MOGAM with GAT achieved the highest F1-score among the baselines, indicating that the cross-attention mechanism is a suitable approach for learning representations based on the relationships between objects. This finding underscores the importance of capturing inter-object relationships in effectively detecting depression patterns. Consistent with the findings of prior work (Yoon et al., 2022), the models incorporating multimodal features demonstrated superior performance compared to the baselines. This finding suggests that the integration of object-based graph features with visual and metadata features creates a robust framework for effectively identifying depression in vlogs."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Daily & High-risk Potential",
            "text": "To learn the symptoms and patterns of the high-risk depression state prior to clinical diagnosis, we trained our models using the daily and high-risk potential datasets. The performance comparisons on daily and high-risk depression data are also presented in Table 2. The data structure and classification model architecture for the daily datasets are identical to those described in the previous section. We observed that incorporating multimodal features enables the model to capture the distinctions between daily and high-risk depression data. Importantly, with the implementation of MOGAM, all models achieved accurate discrimination between daily and high-risk depression data (F1-score of 0.997)."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Multiclass Classification",
            "text": "Finally, a multiclass classification experiment was conducted within the same experimental environment as the previous sections, aiming to simultaneously learn and distinguish symptoms and patterns across all three states. The models were trained on datasets representing daily, depression, and high-risk potential states, and the comparison of results is presented in Table 3  ###reference_###. In contrast to the binary classification, the GraphSAGE model combined with MOGAM exhibited the best F1-score of 0.815 and 0.750 for daily and high-risk potential states, respectively while the GAT model with MOGAM presented superior performance in the depression group with an F1-score of 0.783. However, the GCN, GraphSAGE, and GAT models without MOGAM exhibited limited ability to distinguish high-risk potential states, mostly misclassifying them as daily and depression (F1-score: 0.061, 0.083, 0.182). In contrast, the proposed MOGAM ensured comprehensive classification of high-risk potential states across all models (F1-score: 0.713, 0.750, 0.736). Consequently, we observed that MOGAM is suited for capturing and classifying image-specific features of each state simultaneously, even in an imbalanced data environment."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Scalability Evaluation",
            "text": "To address the scalability of the proposed methods, we conducted experiments on COCO (Lin et al., 2014), which is used in several prior research for image recognition tasks. It consists of over 200,000 images with more than 80 object categories. To conduct a fair evaluation, we followed the same procedures to extract object-based graph, visual, and metadata features from COCO. Subsequently, we applied the models trained on our image dataset, including both baseline and multimodal features, to the COCO dataset. Among these models, the GAT-based model trained with multimodal features achieved the highest F1-score of 0.612, which is comparable to the reported results in prior research (0.635). Figure 3 shows the comparison results."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Object Distribution",
            "text": "To investigate gaps in the frequency of object appearance across different state vlogs, we conducted one-way analyses of variance (ANOVA) and post hoc tests. Employing an ANOVA involving 80 objects, we identified 47 objects exhibiting statistically significant differences in their mean values among the three groups. Subsequently, the results of post hoc tests present that the object \u201cfork\" exhibited significant differences in all groups, while only eight objects (knife, cake, handbag, sandwich, apple, wine glass, banana, and vase) exhibited differences between the daily and non-daily group (depression and high-risk potential). The objects associated with food demonstrated notably low p-values, indicating a strong association between food-related objects and the user\u2019s states in vlogs. Moreover, the frequency of object appearance is higher in the daily group compared to the non-daily group. The mean and standard deviation (SD) normalized appearance counts for the five objects with the lowest p-value are presented in Table 4. The entire results of analysis and the post hoc test are shown in the appendix."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "In this study, we proposed MOGAM, a new multimodal object-oriented graph attention model for depression detection on YouTube vlogs. To facilitate our approach, we collected a vlog dataset including vlogs of daily, depression, and high-risk potential depression groups. Then, we extracted three key multimodal features (graph, visual, and metadata) from the vlogs and combined them using a cross-attention mechanism. By leveraging these integrated multimodal features, our proposed model aims to determine whether the vlog uploader is experiencing depression. The results on the dataset demonstrate that our model effectively captures and detects depression symptoms and patterns in the vlogs by leveraging the power of multimodal features.\nBased on our findings, we can draw several implications. Many researchers utilize social media platforms for depression detection, leading to the development of various depression-related datasets (Gratch et al., 2014  ###reference_b18###; Yoon et al., 2022  ###reference_b14###). However, there is often a trade-off between dataset size and reliability. Building a dataset with clinical diagnoses can be prohibitively expensive, making it challenging to create large-scale datasets of this nature. In contrast, our vlog dataset comprises vlogs from individuals who have undergone clinical diagnosis, providing valuable insights into real depression patterns and symptoms. Moreover, our study validates the effectiveness of the object-oriented graph encoder and multimodal features for depression detection in YouTube videos. The scalability experiment results suggest that our proposed MOGAM can be applied to other datasets, including those in different languages and environments.\nAlthough we present several implications, notable concerns remain. First, the extraction of object-oriented graph features depends on the performance of the employed object detection model (e.g., Yolo v5). If the object detection model poorly performs, the constructed object co-occurrence network may not accurately represent the entire vlog, that can lead to the lower performance level of MOGAM. Second, because additional features we employed can depend on off-the-shelf models, we extracted them using pre-trained encoders (ResNet and KoBERT), which were trained on other datasets. Therefore, the quality and performance of these off-the-shelf models can impact the effectiveness of the additional features in our framework.\nIn future research, we aim to extend our method. We could apply state-of-the-art (SOTA) object detection methods. The SOTA object detection model can take advantage of a large number of objects and accurate detection performance, letting the model to capture complex relationships among objects. In addition, by utilizing encoders such as Transformer, we can effectively encode multimodal features and generate integrated representations of vlogs, leading to the improved performance. Furthermore, since depression is not the only mental disorder, we could be valuable to collect vlogs related to other mental disorders (e.g. anxiety, BPD, bipolar disorder) and apply our models to them.\nWe believe that our dataset and method as valuable tools for gaining insights into potential depression symptoms in vlogs. Consequently, wee hope that our model can assist individuals who are suffering from depression but may be unaware of their symptoms, as well as those who are not receiving adequate treatment. By effectively detecting inherent depression patterns in their vlogs, our model has the potential to offer support and guidance to those in need, ultimately leading to improved mental health outcomes."
        }
    ],
    "appendix": [],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T1\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.1\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" id=\"S3.T1.1.1.1.1\"></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"S3.T1.1.1.1.2\"># of Vlogs</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"S3.T1.1.1.1.3\">Avg. duration</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S3.T1.1.2.1.1\">Daily</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T1.1.2.1.2\">1888</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T1.1.2.1.3\">903.39s</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S3.T1.1.3.2.1\">Depression</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T1.1.3.2.2\">2237</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T1.1.3.2.3\">416.03s</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r\" id=\"S3.T1.1.4.3.1\">High-risk depression</th>\n<td class=\"ltx_td ltx_align_right ltx_border_b\" id=\"S3.T1.1.4.3.2\">642</td>\n<td class=\"ltx_td ltx_align_right ltx_border_b\" id=\"S3.T1.1.4.3.3\">515.74s</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Descriptive statistics of the collected dataset.</figcaption>\n</figure>",
            "capture": "Table 1: Descriptive statistics of the collected dataset."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T2\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T2.1\" style=\"width:466.1pt;height:427.5pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-12.3pt,11.3pt) scale(0.95,0.95) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T2.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.1.1\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.1\" style=\"width:78.2pt;\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.2\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.3\">Label</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.1.1.1.1.4\">Accuracy</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.1.1.1.1.5\">Precision</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.1.1.1.1.6\">Recall</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.1.1.1.1.7\">F1-Score</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.2.1\">\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.2.1.1\" rowspan=\"12\" style=\"width:78.2pt;\"><span class=\"ltx_text ltx_align_top\" id=\"S4.T2.1.1.2.1.1.1\">Daily and Depression</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.2.1.2\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.1.1.2.1.2.1\">GCN</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.2.1.3\">Daily</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.2.1.4\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.1.1.2.1.4.1\">0.802</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.2.1.5\">0.800</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.2.1.6\">0.784</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.2.1.7\">0.792</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.3.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.3.2.1\">Depression</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.3.2.2\">0.803</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.3.2.3\">0.818</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.3.2.4\">0.810</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.4.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.4.3.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.1.1.4.3.1.1\">GraphSAGE</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.4.3.2\">Daily</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.4.3.3\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.1.1.4.3.3.1\">0.794</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.4.3.4\">0.779</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.4.3.5\">0.799</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.4.3.6\">0.789</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.5.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.5.4.1\">Depression</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.5.4.2\">0.809</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.5.4.3\">0.790</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.5.4.4\">0.799</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.6.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.6.5.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.1.1.6.5.1.1\">GAT</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.6.5.2\">Daily</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.6.5.3\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.1.1.6.5.3.1\">0.826</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.6.5.4\">0.798</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.6.5.5\">0.854</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.6.5.6\">0.825</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.7.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.7.6.1\">Depression</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.7.6.2\">0.855</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.7.6.3\">0.799</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.7.6.4\">0.826</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.8.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.8.7.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.1.1.8.7.1.1\">MOGAM with GCN</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.8.7.2\">Daily</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.8.7.3\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.1.1.8.7.3.1\">0.839</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.8.7.4\">0.787</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.8.7.5\">0.851</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.8.7.6\">0.818</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.9.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.9.8.1\">Depression</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.9.8.2\">0.883</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.9.8.3\">0.831</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.9.8.4\">0.857</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.10.9\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.10.9.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.1.1.10.9.1.1\">MOGAM with GraphSAGE</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.10.9.2\">Daily</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.10.9.3\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.1.1.10.9.3.1\">0.864</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.10.9.4\">0.869</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.10.9.5\">0.799</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.10.9.6\">0.832</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.11.10\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.11.10.1\">Depression</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.11.10.2\">0.861</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.11.10.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.11.10.3.1\">0.911</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.11.10.4\">0.885</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.12.11\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.12.11.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.1.1.12.11.1.1\">MOGAM with GAT</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.12.11.2\">Daily</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.12.11.3\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.12.11.3.1\">0.871</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.12.11.4\">0.850</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.12.11.5\">0.845</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.12.11.6\">0.847</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.13.12\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.13.12.1\">Depression</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.13.12.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.13.12.2.1\">0.887</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.13.12.3\">0.890</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.13.12.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.13.12.4.1\">0.888</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.14.13\">\n<td class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.14.13.1\" rowspan=\"12\" style=\"width:78.2pt;\"><span class=\"ltx_text ltx_align_top\" id=\"S4.T2.1.1.14.13.1.1\">Daily and High-risk Depression</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.14.13.2\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.1.1.14.13.2.1\">GCN</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.14.13.3\">Daily</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.14.13.4\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.1.1.14.13.4.1\">0.811</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.14.13.5\">0.832</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.14.13.6\">0.930</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.14.13.7\">0.878</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.15.14\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.15.14.1\">High-risk potential</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.15.14.2\">0.717</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.15.14.3\">0.485</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.15.14.4\">0.579</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.16.15\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.16.15.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.1.1.16.15.1.1\">GraphSAGE</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.16.15.2\">Daily</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.16.15.3\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.1.1.16.15.3.1\">0.771</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.16.15.4\">0.814</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.16.15.5\">0.893</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.16.15.6\">0.851</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.17.16\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.17.16.1\">High-risk potential</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.17.16.2\">0.600</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.17.16.3\">0.441</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.17.16.4\">0.509</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.18.17\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.18.17.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.1.1.18.17.1.1\">GAT</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.18.17.2\">Daily</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.18.17.3\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.1.1.18.17.3.1\">0.823</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.18.17.4\">0.844</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.18.17.5\">0.930</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.18.17.6\">0.885</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.19.18\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.19.18.1\">High-risk potential</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.19.18.2\">0.735</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.19.18.3\">0.529</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.19.18.4\">0.615</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.20.19\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.20.19.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.1.1.20.19.1.1\">MOGAM with GCN</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.20.19.2\">Daily</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.20.19.3\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.1.1.20.19.3.1\">0.996</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.20.19.4\">0.995</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.20.19.5\">1.000</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.20.19.6\">0.997</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.21.20\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.21.20.1\">High-risk potential</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.21.20.2\">1.000</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.21.20.3\">0.986</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.21.20.4\">0.993</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.22.21\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.22.21.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.1.1.22.21.1.1\">MOGAM with GraphSAGE</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.22.21.2\">Daily</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.22.21.3\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.1.1.22.21.3.1\">0.996</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.22.21.4\">0.995</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.22.21.5\">1.000</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.22.21.6\">0.997</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.23.22\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.23.22.1\">High-risk potential</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.23.22.2\">1.000</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.23.22.3\">0.986</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.23.22.4\">0.993</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.24.23\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.24.23.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.1.1.24.23.1.1\">MOGAM with GAT</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.24.23.2\">Daily</td>\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_t\" id=\"S4.T2.1.1.24.23.3\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.24.23.3.1\">0.996</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.24.23.4\">0.995</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.24.23.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.24.23.5.1\">1.000</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.24.23.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.24.23.6.1\">0.997</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.25.24\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S4.T2.1.1.25.24.1\">High-risk potential</td>\n<td class=\"ltx_td ltx_align_right ltx_border_b\" id=\"S4.T2.1.1.25.24.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.25.24.2.1\">1.000</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_b\" id=\"S4.T2.1.1.25.24.3\">0.986</td>\n<td class=\"ltx_td ltx_align_right ltx_border_b\" id=\"S4.T2.1.1.25.24.4\">0.993</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Performance comparisons between baselines and proposed models for our datasets.</figcaption>\n</figure>",
            "capture": "Table 2: Performance comparisons between baselines and proposed models for our datasets."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T3\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T3.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.1\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T3.1.1.1.1\" style=\"width:113.8pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.1.1.1.1\">Model</p>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T3.1.1.1.2\">Label</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.1.1.1.3\">Accuracy</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.1.1.1.4\">Precision</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.1.1.1.5\">Recall</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.1.1.1.6\">F1-Score</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T3.1.2.1\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T3.1.2.1.1\" rowspan=\"3\" style=\"width:113.8pt;\"><span class=\"ltx_text ltx_align_top\" id=\"S4.T3.1.2.1.1.1\">MOGAM with GCN</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T3.1.2.1.2\">Daily</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T3.1.2.1.3\" rowspan=\"3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.2.1.3.1\">0.790</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T3.1.2.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.2.1.4.1\">0.832</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T3.1.2.1.5\">0.806</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T3.1.2.1.6\">0.819</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.1.3.2.1\">Depression</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.1.3.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.3.2.2.1\">0.724</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.1.3.2.3\">0.849</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.1.3.2.4\">0.782</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.1.4.3.1\">High-risk potential</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.1.4.3.2\">0.973</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.1.4.3.3\">0.563</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.1.4.3.4\">0.713</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.5.4\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T3.1.5.4.1\" rowspan=\"3\" style=\"width:113.8pt;\"><span class=\"ltx_text ltx_align_top\" id=\"S4.T3.1.5.4.1.1\">MOGAM with GraphSAGE</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T3.1.5.4.2\">Daily</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T3.1.5.4.3\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T3.1.5.4.3.1\">0.788</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T3.1.5.4.4\">0.789</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T3.1.5.4.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.5.4.5.1\">0.843</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T3.1.5.4.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.5.4.6.1\">0.815</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.6.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.1.6.5.1\">Depression</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.1.6.5.2\">0.749</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.1.6.5.3\">0.792</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.1.6.5.4\">0.770</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.7.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.1.7.6.1\">High-risk potential</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.1.7.6.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.7.6.2.1\">0.975</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.1.7.6.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.7.6.3.1\">0.609</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.1.7.6.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.7.6.4.1\">0.750</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.8.7\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\" id=\"S4.T3.1.8.7.1\" rowspan=\"3\" style=\"width:113.8pt;\"><span class=\"ltx_text ltx_align_top\" id=\"S4.T3.1.8.7.1.1\">MOGAM with GAT</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T3.1.8.7.2\">Daily</th>\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_t\" id=\"S4.T3.1.8.7.3\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T3.1.8.7.3.1\">0.785</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T3.1.8.7.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.8.7.4.1\">0.832</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T3.1.8.7.5\">0.775</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T3.1.8.7.6\">0.802</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.9.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.1.9.8.1\">Depression</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.1.9.8.2\">0.723</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.1.9.8.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.9.8.3.1\">0.854</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.1.9.8.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.9.8.4.1\">0.783</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.10.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r\" id=\"S4.T3.1.10.9.1\">High-risk potential</th>\n<td class=\"ltx_td ltx_align_right ltx_border_b\" id=\"S4.T3.1.10.9.2\">0.929</td>\n<td class=\"ltx_td ltx_align_right ltx_border_b\" id=\"S4.T3.1.10.9.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.10.9.3.1\">0.609</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_b\" id=\"S4.T3.1.10.9.4\">0.736</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Results of the multiclass classification. The train dataset consists of 1,507 daily, 1,567 depression, and 502 high-risk potential vlogs and the test dataset consists of 191, 192, and 64. The results of baselines are presented in appendix.</figcaption>\n</figure>",
            "capture": "Table 3: Results of the multiclass classification. The train dataset consists of 1,507 daily, 1,567 depression, and 502 high-risk potential vlogs and the test dataset consists of 191, 192, and 64. The results of baselines are presented in appendix."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T4\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T4.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T4.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S4.T4.1.1.1.1\">Object</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S4.T4.1.1.1.2\">Label</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"S4.T4.1.1.1.3\">Mean (SD)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T4.1.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T4.1.2.1.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T4.1.2.1.1.1\">Knife</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.2.1.2\">Daily</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T4.1.2.1.3\">0.284 (0.591)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.3.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.3.2.1\">Depression</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T4.1.3.2.2\">0.068 (0.291)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.4.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.4.3.1\">High-risk potential</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T4.1.4.3.2\">0.079 (0.332)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.5.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T4.1.5.4.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T4.1.5.4.1.1\">Fork</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.5.4.2\">Daily</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T4.1.5.4.3\">0.240 (0.517)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.6.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.6.5.1\">Depression</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T4.1.6.5.2\">0.071 (0.161)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.7.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.7.6.1\">High-risk potential</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T4.1.7.6.2\">0.374 (0.631)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.8.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T4.1.8.7.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T4.1.8.7.1.1\">Cake</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.8.7.2\">Daily</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T4.1.8.7.3\">0.340 (1.286)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.9.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.9.8.1\">Depression</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T4.1.9.8.2\">0.078 (0.530)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.10.9\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.10.9.1\">High-risk potential</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T4.1.10.9.2\">0.093 (0.451)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.11.10\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T4.1.11.10.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T4.1.11.10.1.1\">Handbag</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.11.10.2\">Daily</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T4.1.11.10.3\">0.527 (1.316)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.12.11\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.12.11.1\">Depression</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T4.1.12.11.2\">0.211 (1.670)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.13.12\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.13.12.1\">High-risk potential</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T4.1.13.12.2\">0.298 (1.549)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.14.13\">\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\" id=\"S4.T4.1.14.13.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T4.1.14.13.1.1\">Sandwich</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.14.13.2\">Daily</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T4.1.14.13.3\">0.130 (0.562)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.15.14\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.15.14.1\">Depression</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T4.1.15.14.2\">0.040 (0.314)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.16.15\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"S4.T4.1.16.15.1\">High-risk potential</td>\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_t\" id=\"S4.T4.1.16.15.2\">0.056 (0.310)</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>Descriptive information of top-object appearances.</figcaption>\n</figure>",
            "capture": "Table 4: Descriptive information of top-object appearances."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.15485v1_figure_1.png",
            "caption": "Figure 1: Examples of the collected datasets."
        },
        "2": {
            "figure_path": "2403.15485v1_figure_2.png",
            "caption": "Figure 1: Examples of the collected datasets."
        },
        "3": {
            "figure_path": "2403.15485v1_figure_3.png",
            "caption": "Figure 1: Examples of the collected datasets."
        },
        "4": {
            "figure_path": "2403.15485v1_figure_4.png",
            "caption": "Figure 1: Examples of the collected datasets."
        },
        "5": {
            "figure_path": "2403.15485v1_figure_5.png",
            "caption": "Figure 1: Examples of the collected datasets."
        },
        "6": {
            "figure_path": "2403.15485v1_figure_6.png",
            "caption": "Figure 2: The overview of MOGAM; It is organized by three parts: data collection, feature extraction, and detection."
        },
        "7": {
            "figure_path": "2403.15485v1_figure_7.png",
            "caption": "Figure 3: Performance evaluation of prior multimodal fusion models and MOGAM with GAT on D-Vlog dataset"
        }
    },
    "references": [
        {
            "1": {
                "title": "Depressive disorder (depression).",
                "author": "WHO.",
                "venue": "https://www.who.int/news-room/fact-sheets/detail/depression, 2023.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Depression-level prediction during covid-19 pandemic among the people of bangladesh using ensemble technique: Mirf stacking and mirf voting.",
                "author": "Anik Pramanik, Md Hasan Imam Bijoy, and Md Sadekur Rahman.",
                "venue": "In Proc. of IC4IR \u201921, pages 71\u201387. Springer, 2022.",
                "url": null
            }
        },
        {
            "3": {
                "title": "The ces-d scale: A self-report depression scale for research in the general population.",
                "author": "Lenore Sawyer Radloff.",
                "venue": "Applied psychological measurement, 1(3):385\u2013401, 1977.",
                "url": null
            }
        },
        {
            "4": {
                "title": "The phq-9: validity of a brief depression severity measure.",
                "author": "Kurt Kroenke, Robert L Spitzer, and Janet BW Williams.",
                "venue": "Journal of general internal medicine, 16(9):606\u2013613, 2001.",
                "url": null
            }
        },
        {
            "5": {
                "title": "Conducting interviews during the covid-19 pandemic and beyond.",
                "author": "Becky Self et al.",
                "venue": "In Forum Qualitative Sozialforschung/Forum: Qualitative Social Research, volume 22. DEU, 2021.",
                "url": null
            }
        },
        {
            "6": {
                "title": "Machine learning for mental health in social media: bibliometric study.",
                "author": "Jina Kim, Daeun Lee, Eunil Park, et al.",
                "venue": "Journal of Medical Internet Research, 23(3):e24870, 2021.",
                "url": null
            }
        },
        {
            "7": {
                "title": "Sensemood: depression detection on social media.",
                "author": "Chenhao Lin, Pengwei Hu, Hui Su, Shaochun Li, Jing Mei, Jie Zhou, and Henry Leung.",
                "venue": "In Proc. of ICMR \u201920, pages 407\u2013411, 2020.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Covid-19 information overload and generation z\u2019s social media discontinuance intention during the pandemic lockdown.",
                "author": "Hongfei Liu, Wentong Liu, Vignesh Yoganathan, and Victoria-Sophie Osburg.",
                "venue": "Technological Forecasting and Social Change, 166:120600, 2021.",
                "url": null
            }
        },
        {
            "9": {
                "title": "A deep learning model for detecting mental illness from user content on social media.",
                "author": "Jina Kim, Jieon Lee, Eunil Park, and Jinyoung Han.",
                "venue": "Scientific reports, 10(1):1\u20136, 2020.",
                "url": null
            }
        },
        {
            "10": {
                "title": "Deep learning for depression detection of twitter users.",
                "author": "Ahmed Husseini Orabi, Prasadith Buddhitha, Mahmoud Husseini Orabi, and Diana Inkpen.",
                "venue": "In Proc. of CLPsych \u201918, pages 88\u201397, 2018.",
                "url": null
            }
        },
        {
            "11": {
                "title": "A lexicon-based approach to examine depression detection in social media: the case of twitter and university community.",
                "author": "Junyeop Cha, Seoyun Kim, and Eunil Park.",
                "venue": "Humanities and Social Sciences Communications, 9(1):1\u201310, 2022.",
                "url": null
            }
        },
        {
            "12": {
                "title": "Depression detection on reddit with an emotion-based attention network: algorithm development and validation.",
                "author": "Lu Ren, Hongfei Lin, Bo Xu, Shaowu Zhang, Liang Yang, and Shichang Sun.",
                "venue": "JMIR Medical Informatics, 9(7):e28754, 2021.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Predicting depression with social media images.",
                "author": "S Maxim, I Smirnov, and N Ignatiev.",
                "venue": "In Proc. of ICPRAM \u201920, pages 235\u2013240, 2020.",
                "url": null
            }
        },
        {
            "14": {
                "title": "D-vlog: Multimodal vlog dataset for depression detection.",
                "author": "Jeewoo Yoon, Chaewon Kang, Seungbae Kim, and Jinyoung Han.",
                "venue": "In Proc. of AAAI \u201922, volume 36, pages 12226\u201312234, 2022.",
                "url": null
            }
        },
        {
            "15": {
                "title": "Is \u201cyoutube\u201d telling or selling you something? tobacco content on the youtube video-sharing website.",
                "author": "Becky Freeman and Simon Chapman.",
                "venue": "Tobacco control, 16(3):207\u2013210, 2007.",
                "url": null
            }
        },
        {
            "16": {
                "title": "Vlogging about school on youtube: An exploratory study.",
                "author": "Chareen Snelson.",
                "venue": "New Media & Society, 17(3):321\u2013339, 2015.",
                "url": null
            }
        },
        {
            "17": {
                "title": "Health vlogs as social support for chronic illness management.",
                "author": "Jina Huh, Leslie S Liu, Tina Neogi, Kori Inkpen, and Wanda Pratt.",
                "venue": "ACM Transactions on Computer-Human Interaction (TOCHI), 21(4):1\u201331, 2014.",
                "url": null
            }
        },
        {
            "18": {
                "title": "The distress analysis interview corpus of human and computer interviews.",
                "author": "Jonathan Gratch, Ron Artstein, Gale Lucas, Giota Stratou, Stefan Scherer, Angela Nazarian, Rachel Wood, Jill Boberg, David DeVault, Stacy Marsella, et al.",
                "venue": "Technical report, University of Southern California Los Angeles, 2014.",
                "url": null
            }
        },
        {
            "19": {
                "title": "Social risk and depression: Evidence from manual and automatic facial expression analysis.",
                "author": "Jeffrey M Girard, Jeffrey F Cohn, Mohammad H Mahoor, Seyedmohammad Mavadati, and Dean P Rosenwald.",
                "venue": "In Proc. of FG \u201913, pages 1\u20138. IEEE, 2013.",
                "url": null
            }
        },
        {
            "20": {
                "title": "Multi-level attention network using text, audio and video for depression prediction.",
                "author": "Anupama Ray, Siddharth Kumar, Rutvik Reddy, Prerana Mukherjee, and Ritu Garg.",
                "venue": "In Proc. of AVEC \u201919, pages 81\u201388, 2019.",
                "url": null
            }
        },
        {
            "21": {
                "title": "Relative body parts movement for automatic depression analysis.",
                "author": "Jyoti Joshi, Abhinav Dhall, Roland Goecke, and Jeffrey F Cohn.",
                "venue": "In 2013 Humaine association conference on affective computing and intelligent interaction, pages 492\u2013497. IEEE, 2013.",
                "url": null
            }
        },
        {
            "22": {
                "title": "Public health significance of bipolar disorder: implications for early intervention and prevention.",
                "author": "Philippe Conus, Craig Macneil, and Patrick D McGorry.",
                "venue": "Bipolar disorders, 16(5):548\u2013556, 2014.",
                "url": null
            }
        },
        {
            "23": {
                "title": "Machine learning in mental health: A systematic review of the hci literature to support the development of effective and implementable ml systems.",
                "author": "Anja Thieme, Danielle Belgrave, and Gavin Doherty.",
                "venue": "ACM Transactions on Computer-Human Interaction (TOCHI), 27(5):1\u201353, 2020.",
                "url": null
            }
        },
        {
            "24": {
                "title": "Predicting mental health status on social media: A preliminary study on microblog.",
                "author": "Bibo Hao, Lin Li, Ang Li, and Tingshao Zhu.",
                "venue": "In Proc. of CCD \u201913, pages 101\u2013110. Springer, 2013.",
                "url": null
            }
        },
        {
            "25": {
                "title": "Detecting and characterizing eating-disorder communities on social media.",
                "author": "Tao Wang, Markus Brede, Antonella Ianni, and Emmanouil Mentzakis.",
                "venue": "In Proc. of WSDM \u201917, pages 91\u2013100, 2017.",
                "url": null
            }
        },
        {
            "26": {
                "title": "Depression: the benefits of early and appropriate treatment.",
                "author": "Aron Halfin.",
                "venue": "American Journal of Managed Care, 13(4):S92, 2007.",
                "url": null
            }
        },
        {
            "27": {
                "title": "Individualized prediction of depressive disorder in the elderly: a multitask deep learning approach.",
                "author": "Zhongzhi Xu, Qingpeng Zhang, Wentian Li, Mingyang Li, and Paul Siu Fai Yip.",
                "venue": "International journal of medical informatics, 132:103973, 2019.",
                "url": null
            }
        },
        {
            "28": {
                "title": "Development and validation of an international risk prediction algorithm for episodes of major depression in general practice attendees: the predictd study.",
                "author": "Michael King, Carl Walker, Gus Levy, Christian Bottomley, Patrick Royston, Scott Weich, Juan Angel Bellon-Saameno, Berta Moreno, Igor \u0160vab, Danica Rotar, et al.",
                "venue": "Archives of general psychiatry, 65(12):1368\u20131376, 2008.",
                "url": null
            }
        },
        {
            "29": {
                "title": "Predicting depression via social media.",
                "author": "Munmun De Choudhury, Michael Gamon, Scott Counts, and Eric Horvitz.",
                "venue": "In Proc. of AAAI \u201913, volume 7, pages 128\u2013137, 2013.",
                "url": null
            }
        },
        {
            "30": {
                "title": "Role of social media in tackling challenges in mental health.",
                "author": "Munmun De Choudhury.",
                "venue": "In Proc. of SAM \u201913, pages 49\u201352, 2013.",
                "url": null
            }
        },
        {
            "31": {
                "title": "Social media for mental illness risk assessment, prevention and support.",
                "author": "Munmun De Choudhury.",
                "venue": "In Proc. of SIdEWayS \u201815, page 1, 2015.",
                "url": null
            }
        },
        {
            "32": {
                "title": "Detecting and characterizing mental health related self-disclosure in social media.",
                "author": "Sairam Balani and Munmun De Choudhury.",
                "venue": "In Proc. of CHI EA \u201915, pages 1373\u20131378, 2015.",
                "url": null
            }
        },
        {
            "33": {
                "title": "Depression detection by analyzing social media posts of user.",
                "author": "Nafiz Al Asad, Md Appel Mahmud Pranto, Sadia Afreen, and Md Maynul Islam.",
                "venue": "In Proc. of SPICSCON \u201919, pages 13\u201317. IEEE, 2019.",
                "url": null
            }
        },
        {
            "34": {
                "title": "Identifying depression on reddit: The effect of training data.",
                "author": "Inna Pirina and \u00c7a\u011fr\u0131 \u00c7\u00f6ltekin.",
                "venue": "In Proc. of the 2018 EMNLP Workshop SMM4H: The 3rd Social Media Mining for Health Applications Workshop & Shared Task, pages 9\u201312, 2018.",
                "url": null
            }
        },
        {
            "35": {
                "title": "Detection of depression-related posts in reddit social media forum.",
                "author": "Michael M Tadesse, Hongfei Lin, Bo Xu, and Liang Yang.",
                "venue": "IEEE Access, 7:44883\u201344893, 2019.",
                "url": null
            }
        },
        {
            "36": {
                "title": "Automatic depression detection via learning and fusing features from visual cues.",
                "author": "Yanrong Guo, Chenyang Zhu, Shijie Hao, and Richang Hong.",
                "venue": "IEEE Transactions on Computational Social Systems, 2022.",
                "url": null
            }
        },
        {
            "37": {
                "title": "Facial expression video analysis for depression detection in chinese patients.",
                "author": "Qingxiang Wang, Huanxin Yang, and Yanhong Yu.",
                "venue": "Journal of Visual Communication and Image Representation, 57:228\u2013233, 2018.",
                "url": null
            }
        },
        {
            "38": {
                "title": "Sequential fusion of facial appearance and dynamics for depression recognition.",
                "author": "Qian Chen, Iti Chaturvedi, Shaoxiong Ji, and Erik Cambria.",
                "venue": "Pattern Recognition Letters, 150:115\u2013121, 2021.",
                "url": null
            }
        },
        {
            "39": {
                "title": "ultralytics/yolov5: v3.1 - Bug Fixes and Performance Improvements.",
                "author": "Glenn Jocher, Alex Stoken, Jirka Borovec, NanoCode012, ChristopherSTAN, Liu Changyu, Laughing, tkianai, Adam Hogan, lorenzomammana, yxNONG, AlexWang1900, Laurentiu Diaconu, Marc, wanghaoyang0106, ml5ah, Doug, Francisco Ingham, Frederik, Guilhen, Hatovix, Jake Poznanski, Jiacong Fang, Lijun Yu \u4e8e\u529b\u519b, changyu98, Mingyu Wang, Naman Gupta, Osama Akhtar, PetrDvoracek, and Prashant Rai.",
                "venue": "https://doi.org/10.5281/zenodo.4154370, October 2020.",
                "url": null
            }
        },
        {
            "40": {
                "title": "Semi-supervised classification with graph convolutional networks, 2016.",
                "author": "Thomas N Kipf and Max Welling.",
                "venue": null,
                "url": null
            }
        },
        {
            "41": {
                "title": "Inductive representation learning on large graphs.",
                "author": "Will Hamilton, Zhitao Ying, and Jure Leskovec.",
                "venue": "Advances in neural information processing systems, 30, 2017.",
                "url": null
            }
        },
        {
            "42": {
                "title": "Graph attention networks, 2018.",
                "author": "Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio.",
                "venue": null,
                "url": null
            }
        },
        {
            "43": {
                "title": "Deep residual learning for image recognition.",
                "author": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.",
                "venue": "In Proc. of CVPR \u201916, pages 770\u2013778, 2016.",
                "url": null
            }
        },
        {
            "44": {
                "title": "Kobert.",
                "author": "SKTBrain.",
                "venue": "https://github.com/SKTBrain/KoBERT, 2023.",
                "url": null
            }
        },
        {
            "45": {
                "title": "How multilingual is multilingual BERT?",
                "author": "Telmo Pires, Eva Schlinger, and Dan Garrette.",
                "venue": "In Proc. of ACL \u201919, Florence, Italy, July 2019. Association for Computational Linguistics.",
                "url": null
            }
        },
        {
            "46": {
                "title": "Cross-lingual language model pretraining, 2019.",
                "author": "Guillaume Lample and Alexis Conneau.",
                "venue": null,
                "url": null
            }
        },
        {
            "47": {
                "title": "Attention is all you need.",
                "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin.",
                "venue": "Advances in neural information processing systems, 30, 2017.",
                "url": null
            }
        },
        {
            "48": {
                "title": "Microsoft coco: Common objects in context.",
                "author": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick.",
                "venue": "In Proc. of ECCV \u201914, pages 740\u2013755. Springer, 2014.",
                "url": null
            }
        },
        {
            "49": {
                "title": "Adam: A method for stochastic optimization, 2014.",
                "author": "Diederik P Kingma and Jimmy Ba.",
                "venue": null,
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.15485v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.2.1",
            "3.2.2",
            "3.2.3",
            "3.2.4"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4",
            "4.5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ]
    },
    "research_context": {
        "paper_id": "2403.15485v1",
        "paper_title": "MOGAM: A Multimodal Object-oriented Graph Attention Model for Depression Detection",
        "research_background": "### Motivation:\nThe primary motivation for this paper is the growing concern regarding depression, a severe mental illness that affects about 5% of adults globally, as reported by the World Health Organization. The issue has been further intensified by the COVID-19 pandemic, which has led to a significant increase in the prevalence of depression. Traditional methods of diagnosing depression via face-to-face interviews have been notably challenged during the pandemic, necessitating alternative approaches for effective detection. Given the interactive nature and widespread use of social media, the authors are motivated to explore this platform as a means for diagnosing depression. \n\n### Research Problem:\nThe central research problem addressed by this paper revolves around the development and evaluation of a novel method for detecting depression through vlogs on social media platforms like YouTube. Specifically, the paper seeks to answer whether it is possible to accurately detect depression and high-risk depression in vlogs using a proposed multimodal object-oriented graph attention model (MOGAM). By leveraging object co-occurrence networks and integrating features from different modalities, the study aims to overcome challenges associated with traditional diagnostic methods and improve the robustness and applicability of depression detection models.\n\n### Relevant Prior Work:\n1. **Traditional Diagnostic Tools:**\n   - Screening instruments (CES-D, PHQ-9) for face-to-face interviews (Radloff, 1977; Kroenke et al., 2001).\n\n2. **Challenges Due to COVID-19:**\n   - Restrictions on face-to-face interviews during the pandemic (Self et al., 2021).\n\n3. **Social Media as a Diagnostic Tool:**\n   - Using social media for psychological state analysis (Lin et al., 2020; Liu et al., 2021) including natural language processing (NLP) and computer vision (CV) techniques on platforms like Twitter (Orabi et al., 2018; Cha et al., 2022) and Reddit (Ren et al., 2021).\n\n4. **Multimodal Analysis:**\n   - Exploring both text and visual content via CV on platforms like Instagram (Maxim et al., 2020) and YouTube (Yoon et al., 2022).\n\n5. **Video Blogs (Vlogs):**\n   - Utilization of vlogs for sharing experiences and states, particularly among individuals with illnesses (Freeman and Chapman, 2007; Snelson, 2015; Huh et al., 2014).\n\n6. **Clinical Interview Videos:**\n   - Prior studies on depression detection using clinical interview videos focusing on facial expressions, acoustic signals, and body movements (Gratch et al., 2014; Girard et al., 2013; Ray et al., 2019; Joshi et al., 2013).\n\nThe paper leverages a synthesis of these past approaches and introduces MOGAM to address existing limitations by decoding object interactions within vlogs and incorporating them within a graph-based model for a more flexible and generalized detection method.",
        "methodology": "Sure, based on the methodology section provided, here's a detailed description that retains as much of the original wording and phrases as possible:\n\n---\n\n**Methodology:**\n\nIn this section, we introduce the construction process of our vlog dataset and the classification method. Specifically, we cover two key components:\n\n1. **The Procedure for Collecting and Preprocessing YouTube Vlogs:**\n   - This step involves gathering a large amount of YouTube vlogs relevant to our study. These vlogs are sourced based on certain criteria that ensure they carry potential indicators of depression. Following the collection process, each vlog undergoes a thorough preprocessing phase. This involves tasks such as trimming, noise reduction, and possibly anonymization to preserve the privacy of vlog creators. The prepared dataset then serves as input for subsequent stages of the analysis.\n\n2. **The Introduction of Our Multimodal Object-Oriented Graph Attention Model: MOGAM:**\n   - The core of our methodology is the development and implementation of the MOGAM. This novel model integrates multiple data modalities, leveraging the rich, multifaceted information present in the vlogs. Specifically, it constructs an object-oriented graph where nodes represent different entities such as people, their expressions, and various objects within the vlog scenes. The relationships between these entities are captured as edges.\n   \n   - MOGAM employs an attention mechanism to weigh the importance of different nodes and edges dynamically. This allows the model to focus on the most relevant aspects of the vlogs that are indicative of depressive behavior. The graph attention layers facilitate the aggregation of information from different parts of the vlog, enabling the model to form a comprehensive understanding of the depicted scenes and interactions.\n\nBy combining these components, MOGAM effectively capitalizes on the multimodal nature of vlogs, utilizing visual, auditory, and textual cues to enhance the detection of depression.\n\n---\n\nThis description outlines the procedure for data collection and preprocessing, as well as the innovative aspects of the MOGAM, including its multimodal approach and graph attention mechanism.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Experiment Setup:\n1. **Conditional Experiments**: The primary focus was on three key comparisons:\n   - Daily versus Depression\n   - Daily versus High-risk Depression\n   - Scalability Evaluation \n\n2. **Multimodal and Unimodal Approaches**: Experiments were carried out using both multimodal and unimodal approaches to assess the impact of incorporating various features on model performance.\n\n3. **Hardware and Software**: Experiments were conducted on a single NVIDIA RTX A6000 48GB GPU using Python 3.7. The Yolo v5 model, pre-trained on the COCO dataset (Lin et al., 2014), was employed.\n\n4. **Dataset Splits**: Both datasets were divided into training, validation, and test sets in an 8:1:1 ratio, repeated five times. Specific numbers for each split are provided in the appendix.\n\n5. **Optimization and Model Parameters**: The Adam optimizer (Kingma and Ba, 2014) was utilized, with batch size set to 32, epochs to 500, and hidden dimensions to 1024.\n\n#### Datasets:\n- Utilized datasets are divided into training, validation, and test sets in an 8:1:1 ratio.\n- Exact dataset quantities for train, validation, and test sets are detailed in the appendix.\n\n#### Baselines:\n- Unimodal approaches.\n- Multimodal approaches involving object detection and other features.\n\n#### Evaluation Metrics:\n- Specific evaluation metrics used for performance measurement have not been explicitly mentioned in the provided text.\n\n#### Main Experimental Results:\n- Specific results for the three conditional experiments (Daily vs. Depression, Daily vs. High-risk Depression, and Scalability Evaluation) were not provided in the outlined text.\n"
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the performance of the proposed MOGAM model on distinguishing between daily vlogs and depression vlogs.",
            "experiment_process": "The performance of MOGAM was compared against baseline models on a dataset consisting of daily and depression vlogs, with a specific focus on the use of multimodal features. The experiments were conducted using a single NVIDIA RTX A6000 48GB GPU with Python 3.7, utilizing the Yolo v5 model pre-trained on the COCO dataset. Datasets were split into train, validation, and test sets in an 8:1:1 proportion across five iterations. The Adam optimizer was used, with batch size, epochs, and hidden dimensions set to 32, 500, and 1024, respectively.",
            "result_discussion": "The MOGAM model with GAT achieved the highest F1-score among all baselines, indicating the effectiveness of the cross-attention mechanism for learning object relationships. Multimodal feature integration with object-based graph features demonstrated superior performance over baselines, suggesting a robust framework for identifying depression in vlogs.",
            "ablation_id": "2403.15485v1.No1"
        },
        {
            "research_objective": "To capture and distinguish symptoms and patterns of high-risk depression prior to clinical diagnosis using the MOGAM model.",
            "experiment_process": "Models were trained on datasets consisting of daily and high-risk depression vlogs. The experimental setup and data structure mirrored those used in the previous study (daily vs. depression vlogs), emphasizing the impact of multimodal features.",
            "result_discussion": "MOGAM successfully enabled models to differentiate between daily and high-risk depression vlogs, achieving high discrimination accuracy (F1-score of 0.997). This indicates the model's robustness in identifying high-risk depression states.",
            "ablation_id": "2403.15485v1.No2"
        },
        {
            "research_objective": "To evaluate the capability of the MOGAM model in a multiclass classification task, distinguishing between daily, depression, and high-risk potential states.",
            "experiment_process": "Within the same experimental environment, models were trained and tested on datasets representing daily, depression, and high-risk potential states. Performance metrics (F1-scores) were used to compare results across three groups: daily, depression, and high-risk potential.",
            "result_discussion": "GraphSAGE combined with MOGAM excelled in the daily and high-risk potential states, while the GAT model with MOGAM performed best for the depression state. Models without MOGAM struggled to distinguish high-risk states, misclassifying them frequently. MOGAM, however, facilitated comprehensive classification across all states, demonstrating its suitability for the multiclass context, especially in imbalanced data environments.",
            "ablation_id": "2403.15485v1.No3"
        },
        {
            "research_objective": "To test the scalability of MOGAM on a benchmark dataset (D-Vlog) previously used for depression detection research.",
            "experiment_process": "The D-Vlog dataset, comprising 555 depression and 406 non-depression vlogs, was used to evaluate scalability. Object-based graph, visual, and metadata features were extracted using the same procedures as earlier experiments. The models, trained on the initial vlog dataset incorporating both baseline and multimodal features, were applied to the D-Vlog data.",
            "result_discussion": "The GAT-based model trained with multimodal features on the D-Vlog dataset achieved an F1-score of 0.612, which is comparable to prior research results (0.635). This confirms the scalability and robustness of the MOGAM model across different datasets.",
            "ablation_id": "2403.15485v1.No4"
        }
    ]
}