{
    "title": "Your Transformer is Secretly Linear",
    "abstract": "This paper reveals a novel linear characteristic exclusive to transformer decoders, including models such as GPT, LLaMA, OPT, BLOOM and others. We analyze embedding transformations between sequential layers, uncovering a near-perfect linear relationship (Procrustes similarity score of 0.99). However, linearity decreases when the residual component is removed due to a consistently low output norm of the transformer layer. Our experiments show that removing or linearly approximating some of the most linear blocks of transformers does not affect significantly the loss or model performance. Moreover, in our pretraining experiments on smaller models we introduce a cosine-similarity-based regularization, aimed at reducing layer linearity. This regularization improves performance metrics on benchmarks like CIFAR-10 and SuperGLUE and as well successfully decreases the linearity of the models. This study challenges the existing understanding of transformer architectures, suggesting that their operation may be more linear than previously assumed.111https://github.com/AIRI-Institute/LLM-Microscope",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Transformers have revolutionized the field of natural language processing, offering unprecedented advances in a wide range of applications Islam et al. (2023  ###reference_b15###). However, despite their widespread adoption and success, the complex work of these models remains an area of active research Lin et al. (2021  ###reference_b18###). One aspect that has received less attention is the inherent linearity of intermediate embedding transformations within these architectures. In this study, we embark on an in-depth analysis of the linearity properties of transformers, specifically focusing on decoders, and explore its implications during the pretraining and fine-tuning phases.\nOur investigation reveals a surprising discovery: the embedding transformations between sequential layers in transformer decoders exhibit almost linear properties. This observation is quantified using Procrustes similarity analysis, demonstrating a near-perfect linearity score of 0.99. Such a discovery not only challenges the traditional understanding of transformer architectures but also opens new opportunities for model optimization and efficiency.\nBased on this insight, we introduce several new contributions to the field:\nExtensive analysis of the linearity properties of transformer decoders and its dynamics at the pretraining and fine-tuning stages.\nThe development of new algorithms for depth pruning of transformer decoders, allowing to remove the most linear layers without a significant loss in performance.\nA novel distillation technique that involves pruning, replacing certain layers with linear approximations, and then distilling layer-wise embeddings to preserve model performance.\nIntroducing a new regularization approach for pretraining based on the cosine similarity, designed to decrease the layer linearity. This method not only enhances the performance of transformer models on benchmark datasets such as SuperGLUE and CIFAR-10 Eldan and Li (2023  ###reference_b9###), but also improves the expressiveness of embeddings, as evidenced by linear probing tasks.\nWith our findings, we are paving the way for more computationally efficient transformer architectures without sacrificing their effectiveness, thereby addressing one of the critical challenges in deploying these models.\n###figure_1###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Research on evaluating and leveraging sparsity for model pruning has become one of the most significant topics within the machine learning community. Molchanov et al. (2016  ###reference_b20###) explored the sparsity of convolutional neural networks through backpropagation and fine-tuning, laying the groundwork for understanding the potential applications of sparsity in resource-efficient inference. The verification approach utilized in a more recent DejaVu Borse et al. (2023  ###reference_b6###) paper is based on Molchanov\u2019s research.\nPrevious work Kurtic et al. (2023  ###reference_b16###) has addressed the challenges associated with naive sparse fine-tuning in the context of LLMs. Issues such as training instability, poor recovery, and overfitting have prompted an exploration for alternative approaches. The study introduced SquareHead distillation, a method that consistently addresses the challenges in naive sparse fine-tuning, demonstrating accurate recovery even at high sparsity levels.\nIn a more recent study WANDA Sun et al. (2023  ###reference_b25###), the authors present a technique for pruning LLMs to high degrees of sparsity without modifying the remaining weights. Unlike SparseGPT Frantar and Alistarh (2023  ###reference_b12###), WANDA seamlessly implements pruning in a single forward pass, leveraging feature norm statistics for efficient pruning. This method achieves noticeable sparsity without the need for a sophisticated iterative weight update procedure, differentiating itself from other pruning techniques.\nContextual sparsity introduced by Borse et al. (2023  ###reference_b6###) involves sparsifying MLP and attention blocks in LLMs to reduce generation latency. The study identifies essential attention heads and MLP neurons for computation, maintaining performance across in-context learning and language modeling tasks.\nRecent work by Ashkboos et al. (2024  ###reference_b3###) shows that LLMs can be sparsified post hoc. Their approach introduces a scheme to replace each weight matrix with a smaller dense matrix, thereby reducing the dimensionality of the networks. Their results show that models of different sizes can be reduced with varying degrees of success. For example, LLAMA-2 70B and OPT 66B can maintain 99% zero-shot accuracy while reducing 25% of the parameters reduced while performing LLM evaluation tasks. In contrast, the smaller Phi-2 is more sensitive to pruning, experiencing a 10% drop compared to its dense version.\nThe inner structure of transformer models has captured significant attention among researchers Nostalgebraist (2020  ###reference_b21###); Xu et al. (2021  ###reference_b27###); Belrose et al. (2023  ###reference_b5###); Din et al. (2023  ###reference_b8###). Primarily, in \u201clogit lens\u201d Nostalgebraist (2020  ###reference_b21###) and subsequently in Belrose et al. (2023  ###reference_b5###), the authors have focused on analyzing how hidden representations evolve across different layers of transformer architecture, aiming to elucidate their impact on final model outputs. Complementing these findings, the Anthropic team\u2019s research into small transformer-based models Elhage et al. (2021  ###reference_b11###) uncovers a profound linear structure inherent in this architecture. Their work demonstrates the effectiveness of decomposing operations into individual sum components and multiplying chains of matrices, thus highlighting the linear complexity within these sophisticated neural architectures.\nTopological features that analyze the structure of inner embeddings in transformer-based models are also useful in LLM pruning and distillation.\nPrevious research examined the intrinsic dimensionality of neural networks to evaluate their capacity and effectiveness in the fine-tuning process Ansuini et al. (2019  ###reference_b2###); Aghajanyan et al. (2020  ###reference_b1###); Razzhigaev et al. (2023  ###reference_b23###). Decoder-based models are shown to achieve a high level of anisotropy, especially in their middle layers, and have low intrinsic dimensionality Razzhigaev et al. (2023  ###reference_b23###). Recent popular approaches include low-rank approximation, which replaces or adjusts the weight matrix with the product of two matrices with a smaller inner dimension. This approach typically requires a fine-tuning procedure that adjusts the matrix representations. For example, LoRA Hu et al. (2021  ###reference_b14###) was inspired by the previous work Aghajanyan et al. (2020  ###reference_b1###) showing that neural networks can be successively trained in lower-dimensional subspaces. The research also shows that there it is not necessary to update millions of parameters on small fine-tuning datasets. Our results are on par with the results of this research, showing that via fine-tuning, the linearization of models grows steadily.\nThe Bonsai model Dery et al. (2024  ###reference_b7###) tends to prune the LLMs relying only on the inference step, while they achieve performance comparable to half-sized semistructured sparsity with WANDA 2:4 and outperforms the LLM-Pruner Ma et al. (2023  ###reference_b19###) and LoRAPrune Zhang et al. (2023  ###reference_b28###) on 4 out of 6 evaluation settings in the experiments conducted.\nIn this paper, we investigate several techniques for pruning LLMs, leveraging the linearity of the decoder-based layers. Our techniques offer efficient yet lightweight methods, maintaining high model performance on the evaluated benchmarks."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Analysis of Pretrained Architectures",
            "text": "###figure_2### ###figure_3### In our study of the embedding properties of various layers of transformer decoders, we focus on understanding the degree of linearity and smoothness of transformations between sequential layers."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Linearity Score",
            "text": "To determine the degree of linear dependence of two sets of vectors, we used a metric obtained by generalizing the Procrustes similarity Gower (1975  ###reference_b13###) to the case of arbitrary linear transformations.\nLet  represent the centered sets of embeddings, to calculate linearity score we use normalized matrices ,  (where  denotes the Frobenius norm of the matrix) and defined\nThis is almost the same formula as in Procrustes similarity, the only difference is that, instead of considering the minimum among orthogonal transformations, we use the minimum among all linear transformations to find the optimal mapping in terms of squared errors.\nWe chose such approach for its robustness in evaluating the linearity of embeddings, especially considering the scale variance across transformer layers. Unlike  norm, which lacks scale invariance, Procrustes normalization offers a bounded metric in the range [0,1].\nSurprisingly, the linearity scores of layers in all tested transformer decoders were found to be close to 1, indicating a high degree of linearity in embedding transformations (Figure 1  ###reference_###).\nThis phenomenon can be partly explained by the observation that the norm of each block\u2019s contribution to the residual stream is remarkably low (Figure 3  ###reference_###). Moreover, when assessing the linearity of the main stream (embeddings w/o residual component) by subtracting the embedding values of each layer from the previous layer, one can notice that the degree of linearity significantly decreases (Figure 1  ###reference_###). This suggests that the inherent linearity is not as straightforward as it is initially estimated. Moreover, the low norm contribution of individual blocks resulted in embeddings from adjacent layers being closely aligned in terms of cosine similarity.\nOne more insight is that the combination of seemingly linear blocks can lead to non-linear outcomes. Elhage et al. (2022  ###reference_b10###) suggests that complex features can be encoded across components of neural networks, applicable to attention heads in transformers. This indicates that the cumulative effect of linear transformations might enable the encoding of intricate non-linear representations.\nFurthermore, our feature triggering regime hypothesis proposes that rare specific features on a few tokens with high non-linearity significantly influence model behavior \u2014 in the Figure 9  ###reference_### one can see that some layers of OPT-1.3B have the long tailed distribution of  errors, which means that there are still sparse spikes of non-linearity.\nBorse et al. (2023  ###reference_b6###) explored how a sparse subset of model parameters can be dynamically activated for efficient inference, supporting the idea that within predominantly linear architectures, certain non-linear interactions are crucial for model functionality."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Linearity Dynamics at Pretraining and Fine-tuning",
            "text": "Our exploration extends to examining the linearity dynamics of both open-source models with publicly available intermediate checkpoints and our custom models trained on small datasets. Through this analysis, we aimed to understand the dynamics of linearity, especially in the main stream (contextualized embeddings including the residual component), across different stages of model training.\nAs illustrated in the Figure 2  ###reference_###, the analysis reveals a notable trend: as the models undergo pretraining, the linearity of the main stream gradually decreases on average. This phenomenon is consistently observed in all models examined, indicating a fundamental aspect of transformer-decoder learning dynamics.\nIn our analysis of the fine-tuning phase across diverse tasks, including those in the SuperGLUE benchmark Wang et al. (2019  ###reference_b26###) and the reward-modeling task on the Anthropic-Helpful dataset Bai et al. (2022  ###reference_b4###), we notice an interesting change. Contrary to the decreasing trend of linearity observed during the pretraining phase, all models under study show an increase in linearity during fine-tuning. This finding indicates that task-specific fine-tuning tends to reinforce and amplify the linear characteristics of transformer models, as shown in Table 1  ###reference_###.\nIn fine-tuning, we train models on three NLI tasks from the SuperGLUE benchmark: MultiRC, BoolQ, and CB, treating them as binary text classification challenges. In the BoolQ task, for instance, we combine the question and the passage into a single text, marking them with \"question:\" and \"passage:\" respectively, and consider the binary answer as the classification label.\nReward models trained on text pairs with contrastive loss Ouyang et al. (2022  ###reference_b22###) demonstrate a similar trend in linearity scores, proving even more stability across different seed values."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Improving Linearity with Regularized Pretraining",
            "text": "Aiming to understand the impact of linearity on transformer models, we embark on pretraining experiments using the Mistral architecture with model sizes of 150M, and 650M. These models are pretrained on carefully selected clean datasets, CIFAR-10, chosen for its diverse and rich content, which has been proven to be suitable for fast training of the small models and architecture experiments. We introduce specific loss terms to adjust the relations between embeddings within transformer layers:  \nMSE regularization term: Experimentation with mean squared error (MSE) loss between embeddings of consecutive layers, designed to minimize the distance between these embeddings, thereby promoting consistency across the layers.  \nCosine Similarity regularization term: The application of a cosine-based regularization that encourages contextualized embeddings from sequential layers to align closer to each other, effectively reducing their angular difference to zero on average.  \nThe most promising results are achieved using a cosine-based approach that encourages the embeddings of sequential layers to converge, effectively making the cosine similarity between them closer to 1 on average. This method shows significant perspectives in the enhancing model performance. We evaluate the effectiveness of our approach through validation using GPT-4 on CIFAR-10 prompts according to the methodology, linear probing techniques, and evaluation on SuperGLUE benchmarks. The results are presented in the Table 2 and Table 3. As it can be seen in the Figure 5, linearity scores are lower at each layer of the model after pretraining with such regularization.  \nTo further assess the expressiveness of embeddings across different layers, we conducted linear probing on outputs of all the layers of the Mistral-650M model, both pretrained with and without cosine regularization, on the xstorycloze-en task from SuperGLUE. The results clearly indicate that embeddings from the model pretrained with regularization exhibit better performance compared to those from the standard model.  \nThis contradictory outcome, where the term appears to draw embeddings from neighbouring layers closer together, making them more similar in terms of cosine similarity, has prompted a deeper investigation. Our observations suggest that as embeddings become more similar across layers, the model may compensate for the reduction in variability by amplifying non-linear processing capabilities in the residual stream. Although this hypothesis requires further exploration, it offers a fascinating insight into the adaptive mechanisms of transformer models in response to altered internal dynamics."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Exploiting Linearity for Pruning",
            "text": "Leveraging the inherent linearity of transformer layers, we explore a pruning strategy that sequentially removes the most linear layers. This approach allows you to reduce the size of the model slightly by removing just a few layers without significantly compromising performance. Further enhancement of this strategy involves replacing the pruned layers with linear approximation and incorporating a distillation loss (specifically MSE layerwise) to minimize performance degradation. The training focuses on these linear replacements, fine-tuning them to effectively mimic the original layers\u2019 function. The effectiveness and the impact of these methods are detailed in the Figure 8  ###reference_###. We use CIFAR-10 for linear approximation and distillation training stage. As it can be seen in the Figure 7  ###reference_###, perplexity is less affected by pruning with linear replacements and following distillation compared to just removing transformer layers.\n###figure_6### ###figure_7### ###figure_8###"
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In our study we provide an in-depth exploration of linearity within transformer decoders, revealing their inherent near-linear behavior in various models. We discover that while pretraining tends to increase nonlinearity within layers, fine-tuning on specific tasks can paradoxically reduce it. We propose new pruning and distillation techniques inspired by previous observations, demonstrating that it is possible to refine and optimize transformer models without compromising their performance. The suggested cosine-based regularization approach during pretraining further contributes to model efficiency and performance on benchmarks such as SuperGLUE and CIFAR-10, while reducing the linearity of its layers (w/o residual components). Our study highlights the significant relationship between linearity and performance of transformer decoders, offering strategic guidance for future developments in the efficiency and flexibility of these models."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "Despite the promising advancements presented in this study, it is essential to acknowledge its limitations. Firstly, our analysis predominantly focuses on transformer decoders, thus the generalizability of our findings to encoder-only or encoder-decoder architectures may be limited.\nSecondly, the depth pruning and distillation techniques, while being effective in our experiments, were evaluated within a specific set of conditions and models. The scalability of these methods to larger, more complex models or different domains is yet to be fully ascertained.\nMoreover, the new regularization approach aimed at pretraining demonstrates potential, yet its effectiveness across a broader spectrum of tasks requires further validation."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Ethics Statement",
            "text": "We are committed to ethical principles for AI research, focusing on transparency and responsible experimentation. Our research, while suggesting efficiency improvements, prompts consideration of implications such as privacy and fairness."
        }
    ],
    "url": "http://arxiv.org/html/2405.12250v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "4",
            "5"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "3.1",
            "3.2",
            "4",
            "5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "1",
            "4",
            "5"
        ]
    },
    "research_context": {
        "paper_id": "2405.12250v1",
        "paper_title": "Your Transformer is Secretly Linear",
        "research_background": "### Motivation:\nThe motivation for this paper stems from the transformative impact of transformers on natural language processing, as evidenced by their success in a wide array of applications (Islam et al., 2023). Despite the rapid adoption and significant achievements of these models, their complex internal workings remain largely enigmatic and are a topic of ongoing investigation (Lin et al., 2021). The aspect of intermediate embedding transformations' linearity within these architectures has been underexplored, prompting this study to delve deeply into this facet, particularly focusing on transformer decoders. \n\n### Research Problem:\nThe primary research problem addressed in this paper is the unexpected almost linear nature of the embedding transformations between sequential layers in transformer decoders and its implications during the pretraining and fine-tuning phases of these models. The study aims to quantify this linearity and explore how it can be leveraged for model optimization and efficiency.\n\n### Relevant Prior Work:\n1. **Transformative Impact of Transformers:** Previous research by Islam et al. (2023) has highlighted the revolutionary role of transformers in natural language processing, underscoring their extensive applications and success.\n2. **Complexity and Functioning of Transformers:** The complex operations and inner workings of transformers, which remain an active area of research, have been discussed by Lin et al. (2021). Their work explores the intricate dynamics within these models.\n3. **Benchmark Datasets for Validation:** The performance assessment and effectiveness of proposed methods in transformers often rely on benchmark datasets such as SuperGLUE and TinyStories, as referenced by Eldan and Li (2023).\n\nBy integrating the insights from these prior studies, the current paper seeks to challenge conventional notions about transformer architectures and contribute novel methods for enhancing their computational efficiency and performance.",
        "methodology": "The proposed methodology investigates the influence of linearity in transformer models by focusing on the Mistral architecture with model sizes of 150M and 650M parameters. The researchers conduct pretraining experiments using datasets known for their effective and fast training characteristics: TinyStories and Tiny-textbooks. They introduce two specific loss regularization terms to enhance the model's embedding relations across transformer layers: \n\n1. **MSE Regularization Term**: This term involves a mean squared error (MSE) loss between embeddings of consecutive layers. The objective is to minimize the distance between these embeddings, thereby promoting consistency and minimizing the variability across the layers.\n\n2. **Cosine Similarity Regularization Term**: This term uses a cosine-based regularization approach that encourages contextualized embeddings from sequential layers to align closer to each other. Ideally, this reduces their angular difference to zero, making the cosine similarity between them closer to 1 on average. This method has shown significant promise in enhancing model performance, leading to better results.\n\nThe efficiency and effectiveness of these methods are validated through multiple evaluations, including:\n- **GPT-4 Validation on TinyStories Prompts**: Following a methodology similar to Eldan and Li (2023).\n- **Linear Probing Techniques**: Employing linear probing on the outputs of all layers of the pretrained models.\n- **SuperGLUE Benchmarks**: Assessing performance using metrics from tasks in the SuperGLUE benchmark suite.\n\nA notable observation in the results is the contradictory outcome from the cosine-based regularization approach. Although this term draws embeddings from neighboring layers closer in terms of cosine similarity, making them more similar, it seems to enhance the model's performance. This phenomenon has led to the hypothesis that the model compensates for the reduced variability in embeddings by amplifying its non-linear processing capabilities in the residual stream. While this hypothesis offers intriguing insights into the adaptive mechanisms of transformer models, it necessitates further investigation for conclusive understanding.\n\nIn summary, the proposed methods aim to reduce the variability across consecutive transformer layers' embeddings by employing MSE and cosine similarity regularization terms, which consequentially enhance the model's performance as evidenced by comprehensive evaluations.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\nIn the study \"Your Transformer is Secretly Linear\", the authors aim to understand the embedding properties of various layers of transformer decoders, specifically focusing on the degree of linearity and smoothness of transformations between sequential layers.\n\n**Datasets:**\n- The specific datasets used for this experiment are not mentioned in the provided excerpt. Typically, such studies utilize standard benchmarks such as WMT (for machine translation tasks), OpenWebText, or other large-scale text corpora for language modeling.\n\n**Baselines:**\n- As the text focuses on the inherent properties of the transformer layers, it's plausible that the baselines might include comparisons with standard transformer architectures, potentially against variations with different hyperparameters or architectures such as LSTMs or simpler feed-forward networks. Details on specific baselines are not provided in the excerpt.\n\n**Evaluation Metrics:**\n- The primary metric for evaluation is likely related to quantifying linearity and smoothness between layers. This could include measures like the linear correlation coefficient, smoothness metrics, or other statistical measures that capture the degree of transformation properties.\n\n**Main Experimental Results:**\n- The experimental results, as suggested by the study's title and emphasis, reveal insights into the inherent properties of transformer layers. While detailed numeric results are not provided, the main finding likely suggests that transformers exhibit a degree of linearity in their transformations between sequential layers. This insight contributes to the understanding of how transformers process and encode information, potentially leading to implications for improving model interpretability and efficiency.\n\nIn conclusion, the main experiment demonstrates that there are underlying linear properties in transformer layers, offering a new perspective on the internal workings of these models. The experiment does not seem to include ablation studies which are typically designed to evaluate specific module contributions; instead, it provides a broader analysis of the transformer's inherent attributes."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Understand the impact of linearity on transformer models and improve model performance by introducing regularization methods during pretraining.",
            "experiment_process": "The experiments are performed using the Mistral architecture with model sizes of 150M and 650M. These models are pretrained on clean datasets, TinyStories and Tiny-textbooks. Two specific loss terms are introduced to adjust the relations between embeddings within transformer layers: 1) MSE regularization term, minimizing the distance between embeddings of consecutive layers; 2) Cosine Similarity regularization term, aligning contextualized embeddings from sequential layers to reduce their angular difference. The method's effectiveness is validated using GPT-4 on TinyStories prompts, linear probing techniques, and evaluation on SuperGLUE benchmarks. The results are presented in various figures and tables in the paper.",
            "result_discussion": "The most promising results are achieved using a cosine-based approach that encourages the embeddings of sequential layers to converge, making the cosine similarity between them closer to 1 on average. This method significantly enhances model performance. Validation results show that embeddings from the model pretrained with regularization exhibit better performance compared to the standard model. The contradictory outcome, where embeddings become more similar across layers, suggests that the model may compensate for reduced variability by amplifying non-linear processing capabilities in the residual stream.",
            "ablation_id": "2405.12250v1.No1"
        },
        {
            "research_objective": "Leverage the inherent linearity of transformer layers to develop a pruning strategy that reduces model size without significantly compromising performance.",
            "experiment_process": "Exploration of a pruning strategy involves sequentially removing the most linear layers without significantly compromising performance. An enhancement involves replacing pruned layers with linear approximations and incorporating a distillation loss (MSE layerwise) to minimize performance degradation. The training focuses on fine-tuning these linear replacements to mimic the original layers' functions. TinyStories dataset is used for the linear approximation and distillation training stage. The effectiveness and impact are detailed in various figures within the paper, showing the perplexity differences with different pruning strategies.",
            "result_discussion": "The results indicate that perplexity is less affected by pruning with linear replacements followed by distillation compared to just removing transformer layers. This suggests that the introduced methods successfully maintain model performance while taking advantage of the transformer layers' inherent linearity.",
            "ablation_id": "2405.12250v1.No2"
        }
    ]
}