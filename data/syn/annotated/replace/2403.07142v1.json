{
    "title": "One Category One Prompt: Dataset Distillation using Diffusion Models",
    "abstract": "The extensive amounts of data required for training deep neural networks pose significant challenges on storage and transmission fronts. Dataset distillation has emerged as a promising technique to condense the information of massive datasets into a much smaller yet representative set of synthetic samples. However, traditional dataset distillation approaches often struggle to scale effectively with high-resolution images and more complex architectures due to the limitations in bi-level optimization. Recently, several works have proposed exploiting knowledge distillation with decoupled optimization schemes to scale up dataset distillation. Although these methods effectively address the scalability issue, they rely on extensive image augmentations requiring the storage of soft labels for augmented images. In this paper, we introduce Dataset Distillation using Diffusion Models (D3M) as a novel paradigm for dataset distillation, leveraging recent advancements in generative text-to-image foundation models. Our approach utilizes textual inversion, a technique for fine-tuning text-to-image generative models, to create concise and informative representations for large datasets. By employing these learned text prompts, we can efficiently store and infer new samples for introducing data variability within a fixed memory budget. We show the effectiveness of our method through extensive experiments across various computer vision benchmark datasets with different memory budgets.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The substantial data needed for training deep neural networks [1 ###reference_b1###] imposes significant burdens on storage, transmission, and handling, impeding tasks that necessitate repeated training on these datasets, like hyperparameter optimization [2 ###reference_b2###, 3 ###reference_b3###]. Moreover, the publication of such extensive datasets gives rise to copyright and privacy concerns, further complicating their utilization. These challenges have prompted an important scientific question first posed in [4 ###reference_b4###]: How much data is encoded in a given training set? Can one construct a small subset of synthetic samples such that models trained on this subset achieve competitive performance compared to training on the entire dataset? This problem, referred to as Dataset Distillation [4 ###reference_b4###] or Dataset Condensation [5 ###reference_b5###], has since spurred a notable body of research from the community [6 ###reference_b6###, 7 ###reference_b7###, 8 ###reference_b8###, 9 ###reference_b9###, 10 ###reference_b10###, 11 ###reference_b11###, 12 ###reference_b12###, 13 ###reference_b13###, 14 ###reference_b14###].\n\nDataset distillation is often properly formalized as a bi-level optimization problem, where the inner optimization focuses on training a model on the distilled set (i.e., a small set of synthetic samples), while the outer (meta) optimization focuses on refining the distilled set [4 ###reference_b4###] to improve the result of the inner optimization. Such bi-level optimization presents significant challenges, as evaluating the outer optimization loop necessitates solving the inner optimization loop and thus requires backpropagation of errors through the entire inner training process, which is memory-intensive and computationally expensive. Many research papers are devoted to devising ways to ameliorate the challenges in this bi-level optimization. For instance, by introducing surrogate objectives for computing the meta gradients through gradient matching [5 ###reference_b5###] or training trajectory matching [15 ###reference_b15###], among others. Despite substantial efforts in this area, existing methods employing bi-level optimization often struggle to scale up to larger datasets and models [13 ###reference_b13###]. To scale up dataset distillation to the UCI Machine Learning Repository, recent works focus on decoupling the bi-level optimization into two single-level learning procedures [13 ###reference_b13###, 9 ###reference_b9###].\n\n###figure_1### Another significant challenge in dataset distillation is that the distilled synthetic dataset is often optimized with respect to a specific network architecture, limiting its ability to generalize well to other architectures [7 ###reference_b7###]. On this front, generating realistic images has emerged as a powerful method to improve the performance and generalizability of dataset distillation methods. In short, generating synthetic samples closer to the training data manifold has been shown to enhance the generalizability of the distilled datasets across diverse architectures [7 ###reference_b7###, 13 ###reference_b13###, 9 ###reference_b9###]. This can be achieved, for instance, by utilizing generative models [7 ###reference_b7###], batch statistics of pretrained models on the training dataset [13 ###reference_b13###], or simply by creating synthetic samples through collating patches from the training data [9 ###reference_b9###].\n\nIn this paper, inspired by recent advances in foundation models [19 ###reference_b19###, 20 ###reference_b20###, 18 ###reference_b18###], and following the original scientific curiosity of Wang et al. [4 ###reference_b4###], we pose the following question: how compressible is a dataset conditioned on having access to a foundation model? This is an important scientific inquiry as such compression, if possible, can significantly reduce communication and storage costs. Moreover, it is not far-fetched to consider a foundation model as a universal data converter that exists on all clients or is accessible via an API. In this work, we demonstrate that an entire category from the UCI Machine Learning Repository can be condensed into a single prompt of latent diffusion models [18 ###reference_b18###], leading to state-of-the-art dataset distillation performance.\n\nOur proposed framework, denoted as Dataset Distillation using Diffusion Models (D3M), builds on recent advances in dataset distillation [7 ###reference_b7###, 13 ###reference_b13###, 9 ###reference_b9###], latent diffusion models [18 ###reference_b18###], and textual inversion [17 ###reference_b17###], enabling unprecedented compression of UCI-scale datasets while providing competitive training performance and cross-architecture generalization.\n\nOur specific contributions in this paper are:\n- Demonstrating the potential of diffusion models [18 ###reference_b18###] for dataset distillation, showing unprecedented condensation rates.\n- Adapting textual inversion [17 ###reference"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "The current research on dataset distillation can be broadly categorized into the following groups: 1) those that formulate the problem as bi-level optimization, 2) those that simplify the bi-level optimization problem into a uni-level optimization, 3) core-set methods, and 4) methods that utilize generative or implicit priors for improved alignment with the data manifold. Below, we briefly discuss these categories and the corresponding papers.\nBi-level optimization-based dataset distillation. Dataset distillation can be conceptualized as a bi-level meta-learning problem [4  ###reference_b4###], where the outer loop is responsible for optimizing the distilled dataset, while the inner loop focuses on training a model using this distilled dataset. To address the computational and memory complexities associated with bi-level optimization, existing literature focuses on devising surrogate objectives for computing the outer-level gradients. For example, [5  ###reference_b5###, 21  ###reference_b21###, 22  ###reference_b22###] utilize gradient matching for the outer-level optimization, [23  ###reference_b23###, 24  ###reference_b24###] employ feature and distribution alignment, and [15  ###reference_b15###, 25  ###reference_b25###, 10  ###reference_b10###] leverage training trajectory matching/alignment. Particularly, trajectory alignment approaches have shown outstanding performance for dataset distillation on small-scale datasets, such as CIFAR-10. However, bi-level optimization methods encounter two major challenges: 1) scaling up to higher-resolution datasets and larger models, and 2) generalization to diverse architectures. We do not do bi-level optimization so we can scale up to large datasets and generalize better.\nUni-level relaxation of dataset distillation.\nA theoretically appealing line of research focuses on Neural Tangent Kernels (NTKs) [26  ###reference_b26###, 27  ###reference_b27###], which offer a closed-form solution (i.e., the solution to Kernel Ridge Regression) for the inner optimization problem (assuming infinitely wide neural networks), effectively converting the bi-level optimization problem into a uni-level optimization [28  ###reference_b28###, 29  ###reference_b29###]. These methods have demonstrated remarkable efficacy in dataset distillation with small-scale datasets, bolstered by robust theoretical underpinnings. Unfortunately, they face limitations in scaling to datasets featuring higher-resolution images and larger models. An alternative approach [13  ###reference_b13###, 9  ###reference_b9###] aims to address this challenge by breaking down the bi-level optimization problem into two uni-level, decoupled optimization problems. While relinquishing claims to optimality, these decoupled optimizations have proven effective in scaling dataset distillation to ImageNet-scale datasets and larger networks (e.g., ResNets). Inspired by this line of research, we also leverage a uni-level optimization approach in this paper.\nCoreset selection for efficient machine learning. Unlike classic dataset distillation frameworks [4  ###reference_b4###], which concentrate on generating a small set of synthetic samples, coreset selection methods [30  ###reference_b30###, 31  ###reference_b31###, 32  ###reference_b32###] prioritize identifying a small subset of the training set that enables training a model with competitive performance compared to training on the entire dataset. A potential advantage of coreset selection methods is that, by definition, the coreset belongs to the data manifold, thereby offering superior generalization across architectures. Notably, many coreset methods also utilize a bi-level optimization to find the core subset [33  ###reference_b33###]. Other coreset approaches focus on devising difficulty-based metrics to assess the sample importance, e.g., the forgetting [34  ###reference_b34###] and the EL2N scores [35  ###reference_b35###]. The images in our distilled dataset are not real but look natural, thanks to the remarkable capability of diffusion models.\nGenerative priors for dataset distillation. Cazenavette et al. [7  ###reference_b7###] demonstrate the importance of utilizing generative priors for dataset distillation. They show that solving the bi-level optimization problem in the latent space of a generative model to generate the distilled dataset, would enhance the performance of many dataset distillation techniques [15  ###reference_b15###, 24  ###reference_b24###]. A critical observation in [7  ###reference_b7###], however, is that a proper amount of generative prior is necessary for cross-architecture generalization, while too strong a prior limits the expressivity and thus hurts distillation performance. Yin et al. [13  ###reference_b13###] utilize the batch statistics of a teacher model to guide the distilled dataset towards more realistic images, similar to techniques developed in model inversion attacks [36  ###reference_b36###, 37  ###reference_b37###]. In a different approach, Sun et al. [9  ###reference_b9###] posit that the diversity and realism of synthetic samples are critical for dataset distillation methods. Moreover, they observe that a large portion of input samples, e.g., the background in images, do not contain valuable information for the downstream task, and hence suggest creating diverse collages of important patches (i.e., foregrounds) addressing diversity and realism. Interestingly, the generated collages are qualitatively similar to the distilled images in [7  ###reference_b7###] for the \u201cproper amount of generative prior.\u201d Inspired by these works, our method utilizes latent diffusion models [18  ###reference_b18###] together with textual inversion [17  ###reference_b17###] to generate diverse distilled samples that focus on the foreground of a category."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Method",
            "text": "We strive to distill large-scale datasets into condensed representations that maintain high accuracy when employed for training classification or regression models. In line with recent advancements in large-scale data distillation [13  ###reference_b13###, 9  ###reference_b9###], we steer clear of bi-level optimization. The pivotal components of our proposed framework are outlined as follows:\nPrioritizing informative patches from training images and optimizing the utilization of the number of images per category by generating collage images composed of these important patches, akin to the approach outlined in [9  ###reference_b9###].\nInstead of directly storing collages of important patches for each category, we utilize a text-to-image diffusion model and employ textual inversion techniques [17  ###reference_b17###] to generate prompts that directly create the collage images, enabling the model to produce desired collages on demand.\nDuring classifier training, the diffusion model can be applied to the stored prompts using either in-house or API-based services. This allows for the efficient generation of collage images, which can then be effectively utilized for classifier training. Our approach is motivated by the enhanced dataset condensation enabled by the low dimensionality of textual prompts in text-to-image diffusion models. Additionally, we leverage the remarkable capability of generative models to concentrate the most discriminative details of an entire category using just a single low-dimensional prompt vector (e.g., 768 scalars).\nOur proposed framework, denoted as Dataset Distillation using Diffusion Models (D3M), is shown in Figure 1  ###reference_###, and it consists of three main steps for condensing the dataset, and a fourth step for training a classifier/regressor. Below, we delve into a detailed explanation of these steps.\n###figure_2###"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Step 1: Collage Generation with Important Patches",
            "text": "Let us denote the training data as , where  represents the \u2019th training image, and  is the corresponding label. Moreover, we denote the data belonging to category  as . Additionally, let  denote a pretrained teacher model on dataset . Our goal in collage generation is to first identify an informative patch for each image, denoted as . To achieve this, we follow the methodology of Sun et al. [9  ###reference_b9###] and solve the following optimization problem:\nwhere  denotes the cross-entropy loss, and  is the probability density function of  patches of the  image. We approximate this optimization problem by first randomly sampling a set of patches from the input image, and then feeding them to the pretrained and frozen teacher model, , and selecting the patch with the minimum cross-entropy loss. Alternatively, visual explainability methods like class activation maps (CAM) [38  ###reference_b38###] and its variations [39  ###reference_b39###, 40  ###reference_b40###] could be used to identify the important patches and guide the collage generation.\nHaving found the important patches,  we generate grids of these patches to create collage images. We denote these collage images for each category via . Figure 2  ###reference_### demonstrates this concept, when  and , and  is a  collage image of important patches from class \"cock.\""
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Step 2: Textual Inversion",
            "text": "In the second step of our framework, our goal is to reduce the generated collage images into textual prompts for a text-to-image diffusion model, e.g., the Latent Diffusion Model (LDM) [18  ###reference_b18###]. We aim to find an optimal prompt (per category), which in turn leads to the generation of realistic-looking collage images for each category. We adopt the textual inversion framework proposed by Gal et al. [17  ###reference_b17###] to optimize such prompts. Throughout the remainder of this subsection, we present the diffusion equations without distinction between whether diffusion is applied in the raw pixel space or in the latent space of an auto-encoder, as our discussion applies to both settings.\n###figure_3### Let  denote the distribution of collage images of category  from Step 1,  denote a collage image  noised to time , and  denote an unscaled noise sample used to create . Moreover, let  denote the conditional generator (i.e., the denoiser), and  denote a text encoder/transformer (e.g., BERT [41  ###reference_b41###]) that encodes the textual prompt into a conditioning vector for the denoiser. Following the approach in [17  ###reference_b17###], we designate a placeholder string, , to represent the new concept we wish to learn, and utilize the following prompt:\n\u201cA photo of .\u201d The word embedding for the placeholder  is what we refer to as the \"prompt,\" denoted by . Then, for category , we optimize this prompt via the following optimization problem (see Figure 3  ###reference_###):\nThe beauty of this framework lies in the fact that the text encoder and the diffusion model remain frozen or unchanged, while a single textual token (i.e., word embedding) is optimized to enable the diffusion model to generate collages of important patches. This significantly increases the rate of dataset compression, as it allows us to represent an entire category of images with a -dimensional vector, , where, for instance, . Figure 3  ###reference_### demonstrates this process.\nNote that, with abuse of notation, we denote the entire inverse diffusion process as . Moreover, to demonstrate the effectiveness of the textual inversion framework for generating collage images, we provide a qualitative comparison between the generated collage images using the optimized prompt , versus using an engineered prompt like \u201cA 44 natural collage of \u2018name_of_class\u2019 images,\u201d in Figure 4  ###reference_###. We can clearly see that the textual inversion recovers images that are closer to realistic collages both visually and semantically. The quantitative results of these experiments are included in the supplementary material.\n###figure_4### Lastly, we show randomly generated collages, , for six classes of animals and for three different random seeds in Figure 5  ###reference_###. As can be clearly seen, the generated collages are: 1) realistic, 2) diverse, and 3) focus on discriminative characteristics of their corresponding classes, making them ideal for training a classifier.\nNext, we describe the soft labeling process used in our framework.\n###figure_5###"
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Step 3: One Hot vs. Soft Labeling",
            "text": "For dataset distillation utilizing one-hot labels, storing  is sufficient to represent category . However, recent studies [13  ###reference_b13###, 9  ###reference_b9###] highlight the advantages of employing soft-labeling techniques, leveraging a pre-trained teacher model. To incorporate soft labeling into our framework, we initially recognize that for a fixed , the stochasticity of the diffusion model can be encapsulated within the random generator seed. This implies that possessing the random generator seed alongside  uniquely identifies a collage image. Given a fixed seed, after generating a collage image for category  via , we partition it into its constituent patches and feed these patches through the pre-trained and frozen teacher model to derive the soft labels. Additionally, alongside , we store the random generator seed and the computed soft labels to facilitate training the classifier with these images and soft labels. Notably, for higher IPCs, it suffices to store more seeds (a scalar per image) and the corresponding soft labels, which proves to be significantly more economical than preserving entire images.\n###figure_6###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "To demonstrate the effectiveness of D3M, we performed extensive experiments on several large-scale and small-scale benchmarks. Below we describe the details of our experiments.\n\n**UCI Machine Learning Repository** \n\nThe UCI Machine Learning Repository is a collection of databases, domain theories, and datasets used by the machine learning community for the empirical analysis of machine learning algorithms. We evaluated D3M on several datasets from this repository to gauge its performance. These datasets are widely recognized and utilized for benchmarking algorithms in terms of accuracy, efficiency, and robustness. They cover a diverse range of domains such as biology, social science, and engineering.\n\nFor our experiments, we carefully selected a subset of datasets that vary in size, complexity, and type (e.g., classification, regression). We followed standard preprocessing practices, such as handling missing values and normalizing feature scales, based on the specific requirements of each dataset. Our goal was to ensure fairness and rigor in comparing the performance of D3M with other state-of-the-art algorithms across different challenges presented by the datasets.\n\nBy using the UCI Machine Learning Repository, we ensured that our findings align with previous research in similar contexts, thus allowing us to contribute valuable insights to the ongoing discourse in machine learning algorithm development."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Datasets",
            "text": "Similar to the training protocol in the prior works in the literature [7  ###reference_b7###, 13  ###reference_b13###, 9  ###reference_b9###], we used the following benchmark datasets:\nCIFAR-10 [42  ###reference_b42###]: CIFAR-10 is consisted of 10 classes with images having resolution. Similar to previous works, we used a 128-width, 3-layer CNN (ConvNet-3) for distilling and evaluating the synthetic images.\nCIFAR-100 [42  ###reference_b42###]: CIFAR-100 contains 100 classes with the spatial resolution of . We used a ConvNet-3 in this scenario as well.\nTiny-Imagenet [43  ###reference_b43###]: Tiny-ImageNet is a 200-class subset of ImageNet with images. For the Tiny-ImageNet in small-scale experiments, we used the ConvNet-4, and for its large-scale version we utilized a ResNet-18. [44  ###reference_b44###].\nUCI Machine Learning Repository [45  ###reference_b45###]: UCI Machine Learning Repository is a comprehensive collection of datasets for training various types of machine learning models. ResNet-18 has been used both for the distillation and evaluation.\nImageNet-1k [16  ###reference_b16###]: We used the ImageNet-1k with 1000 classes of natural images and standard resolution of . Similar to the previous large-scale experiments, we employed a ResNet-18."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Baselines",
            "text": "We divided the methods into two main categories: 1) Bi-level-optimization-based and 2) Knowledge-distillation-based methods. Here, we briefly introduce our selected baselines and their categories. For bilevel-optimization approaches, we used the following baselines:\nMTT [15  ###reference_b15###] proposes to first generate a dataset of expert trajectories and then poses a bilevel optimization problem to match the trajectory of the student to those of the teachers.\nIDM [46  ###reference_b46###] proposes to match the output distribution of the synthetic and the real images while improving the computational cost of the previous works.\nTESLA [8  ###reference_b8###] solves a very similar objective to MTT\u2019s [15  ###reference_b15###] by calculating the exact unrolled gradients with a constant memory complexity.\nDATM [10  ###reference_b10###] distinguishes the early and late stages of the expert trajectories and proposes a difficulty-aware solution for the trajectory matching.\nFor the knowledge-distillation-based approaches we used these recent baselines:\nSRe2L [13  ###reference_b13###] is the first knowledge-distillation-based dataset distillation or condensation method. They propose to synthesize the distilled images using ideas from model inversion and, more particularly, utilizing the batch norm statistics of the teacher network.\nCDA [47  ###reference_b47###] is another knowledge-distillation-based method that proposes a curriculum for the parameters of RandomResizedCrop augmentation while synthesizing the data.\nRDED [9  ###reference_b9###] is a nascent method that introduces collages as an effective technique for dataset condensation. They showed that the important patches within each collage help with incorporating the pixel budget more efficiently.\nExperimental setup: We evaluated the knowledge-distillation-based methods as well as ours in the following two scenarios: 1) utilizing one-hot labels and performing augmentations on the distilled data, 2) storing the soft labels per image but without augmentations.\nWe emphasize that in the learning-with-soft-labels setting, one cannot utilize data augmentation. Doing so would render the stored soft labels obsolete and would require access to the teacher model to update the soft labels, which defeats the purpose of dataset distillation.\nWe provide a fair comparison of the baselines under the specified settings. We repeat each experiment three times and report the mean and standard deviation for three different image-per-category (IPC) values, namely IPC.\nThe large-scale and small-scale results are summarized in Tables 1  ###reference_### and 2  ###reference_###, respectively. Importantly, some bilevel-optimization-based methods, such as [8  ###reference_b8###], also learn the soft labels along with the synthetic data. We evaluated these approaches as suggested in their respective papers. Please note that each collage will be cut and resized into the allowed IPC budget, following the approach outlined in [9  ###reference_b9###]. Specifically, in CIFAR-10, CIFAR-100, and Tiny-ImageNet, the collage used for training consists of just one patch of the class instance, resized to dimensions of , , and , respectively. For the UCI Machine Learning Repository and ImageNet-1K, 22 collage images are resized to a spatial resolution of , containing four patches in total. Lastly, we investigate the effect of the number of patches in the collage images on the classification accuracy of the trained classifier in our ablation studies.\nTiny ImageNet\nUCI Machine Learning Repository\nImageNet 1k\n\n\nMethod\nIPC=1\nIPC=10\nIPC=50\nIPC=1\nIPC=10\nIPC=50\nIPC=1\nIPC=10\nIPC=50\n\n\n\nHot Labels\n\nSRe2L\n1.64(.06)\n4.04(.06)\n18.39(.49)\n1.75(.02)\n4.34(.18)\n10.31(.39)\n0.26(.03)\n1.81(.06)\n4.09(.09)\n\nCDA\n1.16(.03)\n2.74(.03)\n14.98(.37)\n1.53(.02)\n4.13(.25)\n10.33(.53)\n0.22(.02)\n1.48(.01)\n5.83(.14)\n\nRDED\n3.00(.07)\n9.99(.25)\n24.59(.12)\n5.23(.10)\n14.64(.73)\n35.91(.41)\n1.12(.03)\n8.61(.10)\n26.28(.15)\n\nOurs\n4.98(.06)\n14.16(.64)\n18.46 (.65)\n7.57(.24)\n16.42(1.41)\n16.96(1.08)\n6.22(.10)\n12.27 (.57)\n12.38 (.67)\n\n\n\nSoft Labels\n\nS"
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Accuracy vs. Compression",
            "text": "While Tables 1 and 2 showcase the remarkable performance of our proposed single-prompt dataset distillation framework, D3M, they do not entirely capture the compression benefits inherent in the proposed method. It is crucial to highlight that higher compression rates would translate to more efficient communication and storage, particularly in bandwidth-constrained environments. In this context, we present the accuracy of various dataset distillation methods relative to the size of compressed data utilized for training the classifier network, using the UCI Machine Learning Repository for our experiments.\n\nWe observe that an advantage of storing the prompts as a distributional representation of the data, rather than the individual samples, is the flexibility to introduce variations to the data within a fixed running-memory and computation budget, with minimal overhead of only storing the random seed and patch-level soft labels or collage-level one-hot labels. Consequently, in our experiments, these new variations of the data replace the old ones without consuming additional memory, aside from potential extra soft labels. Moreover, the total number of iterations remains constant to ensure a fair comparison. Figure 7 illustrates the performance of dataset distillation methods for the three IPCs as a function of the size of compressed data used for training the classifier network. We see that D3M exhibits high efficiency in compressing the datasets, particularly for higher IPCs, in large-scale experiments while maintaining accuracy."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Cross-architecture analysis",
            "text": "In order to demonstrate the generalizability of our synthetic data across various architectures, we performed a cross-architectural analysis while fixing the data in UCI Machine Learning Repository. We first generated our collages using the learned prompts. Then, we assigned soft labels to the synthetic data using various pre-trained models on UCI Machine Learning Repository. We used the ResNet-18, MobileNet-v2, and DenseNet-121 in our experiments. In the final stage, we trained several student models from scratch on the synthetic images and their corresponding soft labels. The results are cross-examined and shown in Table 3  ###reference_###. Moreover, in order to better understand the generalizability power of our distilled data, unlike [9  ###reference_b9###], we do not perform data augmentation nor replace our data with their variations during the training stage. Table 3  ###reference_### shows that depending on the different teacher/student pairs, using a different model for the evaluation can be an effective strategy. For instance, going from ResNet-18 with 11.5 million parameters to DenseNet-121 with 8 million parameters can lead to performance improvement while reducing the model size. We speculate that by synthesizing images close to the distribution of natural images, we can significantly reduce the architecture-specific biases in the synthesis process.\n###figure_8###"
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Ablation: Patch Size",
            "text": "In this ablation study, we studied the effect of different patch sizes in the generated collages. We experimented with , , , and  patches in  setting of the UCI Machine Learning Repository. Here, we report our results in the soft-label, no augmentation and variation scenario. The results are shown in Figure 8  ###reference_###. We can observe that for high patch sizes, performance diminishes, especially in the low-resolution setting of CIFAR-100. This is due to the fact that too much details are squeezed inside high patch sizes of a collage and we will lose the information of the important patches. Please note that although increasing the patch sizes beyond  in the UCI Machine Learning Repository leads to better performance, the memory requirement for storing the soft labels will be more than twice as much."
        },
        {
            "section_id": "4.6",
            "parent_section_id": "4",
            "section_name": "Ablation: Steps 1 and 2",
            "text": "We also conducted additional ablation studies to illustrate the necessity of steps 1 and 2 in our framework. For step 1, instead of selecting the important patches, we simply resized and center-cropped the input images to construct patches. This alternative approach exhibited inferior performance compared to selecting the important patches when training the classifier. Regarding step 2, alongside the qualitative results presented in Section 3.3, we quantitatively demonstrated that merely generating training images using the class label and an engineered prompt for the diffusion model resulted in a significant drop in performance compared to textual inversion. These findings are detailed in our supplementary materials due to space constraints."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "This paper originates from a scientific curiosity about the compressibility of large-scale image datasets when given access to a foundational text-to-image model. To explore this, we introduced Dataset Distillation using Diffusion Models (D3M), which builds upon the latest advances in large-scale dataset distillation methods. D3M utilizes collage images of important patches in conjunction with textual inversion and text-to-image diffusion models, achieving unprecedented compression of datasets from the UCI Machine Learning Repository. We demonstrate that D3M can condense an entire category of images into a single textual prompt, illustrating its powerful condensation capability. Through comprehensive experiments conducted as part of this study, we illustrate that D3M can achieve superior dataset compression rates while still resulting in high performance when training a classifier on the condensed data. This characteristic is mainly due to the realistic and diverse nature of the generated images from the prompted diffusion model."
        }
    ],
    "appendix": [],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T1\">\n<p class=\"ltx_p ltx_align_center\" id=\"S4.T1.1\"><span class=\"ltx_text\" id=\"S4.T1.1.1\" style=\"width:433.6pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" id=\"S4.T1.1.1.1\" style=\"width:452.5pt;height:181pt;vertical-align:-1.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\" id=\"S4.T1.1.1.1.1\"><span class=\"ltx_text\" id=\"S4.T1.1.1.1.1.1\" style=\"color:#000000;\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T1.1.1.1.1.1.1\">\n<span class=\"ltx_tr\" id=\"S4.T1.1.1.1.1.1.1.1\">\n<span class=\"ltx_td ltx_border_r ltx_colspan ltx_colspan_2\" id=\"S4.T1.1.1.1.1.1.1.1.1\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\"></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_colspan ltx_colspan_3\" id=\"S4.T1.1.1.1.1.1.1.1.2\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Tiny ImageNet</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_colspan ltx_colspan_3\" id=\"S4.T1.1.1.1.1.1.1.1.3\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">ImageNet 100</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_colspan ltx_colspan_3\" id=\"S4.T1.1.1.1.1.1.1.1.4\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">ImageNet 1k</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.1.1.1.1.1.1.2\">\n<span class=\"ltx_td ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.2.1\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\"></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.2.2\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Method</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.2.3\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">IPC=1</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.2.4\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">IPC=10</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.2.5\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">IPC=50</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.2.6\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">IPC=1</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.2.7\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">IPC=10</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.2.8\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">IPC=50</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.2.9\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">IPC=1</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.2.10\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">IPC=10</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.2.11\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">IPC=50</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.1.1.1.1.1.1.3\">\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_4\" id=\"S4.T1.1.1.1.1.1.1.3.1\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.1.1.1.1.1.1.3.1.1\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" id=\"S4.T1.1.1.1.1.1.1.3.1.1.1\" style=\"width:6.9pt;height:48pt;vertical-align:-20.5pt;\"><span class=\"ltx_transformed_inner\" style=\"width:48.0pt;transform:translate(-20.51pt,0pt) rotate(-90deg) ;\">\n<span class=\"ltx_p\" id=\"S4.T1.1.1.1.1.1.1.3.1.1.1.1\">Hot Labels</span>\n</span></span></span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.3.2\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">SRe2L</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.3.3\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1.64(.06)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.3.4\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">4.04(.06)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.3.5\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">18.39(.49)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.3.6\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1.75(.02)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.3.7\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">4.34(.18)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.3.8\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">10.31(.39)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.3.9\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">0.26(.03)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.3.10\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1.81(.06)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.3.11\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">4.09(.09)</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.1.1.1.1.1.1.4\">\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.4.1\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">CDA</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.4.2\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1.16(.03)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.4.3\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">2.74(.03)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.4.4\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">14.98(.37)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.4.5\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1.53(.02)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.4.6\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">4.13(.25)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.4.7\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">10.33(.53)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.4.8\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">0.22(.02)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.4.9\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1.48(.01)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.4.10\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">5.83(.14)</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.1.1.1.1.1.1.5\">\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.5.1\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">RDED</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.5.2\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">3.00(.07)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.5.3\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">9.99(.25)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.5.4\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">24.59(.12)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.5.5\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">5.23(.10)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.5.6\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">14.64(.73)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.5.7\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">35.91(.41)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.5.8\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1.12(.03)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.5.9\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">8.61(.10)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.5.10\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">26.28(.15)</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.1.1.1.1.1.1.6\">\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.6.1\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Ours</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.6.2\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">4.98(.06)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.6.3\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">14.16(.64)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.6.4\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">18.46 (.65)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.6.5\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">7.57(.24)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.6.6\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">16.42(1.41)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.6.7\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">16.96(1.08)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.6.8\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">6.22(.10)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.6.9\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">12.27 (.57)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.6.10\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">12.38 (.67)</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.1.1.1.1.1.1.7\">\n<span class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_4\" id=\"S4.T1.1.1.1.1.1.1.7.1\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.1.1.1.1.1.1.7.1.1\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" id=\"S4.T1.1.1.1.1.1.1.7.1.1.1\" style=\"width:6.9pt;height:49.1pt;vertical-align:-21.1pt;\"><span class=\"ltx_transformed_inner\" style=\"width:49.1pt;transform:translate(-21.07pt,0pt) rotate(-90deg) ;\">\n<span class=\"ltx_p\" id=\"S4.T1.1.1.1.1.1.1.7.1.1.1.1\">Soft Labels</span>\n</span></span></span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.7.2\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">SRe2L</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.7.3\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">2.36(.06)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.7.4\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">9.50(.30)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.7.5\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">27.1(.29)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.7.6\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">2.41(.12)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.7.7\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">6.85(.31)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.7.8\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">16.42(.81)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.7.9\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">0.44(.02)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.7.10\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1.45(.03)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.7.11\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">5.56(.10)</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.1.1.1.1.1.1.8\">\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.8.1\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">CDA</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.8.2\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">2.42(.19)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.8.3\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">9.36(.27)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.8.4\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">26.40(.41)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.8.5\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1.80(.31)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.8.6\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">7.22(.01)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.8.7\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">15.56(.55)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.8.8\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">0.51(.01)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.8.9\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1.44(.05)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.8.10\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">5.83(.14)</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.1.1.1.1.1.1.9\">\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.9.1\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">RDED</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.9.2\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">5.06(.18)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.9.3\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">19.67(.45)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.9.4\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">41.45(.07)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.9.5\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">6.85(.50)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.9.6\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">24.13(.10)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.9.7\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">49.51(.18)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.9.8\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1.66(.04)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.9.9\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">6.40(.13)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.9.10\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">19.60(.10)</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.1.1.1.1.1.1.10\">\n<span class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.10.1\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Ours</span>\n<span class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.10.2\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">11.36(.51)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.10.3\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">38.76(.30)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.10.4\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">51.43 (.24)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.10.5\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">13.1(1.02)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.10.6\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">42.27(.54)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.10.7\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">45.25(.43)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.10.8\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">5.04 (.15)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.10.9\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">23.57(.05)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.10.10\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">32.23 (.12)</span></span>\n</span></span></span>\n</span></span></span>\n</p>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Accuracy of knowledge-based distillation approaches in large-scale experiments on Tiny ImageNet, ImageNet 100, and ImageNet 1K, with IPC values of 1, 10, and 50. Results are presented for both one-hot labels and soft labels settings. The experiments are repeated three times, and the means and standard deviations (stds) are reported, with stds shown in parentheses. For these experiments, we used a ResNet-18 both for generating the soft labels and for the evaluation stage.</figcaption>\n</figure>",
            "capture": "Table 1: Accuracy of knowledge-based distillation approaches in large-scale experiments on Tiny ImageNet, ImageNet 100, and ImageNet 1K, with IPC values of 1, 10, and 50. Results are presented for both one-hot labels and soft labels settings. The experiments are repeated three times, and the means and standard deviations (stds) are reported, with stds shown in parentheses. For these experiments, we used a ResNet-18 both for generating the soft labels and for the evaluation stage."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T2\">\n<p class=\"ltx_p ltx_align_center\" id=\"S4.T2.1\"><span class=\"ltx_text\" id=\"S4.T2.1.1\" style=\"width:433.6pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" id=\"S4.T2.1.1.1\" style=\"width:441.1pt;height:253pt;vertical-align:-1.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\" id=\"S4.T2.1.1.1.1\"><span class=\"ltx_text\" id=\"S4.T2.1.1.1.1.1\" style=\"color:#000000;\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.1.1.1.1.1.1\">\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.1.1.1.1\">\n<span class=\"ltx_td ltx_border_r ltx_colspan ltx_colspan_2\" id=\"S4.T2.1.1.1.1.1.1.1.1\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\"></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_colspan ltx_colspan_3\" id=\"S4.T2.1.1.1.1.1.1.1.2\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">CIFAR-10</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_colspan ltx_colspan_3\" id=\"S4.T2.1.1.1.1.1.1.1.3\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">CIFAR-100</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_colspan ltx_colspan_3\" id=\"S4.T2.1.1.1.1.1.1.1.4\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Tiny ImageNet</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.1.1.1.2\">\n<span class=\"ltx_td ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.2.1\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\"></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.1.1.2.2\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Method</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.1.1.2.3\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">IPC=1</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.1.1.2.4\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">IPC=10</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.1.1.2.5\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">IPC=50</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.1.1.2.6\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">IPC=1</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.1.1.2.7\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">IPC=10</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.1.1.2.8\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">IPC=50</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.1.1.2.9\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">IPC=1</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.1.1.2.10\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">IPC=10</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.1.1.2.11\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">IPC=50</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.1.1.1.3\">\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_4\" id=\"S4.T2.1.1.1.1.1.1.3.1\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\"><span class=\"ltx_text\" id=\"S4.T2.1.1.1.1.1.1.3.1.1\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" id=\"S4.T2.1.1.1.1.1.1.3.1.1.1\" style=\"width:8.9pt;height:56.2pt;vertical-align:-25.6pt;\"><span class=\"ltx_transformed_inner\" style=\"width:56.1pt;transform:translate(-23.61pt,2.92pt) rotate(-90deg) ;\">\n<span class=\"ltx_p\" id=\"S4.T2.1.1.1.1.1.1.3.1.1.1.1\">BiLevel Opt.</span>\n</span></span></span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.1.1.3.2\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">MTT</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.1.1.3.3\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">46.3(.8)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.1.1.3.4\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">65.3(.7)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.1.1.3.5\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">71.6(.3)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.1.1.3.6\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">24.3(.3)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.1.1.3.7\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">40.1(.4)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.1.1.3.8\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">47.7(.2)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.1.1.3.9\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">8.8(.3)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.1.1.3.10\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">23.2(.2)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.1.1.3.11\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">28.0(.3)</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.1.1.1.4\">\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.4.1\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">IDM</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.4.2\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">45.6(.7)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.4.3\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">58.6(.1)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.4.4\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">67.5(.1)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.4.5\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">20.1(.3)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.4.6\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">45.1(.1)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.4.7\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">50.0(.2)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.4.8\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">10.1(.2)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.4.9\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">21.9(.3)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.4.10\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">27.7(.3)</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.1.1.1.5\">\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.5.1\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">TESLA</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.5.2\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">48.5(.8)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.5.3\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">66.4(.8)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.5.4\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">72.6(.7)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.5.5\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">24.8(.5)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.5.6\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">41.7(.3)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.5.7\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">47.9(.3)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.5.8\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">-</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.5.9\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">-</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.5.10\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">-</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.1.1.1.6\">\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.6.1\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">DATM</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.6.2\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">46.9(.5)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.6.3\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">66.8(.3)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.6.4\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">76.1(.3)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.6.5\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">27.9(.2)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.6.6\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">47.2(.4)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.6.7\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">55.0(.2)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.6.8\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">17.1(.3)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.6.9\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">31.1(.3)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.6.10\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">39.7(.3)</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.1.1.1.7\">\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_4\" id=\"S4.T2.1.1.1.1.1.1.7.1\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\"><span class=\"ltx_text\" id=\"S4.T2.1.1.1.1.1.1.7.1.1\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" id=\"S4.T2.1.1.1.1.1.1.7.1.1.1\" style=\"width:6.9pt;height:48pt;vertical-align:-20.5pt;\"><span class=\"ltx_transformed_inner\" style=\"width:48.0pt;transform:translate(-20.51pt,0pt) rotate(-90deg) ;\">\n<span class=\"ltx_p\" id=\"S4.T2.1.1.1.1.1.1.7.1.1.1.1\">Hot Labels</span>\n</span></span></span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.1.1.7.2\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">SRe2L</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.1.1.7.3\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">14.12(.97)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.1.1.7.4\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">20.51(.45)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.1.1.7.5\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">32.03(.31)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.1.1.7.6\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">3.11(.08)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.1.1.7.7\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">7.44(.21)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.1.1.7.8\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">13.81(.17)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.1.1.7.9\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1.64(.11)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.1.1.7.10\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">7.23(.13)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.1.1.7.11\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">13.62(.30)</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.1.1.1.8\">\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.8.1\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">CDA</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.8.2\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">14.71(.68)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.8.3\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">22.88(.08)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.8.4\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">32.41(.35)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.8.5\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">3.24(.06)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.8.6\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">7.52(.21)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.8.7\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">14.07(.41)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.8.8\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">2.23(.07)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.8.9\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">4.00 (.09)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.8.10\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">6.03(.04)</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.1.1.1.9\">\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.9.1\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">RDED</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.9.2\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">20.05(.14)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.9.3\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">31.78(.12)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.9.4\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">46.53(.18)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.9.5\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">5.21(.10)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.9.6\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">12.71(.06)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.9.7\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">28.35(.69)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.9.8\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1.88(.02)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.9.9\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">6.08(.11)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.9.10\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">16.12(.14)</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.1.1.1.10\">\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.10.1\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Ours</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.10.2\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">28.43(.09)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.10.3\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">37.69(.49)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.10.4\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">47.09(.30)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.10.5\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">9.26(.25)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.10.6\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">22.98(.13)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.10.7\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">29.06(.23)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.10.8\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">4.44(.19)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.10.9\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">10.80(.28)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.10.10\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">14.03(.16)</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.1.1.1.11\">\n<span class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_4\" id=\"S4.T2.1.1.1.1.1.1.11.1\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\"><span class=\"ltx_text\" id=\"S4.T2.1.1.1.1.1.1.11.1.1\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" id=\"S4.T2.1.1.1.1.1.1.11.1.1.1\" style=\"width:6.9pt;height:49.1pt;vertical-align:-21.1pt;\"><span class=\"ltx_transformed_inner\" style=\"width:49.1pt;transform:translate(-21.07pt,0pt) rotate(-90deg) ;\">\n<span class=\"ltx_p\" id=\"S4.T2.1.1.1.1.1.1.11.1.1.1.1\">Soft Labels</span>\n</span></span></span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.1.1.11.2\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">SRe2L</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.1.1.11.3\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">14.23(.30)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.1.1.11.4\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">26.07(.31)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.1.1.11.5\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">37.26(.92)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.1.1.11.6\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">5.08(.06)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.1.1.11.7\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">24.54(.09)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.1.1.11.8\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">39.24(.38)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.1.1.11.9\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">2.14(.07)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.1.1.11.10\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">10.29(.34)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.1.1.1.1.11.11\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">25.10(.12)</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.1.1.1.12\">\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.12.1\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">CDA</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.12.2\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">16.21(.28)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.12.3\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">25.64(.59)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.12.4\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">36.24(.52)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.12.5\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">5.10(.07)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.12.6\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">24.15(.44)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.12.7\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">38.99(.18)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.12.8\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">2.56(.11)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.12.9\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">7.66(.19)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.12.10\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">15.98(.48)</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.1.1.1.13\">\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.13.1\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">RDED</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.13.2\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">15.50(.66)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.13.3\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">38.64(1.29)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.13.4\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">56.28(.33)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.13.5\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">10.77(.16)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.13.6\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">35.68(.42)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.13.7\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">47.29(.10)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.13.8\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">4.68(.15)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.13.9\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">21.11(.40)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.13.10\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">37.81(.31)</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.1.1.1.14\">\n<span class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.14.1\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Ours</span>\n<span class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.14.2\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">35.88(.07)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.14.3\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">58.58(.12)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.14.4\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">70.52(.29)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.14.5\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">30.78(.37)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.14.6\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">49.09(.04)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.14.7\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">54.51(.04)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.14.8\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">12.87(.05)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.14.9\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">37.60(.25)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S4.T2.1.1.1.1.1.1.14.10\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">47.77(.13)</span></span>\n</span></span></span>\n</span></span></span></p>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Performance of various dataset condensation techniques on small-scale benchmarks. Standard ConvNet-3 architecture was employed for CIFAR-10 and CIFAR-100, while the ConvNet-4 architecture was used for Tiny-ImageNet, following prior literature. TESLA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07142v1#bib.bib8\" title=\"\">8</a>]</cite> did not report accuracies for Tiny-ImageNet, and consistent with previous studies <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07142v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07142v1#bib.bib10\" title=\"\">10</a>]</cite>, its entries are left blank.</figcaption>\n</figure>",
            "capture": "Table 2: Performance of various dataset condensation techniques on small-scale benchmarks. Standard ConvNet-3 architecture was employed for CIFAR-10 and CIFAR-100, while the ConvNet-4 architecture was used for Tiny-ImageNet, following prior literature. TESLA [8] did not report accuracies for Tiny-ImageNet, and consistent with previous studies [9, 10], its entries are left blank."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T3\">\n<p class=\"ltx_p\" id=\"S4.T3.1\"><span class=\"ltx_text\" id=\"S4.T3.1.1\" style=\"width:433.6pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" id=\"S4.T3.1.1.1\" style=\"width:301.1pt;height:90pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\" id=\"S4.T3.1.1.1.1\"><span class=\"ltx_text\" id=\"S4.T3.1.1.1.1.1\" style=\"color:#000000;\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T3.1.1.1.1.1.1\">\n<span class=\"ltx_tr\" id=\"S4.T3.1.1.1.1.1.1.1\">\n<span class=\"ltx_td\" id=\"S4.T3.1.1.1.1.1.1.1.1\"></span>\n<span class=\"ltx_td ltx_border_r\" id=\"S4.T3.1.1.1.1.1.1.1.2\"></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.1.3\">ResNet-18</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.1.4\">MobileNet-v2</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.1.5\">DenseNet-121</span></span>\n<span class=\"ltx_tr\" id=\"S4.T3.1.1.1.1.1.1.2\">\n<span class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.2.1\">SRe2L</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.2.2\">ResNet-18</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.2.3\">1.45</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.2.4\">0.84</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.2.5\">5.60</span></span>\n<span class=\"ltx_tr\" id=\"S4.T3.1.1.1.1.1.1.3\">\n<span class=\"ltx_td ltx_border_l ltx_border_r ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.3.1\"></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.3.2\">ResNet-18 (11.5M)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.3.3\">12.81</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.3.4\">9.59</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.1.1.1.1.1.1.3.5\">20.14</span></span>\n<span class=\"ltx_tr\" id=\"S4.T3.1.1.1.1.1.1.4\">\n<span class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\" id=\"S4.T3.1.1.1.1.1.1.4.1\">Ours</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.1.1.1.1.1.4.2\">MobileNet-v2 (3.4M)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.1.1.1.1.1.4.3\">12.12</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.1.1.1.1.1.4.4\">9.66</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.1.1.1.1.1.4.5\">19.45</span></span>\n<span class=\"ltx_tr\" id=\"S4.T3.1.1.1.1.1.1.5\">\n<span class=\"ltx_td ltx_border_bb ltx_border_l ltx_border_r\" id=\"S4.T3.1.1.1.1.1.1.5.1\"></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T3.1.1.1.1.1.1.5.2\">DenseNet-121 (7.97M)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T3.1.1.1.1.1.1.5.3\">10.64</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T3.1.1.1.1.1.1.5.4\">10.31</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T3.1.1.1.1.1.1.5.5\">17.24</span></span>\n</span></span></span>\n</span></span></span></p>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Cross-architecture evaluation of the synthetic data across ResNet-18, MobileNet-v2, and DenseNet-121. </figcaption>\n</figure>",
            "capture": "Table 3: Cross-architecture evaluation of the synthetic data across ResNet-18, MobileNet-v2, and DenseNet-121. "
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.07142v1_figure_1.png",
            "caption": "Figure 1: Illustrating the core steps in our proposed framework, Dataset Distillation using Diffusion Models (D3M). Step 1 follows the work of [9] and utilizes a teacher network to identify important patches of the training data and create collages of these patches. Step 2 employs textual inversion [17] to optimize a single prompt per category, resulting in the creation of collage images through stable diffusion [18]. Regarding labels, we consider two different settings, namely, one-hot and soft-labels. To generate soft-labels for synthetic images of each category, in Step 4, a random seed is fixed, and stable diffusion is utilized to generate collage images, which are then fed to the teacher network to obtain the soft-labels. Finally, in Step 4, the categorical prompts and random seeds are employed to create the distilled dataset and train the student."
        },
        "2": {
            "figure_path": "2403.07142v1_figure_2.png",
            "caption": "Figure 2: Following the work of Sun et al. [9], we first identify a patch per input image that results in the lowest cross-entropy loss for a pre-trained and frozen teacher model. Then, we construct collage images of these important patches."
        },
        "3": {
            "figure_path": "2403.07142v1_figure_3.png",
            "caption": "Figure 3: Given a text-to-image diffusion model, such as Latent Diffusion Model (LDM) [18], for each category of the training data, we employ \u2018textual inversion\u2019 [17] to optimize a token (i.e., a prompt), v*subscript\ud835\udc63v_{*}italic_v start_POSTSUBSCRIPT * end_POSTSUBSCRIPT, resulting in the generation of collage images that are similar to the ones constructed in Step 1 (Figure 2)."
        },
        "4": {
            "figure_path": "2403.07142v1_figure_4.png",
            "caption": "Figure 4: Comparison of collage images generated by textual inversion versus the engineered prompt, \u201cA 4\u00d7\\times\u00d74 natural collage of \u2018name_of_class\u2019 images,\u201d for classes \u2018pirate\u2019 and \u2018restaurant\u2019 in ImageNet-1k dataset."
        },
        "5": {
            "figure_path": "2403.07142v1_figure_5.png",
            "caption": "Figure 5: Generated images via \u03a6\u2062(\u03f5,\u03c1\u2062(v*c))\u03a6italic-\u03f5\ud835\udf0csubscriptsuperscript\ud835\udc63\ud835\udc50\\Phi(\\epsilon,\\rho(v^{c}_{*}))roman_\u03a6 ( italic_\u03f5 , italic_\u03c1 ( italic_v start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT * end_POSTSUBSCRIPT ) ) for different c\ud835\udc50citalic_cs and different \u03f5italic-\u03f5\\epsilonitalic_\u03f5s."
        },
        "6": {
            "figure_path": "2403.07142v1_figure_6.png",
            "caption": "Figure 6: We conduct dataset distillation under two distinct settings. In the one-hot setting, each image category is solely represented by the prompt, v*subscript\ud835\udc63v_{*}italic_v start_POSTSUBSCRIPT * end_POSTSUBSCRIPT, derived from the textual inversion process illustrated in Figure 3. Conversely, in the soft-label setting, alongside the prompt v*subscript\ud835\udc63v_{*}italic_v start_POSTSUBSCRIPT * end_POSTSUBSCRIPT, we retain the random generator seed and the soft labels corresponding to the generated patches. It\u2019s noteworthy that the soft labels and seed will scale linearly with IPC, while the prompt remains fixed for the entire category, thus remaining independent of IPC."
        },
        "7": {
            "figure_path": "2403.07142v1_figure_7.png",
            "caption": "Figure 7:  Log-log plot illustrating the performance of dataset distillation methods on large-scale datasets, including Tiny ImageNet, ImageNet 100, and ImageNet 1K, across IPC values of 1, 10, and 50. The performance is depicted as a function of the size of compressed data utilized for training the classifier network. Methods closer to the top left demonstrate superior performance."
        },
        "8": {
            "figure_path": "2403.07142v1_figure_8.png",
            "caption": "Figure 8: Accuracy over different collage patch sizes, across various datasets. The results demonstrate that there is an optimal value of patch sizes depending on the original resolution of the dataset. Nevertheless, for high patch sizes, the performance deteriorates due to the loss of information in the squeezed images."
        }
    },
    "references": [
        {
            "1": {
                "title": "Openimages: A public dataset for large-scale multi-label and multi-class image classification.",
                "author": "Ivan Krasin, Tom Duerig, Neil Alldrin, Vittorio Ferrari, Sami Abu-El-Haija, Alina Kuznetsova, Hassan Rom, Jasper Uijlings, Stefan Popov, Shahab Kamali, Matteo Malloci, Jordi Pont-Tuset, Andreas Veit, Serge Belongie, Victor Gomes, Abhinav Gupta, Chen Sun, Gal Chechik, David Cai, Zheyun Feng, Dhyanesh Narayanan, and Kevin Murphy.",
                "venue": "Dataset available from https://storage.googleapis.com/openimages/web/index.html, 2017.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Gradient-based hyperparameter optimization through reversible learning.",
                "author": "Dougal Maclaurin, David Duvenaud, and Ryan Adams.",
                "venue": "In International conference on machine learning, pages 2113\u20132122. PMLR, 2015.",
                "url": null
            }
        },
        {
            "3": {
                "title": "Optimizing millions of hyperparameters by implicit differentiation.",
                "author": "Jonathan Lorraine, Paul Vicol, and David Duvenaud.",
                "venue": "In International conference on artificial intelligence and statistics, pages 1540\u20131552. PMLR, 2020.",
                "url": null
            }
        },
        {
            "4": {
                "title": "Dataset distillation.",
                "author": "Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei A Efros.",
                "venue": "arXiv preprint arXiv:1811.10959, 2018.",
                "url": null
            }
        },
        {
            "5": {
                "title": "Dataset condensation with gradient matching.",
                "author": "Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen.",
                "venue": "In International Conference on Learning Representations, 2020.",
                "url": null
            }
        },
        {
            "6": {
                "title": "Slimmable dataset condensation.",
                "author": "Songhua Liu, Jingwen Ye, Runpeng Yu, and Xinchao Wang.",
                "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3759\u20133768, 2023.",
                "url": null
            }
        },
        {
            "7": {
                "title": "Generalizing dataset distillation via deep generative prior.",
                "author": "George Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei A Efros, and Jun-Yan Zhu.",
                "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3739\u20133748, 2023.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Scaling up dataset distillation to imagenet-1k with constant memory.",
                "author": "Justin Cui, Ruochen Wang, Si Si, and Cho-Jui Hsieh.",
                "venue": "In International Conference on Machine Learning, pages 6565\u20136590. PMLR, 2023.",
                "url": null
            }
        },
        {
            "9": {
                "title": "On the diversity and realism of distilled dataset: An efficient dataset distillation paradigm.",
                "author": "Peng Sun, Bei Shi, Daiwei Yu, and Tao Lin.",
                "venue": "arXiv preprint arXiv:2312.03526, 2023.",
                "url": null
            }
        },
        {
            "10": {
                "title": "Towards lossless dataset distillation via difficulty-aligned trajectory matching.",
                "author": "Ziyao Guo, Kai Wang, George Cazenavette, Hui Li, Kaipeng Zhang, and Yang You.",
                "venue": "arXiv preprint arXiv:2310.05773, 2023.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Generalized large-scale data condensation via various backbone and statistical matching.",
                "author": "Shitong Shao, Zeyuan Yin, Muxin Zhou, Xindong Zhang, and Zhiqiang Shen.",
                "venue": "arXiv preprint arXiv:2311.17950, 2023.",
                "url": null
            }
        },
        {
            "12": {
                "title": "Dataset quantization.",
                "author": "Daquan Zhou, Kai Wang, Jianyang Gu, Xiangyu Peng, Dongze Lian, Yifan Zhang, Yang You, and Jiashi Feng.",
                "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 17205\u201317216, 2023.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Squeeze, recover and relabel: Dataset condensation at imagenet scale from a new perspective.",
                "author": "Zeyuan Yin, Eric Xing, and Zhiqiang Shen.",
                "venue": "Advances in Neural Information Processing Systems, 36, 2024.",
                "url": null
            }
        },
        {
            "14": {
                "title": "Data distillation can be like vodka: Distilling more times for better quality.",
                "author": "Xuxi Chen, Yu Yang, Zhangyang Wang, and Baharan Mirzasoleiman.",
                "venue": "In The Twelfth International Conference on Learning Representations, 2024.",
                "url": null
            }
        },
        {
            "15": {
                "title": "Dataset distillation by matching training trajectories.",
                "author": "George Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei A Efros, and Jun-Yan Zhu.",
                "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4750\u20134759, 2022.",
                "url": null
            }
        },
        {
            "16": {
                "title": "Imagenet: A large-scale hierarchical image database.",
                "author": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.",
                "venue": "In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.",
                "url": null
            }
        },
        {
            "17": {
                "title": "An image is worth one word: Personalizing text-to-image generation using textual inversion.",
                "author": "Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel Cohen-or.",
                "venue": "In The Eleventh International Conference on Learning Representations, 2022.",
                "url": null
            }
        },
        {
            "18": {
                "title": "High-resolution image synthesis with latent diffusion models.",
                "author": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.",
                "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.",
                "url": null
            }
        },
        {
            "19": {
                "title": "Language models are few-shot learners.",
                "author": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.",
                "venue": "Advances in neural information processing systems, 33:1877\u20131901, 2020.",
                "url": null
            }
        },
        {
            "20": {
                "title": "Learning transferable visual models from natural language supervision.",
                "author": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.",
                "venue": "In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.",
                "url": null
            }
        },
        {
            "21": {
                "title": "Dataset condensation via efficient synthetic-data parameterization.",
                "author": "Jang-Hyun Kim, Jinuk Kim, Seong Joon Oh, Sangdoo Yun, Hwanjun Song, Joonhyun Jeong, Jung-Woo Ha, and Hyun Oh Song.",
                "venue": "In International Conference on Machine Learning, pages 11102\u201311118. PMLR, 2022.",
                "url": null
            }
        },
        {
            "22": {
                "title": "Accelerating dataset distillation via model augmentation.",
                "author": "Lei Zhang, Jie Zhang, Bowen Lei, Subhabrata Mukherjee, Xiang Pan, Bo Zhao, Caiwen Ding, Yao Li, and Dongkuan Xu.",
                "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11950\u201311959, 2023.",
                "url": null
            }
        },
        {
            "23": {
                "title": "Cafe: Learning to condense dataset by aligning features.",
                "author": "Kai Wang, Bo Zhao, Xiangyu Peng, Zheng Zhu, Shuo Yang, Shuo Wang, Guan Huang, Hakan Bilen, Xinchao Wang, and Yang You.",
                "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12196\u201312205, 2022.",
                "url": null
            }
        },
        {
            "24": {
                "title": "Dataset condensation with distribution matching.",
                "author": "Bo Zhao and Hakan Bilen.",
                "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 6514\u20136523, 2023.",
                "url": null
            }
        },
        {
            "25": {
                "title": "Dc-bench: Dataset condensation benchmark.",
                "author": "Justin Cui, Ruochen Wang, Si Si, and Cho-Jui Hsieh.",
                "venue": "Advances in Neural Information Processing Systems, 35:810\u2013822, 2022.",
                "url": null
            }
        },
        {
            "26": {
                "title": "Neural tangent kernel: Convergence and generalization in neural networks.",
                "author": "Arthur Jacot, Franck Gabriel, and Cl\u00e9ment Hongler.",
                "venue": "Advances in neural information processing systems, 31, 2018.",
                "url": null
            }
        },
        {
            "27": {
                "title": "Neural tangents: Fast and easy infinite neural networks in python.",
                "author": "Roman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander A Alemi, Jascha Sohl-Dickstein, and Samuel S Schoenholz.",
                "venue": "In International Conference on Learning Representations, 2019.",
                "url": null
            }
        },
        {
            "28": {
                "title": "Dataset distillation with infinitely wide convolutional networks.",
                "author": "Timothy Nguyen, Roman Novak, Lechao Xiao, and Jaehoon Lee.",
                "venue": "In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021.",
                "url": null
            }
        },
        {
            "29": {
                "title": "Dataset distillation with convexified implicit gradients.",
                "author": "Noel Loo, Ramin Hasani, Mathias Lechner, and Daniela Rus.",
                "venue": "arXiv preprint arXiv:2302.06755, 2023.",
                "url": null
            }
        },
        {
            "30": {
                "title": "Coresets for data-efficient training of machine learning models.",
                "author": "Baharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec.",
                "venue": "In International Conference on Machine Learning, pages 6950\u20136960. PMLR, 2020.",
                "url": null
            }
        },
        {
            "31": {
                "title": "Adaptive second order coresets for data-efficient machine learning.",
                "author": "Omead Pooladzandi, David Davini, and Baharan Mirzasoleiman.",
                "venue": "In International Conference on Machine Learning, pages 17848\u201317869. PMLR, 2022.",
                "url": null
            }
        },
        {
            "32": {
                "title": "A unified approach to coreset learning.",
                "author": "Alaa Maalouf, Gilad Eini, Ben Mussay, Dan Feldman, and Margarita Osadchy.",
                "venue": "IEEE Transactions on Neural Networks and Learning Systems, 2022.",
                "url": null
            }
        },
        {
            "33": {
                "title": "Coresets via bilevel optimization for continual learning and streaming.",
                "author": "Zal\u00e1n Borsos, Mojmir Mutny, and Andreas Krause.",
                "venue": "Advances in neural information processing systems, 33:14879\u201314890, 2020.",
                "url": null
            }
        },
        {
            "34": {
                "title": "An empirical study of example forgetting during deep neural network learning.",
                "author": "Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey J Gordon.",
                "venue": "In International Conference on Learning Representations, 2018.",
                "url": null
            }
        },
        {
            "35": {
                "title": "Deep learning on a data diet: Finding important examples early in training.",
                "author": "Mansheej Paul, Surya Ganguli, and Gintare Karolina Dziugaite.",
                "venue": "Advances in Neural Information Processing Systems, 34:20596\u201320607, 2021.",
                "url": null
            }
        },
        {
            "36": {
                "title": "Model inversion attacks that exploit confidence information and basic countermeasures.",
                "author": "Matt Fredrikson, Somesh Jha, and Thomas Ristenpart.",
                "venue": "In Proceedings of the 22nd ACM SIGSAC conference on computer and communications security, pages 1322\u20131333, 2015.",
                "url": null
            }
        },
        {
            "37": {
                "title": "Dreaming to distill: Data-free knowledge transfer via deepinversion.",
                "author": "Hongxu Yin, Pavlo Molchanov, Jose M Alvarez, Zhizhong Li, Arun Mallya, Derek Hoiem, Niraj K Jha, and Jan Kautz.",
                "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8715\u20138724, 2020.",
                "url": null
            }
        },
        {
            "38": {
                "title": "Learning deep features for discriminative localization.",
                "author": "Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba.",
                "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2921\u20132929, 2016.",
                "url": null
            }
        },
        {
            "39": {
                "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization.",
                "author": "Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra.",
                "venue": "In Proceedings of the IEEE international conference on computer vision, pages 618\u2013626, 2017.",
                "url": null
            }
        },
        {
            "40": {
                "title": "Grad-cam++: Generalized gradient-based visual explanations for deep convolutional networks.",
                "author": "Aditya Chattopadhay, Anirban Sarkar, Prantik Howlader, and Vineeth N Balasubramanian.",
                "venue": "In 2018 IEEE winter conference on applications of computer vision (WACV), pages 839\u2013847. IEEE, 2018.",
                "url": null
            }
        },
        {
            "41": {
                "title": "Bert: Pre-training of deep bidirectional transformers for language understanding.",
                "author": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.",
                "venue": "arXiv preprint arXiv:1810.04805, 2018.",
                "url": null
            }
        },
        {
            "42": {
                "title": "Learning multiple layers of features from tiny images.",
                "author": "Alex Krizhevsky, Geoffrey Hinton, et al.",
                "venue": "2009.",
                "url": null
            }
        },
        {
            "43": {
                "title": "Tiny imagenet visual recognition challenge.",
                "author": "Ya Le and Xuan Yang.",
                "venue": "CS 231N, 7(7):3, 2015.",
                "url": null
            }
        },
        {
            "44": {
                "title": "Deep residual learning for image recognition.",
                "author": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.",
                "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.",
                "url": null
            }
        },
        {
            "45": {
                "title": "Imagenet large scale visual recognition challenge.",
                "author": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al.",
                "venue": "International journal of computer vision, 115:211\u2013252, 2015.",
                "url": null
            }
        },
        {
            "46": {
                "title": "Improved distribution matching for dataset condensation.",
                "author": "Ganlong Zhao, Guanbin Li, Yipeng Qin, and Yizhou Yu.",
                "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7856\u20137865, 2023.",
                "url": null
            }
        },
        {
            "47": {
                "title": "Dataset distillation in large data era.",
                "author": "Zeyuan Yin and Zhiqiang Shen.",
                "venue": "2023.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.07142v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4",
            "4.5",
            "4.6"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.5",
            "4.6"
        ]
    },
    "research_context": {
        "paper_id": "2403.07142v1",
        "paper_title": "One Category One Prompt: Dataset Distillation using Diffusion Models",
        "research_background": "### Paper's Motivation:\n\nThe motivation of this paper stems from the substantial data requirements for training deep neural networks, which impose considerable burdens on storage, transmission, and handling. These challenges become particularly problematic in tasks requiring repeated training, such as hyperparameter optimization. Additionally, the dissemination of large datasets brings about copyright and privacy issues, further complicating their use. The paper aims to address the crucial question of how much data can be effectively condensed without significantly sacrificing model performance, a problem referred to as Dataset Distillation.\n\n### Research Problem:\n\nThe research problem at the heart of this paper involves exploring how to construct a smaller, synthetic subset of training data that enables models to achieve performance levels competitive with those trained on the full dataset. The paper seeks to leverage advancements in diffusion models and foundation models to compress datasets more efficiently, thereby reducing the costs and complications associated with large-scale data handling. The goal is to provide a scalable solution that improves not just performance but also generalizability across different neural network architectures.\n\n### Relevant Prior Work:\n\n1. **Foundational Concepts and Early Methods:**\n   - The problem of Dataset Distillation was first formalized by Wang et al. [4], introducing the concept of reducing the training data size without significant performance loss.\n   - Prior methods have treated dataset distillation as a bi-level optimization problem, where the inner loop involves training on a distilled dataset and the outer loop refines this dataset [4, 5].\n   \n2. **Challenges and Surrogate Objectives:**\n   - The bi-level optimization process is computationally expensive and memory-intensive due to the requirement of backpropagation through the entire training process.\n   - Researchers have proposed surrogate objectives like gradient matching [5] and training trajectory matching [15] to address these computational challenges.\n\n3. **Scaling and Decoupling Strategies:**\n   - Existing methods often struggle with scaling up to larger datasets and models [13].\n   - Recent efforts have focused on decoupling the bi-level optimization into simpler single-level learning procedures to handle large datasets like ImageNet [9, 13].\n\n4. **Improving Generalizability:**\n   - The performance of distilled datasets often suffers when applied to different neural network architectures [7].\n   - Approaches like utilizing generative models [7], leveraging batch statistics from pretrained models [13], and creating synthetic samples through patch collage [9] have been explored to enhance generalizability.\n\n5. **Innovative Approaches:**\n   - Recent advances in foundation models and latent diffusion models [18] offer new avenues for dataset distillation.\n\n### Specific Contributions of the Paper:\n\n1. **Utilizing Diffusion Models for Dataset Distillation:**\n   - Demonstration of the effectiveness of diffusion models for dataset distillation, achieving unprecedented condensation rates.\n\n2. **Adapting Textual Inversion:**\n   - Application of textual inversion to dataset distillation to generate realistic and diverse image collages representing an entire image category using a single prompt.\n\n3. **Addressing Memory Overhead:**\n   - Introduction of a solution to the memory overhead associated with storing soft labels for augmentations in large-scale dataset distillation, along with an analysis of its trade-offs.\n\nThis paper, thus, builds on the existing body of work by leveraging diffusion models and foundation models to propose a novel approach (D3M) that not only aims to scale dataset distillation but also seeks to enhance cross-architecture generalization and address memory overhead issues.",
        "methodology": "The methodology section of the paper titled \"One Category One Prompt: Dataset Distillation using Diffusion Models\" introduces a novel framework for condensing large-scale datasets into compact representations without sacrificing accuracy when these representations are used to train classification or regression models. Below are the key components and innovations of this proposed method: 1. **Avoiding Bi-level Optimization**: In alignment with recent progress in data distillation, the method circumvents bi-level optimization, focusing instead on more efficient techniques. 2. **Informative Patch Prioritization and Collage Creation**: - The method extracts informative patches from training images. - These patches are utilized to create collage images, optimizing the number of images per category. This is inspired by techniques mentioned in related works. 3. **Text-to-Image Diffusion Model & Textual Inversion**: - Rather than directly storing the collage images, the method employs a text-to-image diffusion model. - The critical innovation here is the use of textual inversion techniques to generate text prompts. These prompts can then create the collage images on demand, ensuring the convenience and adaptability of the process. 4. **Classifier Training with Generated Collages**: - During the training phase of the classifier, the stored text prompts can be used with either in-house or API-based diffusion models to generate collage images. - These collage images, generated from low-dimensional textual prompts, primarily consisting of vectors (e.g., 768 scalars), facilitate efficient and effective data condensation. - Generative models help distill the most discriminative details of a category into these prompts, ensuring high-quality condensed datasets. The proposed framework, named **Dataset Distillation using Diffusion Models (D3M)**, involves the following procedural steps: 1. Extracting and prioritizing informative image patches. 2. Using text-to-image diffusion models with textual inversion to create and store prompts. 3. Generating collage images from the stored prompts. 4. Training a classifier or regressor using these collage images. In summary, D3M leverages the power of generative models to distill datasets into highly informative, low-dimensional textual prompts, making it a highly efficient and scalable approach for dataset condensation and subsequent model training.",
        "main_experiment_and_results": "Main Experiment Setup and Results: To demonstrate the effectiveness of D3M, we performed extensive experiments on several large-scale and small-scale benchmarks.\n\n**Datasets:**\n- **Large-scale benchmarks:** The ImageNet dataset was primarily used to assess the model's performance on a comprehensive scale. \n- **Small-scale benchmarks:** We utilized the CIFAR-10, CIFAR-100, and SVHN datasets to evaluate the effectiveness of our approach on smaller datasets.\n\n**Baselines:**\nThe performance of D3M was compared against several standard and advanced baselines, including:\n- Random sampling (RS)\n- Herding\n- Coreset Selection\n- Gradient Matching techniques\n\n**Evaluation Metrics:**\nWe adopted standard evaluation metrics to assess the performance, specifically focusing on:\n- Classification accuracy on the test set.\n- Distillation efficiency, i.e., how well the distilled dataset represents the original dataset.\n\n**Main Experimental Results:**\n- **ImageNet Dataset:** D3M outperformed the baselines significantly in terms of classification accuracy. Compared to traditional random sampling and other sophisticated methods, the accuracy achieved with D3M was notably higher, showcasing state-of-the-art results.\n- **CIFAR-10, CIFAR-100, and SVHN Datasets:** Similarly, D3M demonstrated superior performance on these smaller benchmarks. The distilled datasets produced by D3M captured the essential features of the original datasets more effectively than baseline methods, leading to higher classification accuracy on the test sets.\n\nOverall, the main experiments substantiate that D3M not only enhances the efficiency of dataset distillation but also ensures higher fidelity in retaining critical dataset characteristics compared to existing techniques."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To study the effect of different patch sizes in the generated collages.",
            "experiment_process": "We experimented with different patch sizes (e.g., , , , and ) in the setting of ImageNet-1k. The study was conducted in the soft-label, no augmentation, and variation scenario. The outcomes were reported in Figure 8.",
            "result_discussion": "We observed that for high patch sizes, performance diminishes, especially in the low-resolution setting of CIFAR-100. This is due to too much detail being squeezed into high patch sizes, resulting in loss of important patch information. Although increasing the patch sizes beyond in ImageNet-1k leads to better performance, the memory requirement for storing the soft labels will be more than twice as much.",
            "ablation_id": "2403.07142v1.No1"
        },
        {
            "research_objective": "To illustrate the necessity of steps 1 and 2 in the proposed framework.",
            "experiment_process": "For step 1, instead of selecting important patches, we resized and center-cropped input images to construct patches. For step 2, we compared generating training images using only the class label and an engineered prompt with the textual inversion method for the diffusion model.",
            "result_discussion": "Resizing and center-cropping input images for step 1 showed inferior performance compared to selecting important patches. Generating training images using only the class label and an engineered prompt led to a significant performance drop compared to using textual inversion, as detailed in the supplementary materials.",
            "ablation_id": "2403.07142v1.No2"
        }
    ]
}