{
    "title": "Dialectical Alignment: Resolving the Tension of 3H and Security Threats of LLMs",
    "abstract": "With the rise of large language models (LLMs), ensuring they embody the principles of being helpful, honest, and harmless (3H), known as Human Alignment, becomes crucial. While existing alignment methods like RLHF, DPO, etc., effectively fine-tune LLMs to match preferences in the preference dataset, they often lead LLMs to highly receptive human input and external evidence, even when this information is poisoned. This leads to a tendency for LLMs to be Adaptive Chameleons when external evidence conflicts with their parametric memory. This exacerbates the risk of LLM being attacked by external poisoned data, which poses a significant security risk to LLM system applications such as Retrieval-augmented generation (RAG). To address the challenge, we propose a novel framework: Dialectical Alignment (DA), which (1) utilizes AI feedback to identify optimal strategies for LLMs to navigate inter-context conflicts and context-memory conflicts with different external evidence in context window (i.e., different ratios of poisoned factual contexts); (2) constructs the SFT dataset as well as the preference dataset based on the AI feedback and strategies above; (3) uses the above datasets for LLM alignment to defense poisoned context attack while preserving the effectiveness of in-context knowledge editing. Our experiments show that the dialectical alignment model improves poisoned data attack defense by 20% and does not require any additional prompt engineering or prior declaration of \u201cyou may be attacked\u201c to the LLMs\u2019 context window.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large language models (LLMs) trained on large datasets and with significant computational resources have shown unprecedented capabilities (Brown et al., 2020  ###reference_b9###; Kaddour et al., 2023  ###reference_b31###; Zhao et al., 2023  ###reference_b87###; Bubeck et al., 2023  ###reference_b10###).\nFor instance, Claude-3\u2019s latest release exhibits tentative self-awareness111https://www.anthropic.com/news/claude-3-family  ###reference_mily### in the Needle In A Haystack eval222https://github.com/gkamradt/LLMTest_NeedleInAHaystack/tree/main  ###reference_InAHaystack/tree/main###, highlighting the importance of further enhancing the security, user-friendliness, and controllability of LLMs (See et al., 2019  ###reference_b53###; Wang et al., 2023a  ###reference_b61###; d  ###reference_b70###).\nA straightforward way to achieve these goals is to align LLMs\u2019 behavior with human feedback (Stiennon et al., 2022  ###reference_b57###; Ouyang et al., 2022  ###reference_b46###).\nExisting alignment methods first allow humans (or LLMs) to select preferred model responses based on specific criteria. Then, reinforcement learning techniques, e.g., RLHF (Ziegler et al., 2020  ###reference_b92###; Ramamurthy et al., 2022  ###reference_b50###; Stiennon et al., 2022  ###reference_b57###), RLAIF (Bai et al., 2022b  ###reference_b5###; Lee et al., 2023  ###reference_b35###; Chu et al., 2023  ###reference_b14###),\nand Direct Preference Optimization (DPO) (Rafailov et al., 2023  ###reference_b48###)\nmethods are used to train LLMs towards preference-specific behaviors. However, utilizing the existing widely used helpful, honest, harmless (3H) preference criteria to align LLMs poses potential risks as follows:\n(i) In terms of the model performance, over-optimizing 3H rewards\naccording to Goodhart\u2019s law\n333https://www.lesswrong.com/tag/goodhart-s-law  ###reference_aw###\nmay hamper truthfulness performance (Gao et al., 2022  ###reference_b22###), e.g., balancing harmlessness vs usefulness (Bai et al., 2022a  ###reference_b4###; Dai et al., 2023  ###reference_b16###);\n(ii) 3H models tend to imply that LLMs overly\nprefer human inputs in favor of self-positions\n(Xie et al., 2024  ###reference_b76###; Xu et al., 2023a  ###reference_b77###), rendering them susceptible to camouflaged red-team attacks or poisoned text in the context window (Liu et al., 2023c  ###reference_b40###; Zou et al., 2024  ###reference_b93###). The phenomenon that LLMs are highly receptive to external information is referred to as Adaptive Chameleon by Xie et al. (2024  ###reference_b76###).\nIn this paper, we introduce Dialectical Alignment (DA) to address the challenge that 3H LLMs frequently alter their answers when encountering knowledge conflicts due to their tendency to trust external input (by humans) easily. DA empowers aligned LLMs to think dialectically upon conflicting knowledge so that they can spontaneously decide whether to trust the external information or reject the poisoned contexts. We first explore two tasks representing two sides of a coin in dealing with knowledge conflicts: In-context Knowledge Editing (IKE) (Zheng et al., 2023a  ###reference_b88###) and Poisoned Context Attack (PCA) (Zhong et al., 2023  ###reference_b91###; Liu et al., 2023d  ###reference_b41###; Zou et al., 2024  ###reference_b93###) (see more details in Section 2  ###reference_.SSS0.Px2###). The existing studies mentioned above have investigated these two tasks separately, without recognizing their correlation: if an LLM is more susceptible to the external information in the context, the effectiveness of IKE improves while the success rate of PCA also increases; conversely, if a model tends to adhere to its own parametric memory, Poisoned Context Attack will be defended while the model also rejects IKE. We give intuitive examples of this fact in Figure 1  ###reference_### and Figure 2  ###reference_###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Shayegani et al. (2023  ###reference_b54###) proposed an important mode for the failure of LLMs\u2019 safety training - competing objectives. This mode points to the difficulty of balancing the safety and helpfulness of a model when the LLMs\u2019 function (e.g., should always follow instructions and be helpful) conflicts with their safety objectives (Wei et al., 2023a  ###reference_b71###; Shen et al., 2023b  ###reference_b56###). An overly security-trained LLM can easily reject innocuous user instructions (Ganguli et al., 2022  ###reference_b21###) (e.g., role-playing), while complex jailbreak instructions can easily attack a model that lacks security training. On top of this, we presented a more indistinguishable example of knowledge editing and poisoned context attacks in Figure 1  ###reference_###, which both exploit the model\u2019s property of following user instructions as well as trusting the human input but achieve different goals (edit vs. attack). Existing alignment work does not focus on the balance between these two, while our Dialectical Alignment trains models to have the ability to spontaneously make judgments for trust the input or not.\n###figure_1### In-context knowledge editing (IKE) is a novel strategy for LLMs\u2019 knowledge editing without retraining (Zheng et al., 2023a  ###reference_b88###). Compared to other parameter-updating approaches (Yao et al., 2023  ###reference_b84###), it effectively adapts the factual knowledge in language models without parameter updating and with fewer unwanted side-effects (Onoe et al., 2023  ###reference_b45###; Cohen et al., 2023  ###reference_b15###).\nDespite the effectiveness brought by in-context learning (Brown et al., 2020  ###reference_b9###), uncurated external information also introduces hazards to LLMs. When encountering knowledge conflicts between context and parametric memory (Xu et al., 2024a  ###reference_b78###), studies by Qian et al. (2023  ###reference_b47###) and Xie et al. (2024  ###reference_b76###) reveal that LLMs are more inclined towards external evidence, especially when it appears coherent and convincing. Consequently, similar to various context-based attacks (Liu et al., 2022  ###reference_b38###; Mei et al., 2023  ###reference_b43###; Liu et al., 2023c  ###reference_b40###; Toyer et al., 2024  ###reference_b60###; Schulhoff et al., 2023  ###reference_b52###), malicious users could easily exploit IKE to attack LLMs with false information (Zou et al., 2024  ###reference_b93###), which is namely poisoned context attack (Lukas & Kerschbaum, 2023  ###reference_b42###).\nIn an effort to mitigate the risk of data poisoning attacks, some defense methods are proposed for pre-trained language models (Zhang et al., 2022  ###reference_b86###; Wang et al., 2022a  ###reference_b67###; Jia et al., 2022  ###reference_b29###; Wang et al., 2022b  ###reference_b68###; Wang & Feizi, 2023  ###reference_b66###). Chen et al. (2022  ###reference_b11###) conduct a calibration study to discourage models from providing a single answer when confronted with multiple conflicting pieces of evidence. However, there is limited research addressing the potential danger associated with IKE. Our work aims to leverage the reasoning capability of LLMs to dialectically reassess the information factuality in their context window, thereby offering an aligned approach to resolving knowledge conflicts in the retrieval augmentation of LLMs."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Preliminaries",
            "text": "###figure_2### RLHF pipeline (Ziegler et al., 2020  ###reference_b92###; Stiennon et al., 2022  ###reference_b57###) is effective at aligning LLMs to human preferences, but gathering high-quality human preference labels is a key bottleneck. Bai et al. (2022b  ###reference_b5###) proposed Constitutional AI for defining a set of principles to guide LLMs in self-criticizing and improving and thus collecting preference data. RLAIF (Lee et al., 2023  ###reference_b35###)(Reinforcement Learning from AI Feedback) trains language models using preferences labeled by an AI system instead of humans. Lee et al. (2023  ###reference_b35###) indicate that RLAIF achieves comparable performance to RLHF. More proximately, AI feedback data solves the data bottleneck of LLM preference learning, but first training a reward model and then reinforcement learning is still a complex and unstable process. Rafailov et al. (2023  ###reference_b48###) propose DPO, which directly optimizes for the policy best satisfying the preferences with a simple classification objective.\nXu et al. (2024a  ###reference_b78###) categorized existing conflicts in large model knowledge into three types: context-memory, inter-context, and intra-memory conflict. Retrieval augment generation has become mainstream in existing applications of LLMs (Lewis et al., 2020  ###reference_b36###; Gao et al., 2024  ###reference_b23###). However, external information is often erroneous and noisy (Chen et al., 2023a  ###reference_b12###; Liu et al., 2023b  ###reference_b39###; Ali et al., 2020  ###reference_b2###; 2021  ###reference_b3###; 2019  ###reference_b1###), thus context-memory and inter-context conflicts increasingly impact the credibility of LLMs generated content. In our experiments, we emphasize both of these conflicts. In our scenario, inter-context conflict manifests when both correct factual information and poisoned contexts are simultaneously input into the LLM\u2019s context window. Context-memory conflict, on the other hand, arises in our experiments due to conflicts between prior knowledge memorized by the model parameters and external information, Figure 2  ###reference_### offers an intuitive example."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "To address the challenge of getting LLMs to balance IKE and PCA defense mentioned above, unlike prior alignment work that focuses on making the model helpful and harmless, and using offensive or disturbing topics as red team behaviors (Bai et al., 2022a  ###reference_b4###; Ganguli et al., 2022  ###reference_b21###), our goal is to defend more insidious red team behaviors, where they exploit the human-friendly and instruction-following nature of the aligned LLMs for the attacks (Wei et al., 2023b  ###reference_b72###; Zou et al., 2024  ###reference_b93###). Specifically, we attempt to align the LLMs to dialectically recognize the user\u2019s purpose (attack or not) and thus selectively choose to believe or reject the input in their context window. Below, we provide precise details regarding our end-to-end Dialectical Alignment framework. See Figure 3  ###reference_### for an illustration.\n###figure_3###"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Base Response Evaluation",
            "text": "Base Generate with Extra Contexts (Base)\nGenerate with Tips (Tips)\nGenerate with Base CoT (Base CoT)\nGenerate with Given No Prior Knowledge CoT (CoT-NoPK)\nGenerate with Given Prior Knowledge CoT (CoT-PK)\nPart 1: Entity Extract Extract the key concepts in the given {question} by format \\[\u2019concept1\u2019,\u2019concept2\u2019,...]\nPart 2: Long Context Generate Tell me what you know about {concept}\nPart 3: CoT Generate\nRetrieved Contexts: {context}  Dialectical KEY:Follow the steps below: 1.Judge the accuracy of the content based on context generated in Part 2; 2.Decide whether or not to refer to this content; 3. Give the\ncorrect answer\nQuery: {question}\nIn STEP 1 in Figure 3  ###reference_###, the initial LLM responds to external information and corresponding questions by the instruction with named Base in Table 1  ###reference_###, which may include poisoned context or factual evidence, or a combination of both in LLMs\u2019 context window. At the same time, a more robust LLM serves as the evaluator for these responses. In our experiments, the evaluator is GLM-4 from Zhipu AI 444https://zhipuai.cn/devday  ###reference_zhipuai.cn/devday###.\nSee Appendix  A  ###reference_### for more discussion on LLM in evaluation. The selection process identifies poisoned responses, comprising those influenced by poisoned context, resulting in incorrect answers and responses containing factual data but yielding incorrect answers. The goal of this step is to establish a baseline of not providing any hints to the model that the external information may be incorrect."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Dialectical Path Testing and Revision",
            "text": "Next, we aim to inject different reasoning strategies into the instructions to help the model verify the trustworthiness of the external information. Specifically, in STEP 2 in Figure 3  ###reference_###), we test various reasoning strategies, which are referred to as Dialectical Paths for the same external information as in STEP 1. These paths, detailed in Table 1  ###reference_###, are designed to find the most effective strategies for improving the LLM in terms of generating correct answers in different situations, i.e., better IKE and PCA defense capabilities. We follow the logic of designing these paths from simple to complex, from single to multiple turns of dialogs. Specifically, we first introduce only Tips, suggesting that external context may be incorrect first. Building on the previous research that Chains-of-Thought have been encoded in the model\u2019s parameters (Wei et al., 2023c  ###reference_b73###), we aim to investigate whether providing such tips can prompt LLMs\u2019 dialectical thinking. Then, we use the Base CoT prompt \u201clet\u2019s think step by step\u201d proposed by  Kojima et al. (2023  ###reference_b32###), without referring to explicit thinking steps. Following this, we describe CoT-NoPK, where the reasoning steps that the model is prompted to follow are clearly described, including fact-checking and filtering the poisoned context.\nHowever, in our experiments, we find that under the aforementioned dialectical paths, the LLM still tends to engage in lazy thinking, i.e., directly paraphrasing contexts from the external evidence without referencing the knowledge memoried in its parameters. Therefore, we devise a multi-turn dialog dialectical path, CoT-PK: first, we prompt the model to extract entities from the question, then based on its memory, output knowledge regarding these entities (referred to as prior knowledge in our study); finally, we prompt the model to assess the credibility of the external contexts based on this knowledge. Finally, we prompt the model to perform reasoning using the same strategy as CoT-NoPK.\nIt is worth noting that responses generated by LLMs following our specific dialectical path testing tend to be less natural (e.g., always in the format of \u201cstep 1\u2026, step 2\u2026, step 3\u2026\u201d). Thus we use another SOTA LLM to revise them to be more readable in STEP 3 (e.g., \u201cFirst, based\non the\u2026, so\u2026, lastly\u2026\u201d). We use different SOTA LLMs in the LLM feedback and revision step to prevent LLMs from self-serving bias over their own generated answers (Xu et al., 2024b  ###reference_b79###) and affecting the fairness of accuracy evaluation."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Model Supervised Fine-Tuning (SFT)",
            "text": "In STEP 4, we use the revised optimal dialectical paths\u2019 responses in the previous steps (named Dialectical Response in Figure 3  ###reference_###) to construct the dialectical supervised dataset. Subsequently, we conduct supervised fine-tuning (SFT) with the objective of imparting the model with foundational dialectical reasoning skills. Specifically, we construct a dataset using the Alpace (Taori et al., 2023  ###reference_b59###) format, i.e., {\"instruction\"; \"output\"}, where the instructions are in the format of the Base path in Table 1  ###reference_###, and the outputs use the revised responses corresponding to the cases with different external information. An example sample of the constructed dataset is illustrated in Table 5  ###reference_### in the Appendix. Through the steps above, we eschew costly human feedback in favor of utilizing AI feedback (Bai et al., 2022b  ###reference_b5###; Lee et al., 2023  ###reference_b35###) to navigate the most efficient path toward dialectical thinking and aligning LLMs to acquire this skillset."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this section, we describe our experimental details. First, based on the analogous nature of IKE and PCA in utilizing external evidence, as demonstrated in Figure 1 ###reference_###, we design a unified experimental framework to study these two tasks. Second, we compare in detail the effects of different reasoning strategies in Table 1 ###reference_### on IKE and PCA defense. Finally, we construct SFT datasets and use them for Dialectical supervised fine-tuning."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Experimental Settings",
            "text": "Tasks. Our experiments explore the ability of LLMs to give correct answers grounded in external information within the context window. Our experiments are for tasks: (1) PCA Defense: LLM needs to defend the attack of the poisoned information in their context window; (2) IKE: LLM needs to update its answer based on the factual information in the context and thus generates the correct answer. Referring to our description in Section 2 and Figure 1, these two tasks are essentially two sides of the same coin of knowledge conflicts. Therefore, we tailor these tasks simply by controlling the ratio of factual and poisoned information in the external context.\n\nModels. We selected TinyDolphin-2.8-1.1B (shortened as TinyDolphin in the paper) and Mistral-7B-Instruct-v0.2 (shortened as Mistral-7B in the paper) in our experiments. TinyDolphin is trained from TinyLlama on the Dolphin 2.8 dataset, a dataset that filters out samples such as alignments, rejected answers, etc., to fine-tune TinyLlama into an unaligned and uncensored model. Mistral-7B is a model that outperforms all other 7B models on MT-Bench and stands out as a model comparable to the 13B chat model. We select gpt-3.5-turbo-16k as the revision LLM in STEP 3 of DA and GLM-4 as the scoring model for all experiments.\n\nDatasets. We follow the poisoned dataset format of previous studies. These datasets are sampled from ImageNet, MS-MARCO (MS), and Natural Questions (NQ), and each sample consists of one question, one correct and one incorrect answer, five poisoned contexts that support the incorrect answer, and one to two factual contexts that support the correct answer, see an example in Appendix B. We utilized 300 samples to identify efficient inference paths and build the training data. Another 300 non-overlapping samples were set aside for the test set to prevent model memorization of correct answers during training. Unless specified otherwise, the results presented in the paper pertain to the test set.\n\nEvaluation. We employ GLM-4 to assess the accuracy (ACC) of the LLMs\u2019 responses based on the correct answers. We use the template in Table 7 to instruct GLM-4 to make the judgment.\n\nExperimental Variables. LLMs are vulnerable to various factors when utilizing external information to answer questions, including the temperature used during inference, the length of information in their context window, the noise in content, etc. Our experiments manage these variables. Specifically, our experiments are evaluated at two temperatures (T), 0.1 and 0.7, where higher temperatures represented higher creativity and diversity of responses. The number of poisoned contexts (PCN) ranges from 0 to 5, accompanied by whether the context window has factual contexts supporting the correct answer (FC in Table 6) and whether the factual context is located at the beginning or end of LLM\u2019s context window. Details of the experimental variables are introduced in Table 6 in Appendix C.\n\nUnified Experimental Framework for IKE and PCA Defense. Our above variable setup allows us to experiment uniformly with IKE and PCA defense tasks. Specifically, when LLM simply answers based on its memory; when LLM answers based on the factual contexts in its context window. In this process, LLM deals with knowledge conflicts between its own parametric memory and external context (called context-memory conflict by Xu et al., which is consistent with the IKE setting. When we focus on scenarios where LLM handles context-memory conflict and PCA defense. Finally, when, we focus on scenarios where LLM handles both inter-context conflict and context-memory conflict to defend PCA. In addition, we use RO to stand for reorder, which means putting the factual evidence in front of the poisoned contexts."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Finding Paths to Motivate LLMs\u2019 Dialectical Thinking",
            "text": "In this section, we explore in detail the dialectical thinking paths of LLMs when dealing with Inter-context conflict and Context-memory conflict, and we analyze the most efficient paths under different variable settings. In Table 2, we present the accuracy of the model in generating answers directly, without utilizing any external data. We provide the average ACC of the two models under the PCA defense task (i.e., ) when answering using different paths in Figure 4 and Figure 5. The results for the IKE task (i.e., only factual context provided) are detailed in Table 3. Our findings on path selection strategy are as follows:\n\nAligned LLM is more receptive to external evidence than unaligned LLM. As shown in Table 2, TinyDolphin exhibits significantly lower ACC than Mistral-7B when no external information is available. Additionally, in Figure 4 and 5, we observe that across all paths, TinyDolphin consistently has lower ACC than Mistral when and factual evidence is present in the context (represented by green and blue lines in these figures). Despite this, TinyDolphin still maintains an ACC of about when the external information contains only poisoned data (indicated by the yellow line with the \u201dWithout FC\u201d tag), while Mistral-7B\u2019s ACC is very close to zero, especially when . These results appear to contradict the findings of Xie et al. (2024), which hypothesized that larger LLMs would be more stubborn to their own parameter memories due to their enhanced memory and reasoning abilities and greater sensitivity to poisoned datasets.\n\nOur results suggest that even though a larger aligned model possesses better memory and reasoning abilities, it could be more susceptible to a poisoned data attack because of the high trust in (human) inputs.\n\nThink less but become more dialectical? The path that excels in both IKE and PCA defense seems to be non-existent. From Table 3, we observe that in the IKE task, employing complex multi-turn dialog CoT paths (CoT-PK) leads to a notable decrease in the model\u2019s answer accuracy compared to other paths. Furthermore, there are no substantial disparities among other paths with no prior knowledge. This suggests that when the model encounters a knowledge conflict and emphasizes the knowledge memoried in its parameter first, it tends to become more stubborn, which can be particularly detrimental in knowledge editing. However, upon comparing subfigures 5 and 4 with the other subfigures in Figure 4 and 5, it becomes evident that CoT-PK allows the model to better resist the attack of poisoned contexts. Conversely, the effectiveness of defense diminishes for the other paths as the amount of poisoned evidence increases. In particular, for Mistral-7B, when the number of poisoned contexts and the number of factual contexts in the model context are almost equal (PCN = 1), the paths with no prior knowledge exhibit a higher ACC (as depicted in subfigures 5-5). However, when PCA significantly surpasses the factual contexts (PNC 2), these paths result in a lower ACC, which is consistent with previous research indicating that LLMs tend to choose the side supported by more evidence (Xie et al., 2024; Xu et al., 2024a). That\u2019s why it\u2019s crucial to train LLMs to learn to dialectically adopt the optimal path based on the distribution of poisoned and factual information in the context window. We provide more detailed results in Appendix D to analyze the effects of different temperatures and the order of external evidence."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Finetuning LLM with Dialectical Data",
            "text": "Based on the previous findings, we construct the Dialectical SFT dataset using different dialectical path responses on different external evidence distributions. Specifically, when , we opt for the responses generated by the four paths without prior knowledge. When , we choose the CoT-PK responses to compile the SFT data. Notably, due to the consistently low ACC of TinyDophin, we exclusively utilize revisoned Mistral-7B\u2019s responses for constructing the training data. We constructed a total of 9,012 data in Alpaca format (see an example in Table 5  ###reference_###) and fine-tuned the model using LoRA (Hu et al., 2022a  ###reference_b24###). Figure 6  ###reference_### and Table 3  ###reference_### display the results of these fine-tuned models on the test dataset, showcasing their enhanced ability to defend PCA while maintaining strong performance in IKE tasks. Moreover, the improvement is particularly notable when PCN significantly exceeds factual context (i.e. when)."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Disscussion",
            "text": "In this work, we propose Dialectical Alignment to address the challenge that helpful, honest, and harmless LLMs are highly receptive to human input, which can result in vulnerability to poisoned data attacks (PCA). Unlike previous research, our goal is to maintain the In-context knowledge editing (IKE) performance while simultaneously enhancing the model\u2019s resilience against such attacks. To strike such a balance, we design five dialectical paths ranging from simple to complex to observe the effective reasoning strategies of LLMs when dealing with poisoned and factual contexts with their context window. Building upon this insight, we construct a dialectical dataset and conduct supervised fine-tuning of the model.\nThe resulting fine-tuned model effectively defends against PCA while ensuring IKE\u2019s effectiveness remains intact. We anticipate that our approach will provide actionable insights and solutions for enhancing retrieval augmented generation systems of LLMs."
        }
    ],
    "url": "http://arxiv.org/html/2404.00486v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.1",
            "5.2",
            "5.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "5",
            "5.1",
            "5.2",
            "5.3"
        ]
    },
    "research_context": {
        "paper_id": "2404.00486v1",
        "paper_title": "Dialectical Alignment: Resolving the Tension of 3H and Security Threats of LLMs",
        "research_background": "### Paper's Motivation\nThe paper is motivated by the need to address the limitations and security threats posed by aligning large language models (LLMs) with human feedback using the helpful, honest, harmless (3H) criteria. The increased capacity of LLMs to exhibit capabilities such as self-awareness highlights the importance of improving their security, user-friendliness, and controllability. However, current alignment methods using 3H criteria can lead to significant risks, such as reduced model truthfulness and increased susceptibility to attacks like camouflaged red-team attacks or poisoned text in the context window.\n\n### Research Problem\nThe central research problem this paper addresses is how to improve the alignment of LLMs such that they can handle knowledge conflicts when subjected to external information (e.g., poisoned contexts or camouflaged attacks) without sacrificing their performance in tasks like In-context Knowledge Editing (IKE). The challenge lies in training LLMs to discern when to trust external information and when to rely on their internal knowledge, thus mitigating security threats while maintaining beneficial behaviors.\n\n### Relevant Prior Work\n- **Large Language Models (LLMs)**: Studies have highlighted their unprecedented capabilities (Brown et al., 2020; Kaddour et al., 2023; Zhao et al., 2023; Bubeck et al., 2023).\n- **Existing Alignment Methods**: Current approaches include reinforcement learning from human feedback (RLHF) (Stiennon et al., 2022; Ouyang et al., 2022; Ziegler et al., 2020; Ramamurthy et al., 2022; Bai et al., 2022a), reinforcement learning from AI feedback (RLAIF) (Bai et al., 2022b; Lee et al., 2023; Chu et al., 2023), and Direct Preference Optimization (DPO) (Rafailov et al., 2023).\n- **Limitations of 3H Criteria**: These criteria can lead to issues such as hampered truthfulness (Gao et al., 2022) and increased susceptibility to attacks (Xie et al., 2024; Xu et al., 2023a; Liu et al., 2023b; Zou et al., 2024).\n- **Related Tasks**:\n  - **In-context Knowledge Editing (IKE)**: Techniques to modify knowledge in LLMs based on context (Zheng et al., 2023a).\n  - **Poisoned Context Attack (PCA)**: Attacks that exploit LLMs\u2019 susceptibility to external information (Zhong et al., 2023; Liu et al., 2023d; Zou et al., 2024).\n- **Adaptive Chameleon**: The phenomenon where LLMs easily trust external information, increasing their vulnerability (Xie et al., 2024).\n\n### Conclusion\nDialectical Alignment (DA) is proposed to enable LLMs to think dialectically and resolve conflicts between external inputs and internal knowledge effectively. The approach leverages AI feedback to fine-tune models, ensuring they can handle knowledge conflicts appropriately without additional prompts or defenses. Experiments demonstrate that DA improves PCA defense capabilities while minimally impacting IKE performance, making it highly valuable for real-world applications where both reliable and poisoned data might coexist.",
        "methodology": "### Methodology: Dialectical Alignment Framework\n\nTo address the challenge of balancing Interpretative Knowledge Evaluation (IKE) and Principled Content Alignment (PCA) defense in Large Language Models (LLMs), our methodology diverges significantly from prior alignment efforts. While traditional alignment techniques prioritize making the model helpful and harmless and often use offensive or disturbing topics as red team behaviors (Bai et al., 2022a; Ganguli et al., 2022), our approach is distinct in that it focuses on defending against more covert red team behaviors. These behaviors aim to exploit the human-friendly and instruction-following nature of aligned LLMs for launching attacks (Wei et al., 2023b; Zou et al., 2024).\n\nOur proposed method, Dialectical Alignment, involves a framework that enables LLMs to dialectically assess the user's purpose\u2014distinguishing between benign and malicious intents\u2014and subsequently decide whether to accept or reject the input in their context window. By doing so, the LLM can selectively filter out potentially harmful instructions while maintaining its utility for legitimate purposes.\n\n### Key Components:\n\n1. **Dialectical Recognition**: This component allows the LLM to discern the user's intent, whether it's an attempt to carry out an attack or a genuinely benign purpose. This differentiation is crucial for the selective acceptance or rejection of the input.\n\n2. **Selective Rejection Mechanism**: This mechanism enables the LLM to reject inputs that are deemed malicious based on the dialectical recognition component. This works in real-time as the input progresses through the context window.\n\n### Innovations:\n\n- **Context-Aware Filtering**: Unlike static red teaming methods, our approach dynamically evaluates context to ensure that only harmful instructions are rejected without impeding the model's overall helpfulness.\n\n- **User\u2019s Purpose Evaluation**: By focusing on the user's intent, rather than just the nature of the topic, we enhance the LLM's capacity to deal with more sophisticated and insidious attack vectors.\n\nThis innovative methodology ensures that LLMs can maintain their beneficial functions while safeguarding against subtle security threats.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\n**Experiment Details:**\n\n1. **Experimental Framework:**\n   - We designed a unified experimental framework to study two tasks: IKE (Implicit Knowledge Extraction) and PCA (Post Condition Alignment) defense, leveraging the analogous nature of their utilization of external evidence.\n\n2. **Reasoning Strategies Comparison:**\n   - Different reasoning strategies were compared in detail for their effects on IKE and PCA defense. This comparison is presented in Table 1 of the paper.\n\n3. **Construction of SFT Datasets:**\n   - SFT (Supervised Fine-Tuning) datasets were constructed and used specifically for Dialectical supervised fine-tuning, enabling the evaluation of our method.\n\n**Datasets:**\n   - The specific datasets used in the experiment were selected and constructed to support the evaluation of IKE, PCA defense, and the Dialectical Alignment approach.\n\n**Baselines:**\n   - Various reasoning strategies were used as baselines to compare their effectiveness in IKE and PCA defense.\n\n**Evaluation Metrics:**\n   - The primary metrics used to evaluate the performance of the methods included:\n     - Accuracy of IKE and PCA defense\n     - Effectiveness of reasoning strategies\n     - Improvement in model performance after Dialectical supervised fine-tuning\n\n**Main Experimental Results:**\n   - The results, as reflected in detailed comparisons (referenced in Table 1 of the paper), showcased the varying effects of different reasoning strategies on IKE and PCA.\n   - The construction and utilization of SFT datasets for Dialectical supervised fine-tuning demonstrated a significant improvement in model alignment and defense capabilities against security threats.\n   - The comparative analysis underscored the superiority of our proposed Dialectical Alignment method over traditional reasoning strategies, emphasizing its efficacy in resolving the tension of 3H (Human-Human, Human-Machine, and Machine-Machine) interactions and enhancing security defenses.\n\nThese results affirm the robustness and effectiveness of the Dialectical Alignment approach in addressing key challenges associated with large language models (LLMs)."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Investigate LLMs' ability to give correct answers based on external information in the context window, specifically focused on PCA Defense and IKE tasks to enhance defense against poisoned context attacks.",
            "experiment_process": "Experiments explore PCA Defense (defense against poisoned information) and IKE (updating answers based on factual context). Models TinyDolphin and Mistral-7B are evaluated, with gpt-3.5-turbo-16k for revision and GLM-4 for scoring. Datasets are sampled from HotpotQA, MS-MARCO, and Natural Questions, including questions, correct and incorrect answers, poisoned contexts, and factual contexts. 300 samples are used for training and 300 for testing. Variables such as inference temperature (0.1 and 0.7) and the number of poisoned contexts (0 to 5) are managed. Evaluation accuracy is assessed based on correct answers using a template instructed to GLM-4.",
            "result_discussion": "TinyDolphin shows lower ACC without external data compared to Mistral-7B. TinyDolphin's ACC improves significantly when external poisoned data is present, contradicting the hypothesis that larger LLMs are more stubborn to their memories. The optimal dialectical thinking paths vary: complex CoT paths diminish accuracy in IKE tasks but improve PCA defense. The balance between factual and poisoned contexts significantly impacts ACC. The study emphasizes training LLMs to adopt optimal paths based on the evidence distribution to defend against attacks.",
            "ablation_id": "2404.00486v1.No1"
        },
        {
            "research_objective": "Construct Dialectical SFT dataset to fine-tune LLMs using optimal dialectical paths based on previous findings to enhance defense against PCA while maintaining IKE performance.",
            "experiment_process": "Dialectical SFT dataset is created using dialectical path responses for different external evidence distributions. Four paths without prior knowledge are used for fact-to-poison ratios of \u2264 1; CoT-PK responses are used otherwise. Only revised Mistral-7B's responses are utilized due to TinyDolphin's low ACC. The dataset includes 9,012 Alpaca format data for fine-tuning with LoRA.",
            "result_discussion": "Fine-tuned models demonstrate improved PCA defense and robust IKE performance, particularly when poisoned contexts outweigh factual contexts. The tuned models exhibit enhanced capability in handling poisoned data and maintaining performance integrity.",
            "ablation_id": "2404.00486v1.No2"
        }
    ]
}