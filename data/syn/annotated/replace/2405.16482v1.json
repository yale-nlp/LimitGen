{
    "title": "DarijaBanking: A New Resource for Overcoming Language Barriers in Banking Intent Detection for Moroccan Arabic Speakers",
    "abstract": "Navigating the complexities of language diversity is a central challenge in developing robust natural language processing systems, especially in specialized domains like banking. The Moroccan Dialect (Darija) serves as the common language that blends cultural complexities, historical impacts, and regional differences. The complexities of Darija present a special set of challenges for language models, as it differs from Modern Standard Arabic with strong influence from French, Spanish, and Tamazight, it requires a specific approach for effective communication. To tackle these challenges, this paper introduces the Enron Email Dataset, a large dataset comprising around 500,000 emails generated by 150 Enron employees. This dataset is one of the largest publicly available collections of email messages that chronicle the operations within a major corporation and is frequently used to enhance language models in the analysis of professional correspondence. The Enron Email Dataset informs intent classification through the examination of various communication patterns and intents present in corporate emails, addressing the critical need for automatic systems capable of processing and categorizing emails effectively. We experimented with various intent classification methods, including full fine-tuning of monolingual and multilingual models, zero-shot learning, retrieval-based approaches, and Large Language Model prompting. One of the main contributions of this work is BERTouch, our BERT-based language model for intent classification, although results remain specific to other datasets, as we explored benchmarking beyond Enron for comparison.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The field of Natural Language Processing (NLP) has gained significant traction, particularly with the emergence and advancement of Large Language Models (LLMs). This surge in interest and implementation is especially notable in industries where customer engagement and relationships are critical. In the realm of retail banking, Financial NLP and LLMs are reshaping the dynamics of client interactions, fostering enhanced personalization, responsiveness, and ultimately, customer loyalty and satisfaction (et al., 2020  ###reference_b21###). In the ecosystem of generative AI-enhanced customer service, powerful LLMs such as GPT-4 (OpenAI, 2023a  ###reference_b38###) act as the brain of the system, orchestrating various components to deliver nuanced and contextually relevant interactions (et al., 2023b  ###reference_b22###). Among these components, Retrieval Augmented Generation (RAG) (Lewis, 2020  ###reference_b31###) stands out by augmenting the agents\u2019 responses with information retrieved from a dense knowledge base, thereby enriching the quality and relevance of the interactions. However, the linchpin in this sophisticated setup is the Intent Classification Module, a critical aspect of Financial NLP\u2019s Natural Language Understanding (NLU).\n\nIntent classification, also known as Intent Detection, focuses on deciphering the semantic essence of user inputs to elicit the most appropriate response. It involves creating a linkage between a user\u2019s request and the corresponding actions initiated by the chatbot, as outlined by Adamopoulou (2020  ###reference_b3###). Typically framed as a classification challenge, this process entails associating each user utterance with one or, in certain cases, multiple intended actions. The task of intent classification presents notable difficulties. Conversations with chatbots often involve terse utterances that offer minimal contextual clues for accurate intent prediction. Furthermore, the extensive variety of potential intents necessitates a vast dataset for annotation, complicating the detection process due to the expansive label space that must be managed.\n\nIn this paper, we introduce DarijaBanking, a comprehensive Darija intent dataset and embark on a systematic comparison of diverse intent classification methodologies, encompassing full fine-tuning of both monolingual and multilingual models, zero-shot learning, retrieval-based strategies, and Large Language Model (LLM) prompting. DarijaBanking is meticulously curated from three foundational English banking datasets, namely:\n\nEnron Email Dataset (Casanueva, 2020  ###reference_b10###), which provides a corpus of over 600,000 emails from Enron employees,\nBanking-Faq-Bot dataset (Patel, 2017  ###reference_b40###), which includes 1,764 questions distributed across 7 intent categories, and\nSmart-Banking-Chatbot dataset (Lakkad, 2018  ###reference_b30###), encompassing 30,100 questions sorted into 41 intents, cumulatively aggregating to a rich repository of 42,000 questions.\n\nThe initial phase involved a rigorous cleaning process aimed at intent unification across these diverse sources, addressing various challenges such as generic or colloquially dense sentences, redundancy, and contextual incompatibilities with the Moroccan banking sector. This meticulous refinement resulted in a distilled collection of 1,800 English sentences, subsequently translated into French and Modern Standard Arabic (MSA) using the OPUS MT (Tiedemann and Thottingal, 2020  ###reference_b50###) and Turjuman (Nagoudi, 2022  ###reference_b35###) models, respectively. Subsequently, a crucial translation step involved leveraging GPT-4 (OpenAI, 2023a  ###reference_b38###) to translate these English sentences into Darija.\n\nTo ensure the highest level of accuracy and contextual relevance, particularly for the Darija translations, we employed a team of five external human native speakers as annotators. The annotators conducted a thorough manual verification and correction process, focusing on the nuanced aspects of the Moroccan dialect that automated systems might overlook. Their invaluable contributions were instrumental in refining the dataset to accurately reflect the linguistic intricacies of Darija, thereby enhancing the dataset\u2019s utility for our intended applications. It is noteworthy that all utterances underwent manual review, with approximately 47% of them were edited to further ensure their accuracy and idiomatic appropriateness.\n\nThe resultant dataset, featuring a total of 7,200 queries across English, French, MSA, and Darija comprising of 1,800 queries for each language. The MSA and Darija subsets serve as the foundation for assessing various intent classification methods, including full fine-tuning of monolingual and multilingual models, zero-shot learning, retrieval-based approaches, and Large Language Models prompting.\n\nThe main contributions of our paper can be succinctly summarized as follows: The DarijaBanking dataset is introduced, serving as a novel resource for banking intent detection in Darija, featuring over 7,200 multilingual queries across"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "In the realm of NLP, the Arabic language presents unique challenges, primarily due to its rich dialectical diversity and the scarcity of domain-specific labeled datasets. This scarcity significantly impedes the advancement of Arabic NLP applications, including intent detection and conversational systems. Recent research endeavors, as documented by Darwish (2021  ###reference_b12###) and Naser-Karajah (2021  ###reference_b36###), highlight the acute shortage of labeled datasets for Arabic, especially for dialectal and specialized tasks. Similarly, (Fuad, 2022  ###reference_b24###) and (Ahmed, 2022  ###reference_b4###) underscore the resultant stagnation in the development of Arabic conversational machine learning systems, attributing this lag to the inadequate dataset resources. Furthermore, there has been a recent surge in interest in Arabic NLP, as evidenced by the shared task (Malaysha, 2024  ###reference_b33###), which aims to foster further research and development in this area.\nDespite these challenges, some strides have been made towards understanding and processing the Arabic language more effectively. A pioneering step in the domain of Arabic intent detection was taken by (Mezzi, 2022  ###reference_b34###), who introduced an intent detection framework tailored for the mental health domain in Tunisian Arabic. Their innovative approach involved simulating psychiatric interviews using a 3D avatar as the interviewer, with the conversations transcribed from audio to text for processing. The application of a BERT encoder and binary classifiers for each mental health aspect under investigation\u2014depression, suicide, panic disorder, social phobia, and adjustment disorder\u2014yielded impressive results, achieving an F1 score of 0.94.\nExploring other facets of Arabic NLP, (Hijjawi, 2013  ###reference_b25###) ventured into the classification of questions and statements within chatbot interactions, leveraging decision trees for this task. Their methodology was later integrated into ArabChat (Hijjawi, 2014  ###reference_b26###), enhancing the system\u2019s ability to preprocess and understand user inputs. Moreover, (Joukhadar, 2019  ###reference_b29###) contributed to the field by creating a Levantine Arabic corpus, annotated with various communicative acts. Their experiments with different classifiers revealed that the Support Vector Machine (SVM), particularly when utilizing 2-gram features, was most effective, achieving an accuracy rate of 0.86.\nThe quest for understanding Arabic speech acts and sentiments led Elmadany (2018  ###reference_b16###) to develop the ArSAS dataset, encompassing around 21K tweets labeled for speech-act recognition and sentiment analysis. The dataset, marked by its categorization into expressions, assertions, questions, and sentiments (negative, positive, neutral, mixed), provided a fertile ground for subsequent studies. Utilizing ArSAS, Algotiml (2019  ###reference_b5###) employed Bidirectional Long-Short Term Memory (BiLSTM) and SVM to model these nuanced linguistic features, achieving an accuracy of 0.875 and a macro F1 score of 0.615. Lastly, Zhou (2022  ###reference_b51###) demonstrated the potential of contrastive-based learning to enhance model performance on out-of-domain data, testing their methodology across several datasets, including the banking domain (Casanueva, 2020  ###reference_b10###), and showed that it\u2019s possible to improve adaptability without sacrificing the accuracy for in-domain data.\nResearch on intent detection has progressed beyond Arabic, with notable studies in languages like Urdu and Indonesian. Shams (2019  ###reference_b47###) translated key datasets to Urdu and employed CNNs, LSTMs, and BiLSTMs, finding CNNs most effective for ATIS with a 0.924 accuracy and BiLSTMs best for AOL at 0.831 accuracy. This work was further refined to achieve a 0.9112 accuracy (Shams, 2022  ###reference_b48###). Similarly, in Indonesian, Bilah (2022  ###reference_b8###) utilized ATIS to inform their CNN model, achieving a 0.9584 accuracy.\nExpanding the scope, Basu (2022  ###reference_b7###) explored a meta-learning approach with contrastive learning on Snips and ATIS datasets for diverse domains, emphasizing the complexity of intent detection across different contexts.\nAddressing the gap in Arabic intent detection, Jarrar et al. (2023  ###reference_b28###) introduced ArEnronEmailDataset, an arabized dataset from Enron Email Dataset, enhanced with MSA and Palestinian dialect queries, totaling 31,404 queries across 77 intents. A BERT-based model finetuned on this dataset achieved F1-scores of 0.9209 for MSA and 0."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "The DarijaBanking Corpus",
            "text": "The DarijaBanking Corpus111https://github.com/abderrahmanskiredj/DarijaBanking represents a novel endeavour to tailor and enrich banking-related linguistic resources specifically for the Moroccan context, leveraging the foundational structures of three significant English datasets, namely: (1) Enron Email Dataset, a dataset consisting of approximately 600,000 emails from the executives of Enron; (2) the banking-faq-bot dataset (Patel, 2017  ###reference_b40###), comprising 1,764 questions distributed across seven intent categories; and (3) the smart-banking-chatbot dataset (Lakkad, 2018  ###reference_b30###), that includes a broad spectrum of 30,100 questions sorted into 41 intents. Collectively, these resources amalgamate into a comprehensive repository of 42,000 questions.\nSubsequently, we will detail the various stages involved in our corpus\u2019s data collection and validation process.\nThe first step in developing the DarijaBanking corpus was a rigorous cleaning process tailored to align the dataset with the nuances of Morocco\u2019s banking sector. This essential phase focused on eliminating queries and intents associated with banking practices and products common in countries like the US or UK but irrelevant or nonexistent in Morocco. We aimed to exclude references to unfamiliar banking services within the Moroccan banking environment, such as specific types of loans, investment opportunities, or account functionalities unavailable in local banks. For instance, intents related to \"Apple Pay or Google Pay,\" \"Automatic Top-Up,\" \"Disposable Card Limits,\" \"Exchange Via App,\" \"Get Disposable Virtual Card,\" \"Topping Up by Card,\" and \"Virtual Card Not Working\" were removed due to their limited relevance to Moroccan banking users. This is because the penetration of digital wallet services such as Apple Pay and Google Pay is not as extensive in Morocco, making these services less applicable. Additionally, the concept of automatic top-ups, disposable virtual cards, and app-based foreign exchange are indicative of fintech advancements not fully adopted or supported by Moroccan banking institutions, which tend to offer more traditional services. The use of virtual cards and card-based account top-ups, while increasing worldwide, might not yet align with common banking practices or the digital infrastructure in Morocco.\nThe cleaning also involved a critical step of utterance-level filtering to bolster the corpus\u2019s relevance to the Moroccan banking context, by eliminating references to:\nTransactions linked to UK bank accounts, e.g., \"I made a transfer from my UK bank account.\"\nQueries on services for non-residents, e.g., \"Can I get a card if I\u2019m not in the UK?\"\nThe use of foreign currencies and credit cards, e.g., \"I need Australian dollars,\" \"Can I use a US credit card to top up?\"\nInternational transfers and related issues, e.g., \"How long does a transfer from the UK take?\" \"My transfer from the UK isn\u2019t showing.\"\nRemoving utterances involving international banking contexts, uncommon currency conversions in Morocco, or services related to foreign accounts made the corpus more reflective of Moroccan banking scenarios.\nA subsequent challenge was addressing ambiguous intent clusters that hindered clear intent detection. Clusters such as \"card not working\" and \"compromised card\" exemplified the issue, with the former combining intents like \"activate_my_card\" and \"declined_card_payment,\" and the latter grouping \"compromised_card\" with \"lost_or_stolen_card.\" Similarly, \"transfer problems\" and \"identity verification\" clusters highlighted the difficulty in distinguishing between closely related intents, such as \"failed_transfer\" and \"transfer_not_received_by_recipient,\" or \"unable_to_verify_identity\" and \"verify_my_identity,\" respectively. These clusters, among others, demonstrated the substantial overlap and frequent misclassification of intents, complicating the dataset\u2019s utility for accurate intent recognition.\nTo mitigate this, a strategic decision was made to refine the dataset by merging similar intents and eliminating those prone to classification errors. This consolidation aimed to simplify the intent detection process, enhancing the model\u2019s accuracy by focusing on broader, more distinguishable intent categories.\nFurthermore, deduplication was a key step in refining the DarijaBanking corpus, targeting nearly identical queries with slight wording differences but the same intent, such as inquiries about SWIFT transfers or using Apple Pay. This step improved the dataset\u2019s precision, aiding the intent detection model in more accurately classifying customer intents in Morocco\u2019s banking sector.\nThrough detailed refinement, including deduplication and correction of incorrect utterance-intent associations, we developed a polished collection of 1,660 English sentences, significantly enhancing the dataset\u2019s utility for accurate intent recognition within the Moroccan banking context.\nIn the subsequent phase, we expanded the DarijaBanking corpus by incorporating two additional intent categories: In-Domain Out-Of-Scope (IDOOS) and Out-Of-Domain Out-Of-Scope (OODOOS). IDOOS encompasses requests that, while not previously listed, remain pertinent to the banking"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Data Collection",
            "text": "The Data Collection comprises 4 phases:\n\nThe first step in developing the dataset was a comprehensive process of selecting and organizing the Enron Email Dataset. This phase involved identifying a representative sample of emails encompassing different departments, hierarchical levels, and typical communication scenarios within the Enron corporation. The focus was to annotate emails based on thematic content, including business negotiations, internal memos, employee exchanges, and client communications, to ensure that the dataset reflected the varied nature of email communication within a corporate structure. The intricacy of filtering through the dataset to remove non-relevant emails and ensuring a balanced representation of various categories made this step pivotal. Particular efforts were made to anonymize sensitive information to respect privacy concerns while maintaining the integrity of the communication context.\n\nThe next critical step involved categorizing intents from the communication data to support distinct tasks and identify sentiment or thematic elements. Addressing ambiguous intent clusters was crucial for effective dataset organization. For example, differentiating between \"project updates\" vs. \"urgent project issues\" required parsing emails for contextual keywords and categorizing them accurately. Such differentiation facilitated clearer thematic understanding, helping algorithms process and classify content with higher precision.\n\nMoreover, deduplication was a necessary step in refining the dataset, ensuring that repeated or formulaic email exchanges, such as meeting confirmations or status updates, did not skew the analytical models being trained. This phase enhanced the dataset\u2019s precision, aiding in more accurate thematic recognition and enabling models to focus on unique communication nuances within the email exchanges.\n\nThrough detailed organization, including deduplication and careful thematic categorization, we developed a robust collection of processed emails, significantly enhancing the dataset\u2019s utility for research into business communication patterns and sentiment analysis in the corporate environment.\n\nIn the subsequent phase, we expanded the dataset by incorporating hypothetical communication scenarios that can occur in an office setting, helping to broaden the dataset\u2019s scope and applicability. This expansion aimed to ensure the model can recognize and manage a wider range of communication inquiries, enhancing analytical breadth and accuracy.\n\nFor each of these new categories, we integrated additional data generated through synthetically created emails designed to mimic real-world office communication scenarios, enriching the dataset\u2019s capacity for research into diverse communication patterns.\n\nIn this phase, we employed advanced linguistic models to ensure the subset of communications was accurately processed, emphasizing thematic precision while contextualizing the dataset within a professional setting. The focus here was not only on linguistic accuracy but also on ensuring contextual relevance across typical corporate communication scenarios. Understanding the specific jargon and operational terminology was crucial for the dataset\u2019s relevance and utility in analyzing corporate communications.\n\nThe process of manual correction and verification was meticulously conducted by a team of analysts familiar with corporate communication, ensuring that the interpretation of thematic elements and sentiment was accurately captured. This step was pivotal in refining the dataset\u2019s reliability for practical applications in sentiment analysis and organizational communication studies. The corrections made ensured that interpretations were not only linguistically accurate but also contextually resonant, aligning with the expected norms and practices in business communications."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Comprehensive Intent Catalogue",
            "text": "Table 2 delineates a comprehensive list of topics featured in the corpus, along with their corresponding definitions. The detailed overview serves to illustrate the dataset\u2019s scope concerning internal communications and interactions specific to corporate environments. In total, the corpus comprises a wide range of topics, reflective of corporate discussions. The number of email exchanges per topic varies widely, indicating a diverse distribution across the dataset."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Data Segmentation and Descriptive Analysis",
            "text": "The final dataset emerged as an extensive collection, comprising 7,200 queries distributed among English, French, Arabic, and Darija languages, with a concentrated segment of 3,600 queries specifically in Arabic and Darija, spanning across 24 distinct intents. The division of the dataset for training and testing purposes followed an 80:20 ratio, respectively, opting not to designate a separate validation subset due to the manageable size of the dataset. This stratified split was meticulously designed to maintain the original proportion of utterances per intent, ensuring that these ratios were consistently mirrored in both the training and testing subsets. Table 3  ###reference_### provides detailed statistics on the Enron Email Dataset, offering insights into the dataset\u2019s composition and the distribution of utterances among the various intents."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Intent Detection Approaches",
            "text": "In this section, we embark on a systematic comparison of diverse methodologies for intent detection, which can broadly be categorized into three distinct approaches: BERT-like models finetuning, retrieval-based strategies, and LLM prompting. The finetuning of BERT-like models Devlin (2019  ###reference_b13###) includes both monolingual and multilingual variations, where we specifically focus on adapting monolingual Arabic models for Arabic and Moroccan Darija, and employ cross-lingual transfer techniques with multilingual models like XLM-RoBERTa (Conneau, 2020  ###reference_b11###) across English, French, Arabic, and Darija datasets. Additionally, we explore zero-shot learning scenarios to evaluate model performance on languages or dialects not explicitly included in the training phase. Meanwhile, the retrieval-based strategy leverages text embedding models to index and match queries to the nearest utterance, thus inferring intent based on semantic similarity. Lastly, LLM prompting involves the utilization of advanced models such as ChatGPT-3.5 OpenAI (2022  ###reference_b37###) and GPT-4 (OpenAI, 2023a  ###reference_b38###), which are prompted to classify intents by providing a comprehensive list of intents and their descriptions. Each of these approaches offers a unique perspective on intent detection, highlighting the versatility and adaptability of current technologies to understand and classify user intents across a range of languages and contexts.\nText embedding models, particularly those inspired by the BERT architecture, have been central to advancements in NLP. However, traditional BERT models do not directly compute sentence embeddings, which poses challenges for applications requiring fixed-length vector representations of text. A common workaround involves averaging the output vectors or utilizing the special CLS token\u2019s output, though these methods often result in suboptimal sentence embeddings. To address these limitations, Sentence-BERT (Reimers, 2019  ###reference_b41###) was introduced as an adaptation of the BERT framework, incorporating siamese and triplet network structures to generate semantically meaningful sentence embeddings. These embeddings can then be compared using cosine similarity, offering a significant boost in efficiency and effectiveness across various sentence-level tasks.\nSeveral models have emerged to support multilingual and dialect-specific embeddings, crucial for applications involving Arabic and its dialects:\nSentence Transformers and Knowledge Distillation: The approach taken by models such as the one described by (Reimers, 2020a  ###reference_b42###) combines the strength of Sentence Transformers with the concept of knowledge distillation. Here, a monolingual \"teacher\" model guides a \"student\" model to produce comparable sentence embeddings across languages, facilitated by training on parallel corpora. Within this family of models, several have been specifically considered for their ability to handle Arabic. These include distiluse-base-multilingual-cased-v1 (Reimers, 2020b  ###reference_b43###) which operates within a 512-dimensional dense vector space; paraphrase-multilingual-mpnet-base-v2 (Reimers, 2020d  ###reference_b45###), offering embeddings in a 768-dimensional space; and paraphrase-multilingual-MiniLM-L12-v2 (Reimers, 2020c  ###reference_b44###), which provides embeddings in a 384-dimensional dense vector space.\nLaBSE (Language-Agnostic BERT Sentence Embedding): Google\u2019s LaBSE model (Feng, 2020  ###reference_b23###) represents a significant leap forward, blending techniques for monolingual and cross-lingual embedding generation. By integrating masked language modeling, translation language modeling, and other advanced methods, LaBSE achieves robust multilingual and dialectal coverage. LaBSE offers embeddings in a 768-dimensional space.\nLASER (Language-Agnostic SEntence Representations): Developed by Meta, LASER (et al., 2019  ###reference_b19###) employs a BiLSTM encoder with a shared BPE vocabulary across languages. This model excels in generating language-agnostic sentence embeddings, supported by extensive parallel corpora training. LASER offers embeddings in a 1024-dimensional space.\nE5 Model: The E5 model (et al., 2022  ###reference_b18###) introduces a contrastive training approach with weak supervision, yielding embeddings that excel in retrieval, clustering, and classification tasks across languages, including zero-shot and finetuned scenarios. Within this family of models, the base model with its 12 layers was used. It provides embeddings in a 768-dimensional space.\nOpenAI\u2019s text-embedding-3-large (OpenAI, 2023b  ###reference_b39###): This model represents OpenAI\u2019s latest advancement in text embedding technology. It\u2019s designed for high performance and flexibility, offering embeddings with up to 3072 dimensions. This model is particularly notable for its enhanced multilingual performance, making it a valuable tool for tasks requiring a nuanced understanding of text across various languages.\nDespite the advancements in text embedding models and their application to a multitude of languages, it is important to acknowledge a significant limitation when it comes to handling Arabic and its diverse dialects. The aforementioned models, while multilingual and capable of supporting Arabic to a degree, are not specifically designed with Arabic and its dialects as a primary focus. Their training datasets predominantly encompass MSA, with limited or, in some cases, no exposure to the various dialects spoken across the Arab world. This gap underscores a critical challenge in the field of NLP: the development of embedding models that can accurately capture the nuances and variations inherent in dialectal Arabic."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "BERT-like Models Finetuning",
            "text": "The BERT model, developed by Google in 2018, has revolutionized the field of NLP by introducing a transformer-based architecture that excels in a wide range of common language tasks, including sentiment analysis, named entity recognition, and question answering (Devlin, 2019  ###reference_b13###). The core of BERT\u2019s architecture is the transformer, utilizing an attention mechanism that efficiently learns contextual relationships between words in a given sequence. This architecture comprises two primary components: an encoder that processes the input text, and a decoder that generates predictions for the task at hand, such as masked token prediction or next-sentence prediction. This innovative approach has enabled BERT to achieve state-of-the-art performance across various NLP benchmarks.\nIn the context of this paper, we finetune a pre-trained BERT model for intent detection. To adapt BERT for the intent classification task, a single linear layer is appended to the pre-existing transformer layers of the BERT model. This modification allows for the direct application of BERT\u2019s contextual embeddings to the task of intent detection, leveraging its understanding of language nuances to achieve high accuracy.\nGiven the linguistic diversity and complexity of the Arabic language and its dialects, the performance of multilingual pre-trained transformers, including BERT, often varies. Recognizing this challenge, researchers have developed several BERT-like models tailored to Arabic and its dialects. These models have been pre-trained on vast corpora of Arabic text, encompassing both MSA and various regional dialects, to capture the rich linguistic features unique to Arabic.\nAmong these models, AraBERT (Antoun, 2020  ###reference_b6###) stands out, having been trained on substantial Arabic datasets, including the 1.5 billion-word Abu El-Khair corpus and the 3.5 million-article OSIAN corpus. Similarly, ARBERT and MARBERT (Abdul-Mageed, 2021  ###reference_b2###), as well as MARBERTv2, have been trained on extensive collections of MSA and dialectical Arabic texts, with MARBERTv2 benefiting from even more data and longer training regimes. QARiB (Abdelali, 2021  ###reference_b1###) represents another significant contribution, developed by the Qatar Computing Research Institute (QCRI) and trained on a diverse mix of Arabic Gigaword, Abu El-Khair Corpus, and Open Subtitles. Lastly, CAMeLBERT-Mix (Inoue, 2021  ###reference_b27###) incorporates a broad spectrum of MSA and dialectical Arabic sources, including the Gigaword Fifth Edition and the OpenITI corpus, to create a model that is well-suited for a wide range of Arabic NLP tasks.\nBuilding on the landscape of BERT-like language models, XLM-RoBERTa (Conneau, 2020  ###reference_b11###) emerges as a pivotal development, extending the capabilities of language understanding beyond the realm of single-language models. XLM-RoBERTa is architected on the robust foundation of RoBERTa (Liu, 2019  ###reference_b32###), leveraging a Transformer-based framework to foster deep contextual understanding across languages. Trained on an expansive corpus encompassing texts from 100 different languages, XLM-RoBERTa utilizes a large-scale approach without the need for parallel corpora, focusing on the Masked Language Model (MLM) objective to predict randomly masked tokens within a text for profound contextual comprehension. Additionally, it incorporates the Translation Language Model (TLM) objective in specific training setups, where it learns to predict masked tokens in bilingual text pairs, further enhancing its cross-lingual capabilities.\nIn this paper, we evaluate these Arabic pre-trained transformer models, alongside the multilingual XLM-Roberta (Conneau, 2020  ###reference_b11###) on the DarijaBanking dataset."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Retrieval-based Intent Detection",
            "text": "Retrieval-based intent detection represents a pragmatic approach towards understanding and classifying user intents. By employing sophisticated text embedding models, each utterance within the training dataset is transformed into a dense vector representation. When a new query is received, it is also embedded into the same vector space. The intent of the query is inferred by identifying the nearest utterance in the training set, where \"nearest\" is quantified by the highest cosine similarity between the query\u2019s embedding and those of the dataset utterances. This method hinges on the premise that semantically similar utterances, when properly embedded, will occupy proximate regions in the embedding space, facilitating an efficient and effective retrieval-based classification.\nText embedding models, particularly those inspired by the BERT architecture, have been central to advancements in NLP. However, traditional BERT models do not directly compute sentence embeddings, which poses challenges for applications requiring fixed-length vector representations of text. A common workaround involves averaging the output vectors or utilizing the special CLS token\u2019s output, though these methods often result in suboptimal sentence embeddings. To address these limitations, Sentence-BERT (Reimers, 2019  ###reference_b41###  ###reference_b41###) was introduced as an adaptation of the BERT framework, incorporating siamese and triplet network structures to generate semantically meaningful sentence embeddings. These embeddings can then be compared using cosine similarity, offering a significant boost in efficiency and effectiveness across various sentence-level tasks.\nSeveral models have emerged to support multilingual and dialect-specific embeddings, crucial for applications involving Arabic and its dialects:\nSentence Transformers and Knowledge Distillation: The approach taken by models such as the one described by (Reimers, 2020a  ###reference_b42###  ###reference_b42###) combines the strength of Sentence Transformers with the concept of knowledge distillation. Here, a monolingual \"teacher\" model guides a \"student\" model to produce comparable sentence embeddings across languages, facilitated by training on parallel corpora. Within this family of models, several have been specifically considered for their ability to handle Arabic. These include distiluse-base-multilingual-cased-v1 (Reimers, 2020b  ###reference_b43###  ###reference_b43###) which operates within a 512-dimensional dense vector space; paraphrase-multilingual-mpnet-base-v2 (Reimers, 2020d  ###reference_b45###  ###reference_b45###), offering embeddings in a 768-dimensional space; and paraphrase-multilingual-MiniLM-L12-v2 (Reimers, 2020c  ###reference_b44###  ###reference_b44###), which provides embeddings in a 384-dimensional dense vector space.\nLaBSE (Language-Agnostic BERT Sentence Embedding): Google\u2019s LaBSE model (Feng, 2020  ###reference_b23###  ###reference_b23###) represents a significant leap forward, blending techniques for monolingual and cross-lingual embedding generation. By integrating masked language modeling, translation language modeling, and other advanced methods, LaBSE achieves robust multilingual and dialectal coverage. LaBSE offers embeddings in a 768-dimensional space.\nLASER (Language-Agnostic SEntence Representations): Developed by Meta, LASER (et al., 2019  ###reference_b19###  ###reference_b19###) employs a BiLSTM encoder with a shared BPE vocabulary across languages. This model excels in generating language-agnostic sentence embeddings, supported by extensive parallel corpora training. LASER offers embeddings in a 1024-dimensional space.\nE5 Model: The E5 model (et al., 2022  ###reference_b18###  ###reference_b18###) introduces a contrastive training approach with weak supervision, yielding embeddings that excel in retrieval, clustering, and classification tasks across languages, including zero-shot and finetuned scenarios. Within this family of models, the base model with its 12 layers was used. It provides embeddings in a 768-dimensional space.\nOpenAI\u2019s text-embedding-3-large (OpenAI, 2023b  ###reference_b39###  ###reference_b39###): This model represents OpenAI\u2019s latest advancement in text embedding technology. It\u2019s designed for high performance and flexibility, offering embeddings with up to 3072 dimensions. This model is particularly notable for its enhanced multilingual performance, making it a valuable tool for tasks requiring a nuanced understanding of text across various languages.\nDespite the advancements in text embedding models and their application to a multitude of languages, it is important to acknowledge a significant limitation when it comes to handling Arabic and its diverse dialects. The aforementioned models, while multilingual and capable of supporting Arabic to a degree, are not specifically designed with Arabic and its dialects as a primary focus. Their training datasets predominantly encompass MSA, with limited or, in some cases, no exposure to the various dialects spoken across the Arab world. This gap underscores a critical challenge in the field of NLP: the development of embedding models that can accurately capture the nuances and variations inherent in dialectal Arabic."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Intent Detection by LLM Prompting",
            "text": "This approach leverages the capabilities of LLMs such as ChatGPT-3.5 (OpenAI, 2022  ###reference_b37###), GPT-4 (OpenAI, 2023a  ###reference_b38###), and JAIS 13B Chat (et al., 2023a  ###reference_b20###) to classify customer intents through a strategic prompting methodology. This method provides the LLM with detailed context, its role as a classifier, and an extensive list of 24 predefined intents alongside their descriptions.\nThe prompt is structured as follows:\n[[ Context: You are an advanced banking chatbot designed for a Moroccan bank, equipped to assist customers with a range of inquiries and services related to banking. Your capabilities extend from handling basic account management to addressing complex service requests. Your primary objective is to accurately discern the customer\u2019s intent from their utterances, using the list of predefined intents to provide relevant assistance or guide them to the appropriate service channel.\nHere is the list of all intents and their meanings:\nactivate_my_card: Initiate the use of a new banking card.\nage_limit: Inquire about the minimum age requirement for a service.\ncancel_order: Request to cancel a previously placed order.\n\u2026\noodoos: An intent not in the list of intents and not related to banking, like asking the distance between the Earth and the Moon.\nWhen you receive the 5 utterances from a customer, analyze the content to determine the most applicable intents. Consider the context of banking practices in Morocco, including services and customer expectations.\nInstructions:\n1. Read the customer\u2019s utterances carefully.\n2. Identify the most relevant intent for each utterance from the predefined list.\n3. Return the detected intents in JSON format for easy parsing:\n\u201c\u2018\n{\"intents\": [\"intent1\", \"intent2\", \"intent3\", \"intent4\", \"intent5\"]}\n\u201c\u2018\nMake sure to return only one intent for each utterance. Select the intent that best matches the customer\u2019s query or service need for each of the five utterances. If an utterance does not fit any predefined intents or falls outside the banking domain, use \"oodoos\" for unrelated queries and \"idoos\" for banking-related queries not listed among the predefined intents.]]\nGiven the complexity and the need for efficiency, multiple utterances were classified within the same prompt rather than individually, optimizing both cost and computational resources.\nFor JAIS 13B Chat, the approach was similar, with the adaptation being the translation of the prompt into Arabic and adjusting the formatting to suit the specifics of JAIS prompting schema."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Intent Detection Through NMT Pipeline",
            "text": "One effective strategy that balances intuition and competitiveness is to employ a neural machine translation (NMT) pipeline. This approach entails translating text from a low-resource language (LRL) to a high-resource language (HRL) before executing natural language processing tasks. Research by (Song, 2023  ###reference_b49###) demonstrated that integrating a translation pipeline can yield high-quality NLP outcomes for low-resource languages. In our specific scenario, we could translate Darija queries into English and subsequently apply English-based Intent Detection models for further analysis."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments and Results",
            "text": "In this section, we present a comprehensive analysis of our experimental studies, conducted to assess the effectiveness of the three distinct approaches previously discussed. Specifically, in subsection 5.1, we delve into the nuances of fine-tuning BERT-like models. This subsection is bifurcated into an examination of both the application of zero-shot learning techniques and the comprehensive fine-tuning process. Subsequently, subsection 5.2 is dedicated to exploring the Retrieval-based Intent Detection method, wherein we benchmark the various models listed to evaluate their performance. In subsection 5.3, we shift our focus to investigating intent detection through the lens of LLM Prompting techniques, offering a fresh perspective on this approach. The culmination of our experimental journey is presented in subsection 5.4, where we discuss the outcomes of our investigations, aiming to extract meaningful insights and implications from the gathered data. To provide a robust evaluation of model efficacy, we report performance metrics on the test set, including macro F1 scores, precision, and recall, thereby ensuring a holistic assessment of each model\u2019s capabilities."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "BERT-like models Finetuning Results",
            "text": "Zero-Shot Cross-Lingual Transfer Learning:\nThis section explores the efficacy of zero-shot cross-lingual transfer learning on MSA and Darija, employing the XLM-Roberta model. We conduct experiments to assess the performance of this approach using the DarijaBanking English + French training dataset. Initial results revealed an F1-score of 74.06 for MSA and 47.61 for Darija (Table 4  ###reference_###). Further experimentation, which included fine-tuning XLM-R with an enriched training dataset comprising English, French, and MSA, yielded improved F1-scores of 93.10 for MSA and 80.76 for Darija. The most comprehensive training set, encompassing English, French, Arabic, and Darija, allowed XLM-R to achieve F1-scores of 95 for MSA and 93.64 for Darija. These findings underscore the challenges faced by multilingual pre-trained models in accurately capturing the nuances of MSA and dialectical Arabic, highlighting the necessity for dedicated data annotations in these languages.\nComprehensive Fine-tuning of Pre-Trained Transformers:\nIn the preceding section, we observed that multilingual pre-trained transformers exhibit suboptimal performance on both MSA and Darija, with Darija presenting particular challenges. This section extends our evaluation to a range of Arabic pre-trained transformer models that were previously introduced in section 4.1  ###reference_###, including XLM-R, using the DarijaBanking dataset. We present the results in Table 5  ###reference_###, ranking the models by their F1-scores on the Darija test set.\nNotably, Arabertv02-twitter emerges as the top-performing model, achieving impressive F1-scores of 95.55 for MSA and 97.87 for Darija. The model\u2019s efficiency is particularly noteworthy given its relatively small size, which allows for deployment on standard CPUs without compromising performance."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Retrieval-based Intent Detection Results",
            "text": "In this section, we conduct a comprehensive evaluation of the retrieval-based intent detection approach, leveraging the capabilities of pre-trained text embedding models as detailed in Section 4.2  ###reference_###, with a focus on their performance using the DarijaBanking dataset. The assessment\u2019s outcomes are systematically presented in Table 6  ###reference_###, where models are ordered according to their F1 scores derived from the Darija test set.\nAmong the models evaluated, the \"text-embedding-3-large\" model by OpenAI stands out in the closed-source category, along with Microsoft\u2019s \"multilingual-e5-base\" in the open-source domain. The former demonstrates exceptional performance with F1 scores of 90.70 for MSA and 88.44 for Darija. The latter, while slightly trailing with F1 scores of 88.91 for MSA and 86.89 for Darija, offers significant advantages in terms of efficiency and deployability. Its smaller size enables deployment on standard CPUs without a loss in performance, and its open-source nature facilitates in-house deployment."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Intent Detection by LLM Prompting Results",
            "text": "In this section, we explore the application of LLMs for intent detection, with a particular focus on their performance within the context of the DarijaBanking dataset.\nDespite the sophisticated linguistic processing and generative capabilities of these models, their performance in the task of intent detection within the DarijaBanking dataset leaves much to be desired. ChatGPT-3.5, in particular, showcases a significant gap in effectiveness, delivering results that fall short of expectations. GPT-4, although slightly more proficient, still only achieves what can best be described as mediocre performance. This outcome is notably surprising, considering GPT-4\u2019s advanced language understanding and generation abilities, including its application to languages and dialects as complex as Moroccan Darija.\nThe performance of the Jais-13B model further complicates the landscape. This model demonstrates a notable difficulty in accurately aligning with the predefined set of 24 intents, often misclassifying or completely missing the correct intent. This inconsistency underscores the limitations of Jais-13B as an intent classifier despite its potential advantages in other generative applications. The evidence suggests that while Jais-13B may excel in content generation, its utility as a classifier in intent detection tasks, especially those involving the DarijaBanking dataset, is limited.\nThese findings indicate that the general-purpose nature of LLMs might not be ideally suited for specific classification tasks such as intent detection, particularly when dealing with languages or dialects with less online presence and resources. The study underscores the necessity for a more nuanced approach, suggesting that developing and fine-tuning smaller, domain-specific language models could offer a more effective solution, as shown in subsection 5.1  ###reference_###. Table 7  ###reference_### reports the results obtained"
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Intent Detection Through NMT Pipeline",
            "text": "In this part, we explore the NMT-based pipeline that leverages well-resourced English models for intent classification. The first step of this pipeline involves the automatic translation of queries from target languages, such as Darija, to English using proficient models. Subsequently, the resulting English queries are then fed into an English intent classification model.\nFor English Intent Classification, we fine-tuned a Bert-base-uncased model on the English queries extracted from our training set, employing identical hyperparameters as the other models.\nFor the evaluation phase, we employed the large multilingual hf-seamless-m4t-large model, recently introduced by Meta (Seamless Communication, 2023  ###reference_b46###), to translate the test queries from both Darija and MSA to English. Subsequently, we computed the corpus-level BLEU score between the original English queries and their translated counterparts, resulting in a score of 0.328.\nrow1 = c,\ncell12 = c=4,\ncell16 = c=4,\ncell21 = r=2,\ncell22 = c,\ncell23 = c,\ncell24 = c,\ncell25 = c,\ncell26 = c,\ncell27 = c,\ncell28 = c,\ncell29 = c,\nvlines,\nhline1-2,4 = -,\nhline3 = 2-9,\n\n& MSA split of DarijaBanking test set Darija split of DarijaBanking test set\n hf-seamless-m4t-large +\nBert-base-uncased Accuracy Precision Recall F1_score Accuracy Precision Recall F1_score\n 83,97 85,70 83,71 83,70 89.22 90.70 89.43 89.03\nThe results presented in Table 8  ###reference_### demonstrate that the NMT pipeline-based solution outperforms other models including GPT-4 for both MSA and Darija in terms of F1-score, achieving 83.70 and 89.03, respectively."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "The exploration of three different finetuning techniques for monolingual and multilingual models, in both comprehensive and zero-shot scenarios, including retrieval-based approaches and prompting with Large Language Models, for intent detection in Moroccan Darija within the banking sector, offers critical insights and implications for both future research and practical application.\nFirst and foremost, our study demonstrates that relying solely on cross-lingual transfer learning might not be the most effective approach for addressing the complexities of Moroccan Darija. The initial results reveal a significative variance in F1 scores between MSA and Darija, underscoring the need for dedicated linguistic resources tailored to dialectical Arabic. This gap significantly narrows with the comprehensive integration of languages and fine-tuning, as shown by the enhanced performance of models like Arabertv02-twitter when trained on datasets that encompass Darija. This emphasizes the value of investing in high-quality, domain-specific data labeling to improve model accuracy and efficiency, especially in bounded domains like banking where the complexity and size of the model can be balanced with targeted, high-quality data.\nFurthermore, the findings urge caution against an overreliance on powerful LLMs for specialized classification tasks such as intent detection. Despite their impressive generative capabilities, models like ChatGPT-3.5 and GPT-4 exhibit limitations in accurately classifying intents within the DarijaBanking dataset. This highlights the importance of incorporating specialized classifiers for the Intent Classification Module in banking chatbots, where precision in understanding customer queries is paramount. LLMs, while beneficial in generating human-like responses and enhancing the chatbot\u2019s conversational capabilities, should complement rather than replace dedicated classifiers finetuned for specific intents.\nHowever, the study also acknowledges the economic and logistical constraints of data labeling, recognizing that it can be prohibitively expensive or resource-intensive for some organizations. In such scenarios, retrieval-based approaches using pre-trained text embeddings, such as OpenAI\u2019s \"text-embedding-3-large\" or the \"multilingual-e5-base\" model, offer a viable alternative. These models demonstrate respectable performance and efficiency, providing a practical solution for intent detection that balances cost and accuracy.\nDespite these advancements, it is important to acknowledge the limitations of the study. The intent list used is not exhaustive, and real-world applications will likely require further adjustments to accommodate the full spectrum of banking queries. Moreover, the current approach does not account for the contextual and multi-intent nature of real-world conversations, which could provide valuable signals for more accurate intent classification."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In conclusion, this paper introduces DarijaBanking, an innovative dataset designed to enhance intent detection within the banking sector for both Moroccan Darija and MSA. Through the adaptation and refinement of the Enron Email Dataset, DarijaBanking emerges as a vital tool for nuanced language processing, comprising over 3,600 queries across 24 intent classes. Our analysis, spanning model fine-tuning, zero-shot learning, retrieval-based techniques, and Large Language Model prompting, highlights the critical need for tailored approaches in processing dialect-specific languages and underscores the effectiveness of ArabertV0.2 when finetuned on this dataset.  \nThe research emphasizes the importance of domain-specific classifiers and the limitations of relying solely on general-purpose Large Language Models for precise intent detection. It also presents retrieval-based approaches as practical, cost-effective alternatives for scenarios where data labeling poses significant economic and logistical challenges. These approaches provide a pragmatic balance between performance and resource allocation, facilitating the advancement of AI-driven solutions in settings that are linguistically diverse and resource-limited.  \nHowever, we recognize the limitations of our study, including the non-exhaustive nature of our intent list and the lack of consideration for the contextual and multi-intent dynamics of real-world interactions. These aspects offer avenues for future research to further refine and enhance the application of AI in understanding and servicing the diverse needs of banking customers.  \nBy contributing to the development of resources like DarijaBanking, this paper aims to support the broader goal of making AI technologies more adaptable and effective across various linguistic contexts. In doing so, we hope to inspire continued efforts towards creating more inclusive digital banking solutions and advancing the field of NLP."
        }
    ],
    "url": "http://arxiv.org/html/2405.16482v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.1",
            "5.2",
            "5.3",
            "5.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.3",
            "5",
            "5.1",
            "5.2",
            "5.3"
        ]
    },
    "research_context": {
        "paper_id": "2405.16482v1",
        "paper_title": "DarijaBanking: A New Resource for Overcoming Language Barriers in Banking Intent Detection for Moroccan Arabic Speakers",
        "research_background": "**Paper Title:** DarijaBanking: A New Resource for Overcoming Language Barriers in Banking Intent Detection for Moroccan Arabic Speakers\n\n### Motivation\n\nThe impetus behind this paper is driven by the significant advancements in Natural Language Processing (NLP), particularly through Large Language Models (LLMs), and their transformative impact on customer engagement across various industries, especially retail banking. As customer interactions shift increasingly towards automated systems, the demand for sophisticated intent detection mechanisms becomes paramount to ensure effective communication and enhanced customer satisfaction. However, a notable gap exists in addressing language barriers, specifically for non-standard dialects like Moroccan Arabic (Darija), within these NLP frameworks.\n\n### Research Problem\n\nThe primary research problem tackled in this paper revolves around improving banking intent detection for Moroccan Arabic speakers, a dialect that lacks adequate NLP resources. The authors aim to create and validate \"DarijaBanking,\" a comprehensive dataset specifically designed to enable the development of more accurate and contextually relevant intent classification systems for Darija-speaking banking customers. This involves:\n1. Gathering and curating multilingual banking queries and translating them into Darija.\n2. Conducting a comparative analysis of various intent classification methodologies, including fine-tuning monolingual/multilingual models, zero-shot learning, retrieval-based strategies, and LLM prompting.\n3. Developing and testing a tailored Darija-specific BERT-based model, BERTouch, for intent classification.\n\n### Relevant Prior Work\n\nThe groundwork for this research is built upon several foundational studies and advancements in the fields of NLP and intent classification:\n1. **Large Language Models and Customer Engagement**: Prior studies (et al., 2020  ###reference_b21###) have showcased how LLMs can significantly enhance personalization and responsiveness in customer interactions, which is critical for industries like retail banking.\n2. **Use of LLMs in Customer Service**: Tools such as GPT-4 (OpenAI, 2023a ###reference_b38###) have been instrumental in generating nuanced responses by leveraging Retrieval Augmented Generation (RAG) (Lewis, 2020 ###reference_b31###), thus enriching the quality of chatbot interactions.\n3. **Intent Classification Challenges**: Addressed by Adamopoulou (2020 ###reference_b3###), intent classification is complex due to terse user utterances and the vast variety of potential intents, demanding extensive annotated datasets.\n4. **Existing Banking Datasets**: The paper builds upon existing datasets like Banking77 (Casanueva, 2020 ###reference_b10###), Banking-Faq-Bot (Patel, 2017 ###reference_b40###), and Smart-Banking-Chatbot (Lakkad, 2018 ###reference_b30###), which serve as the source for the curated and translated queries used in the DarijaBanking dataset.\n\nThe introduction and related work sections help establish the context, highlighting the need for specialized resources for Darija and grounding the study in the broader advancements of NLP and its applications in banking.",
        "methodology": "### Proposed Method or Model\n\nThe methodology outlined in the DarijaBanking paper presents a comprehensive approach to developing and refining a linguistic resource tailored specifically for the Moroccan banking sector. This involves leveraging existing English datasets and meticulously adapting them to fit the Moroccan context through a series of innovative processes. Below are the key components and innovations of the proposed method:\n\n1. **Foundation on Existing Linguistic Resources**:\n   - The DarijaBanking corpus builds on three significant English datasets: Banking77, banking-faq-bot, and smart-banking-chatbot, blending their collections to form a starting repository of 42,000 banking-related questions.\n\n2. **Contextual Cleaning**:\n   - A rigorous cleaning process aligns the dataset with Morocco's banking practices and products by eliminating queries and intents irrelevant to Moroccan users, such as those referencing services like Apple Pay, automatic top-ups, and virtual cards.\n   - The process also involves filtering out utterances related to foreign banking contexts, non-resident services, and international currency transactions.\n\n3. **Addressing Ambiguous Intents**:\n   - The methodology tackles the challenge of ambiguous intent clusters by merging similar intents and eliminating those prone to misclassification, thereby simplifying the intent detection process and enhancing model accuracy.\n\n4. **Deduplication**:\n   - Identical queries with slight wording differences but the same intent are deduplicated, improving dataset precision and aiding in more accurate intent classification.\n\n5. **Incorporating New Intent Categories**:\n   - Two new categories are introduced: In-Domain Out-Of-Scope (IDOOS) and Out-Of-Domain Out-Of-Scope (OODOOS). \n   - IDOOS includes relevant but previously unlisted banking inquiries, enhancing the chatbot's capability to handle a broader range of questions.\n   - OODOOS involves entirely unrelated queries, helping to maintain the chatbot's focus on banking topics and enhancing user experience by appropriately managing non-banking questions.\n\n6. **Translation and Localization**:\n   - The refined English sentences are translated into French, Arabic, and Moroccan Darija using models such as OPUS MT, Turjuman, and GPT-4.\n   - The translation process emphasizes not just linguistic precision but also contextual alignment with Moroccan banking terminology, ensuring translations are culturally and contextually accurate.\n\n7. **Manual Correction and Verification**:\n   - A team of five native Moroccan Darija speakers manually checks and corrects the translations to enhance accuracy and idiomatic expression.\n   - Approximately 47% of translations were edited for better alignment with Moroccan Darija, with corrections ensuring culturally accurate and idiomatic language use.\n\n8. **Detailed Refinement and Quality Control**:\n   - Corrections involve standardizing terms, correcting verb conjugations, and ensuring dialectally appropriate vocabulary, ensuring translations are not just understood but feel natural to Moroccan Darija speakers.\n\nEach of these methodological components ensures that the DarijaBanking corpus is not only linguistically accurate but also culturally resonant, making it a robust resource for intent detection in the Moroccan banking sector. This innovative approach aligns the linguistic resource closely with local practices, enhancing its utility and accuracy for Moroccan Arabic-speaking users.",
        "main_experiment_and_results": "**Main Experiment Setup**\n\nIn this main experiment, the authors aimed to evaluate the effectiveness of three different approaches for intent detection in Moroccan Arabic banking scenarios. These approaches are:\n\n1. **Fine-tuning BERT-like models**: This approach involves both zero-shot learning techniques and a comprehensive fine-tuning process of pre-trained BERT models.\n2. **Retrieval-based Intent Detection**: Various models are benchmarked to evaluate their performance using this retrieval-based approach.\n3. **LLM Prompting techniques**: This approach looks into how large language model (LLM) prompting can be utilized for intent detection.\n\n**Datasets**:\n- The dataset used for this experiment is termed \"DarijaBanking,\" which comprises a collection of banking-related intents in Moroccan Arabic (Darija).\n\n**Baselines**:\n- The study compares multiple models, including fine-tuned BERT-like models, retrieval-based models, and LLM prompting techniques.\n\n**Evaluation Metrics**:\n- The performance of each model is assessed using:\n  - **Macro F1 Score**\n  - **Precision**\n  - **Recall**\n\n**Main Experimental Results**:\n- The results of the experimental studies are detailed in subsection 5.4 of the paper.\n- The authors provide performance metrics on the test set, including macro F1 scores, precision, and recall, offering a holistic assessment of each model\u2019s capabilities.\n\nThese evaluations and results help in understanding the efficacy and potential of different models and approaches in addressing language barriers in banking intent detection for Moroccan Arabic speakers."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the efficacy of zero-shot cross-lingual transfer learning and fine-tuning of pre-trained transformer models on intent classification in MSA and Darija using the DarijaBanking dataset.",
            "experiment_process": "The experiments employed the XLM-Roberta model, initially using the DarijaBanking English + French training dataset. The initial results were refined by incorporating English, French, and MSA into the training set, and ultimately, the most comprehensive training set included queries in English, French, Arabic, and Darija. The models' performances were benchmarked using F1-scores.",
            "result_discussion": "Initial F1-scores were 74.06 for MSA and 47.61 for Darija. Fine-tuning with an enriched dataset improved F1-scores to 93.10 for MSA and 80.76 for Darija. The most comprehensive dataset yielded F1-scores of 95 for MSA and 93.64 for Darija, highlighting the challenges multilingual models face in capturing MSA and dialectical Arabic nuances and the necessity for dedicated data annotations.",
            "ablation_id": "2405.16482v1.No1"
        },
        {
            "research_objective": "To examine the performance of various Arabic pre-trained transformer models for intent classification in MSA and Darija using the DarijaBanking dataset.",
            "experiment_process": "The evaluation extended to several Arabic pre-trained transformer models, including XLM-R and others as introduced in section 4.1, using the DarijaBanking dataset. The models were ranked based on their F1-scores on the Darija test set, presented in Table 5.",
            "result_discussion": "Arabertv02-twitter demonstrated the highest performance with F1-scores of 95.55 for MSA and 97.87 for Darija, balancing efficiency and deployability by supporting standard CPU deployment without compromising performance.",
            "ablation_id": "2405.16482v1.No2"
        },
        {
            "research_objective": "To assess the performance of pre-trained text embedding models in a retrieval-based intent detection approach using the DarijaBanking dataset.",
            "experiment_process": "Various pre-trained text embedding models were evaluated using the DarijaBanking dataset. The models were systematically ranked by their F1-scores derived from the Darija test set, as shown in Table 6. The models included \"text-embedding-3-large\" by OpenAI and \"multilingual-e5-base\" by Microsoft, representing both closed-source and open-source categories.",
            "result_discussion": "OpenAI's \"text-embedding-3-large\" achieved F1 scores of 90.70 for MSA and 88.44 for Darija, whereas Microsoft's \"multilingual-e5-base\" reached F1 scores of 88.91 for MSA and 86.89 for Darija. The latter model's open-source nature and efficiency for standard CPU deployment present notable advantages.",
            "ablation_id": "2405.16482v1.No3"
        },
        {
            "research_objective": "To explore the application of large language models (LLMs) for intent detection within the context of the DarijaBanking dataset.",
            "experiment_process": "The study assessed the performance of LLMs like ChatGPT-3.5, GPT-4, and Jais-13B for intent detection on the DarijaBanking dataset. The LLMs were evaluated for their ability to accurately classify intents among the 24 predefined categories.",
            "result_discussion": "ChatGPT-3.5 and GPT-4 demonstrated underwhelming performance, with GPT-4 achieving only marginally better results. Jais-13B struggled with intent alignment, often misclassifying intents. These findings indicate that general-purpose LLMs may not be well-suited for specific classification tasks such as intent detection in languages or dialects with less online presence, suggesting the need for smaller, domain-specific language models.",
            "ablation_id": "2405.16482v1.No4"
        }
    ]
}