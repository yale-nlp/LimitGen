{
    "title": "Annotations on a Budget: Leveraging Geo-Data Similarity to Balance Model Performance and Annotation Cost",
    "abstract": "Current foundation models have shown impressive performance across various tasks. However, several studies have revealed that these models are not effective for everyone due to the imbalanced geographical and economic representation of the data used in the training process. Most of this data comes from Western countries, leading to poor results for underrepresented countries. To address this issue, more data needs to be collected from these countries, but the cost of annotation can be a significant bottleneck.\nIn this paper, we propose methods to identify the data to be annotated to balance model performance and annotation costs. Our approach first involves finding the countries with images of topics (objects and actions) most visually distinct from those already in the training datasets used by current large vision-language foundation models. Next, we identify countries with higher visual similarity for these topics and show that using data from these countries to supplement the training data improves model performance and reduces annotation costs. The resulting lists of countries and corresponding topics are made available at https://github.com/MichiganNLP/visual_diversity_budget.\n\nKeywords:\u2009geo-diverse datasets, active learning, effective annotations, visual similarity, vision-language models",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "Vision-language models have shown remarkable advances in recent years (Li et al., 2019; Zhang et al., 2021; Radford et al., 2021; Zellers et al., 2021; Li et al., 2022; Kirillov et al., 2023a; Huang et al., 2023b). These models have shown great performance on a variety of tasks, from lower-level tasks such as object detection, image segmentation (Kirillov et al., 2023a), and image and video classification to higher-level tasks such as image/video captioning (Li et al., 2022; Huang et al., 2023b), text-image/video retrieval (Radford et al., 2021), visual question answering and visual commonsense reasoning (Zellers et al., 2021, 2022). At the same time, prior work has demonstrated that these models do not work well for everyone (De Vries et al., 2019). Specifically, models do not work well on out-of-domain data, and data from low-income and non-western countries Nwatu et al. (2023). This is due to the imbalanced geographical and economic representation of the data used to train these models, as it comes mainly from North America and Western Europe (Shankar et al., 2017). One solution that Rojas et al. (2022) and Ramaswamy et al. (2023) propose is to collect more data from underrepresented countries. However, as Ramaswamy et al. (2023) highlights, annotation costs are a substantial bottleneck; when crowdsourcing the data, fair pay is about 1.08$ per image without including researcher time.\n\nAs a complementary solution, we leverage the COCO Dataset to reduce the annotation budget while finding effective annotation data. Specifically, our paper aims to answer two main research questions: Which countries are less represented in the training data of vision-language models? We aim to find ways to effectively focus future annotation efforts on specific countries and their corresponding topics (objects and actions). Throughout the paper, for brevity, we use the term country to refer to a country or territory. Our study highlights the visual diversity of common topics across countries and those that differ the most from the primarily Western data used to train most multimodal foundation models. How can we leverage cross-country data similarity to improve the representation of vision-language models? We obtain groups of countries that are visually similar in their representation of a given topic from the COCO Dataset. This is particularly useful when there is not enough data for one of the countries in the group, and there is no annotation budget. We can supplement the data from this country using data from the other countries in the group.\n\nWe summarize our contributions as follows. First, we identify the data likely to most benefit from annotations by analyzing which countries and corresponding topics are less represented in the training data of vision-language models. Second, across 52 countries and 94 topics, we identify the groups of countries that are visually similar in their representation of a topic using the COCO Dataset and show that they can be used to supplement training data effectively. Third, our main takeaways create opportunities for affordable and geo-diverse data collection, encouraging contributions to creating datasets and models that work for everyone."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Related Work",
            "text": "There have been numerous studies on the use of semi-supervised models to leverage a combination of limited labeled data and vast amounts of unlabeled data to improve model performance at lower costs (Hady and Schwenker, 2013  ###reference_b31###; Oliver et al., 2018  ###reference_b50###; Taha, 2023  ###reference_b68###; Chen et al., 2022  ###reference_b10###). However, model-generated labels could be inconsistent and unrepresentative with semi-supervision, leading to reduced model performance (Ahfock and McLachlan, 2023  ###reference_b1###; Elezi et al., 2022  ###reference_b17###; Wang et al., 2021  ###reference_b73###). While similar to semi-supervised learning in objective, active learning methods seek to capture the entire data distribution by focusing labeling efforts on the data points that provide the most information for training the best-performing models (Ren et al., 2021  ###reference_b56###; Citovsky et al., 2021  ###reference_b11###; Monarch, 2021  ###reference_b48###; Yang et al., 2017  ###reference_b74###) using approaches such as uncertainty-based sampling in Gal and Ghahramani (2016  ###reference_b22###); Beluch et al. (2018  ###reference_b2###) and geometric-based methods in Sener and Savarese (2018  ###reference_b64###). Unsupervised subset selection methods like K-means and K-median core set in Har-Peled and Kushal (2005  ###reference_b33###), which form the foundation for geometric-based active learning approaches are similar to our work which seeks to select a subset that is representative of the entire dataset using distance metrics. However, the objective of the selection is to include images from a low-resource dataset with the least similarity to data of the same class in a high-resource dataset. There exists a considerable body of literature evaluating the fairness and the unequal performance of vision and vision-language models on diverse groups categorized according to race (Gebru, 2020  ###reference_b26###), gender (Buolamwini and Gebru, 2018  ###reference_b4###), geolocation (Kim et al., 2021  ###reference_b40###; Shankar et al., 2017  ###reference_b65###; Goyal et al., 2022a  ###reference_b28###) and income (De Vries et al., 2019  ###reference_b13###; Nwatu et al., 2023  ###reference_b49###). Further analysis of these disparities reveals that factors such as ambiguous label definitions, domain shifts, annotator disagreement (Hall et al., 2023  ###reference_b32###; Kalluri et al., 2023  ###reference_b39###), as well as image properties relating to texture, lighting, and occlusion in vision and vision-language datasets (Gustafson et al., 2023  ###reference_b30###) contribute to disparities in datasets which carry over to affect model performance. Frameworks have been developed to facilitate the detection of bias through guided human-in-the-loop inspection, either in datasets Hu et al. (2020  ###reference_b34###) or in models Goyal et al. (2022b  ###reference_b29###). Our work focuses on exploring the presence of variations in image representations across demographic groups in existing datasets, to inform cost-effective methods for building balanced, diverse datasets. Efforts toward improving equal representation in AI and equitable AI impact revolve around model adaptation, transfer learning, and dataset diversity. However, Salman et al. (2022  ###reference_b60###); Kalluri et al. (2023  ###reference_b39###); Dubey et al. (2021  ###reference_b16###); Wang and Russakovsky (2023  ###reference_b72###) suggest that transfer learning and model adaptation methods might not be enough to eradicate the issue of under-representation in AI models. On the other hand, adding diverse data to training datasets tends to yield significant improvements in model performance across different groups (Ramaswamy et al., 2023  ###reference_b54###; Rojas et al., 2022  ###reference_b58###). The need for more diverse datasets has become apparent, leading to the development of datasets like GeoYFCC (Dubey et al., 2021  ###reference_b16###), GeoDE (Ramaswamy et al., 2023  ###reference_b54###), Coco Dataset (Rojas et al., 2022  ###reference_b58###), and Segment Anything (Kirillov et al., 202"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   Methodology",
            "text": "We start by collecting two datasets that reflect the low-resource and high-resource settings. First, we compile a crowd-sourced geo-diverse dataset collected from a large number of countries, which we refer to as \u201clow-resource data\u201d due to the low number of images that could be collected for each country in the set and the difficulty of gathering more. Second, we also compile a web-scraped dataset used for training foundation models, which we refer to as \u201chigh-resource\u201d due to its vast size consisting of billions of images (e.g., LAION-5B222https://laion.ai/blog/laion-5b/  ###reference_laion.ai/blog/laion-5b/###) and the ease of gathering more data.\nNext, we pre-process the data by mapping the topics between the two data sources, filtering out topics and countries with very few images.\nFinally, we utilize the collected data to generate visual representations through vision-language foundation models. These representations are then used to determine the visual similarity between images of topics in low-resource data and their corresponding topics in high-resource data.\nImageNet and ImageNet Large Scale Visual Recognition Challenge (ILSVRC) are pioneers in advancing object detection and classification progress.\nThe imagenet21k dataset (Deng et al., 2009  ###reference_b14###) contains around 21,000 WordNet (Fellbaum, 2000  ###reference_b19###) synsets and more than 14 million annotated images. We use the processed version of ImageNet21k (Ridnik et al., 2021  ###reference_b57###), with removed invalid classes and resized images. We also tried using ImageNet1k, but it did not have enough classes for our purpose, and we chose to use it to supplement the ImageNet21k data.\nLarge language-vision models such as CLIP or ALIGN\nhave been trained on billions of image-text pairs unavailable to the public. LAION-5B (Schuhmann et al., 2022  ###reference_b61###) was created to address this problem by open-sourcing a CLIP-filtered dataset333The data is filtered using OpenAI\u2019s CLIP ViT-L/14 by calculating the cosine similarity between the text and image embeddings and dropping those with a similarity below 0.3.\n of 5,85 billion high-quality image-text pairs.\nWe use LAION-400M (Schuhmann et al., 2021  ###reference_b62###), a subset of LAION-5B that contains 400 million English image and text pairs.\nWe pre-process and combine the low-resource datasets to increase the number of topics, images, and country diversity.\nFirst, we manually group and rename the topics from Coco Dataset with the same meaning (e.g., \u201cbathroom privacy\u201d, \u201cbathroom/ toilet\u201d are renamed \u201cbathroom\u201d).\nNext, we rename the topics from Coco Dataset that match those in GeoDE (e.g., \u201cbike\u201d to \u201cbicycle\u201d, \u201cmedication\u201d to \u201cmedicine\u201d).\nWe remove three topics with less than  images per topic.\nFinally, we obtain a total of  unique topics,  images, from  continents,  regions, and  countries.\nWe map the  topics from the low-resource data to the high-resource data, ImageNet, and LAION by identifying the images with similar labels.\nFirst, we map  topics from the low-resource data to an exact match to ImageNet21k or ImageNet1k. We could not find an exact match for  topics because these topics are too abstract (e.g., \u201cjewelry\u201d, \u201csource of cool\u201d, \u201creligious building\u201d). Instead, we find mappings for their hyponyms (e.g., for \u201cjewelry\u201d, we map \u201cbangle\u201d, \u201cnecklace\u201d, \u201cbracelet\u201d and \u201cring\u201d). The remaining  topics for which we could not find any exact or hyponym mapping to ImageNet21k or ImageNet1k are mapped to LAION.\nWe map data in LAION by selecting the images with captions that contain the topic query. Because LAION data is web-crawled, we find that the images are lower quality than ImageNet and not always relevant to the topic query: e.g., the \u201cTV\u201d topic in LAION contains images of people on TV, not of the object TV. Therefore, to ensure the correctness of the mapping, we manually inspect the images and map a topic to LAION only when most images are relevant to the topic query. We map  topics to LAION.\nNote, however, that the number of hyponyms and the quality of LAION images limit how comprehensive the mapping process is.\nTwo independent annotators check 20 random images from each topic and find that most noisy images come from LAION. Therefore, we decide to limit the amount of data from LAION"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   Low-resource Multimodal Data",
            "text": "We combine two geographically diverse datasets: GeoDE (Ramaswamy et al., 2023  ###reference_b54###) and Coco Dataset (Rojas et al., 2022  ###reference_b58###).\nFor brevity, we call topics all the labels used for all the objects and actions in these two datasets.\nGeoDE.\nThe GeoDE dataset contains crowd-sourced images of objects. The data is balanced across six regions (West Asia, Africa, East Asia, South East Asia, Americas, and Europe), each with 3-4 countries. These regions were chosen due to their scarcity in most public datasets.\nUsing a combination of heuristics and manual validation, the authors selected the objects likely to be visually distinct across the six regions.\nCoco Dataset.\nThe Coco Dataset consists of annotated images featuring a wide array of everyday scenes, capturing objects and actions typical across different cultural and geographical contexts. These include categories like \u201cbicycle\u201d, \u201ccar\u201d, \u201ctennis racket\u201d, and others, designed to reflect variability in visual appearance and context.\nThe data contains a diverse range of topics and annotations that enable studies into object recognition and detection, with a focus on clear, objective categories.\nThe number of images varies widely across categories, providing a rich source for evaluating object detection and classification algorithms."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   High-resource Multimodal Data",
            "text": "As high-resource datasets, we sample data from ImageNet (Deng et al., 2009  ###reference_b14###) and LAION (Schuhmann et al., 2022  ###reference_b61###).\nWe chose these datasets due to their popularity in vision-language models.\nImageNet and ImageNet Large Scale Visual Recognition Challenge (ILSVRC) are pioneers in advancing object detection and classification progress.\nThe imagenet21k dataset (Deng et al., 2009  ###reference_b14###  ###reference_b14###) contains around 21,000 WordNet (Fellbaum, 2000  ###reference_b19###  ###reference_b19###) synsets and more than 14 million annotated images. We use the processed version of ImageNet21k (Ridnik et al., 2021  ###reference_b57###  ###reference_b57###), with removed invalid classes and resized images. We also tried using ImageNet1k, but it did not have enough classes for our purpose, and we chose to use it to supplement the ImageNet21k data.\nLarge language-vision models such as CLIP or ALIGN\nhave been trained on billions of image-text pairs unavailable to the public. LAION-5B (Schuhmann et al., 2022  ###reference_b61###  ###reference_b61###) was created to address this problem by open-sourcing a CLIP-filtered dataset333The data is filtered using OpenAI\u2019s CLIP ViT-L/14 by calculating the cosine similarity between the text and image embeddings and dropping those with a similarity below 0.3.\n of 5,85 billion high-quality image-text pairs.\nWe use LAION-400M (Schuhmann et al., 2021  ###reference_b62###  ###reference_b62###), a subset of LAION-5B that contains 400 million English image and text pairs. \n\nIn addition, we incorporated the Coco Dataset, a large-scale dataset known for its semantic segmentation, keypoint detection, and panoptic segmentation tasks. The Coco Dataset provides extensive annotations and challenging benchmarks that have pushed forward the development of deep learning models in understanding context and compositions in images. By integrating the Coco Dataset, we enriched our model's training and evaluation capabilities, ensuring a more comprehensive understanding of diverse visual scenes."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "3.3.   Data Pre-processing",
            "text": "We pre-process and combine the low-resource datasets to increase the number of topics, images, and country diversity. The Coco Dataset is a large-scale object detection, segmentation, and captioning dataset. It contains over 200,000 labeled images and is designed to spur progress in the field of computer vision. Originally introduced by the Microsoft Coco team, it provides a comprehensive platform for training and evaluating algorithms on common object detection tasks. Next, we manually group and rename the topics from Coco Dataset with the same meaning. We remove three topics with fewer images per topic. Finally, we obtain a total of unique topics, images, from continents, regions, and countries. We map the topics from the low-resource data to the high-resource data, ImageNet, and LAION by identifying the images with similar labels. First, we map topics from the low-resource data to an exact match to ImageNet21k or ImageNet1k. We could not find an exact match for topics because these topics are too abstract (e.g., \u201cjewelry\u201d, \u201csource of cool\u201d, \u201creligious building\u201d). Instead, we find mappings for their hyponyms (e.g., for \u201cjewelry\u201d, we map \u201cbangle\u201d, \u201cnecklace\u201d, \u201cbracelet\u201d and \u201cring\u201d). The remaining topics for which we could not find any exact or hyponym mapping to ImageNet21k or ImageNet1k are mapped to LAION. We map data in LAION by selecting the images with captions that contain the topic query. Because LAION data is web-crawled, we find that the images are lower quality than ImageNet and not always relevant to the topic query: e.g., the \u201cTV\u201d topic in LAION contains images of people on TV, not of the object TV. Therefore, to ensure the correctness of the mapping, we manually inspect the images and map a topic to LAION only when most images are relevant to the topic query. We map topics to LAION. Note, however, that the number of hyponyms and the quality of LAION images limit how comprehensive the mapping process is. Two independent annotators check 20 random images from each topic and find that most noisy images come from LAION. Therefore, we decide to limit the amount of data from LAION and add more images from ImageNet. Specifically, we randomly sample around images per topic from LAION and around images per topic from ImageNet. Note that the high-resource data does not contain country information. We show the data before and after pre-processing and the topic mapping in our repository. ###figure_3### The low-resource data is unbalanced, as the total number of images per country varies from 6,549 for Japan to 1 for Bulgaria and Venezuela, with a median of 345 images per country. The number of images per topic is also unbalanced, from 3,049 for \u201cwaste container\u201d to 18 for \u201changing clothes to dry\u201d. However, balancing the data by down-sampling significantly reduces the number of countries represented for each topic. Having numerous countries represented is essential for our setup. Therefore, we choose not to balance the data. Instead, we remove the (topic, country) pairs containing less than images, considering this threshold a minimum for experiment significance. This also removes considerable data: 3,329/4,830 (topic, country) tuple pairs, 5/99 topics, and 31/83 countries. We show the removed topics and corresponding countries in our repository and highlight the need for more data for these pairs to obtain significant results. We show the statistics after the data collection and pre-processing in Table 1 and the image distribution of countries per topic in Appendix Figure 10."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "3.4.   Data Representation",
            "text": "We use an ensemble of three representations to compute the image similarity and to ensure the results generalize across representation types. We choose CLIP (Radford et al., 2021), ALIGN (Jia et al., 2021), and BLIP-2 (Li et al., 2023) due to their popularity as foundation models (Bommasani et al., 2022), i.e., their use in a multitude of models and their high zero-shot performance across various tasks and datasets, such as text-to-image retrieval, image question answering, human action segmentation, image-sentence alignment, image captioning (Cafagna et al., 2021; Saharia et al., 2022; Kirillov et al., 2023b; Huang et al., 2023a). We use the pre-trained Vision Transformer ViT-B/32 (Dosovitskiy et al., 2021) from the CLIP model (Radford et al., 2021) to encode the visual representations of the images. The training dataset for CLIP was created from the results of numerous queries to various publicly available Internet sources. The dataset referred to as WebImageText WIT contains 400 million (image, text) pairs and is not available to the public. We also extract image features following the ALIGN (Jia et al., 2021) model setup, using a pre-trained EfficientNet (Tan and Le, 2019) as a vision encoder. Since the original code has not been released, our implementation is based on the Kakao Brain code that reproduced the original paper.555https://huggingface.co/docs/transformers/model_doc/align. ALIGN was trained on 1.8 billion image-text pairs collected following the methodology used for the Conceptual Captions dataset (Sharma et al., 2018). Since the emphasis was on scale instead of quality, the dataset underwent fewer post-processing steps, thus leading to a noisier dataset. This dataset is currently unavailable for public access. We also extract image features using BLIP-2 (Li et al., 2023), which uses ViT-g/14 from EVA-CLIP (Sun et al., 2023) as image encoder and removes the second last layer\u2019s output features to increase the performance. BLIP-2 was trained on a total of 129M images aggregated from the COCO Dataset (Lin et al., 2014), Visual Genome (Krishna et al., 2017), CC3M (Sharma et al., 2018), CC12M (Changpinyo et al., 2021), SBU (Ordonez et al., 2011), and the LAION400M datasets (Schuhmann et al., 2021). Captions for the web images were generated using CapFilt (Li et al., 2022). COCO, the Common Objects in Context dataset, represents a widely used large-scale object detection, segmentation, and captioning dataset, which contains more than 200,000 labeled images. It is specifically designed to advance the state-of-the-art in computer vision applications and enhance visual understanding in machines."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Mapping the Representation of Vision-Language Models",
            "text": "In this section, we address the first research question: RQ1: Which countries are less represented in the training data of vision-language models?\nFor each (topic, country) pair, we compute the cosine similarity between\nthe average visual representations of all the corresponding images in the low-resource data and the average visual representations of all the corresponding images in the high-resource data. Note that the average is computed over all three visual representation types, i.e., CLIP, BLIP, and ALIGN.\nWe select the (topic, country) pairs with a similarity score lower than a threshold computed as the average similarity score between all the image representations in the low-resource data and the corresponding representations in the high-resource data. This process is repeated for each visual representation type.666Thresholds and data representations can be changed to fit the purpose of the analysis or application.\nFinally, the (topic, country) pairs selected for all three visual representations are the ones we find to be consistently different from the high-resource data and, thus, the ones that benefit the most from annotations.\nWe find 422 such (topic, country) pairs out of 1,501 unique (topic, country) pairs, potentially reducing the annotation budget to less than a third of the initial amount. We share the results in our repository.\\@footnotemark\nWe compute a similarity heatmap where the rows are the topics, and the columns are countries.\nWe sort the rows (countries) and columns (topics) from the least to the most similar based on the average similarity score per country and topic, leaving out the  values (the grey, empty cells).\nWe show in Figure 3  ###reference_### the similarity heatmap for the CLIP representation and highlight the (topic, country) pairs we find to benefit the most from annotations based on consistently low similarity with the high-resource data across the three visual representations.\nFrom Figure 3  ###reference_###, we can also see that the countries with the fewest data are usually the ones with the most topics in need of annotations (e.g., from  to ). Exceptions to this are countries such as , , , and , which have more data points (topics), but more than half of the topics require annotations, and countries such as  ,  or  which have very few topics and none require annotations.\nIn Figure 3  ###reference_###, we see a few topics in   that are marked to require annotations: \u201cmedicine\u201d, \u201cspice\u201d, \u201cceiling\u201d, \u201cclothes\u201d and \u201cmakeup\u201d. We show in Appendix Figure 11  ###reference_### representative images from these topics from the two data sources, which explain the visual differences.\nFor the rest of the topics, as expected,   data is similar to the high-resource data.\nWe considered using the   as the high-resource data source. However, due to the lack of data on some topics and relatively few images per topic compared to other countries, it was not feasible.\nThere are differences between the results obtained with each visual representation type regarding similarity score intervals and which (topic, country) pairs are similar to the high-resource data. However, the general similarity trend is consistent as most (topic, country) pairs have only low or high similarity scores across all three representations. This is also supported by the strong Pearson correlations between the scores obtained with the three representation types: CLIP and BLIP scores correlate , CLIP and ALIGN scores correlate , ALIGN and BLIP scores correlate .\nWe show in the Appendix Figure 12  ###reference_###, 13  ###reference_###, and 14  ###reference_###, the similarity heatmaps for each representation type: CLIP, ALIGN, and BLIP respectively.\n###figure_4### To show how the topic visual representations vary per low-resource and high-resource data,\nwe perform a 2D transformation using Principal Component Analysis (PCA) (F.R.S., 1901  ###reference_b21###).\nIn Figure 4  ###reference_###, we show the CLIP average representations per country in the low-resource and the corresponding high-resource data for the topic \u201ctoothbrush\u201d.\nWe can observe that, for this topic, there is considerable visual diversity across countries. When comparing to the high-resource data, , we observe visually different countries, such as , , and , and countries very visually similar, such as , , and .\nIn addition, we observe many countries that tend to be clustered together, i.e., visually similar for this particular topic, such as , , ,  , and others.\nWe examine more about the similarities between countries when answering RQ2, in the following section.\nIn Appendix Figure 15  ###reference_###, 16  ###reference_###, 17  ###reference_### we show results for other topics (\u201chand washing\u201d, \u201ctoilet\u201d, \u201cwall\u201d) in low-resource and high-resource data.\n###figure_5###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Cross-country Data Similarity for Improved Model Representation",
            "text": "We now turn to the second research question RQ2: How can we leverage cross-country data similarity to improve the representation of vision-language models?\nWe calculate the cosine similarity between the average visual representations of images for each topic across countries, and repeat this process for all three visual representations.\nGiven a topic, the final visual similarity score between two countries is obtained by averaging the similarity values obtained for each visual representation type.\nFor each (country, topic) pair, we obtain the visually similar countries, along with their similarity score, from the most to the least similar, and share them in our repository.\\@footnotemark\nWe calculate the average similarity score for each country across all corresponding topics and for each topic across all corresponding countries.\nWe show the similarity score distribution for the top three and last three countries and topics in Figure 5  ###reference_###,\nand for all countries and topics in the Appendix Figure 18  ###reference_### and 19  ###reference_###.\nAs shown in Figure 5  ###reference_###,  has the lowest similarity score of , indicating that it is the most different country compared to the others and needs its own annotations.\nOn the other hand,  has the highest similarity scores of , indicating a high similarity to other countries. These results imply that annotating data from  would help other countries.\nThe most visually different topic is \u201creligious building\u201d with a score of , and the most similar topic is \u201chat\u201d with a score of . These results imply that \u201creligious buildings\u201d should be annotated more widely as their visual appearance varies across countries.\nFinally, we investigate whether performance of similarity calculation depends on amount of annotated data. We find that at topic level the similarity scores are not correlated with the amount of annotated data (Pearson correlation coefficient is -0.02). We discuss more about the effect of data size on our analysis results in the Appendix.\n###figure_6### To show how the topic visual representations vary per country in the low-resource data, we perform a 2D transformation using Principal Component Analysis (PCA) (F.R.S., 1901  ###reference_b21###).\nIn Figure 6  ###reference_###, we show the CLIP average representations per country for the topics with the most and least visual differences across countries: \u201creligious building\u201d and \u201chat\u201d, respectively. As expected, the representations for \u201creligious building\u201d are much more spread across countries than those for \u201chat\u201d, which tend to cluster together.\nIn Appendix Figure 20  ###reference_###, 21  ###reference_###, and 22  ###reference_###, we show representations for other topics visually different across countries: \u201cget water\u201d, \u201chouse\u201d and \u201cbackyard.\u201d\n###figure_7### We measure if the visual similarity between countries correlates with the geographical distance. The geographical distance between two countries is calculated using Vincenty\u2019s distance Vincenty (1975  ###reference_b70###) between their capital cities.777https://github.com/rahulbot/distances-between-countries  ###reference_een-countries###\nThe visual similarity between any two countries is calculated across all their shared topics.\nWe compute the Pearson correlation coefficient Freedman et al. (2007  ###reference_b20###) over all countries and obtain a value of , indicating a weak negative correlation. A strong negative correlation is initially more expected as, intuitively, their visual similarity should increase as the distance between countries decreases.\nHowever, when we break down the correlation at the country level, the correlation coefficient varies significantly per country.\n###figure_8### In\nFigure 7  ###reference_###, we show countries with weak to moderate positive correlations (e.g.,  with ,  with ), countries with weak to moderate negative correlations (e.g.,  with ,  with ), most countries have values close to , indicating no correlation between visual similarity and geographical distance.\nUpon close examination of the results, we determine the reasons behind this result: countries with positive correlation are often visually similar to countries from different continents (e.g.,  is more similar to  with an average similarity  and distance  than to  with\nwith an average similarity  and distance ). We hypothesize this might be due to history, climate, and/or income differences, which could contribute more to visual similarity than distance alone.\nOur analysis shows that geographical location does not generally correlate with visual similarity.\nTherefore, collecting globally diverse annotations on a budget requires considering other complementary information, such as the country\u2019s income, culture, history, and climate. Our results on which countries are similar to each other provide valuable insights into how to distribute the annotation budget effectively and can be used along with this complementary information.\nWe train a classifier to predict the topic of the input images and measure the accuracy while controlling for the countries.\nSpecifically, we input the CLIP visual representation in a linear layer, followed by a softmax to predict the topics of the input images.888We set the learning rate as 5e-3, use AdamW as the optimizer, and conduct training over 250 epochs with a batch size of 512. Additionally, we use a cosine annealing schedule with 50 warm-up epochs.\nWe select one random country for each topic from the low-resource data, which we call target (topic, country) pairs.\nNext, we split the data into training and test sets in a 90-10% data split to include all the target (topic, country) pairs in both sets.\nFinally, we replace different ratios (100%, 90%, 70%, 50%, 30%, 10%, 0%) of the target-country data with images from: (1) the most similar countries to the target-country given the target-topic; (2) the most dissimilar countries to the target-country given the target-topic; (3) high-resource data corresponding to the target-topic.\nThe topic classification accuracy when using all the training target-country data is , which is an upper bound. In Figure 8  ###reference_###, we show the accuracy when adding data from (1), (2) and (3).\nThe main takeaway is that adding data from similar countries improves the performance more than adding data from dissimilar countries or high-resource data, and the gap in performance increases with the replacement ratio. Additionally, supplementing with high-resource data is generally more beneficial than supplementing with data from dissimilar countries.\nWe also compute the accuracy when no data is added, and find that adding data from dissimilar countries or from high-resource data can hurt the performance compared to not adding data, especially for high replacement ratios\n(). We show the results in the Appendix, in Figure 23  ###reference_###.\n###figure_9###"
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Main Takeaways",
            "text": "Our analyses provide multiple insights into the current state of vision-language annotations for various topics across different countries, and show the coverage limitations of existing large-scale datasets. We highlight the main takeaways and propose actionable steps to help future work create more inclusive datasets and models.\nTo have more inclusive models and datasets, we need to collect more globally diverse annotations. Because annotations are expensive, we propose to focus future annotation efforts on specific countries and their topics.\nTo assist with these efforts, we provide a list of countries and corresponding topics that are consistently unrepresented in the training data of vision-language models.\nFurthermore, most countries have less than ten images per topic.\nFor most countries and corresponding topics \u2013 3,329/ 4,830, we could not determine how similar they are to the high-resource data because of the lack of data. These countries have less than ten images per topic and, therefore, already need annotations.\nAs an alternative solution, we recommend developing algorithms that can perform well with limited amount of data.\nWhen we do not have a sufficient budget to annotate more data for a target country and topic, we propose using the available data from countries with similar visual representations of that given topic. We provide a list of similar countries for each target country and topic and show that using this data improves model performance more than using data from dissimilar countries or high-resource data.\nWe compute the Pearson correlation coefficient between the visual similarity and the geographical distance between all countries and find a very weak negative correlation of -0.01. Therefore, collecting globally diverse annotations requires considering additional information.\nMultiple other factors, such as income, history, or cultural heritage, can contribute to the visual similarity between countries. We find this hypothesis worth investigating in depth in future work.\nWhile examining images of topics across countries, we notice visually similar topics with very different backgrounds, which influence the visual similarity score. For example, in  Figure 9  ###reference_###, many countries have the same type of toothbrush, but because their storage place is different, their visual similarity score is low. In this paper, we measure similarity at the context level, considering both the topic and the context (e.g., background, storage space). However, as future work, we propose to investigate further which type of similarity to consider when we annotate diverse data: either at the topic level, by extracting the segmentation mask of the topic, or at the context level, by considering the entire image.\n###figure_10###"
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7.   Conclusion",
            "text": "In this paper, we addressed the need for balanced data representation used to train vision-language models. Because data annotations are expensive, we proposed to annotate primarily images from unrepresented countries. To find which countries are less represented in the training data of vision-language models, we compared the visual similarity of images across 94 topics and 52 countries using the Coco Dataset. We used three visual representations, CLIP, BLIP-2, and ALIGN, to ensure the results generalize across representation types. Additionally, we proposed to leverage cross-country data similarity to improve model performance. We found visually similar countries for each country and corresponding topics and made them available in our repository: https://github.com/MichiganNLP/visual_diversity_budget. Finally, our analysis offers multiple takeaways for future work to make informed decisions on what global data to annotate and how to leverage cross-country data similarity to improve model representation. Through our work, we hope to contribute to building more inclusive and affordable vision-language models and datasets to help democratize AI globally."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "8.   Bibliographical References",
            "text": ""
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A Subjective Topics",
            "text": "The 19 subjective topics that we remove: \u201cfavorite home decorations\u201d, \u201cfavourite item in kitchen\u201d, \u201cfavourite sports clubs\u201d, \u201chow the most loved item is used\u201d, \u201cicons\u201d, \u201cidols\u201d, \u201clatest furniture bought\u201d, \u201clooking over the shoulder\u201d, \u201cmost loved item\u201d, \u201cmost loved toy\u201d, \u201cmost played songs on the radio\u201d, \u201cmusic idol\u201d, \u201cnext big thing you are planning to buy\u201d, \u201cplaying with most loved toy\u201d, \u201cthing I dream about having\u201d, \u201cthings I wish I had\u201d, \u201cusing most loved item\u201d, \u201cyouth culture\u201d, \u201cwhat I wish I could buy\u201d."
        },
        {
            "section_id": "Appendix 2",
            "parent_section_id": null,
            "section_name": "Appendix B Data Stats",
            "text": "###figure_11###"
        },
        {
            "section_id": "Appendix 3",
            "parent_section_id": null,
            "section_name": "Appendix C Research Question 1",
            "text": ""
        },
        {
            "section_id": "Appendix 4",
            "parent_section_id": null,
            "section_name": "Appendix D Research Question 2",
            "text": "###figure_12### ###figure_13### ###figure_14### ###figure_15### ###figure_16### ###figure_17###"
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T1\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S3.T1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S3.T1.1.1.1.1\"><span class=\"ltx_text\" id=\"S3.T1.1.1.1.1.1\" style=\"font-size:90%;\"># unique topics</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.1.1.1.2\"><span class=\"ltx_text\" id=\"S3.T1.1.1.1.2.1\" style=\"font-size:90%;\">94</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.2.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.2.2.1\"><span class=\"ltx_text\" id=\"S3.T1.1.2.2.1.1\" style=\"font-size:90%;\"># unique countries</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.2.2.2\"><span class=\"ltx_text\" id=\"S3.T1.1.2.2.2.1\" style=\"font-size:90%;\">52</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.3.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.3.3.1\"><span class=\"ltx_text\" id=\"S3.T1.1.3.3.1.1\" style=\"font-size:90%;\"># unique (topic, country) pairs</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.3.3.2\"><span class=\"ltx_text\" id=\"S3.T1.1.3.3.2.1\" style=\"font-size:90%;\">1,501</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.4.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.4.4.1\"><span class=\"ltx_text\" id=\"S3.T1.1.4.4.1.1\" style=\"font-size:90%;\"># images in low-resource data</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.4.4.2\"><span class=\"ltx_text\" id=\"S3.T1.1.4.4.2.1\" style=\"font-size:90%;\">80,801</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.5.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.5.5.1\"><span class=\"ltx_text\" id=\"S3.T1.1.5.5.1.1\" style=\"font-size:90%;\"># images in high-resource data</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.5.5.2\"><span class=\"ltx_text\" id=\"S3.T1.1.5.5.2.1\" style=\"font-size:90%;\">103,006</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.6.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.6.6.1\"><span class=\"ltx_text\" id=\"S3.T1.1.6.6.1.1\" style=\"font-size:90%;\">average # images per (topic, country)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.6.6.2\"><span class=\"ltx_text\" id=\"S3.T1.1.6.6.2.1\" style=\"font-size:90%;\">53.8</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.7.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T1.1.7.7.1\"><span class=\"ltx_text\" id=\"S3.T1.1.7.7.1.1\" style=\"font-size:90%;\">median # images per (topic, country)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.1.7.7.2\"><span class=\"ltx_text\" id=\"S3.T1.1.7.7.2.1\" style=\"font-size:90%;\">30</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Statistics for the collected number of topics, countries, and images collected from low-resource and high-resource data after data pre-processing.</figcaption>\n</figure>",
            "capture": "Table 1: Statistics for the collected number of topics, countries, and images collected from low-resource and high-resource data after data pre-processing."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.07687v1_figure_1.png",
            "caption": "Figure 1: Vision-language models work poorly on data from underrepresented countries. This is primarily due to the diverse appearance of topics (objects and actions) across countries (e.g., \u201ctoothbrush\u201d).\nHowever, collecting diverse global data is very expensive. As solutions to budget annotations, we propose to (1) annotate the images visually different from the ones in high-resource datasets such as LAION or ImageNet; (2) supplement data from low-resource countries with data from visually similar countries."
        },
        "2": {
            "figure_path": "2403.07687v1_figure_2.png",
            "caption": "Figure 2: Example images (\u201ccooking pot\u201d) in low-resource data (left) vs. in high-resource data (right)."
        },
        "3": {
            "figure_path": "2403.07687v1_figure_3.png",
            "caption": "Figure 3: Similarity heatmap of (topic, country) pairs. Based on the average similarity score, rows and columns are sorted from the least to the most similar. The lighter the color, the lower the similarity between high-resource and low-resource data for that corresponding (topic, country) pair, the more beneficial it is to annotate. We highlight with black the pairs we determine to benefit the most from annotations. Grey cells have less than ten images and are therefore discarded. Best viewed in color."
        },
        "4": {
            "figure_path": "2403.07687v1_figure_4.png",
            "caption": "Figure 4: PCA for the topic \u201ctoothbrush\u201d for all countries that contain this topic in the low-resource data and in the high-resource data. The high-resource data point is highlighted with star symbol. The data is represented as the average of the CLIP representations."
        },
        "5": {
            "figure_path": "2403.07687v1_figure_5.png",
            "caption": "Figure 5: Top three and last three countries (left) and topics (right) sorted by average similarity score."
        },
        "6": {
            "figure_path": "2403.07687v1_figure_6.png",
            "caption": "Figure 6: PCA for the topic \u201creligious building\u201d and \u201chat\u201d for all countries in the low-resource data that contain this topic. The data is represented as the average of the CLIP representations."
        },
        "7": {
            "figure_path": "2403.07687v1_figure_7.png",
            "caption": "Figure 7: \nPearson correlation coefficient between the visual similarity and the geographical distance, across countries. Most countries do not have a significant correlation between visual similarity and location."
        },
        "8": {
            "figure_path": "2403.07687v1_figure_8.png",
            "caption": "Figure 8: Topic classification accuracy (in %) for different target-country data ratios (e.g., target-country ratio 0.0% is equivalent to 100% replacement ratio).\nWe replace different ratios of the target-country data with images from: (1) the most similar countries to the target-country given the target-topic; (2) the most dissimilar countries to the target-country given the target-topic; (3) high-resource data of the target-topic;"
        },
        "9": {
            "figure_path": "2403.07687v1_figure_9.png",
            "caption": "Figure 9: The context of the topic influences the visual similarity. For example, although the same type of toothbrush is depicted, their storage place differs, i.e., on a piece of wood, in a plastic container in the bathroom, in a plastic container tied to a tree, near a brick wall. Therefore, visual diversity is measured not only at the topic level but also at the context level."
        },
        "10": {
            "figure_path": "2403.07687v1_figure_10.png",
            "caption": "Figure 10: The distribution of countries per topic."
        },
        "11": {
            "figure_path": "2403.07687v1_figure_11.png",
            "caption": "Figure 11: Representative images from the visually different topics in low-resource U\u2062S\u2062A\ud835\udc48\ud835\udc46\ud835\udc34USAitalic_U italic_S italic_A data/ L\ud835\udc3fLitalic_L, and high-resource data/ H\ud835\udc3bHitalic_H.\nIn H\ud835\udc3bHitalic_H, \u201cclothes\u201d and \u201cmakeup\u201d are shown on people, while in L\ud835\udc3fLitalic_L they are separated in dressers and containers; in H\ud835\udc3bHitalic_H, \u201cspice\u201d is in in large baskets in markets, while in L\ud835\udc3fLitalic_L they are in small containers in people\u2019s houses; in H\ud835\udc3bHitalic_H, \u201cceiling\u201d is shown in public spaces, while in L\ud835\udc3fLitalic_L is in private homes; in H\ud835\udc3bHitalic_H \u201cmedicine\u201d is usually in bottles, while in L\ud835\udc3fLitalic_L can be in various forms."
        },
        "12": {
            "figure_path": "2403.07687v1_figure_12.png",
            "caption": "Figure 12: Similarity heatmap of (topic, country) pairs with CLIP visual representations. The darker, the less similarity between high-resource and low-resource data for that corresponding (topic, country), the more beneficial it is to annotate. Empty cells do not have any images for (topic, country). Best viewed in color."
        },
        "13": {
            "figure_path": "2403.07687v1_figure_13.png",
            "caption": "Figure 13: Similarity heatmap of (topic, country) pairs with ALIGN visual representations. The darker, the less similarity between high-resource and low-resource data for that corresponding (topic, country), the more beneficial it is to annotate. Empty cells do not have any images for (topic, country). Best viewed in color."
        },
        "14": {
            "figure_path": "2403.07687v1_figure_14.png",
            "caption": "Figure 14: Similarity heatmap of (topic, country) pairs with BLIP visual representations. The darker, the less similarity between high-resource and low-resource data for that corresponding (topic, country), the more beneficial it is to annotate. Empty cells do not have any images for (topic, country). Best viewed in color."
        },
        "15": {
            "figure_path": "2403.07687v1_figure_15.png",
            "caption": "Figure 15: PCA for the topic \u201chand washing\u201d for all countries that contain this topic in the low-resource data and in the high-resource data. The high-resource data point is highlighted. The data is represented as the average of the CLIP representations."
        },
        "16": {
            "figure_path": "2403.07687v1_figure_16.png",
            "caption": "Figure 16: PCA for the topic \u201ctoilet\u201d for all countries that contain this topic in the low-resource data and in the high-resource data. The high-resource data point is highlighted. The data is represented as the average of the CLIP representations."
        },
        "17": {
            "figure_path": "2403.07687v1_figure_17.png",
            "caption": "Figure 17: PCA for the topic \u201cwall\u201d for all countries that contain this topic in the low-resource data and in the high-resource data. The high-resource data point is highlighted. The data is represented as the average of the CLIP representations."
        },
        "18": {
            "figure_path": "2403.07687v1_figure_18.png",
            "caption": "Figure 18: The distribution of average similarity scores per topic."
        },
        "19": {
            "figure_path": "2403.07687v1_figure_19.png",
            "caption": "Figure 19: The distribution of average similarity scores per country."
        },
        "20": {
            "figure_path": "2403.07687v1_figure_20.png",
            "caption": "Figure 20: PCA for the topic \u201cget water\u201d for all countries that contain this topic in the low-resource data. The data is represented as the average of the CLIP representations."
        },
        "21": {
            "figure_path": "2403.07687v1_figure_21.png",
            "caption": "Figure 21: PCA for the topic \u201chouse\u201d for all countries that contain this topic in the low-resource data. The data is represented as the average of the CLIP representations."
        },
        "22": {
            "figure_path": "2403.07687v1_figure_22.png",
            "caption": "Figure 22: PCA for the topic \u201cbackyard\u201d for all countries that contain this topic in the low-resource data. The data is represented as the average of the CLIP representations."
        },
        "23": {
            "figure_path": "2403.07687v1_figure_23.png",
            "caption": "Figure 23: Topic classification accuracy (in %) for different target-country data ratios (e.g., target-country ratio 0.0% is equivalent to 100% replacement ratio). We replace different ratios of the target-country data with images from: (1) the most similar countries to the target-country given the target-topic; (2) the most dissimilar countries to the target-country given the target-topic;\n(3) high-resource data of the target-topic; (4) no replacement data/ addition."
        }
    },
    "references": [
        {
            "1": {
                "title": "Semi-supervised learning of classifiers from a statistical perspective: A brief review.",
                "author": "Daniel Ahfock and Geoffrey J McLachlan. 2023.",
                "venue": "Econometrics and Statistics, 26:124\u2013138.",
                "url": null
            }
        },
        {
            "2": {
                "title": "The power of ensembles for active learning in image classification.",
                "author": "William H Beluch, Tim Genewein, Andreas N\u00fcrnberger, and Jan M K\u00f6hler. 2018.",
                "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 9368\u20139377.",
                "url": null
            }
        },
        {
            "3": {
                "title": "On the opportunities and risks of foundation models.",
                "author": "Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2022.",
                "venue": "arXiv preprint arXiv:2108.07258.",
                "url": null
            }
        },
        {
            "4": {
                "title": "Gender shades: Intersectional accuracy disparities in commercial gender classification.",
                "author": "Joy Buolamwini and Timnit Gebru. 2018.",
                "venue": "In Conference on fairness, accountability and transparency, pages 77\u201391. PMLR.",
                "url": null
            }
        },
        {
            "5": {
                "title": "Multimodal analysis and prediction of latent user dimensions.",
                "author": "Laura Burdick, Rada Mihalcea, Ryan L. Boyd, and James W. Pennebaker. 2017.",
                "venue": "In Social Informatics.",
                "url": null
            }
        },
        {
            "6": {
                "title": "What vision-language modelssee\u2019when they see scenes.",
                "author": "Michele Cafagna, Kees van Deemter, and Albert Gatt. 2021.",
                "venue": "arXiv preprint arXiv:2109.07301.",
                "url": null
            }
        },
        {
            "7": {
                "title": "The limits of global inclusion in ai development.",
                "author": "Alan Kam Leung Chan, Chinasa T. Okolo, Zachary Terner, and Angelina Wang. 2021.",
                "venue": "ArXiv, abs/2102.01265.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts.",
                "author": "Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. 2021.",
                "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3558\u20133568.",
                "url": null
            }
        },
        {
            "9": {
                "title": "Mining semantic affordances of visual object categories.",
                "author": "Yu-Wei Chao, Zhan Wang, Rada Mihalcea, and Jia Deng. 2015.",
                "venue": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4259\u20134267.",
                "url": null
            }
        },
        {
            "10": {
                "title": "Semi-supervised and unsupervised deep visual learning: A survey.",
                "author": "Yanbei Chen, Massimiliano Mancini, Xiatian Zhu, and Zeynep Akata. 2022.",
                "venue": "IEEE transactions on pattern analysis and machine intelligence.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Batch active learning at scale.",
                "author": "Gui Citovsky, Giulia DeSalvo, Claudio Gentile, Lazaros Karydas, Anand Rajagopalan, Afshin Rostamizadeh, and Sanjiv Kumar. 2021.",
                "venue": "Advances in Neural Information Processing Systems, 34:11933\u201311944.",
                "url": null
            }
        },
        {
            "12": {
                "title": "Support-vector networks.",
                "author": "Corinna Cortes and Vladimir Vapnik. 1995.",
                "venue": "Machine learning, 20(3):273\u2013297.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Does object recognition work for everyone?",
                "author": "Terrance De Vries, Ishan Misra, Changhan Wang, and Laurens Van der Maaten. 2019.",
                "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pages 52\u201359.",
                "url": null
            }
        },
        {
            "14": {
                "title": "Imagenet: A large-scale hierarchical image database.",
                "author": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, K. Li, and Li Fei-Fei. 2009.",
                "venue": "2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248\u2013255.",
                "url": "https://api.semanticscholar.org/CorpusID:57246310"
            }
        },
        {
            "15": {
                "title": "An image is worth 16x16 words: Transformers for image recognition at scale.",
                "author": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021.",
                "venue": "ICLR.",
                "url": null
            }
        },
        {
            "16": {
                "title": "Adaptive methods for real-world domain generalization.",
                "author": "Abhimanyu Dubey, Vignesh Ramanathan, Alex \u2019Sandy\u2019 Pentland, and Dhruv Kumar Mahajan. 2021.",
                "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 14335\u201314344.",
                "url": null
            }
        },
        {
            "17": {
                "title": "Not all labels are equal: Rationalizing the labeling costs for training object detection.",
                "author": "Ismail Elezi, Zhiding Yu, Anima Anandkumar, Laura Leal-Taixe, and Jose M Alvarez. 2022.",
                "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14492\u201314501.",
                "url": null
            }
        },
        {
            "18": {
                "title": "Model transfer for tagging low-resource languages using a bilingual dictionary.",
                "author": "Meng Fang and Trevor Cohn. 2017.",
                "venue": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 587\u2013593, Vancouver, Canada. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/P17-2093"
            }
        },
        {
            "19": {
                "title": "Wordnet : an electronic lexical database.",
                "author": "Christiane D. Fellbaum. 2000.",
                "venue": "Language, 76:706.",
                "url": "https://api.semanticscholar.org/CorpusID:5958691"
            }
        },
        {
            "20": {
                "title": "Statistics (international student edition).",
                "author": "David Freedman, Robert Pisani, and Roger Purves. 2007.",
                "venue": "Pisani, R. Purves, 4th edn. WW Norton & Company, New York.",
                "url": null
            }
        },
        {
            "21": {
                "title": "Liii. on lines and planes of closest fit to systems of points in space.",
                "author": "Karl Pearson F.R.S. 1901.",
                "venue": "The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 2(11):559\u2013572.",
                "url": "https://doi.org/10.1080/14786440109462720"
            }
        },
        {
            "22": {
                "title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning.",
                "author": "Yarin Gal and Zoubin Ghahramani. 2016.",
                "venue": "In international conference on machine learning, pages 1050\u20131059. PMLR.",
                "url": null
            }
        },
        {
            "23": {
                "title": "Deep bayesian active learning with image data.",
                "author": "Yarin Gal, Riashat Islam, and Zoubin Ghahramani. 2017.",
                "venue": "In International conference on machine learning, pages 1183\u20131192. PMLR.",
                "url": null
            }
        },
        {
            "24": {
                "title": "Uncurated image-text datasets: Shedding light on demographic bias.",
                "author": "Noa Garcia, Yusuke Hirota, Yankun Wu, and Yuta Nakashima. 2023.",
                "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6957\u20136966.",
                "url": null
            }
        },
        {
            "25": {
                "title": "Identifying cross-cultural differences in word usage.",
                "author": "Aparna Garimella, Rada Mihalcea, and James W. Pennebaker. 2016.",
                "venue": "In International Conference on Computational Linguistics.",
                "url": null
            }
        },
        {
            "26": {
                "title": "Race and gender.",
                "author": "Timnit Gebru. 2020.",
                "venue": "The Oxford handbook of ethics of aI, pages 251\u2013269.",
                "url": null
            }
        },
        {
            "27": {
                "title": "Babel-imagenet: Massively multilingual evaluation of vision-and-language representations.",
                "author": "Gregor Geigle, Radu Timofte, and Goran Glava\u0161. 2023.",
                "venue": "arXiv preprint arXiv:2306.08658.",
                "url": null
            }
        },
        {
            "28": {
                "title": "Vision models are more robust and fair when pretrained on uncurated images without supervision.",
                "author": "Priya Goyal, Quentin Duval, Isaac Seessel, Mathilde Caron, Ishan Misra, Levent Sagun, Armand Joulin, and Piotr Bojanowski. 2022a.",
                "venue": "ArXiv, abs/2202.08360.",
                "url": null
            }
        },
        {
            "29": {
                "title": "Fairness indicators for systematic assessments of visual feature extractors.",
                "author": "Priya Goyal, Adriana Romero-Soriano, Caner Hazirbas, Levent Sagun, and Nicolas Usunier. 2022b.",
                "venue": "Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency.",
                "url": null
            }
        },
        {
            "30": {
                "title": "Pinpointing why object recognition performance degrades across income levels and geographies.",
                "author": "Laura Gustafson, Megan Richards, Melissa Hall, Caner Hazirbas, Diane Bouchacourt, and Mark Ibrahim. 2023.",
                "venue": "ArXiv, abs/2304.05391.",
                "url": null
            }
        },
        {
            "31": {
                "title": "Semi-supervised learning.",
                "author": "Mohamed Farouk Abdel Hady and Friedhelm Schwenker. 2013.",
                "venue": "Handbook on Neural Information Processing, pages 215\u2013239.",
                "url": null
            }
        },
        {
            "32": {
                "title": "Towards reliable assessments of demographic disparities in multi-label image classifiers.",
                "author": "Melissa Hall, Bobbie Chern, Laura Gustafson, Denisse Ventura, Harshad Kulkarni, Candace Ross, and Nicolas Usunier. 2023.",
                "venue": "arXiv preprint arXiv:2302.08572.",
                "url": null
            }
        },
        {
            "33": {
                "title": "Smaller coresets for k-median and k-means clustering.",
                "author": "Sariel Har-Peled and Akash Kushal. 2005.",
                "venue": "In Proceedings of the twenty-first annual symposium on Computational geometry, pages 126\u2013134.",
                "url": null
            }
        },
        {
            "34": {
                "title": "Crowdsourcing detection of sampling biases in image datasets.",
                "author": "Xiao Hu, Haobo Wang, Anirudh Vegesana, Somesh Dube, Kaiwen Yu, Gore Kao, Shuo-Han Chen, Yung-Hsiang Lu, George K. Thiruvathukal, and Ming Yin. 2020.",
                "venue": "In Proceedings of The Web Conference 2020, WWW \u201920, page 2955\u20132961, New York, NY, USA. Association for Computing Machinery.",
                "url": "https://doi.org/10.1145/3366423.3380063"
            }
        },
        {
            "35": {
                "title": "Chatgpt for shaping the future of dentistry: the potential of multi-modal large language model.",
                "author": "Hanyao Huang, Ou Zheng, Dongdong Wang, Jiayi Yin, Zijin Wang, Shengxuan Ding, Heng Yin, Chuan Xu, Renjie Yang, Qian Zheng, et al. 2023a.",
                "venue": "International Journal of Oral Science, 15(1):29.",
                "url": null
            }
        },
        {
            "36": {
                "title": "Tag2text: Guiding vision-language model via image tagging.",
                "author": "Xinyu Huang, Youcai Zhang, Jinyu Ma, Weiwei Tian, Rui Feng, Yuejie Zhang, Yaqian Li, Yandong Guo, and Lei Zhang. 2023b.",
                "venue": null,
                "url": "http://arxiv.org/abs/2303.05657"
            }
        },
        {
            "37": {
                "title": "Scaling up visual and vision-language representation learning with noisy text supervision.",
                "author": "Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 2021.",
                "venue": "In International conference on machine learning, pages 4904\u20134916. PMLR.",
                "url": null
            }
        },
        {
            "38": {
                "title": "The state and fate of linguistic diversity and inclusion in the NLP world.",
                "author": "Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. 2020.",
                "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6282\u20136293, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2020.acl-main.560"
            }
        },
        {
            "39": {
                "title": "Geonet: Benchmarking unsupervised adaptation across geographies.",
                "author": "Tarun Kalluri, Wangdong Xu, and Manmohan Chandraker. 2023.",
                "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15368\u201315379.",
                "url": null
            }
        },
        {
            "40": {
                "title": "Towards a fairer landmark recognition dataset.",
                "author": "Zu Whan Kim, Andre F. de Ara\u00fajo, Bingyi Cao, Cameron S Askew, Jack Sim, Mike Green, N\u2019Mah Fodiatu Yilla, and Tobias Weyand. 2021.",
                "venue": "ArXiv, abs/2108.08874.",
                "url": null
            }
        },
        {
            "41": {
                "title": "Segment anything.",
                "author": "Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00e1r, and Ross B. Girshick. 2023a.",
                "venue": "ArXiv, abs/2304.02643.",
                "url": "https://api.semanticscholar.org/CorpusID:257952310"
            }
        },
        {
            "42": {
                "title": "Segment anything.",
                "author": "Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. 2023b.",
                "venue": "arXiv preprint arXiv:2304.02643.",
                "url": null
            }
        },
        {
            "43": {
                "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations.",
                "author": "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. 2017.",
                "venue": "International journal of computer vision, 123:32\u201373.",
                "url": null
            }
        },
        {
            "44": {
                "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.",
                "author": "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2301.12597"
            }
        },
        {
            "45": {
                "title": "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.",
                "author": "Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. 2022.",
                "venue": "In International Conference on Machine Learning.",
                "url": null
            }
        },
        {
            "46": {
                "title": "VisualBERT: A simple and performant baseline for vision and language.",
                "author": "Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. 2019.",
                "venue": "ArXiv, abs/1908.03557.",
                "url": "https://arxiv.org/abs/1908.03557"
            }
        },
        {
            "47": {
                "title": "Microsoft coco: Common objects in context.",
                "author": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. 2014.",
                "venue": "In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740\u2013755. Springer.",
                "url": null
            }
        },
        {
            "48": {
                "title": "Human-in-the-Loop Machine Learning: Active learning and annotation for human-centered AI.",
                "author": "Robert Munro Monarch. 2021.",
                "venue": "Simon and Schuster.",
                "url": null
            }
        },
        {
            "49": {
                "title": "Bridging the digital divide: Performance variation across socio-economic factors in vision-language models.",
                "author": "Joan Nwatu, Oana Ignat, and Rada Mihalcea. 2023.",
                "venue": "In The 2023 Conference on Empirical Methods in Natural Language Processing.",
                "url": "https://openreview.net/forum?id=M9NdVElcbs"
            }
        },
        {
            "50": {
                "title": "Realistic evaluation of deep semi-supervised learning algorithms.",
                "author": "Avital Oliver, Augustus Odena, Colin A Raffel, Ekin Dogus Cubuk, and Ian Goodfellow. 2018.",
                "venue": "Advances in neural information processing systems, 31.",
                "url": null
            }
        },
        {
            "51": {
                "title": "Im2text: Describing images using 1 million captioned photographs.",
                "author": "Vicente Ordonez, Girish Kulkarni, and Tamara Berg. 2011.",
                "venue": "Advances in neural information processing systems, 24.",
                "url": null
            }
        },
        {
            "52": {
                "title": "Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing.",
                "author": "Edoardo Maria Ponti, Helen O\u2019Horan, Yevgeni Berzak, Ivan Vuli\u0107, Roi Reichart, Thierry Poibeau, Ekaterina Shutova, and Anna Korhonen. 2019.",
                "venue": "Computational Linguistics, 45(3):559\u2013601.",
                "url": "https://doi.org/10.1162/coli_a_00357"
            }
        },
        {
            "53": {
                "title": "Learning transferable visual models from natural language supervision.",
                "author": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021.",
                "venue": "In International Conference on Machine Learning.",
                "url": null
            }
        },
        {
            "54": {
                "title": "Beyond web-scraping: Crowd-sourcing a geographically diverse image dataset.",
                "author": "Vikram V. Ramaswamy, Sing Yu Lin, Dora Zhao, Aaron B. Adcock, Laurens van der Maaten, Deepti Ghadiyaram, and Olga Russakovsky. 2023.",
                "venue": "ArXiv, abs/2301.02560.",
                "url": null
            }
        },
        {
            "55": {
                "title": "Sentence-BERT: Sentence embeddings using Siamese BERT-networks.",
                "author": "Nils Reimers and Iryna Gurevych. 2019.",
                "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982\u20133992, Hong Kong, China. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/D19-1410"
            }
        },
        {
            "56": {
                "title": "A survey of deep active learning.",
                "author": "Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Brij B Gupta, Xiaojiang Chen, and Xin Wang. 2021.",
                "venue": "ACM computing surveys (CSUR), 54(9):1\u201340.",
                "url": null
            }
        },
        {
            "57": {
                "title": "Imagenet-21k pretraining for the masses.",
                "author": "T. Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. 2021.",
                "venue": "ArXiv, abs/2104.10972.",
                "url": "https://api.semanticscholar.org/CorpusID:233347018"
            }
        },
        {
            "58": {
                "title": "The dollar street dataset: Images representing the geographic and socioeconomic diversity of the world.",
                "author": "William Gaviria Rojas, Sudnya Diamos, Keertan Ranjan Kini, David Kanter, Vijay Janapa Reddi, and Cody Coleman. 2022.",
                "venue": "In Neural Information Processing Systems.",
                "url": null
            }
        },
        {
            "59": {
                "title": "Photorealistic text-to-image diffusion models with deep language understanding.",
                "author": "Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. 2022.",
                "venue": "Advances in Neural Information Processing Systems, 35:36479\u201336494.",
                "url": null
            }
        },
        {
            "60": {
                "title": "When does bias transfer in transfer learning?",
                "author": "Hadi Salman, Saachi Jain, Andrew Ilyas, Logan Engstrom, Eric Wong, and Aleksander Madry. 2022.",
                "venue": "arXiv preprint arXiv:2207.02842.",
                "url": null
            }
        },
        {
            "61": {
                "title": "Laion-5b: An open large-scale dataset for training next generation image-text models.",
                "author": "Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. 2022.",
                "venue": "ArXiv, abs/2210.08402.",
                "url": "https://api.semanticscholar.org/CorpusID:252917726"
            }
        },
        {
            "62": {
                "title": "LAION-400M: Open dataset of CLIP-filtered 400 million image-text pairs.",
                "author": "Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. 2021.",
                "venue": "In Proceedings of the NeurIPS Data Centric AI Workshop.",
                "url": null
            }
        },
        {
            "63": {
                "title": "A step toward more inclusive people annotations for fairness.",
                "author": "Candice Schumann, Susanna Ricco, Utsav Prabhu, Vittorio Ferrari, and Caroline Pantofaru. 2021.",
                "venue": "In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, pages 916\u2013925.",
                "url": null
            }
        },
        {
            "64": {
                "title": "Active learning for convolutional neural networks: A core-set approach.",
                "author": "Ozan Sener and Silvio Savarese. 2018.",
                "venue": "In International Conference on Learning Representations.",
                "url": null
            }
        },
        {
            "65": {
                "title": "No classification without representation: Assessing geodiversity issues in open data sets for the developing world.",
                "author": "Shreya Shankar, Yoni Halpern, Eric Breck, James Atwood, Jimbo Wilson, and D. Sculley. 2017.",
                "venue": "arXiv: Machine Learning.",
                "url": null
            }
        },
        {
            "66": {
                "title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning.",
                "author": "Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 2018.",
                "venue": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556\u20132565.",
                "url": null
            }
        },
        {
            "67": {
                "title": "Eva-clip: Improved training techniques for clip at scale.",
                "author": "Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2303.15389"
            }
        },
        {
            "68": {
                "title": "Semi-supervised and un-supervised clustering: A review and experimental evaluation.",
                "author": "Kamal Taha. 2023.",
                "venue": "Information Systems, page 102178.",
                "url": null
            }
        },
        {
            "69": {
                "title": "Efficientnet: Rethinking model scaling for convolutional neural networks.",
                "author": "Mingxing Tan and Quoc Le. 2019.",
                "venue": "In International conference on machine learning, pages 6105\u20136114. PMLR.",
                "url": null
            }
        },
        {
            "70": {
                "title": "Direct and inverse solutions of geodesics on the ellipsoid with application of nested equations.",
                "author": "Thaddeus Vincenty. 1975.",
                "venue": "Scientific Research and Essays, 23:88\u201393.",
                "url": "https://api.semanticscholar.org/CorpusID:120577348"
            }
        },
        {
            "71": {
                "title": "Revise: A tool for measuring and mitigating bias in visual datasets.",
                "author": "Angelina Wang, Arvind Narayanan, and Olga Russakovsky. 2020.",
                "venue": "International Journal of Computer Vision, 130:1790 \u2013 1810.",
                "url": null
            }
        },
        {
            "72": {
                "title": "Overcoming bias in pretrained models by manipulating the finetuning dataset.",
                "author": "Angelina Wang and Olga Russakovsky. 2023.",
                "venue": "arXiv preprint arXiv:2303.06167.",
                "url": null
            }
        },
        {
            "73": {
                "title": "Unsupervised selective labeling for more effective semi-supervised learning.",
                "author": "Xudong Wang, Long Lian, and Stella X. Yu. 2021.",
                "venue": "In European Conference on Computer Vision.",
                "url": null
            }
        },
        {
            "74": {
                "title": "Suggestive annotation: A deep active learning framework for biomedical image segmentation.",
                "author": "Lin Yang, Yizhe Zhang, Jianxu Chen, Siyuan Zhang, and Danny Z Chen. 2017.",
                "venue": "In Medical Image Computing and Computer Assisted Intervention- MICCAI 2017: 20th International Conference, Quebec City, QC, Canada, September 11-13, 2017, Proceedings, Part III 20, pages 399\u2013407. Springer.",
                "url": null
            }
        },
        {
            "75": {
                "title": "Merlot reserve: Neural script knowledge through vision and language and sound.",
                "author": "Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza Salehi, Aditya Kusupati, Jack Hessel, Ali Farhadi, and Yejin Choi. 2022.",
                "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 16354\u201316366.",
                "url": "https://api.semanticscholar.org/CorpusID:245837609"
            }
        },
        {
            "76": {
                "title": "Merlot: Multimodal neural script knowledge models.",
                "author": "Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. 2021.",
                "venue": "In Neural Information Processing Systems.",
                "url": "https://api.semanticscholar.org/CorpusID:235352775"
            }
        },
        {
            "77": {
                "title": "VinVL: Revisiting visual representations in vision-language models.",
                "author": "Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. 2021.",
                "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5579\u20135588.",
                "url": null
            }
        },
        {
            "78": {
                "title": "Recognize anything: A strong image tagging model.",
                "author": "Youcai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li, Zhaochuan Luo, Yanchun Xie, Yuzhuo Qin, Tong Luo, Yaqian Li, Shilong Liu, Yandong Guo, and Lei Zhang. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2306.03514"
            }
        },
        {
            "79": {
                "title": "Men also do laundry: Multi-attribute bias amplification.",
                "author": "Dora Zhao, Jerone Andrews, and Alice Xiang. 2023.",
                "venue": "In International Conference on Machine Learning, pages 42000\u201342017. PMLR.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.07687v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.4"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "5"
        ]
    },
    "research_context": {
        "paper_id": "2403.07687v1",
        "paper_title": "Annotations on a Budget: Leveraging Geo-Data Similarity to Balance Model Performance and Annotation Cost",
        "research_background": "### Paper's Motivation\nThe motivation behind this paper is rooted in the disparity in the performance of vision-language models on diverse datasets, particularly those from underrepresented countries and regions. While recent advances in vision-language models have shown remarkable performance across various tasks, these models struggle with out-of-domain data and data from low-income and non-Western countries. This issue is largely attributed to the imbalanced geographical and economic representation of training data, predominantly sourced from North America and Western Europe. Previous solutions have suggested increasing data collection from underrepresented regions, but the substantial annotation costs pose a significant challenge.\n\n### Research Problem\nThe paper addresses two primary research questions:\n\n1. **Representation Disparity**: Identifying which countries are less represented in the training data of vision-language models. The focus is to pinpoint specific countries and topics that need more data representation to balance the existing datasets.\n   \n2. **Leveraging Geo-Data Similarity**: Exploring how cross-country data similarity can be utilized to improve the representation of vision-language models. The aim is to group visually similar countries to supplement data for underrepresented countries without exceeding the annotation budget.\n\n### Relevant Prior Work\n- **Advances in Vision-Language Models**: Previous work has highlighted the significant progress in vision-language models across various tasks, including object detection, image segmentation, and higher-level tasks like image/video captioning and visual question answering (Li et al., 2019, 2022; Zhang et al., 2021; Radford et al., 2021; Zellers et al., 2021).\n  \n- **Performance Discrepancies**: Studies have shown that these models do not perform well with out-of-domain data and data from low-income and non-Western countries due to training data biases (De Vries et al., 2019; Nwatu et al., 2023).\n  \n- **Geographical and Economic Data Imbalance**: Research has identified the imbalance in the geographical and economic representation in training data, mainly sourced from North America and Western Europe (Shankar et al., 2017).\n  \n- **Strategies to Address Representation**: Proposed solutions include collecting more data from underrepresented regions (Rojas et al., 2022; Ramaswamy et al., 2023). However, these efforts are hindered by high annotation costs associated with fair pay and researcher time.\n\nThis paper contributes by proposing a complementary solution to optimize the annotation budget and effectively enhance the geo-diversity of data in vision-language models.",
        "methodology": "## Methodology\n\n### Data Collection\n- **Low-Resource Data**: A crowd-sourced geo-diverse dataset collected from many countries, limited by the number of images per country, hence referred to as 'low-resource data.'\n- **High-Resource Data**: A vast web-scraped dataset from billions of images used for training foundational models, termed 'high-resource' due to its large size and the ease of data acquisition.\n\n### Data Pre-Processing\n- **Mapping Topics**: Match topics between low-resource and high-resource data sources, filtering out those with very few images.\n- **Grouping and Renaming**: Combine synonymous topics (e.g., merging \"bathroom privacy\" and \"bathroom/toilet\" into \"bathroom\") and standardize Topic Naming (e.g., \"bike\" to \"bicycle\").\n- **Thresholding**: Remove topics from Dollar Street dataset with less than a certain number of images to ensure experiment significance.\n- **Topic-Image Mapping**: Map low-resource topics to corresponding high-resource data or their hyponyms, using ImageNet and LAION datasets. Manual inspection of LAION images ensures topic relevance due to its noisy data.\n\n### Data Integration\n- **Combining Datasets**: Merge the low-resource data with ImageNet and LAION data, addressing imbalances without compromising country representation. Remove (topic, country) pairs with fewer than the threshold number of images.\n\n### Visual Representation Generation\n- **Vision-Language Foundation Models**: Use models like ViT-B/32 from CLIP, EfficientNet from ALIGN setup, and ViT-g/14 from BLIP-2 to encode visual representations of images.\n  - CLIP utilizes 400M image-text pairs from diverse sources, validating its robust image-text alignment.\n  - ALIGN, albeit noisier, brings in 1.8 billion image-text pairs following Conceptual Captions dataset methodology.\n  - BLIP-2 leverages multiple datasets (e.g., COCO, CC3M, CC12M, etc.) to refine visual representation with features extracted before the penultimate layer.\n\n### Data Quality and Balance\n- Pre-processed data is shown in the repository, highlighting the importance of balancing topics and nations for significant results.\n- **Manual Inspection**: Ensures correctness in topic-image mapping from the noisy LAION dataset, reducing dependence on low-quality images.\n\n### Key Components and Innovations\n1. **Geo-Diverse Low-Resource Dataset**: Addresses the imbalance and scarcity of images from various countries.\n2. **High-Resource Data Utilization**: Leverages vast datasets from ImageNet and LAION to supplement low-resource data.\n3. **Mapping and Pre-Processing**: Efficient topic mapping and manual verification maintain data quality and relevance.\n4. **Visual Representation through Advanced Models**: Employs cutting-edge vision-language models (CLIP, ALIGN, BLIP-2) to encode and compare visual data.\n5. **Selective Data Sampling**: Diversifies and limits data from large noisy datasets (LAION) augmenting it with cleaner sources (ImageNet).\n\nBy integrating these components, the proposed methodology aims to balance model performance and annotation costs effectively, utilizing geo-data similarity to maximize the efficiency of low-resource data.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\n**Research Question (RQ1)**: Which countries are less represented in the training data of vision-language models?\n\n#### Setup:\n\n1. **Datasets**: \n   - The experiment utilizes image data categorized by (topic, country) pairs.\n   - The data is divided into low-resource and high-resource categories based on the availability and representation of images per topic and country.\n\n2. **Visual Representations**:\n   - Three types of visual representations are used: CLIP, BLIP, and ALIGN.\n   \n3. **Similarity Computation**:\n   - The cosine similarity is computed between the average visual representations of images in the low-resource data and the corresponding images in the high-resource data. This average is taken over all three visual representation types.\n   \n4. **Threshold Calculation**:\n   - A threshold is determined using the average similarity score between all image representations in the low-resource data and the corresponding ones in the high-resource data. \n\n5. **Selection of Pairs**:\n   - (Topic, country) pairs with a similarity score below the threshold for all three visual representations are identified as needing annotations the most.\n\n6. **Heatmaps and Correlations**:\n   - A similarity heatmap is generated with topics as rows and countries as columns.\n   - Pearson correlations between the similarity scores of the three types of visual representations are computed to validate consistency across different methods.\n\n#### Evaluation Metrics:\n\n1. **Cosine Similarity**: For evaluating similarity between visual representations.\n2. **Pearson Correlation**: For measuring the consistency between different visual representation types.\n\n#### Main Results:\n\n1. **Identification of Under-represented Pairs**:\n   - Out of 1,501 unique (topic, country) pairs, 422 pairs were identified as consistently different from high-resource data and thus benefiting the most from additional annotations.\n   - This selection method potentially reduces the annotation budget to less than a third of the original.\n\n2. **Visualization and Interpretation**:\n   - Some countries with a significant number of data points still have more than half of their topics requiring annotations.\n   \n3. **Examples and Consistency**:\n   - Specific examples provided: e.g., topics in a particular unidentified country \"medicine\", \"spice\", \"ceiling\", \"clothes\", and \"makeup\" require annotations due to visual differences.\n   - Strong Pearson correlations between the visual representation methods: CLIP and BLIP, CLIP and ALIGN, ALIGN and BLIP, indicating the robustness of the similarity trends across different methods.\n\n4. **2D Visualization**:\n   - PCA transformation demonstrates the visual diversity for the topic \"toothbrush\" across various countries, showing clusters of visually similar and dissimilar countries.\n\nThe analysis shows that leveraging geo-data similarity can effectively identify under-represented (topic, country) pairs, thus optimizing annotation efforts for vision-language models. The consistent trends across different visual representations underscore the reliability of the findings."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Identify the countries with the most visually distinct data from those already present in large vision-language foundation model training datasets, focusing on specific topics (objects and actions). The goal is to highlight those that would benefit the most from new annotations.",
            "experiment_process": "For each (topic, country) pair, compute the cosine similarity between the average visual representations in low-resource data and high-resource data using three visual representation types: CLIP, BLIP, and ALIGN. Select pairs with similarity scores lower than a threshold, and identify those consistently different across all three visual representations. Compute similarity heatmaps for each representation type and visualize the differences.",
            "result_discussion": "422 out of 1,501 unique (topic, country) pairs were found to be significantly different from high-resource data, potentially reducing the annotation budget to less than a third. Countries with the most visually distinct data tend to have fewer data points. The similarity scores among visual representation types show strong Pearson correlations, indicating consistency in the similarity trend.",
            "ablation_id": "2403.07687v1.No1"
        },
        {
            "research_objective": "Leverage cross-country visual data similarity to improve vision-language model representation.",
            "experiment_process": "Calculate the cosine similarity between average visual representations of images for each topic across countries, using three visual representation types. Average the similarity values to obtain a final similarity score for each (country, topic) pair.  Create visual similarity distributions and analyze the correlation between visual similarity and geographical distance.",
            "result_discussion": "The most visually distinct country has the lowest similarity score, while the most visually similar country has the highest similarity score. Annotating data from visually similar countries may benefit other countries. Results indicate that visual similarity does not generally correlate with geographical distance, and other factors like culture, climate, and income may play more significant roles in visual similarity.",
            "ablation_id": "2403.07687v1.No2"
        }
    ]
}