{
    "title": "Let\u2019s Think Dot by Dot: Hidden Computation in Transformer Language Models",
    "abstract": "Chain-of-thought responses from language models improve performance across most benchmarks. However, it remains unclear to what extent these performance gains can be attributed to human-like task decomposition or simply the greater computation that additional tokens allow. We show that transformers can use meaningless filler tokens (e.g., \u2018\u2026\u2026\u2019) in place of a chain of thought to solve two hard algorithmic tasks they could not solve when responding without intermediate tokens. However, we find empirically that learning to use filler tokens is difficult and requires specific, dense supervision to converge. We also provide a theoretical characterization of the class of problems where filler tokens are useful in terms of the quantifier depth of a first-order formula. For problems satisfying this characterization, chain-of-thought tokens need not provide information about the intermediate computational steps involved in multi-token computations. In summary, our results show that additional tokens can provide computational benefits independent of token choice. The fact that intermediate tokens can act as filler tokens raises concerns about large language models engaging in unauditable, hidden computations that are increasingly detached from the observed chain-of-thought tokens.111Code is available at https://github.com/JacobPfau/fillerTokens",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Chain-of-thought reasoning improves language model (LM) performance when compared to direct, no chain-of-thought, responses (Wei et al., 2023  ###reference_b17###; Suzgun et al., 2022  ###reference_b14###; Lanham et al., 2023  ###reference_b6###). However, recent empirical work shows that answers arrived at via chains of thought frequently are not faithful to the intermediate reasoning steps taken within the chain (Lanham et al., 2023  ###reference_b6###; Turpin et al., 2023  ###reference_b16###). As a limit case of unfaithfulness, the filler token setting replaces chain-of-thought tokens with arbitrary, repeated tokens, e.g. \u2019\u2026\u2026\u2019, as shown in Figure 1  ###reference_###. By comparing language model performance when given filler tokens instead of chains of thought, we can assess whether a given LM is capable of carrying out cross-token computations that are not reflected in the chain of thought tokens.\nThe most widely used LM alignment methods are purely behavioral. Reinforcement learning from human feedback, constitutional AI, instruction fine-tuning, and automated red-teaming all rely on judging or comparing model output tokens. LMs capable of making use of filler tokens undermine this reliance because the reasoning carried out across filler tokens cannot be judged from the tokens themselves.\nIn this work, we study the strict filler case where filler tokens are repeated dots, \u2019\u2026\u2026\u2019; however, the utility of such tokens depends only on the availability of excess capacity in activation space. The \u2019\u2026\u2026\u2019 case is a minimal version of the more general setting where any sequence of filler tokens is provided between an input prompt and some complex output token. For example, the filler sequence could be \u201cLorem ipsum dolor sit amet, \u2026\u201d or repeating a question back to the user, as long as the string requires minimal computation and precedes a more algorithmically demanding token.\nEmpirically, commercial large language models (LLMs) do not benefit from filler tokens on common QA and math benchmarks; Claude 2 and GPT-3.5 achieve the same performance with filler tokens as they do when responding directly without intermediate tokens (Sachan, 2023  ###reference_b11###; Lanham et al., 2023  ###reference_b6###). However, current LLMs\u2019 limitations cannot be extrapolated to larger scales: The empirical evidence on current LLMs does not clarify whether a failure to use filler tokens is an in-principle limitation of transformer expressivity (or their loss landscapes), or instead, if filler token performance may arise at larger scale. Additionally, it is unclear whether these evaluations targeted tasks where filler tokens would be beneficial. In this work, we demonstrate that transformers trained on the next-token prediction objective can achieve improved performance on certain tasks when given filler tokens, achieving perfect accuracy whereas the no-filler, immediate-answer setting achieves only low accuracy.\n###figure_1### These results also provide interesting insight into how filler tokens extend the expressive power of transformers. As single-token predictors, transformers can only solve problems in a complexity class called , which means transformers cannot express problems like permutation composition or graph connectivity (Merrill & Sabharwal, 2023a  ###reference_b7###; Strobl et al., 2023  ###reference_b13###).\nWhereas linear or polynomial chain-of-thought steps can add power to transformers beyond  (Merrill & Sabharwal, 2023a  ###reference_b7###), transformers remain in  with even a polynomial number of filler tokens.\nThus, unlike for chain of thought, we cannot expect filler tokens to let transformers solve problems outside , e.g. graph connectivity.\nHowever, our results suggest that filler tokens likely extend the expressive power of transformers within .\nIn particular, our results establish that reasoning requiring many nested quantifiers becomes expressible for transformers with filler tokens whereas it is conjectured that no-intermediate-token, immediate-answer transformers cannot solve these problems. We propose synthetic tasks for which transformers without chain of thought have been conjectured inadequate in expressivity (Sanford et al., 2024  ###reference_b12###) and show that using filler tokens, transformers can solve these tasks.\nOur contributions are the following:\nWe construct two synthetic datasets, 3SUM (Figure 3  ###reference_###) and 2SUM-Transform, on which LLAMA transformers fail to solve the task without filler, but achieve 100% and 94% accuracy, respectively, when provided filler tokens.\nWe find that filler token performance increases over immediate answers as the length and complexity of inputs increase (Figures 2  ###reference_### and 5  ###reference_###).\nWe contextualize filler tokens with respect to theoretical expressivity results highlighting that filler-token prompting remains within circuit complexity class , but we show empirically that they do seem to add power within .\nWe find that learning to use filler tokens is difficult and requires specific, dense supervision to converge. Standard chain-of-thought data is insufficient for models to learn to leverage filler tokens effectively, c.f. Section 4.3  ###reference_###.\nTaken together these findings suggest that although current LLMs are unlikely to benefit from filler tokens, this is not an in-principle limitation of current architectures. Given demonstrations of parallelizable task decompositions, we expect that current LLMs would also realize benefits from filler tokens.\n###figure_2###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Recent theoretical work establishes that transformers without additional reasoning tokens are limited to solving only highly parallelizable problems (see Strobl et al., 2023  ###reference_b13### for an overview).\nFormally, Merrill & Sabharwal (2023a  ###reference_b7###) place log-precision transformers in the circuit complexity class , which can be equivalently understood as the class of problems definable in first-order logic with majority quantifiers (Merrill & Sabharwal, 2023b  ###reference_b8###).\nIt follows that problems outside  (those that cannot be defined in first-order majority logic) cannot be solved by transformers without additional reasoning tokens. This includes canonical reasoning problems like composing permutations, graph connectivity, or evaluating boolean formulas.\nThis suggests that\u2014without additional reasoning tokens\u2014transformers are surprisingly limited.\nA natural way to get around these expressiveness limitations is to provide the transformer additional reasoning tokens. When transformers have a chain of thought (i.e., can generate tokens that get added to their input), they can indeed solve problems outside  if the chain of thought is long enough (Merrill & Sabharwal, 2023c  ###reference_b9###; Feng et al., 2023  ###reference_b1###).\nThese results show that chain of thought, in addition to providing a particular decomposition hint for a complex problem, expands the computational power of transformers in a way that is essential for many types of sequential reasoning problems.\nBut what about with filler tokens: i.e., when the context is expanded by appending blank tokens? In this setting, the model clearly cannot benefit from having instructions to follow, but is there still a computational benefit?\nAs long as the number of filler tokens is polynomial, the argument of Merrill & Sabharwal (2023a  ###reference_b7###) goes through to show that transformers with filler tokens can only solve problems in . Merrill & Sabharwal (2023a  ###reference_b7###) show for inputs of size , a transformer can be simulated by an  depth,  size threshold circuit. If we add polynomial filler tokens, this implies we can simulate the circuit with  depth and  size.\nHowever, this does not mean that filler tokens are useless from an expressivity standpoint.\nThere are likely many problems in  that transformers without filler tokens cannot express, including those that fundamentally require resolving many nested quantifiers at the same time (Merrill & Sabharwal, 2023b  ###reference_b8###).\nFiller tokens make problems with deep quantifier nesting clearly solvable: with appropriate positional encodings,222In particular, the construction requires computing  with position arguments. With standard positional encodings, it is not clear whether it is possible to express  in general over all positions. a problem requiring quantifier depth  can be expressed with  filler tokens by using the filler tokens to enumerate over quantified values.\nWe will define such problems and show empirically that transformers cannot solve them without filler tokens, while they can learn to solve them perfectly with filler tokens.\nLanham et al. (2023  ###reference_b6###) and Sachan (2023  ###reference_b11###) both find that, for commercial LLMs, filler tokens generically fail to improve performance over immediate answers when evaluated on NLP and mathematics QA benchmarks.\nPrevious and concurrent research identified cases where token representations contribute to the prediction of tokens occurring multiple indices later showing that, in practice, such contributions both reduce loss on the average case (Janus, 2023  ###reference_b5###; Wu et al., 2024  ###reference_b18###) and can be mechanistically identified via probing (Pal et al., 2023  ###reference_b10###). Complementing these works, we propose filler tokens as a limit case for of coordinated, token-agnostic, non-myopic computation; this case is of particular interest for its expressivity and alignment properties.\nRecent work has also proposed training transformers to predict when further computation is needed for token predictions using pause tokens (Goyal et al., 2024  ###reference_b2###) or meta-tokens (Zelikman et al., 2024  ###reference_b19###). Whereas Goyal et al. (2024  ###reference_b2###) and Zelikman et al. (2024  ###reference_b19###) address the engineering question of how to modify the transformer architecture, language modeling objective, and tokenization process to allow adaptive, filler-like computation; our work addresses the scientific question of under what conditions standard, causal transformers on the unmodified next-token prediction task can learn to use intermediate tokens as filler tokens."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Synthetic data: 3SUM and 2SUM",
            "text": "We would like to understand why previous results found no performance increase from filler tokens on tested LLMs (Lanham et al., 2023  ###reference_b6###). By finding synthetic tasks on which filler tokens improve LM performance, we can determine (1) what kinds of evaluation data can benefit from filler tokens, and (2) what kinds of training data are needed to teach models to use filler tokens (c.f. Section 4.3  ###reference_###). To answer these questions, we construct two synthetic datasets each highlighting a distinct condition under which filler tokens provide performance improvement to transformers.\nThe motivation for this problem comes from two directions. Theoretically, 3SUM is of interest since it is likely not expressible with a single forward pass (as it has quantifier depth greater than 2; c.f. Equation 1  ###reference_###) but is parallelizable\u2013therefore amenable to filler tokens. Intuitively, 3SUM involves simply matching triples of in-context inputs by their meaning. So a demonstration that 3SUM is learnable using filler tokens provides evidence of an expressivity gap between the filler and no filler setting for the general class of nested quantifier resolution problems.\nA secondary, simpler task which involves matching pairs of inputs (summing to zero), but in which we obfuscate the input tokens by applying a transformation only specified in the final token of the input sequence. Leaving the input under-defined until this final transform token prevents in-place computation over input tokens forward passes. The 2SUM-Transform problem is an instance of the more general format in which a question is posed at the end of a long input, as when presenting a document followed by a question about the document."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3SUM Definition and Tokenization",
            "text": "Figure 3  ###reference_### diagrams a simple example of the 3SUM333Sanford et al. (2024  ###reference_b12###) name a variant of this problem \u2019Match-3\u2019. In the Sanford variant, the inputs  and predictions are both multi-hot vectors. problem and the accompanying chain of thought. Formally the 3SUM task statement is: Given ,  as input, predict whether the statement\nis true. Indices i,j,k must be distinct.\nIn the worst case, this task requires considering  summations, i.e.  operations. A standard layer of attention induces only quadratic dependencies between following layer activations and previous layer inputs. Hence, heuristically the 3SUM problem naturally exceeds the expressive capacity of a transformer for large .444The limits of a given transformer size\u2019s expressivity (bounding parameter-counts performance by 3SUM length) is heuristically calculated in Sanford et al. (2024  ###reference_b12###) but the bound is unrealistically large given learning constraints\u2013suggesting small transformers of 10M parameters can solve 3SUM for lengths up to 10,000 inputs. This is a loose bound, which our results show is unrealistic, and a realistic analysis could use sensitivity bounds on learnability as in e.g. Hahn & Rofin (2024  ###reference_b3###).\nOur sequence data consists of input e.g. \u201cA01 B10 C73 D27\u201d, intermediate tokens e.g. \u201c. . .\u201d, and 3SUM-label e.g. \u201cTrue\u201d. Here, \u201cA05\u201d denotes the tuple  and  marks this as the first input, . Inputs are vectorized as multi-hot binary vectors passed to the model as embedding vectors followed by a learned linear layer. The input vectors have masked labels and so do not contribute to the loss.555Besides the masked input tokens, all subsequent tokens are presented as one-hot labels so as to be compatible with the standard cross-entropy language modeling objective. This choice of inputs as embedding vectors and the rest as one-hot tokens is admittedly non-standard (though Sanford et al. (2024  ###reference_b12###) do the same), and was made to reduce the scale of compute needed to realize the separation between no filler and filler settings. To realize the same filler-token to immediate-answer compute gap when using one-hot, digit-wise tokenization of inputs, we would have to increase input length by  which would 2-4x compute cost.\nWe consider three different types of intermediate-token sequences to insert between the problem input and output:\nFiller These sequences use \u201c. . .\u201d, repeated dots, as intermediate tokens e.g. \u201cA05 B75 C22 D13 : . . . . . . . . . . . . ANS True\u201d. These tokens correspond one-to-one with the chain-of-thought tokens below. Each dot is a separate token for a total of  intermediate tokens.\nChain of Thought (Parallelizable CoT Solution) These sequences are of the form: \u201cA05 B75 C22 D13 : AB 70 AC 27 AD 18 BC 97 BD 88 CD B ANS True\u201d.666In practice, we reduce the vocabulary size to accelerate training. We randomly drop one of each paired character in the chain of thought yielding e.g. \u201cA05 B75 C22 D13 : A 7 C 2 D 1 B 9 D 8 C B ANS True\u201d. This change is superficial, since to achieve optimal loss, the predictor must still predict the tokens equivalent to the original (spreading probability mass uniformly). Since wall-clock time for individual gradient steps is linear in sequence length, this change saves us up to a factor of , input dimension, in wall-clock time.\nThis chain of thought reduces the 3SUM problem to a sequence of 2SUM problems by writing all relevant intermediate summations (as shown in Figure 3  ###reference_###). These pairwise sums reduce the cubic cost of 3SUM to the quadratic cost of checking whether an input  exists which matches each pairwise sum\u2013this check can be done using just one attention layer. For each intermediate 2SUM result, if that result matches a third input, we write the index of the third input instead of the sum, as seen at the end of the chain with \u201cCD B\u201d. We choose this particular task decomposition for the chain of thought because it is fully parallelizable.\nChain of Thought (Instance-Adaptive CoT Solution) These sequences are of the form: \u201cA15 B75 C22 D13 : A B C 15 75 22 2 B C D 75 22 13 0 ANS True\u201d (the data-generating process is described in Appendix A  ###reference_###). In the previous, parallelizable solution we neatly factored 3SUM token-wise into parallelizable sub-problems using the same uniform decomposition across all problem instances. However, human reasoning, and the resulting chains of thought, are flexible using instance-specific heuristics to decompose problems as best suits the problem at hand. In particular, when the computation carried out in a later chain-of-thought token depends on the results found in an earlier chain-of-thought token we term this instance-adaptive computation. This kind of computation is incompatible with the parallel structure of filler token computation. Consequently, in order to use filler tokens on natural language data, LLMs would need to discover parallelizable algorithmic solutions given access only to CoT demonstrations lacking parallel structure. By training on instance-adaptive chains of thought, we can study whether models can learn to use filler tokens having seen only more naturalistic chain-of-thought data. These instance-adaptive chains of thought reduce the -dimensional 3SUM problem to a sequence of one-dimensional 3SUM problems. Each triple which sums to zero in the first coordinate is evaluated in its other dimensions individually, so the worst-case777Given our compute constraints, we remove from the training set all sequences having length over 95th percentile. length for these chains of thought is . These serial chains of thought require caching of intermediate results (dimension-wise 3SUMs) and as such cannot be parallelized across filler tokens.\nThe parallelizable CoT solution provides supervision for an algorithm which can be implemented using filler-tokens. To implement this algorithm using filler tokens, individual \u2018.\u2019 tokens compute individual 2SUM results by attending to pairs of inputs\u2013this can be done in one layer. Then the following layer again attends over all inputs checking whether a third matching input exists. The final prediction token can then attend across hidden filler token representations to check whether there exists a representation encoding the zero vector, outputting \u2019True\u2019 if, and only if, 3SUM was satisfied."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "2SUM-Transform",
            "text": "Formally the 2SUM problem is: Given ,  as input, predict\nThis can be done in a single forward pass with a standard transformer, so to demonstrate the utility of filler tokens, we propose the 2SUM-Transform problem in which a permutation888 here denotes the -fold direct product of the cyclic group on  elements\u2013i.e. each digit of every tuple is permuted independently.  is used to obscure the input sequence. This permutation shifts every digit of the input tokens by a random offset. The resulting 2SUM-Transform input is then . We randomly sample 10 such permutations999For 2SUM, a brute-force solution, without knowledge of the transform, requires  comparisons where  is the number of possible transformations.  and uniformly at random sample a permutation to apply for each sample in the dataset.\nFor 2SUM, we use only linearly many chain-of-thought tokens which correspond to the un-transformed . Mimicking the realistic setting in which a LLM might use repeating back the question as filler tokens, we use filler token sequences of the following form for 2SUM: \u201c97 80 94 44  97 . 80 . 94 . ANS:4\u201d. Here, \u201c97 80 94 44\u201d are the permuted inputs ; \u201c\u201d denotes which permutation was applied; and \u201c97 . 80 . 94 .\u201d are the filler tokens\u2013the input repeated back. We train on uniform mixtures of filler-token and chain-of-thought sequences. Chain-of-thought sequences are of the form: \u201c17 84 09 39  17 08 84 73 09 35 ANS:2\u201d, sequentially listing  and . For 2SUM training, we use a binary cross-entropy loss, since both inputs and chain-of-thought data are multi-hot vectors. Causal masking is applied as per the standard language-modelling objective."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "3SUM: transformers converge with filler tokens and fail without",
            "text": "Figure 2  ###reference_### shows that, as expected, for length-6, dimension-3 3SUM instances, 3SUM is learnable both with and without filler tokens. However, as we scale the length of inputs up to length 12, we find increasing performance gaps: The no-filler models achieve near-random accuracy at 66%, whereas with filler tokens, accuracy remains 100%.\nGiven a model trained on filler tokens, we fine-tune the final attention layer (freezing all earlier layers) to predict the solution given reduced numbers of filler tokens. Figure 4  ###reference_### shows that when decoding from learned representations on filler tokens, a frozen model yields monotonically improving predictions as we allow additional filler tokens. Here each point represents a different final-layer fine tune. In Figure 4  ###reference_###, the first half of the filler tokens appear crucial, achieving  performance while using only  of the total filler tokens. This early convergence given half the total filler tokens is to be expected for an efficient algorithm solving 3SUM, since each pair of inputs needs to be summed only once for a total of  comparisons\u2013whereas the total number of filler tokens we provide is .\nGiven the possibility of non-linear, learned probes confounding the interpretation of representations with the probes\u2019 own computation, we compare to the following control condition (Hewitt & Liang, 2019  ###reference_b4###). This ensures that the observed filler-token vs accuracy scaling (Figure 4  ###reference_###) reflects the frozen model layers\u2019 representations and not the probe itself. For the control task, we take a model trained with filler tokens on sufficiently simple 3SUM sequences for which immediate, no-filler, solutions are tractable: length-10, dimension-1 data.131313This was determined by training another model without any intermediate tokens and observing that model achieved 100% accuracy. To confirm that the probe results in Figure 4  ###reference_### reflect filler-token utility, we must confirm that the baseline probe on the dimension-1 control data does not find filler-token representations to be useful. As expected, we find that this length-10, dimension-1 model can achieve 100% accuracy given only  of the original number of filler tokens.1414142% was the minimum number tested, it is likely no filler tokens are necessary. In effect, our probe finds filler token representations are redundant in models which have the expressive capacity to solve problems without filler tokens.\n###figure_3### Figure 2  ###reference_### showed that to take advantage of filler tokens to solve more complex problems than immediate-answer response can, 4 layer models realize maximal benefit starting at length-12 3SUM inputs, that is sequences of token length . Our experiments required  filler tokens to realize an expressivity gap over the no-filler response. This raises the question of whether LLMs with tens or hundreds of layers require prohibitively many filler tokens to see improved performance over the no-filler baseline? To answer this question, we show the effects of scaling input complexity, i.e. dimension, instead of input length, realizing performance gaps at lower filler token counts.\nFigure 5  ###reference_### shows that for fixed length inputs, by increasing input dimension, we can realize performance gaps between the immediate-answer and the no-filler settings for even length-8 inputs. However, given that to realize this performance gap, minimally 6-dimensional inputs are required (when using a small, 34M LM) we expect integer addition tasks will not offer suitably rich structures for taking advantage of filler tokens when using large models\u2014natural-language tasks may offer alternatives. In these experiments, we used as our no-filler baseline a model trained on a 50/50 mixture of filler token sequences, and instance-adaptive CoT (the evaluation is done on the filler token sequences only). We use this mixed-dataset baseline rather than training on only immediate-response sequences, because the mixed-dataset models outperform the immediate-response-only models. In Appendix B  ###reference_###, we provide further results on the effects of scaling tuple dimensionality for length-10 inputs."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experimental Setup",
            "text": "We use a 34M-parameter Llama model with 4 layers, 384 hidden dimension, and 6 attention heads (Touvron et al., 2023  ###reference_b15###). This is a scaled-down, randomly-initialized version of the Llama model. Input 2SUM and 3SUM vectors are given as hard-coded, multi-hot embedding vectors which are projected through a learned dimensional linear layer.101010For 3SUM, following the notation of Equation 1  ###reference_###. These dimensions correspond to tuple digits and hard-coded positional values. Intermediate tokens (filler and chain-of-thought) are given as one-hot tokens. We use Adam with a learning rate of 1e-4 throughout. For all filler and chain-of-thought runs we use a weight decay of 0.01 and gradient clip at norm 1. These hyper-parameters were chosen as standard defaults without hyper-parameter tuning. For the immediate-answer runs, we use a weight decay of 0.1 and gradient clip at norm 0.5; this change was made because using the original set of hyper-parameters leads to loss spikes and training instability. We train on 10,000,000 samples and test on 2,000 samples. We train to convergence: for 5 epochs using random selection of input sequences without any coherent structure or training data, and 25 epochs in the no-filler setting (see Appendix C  ###reference_### for loss plots). We always report the per-run maximum validation performance across epochs, i.e. early-stop performance."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Results",
            "text": "###figure_4### To show that filler tokens confer greater expressive capacity, letting transformers solve hard problems, we must show that transformers without filler cannot solve our task, 3SUM. In particular, we require evidence that the non-learnability of 3SUM in the no-filler setting is due to expressive capacity and not simply a difference in the particularities of the data distribution and presentation. To this end, we show that for short enough inputs, 3SUM can be solved without filler tokens, but for longer inputs, 3SUM cannot be solved without filler tokens. The length and dimension scaling experiments below were trained on a  split of filler and chain-of-thought data. In the below experiments, Figure 2  ###reference_### and Figure 5  ###reference_###, we consider two cases:\n(Blue bars, No-filler Case) Test on immediate-answer, no-intermediate-tokens, data. In Figure 2  ###reference_###, train on immediate-answer, no-intermediate-tokens, data only. In Figure 5  ###reference_###, train on a  mixture of CoT and immediate-answer data.111111Training data varies in order to ensure the strongest baseline, no-intermediate-token performance possible.\n(Brown bars, Filler-tokens Case) Test on filler-token sequences only. Train on a uniform, , mixture of chain-of-thought and filler-token sequences.121212Models converge to  accuracy on chain-of-thought data as well, but we do not show this in figures for simplicity.\nFigure 2  ###reference_###  ###reference_### shows that, as expected, for length-6, dimension-3 3SUM instances, 3SUM is learnable both with and without filler tokens. However, as we scale the length of inputs up to length 12, we find increasing performance gaps: The no-filler models achieve near-random accuracy at 66%, whereas with filler tokens, accuracy remains 100%.\nGiven a model trained on filler tokens, we fine-tune the final attention layer (freezing all earlier layers) to predict the solution given reduced numbers of filler tokens. Figure 4  ###reference_###  ###reference_### shows that when decoding from learned representations on filler tokens, a frozen model yields monotonically improving predictions as we allow additional filler tokens. Here each point represents a different final-layer fine tune. In Figure 4  ###reference_###  ###reference_###, the first half of the filler tokens appear crucial, achieving  performance while using only  of the total filler tokens. This early convergence given half the total filler tokens is to be expected for an efficient algorithm solving 3SUM, since each pair of inputs needs to be summed only once for a total of  comparisons\u2013whereas the total number of filler tokens we provide is .\nGiven the possibility of non-linear, learned probes confounding the interpretation of representations with the probes\u2019 own computation, we compare to the following control condition (Hewitt & Liang, 2019  ###reference_b4###  ###reference_b4###). This ensures that the observed filler-token vs accuracy scaling (Figure 4  ###reference_###  ###reference_###) reflects the frozen model layers\u2019 representations and not the probe itself. For the control task, we take a model trained with filler tokens on sufficiently simple 3SUM sequences for which immediate, no-filler, solutions are tractable: length-10, dimension-1 data.131313This was determined by training another model without any intermediate tokens and observing that model achieved 100% accuracy. To confirm that the probe results in Figure 4  ###reference_###  ###reference_### reflect filler-token utility, we must confirm that the baseline probe on the dimension-1 control data does not find filler-token representations to be useful. As expected, we find that this length-10, dimension-1 model can achieve 100% accuracy given only  of the original number of filler tokens.1414142% was the minimum number tested, it is likely no filler tokens are necessary. In effect, our probe finds filler token representations are redundant in models which have the expressive capacity to solve problems without filler tokens.\n###figure_5### Figure 2  ###reference_###  ###reference_### showed that to take advantage of filler tokens to solve more complex problems than immediate-answer response can, 4 layer models realize maximal benefit starting at length-12 3SUM inputs, that is sequences of token length . Our experiments required  filler tokens to realize an expressivity gap over the no-filler response. This raises the question of whether LLMs with tens or hundreds of layers require prohibitively many filler tokens to see improved performance over the no-filler baseline? To answer this question, we show the effects of scaling input complexity, i.e. dimension, instead of input length, realizing performance gaps at lower filler token counts.\nFigure 5  ###reference_###  ###reference_### shows that for fixed length inputs, by increasing input dimension, we can realize performance gaps between the immediate-answer and the no-filler settings for even length-8 inputs. However, given that to realize this performance gap, minimally 6-dimensional inputs are required (when using a small, 34M LM) we expect integer addition tasks will not offer suitably rich structures for taking advantage of filler tokens when using large models\u2014natural-language tasks may offer alternatives. In these experiments, we used as our no-filler baseline a model trained on a 50/50 mixture of filler token sequences, and instance-adaptive CoT (the evaluation is done on the filler token sequences only). We use this mixed-dataset baseline rather than training on only immediate-response sequences, because the mixed-dataset models outperform the immediate-response-only models. In Appendix B  ###reference_###  ###reference_###, we provide further results on the effects of scaling tuple dimensionality for length-10 inputs."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Filler Tokens Only Improve Performance Given Parallelizable CoT Demonstrations",
            "text": "Despite transformers having the expressive capacity to solve certain filler-token tasks, learning filler token computations poses a hard learning problem. There are two reasons for this: First, it is impossible to densely supervise filler token solutions, because by assumption, filler tokens are used in precisely those cases when underlying, hidden computation decorrelates from the meaning of the corresponding tokens. Second, algorithms learned from chain-of-thought data generically require instance-adaptive, serial computation (Merrill & Sabharwal, 2023c  ###reference_b9###)\u2013such computation is incompatible with the parallel structure of filler-token compute.\nTo quantify the effects of these learning obstacles, we run two ablations: First, we train models on filler-token-only sequences to evaluate the difficulty of learning filler-token computation in the absence of parallelizable chain-of-thought data. In this case, we train on length-14, dimension-3 data, and performance remains at  accuracy across all three random initializations. This performance is the same as the no-filler, immediate-answer condition observed in Figure 2  ###reference_###.\nIn our second ablation, we train on data using instance-adaptive chain-of-thought sequences (described in Section 3.1  ###reference_###). We find that models trained on instance-adaptive CoT data fail to use filler tokens. On filler token sequences, the resulting models remain at, or below, no-intermediate-token, baseline performance, Figure 6  ###reference_###. This indicates that there is no transfer from serial, instance-adaptive demonstrations to filler tokens for the 3SUM problem."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "2SUM Experiments",
            "text": "In the 2SUM setting, a transformer with no filler performs well above random, but significantly below the same model when trained with filler, as shown in Table 1  ###reference_###. Table 1  ###reference_### reports the maximum performance across five random initializations, because we observe significant variance across runs. The chain-of-thought and filler-token results use the same model but evaluate on disjoint subsets of the test data. Filler-token performance approaches chain-of-thought performance, recovering  of the benefits of chain-of-thought tokens over the immediate-answer baseline. We also experimented with training the no-filler model on a mixture of no-filler and chain-of-thought data; this under-performs relative to direct training on no-filler sequences. We use the same hyper-parameters as are used for 3SUM, c.f. Section 4.1  ###reference_###."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "When are the benefits of chain-of-thought reasoning in transformer LMs due to interpretable, serial problem decompositions, or simply additional forward passes? We have seen that, for certain parallelizable problems, transformers achieve improved performance when given filler tokens instead of chain-of-thought tokens. This performance gap demonstrates that, given adequate training data, intermediate tokens between input and answer may be used purely for their computational capacity rather than for the human-like, faithful serial reasoning which such human-generated text represents. In such cases, the intermediate tokens are at best non-informative, as in the \u2019\u2026\u2026\u2019 case, and at worst misleading insofar as they describe reasoning unrelated to the computations occurring in intermediate-token, hidden representations.\nWe have offered a theoretical case for filler token usefulness, the quantifier depth  case, and empirical evidence that filler token usage can be efficiently learned. Returning to our original question of whether LLMs should be expected to make use of filler tokens in the future, we can reduce the problem to asking: First, to what extent do token-parallelizable,  algorithmic problems arise in the natural language context? Second, to what extent does natural-language text provide adequate supervision for filler-token computation, providing parallelizable supervision rather than non-parallelizable, instance-adaptive chains of thought? If these conditions are met, we expect filler token usage to emerge in LLMs."
        }
    ],
    "url": "http://arxiv.org/html/2404.15758v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "3.1",
            "3.2",
            "4",
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "research_context": {
        "paper_id": "2404.15758v1",
        "paper_title": "Let\u2019s Think Dot by Dot: Hidden Computation in Transformer Language Models",
        "research_background": "**Motivation:**\n\nThe primary motivation of this paper is to explore and understand the potential benefits of using filler tokens, such as repeated dots ('\u2026\u2026'), in transformer language models (LMs). This motivation is driven by empirical observations that current chain-of-thought reasoning, while improving performance, often leads to intermediate reasoning steps that are not faithful to the final answer. By investigating the utility of filler tokens, the paper aims to uncover whether LMs can perform cross-token computations that are not evident from the immediate response tokens themselves.\n\n**Research Problem:**\n\nThe research problem addressed in this paper is whether transformer language models can achieve improved performance on certain tasks by using filler tokens. The paper seeks to determine if the use of filler tokens provides a computational advantage and extends the expressive power of transformers, particularly in tasks that require complex reasoning with nested quantifiers. The study further explores whether this potential benefit is constrained by the expressivity limits of transformers or if it is simply a matter of the current model scales and training approaches.\n\n**Relevant Prior Work:**\n1. **Chain-of-Thought Reasoning:** Previous studies, such as those by Wei et al. (2023), Suzgun et al. (2022), and Lanham et al. (2023), have shown that chain-of-thought reasoning improves language model performance. However, chain-of-thought answers often lack faithfulness to the intermediate reasoning steps (Lanham et al., 2023; Turpin et al., 2023).\n   \n2. **Model Alignment Methods:** Common LM alignment methods like reinforcement learning from human feedback, constitutional AI, and instruction fine-tuning rely on judging model outputs. These methods might be undermined by models capable of utilizing filler tokens since the reasoning across filler tokens is not easily judged by output tokens alone.\n   \n3. **Empirical Studies on Filler Tokens:** Empirical evidence indicates that commercial LLMs like Claude 2 and GPT-3.5 do not show performance gains with filler tokens on common QA and math benchmarks (Sachan, 2023; Lanham et al., 2023). However, this does not clarify if this limitation is due to the fundamental expressivity of transformers or if benefits might emerge at larger scales or in different tasks.\n   \n4. **Theoretical Expressivity Limits:** Prior work has pointed out that transformers, as single-token predictors, belong to the complexity class , limiting them from solving problems like permutation composition or graph connectivity (Merrill & Sabharwal, 2023a; Strobl et al., 2023). Linear or polynomial chain-of-thought steps, however, can extend transformer capabilities beyond this class, although filler tokens are still within .\n\n5. **Transformer Task Decomposition:** Sanford et al. (2024) suggested that transformers without intermediate tokens are inadequate for certain expressive tasks. This paper examines whether filler tokens can help transformers solve such conjectured inadequate tasks.\n\nThe research builds upon these works by experimentally demonstrating that transformers given filler tokens can perform significantly better on tasks requiring complex reasoning than those given immediate-answer prompts.",
        "methodology": "### Methodology\n\nThe methodology in this paper aims to investigate the impact of filler tokens on the performance of large language models (LLMs) by crafting synthetic datasets tailored to specific conditions under which these tokens might provide a benefit.\n\n1. **Objective**: \n   - **Evaluate the Effect of Filler Tokens**: Determine what types of evaluation and training data enable models to effectively use filler tokens.\n   - **Identify Beneficial Data Conditions**: Find out what constraints or situations enhance model performance through filler tokens.\n\n2. **Synthetic Datasets**: \n   - **3SUM Task**: \n     - **Theoretical Motivation**: It suggests that the 3SUM problem might surpass the expressive abilities of a single forward pass due to its quantifier depth, indicating a need for a strategy like filler tokens.\n     - **Intuitive Explanation**: Involves matching triples of context-based inputs by their semantic meaning. Success in learning the 3SUM task using filler tokens would signal a notable expressivity gap between models trained with and without filler tokens, particularly for nested quantifier resolution problems.\n   - **2SUM-Transform Task**: \n     - **Task Description**: While simpler than 3SUM, this task requires models to find pairs of inputs that sum to zero. Complexity is introduced by transforming the input tokens in a way that is only revealed in the final token of the sequence.\n     - **Transform Mechanism**: This late specification transformation obfuscates the input, making it impossible for the model to perform computations on the input tokens in a single forward pass effectively. This simulates scenarios where a question about a long document is posed at the end of the document.\n\nBy designing these two synthetic tasks, the study aims to uncover the conditions where filler tokens can meaningfully enhance LLM performance, bridging an understanding gap in previous research (Lanham et al., 2023). This approach provides a structured way to explore the hidden computational capabilities of transformer-based models through the innovative application of filler tokens.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\n**Datasets:**\n- The experiment involves 3SUM problem instances with varying input lengths and dimensions.\n- Key datasets include:\n  - Length-6, dimension-3 instances.\n  - Length-12 instances.\n  - Control task with length-10, dimension-1 data to handle simpler 3SUM sequences.\n\n**Baselines:**\n- A model trained with filler tokens.\n- A no-filler baseline model achieving near-random accuracy at 66% on length-12 inputs.\n- For control conditions, a model trained on sufficiently simple sequences that do not require filler tokens, achieving 100% accuracy on dimension-1 data.\n\n**Evaluation Metrics:**\n- Accuracy of predicting the correct solution to the 3SUM problem.\n\n**Main Experimental Results:**\n1. **Length-6, Dimension-3 Instances:**\n   - Models, both with and without filler tokens, achieved learnability success.\n\n2. **Length-12 Inputs:**\n   - Models with filler tokens maintained 100% accuracy.\n   - No-filler models performed poorly, achieving only about 66% accuracy.\n\n4. **Control Task Results:**\n   - On length-10, dimension-1 sequences, models achieved 100% accuracy with minimal (2%) filler tokens.\n   - Demonstrated that filler tokens add redundancy when the model has sufficient expressivity without them.\n\n6. **Mixed Dataset Baseline:**\n   - Trained on a mix of filler token sequences and instance-adaptive CoT, used for evaluation.\n   - Outperformed models trained solely on immediate-response sequences.\n\nOverall, the experiments indicate the significant role of filler tokens in learning complex sequences and solving the 3SUM problem efficiently compared to no-filler models. The effects were particularly pronounced in higher-dimensional inputs and when using a mixed dataset for training and evaluation."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To understand why previous results found no performance increase from filler tokens on tested LLMs and determine what kinds of evaluation and training data can benefit from filler tokens.",
            "experiment_process": "The experiment utilized two synthetic datasets: 3SUM and 2SUM-Transform. The 3SUM dataset required models to predict if the sum of three distinct indices equaled zero, using sequences that included intermediate tokens (filler tokens, Chain of Thought (CoT) with parallelizable solutions, and CoT with instance-adaptive solutions). Filler sequences used repeated dots (e.g., '...') while CoT sequences logically decomposed the 3SUM task. Similarly, the 2SUM-Transform task involved matching pairs of inputs with permuted tokens, obfuscating the input tokens using a final transformation token to prevent in-place computation.",
            "result_discussion": "The results indicated that fillers can improve model performance on both 3SUM and 2SUM-Transform tasks. The 3SUM task saw greater performance gaps between models with and without filler tokens as input lengths increased. Fine-tuning on reduced filler tokens also showed that the models retained understanding and could make accurate predictions. These findings suggest that filler tokens can serve computational benefits by providing structure, even when the intermediate steps do not carry meaningful task information.",
            "ablation_id": "2404.15758v1.No1"
        },
        {
            "research_objective": "To determine if transformers trained with/without filler tokens can learn to solve the 3SUM problem effectively and if filler tokens confer greater expressive capacity.",
            "experiment_process": "The experimental setup used a 34M-parameter Llama model with 4 layers, hidden dimensions of 384, and 6 attention heads. Input vectors were multi-hot embedding vectors, passed through a learned 64-dimensional linear layer. The experiments consisted of two cases: training and testing on no-filler (immediate-answer) data, and training on a mix of CoT and filler tokens while testing on filler-token sequences. The task was scaled in length and dimension to observe expressivity gaps under these conditions, utilizing standard training setups with Adam optimizer and specific hyper-parameters.",
            "result_discussion": "The results showed that the 3SUM problem is unsolvable without filler tokens beyond short inputs, revealing the impact of input length and dimensional increases on performance gaps. In complex scenarios with filler tokens, models achieved significantly higher accuracy. This highlighted the computational benefits of filler tokens and their role in enabling transformers to address more complex tasks, pointing to their utility in natural language tasks requiring extended computations.",
            "ablation_id": "2404.15758v1.No2"
        },
        {
            "research_objective": "To assess the impact of learning obstacles on filler-token performance and to evaluate the necessity of parallelizable CoT demonstrations for filler token utility.",
            "experiment_process": "Two ablation studies were conducted. First, models were trained only on filler-token sequences (length-14, dimension-3) to evaluate the difficulty in learning filler-token computations without parallelizable CoT data. Second, models were trained using instance-adaptive CoT sequences to see if they could transfer this learning to filler-token computations. Performance was evaluated across multiple initializations and compared to baseline conditions.",
            "result_discussion": "Training on filler-token-only sequences resulted in performance similar to the no-filler, immediate-answer condition, indicating significant learning difficulties. Moreover, training on instance-adaptive CoT sequences did not transfer to filler tokens, with models failing to improve on filler-token data, underscoring that parallelizable CoT data is critical for learning filler-token computations efficiently. These results demonstrate the complexity of training models to use filler tokens effectively without dense supervision and suitable problem structures.",
            "ablation_id": "2404.15758v1.No3"
        }
    ]
}