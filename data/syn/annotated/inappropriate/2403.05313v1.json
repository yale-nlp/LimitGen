{
    "title": "RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation",
    "abstract": "We explore how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models\u2019 reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination.\nIn particular, the proposed method \u2014 retrieval-augmented thoughts (RAT)\n\u2014 revises each thought step one by one with retrieved information relevant to the task query, the current and the past thought steps, after the initial zero-shot CoT is generated.\nApplying RAT to GPT-3.5, GPT-4, and CodeLLaMA-7b substantially improves their performances on various long-horizon generation tasks; on average of relatively increasing rating scores by 13.63% on code generation, 16.96% on mathematical reasoning, 19.2% on creative writing, and 42.78% on embodied task planning.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large Language Models (LLMs) have achieved fruitful progress on various natural language reasoning tasks (Wei et al., 2022  ###reference_b56###; Yao et al., 2022  ###reference_b58###; Wang et al., 2023a  ###reference_b53###; Zhou et al., 2023  ###reference_b66###; Brown et al., 2020  ###reference_b6###), especially when combining large-scale models  (Team, 2022  ###reference_b47###; OpenAI, 2023  ###reference_b39###) with sophisticated prompting strategies, notably  chain-of-thought (CoT) prompting (Wei et al., 2022  ###reference_b56###; Kojima et al., 2022  ###reference_b26###). However, there have been increasing concerns about the factual correctness of LLMs reasoning, citing the possible hallucinations in model responses (Rawte et al., 2023  ###reference_b41###) or the intermediate reasoning paths, i.e. CoTs (Dhuliawala et al., 2023  ###reference_b13###). This issue becomes more significant when it comes to zero-shot CoT prompting, aka. \u201clet\u2019s think step-by-step\u201d (Kojima et al., 2022  ###reference_b26###) and long-horizon generation tasks that require multi-step and context-aware reasoning, including code generation, task planning, mathematical reasoning, etc. Factually valid intermediate thoughts could be critical to the successful completion of these tasks.\nSeveral prompting techniques have been proposed to mitigate this issue, one promising direction,  Retrieval Augmented Generation (RAG) (Lewis et al., 2020b  ###reference_b28###) seeks insights from human reasoning (Holyoak and Morrison, 2012  ###reference_b21###), and utilizes retrieved information to facilitate more factually grounded reasoning.\nIn this paper, we explore how to synergize RAG with sophisticated long-horizon reasoning. Our intuition is that the hallucination within the intermediate reasoning process could be alleviated through the help of outside knowledge. The resulting prompting strategy, retrieval-augmented thoughts (RAT), is illustrated in Figure 1  ###reference_###. Our strategy comprises two key ideas. Firstly, the initial zero-shot CoT produced by LLMs along with the original task prompt will be used as queries to retrieve the information that could help revise the possibly flawed CoT. Secondly, instead of retrieving and revising with the full CoT and producing the final response at once, we devise a progressive approach, where LLMs produce the response step-by-step following the CoT (a series of subtasks), and only the current thought step will be revised based on the information retrieved with task prompt, the current and the past CoTs. This strategy can be an analogy to the human reasoning process: we utilize outside knowledge to adjust our step-by-step thinking during complex long-horizon problem-solving (Holyoak and Morrison, 2012  ###reference_b21###). A comparison of RAT and counterparts can be found in Figure 2  ###reference_###.\nWe evaluate RAT on a wide collection of challenging long-horizon tasks, including code generation, mathematical reasoning, embodied task planning, and creative writing. We employ several LLMs of varied scales: GPT-3.5 (Brown et al., 2020  ###reference_b6###), GPT-4 (OpenAI, 2023  ###reference_b39###), CodeLLaMA-7b (Rozi\u00e8re et al., 2023  ###reference_b44###). The results indicate that combing RAT with these LLMs elicits strong advantages over vanilla CoT prompting and RAG approaches. In particular, we observe new state-of-the-art level of performances across our selection of tasks: 1) code generation: HumanEval (+20.94%), HumanEval+ (+18.89%), MBPP (+14.83%), MBPP+ (+1.86%); 2) mathematical reasoning problems: GSM8K (+8.36%), and GSMHard (+31.37%); 3) Minecraft task planning (2.96 times on executability and +51.94% on plausibility); 4) creative writing (+19.19% on human score). Our additional ablation studies further confirm the crucial roles played by the two key ingredients of RAT: revising CoT using RAG and progressive revision & generation. This work reveals how can LLMs revise their reasoning process in a zero-shot fashion with the help of outside knowledge, just as what humans do."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Retrieval Augmented Thoughts",
            "text": "Our goal is to support long-horizon reasoning and generation while mitigating hallucination when using LLMs. To have satisfying performance on long-horizon tasks, two ingredients are indispensable. Firstly, access to factual information can be facilitated by retrieval. Secondly, appropriate intermediate steps that outline a scratchpad to finish complex tasks, can be facilitated by CoT. Yet, a naive combination of the two would not necessarily yield improvements. Two questions still persist: (1) what is relevant information to retrieve; (2) how to effectively correct reasoning steps with relevant factual information. To better appreciate our method and why our method can address these two questions, we first provide a brief preliminary introduction of RAG and CoT."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Preliminary",
            "text": "Retrieval-Augmented Generation (RAG) targets the problem of generating fictitious facts by providing LLMs with relevant text extracted from trusted sources. It is primarily used in question-answering (QA) tasks (Lewis et al., 2020b  ###reference_b28###). Specifically, given a set of  candidate documents , RAG aims to retrieve the most relevant ones w.r.t. a query , which can be the question/task prompt itself or relevant information generated by LLMs. To achieve this, RAG first extracts semantic-aware embeddings of the documents  ( is the size of the embedding) as well as the query .  can be implemented with various text embedding models, such as Sentence-BERT (Reimers and Gurevych, 2019  ###reference_b42###). The relevance between the query and a document is measured by their cosine similarity:\nBased on their relevance, the top-ranked  documents are then fed into the prompt for LLMs to generate the final answer. With such rich and factual contexts, RAG mitigates the hallucination of LLMs. However, complex reasoning tasks (e.g., those requiring multi-step reasoning) can be difficult to translate into effective search queries, leading to challenges in finding relevant documents and making RAG less applicable. Traditionally, RAG retrieves all relevant information at once. Yet, it overlooks the fact that it is difficult to predict what \u201cfacts\" or information is required in the subsequent reasoning and generation steps. The task prompt itself is hardly sufficient to provide enough clues for this.\nChain of Thoughts (CoT) prompting is designed to enhance the performance of LLMs under tasks that require complex reasoning steps (Wei et al., 2022  ###reference_b56###), such as multi-step math word problems. Specifically, instead of tasking LLMs to generate the correct answer directly, CoT prompting incentivizes LLMs to first output intermediate reasoning steps, termed thoughts, that serve as a scratch space for the task, before summarizing the thoughts into a final answer. Such behavior of LLMs can either be stimulated in zero-shot by prompting terms that encourage CoT reasoning (e.g., \u201clet\u2019s think step by step\u201d) (Kojima et al., 2022  ###reference_b26###), or triggered by few-shot examples that perform CoT in similar tasks. However, since no direct supervision is posed to the intermediate thoughts, LLMs could make errors due to the lack of relevant domain knowledge (Touvron et al., 2023  ###reference_b48###) or biased by hallucinations (Rawte et al., 2023  ###reference_b41###)."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Our Approach",
            "text": "Our intuition to mitigate the issues of CoT prompting and RAG mentioned above is to use a fixed query for all retrieval attempts without revising based on current thought steps. An overview can be found in Figure 1  ###reference_### and Algorithm 1  ###reference_###. Specifically, given a task prompt, we first prompt LLM to generate step-by-step thoughts in zero-shot (\u201clet\u2019s think step-by-step\u201d), where  represents the th thought step. In long-horizon generation tasks,  can either be the intermediate reasoning steps, e.g., the pseudo code with comments in code generation, article outline in creative writing, etc., or the draft response itself, e.g., a list of sub-goals in embodied task planning as shown in Figure 1  ###reference_###.\nSince  could be flawed (e.g., contains hallucination), we proceed by not modifying the retrieval query with respect to any specific thought step and directly generate the final response from these thoughts. Specifically, assuming we have fixed the previous thought steps and now are about to handle , we begin with a pre-defined query  that remains the same for retrieval:\nwhere  can be a generic text encoder or an LLM that generates a query not based on any specific thought but rather a general query  that can be processed by the retrieval system. We adopt RAG to retrieve relevant documents  using this fixed query, which are then used to form a revised thought step .\nFinally, depending on the actual task, the thought steps  can simply be used as the final model response, e.g., embodied task planning. For tasks like code generation, or creative writing, the LLM will be further prompted to produce the complete response (code, passage) from each thought step in a step-by-step fashion.\nThis approach does not take into account the current step  or the complete chain of thoughts  to produce the query for RAG, maintaining a constant query  throughout the process, without revising based on  or previous revised steps :\nThis approach limits the correction of errors in the original thoughts  by consulting the same reference texts for all steps and may not improve the accuracy of each reasoning step with the most relevant information.\nOur hypothesis why this method might address issues is as follows. Firstly, the most straightforward way to know what information will be used in complex reasoning is to \u201csee\u201d the reasoning steps. Our approach leverages all the generated thoughts along with the task prompt without narrowing down retrieval to enhance retrieval. Secondly, some information cannot be directly retrieved, especially information related to the final answer to a hard complex question. Instead, a retrieval approach that targets a broad understanding relevant to intermediate questions, which are assumed to be easier, is more accessible. Despite the compositional nature of many reasoning tasks, this fixed retrieval process may not be as effective. Thirdly, correcting potential hallucinations needs careful revision, but consistently using the same retrieval may miss targeted corrections. This approach could still address two major concerns raised. Quantitative evidence can be found in our ablation studies in Section 3.4  ###reference_###."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We test our proposed method RAT on a diverse set of benchmarks that highlight long-horizon generation and reasoning. Existing methods traditionally struggle in those benchmarks; \u201challucinated\" steps are obvious in LLMs\u2019 outputs. Those steps either fail to stick to the original query or are plainly invalid. We kindly refer readers to subsection 3.3 (case analysis) for a more detailed discussion. Due to space constraints, we do not introduce each benchmark setting, nor do we discuss our results in each benchmark in full length. Rather, this section provides a comprehensive demonstration of our method\u2019s performance and provides a spotlight to provide preliminary empirical analysis about why and when our method works and when it fails.\nFor code generation, the classical pass rate pass@k is selected as the evaluation metrics (Chen et al., 2021  ###reference_b9###; Liu et al., 2023b  ###reference_b35###),  denotes the sampling number. We compute accuracy to evaluate every question in mathematical reasoning tasks, aligning with the established metric for the GSM8K (Cobbe et al., 2021  ###reference_b10###). For embodied planning tasks, we compute the plan execution success rate in MC-TextWorld as executability (Lin et al., 2023  ###reference_b32###). We also conduct human elo rating evaluation to compute the trueskill rating score (Herbrich et al., 2006  ###reference_b20###) for embodied planning (as plausibility) and creative writing tasks. These indicators are better the higher they are.\nTo establish a comprehensive and equitable comparison landscape, we incorporate a suite of baseline methods. Our baselines include the original language models, referred to as DIRECT, and the Retrieval-Augmented Generation (RAG) methodology with  retrieved examples, instantiated in both single-shot (1 shot) and multi-shot (5 shots) configurations, as documented by Lewis et al. (2020b  ###reference_b28###). Additionally, we examine the zero-shot CoT (CoT) approach, as conceptualized by Kojima et al. (2022  ###reference_b26###), which simulates a step-by-step reasoning process to facilitate complex problem-solving tasks under zero demonstration.\nFor different methods, the same language model is used as base models.\nTo ensure a fair comparison, none of the methods used examples from the benchmark as demonstrations for in-context learning.\nRAT leverages the capabilities of Retrieval-Augmented Generation methods, which enhance the performance of language models by integrating external knowledge sources. Specifically, we employed the codeparrot/github-jupyter dataset as our primary search vector library for code generation and mathematical reasoning tasks.\nFor embodied planning tasks in Minecraft, we utilized the Minecraft Wiki111https://minecraft.wiki/  ###reference_minecraft.wiki/### and DigMinecraft222https://www.digminecraft.com/  ###reference_www.digminecraft.com/### websites as the information sources accessible to the LLMs.\nFor open-ended creative writing tasks, we use Google to search the query on the Internet.\nWe utilized OpenAI\u2019s text-embedding-ada-002 API service for all embedding calculations across different methods and base models.\nAcknowledging the risk of benchmark contamination (an issue where the code library may contain solutions to the exact problems being evaluated), we adopted a rigorous pre-processing methodology as described by Guo et al. (2024  ###reference_b19###).\nThe potential implications of benchmark contamination, along with the effectiveness of our pre-processing strategy, are discussed in detail in Appendix D  ###reference_###.\nIn this ablation study, we investigate the influence of various retrieval strategies on the efficacy of RAT, focusing on the optimization of content retrieval for improving generative outputs. The experimental results, detailed in Table 3  ###reference_###, highlight the significant advancements achieved through the iterative refinement of retrieval queries in RAT compared to baseline methods. The baseline denoted as RAG-1, employs a direct approach by using the question itself as the retrieval query. In contrast, CoT+RAG enhances this process by utilizing the entirety of the reasoning thoughts output by the language model as the query, aiming for a broader contextual understanding. However, RAT introduces a more dynamic method by employing continuously modified parts of reasoning thoughts as queries, which allows for a more focused and relevant information retrieval process.\nThe comparative analysis shows that RAT surpasses both the baseline and the CoT+RAG method in terms of pass@1 and pass@5 metrics across the HumanEval and HumanEval+ benchmarks. Specifically, RAT demonstrates an 8.7 percentage point increase in pass@1 and a 7.9 percentage point increase in pass@5 over the baseline in the HumanEval benchmark, and similarly impressive gains in the HumanEval+ benchmark. These improvements underscore the effectiveness of RAT\u2019s retrieval strategy, which by iteratively refining next queries based on evolving reasoning thoughts and previous queries, ensures the retrieval of highly pertinent information. This process not only enhances the relevance of the information retrieved but also significantly improves the quality and accuracy of the final generated outputs. The results firmly establish the superiority of RAT\u2019s dynamic retrieval method in leveraging contextual nuances to drive more precise and effective generative processes.\nIn this ablation study, we systematically examine the impact of causal and non-causal reasoning approaches on the performance of the RAT system, with the Chain of Thought (CoT) serving as our baseline. Our findings, as summarized in Table 4  ###reference_###, reveal significant enhancements in generation capabilities when incorporating causal reasoning techniques. Specifically, the causal approach, which iteratively performs reasoning and retrieval, leads to notable improvements in both pass@1 and pass@5 metrics across HumanEval and HumanEval+ benchmarks. For instance, the causal method outperforms the baseline (CoT) by 11.9 percentage points in pass@1 and by 4.6 percentage points in pass@5 on the HumanEval dataset. This approach contrasts with the non-causal method, which, although also surpassing the baseline, leverages the initial reasoning thought to directly retrieve all necessary steps and generate the final answer. The causal method\u2019s superior performance underscores the value of sequential reasoning and information retrieval in enhancing the accuracy and reliability of generated outputs. This iterative process likely aids in refining the search and reasoning steps based on continuously updated context, allowing for more precise and relevant information retrieval, which in turn supports more accurate final answers. These results firmly establish the efficacy of causal reasoning in long-horizon problem-solving tasks."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Experimental Setups",
            "text": "We adopt four groups of benchmarks.\nCode Generation includess HumanEval (Chen et al., 2021  ###reference_b9###), HumanEval+ (Liu et al., 2023b  ###reference_b35###), MBPP (Austin et al., 2021  ###reference_b2###), and MBPP+ (Liu et al., 2023b  ###reference_b35###). These benchmarks encompass a wide range of programming problems, from simple function implementations to more complex algorithmic challenges, providing a robust testbed for assessing generative capabilities.\nMathematical Reasoning evaluation is conducted on GSM8K and GSM-HARD dataset, which comprises thousands of multi-step mathematical problems (Cobbe et al., 2021  ###reference_b10###; Gao et al., 2022  ###reference_b16###).\nCreative Writing tasks are conducted to evaluate the versatility of RAT, including survey, summarization etc., highlighting different aspects of open-ended text generation.\nEmbodied Planning tasks are evaluated on open-ended environments Minecraft. A set of 100 tasks ranging from simple objectives to challenging diamond objectives are evaluated through MC-TextWorld (Lin et al., 2023  ###reference_b32###).\nFor code generation, the classical pass rate pass@k is selected as the evaluation metrics (Chen et al., 2021  ###reference_b9###  ###reference_b9###; Liu et al., 2023b  ###reference_b35###  ###reference_b35###),  denotes the sampling number. We compute accuracy to evaluate every question in mathematical reasoning tasks, aligning with the established metric for the GSM8K (Cobbe et al., 2021  ###reference_b10###  ###reference_b10###). For embodied planning tasks, we compute the plan execution success rate in MC-TextWorld as executability (Lin et al., 2023  ###reference_b32###  ###reference_b32###). We also conduct human elo rating evaluation to compute the trueskill rating score (Herbrich et al., 2006  ###reference_b20###  ###reference_b20###) for embodied planning (as plausibility) and creative writing tasks. These indicators are better the higher they are.\nTo establish a comprehensive and equitable comparison landscape, we incorporate a suite of baseline methods. Our baselines include the original language models, referred to as DIRECT, and the Retrieval-Augmented Generation (RAG) methodology with  retrieved examples, instantiated in both single-shot (1 shot) and multi-shot (5 shots) configurations, as documented by Lewis et al. (2020b  ###reference_b28###  ###reference_b28###). Additionally, we examine the zero-shot CoT (CoT) approach, as conceptualized by Kojima et al. (2022  ###reference_b26###  ###reference_b26###), which simulates a step-by-step reasoning process to facilitate complex problem-solving tasks under zero demonstration.\nFor different methods, the same language model is used as base models.\nTo ensure a fair comparison, none of the methods used examples from the benchmark as demonstrations for in-context learning.\nRAT leverages the capabilities of Retrieval-Augmented Generation methods, which enhance the performance of language models by integrating external knowledge sources. Specifically, we employed the codeparrot/github-jupyter dataset as our primary search vector library for code generation and mathematical reasoning tasks.\nFor embodied planning tasks in Minecraft, we utilized the Minecraft Wiki111https://minecraft.wiki/  ###reference_minecraft.wiki/###  ###reference_minecraft.wiki/### and DigMinecraft222https://www.digminecraft.com/  ###reference_www.digminecraft.com/###  ###reference_www.digminecraft.com/### websites as the information sources accessible to the LLMs.\nFor open-ended creative writing tasks, we use Google to search the query on the Internet.\nWe utilized OpenAI\u2019s text-embedding-ada-002 API service for all embedding calculations across different methods and base models.\nAcknowledging the risk of benchmark contamination (an issue where the code library may contain solutions to the exact problems being evaluated), we adopted a rigorous pre-processing methodology as described by Guo et al. (2024  ###reference_b19###  ###reference_b19###).\nThe potential implications of benchmark contamination, along with the effectiveness of our pre-processing strategy, are discussed in detail in Appendix D  ###reference_###  ###reference_###."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Results",
            "text": "The code generation results presented in Table 1  ###reference_### and results on other benchmarks presented in Table 2  ###reference_### demonstrate the comprehensive evaluation of the RAT across multiple benchmarks.\nRAT consistently outperforms the other methods across the majority of the benchmarks and metrics, showcasing its superior ability to generate long-horizon context. Notably, in the HumanEval and HumanEval+ benchmarks of code generation, RAT achieves remarkable improvements in pass@1 and pass@5 rates, indicating a significant enhancement in first-attempt accuracy and within the top five attempts. For example, on the HumanEval benchmark, RAT improves pass@1 by up to 20.94% and pass@5 by up to 25.68% relative to the base models\u2019 performances. This trend is observed across different underlying base models, highlighting RAT\u2019s effectiveness regardless of the initial model\u2019s capabilities.\nFor mathematical reasoning tasks, RAT demonstrates a significant relative improvement, with an 8.37% increase in accuracy on GSM8K and a remarkable 31.37% increase on GSMHard, culminating in an overall average improvement of 18.44% when deployed on the GPT-3.5 model.\nRAT significantly outperforms all other methods on open-ended embodied planning tasks in Minecraft, achieving the highest scores with 76.67\u00b18.02% for executability and 29.37 human rating score for plausibility, demonstrating its superior ability to generate feasible and contextually appropriate plans in the complex open-world environment.\nRAT\u2019s superior performance also keeps across a broad spectrum of creative writing tasks. Its ability to generate high-quality content in diverse scenarios was demonstrated, highlighting its potential as a powerful tool for enhancing the general creative writing capabilities of LLMs in open-ended scenarios.\nThe tasks are extremely diverse, while RAT can have consistent improvements over all baselines.\nThese results underline the advantages of RAT\u2019s approach, which leverages iterative refinement of retrieval queries based on evolving reasoning thoughts. This strategy not only enhances the relevance and quality of the information retrieved but also significantly improves the accuracy and efficiency of the generated context."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Case Analysis",
            "text": "Here we take the embodied planning task and creative writing task to do case analysis.\nIn a manner analogous to multi-document question-answering tasks (Trivedi et al., 2022a  ###reference_b49###), the task of long-horizon planning in Minecraft is knowledge-dense, requiring consideration of various items for the completion of each task.\nHowever, open-world Minecraft knowledge on the internet is fragmented, making task completion often dependent on information from multiple sources. We observed that while language models like ChatGPT can identify necessary items through zero-shot CoT reasoning, inaccuracies in procedural steps are common. For example, ChatGPT inaccurately identified the materials for a crafting table as 4 wood blocks (the right answer is 4 planks), indicating lower executability reliability in CoT plans. Classical RAG algorithms, retrieving the knowledge with the question as a query and focusing on the final target item, inadequately retrieve intermediary items, offering minimal task improvement. Contrastingly, RAT improves upon CoT\u2019s initial answers by continuously refining thoughts with targeted retrieval, aligning closely with task progression and relevant item knowledge. This methodology significantly enhances planning effectiveness by ensuring a comprehensive understanding and retrieval of all items involved in a plan, highlighting the synergy between structured reasoning and dynamic knowledge retrieval in addressing long-horizon planning challenges in Minecraft.\nIn addressing open-ended creative writing tasks, assessments of LM\u2019s generations typically focus on completeness and accuracy. When tasked with \u201csummarizing the American Civil War according to a timeline\u201d, LMs under DIRECT and CoT prompts often produce significant hallucinations. For example, the statement \u201cThe Civil War officially began on April 12, 1860, when Confederate troops attacked Fort Sumter in South Carolina, a Union-held fort\u201d contains incorrect information, where the year 1860 is erroneously mentioned instead of the correct year, 1861.\nDirect queries to the internet for this task tend to retrieve limited events, frequently overlooking the accurate start date of the war, April 12, 1861. Moreover, the RAG approach, which tends to summarize content retrieved from searches, often misses this event in its responses, whether it\u2019s RAG-1 or RAG-5. On the other hand, RAT bases its search on a language model\u2019s draft answer, finding that hallucinations usually occur in details, such as specific dates, which do not hinder the search engine from identifying relevant information like \u201cAmerican Civil War starting date\u201d. RAT utilizes the content retrieved to identify and correct errors in the draft answer rather than merely summarizing the retrieved content. Therefore, RAT can achieve a complete generation through reasoning and enhance the accuracy and credibility of the answer by leveraging retrieved knowledge. Experimental results validate the effectiveness of RAT."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Ablation Study",
            "text": "In this ablation study, we investigate the influence of various retrieval strategies on the efficacy of RAT, focusing on the optimization of content retrieval for improving generative outputs. The experimental results, detailed in Table 3  ###reference_###  ###reference_###, highlight the significant advancements achieved through the iterative refinement of retrieval queries in RAT compared to baseline methods. The baseline denoted as RAG-1, employs a direct approach by using the question itself as the retrieval query. In contrast, CoT+RAG enhances this process by utilizing the entirety of the reasoning thoughts output by the language model as the query, aiming for a broader contextual understanding. However, RAT introduces a more dynamic method by employing continuously modified parts of reasoning thoughts as queries, which allows for a more focused and relevant information retrieval process.\nThe comparative analysis shows that RAT surpasses both the baseline and the CoT+RAG method in terms of pass@1 and pass@5 metrics across the HumanEval and HumanEval+ benchmarks. Specifically, RAT demonstrates an 8.7 percentage point increase in pass@1 and a 7.9 percentage point increase in pass@5 over the baseline in the HumanEval benchmark, and similarly impressive gains in the HumanEval+ benchmark. These improvements underscore the effectiveness of RAT\u2019s retrieval strategy, which by iteratively refining next queries based on evolving reasoning thoughts and previous queries, ensures the retrieval of highly pertinent information. This process not only enhances the relevance of the information retrieved but also significantly improves the quality and accuracy of the final generated outputs. The results firmly establish the superiority of RAT\u2019s dynamic retrieval method in leveraging contextual nuances to drive more precise and effective generative processes.\nIn this ablation study, we systematically examine the impact of causal and non-causal reasoning approaches on the performance of the RAT system, with the Chain of Thought (CoT) serving as our baseline. Our findings, as summarized in Table 4  ###reference_###  ###reference_###, reveal significant enhancements in generation capabilities when incorporating causal reasoning techniques. Specifically, the causal approach, which iteratively performs reasoning and retrieval, leads to notable improvements in both pass@1 and pass@5 metrics across HumanEval and HumanEval+ benchmarks. For instance, the causal method outperforms the baseline (CoT) by 11.9 percentage points in pass@1 and by 4.6 percentage points in pass@5 on the HumanEval dataset. This approach contrasts with the non-causal method, which, although also surpassing the baseline, leverages the initial reasoning thought to directly retrieve all necessary steps and generate the final answer. The causal method\u2019s superior performance underscores the value of sequential reasoning and information retrieval in enhancing the accuracy and reliability of generated outputs. This iterative process likely aids in refining the search and reasoning steps based on continuously updated context, allowing for more precise and relevant information retrieval, which in turn supports more accurate final answers. These results firmly establish the efficacy of causal reasoning in long-horizon problem-solving tasks."
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "Robustness of RAT",
            "text": "RAT was rigorously validated across a diverse set of tasks, including code generation, mathematical reasoning, creative writing, and embodied planning. This variety of tasks underscores the generalization capability of RAT, demonstrating its robust performance across highly diverse challenges. Furthermore, all our experimental settings were conducted in a zero-shot manner; we did not design task-specific prompts for RAT, but rather used the simplest possible prompts (which can be found in Appendix B  ###reference_###) to articulate questions or instructions for all methods. This approach ensures RAT\u2019s generalization ability in open-ended scenarios.\nThe diversity of our evaluation was further enhanced by testing RAT across various language models of differing capacities. This included CodeLlama-7b (Rozi\u00e8re et al., 2023  ###reference_b44###), ChatGPT (gpt-3.5-turbo) (Ouyang et al., 2022  ###reference_b40###), and the more advanced GPT-4 (gpt-4) model (OpenAI, 2023  ###reference_b39###). Remarkably, RAT maintained its generalization capability across different scales of language models, showing improvements in benchmarks such as the HumanEval for code generation tasks. Notably, the largest improvement was observed with GPT-4, attributed to its superior ability for in-context learning from retrieved text. On MBPP+, CodeLlama-7b based RAT has demonstrated performance degradation. This decline could be due to the limited in-context learning ability of smaller language models.\nFor mathematical reasoning tasks, RAT demonstrated a significant relative improvement, with an overall average improvement of 18.44% when applied to the GPT-3.5 model. This trend of improvement persisted with GPT-4, which achieved a remarkable 10.26% relative improvement from DIRECT to RAT. These findings highlight RAT\u2019s robustness and its effective enhancement of language models\u2019 performance across a spectrum of computational and creative tasks."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Related Works",
            "text": "Recently, RAG has gained popularity for boosting the performance of LLMs by guiding their generation process using the retrieved knowledge (Zhao et al., 2023  ###reference_b64###). Without updating model parameters that may be expensive (Lewis et al., 2020a  ###reference_b27###) or unstable (Ke et al., 2022b  ###reference_b24###, a  ###reference_b23###), RAG is a cost-effective way for LLMs to interact with the external world (Gu et al., 2018  ###reference_b18###; Lewis et al., 2020a  ###reference_b27###). RAG is widely applied to downstream tasks, such as code generation (Zhou et al., 2022b  ###reference_b67###; Lu et al., 2022  ###reference_b36###; Nashid et al., 2023  ###reference_b37###), question answering (Baek et al., 2023  ###reference_b3###; Siriwardhana et al., 2023  ###reference_b46###), and creative writing (Wen et al., 2023  ###reference_b57###; Asai et al., 2023  ###reference_b1###).\nSome recent works also leverage reasoning to enhance the performance of RAG (Li et al., 2023b  ###reference_b30###). For example, IRCoT (Trivedi et al., 2022b  ###reference_b50###) exploits CoT to generate better queries for retrieval, IRGR (Ribeiro et al., 2022  ###reference_b43###)\nperforms iteratively retrieval to search for suitable\npremises for multi-hop QA, GEEK (Liu et al., 2023a  ###reference_b34###) can choose to query external knowledge or perform\na single logical reasoning step in long-horizon generation tasks, and ITRG (Feng et al., 2023a  ###reference_b14###) performs retrieval based on the last-step generation. However, these previous RAG methods simply adopt a single query to retrieve the knowledge for question-answering tasks (Gao et al., 2023  ###reference_b17###; Feng et al., 2023b  ###reference_b15###), while our proposed RAT performs retrieval using reasoning and draft answers in an autoregressive way, which significantly improves the performance of RAG in various tasks as demonstrated in Figure 2  ###reference_###.\nThe advancement of reasoning in language models has seen notable methodologies emerge since CoT was proposed by Wei et al. (2022  ###reference_b56###), which showcased LMs\u2019 ability to generate self-derived problem-solving strategies. This foundational work spurred further innovations such as the least-to-most prompting (Zhou et al., 2022a  ###reference_b65###), zero-shot CoT (Kojima et al., 2022  ###reference_b26###), self-consistency (Wang et al., 2022  ###reference_b52###), zero-shot CoT without prompting (Wang and Zhou, 2024  ###reference_b51###).\nMoving beyond basic prompting, Creswell et al. (2022  ###reference_b12###) introduced the Selection-Inference framework, while Zelikman et al. (2022  ###reference_b62###) developed STaR to refine reasoning through model finetuning. Creswell and Shanahan (2022  ###reference_b11###) proposed a faithful reasoning model, segmenting reasoning into dedicated steps, similar to Scratchpad\u2019s approach by Nye et al. (2021  ###reference_b38###) for enhancing multi-step computation. Tree-of-Thought (Yao et al., 2023  ###reference_b59###) and Graph-of-Thought (Besta et al., 2023  ###reference_b5###) also expand the reasoning paths into a complex structure instead of linear CoT.\nThese methods usually aim to improve the reasoning ability of LLM by designing prompts or providing feedback from the environment to assist in better planning and decision-making (Wang et al., 2023c  ###reference_b55###; Yao et al., 2022  ###reference_b58###; Shinn et al., 2023  ###reference_b45###; Li et al., 2023a  ###reference_b29###; Zhang et al., 2023  ###reference_b63###).\nHowever, RAT takes a different approach by using RAG to access external knowledge that can help LLM with its reasoning process."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We have presented Retrieval Augmented Thoughts (RAT), a simple yet effective prompting strategy that synergies chain of thought (CoT) prompting and retrieval augmented generation (RAG) to address the challenging long-horizon reasoning and generation tasks. Our key ideas involve revising the zero-shot chain of thoughts produced by LLMs through RAG with the thoughts as queries, and causally revising the thoughts & generating the response progressively. RAT, a zero-shot prompting approach, has demonstrated significant advantages over vanilla CoT prompting, RAG, and other baselines on challenging code generation, mathematics reasoning, embodied task planning, and creative writing tasks."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A Task Details",
            "text": ""
        },
        {
            "section_id": "Appendix 2",
            "parent_section_id": null,
            "section_name": "Appendix B Prompt Details",
            "text": "Our prompts consist of three parts: prompt for generating initial answer, prompt for generating search query, and prompt for revising answers according to retrieved context.\nThe process of query generation is omitted in code generation tasks. Instead, we use the generated code draft as a query and compute the embedding of it based on OpenAI Embedding services. For embodied planning and creative writing tasks, we will generate an additional query."
        },
        {
            "section_id": "Appendix 3",
            "parent_section_id": null,
            "section_name": "Appendix C TrueSkill Evaluation Framework",
            "text": "Part of the tasks in \u201cEmbodied planning\u201d and \u201ccreative writing\u201d involve using humans for labeling. Human labelers have 4 choices: \u201cA is better\u201d, \u201cB is better\u201d, \u201cTie\u201d or \u201cBoth are bad\u201d. In this case, \u201cTie\u201d and \u201cBoth are bad\u201d will be counted as a tie.\nFor each task group, we have selected more than 10 professional annotators to provide labels.\nWe use the Python \u201ctrueskill\u201d package to calculate the win rate and score. The default score for every method is set as 25.\nIn order to facilitate user understanding and selection, we also provide prompts when entering the system.\n###figure_1###"
        },
        {
            "section_id": "Appendix 4",
            "parent_section_id": null,
            "section_name": "Appendix D Disscussions on Benchmark Contamination",
            "text": "To avoid the code library containing solutions to the exact problems being evaluated) in code generation benchmarks, we adopted a rigorous pre-processing methodology as described by Guo et al. (2024  ###reference_b19###).\nThis process was meticulously designed to remove any direct matches or overly similar code snippets from our search vector library, thereby ensuring that our evaluation remains fair and uncontaminated by pre-existing solutions.\nThis examination aims to underscore the importance of maintaining the integrity of the evaluation process while utilizing external knowledge sources to augment the capabilities of language models in code-generation tasks.\nTo further explore the potential benchmark contamination, we also conducted additional finetuning on CodeLLaMA-7B-Python using the code corpus in Table 5  ###reference_###."
        },
        {
            "section_id": "Appendix 5",
            "parent_section_id": null,
            "section_name": "Appendix E More Results",
            "text": ""
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T1\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Code generation results on different benchmarks.All tests are evaluated under zero-shot (0-demonstration) settings. We also report the <span class=\"ltx_text\" id=\"S3.T1.5.1\" style=\"color:#FF0000;\">relative improvements</span> between RAT and DIRECT methods.\n</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S3.T1.3\" style=\"width:429.3pt;height:317.2pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-29.0pt,21.4pt) scale(0.88108685456853,0.88108685456853) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.3.1\">\n<tr class=\"ltx_tr\" id=\"S3.T1.3.1.1\">\n<td class=\"ltx_td ltx_border_t\" id=\"S3.T1.3.1.1.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S3.T1.3.1.1.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S3.T1.3.1.1.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">HumanEval</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S3.T1.3.1.1.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">HumanEval+</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S3.T1.3.1.1.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">MBPP</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S3.T1.3.1.1.7\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">MBPP+</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S3.T1.3.1.1.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Average\u00a0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.1.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.2.1\" rowspan=\"-2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" id=\"S3.T1.3.1.2.1.1\">Base Models</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.2.2\" rowspan=\"-2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" id=\"S3.T1.3.1.2.2.1\">Method</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.2.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">pass@1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.2.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">pass@5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.2.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">pass@1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.2.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">pass@5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.2.7\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">pass@1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.2.8\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">pass@5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.2.9\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">pass@1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.2.10\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">pass@5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.2.11\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">pass@1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.2.12\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">pass@5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.1.3\">\n<td class=\"ltx_td ltx_border_t\" id=\"S3.T1.3.1.3.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.3.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">DIRECT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.3.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">33.78%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.3.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">40.85%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.3.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">30.85%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.3.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">36.59%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.3.7\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">39.27%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.3.8\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">54.27%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.3.9\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">41.22%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.3.10\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">48.17%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.3.11\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">36.28%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.3.12\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">44.97%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.1.4\">\n<td class=\"ltx_td\" id=\"S3.T1.3.1.4.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.4.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">CoT</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.4.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">27.86%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.4.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">29.58%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.4.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">25.12%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.4.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">27.83%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.4.7\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">31.99%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.4.8\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">55.91%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.4.9\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.3.1.4.9.1\">42.19%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.4.10\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">47.51%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.4.11\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">31.79%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.4.12\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">40.21%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.1.5\">\n<td class=\"ltx_td\" id=\"S3.T1.3.1.5.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.5.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">RAG_1 shot</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.5.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">37.50%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.5.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">47.65%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.5.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">33.66%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.5.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">41.83%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.5.7\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">35.41%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.5.8\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">51.63%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.5.9\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">43.66%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.5.10\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">50.09%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.5.11\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">37.56%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.5.12\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">47.80%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.1.6\">\n<td class=\"ltx_td\" id=\"S3.T1.3.1.6.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.6.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">RAG_5 shot</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.6.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">38.90%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.6.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">47.90%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.6.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">35.37%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.6.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">42.75%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.6.7\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">34.06%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.6.8\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">53.90%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.6.9\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">43.35%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.6.10\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.3.1.6.10.1\">51.08%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.6.11\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">37.92%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.6.12\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">48.91%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.1.7\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.7.1\" rowspan=\"-5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" id=\"S3.T1.3.1.7.1.1\">CodeLlama-7b</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.7.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><abbr class=\"ltx_glossaryref\" title=\"Retrieval Augmented Thoughts\"><span class=\"ltx_text ltx_glossary_short\">RAT</span></abbr></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.7.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.3.1.7.3.1\">39.57%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.7.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.3.1.7.4.1\">51.34%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.7.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.3.1.7.5.1\">36.22%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.7.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.3.1.7.6.1\">46.50%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.7.7\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.3.1.7.7.1\">40.86%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.7.8\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.3.1.7.8.1\">60.63%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.7.9\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">39.14%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.7.10\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">48.04%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.7.11\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.3.1.7.11.1\">38.95%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.7.12\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.3.1.7.12.1\">51.63%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.1.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S3.T1.3.1.8.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Relative Improvement</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.8.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" id=\"S3.T1.3.1.8.2.1\" style=\"color:#FF0000;\">17.14%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.8.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" id=\"S3.T1.3.1.8.3.1\" style=\"color:#FF0000;\">25.68%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.8.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" id=\"S3.T1.3.1.8.4.1\" style=\"color:#FF0000;\">17.41%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.8.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" id=\"S3.T1.3.1.8.5.1\" style=\"color:#FF0000;\">27.08%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.8.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" id=\"S3.T1.3.1.8.6.1\" style=\"color:#FF0000;\">4.05%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.8.7\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" id=\"S3.T1.3.1.8.7.1\" style=\"color:#FF0000;\">11.72%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.8.8\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" id=\"S3.T1.3.1.8.8.1\" style=\"color:#FF0000;\">-5.05%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.8.9\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" id=\"S3.T1.3.1.8.9.1\" style=\"color:#FF0000;\">-0.27%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.8.10\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" id=\"S3.T1.3.1.8.10.1\" style=\"color:#FF0000;\">7.35%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.8.11\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" id=\"S3.T1.3.1.8.11.1\" style=\"color:#FF0000;\">14.80%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.1.9\">\n<td class=\"ltx_td ltx_border_t\" id=\"S3.T1.3.1.9.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.9.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">DIRECT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.9.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">50.49%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.9.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">72.56%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.9.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">48.09%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.9.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">70.55%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.9.7\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.3.1.9.7.1\">60.84%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.9.8\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">72.95%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.9.9\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">54.92%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.9.10\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">64.09%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.9.11\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">53.59%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.9.12\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">70.04%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.1.10\">\n<td class=\"ltx_td\" id=\"S3.T1.3.1.10.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.10.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">CoT</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.10.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">47.31%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.10.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">75.88%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.10.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">41.72%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.10.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">74.85%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.10.7\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">55.19%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.10.8\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">65.49%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.10.9\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">47.69%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.10.10\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">62.94%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.10.11\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">47.98%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.10.12\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">69.79%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.1.11\">\n<td class=\"ltx_td\" id=\"S3.T1.3.1.11.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.11.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">RAG_1 shot</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.11.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">50.61%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.11.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">76.22%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.11.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">48.22%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.11.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">70.55%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.11.7\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">55.23%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.11.8\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">70.54%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.11.9\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">53.62%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.11.10\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">68.09%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.11.11\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">51.92%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.11.12\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">71.35%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.1.12\">\n<td class=\"ltx_td\" id=\"S3.T1.3.1.12.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.12.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">RAG_5 shot</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.12.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">45.49%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.12.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">74.39%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.12.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">42.58%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.12.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">70.55%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.12.7\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">54.39%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.12.8\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">69.73%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.12.9\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">55.98%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.12.10\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">70.10%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.12.11\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">49.61%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.12.12\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">71.19%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.1.13\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.13.1\" rowspan=\"-5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" id=\"S3.T1.3.1.13.1.1\">GPT-3.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.13.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><abbr class=\"ltx_glossaryref\" title=\"Retrieval Augmented Thoughts\"><span class=\"ltx_text ltx_glossary_short\">RAT</span></abbr></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.13.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.3.1.13.3.1\">59.27%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.13.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.3.1.13.4.1\">80.49%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.13.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.3.1.13.5.1\">56.31%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.13.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.3.1.13.6.1\">76.07%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.13.7\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" id=\"S3.T1.3.1.13.7.1\" style=\"color:#000000;\">59.31%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.13.8\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" id=\"S3.T1.3.1.13.8.1\" style=\"color:#000000;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.3.1.13.8.1.1\" style=\"color:#000000;\">74.74%</span></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.13.9\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" id=\"S3.T1.3.1.13.9.1\" style=\"color:#000000;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.3.1.13.9.1.1\" style=\"color:#000000;\">59.10%</span></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.13.10\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" id=\"S3.T1.3.1.13.10.1\" style=\"color:#000000;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.3.1.13.10.1.1\" style=\"color:#000000;\">72.61%</span></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.13.11\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.3.1.13.11.1\">58.50%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.13.12\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.3.1.13.12.1\">75.98%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.1.14\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S3.T1.3.1.14.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Relative Improvement</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.14.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" id=\"S3.T1.3.1.14.2.1\" style=\"color:#FF0000;\">17.39%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.14.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" id=\"S3.T1.3.1.14.3.1\" style=\"color:#FF0000;\">10.93%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.14.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" id=\"S3.T1.3.1.14.4.1\" style=\"color:#FF0000;\">17.09%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.14.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" id=\"S3.T1.3.1.14.5.1\" style=\"color:#FF0000;\">7.82%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.14.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" id=\"S3.T1.3.1.14.6.1\" style=\"color:#FF0000;\">-2.51%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.14.7\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" id=\"S3.T1.3.1.14.7.1\" style=\"color:#FF0000;\">2.45%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.14.8\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" id=\"S3.T1.3.1.14.8.1\" style=\"color:#FF0000;\">7.61%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.14.9\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" id=\"S3.T1.3.1.14.9.1\" style=\"color:#FF0000;\">13.29%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.14.10\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" id=\"S3.T1.3.1.14.10.1\" style=\"color:#FF0000;\">9.17%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.14.11\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" id=\"S3.T1.3.1.14.11.1\" style=\"color:#FF0000;\">8.48%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.1.15\">\n<td class=\"ltx_td ltx_border_t\" id=\"S3.T1.3.1.15.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.15.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">DIRECT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.15.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">57.32%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.15.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">78.66%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.15.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">54.36%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.15.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">76.69%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.15.7\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">60.00%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.15.8\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">76.07%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.15.9\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">66.13%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.15.10\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">78.53%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.15.11\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">59.45%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.1.15.12\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">77.49%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.1.16\">\n<td class=\"ltx_td\" id=\"S3.T1.3.1.16.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.16.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">CoT</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.16.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">54.87%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.16.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">72.56%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.16.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">51.90%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.16.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">66.25%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.16.7\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">61.22%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.16.8\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">74.23%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.16.9\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">64.42%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.16.10\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">79.75%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.16.11\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">58.10%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.16.12\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">73.20%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.1.17\">\n<td class=\"ltx_td\" id=\"S3.T1.3.1.17.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.17.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">RAG_1 shot</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.17.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">61.10%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.17.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">79.27%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.17.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">58.04%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.17.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">77.30%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.17.7\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">58.53%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.17.8\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">69.94%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.17.9\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">65.77%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.17.10\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">77.30%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.17.11\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">60.86%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.17.12\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">75.95%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.1.18\">\n<td class=\"ltx_td\" id=\"S3.T1.3.1.18.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.18.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">RAG_5 shot</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.18.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">62.80%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.18.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">82.93%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.18.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">59.51%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.18.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">79.75%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.18.7\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">60.12%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.18.8\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">74.23%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.18.9\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">63.56%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.18.10\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">78.53%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.18.11\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">61.50%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.18.12\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">78.86%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.1.19\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.19.1\" rowspan=\"-5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" id=\"S3.T1.3.1.19.1.1\">GPT-4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.19.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><abbr class=\"ltx_glossaryref\" title=\"Retrieval Augmented Thoughts\"><span class=\"ltx_text ltx_glossary_short\">RAT</span></abbr></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.19.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.3.1.19.3.1\">69.33%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.19.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.3.1.19.4.1\">88.40%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.19.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.3.1.19.5.1\">64.63%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.19.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.3.1.19.6.1\">82.21%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.19.7\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.3.1.19.7.1\">68.90%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.19.8\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.3.1.19.8.1\">79.85%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.19.9\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.3.1.19.9.1\">67.36%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.19.10\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.3.1.19.10.1\">82.14%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.19.11\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.3.1.19.11.1\">67.55%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.1.19.12\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.3.1.19.12.1\">83.15%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.1.20\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" colspan=\"2\" id=\"S3.T1.3.1.20.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Relative Improvement</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S3.T1.3.1.20.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" id=\"S3.T1.3.1.20.2.1\" style=\"color:#FF0000;\">20.94%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S3.T1.3.1.20.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" id=\"S3.T1.3.1.20.3.1\" style=\"color:#FF0000;\">12.38%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S3.T1.3.1.20.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" id=\"S3.T1.3.1.20.4.1\" style=\"color:#FF0000;\">18.89%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S3.T1.3.1.20.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" id=\"S3.T1.3.1.20.5.1\" style=\"color:#FF0000;\">7.20%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S3.T1.3.1.20.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" id=\"S3.T1.3.1.20.6.1\" style=\"color:#FF0000;\">14.83%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S3.T1.3.1.20.7\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" id=\"S3.T1.3.1.20.7.1\" style=\"color:#FF0000;\">4.97%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S3.T1.3.1.20.8\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" id=\"S3.T1.3.1.20.8.1\" style=\"color:#FF0000;\">1.86%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S3.T1.3.1.20.9\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" id=\"S3.T1.3.1.20.9.1\" style=\"color:#FF0000;\">4.60%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S3.T1.3.1.20.10\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" id=\"S3.T1.3.1.20.10.1\" style=\"color:#FF0000;\">13.63%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S3.T1.3.1.20.11\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" id=\"S3.T1.3.1.20.11.1\" style=\"color:#FF0000;\">7.31%</span></td>\n</tr>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 1: Code generation results on different benchmarks.All tests are evaluated under zero-shot (0-demonstration) settings. We also report the relative improvements between RAT and DIRECT methods.\n"
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T2\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.13.1\">Evaluation results on mathematical reasoning, creative writing, and embodied planning tasks.</span> Among them, mathematical reasoning and creative writing use <span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T2.14.2\">gpt-3.5</span> as base models, while embodied planning uses <span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T2.15.3\">gpt-4</span> as base models.  represents the relative improvements than DIRECT.</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S3.T2.9\" style=\"width:429.3pt;height:93.4pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-75.0pt,16.3pt) scale(0.741016390192119,0.741016390192119) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T2.9.7\">\n<tr class=\"ltx_tr\" id=\"S3.T2.6.4.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S3.T2.6.4.4.5\" rowspan=\"2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" id=\"S3.T2.6.4.4.5.1\">Method</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" id=\"S3.T2.4.2.2.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.3.1.1.1.1\">Math Reasoning Accuracy\u00a0</span>\u00a0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" id=\"S3.T2.5.3.3.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.5.3.3.3.1\">Creative Writing</span>\u00a0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" id=\"S3.T2.6.4.4.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.6.4.4.4.1\">Embodied Planning</span>\u00a0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.9.7.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.9.7.7.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">GSM8K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.9.7.7.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">GSMHard</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.7.5.5.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Average\u00a0()</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.9.7.7.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Win Rate</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.8.6.6.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">TrueSkill Rating\u00a0()</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.9.7.7.7\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Uncertainty</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.9.7.7.8\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Executablity</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.9.7.7.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Plausibitlity\u00a0()</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.9.7.7.9\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Uncertainty</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.9.7.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.9.7.8.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">DIRECT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.9.7.8.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">65.85%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.9.7.8.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">51.26%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.9.7.8.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">58.56%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.9.7.8.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">46.67%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.9.7.8.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">24.39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.9.7.8.7\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">1.17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.9.7.8.8\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">19.33\u00b12.08%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.9.7.8.9\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">20.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.9.7.8.10\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">2.05</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.9.7.9\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.9.7.9.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">CoT</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.9.7.9.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">63.82%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.9.7.9.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">44.72%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.9.7.9.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">54.27(-7.32)%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.9.7.9.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">41.67%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.9.7.9.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">24.31(-0.0%)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.9.7.9.7\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">1.09</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.9.7.9.8\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">49.33\u00b13.05%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.9.7.9.9\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">25.75(+25.2%)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.9.7.9.10\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">2.33</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.9.7.10\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.9.7.10.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">RAG-1 shot</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.9.7.10.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">61.81%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.9.7.10.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">51.26%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.9.7.10.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">56.54(+4.17)%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.9.7.10.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">38.71%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.9.7.10.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">23.99(-1.6%)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.9.7.10.7\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">1.11</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.9.7.10.8\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">31.00\u00b15.29%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.9.7.10.9\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">24.97(+21.4%)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.9.7.10.10\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">2.11</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.9.7.11\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.9.7.11.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">RAG-5 shot</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.9.7.11.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">61.81%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.9.7.11.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">56.78%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.9.7.11.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">59.30(+4.88)%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.9.7.11.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">31.67%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.9.7.11.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">23.88(-2.1%)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.9.7.11.7\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">1.22</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.9.7.11.8\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">33.00\u00b13.61%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.9.7.11.9\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">25.02(+21.6%)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.9.7.11.10\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">2.11</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.9.7.12\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T2.9.7.12.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">RAT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.9.7.12.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.9.7.12.2.1\">71.36%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.9.7.12.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.9.7.12.3.1\">67.34%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.9.7.12.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.9.7.12.4.1\">69.35(+16.96)%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.9.7.12.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.9.7.12.5.1\">81.01%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.9.7.12.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.9.7.12.6.1\">29.07(+19.2%)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.9.7.12.7\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">1.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.9.7.12.8\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.9.7.12.8.1\">76.67\u00b18.02%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.9.7.12.9\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.9.7.12.9.1\">29.37(+42.78%)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.9.7.12.10\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">3.37</td>\n</tr>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 2: Evaluation results on mathematical reasoning, creative writing, and embodied planning tasks. Among them, mathematical reasoning and creative writing use gpt-3.5 as base models, while embodied planning uses gpt-4 as base models.  represents the relative improvements than DIRECT."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T3\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Comparative Impact of Retrieval Strategies on <abbr class=\"ltx_glossaryref\" title=\"Retrieval Augmented Thoughts\"><span class=\"ltx_text ltx_glossary_short\">RAT</span></abbr> Performance.</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S3.T3.8\" style=\"width:429.3pt;height:120.9pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(54.9pt,-15.5pt) scale(1.34373372943033,1.34373372943033) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T3.8.8\">\n<tr class=\"ltx_tr\" id=\"S3.T3.8.8.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S3.T3.8.8.9.1\" rowspan=\"2\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text\" id=\"S3.T3.8.8.9.1.1\">Method</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S3.T3.8.8.9.2\" style=\"padding-top:1pt;padding-bottom:1pt;\">HumanEval</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S3.T3.8.8.9.3\" style=\"padding-top:1pt;padding-bottom:1pt;\">HumanEval+</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.8.8.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.2.2.2.2\" style=\"padding-top:1pt;padding-bottom:1pt;\">pass@1()\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.4.4.4.4\" style=\"padding-top:1pt;padding-bottom:1pt;\">pass@5()\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.6.6.6.6\" style=\"padding-top:1pt;padding-bottom:1pt;\">pass@1()\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.8.8.8.8\" style=\"padding-top:1pt;padding-bottom:1pt;\">pass@5()\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.8.8.10\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T3.8.8.10.1\" style=\"padding-top:1pt;padding-bottom:1pt;\">Baseline</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.8.8.10.2\" style=\"padding-top:1pt;padding-bottom:1pt;\">50.6%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.8.8.10.3\" style=\"padding-top:1pt;padding-bottom:1pt;\">76.2%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.8.8.10.4\" style=\"padding-top:1pt;padding-bottom:1pt;\">48.2%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.8.8.10.5\" style=\"padding-top:1pt;padding-bottom:1pt;\">70.5%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.8.8.11\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.8.8.11.1\" style=\"padding-top:1pt;padding-bottom:1pt;\">CoT+RAG</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.8.8.11.2\" style=\"padding-top:1pt;padding-bottom:1pt;\">53.9(<span class=\"ltx_text\" id=\"S3.T3.8.8.11.2.1\" style=\"color:#FF0000;\">+3.3</span>)%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.8.8.11.3\" style=\"padding-top:1pt;padding-bottom:1pt;\">76.8(<span class=\"ltx_text\" id=\"S3.T3.8.8.11.3.1\" style=\"color:#FF0000;\">+0.6</span>)%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.8.8.11.4\" style=\"padding-top:1pt;padding-bottom:1pt;\">51.3(<span class=\"ltx_text\" id=\"S3.T3.8.8.11.4.1\" style=\"color:#FF0000;\">+3.1</span>)%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.8.8.11.5\" style=\"padding-top:1pt;padding-bottom:1pt;\">69.3(<span class=\"ltx_text\" id=\"S3.T3.8.8.11.5.1\" style=\"color:#FF0000;\">-1.2</span>)%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.8.8.12\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T3.8.8.12.1\" style=\"padding-top:1pt;padding-bottom:1pt;\"><abbr class=\"ltx_glossaryref\" title=\"Retrieval Augmented Thoughts\"><span class=\"ltx_text ltx_glossary_short\">RAT</span></abbr></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.8.8.12.2\" style=\"padding-top:1pt;padding-bottom:1pt;\">59.2(<span class=\"ltx_text\" id=\"S3.T3.8.8.12.2.1\" style=\"color:#FF0000;\">+8.7</span>)%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.8.8.12.3\" style=\"padding-top:1pt;padding-bottom:1pt;\">80.4(<span class=\"ltx_text\" id=\"S3.T3.8.8.12.3.1\" style=\"color:#FF0000;\">+7.9</span>)%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.8.8.12.4\" style=\"padding-top:1pt;padding-bottom:1pt;\">56.3(<span class=\"ltx_text\" id=\"S3.T3.8.8.12.4.1\" style=\"color:#FF0000;\">+8.2</span>)%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.8.8.12.5\" style=\"padding-top:1pt;padding-bottom:1pt;\">76.0(<span class=\"ltx_text\" id=\"S3.T3.8.8.12.5.1\" style=\"color:#FF0000;\">+5.5</span>)%</td>\n</tr>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 3: Comparative Impact of Retrieval Strategies on RAT Performance."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T4\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>Ablation Study on Causal vs. Non-Causal Reasoning in <abbr class=\"ltx_glossaryref\" title=\"Retrieval Augmented Thoughts\"><span class=\"ltx_text ltx_glossary_short\">RAT</span></abbr>.</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S3.T4.8\" style=\"width:429.3pt;height:129.7pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(65.6pt,-19.8pt) scale(1.44055589853655,1.44055589853655) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T4.8.8\">\n<tr class=\"ltx_tr\" id=\"S3.T4.8.8.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S3.T4.8.8.9.1\" rowspan=\"2\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text\" id=\"S3.T4.8.8.9.1.1\">Method</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S3.T4.8.8.9.2\" style=\"padding-top:1pt;padding-bottom:1pt;\">HumanEval</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S3.T4.8.8.9.3\" style=\"padding-top:1pt;padding-bottom:1pt;\">HumanEval+</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.8.8.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T4.2.2.2.2\" style=\"padding-top:1pt;padding-bottom:1pt;\">pass@1()\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T4.4.4.4.4\" style=\"padding-top:1pt;padding-bottom:1pt;\">pass@5()\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T4.6.6.6.6\" style=\"padding-top:1pt;padding-bottom:1pt;\">pass@1()\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T4.8.8.8.8\" style=\"padding-top:1pt;padding-bottom:1pt;\">pass@5()\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.8.8.10\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T4.8.8.10.1\" style=\"padding-top:1pt;padding-bottom:1pt;\">Baseline</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T4.8.8.10.2\" style=\"padding-top:1pt;padding-bottom:1pt;\">47.3%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T4.8.8.10.3\" style=\"padding-top:1pt;padding-bottom:1pt;\">75.8%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T4.8.8.10.4\" style=\"padding-top:1pt;padding-bottom:1pt;\">41.7%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T4.8.8.10.5\" style=\"padding-top:1pt;padding-bottom:1pt;\">74.8%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.8.8.11\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T4.8.8.11.1\" style=\"padding-top:1pt;padding-bottom:1pt;\">Non-Causal</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T4.8.8.11.2\" style=\"padding-top:1pt;padding-bottom:1pt;\">57.3(<span class=\"ltx_text\" id=\"S3.T4.8.8.11.2.1\" style=\"color:#FF0000;\">+10.0</span>)%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T4.8.8.11.3\" style=\"padding-top:1pt;padding-bottom:1pt;\">78.0(<span class=\"ltx_text\" id=\"S3.T4.8.8.11.3.1\" style=\"color:#FF0000;\">+2.1</span>)%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T4.8.8.11.4\" style=\"padding-top:1pt;padding-bottom:1pt;\">54.9(<span class=\"ltx_text\" id=\"S3.T4.8.8.11.4.1\" style=\"color:#FF0000;\">+13.2</span>)%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T4.8.8.11.5\" style=\"padding-top:1pt;padding-bottom:1pt;\">74.8(<span class=\"ltx_text\" id=\"S3.T4.8.8.11.5.1\" style=\"color:#FF0000;\">+0.0</span>)%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.8.8.12\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T4.8.8.12.1\" style=\"padding-top:1pt;padding-bottom:1pt;\">Causal</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T4.8.8.12.2\" style=\"padding-top:1pt;padding-bottom:1pt;\">59.2(<span class=\"ltx_text\" id=\"S3.T4.8.8.12.2.1\" style=\"color:#FF0000;\">+11.9</span>)%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T4.8.8.12.3\" style=\"padding-top:1pt;padding-bottom:1pt;\">80.4(<span class=\"ltx_text\" id=\"S3.T4.8.8.12.3.1\" style=\"color:#FF0000;\">+4.6</span>)%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T4.8.8.12.4\" style=\"padding-top:1pt;padding-bottom:1pt;\">56.3(<span class=\"ltx_text\" id=\"S3.T4.8.8.12.4.1\" style=\"color:#FF0000;\">+14.6</span>)%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T4.8.8.12.5\" style=\"padding-top:1pt;padding-bottom:1pt;\">76.0(<span class=\"ltx_text\" id=\"S3.T4.8.8.12.5.1\" style=\"color:#FF0000;\">+1.2</span>)%</td>\n</tr>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 4: Ablation Study on Causal vs. Non-Causal Reasoning in RAT."
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"A3.1\">\n<div class=\"ltx_flex_figure ltx_flex_table\">\n<div class=\"ltx_flex_cell ltx_flex_size_2\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_centering ltx_flex_size_2 ltx_img_landscape\" height=\"662\" id=\"A3.1.g1\" src=\"x4.png\" width=\"822\"/></div>\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<figure class=\"ltx_figure ltx_flex_size_2 ltx_align_center\" id=\"A3.F1\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\">Figure C.1: </span>\nThe human evaluation UI.\nWe will display responses from two different methods for the same instruction on the page simultaneously. The source of the response will be marked as [MASK], and after human labeling, [MASK] will be replaced with the specific method name.\n</figcaption>\n</figure>\n</div>\n</div>\n</figure>",
            "capture": "Figure C.1: \nThe human evaluation UI.\nWe will display responses from two different methods for the same instruction on the page simultaneously. The source of the response will be marked as [MASK], and after human labeling, [MASK] will be replaced with the specific method name.\n"
        },
        "6": {
            "table_html": "<figure class=\"ltx_table\" id=\"A4.T5\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>Benchmark contamination study based on CodeLlama-7b-Python models. We find using the code corpus finetuning the CodeLlama models directly will damage model performance on the code generation benchmark, while using RAT for in-context generation yields better performance.</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"A4.T5.1\" style=\"width:216.8pt;height:105.5pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(15.9pt,-7.7pt) scale(1.17176761843657,1.17176761843657) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"A4.T5.1.1\">\n<tr class=\"ltx_tr\" id=\"A4.T5.1.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"A4.T5.1.1.1.1\" rowspan=\"2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text\" id=\"A4.T5.1.1.1.1.1\">Method</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"A4.T5.1.1.1.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">HumanEval</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"A4.T5.1.1.1.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">HumanEval+</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T5.1.1.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A4.T5.1.1.2.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">pass@1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A4.T5.1.1.2.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">pass@5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A4.T5.1.1.2.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">pass@1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A4.T5.1.1.2.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">pass@5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T5.1.1.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T5.1.1.3.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">DIRECT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A4.T5.1.1.3.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">40.85%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A4.T5.1.1.3.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">53.65%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A4.T5.1.1.3.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">37.43%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A4.T5.1.1.3.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">48.78%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T5.1.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T5.1.1.4.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">FINETUNE</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A4.T5.1.1.4.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">29.02%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A4.T5.1.1.4.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">40.24%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A4.T5.1.1.4.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">26.34%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A4.T5.1.1.4.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">35.98%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T5.1.1.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A4.T5.1.1.5.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">RAT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A4.T5.1.1.5.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">45.73%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A4.T5.1.1.5.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">59.75%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A4.T5.1.1.5.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">43.29%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A4.T5.1.1.5.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">53.66%</td>\n</tr>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 5: Benchmark contamination study based on CodeLlama-7b-Python models. We find using the code corpus finetuning the CodeLlama models directly will damage model performance on the code generation benchmark, while using RAT for in-context generation yields better performance."
        },
        "7": {
            "table_html": "<figure class=\"ltx_table\" id=\"A5.T6\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 6: </span>Page link of retrieved text in embodied Minecraft planning tasks.</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"A5.T6.1\" style=\"width:411.9pt;height:188.8pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-69.0pt,31.6pt) scale(0.749146217274252,0.749146217274252) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"A5.T6.1.1\">\n<tr class=\"ltx_tr\" id=\"A5.T6.1.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A5.T6.1.1.1.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Step</td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"A5.T6.1.1.1.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Item</td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"A5.T6.1.1.1.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Recipe</td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"A5.T6.1.1.1.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Link</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T6.1.1.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A5.T6.1.1.2.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">1</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A5.T6.1.1.2.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">4x Oak Log</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A5.T6.1.1.2.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">-</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A5.T6.1.1.2.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">https://minecraft.fandom.com/wiki/Log</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T6.1.1.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.1.3.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">2</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T6.1.1.3.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">16x Oak Planks</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T6.1.1.3.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">4x Oak Log</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T6.1.1.3.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">https://www.digminecraft.com/basic_recipes/make_oak_wood_plank.php</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T6.1.1.4\">\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.1.4.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">3</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T6.1.1.4.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">4x Stick</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T6.1.1.4.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">2x Oak Planks</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T6.1.1.4.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">https://www.digminecraft.com/basic_recipes/make_stick.php</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T6.1.1.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.1.5.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">4</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T6.1.1.5.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">1x Wooden Pickaxe</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T6.1.1.5.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">3x Oak Planks, 2 Stick</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T6.1.1.5.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">https://www.digminecraft.com/tool_recipes/make_wooden_pickaxe.php</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T6.1.1.6\">\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.1.6.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">5</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T6.1.1.6.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">3x Cobblestone</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T6.1.1.6.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Wooden Pickaxe</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T6.1.1.6.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">https://minecraft.fandom.com/wiki/Cobblestone</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T6.1.1.7\">\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.1.7.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">6</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T6.1.1.7.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">1x Stone Pickaxe</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T6.1.1.7.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">3x Cobblestone, 2 Stick</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T6.1.1.7.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">https://www.digminecraft.com/tool_recipes/make_stone_pickaxe.php</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T6.1.1.8\">\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.1.8.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">7</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T6.1.1.8.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">3x Iron Ore</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T6.1.1.8.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Stone Pickaxe</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T6.1.1.8.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">https://minecraft.fandom.com/wiki/Iron_Ore</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T6.1.1.9\">\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.1.9.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">8</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T6.1.1.9.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">3x Iron Ingot</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T6.1.1.9.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">3x Iron Ore</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T6.1.1.9.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">https://www.digminecraft.com/basic_recipes/make_iron_ingot.php</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T6.1.1.10\">\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.1.10.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">9</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T6.1.1.10.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">1 Iron Pickaxe</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T6.1.1.10.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">3x Iron Ingot, 2x Stick</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T6.1.1.10.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">https://www.digminecraft.com/tool_recipes/make_iron_pickaxe.php</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T6.1.1.11\">\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.1.11.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">10</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T6.1.1.11.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">8x Gold Ore</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T6.1.1.11.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Iron Pickaxe</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T6.1.1.11.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">https://minecraft.fandom.com/wiki/Gold_Ore</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T6.1.1.12\">\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.1.12.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">11</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T6.1.1.12.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">8x Gold Ingot</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T6.1.1.12.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">8x Gold Ore</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T6.1.1.12.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">https://www.digminecraft.com/basic_recipes/make_gold_ingot.php</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T6.1.1.13\">\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.1.13.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">12</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T6.1.1.13.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">1x Apple</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T6.1.1.13.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">-</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T6.1.1.13.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">https://minecraft.fandom.com/wiki/Apple</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T6.1.1.14\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A5.T6.1.1.14.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">13</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A5.T6.1.1.14.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">1x Golden Apple</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A5.T6.1.1.14.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">8x Gold Ingot, 1x Apple</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A5.T6.1.1.14.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">https://www.digminecraft.com/food_recipes/make_golden_apple.php</td>\n</tr>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 6: Page link of retrieved text in embodied Minecraft planning tasks."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.05313v1_figure_1.png",
            "caption": "Figure 1: \nPipeline of RAT. Given a task prompt (denoted as I\ud835\udc3c\\mathit{I}italic_I in the figure), RAT starts from initial step-by-step thoughts (T1,T2,\u22ef,Tnsubscript\ud835\udc471subscript\ud835\udc472\u22efsubscript\ud835\udc47\ud835\udc5bT_{1},T_{2},\\cdots,T_{n}italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_T start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , \u22ef , italic_T start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT) produced by an LLM in zero-shot (\u201clet\u2019s think step by step\u201d). Some thought steps (such as T1subscript\ud835\udc471T_{1}italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT in the figure) may be flawed due to hallucination. RAT iteratively revises each thought step (T1\u22c6,T2\u22c6,\u22ef,Ti\u22121\u22c6,Ti)subscriptsuperscript\ud835\udc47\u22c61subscriptsuperscript\ud835\udc47\u22c62\u22efsubscriptsuperscript\ud835\udc47\u22c6\ud835\udc561subscript\ud835\udc47\ud835\udc56(T^{\\star}_{1},T^{\\star}_{2},\\cdots,T^{\\star}_{i-1},T_{i})( italic_T start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_T start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , \u22ef , italic_T start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT , italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) using RAG from an external knowledge base (denoted as Library). Detailed prompting strategy can be found in Section 2.2."
        },
        "2": {
            "figure_path": "2403.05313v1_figure_2.png",
            "caption": "Figure 1: \nPipeline of RAT. Given a task prompt (denoted as I\ud835\udc3c\\mathit{I}italic_I in the figure), RAT starts from initial step-by-step thoughts (T1,T2,\u22ef,Tnsubscript\ud835\udc471subscript\ud835\udc472\u22efsubscript\ud835\udc47\ud835\udc5bT_{1},T_{2},\\cdots,T_{n}italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_T start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , \u22ef , italic_T start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT) produced by an LLM in zero-shot (\u201clet\u2019s think step by step\u201d). Some thought steps (such as T1subscript\ud835\udc471T_{1}italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT in the figure) may be flawed due to hallucination. RAT iteratively revises each thought step (T1\u22c6,T2\u22c6,\u22ef,Ti\u22121\u22c6,Ti)subscriptsuperscript\ud835\udc47\u22c61subscriptsuperscript\ud835\udc47\u22c62\u22efsubscriptsuperscript\ud835\udc47\u22c6\ud835\udc561subscript\ud835\udc47\ud835\udc56(T^{\\star}_{1},T^{\\star}_{2},\\cdots,T^{\\star}_{i-1},T_{i})( italic_T start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_T start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , \u22ef , italic_T start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT , italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) using RAG from an external knowledge base (denoted as Library). Detailed prompting strategy can be found in Section 2.2."
        },
        "3": {
            "figure_path": "2403.05313v1_figure_3.png",
            "caption": "Figure 2: \nTop: An example of different LLM reasoning methods on creative generation tasks.\nRed text indicates errors or illusions in the text generated by LLM, while green text represents correct generation. Methods without RAG often generate incorrect information with hallucination, classical RAG is highly related to retrieved content with a loose structure, and RAT-generated texts perform best in terms of accuracy and completeness.\nBottom: The quantitative performance comparison for different LLM reasoning methods on complex embodied planning, mathematical reasoning, code generation, and creative generation tasks. Our RAT outperforms all the baselines on all tasks."
        },
        "4": {
            "figure_path": "2403.05313v1_figure_4.png",
            "caption": "Figure C.1: \nThe human evaluation UI.\nWe will display responses from two different methods for the same instruction on the page simultaneously. The source of the response will be marked as [MASK], and after human labeling, [MASK] will be replaced with the specific method name."
        }
    },
    "references": [
        {
            "1": {
                "title": "Self-rag: Learning to retrieve, generate, and critique through self-reflection.",
                "author": "A. Asai, Z. Wu, Y. Wang, A. Sil, and H. Hajishirzi.",
                "venue": "arXiv preprint arXiv:2310.11511, 2023.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Program synthesis with large language models.",
                "author": "J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le, et al.",
                "venue": "arXiv preprint arXiv:2108.07732, 2021.",
                "url": null
            }
        },
        {
            "3": {
                "title": "Knowledge-augmented language model prompting for zero-shot knowledge graph question answering.",
                "author": "J. Baek, A. F. Aji, and A. Saffari.",
                "venue": "arXiv preprint arXiv:2306.04136, 2023.",
                "url": null
            }
        },
        {
            "4": {
                "title": "Video pretraining (vpt): Learning to act by watching unlabeled online videos.",
                "author": "B. Baker, I. Akkaya, P. Zhokhov, J. Huizinga, J. Tang, A. Ecoffet, B. Houghton, R. Sampedro, and J. Clune.",
                "venue": "arXiv preprint arXiv:2206.11795, 2022.",
                "url": null
            }
        },
        {
            "5": {
                "title": "Graph of thoughts: Solving elaborate problems with large language models.",
                "author": "M. Besta, N. Blach, A. Kubicek, R. Gerstenberger, L. Gianinazzi, J. Gajda, T. Lehmann, M. Podstawski, H. Niewiadomski, P. Nyczyk, et al.",
                "venue": "arXiv preprint arXiv:2308.09687, 2023.",
                "url": null
            }
        },
        {
            "6": {
                "title": "Language models are few-shot learners.",
                "author": "T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al.",
                "venue": "Advances in neural information processing systems, 33:1877\u20131901, 2020.",
                "url": null
            }
        },
        {
            "7": {
                "title": "Open-world multi-task control through goal-aware representation learning and adaptive horizon prediction.",
                "author": "S. Cai, Z. Wang, X. Ma, A. Liu, and Y. Liang.",
                "venue": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 13734\u201313744, 2023a.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Groot: Learning to follow instructions by watching gameplay videos.",
                "author": "S. Cai, B. Zhang, Z. Wang, X. Ma, A. Liu, and Y. Liang.",
                "venue": "arXiv preprint arXiv:2310.08235, 2023b.",
                "url": null
            }
        },
        {
            "9": {
                "title": "Evaluating large language models trained on code.",
                "author": "M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al.",
                "venue": "arXiv preprint arXiv:2107.03374, 2021.",
                "url": null
            }
        },
        {
            "10": {
                "title": "Training verifiers to solve math word problems.",
                "author": "K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman.",
                "venue": "arXiv preprint arXiv:2110.14168, 2021.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Faithful reasoning using large language models.",
                "author": "A. Creswell and M. Shanahan.",
                "venue": "arXiv preprint arXiv:2208.14271, 2022.",
                "url": null
            }
        },
        {
            "12": {
                "title": "Selection-inference: Exploiting large language models for interpretable logical reasoning.",
                "author": "A. Creswell, M. Shanahan, and I. Higgins.",
                "venue": "arXiv preprint arXiv:2205.09712, 2022.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Chain-of-verification reduces hallucination in large language models.",
                "author": "S. Dhuliawala, M. Komeili, J. Xu, R. Raileanu, X. Li, A. Celikyilmaz, and J. Weston.",
                "venue": "arXiv preprint arXiv: 2309.11495, 2023.",
                "url": null
            }
        },
        {
            "14": {
                "title": "Retrieval-generation synergy augmented large language models.",
                "author": "Z. Feng, X. Feng, D. Zhao, M. Yang, and B. Qin.",
                "venue": "ArXiv, abs/2310.05149, 2023a.",
                "url": null
            }
        },
        {
            "15": {
                "title": "Retrieval-generation synergy augmented large language models.",
                "author": "Z. Feng, X. Feng, D. Zhao, M. Yang, and B. Qin.",
                "venue": "arXiv preprint arXiv:2310.05149, 2023b.",
                "url": null
            }
        },
        {
            "16": {
                "title": "Pal: Program-aided language models.",
                "author": "L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and G. Neubig.",
                "venue": "arXiv preprint arXiv:2211.10435, 2022.",
                "url": null
            }
        },
        {
            "17": {
                "title": "Retrieval-augmented generation for large language models: A survey.",
                "author": "Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, and H. Wang.",
                "venue": "arXiv preprint arXiv:2312.10997, 2023.",
                "url": null
            }
        },
        {
            "18": {
                "title": "Search engine guided neural machine translation.",
                "author": "J. Gu, Y. Wang, K. Cho, and V. O. Li.",
                "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence, 2018.",
                "url": null
            }
        },
        {
            "19": {
                "title": "Deepseek-coder: When the large language model meets programming \u2013 the rise of code intelligence.",
                "author": "D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu, Y. K. Li, F. Luo, Y. Xiong, and W. Liang.",
                "venue": "arXiv preprint arXiv:2401.14196, 2024.",
                "url": null
            }
        },
        {
            "20": {
                "title": "Trueskill\u2122: a bayesian skill rating system.",
                "author": "R. Herbrich, T. Minka, and T. Graepel.",
                "venue": "Advances in neural information processing systems, 19, 2006.",
                "url": null
            }
        },
        {
            "21": {
                "title": "The Oxford handbook of thinking and reasoning.",
                "author": "K. J. Holyoak and R. G. Morrison.",
                "venue": "Oxford University Press, 2012.",
                "url": null
            }
        },
        {
            "22": {
                "title": "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents.",
                "author": "W. Huang, P. Abbeel, D. Pathak, and I. Mordatch.",
                "venue": "ICML, 2022.",
                "url": null
            }
        },
        {
            "23": {
                "title": "Continual training of language models for few-shot learning.",
                "author": "Z. Ke, H. Lin, Y. Shao, H. Xu, L. Shu, and B. Liu.",
                "venue": "arXiv preprint arXiv:2210.05549, 2022a.",
                "url": null
            }
        },
        {
            "24": {
                "title": "Adapting a language model while preserving its general knowledge.",
                "author": "Z. Ke, Y. Shao, H. Lin, H. Xu, L. Shu, and B. Liu.",
                "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 10177\u201310188, 2022b.",
                "url": null
            }
        },
        {
            "25": {
                "title": "Continual pre-training of language models.",
                "author": "Z. Ke, Y. Shao, H. Lin, T. Konishi, G. Kim, and B. Liu.",
                "venue": "In The Eleventh International Conference on Learning Representations, 2023.",
                "url": null
            }
        },
        {
            "26": {
                "title": "Large language models are zero-shot reasoners.",
                "author": "T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa.",
                "venue": "Advances in neural information processing systems, 35:22199\u201322213, 2022.",
                "url": null
            }
        },
        {
            "27": {
                "title": "Retrieval-augmented generation for knowledge-intensive nlp tasks.",
                "author": "P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. K\u00fcttler, M. Lewis, W.-t. Yih, T. Rockt\u00e4schel, et al.",
                "venue": "Advances in Neural Information Processing Systems, 33:9459\u20139474, 2020a.",
                "url": null
            }
        },
        {
            "28": {
                "title": "Retrieval-augmented generation for knowledge-intensive nlp tasks.",
                "author": "P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. K\u00fcttler, M. Lewis, W.-t. Yih, T. Rockt\u00e4schel, et al.",
                "venue": "Advances in Neural Information Processing Systems, 33:9459\u20139474, 2020b.",
                "url": null
            }
        },
        {
            "29": {
                "title": "Chain of code: Reasoning with a language model-augmented code emulator, 2023a.",
                "author": "C. Li, J. Liang, A. Zeng, X. Chen, K. Hausman, D. Sadigh, S. Levine, L. Fei-Fei, F. Xia, and B. Ichter.",
                "venue": null,
                "url": null
            }
        },
        {
            "30": {
                "title": "Chain-of-knowledge: Grounding large language models via dynamic knowledge adapting over heterogeneous sources.",
                "author": "X. Li, R. Zhao, Y. K. Chia, B. Ding, S. Joty, S. Poria, and L. Bing.",
                "venue": "In The Twelfth International Conference on Learning Representations, 2023b.",
                "url": null
            }
        },
        {
            "31": {
                "title": "Steve-1: A generative model for text-to-behavior in minecraft.",
                "author": "S. Lifshitz, K. Paster, H. Chan, J. Ba, and S. McIlraith.",
                "venue": "arXiv preprint arXiv:2306.00937, 2023.",
                "url": null
            }
        },
        {
            "32": {
                "title": "Mcu: A task-centric framework for open-ended agent evaluation in minecraft.",
                "author": "H. Lin, Z. Wang, J. Ma, and Y. Liang.",
                "venue": "arXiv preprint arXiv:2310.08367, 2023.",
                "url": null
            }
        },
        {
            "33": {
                "title": "Selecting large language model to fine-tune via rectified scaling law.",
                "author": "H. Lin, B. Huang, H. Ye, Q. Chen, Z. Wang, S. Li, J. Ma, X. Wan, J. Zou, and Y. Liang.",
                "venue": "arXiv preprint arXiv:2402.02314, 2024.",
                "url": null
            }
        },
        {
            "34": {
                "title": "Gradually excavating external knowledge for implicit complex question answering.",
                "author": "C. Liu, X. Li, L. Shang, X. Jiang, Q. Liu, E. Y. Lam, and N. Wong.",
                "venue": "In Conference on Empirical Methods in Natural Language Processing, 2023a.",
                "url": null
            }
        },
        {
            "35": {
                "title": "Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation.",
                "author": "J. Liu, C. S. Xia, Y. Wang, and L. Zhang.",
                "venue": "In Thirty-seventh Conference on Neural Information Processing Systems, 2023b.",
                "url": null
            }
        },
        {
            "36": {
                "title": "Reacc: A retrieval-augmented code completion framework.",
                "author": "S. Lu, N. Duan, H. Han, D. Guo, S.-w. Hwang, and A. Svyatkovskiy.",
                "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6227\u20136240, 2022.",
                "url": null
            }
        },
        {
            "37": {
                "title": "Retrieval-based prompt selection for code-related few-shot learning.",
                "author": "N. Nashid, M. Sintaha, and A. Mesbah.",
                "venue": "In Proceedings of the 45th International Conference on Software Engineering (ICSE\u201923), 2023.",
                "url": null
            }
        },
        {
            "38": {
                "title": "Show your work: Scratchpads for intermediate computation with language models.",
                "author": "M. Nye, A. J. Andreassen, G. Gur-Ari, H. Michalewski, J. Austin, D. Bieber, D. Dohan, A. Lewkowycz, M. Bosma, D. Luan, et al.",
                "venue": "arXiv preprint arXiv:2112.00114, 2021.",
                "url": null
            }
        },
        {
            "39": {
                "title": "Gpt-4 technical report, 2023.",
                "author": "OpenAI.",
                "venue": null,
                "url": null
            }
        },
        {
            "40": {
                "title": "Training language models to follow instructions with human feedback.",
                "author": "L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al.",
                "venue": "arXiv preprint arXiv:2203.02155, 2022.",
                "url": null
            }
        },
        {
            "41": {
                "title": "A survey of hallucination in large foundation models.",
                "author": "V. Rawte, A. Sheth, and A. Das.",
                "venue": "arXiv preprint arXiv:2309.05922, 2023.",
                "url": null
            }
        },
        {
            "42": {
                "title": "Sentence-bert: Sentence embeddings using siamese bert-networks.",
                "author": "N. Reimers and I. Gurevych.",
                "venue": "arXiv preprint arXiv:1908.10084, 2019.",
                "url": null
            }
        },
        {
            "43": {
                "title": "Entailment tree explanations via iterative retrieval-generation reasoner.",
                "author": "D. Ribeiro, S. Wang, X. Ma, R. Dong, X. Wei, H. Zhu, X. Chen, Z. Huang, P. Xu, A. Arnold, et al.",
                "venue": "arXiv preprint arXiv:2205.09224, 2022.",
                "url": null
            }
        },
        {
            "44": {
                "title": "Code llama: Open foundation models for code.",
                "author": "B. Rozi\u00e8re, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin, A. Kozhevnikov, I. Evtimov, J. Bitton, M. P. Bhatt, C. C. Ferrer, A. Grattafiori, W. Xiong, A. D\u2019efossez, J. Copet, F. Azhar, H. Touvron, L. Martin, N. Usunier, T. Scialom, and G. Synnaeve.",
                "venue": "ArXiv, abs/2308.12950, 2023.",
                "url": null
            }
        },
        {
            "45": {
                "title": "Reflexion: an autonomous agent with dynamic memory and self-reflection.",
                "author": "N. Shinn, B. Labash, and A. Gopinath.",
                "venue": "arXiv preprint arXiv:2303.11366, 2023.",
                "url": null
            }
        },
        {
            "46": {
                "title": "Improving the domain adaptation of retrieval augmented generation (rag) models for open domain question answering.",
                "author": "S. Siriwardhana, R. Weerasekera, E. Wen, T. Kaluarachchi, R. Rana, and S. Nanayakkara.",
                "venue": "Transactions of the Association for Computational Linguistics, 11:1\u201317, 2023.",
                "url": null
            }
        },
        {
            "47": {
                "title": "Palm: Scaling language modeling with pathways.",
                "author": "G. P. Team.",
                "venue": "arXiv preprint arXiv: 2204.02311, 2022.",
                "url": null
            }
        },
        {
            "48": {
                "title": "Llama 2: Open foundation and fine-tuned chat models.",
                "author": "H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al.",
                "venue": "arXiv preprint arXiv:2307.09288, 2023.",
                "url": null
            }
        },
        {
            "49": {
                "title": "Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions.",
                "author": "H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal.",
                "venue": "ArXiv, abs/2212.10509, 2022a.",
                "url": null
            }
        },
        {
            "50": {
                "title": "Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions.",
                "author": "H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal.",
                "venue": "arXiv preprint arXiv:2212.10509, 2022b.",
                "url": null
            }
        },
        {
            "51": {
                "title": "Chain-of-thought reasoning without prompting.",
                "author": "X. Wang and D. Zhou.",
                "venue": "arXiv preprint arXiv:2402.10200, 2024.",
                "url": null
            }
        },
        {
            "52": {
                "title": "Self-consistency improves chain of thought reasoning in language models.",
                "author": "X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou.",
                "venue": "arXiv preprint arXiv:2203.11171, 2022.",
                "url": null
            }
        },
        {
            "53": {
                "title": "Self-consistency improves chain of thought reasoning in language models.",
                "author": "X. Wang, J. Wei, D. Schuurmans, Q. V. Le, E. H. Chi, S. Narang, A. Chowdhery, and D. Zhou.",
                "venue": "In The Eleventh International Conference on Learning Representations, ICLR 2023, 2023a.",
                "url": null
            }
        },
        {
            "54": {
                "title": "Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models.",
                "author": "Z. Wang, S. Cai, A. Liu, Y. Jin, J. Hou, B. Zhang, H. Lin, Z. He, Z. Zheng, Y. Yang, X. Ma, and Y. Liang.",
                "venue": "ArXiv, abs/2311.05997, 2023b.",
                "url": null
            }
        },
        {
            "55": {
                "title": "Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents.",
                "author": "Z. Wang, S. Cai, A. Liu, X. Ma, and Y. Liang.",
                "venue": "arXiv preprint arXiv:2302.01560, 2023c.",
                "url": null
            }
        },
        {
            "56": {
                "title": "Chain of thought prompting elicits reasoning in large language models.",
                "author": "J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou.",
                "venue": "36th Conference on Neural Information Processing Systems (NeurIPS 2022), 2022.",
                "url": null
            }
        },
        {
            "57": {
                "title": "Grove: a retrieval-augmented complex story generation framework with a forest of evidence.",
                "author": "Z. Wen, Z. Tian, W. Wu, Y. Yang, Y. Shi, Z. Huang, and D. Li.",
                "venue": "arXiv preprint arXiv:2310.05388, 2023.",
                "url": null
            }
        },
        {
            "58": {
                "title": "React: Synergizing reasoning and acting in language models.",
                "author": "S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao.",
                "venue": "arXiv preprint arXiv:2210.03629, 2022.",
                "url": null
            }
        },
        {
            "59": {
                "title": "Tree of thoughts: Deliberate problem solving with large language models, 2023.",
                "author": "S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. Narasimhan.",
                "venue": null,
                "url": null
            }
        },
        {
            "60": {
                "title": "Plan4mc: Skill reinforcement learning and planning for open-world minecraft tasks.",
                "author": "H. Yuan, C. Zhang, H. Wang, F. Xie, P. Cai, H. Dong, and Z. Lu.",
                "venue": "arXiv preprint arXiv:2303.16563, 2023.",
                "url": null
            }
        },
        {
            "61": {
                "title": "Pre-training goal-based models for sample-efficient reinforcement learning.",
                "author": "H. Yuan, Z. Mu, F. Xie, and Z. Lu.",
                "venue": "In The Twelfth International Conference on Learning Representations, 2024.",
                "url": null
            }
        },
        {
            "62": {
                "title": "Star: Bootstrapping reasoning with reasoning.",
                "author": "E. Zelikman, Y. Wu, J. Mu, and N. Goodman.",
                "venue": "Advances in Neural Information Processing Systems, 35:15476\u201315488, 2022.",
                "url": null
            }
        },
        {
            "63": {
                "title": "Proagent: Building proactive cooperative ai with large language models.",
                "author": "C. Zhang, K. Yang, S. Hu, Z. Wang, G. Li, Y. Sun, C. Zhang, Z. Zhang, A. Liu, S.-C. Zhu, et al.",
                "venue": "arXiv preprint arXiv:2308.11339, 2023.",
                "url": null
            }
        },
        {
            "64": {
                "title": "Retrieving multimodal information for augmented generation: A survey.",
                "author": "R. Zhao, H. Chen, W. Wang, F. Jiao, X. L. Do, C. Qin, B. Ding, X. Guo, M. Li, X. Li, and S. R. Joty.",
                "venue": "ArXiv, abs/2303.10868, 2023.",
                "url": null
            }
        },
        {
            "65": {
                "title": "Least-to-most prompting enables complex reasoning in large language models.",
                "author": "D. Zhou, N. Sch\u00e4rli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, C. Cui, O. Bousquet, Q. Le, et al.",
                "venue": "arXiv preprint arXiv:2205.10625, 2022a.",
                "url": null
            }
        },
        {
            "66": {
                "title": "Least-to-most prompting enables complex reasoning in large language models.",
                "author": "D. Zhou, N. Sch\u00e4rli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, C. Cui, O. Bousquet, Q. V. Le, and E. H. Chi.",
                "venue": "In The Eleventh International Conference on Learning Representations, ICLR 2023, 2023.",
                "url": null
            }
        },
        {
            "67": {
                "title": "Docprompting: Generating code by retrieving the docs.",
                "author": "S. Zhou, U. Alon, F. F. Xu, Z. Jiang, and G. Neubig.",
                "venue": "In The Eleventh International Conference on Learning Representations, 2022b.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.05313v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "4"
        ],
        "methodology_sections": [
            "2",
            "2.1",
            "2.2"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.4",
            "3.5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.4"
        ]
    },
    "research_context": {
        "paper_id": "2403.05313v1",
        "paper_title": "RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation",
        "research_background": "**Motivation:**\nThe motivation behind this paper is to address the increasingly significant issue of factual correctness in the reasoning processes of Large Language Models (LLMs) during long-horizon generation tasks. While current techniques like chain-of-thought (CoT) prompting have shown promise in enhancing reasoning tasks, they are prone to hallucinations, especially in zero-shot settings and tasks requiring multiple contextual reasoning steps.\n\n**Research Problem:**\nThe primary research problem tackled in this paper is the hallucination and factual correctness of intermediate reasoning steps in LLM responses, especially in tasks that demand long-horizon and context-aware reasoning. The goal is to explore how integrating Retrieval Augmented Generation (RAG) can mitigate these flaws and improve the quality and factual accuracy of the generated responses.\n\n**Relevant Prior Work:**\n1. **Chain-of-Thought (CoT) Prompting:** This technique has been noted for enabling sophisticated reasoning by inducing LLMs to think step-by-step (Wei et al., 2022; Kojima et al., 2022). However, CoT prompting has been found vulnerable to hallucinations (Dhuliawala et al., 2023).\n   \n2. **Zero-shot CoT Prompting:** Popularized as \u201clet\u2019s think step-by-step\u201d (Kojima et al., 2022), this approach helps LLMs tackle new tasks without task-specific examples but can struggle with factual correctness.\n\n3. **Retrieval Augmented Generation (RAG):** Introduced by Lewis et al. (2020), RAG enhances model reasoning by leveraging external knowledge sources, thereby grounding model outputs in factual information (Holyoak and Morrison, 2012).\n\nThe proposed approach, Retrieval-Augmented Thoughts (RAT), builds on these previous strategies by combining the underlying principles of CoT prompting with the factual grounding facilitated by RAG. The technique enhances intermediate reasoning by progressively revising and generating responses based on retrieved external information at each step.\n\n**Summary of Methodology and Evaluation:**\nRAT involves an initial zero-shot CoT to generate intermediate steps, which are then revised using retrieved information. This progressive approach ensures the accuracy of each step by continuously integrating external knowledge. The method is tested across various challenging long-horizon tasks such as code generation, mathematical reasoning, task planning, and creative writing, demonstrating significant performance improvements over existing methods, including vanilla CoT and RAG.\n\nBy highlighting these key elements, the paper presents a novel strategy that emulates human reasoning processes by continuously revising and grounding intermediate steps with external knowledge, ultimately contributing to more reliable and context-aware outputs from LLMs.",
        "methodology": "**Methodology:**\n\nOur goal is to support long-horizon reasoning and generation while mitigating hallucination when using LLMs. To have satisfying performance on long-horizon tasks, two ingredients are indispensable. Firstly, access to factual information can be facilitated by retrieval. Secondly, appropriate intermediate steps, that outline a scratchpad to finish complex tasks, can be facilitated by CoT. Yet, a naive combination of the two would not necessarily yield improvements. Two questions still persist: (1) what is relevant information to retrieve; (2) how to effectively correct reasoning steps with relevant factual information. To better appreciate our method and why our method can address these two questions, we first provide a brief preliminary introduction of RAG and CoT.\n\n### Proposed Method: Retrieval Augmented Thoughts (RAT)\nThe proposed method, Retrieval Augmented Thoughts (RAT), introduces a novel way to effectively support long-horizon reasoning and generation in large language models (LLMs). The core innovation lies in integrating retrieval mechanisms with context-aware reasoning steps to mitigate hallucination and enhance task performance.\n\n**Key Components:**\n1. **Retrieval of Factual Information:**\n    - **Objective:** To ensure the model has access to accurate and relevant data to use in reasoning and generation tasks.\n    - **Mechanism:** Employ retrieval techniques to source factual information that can be fed into the LLM to ground its responses.\n\n2. **Chain-of-Thought (CoT):**\n    - **Objective:** To outline and execute intermediate reasoning steps that guide the LLM through complex tasks.\n    - **Mechanism:** Utilize a structured approach to break down tasks into manageable parts, creating a \"scratchpad\" that helps in logically organizing information and reasoning processes.\n\n**Innovations:**\n- **Combining Retrieval and CoT:**\n    - The integration of retrieval (RAG) and chain-of-thought (CoT) approaches in a non-naive manner addresses the inherent challenges posed by each method on its own.\n    - By systematically incorporating factual information at each step of the reasoning process (retrieval augmented thoughts), the method aims to:\n        1. Identify what information is relevant to retrieve.\n        2. Correct and enhance reasoning steps with the relevant factual information.\n\n### Addressing Key Questions:\n1. **Relevance of Information to Retrieve:**\n    - The method employs intelligent retrieval mechanisms that dynamically determine and fetch the most pertinent information needed for accurate reasoning and generation.\n\n2. **Effectiveness in Correcting Reasoning Steps:**\n    - By using the retrieved factual information to inform each intermediate step in the CoT process, the method ensures that the reasoning remains grounded and consistent with real-world facts, thus significantly reducing the risk of hallucination.\n\nIn summary, Retrieval Augmented Thoughts (RAT) effectively combines the strengths of retrieval mechanisms and chain-of-thought processes to enhance long-horizon reasoning and generation, while simultaneously addressing the challenges of identifying relevant information and correcting reasoning steps with factual data.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n**Experiment Setup:**\n\n1. **Datasets:**\n   - **Code Generation:** The codeparrot/github-jupyter dataset was used for code generation and mathematical reasoning tasks.\n   - **Embodied Planning:** Minecraft Wiki and DigMinecraft were used as information sources for embodied planning tasks in Minecraft.\n   - **Creative Writing:** Google searches were utilized for open-ended creative writing tasks.\n\n2. **Baselines:**\n   - **DIRECT:** The original language model.\n   - **RAG (Retrieval-Augmented Generation):** Used in single-shot (1 shot) and multi-shot (5 shots) configurations.\n   - **CoT (Chain of Thought):** Zero-shot approach that simulates step-by-step reasoning.\n\n3. **Evaluation Metrics:**\n   - **Code Generation:** Pass rate pass@k.\n   - **Mathematical Reasoning:** Accuracy for each question.\n   - **Embodied Planning:** Plan execution success rate (executability) in MC-TextWorld.\n   - **Creative Writing:** Human elo rating evaluation and trueskill rating score for plausibility.\n   \n4. **Fairness Considerations:** \n   - The same language model was used across different methods.\n   - No examples from the benchmark were used for in-context learning.\n   - Rigorous pre-processing strategy was applied to avoid benchmark contamination.\n\n**Main Experimental Results:**\n\nThe main results emphasize the efficacy of RAT in outperforming benchmark baselines by a significant margin, particularly in long-horizon reasoning and generation tasks.\n\n- **Pass@1 and Pass@5 Metrics:**\n  - **HumanEval Benchmark:** \n    - RAT showed an 8.7 percentage point increase in pass@1 and a 7.9 percentage point increase in pass@5 over baseline methods.\n  - **HumanEval+ Benchmark:** \n    - Similar impressive gains were achieved when compared to the baseline.\n\nThis substantial improvement can be attributed to:\n   - **Dynamic Retrieval Method of RAT:** By using iteratively refined parts of reasoning thoughts as queries, RAT ensures the retrieval of highly pertinent information.\n   - **Enhanced Contextual Understanding:** Continuous modification of retrieval queries based on evolving reasoning thoughts and previous queries.\n   - **Superior Performance in Long-Horizon Tasks:** More accurate and relevant information leads to higher-quality generative outputs.\n\nThese results firmly establish the superiority of RAT's dynamic retrieval strategy in leveraging contextual nuances to improve generative processes in complex problem-solving tasks."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "We investigate the influence of various retrieval strategies on the efficacy of RAT, focusing on the optimization of content retrieval for improving generative outputs.",
            "experiment_process": "The experimental setup includes a comparison between baseline methods and the RAT method. The baseline, RAG-1, uses the question itself as the retrieval query. CoT+RAG improves on this by using the entirety of the reasoning thoughts output by the language model as the query. RAT, however, employs continuously modified parts of reasoning thoughts as queries. The comparison is measured using pass@1 and pass@5 metrics on the HumanEval and HumanEval+ benchmarks.",
            "result_discussion": "RAT demonstrates an 8.7 percentage point increase in pass@1 and a 7.9 percentage point increase in pass@5 over the baseline in the HumanEval benchmark, with similarly impressive gains on the HumanEval+ benchmark. These results highlight the effectiveness of RAT's dynamic retrieval method in enhancing the relevance and quality of retrieved information, thereby improving the final generative outputs.",
            "ablation_id": "2403.05313v1.No1"
        },
        {
            "research_objective": "We systematically examine the impact of causal and non-causal reasoning approaches on the performance of the RAT system, with the Chain of Thought (CoT) serving as our baseline.",
            "experiment_process": "The experimental comparison includes the causal reasoning approach, which iteratively performs reasoning and retrieval, and the non-causal approach, which uses the initial reasoning thought to directly retrieve all necessary steps for the final answer. Both methods are evaluated against the baseline (CoT) on the HumanEval and HumanEval+ benchmarks using pass@1 and pass@5 metrics.",
            "result_discussion": "The causal method outperforms the baseline by 11.9 percentage points in pass@1 and by 4.6 percentage points in pass@5 on the HumanEval dataset. These results underscore the value of sequential reasoning and retrieval in enhancing generative outputs' accuracy and reliability, supporting more precise and relevant information retrieval.",
            "ablation_id": "2403.05313v1.No2"
        }
    ]
}