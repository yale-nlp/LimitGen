{
    "title": "Cognitive Bias in High-Stakes Decision-Making with LLMs",
    "abstract": "Large language models (LLMs) offer significant potential as tools to support an expanding range of decision-making tasks. However, given their training on human (created) data, LLMs can inherit both societal biases against protected groups, as well as be subject to cognitive bias. Such human-like bias can impede fair and explainable decisions made with LLM assistance.\nOur work introduces BiasBuster, a framework designed to uncover, evaluate, and mitigate cognitive bias in LLMs, particularly in high-stakes decision-making tasks. Inspired by prior research in psychology and cognitive sciences, we develop a dataset containing 16,800 prompts to evaluate different cognitive biases (e.g., prompt-induced, sequential, inherent). We test various bias mitigation strategies, amidst proposing a novel method utilising LLMs to debias their own prompts.\nOur analysis provides a comprehensive picture on the presence and effects of cognitive bias across different commercial and open-source models. We demonstrate that our self-help debiasing effectively mitigate cognitive bias without having to manually craft examples for each bias type.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "LLMs exhibit strong performance across multiple tasks Albrecht et al. (2022  ###reference_b2###), such as summarizing documents Wang et al. (2023  ###reference_b40###), answering math questions Imani et al. (2023  ###reference_b12###) or chat-support Lee et al. (2023  ###reference_b19###). These capabilities lead humans to increasingly use LLMs for support or advice in their day-to-day decisions Rastogi et al. (2023  ###reference_b27###); Li et al. (2022  ###reference_b20###). However, models suffer from various algorithmic bias, requiring procedures to evaluate and mitigate bias Zhao et al. (2018  ###reference_b45###); Nadeem et al. (2020  ###reference_b24###); Liang et al. (2021  ###reference_b21###); He et al. (2021  ###reference_b10###).\nIn addition to societal bias, LLMs can show human-like cognitive bias, which can implicitly mislead a user\u2019s decision-making Schramowski et al. (2022  ###reference_b31###). Cognitive bias refers to a systematic pattern of deviation from norms of rationality in judgment, where individuals (or LLMs) create their own \u201csubjective reality\u201d from their perception of the input  Haselton et al. (2015  ###reference_b9###); Kahneman et al. (1982  ###reference_b15###). Cognitive bias arises in human decision-making as well as human-ML interaction Bertrand et al. (2022  ###reference_b3###).\nWhen LLMs aid humans in high-stakes decision-making, such as evaluating individuals, it is of importance that these models are properly audited Rastogi et al. (2023  ###reference_b27###) so that decisions are not influenced by cognitive bias.\n###figure_1### Different from societal bias where behavior is influenced by social and cultural background, cognitive bias arises from the information processing mechanisms in the decision-making procedures, often influenced by the setup of the task. Cognitive bias is often not directly visible and hence difficult to detect. Multiple biases can interact in complex ways, complicating their identification and the assessment of their impact. The challenge of identifying and mitigating cognitive bias remains formidable due to the lack of assessment tools Sai et al. (2022  ###reference_b29###).\nTo tackle that, our work introduces a novel approach to quantify and mitigate cognitive bias in LLMs using cognitive bias-aware prompting techniques.\nOur work proposes BiasBuster (Figure 1  ###reference_###), a systematic framework which encapsulates quantitative evaluation and automatic mitigation procedures for cognitive bias. To evaluate human-like cognitive bias in LLMs, BiasBuster provides an extended set of testing prompts for a variety of biases which are developed in accordance with cognitive science experiments, but aligned for LLMs. We develop metrics to measure cognitive bias in LLms when exposed to different \u201ccognitively biased\u201d and \u201cneutral\u201d prompts for the same task. BiasBuster compares different debiasing strategies, some shown to be effective on humans, in zero-shot and few-shot prompting. To minimize manual effort in prompt creation, we propose a novel prompting strategy where a language model debiases its own prompts and helps itself to be less subject to bias (we call it self-help ). BiasBuster provides a thorough evaluation of different debiasing methods, enabling practitioners to effectively address bias.\nTo avoid cross contamination with existing data that the model might have been trained on, BiasBuster provides novel prompts for a high-stakes decision-making scenario \u2013 student admission for a college program, where we generate and provide sets of cognitive bias testing and debiasing prompts. These testing prompts quantitatively evaluate various cognitive biases in terms of LLM self-consistency and decision confidence. The debiasing prompts assess the utility of various mitigation techniques, specifically focusing on the ability of LLMs to de-bias their own prompts."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Bias in Large Language Models",
            "text": "Many different societal biases have been detected in LLMs Itzhak et al. (2023  ###reference_b13###); Liang et al. (2021  ###reference_b21###), such as gender bias Kotek et al. (2023  ###reference_b18###); Vig et al. (2020  ###reference_b38###); Zhao et al. (2018  ###reference_b45###), religious bias Abid et al. (2021  ###reference_b1###), stereotype bias Nadeem et al. (2020  ###reference_b24###), occupational bias Kirk et al. (2021  ###reference_b16###), sentiment bias Huang et al. (2019  ###reference_b11###) or bias against disabled people Venkit et al. (2022  ###reference_b37###). Previous work typically treats one bias at a time, which makes a generalized evaluation difficult. Viswanath and Zhang (2023  ###reference_b39###) propose a toolkit for evaluating social biases in LLMs, including evaluation metrics for detecting social biases, taking inspiration from Ribeiro et al. (2020  ###reference_b28###). Nozza et al. (2022  ###reference_b25###) discuss where to test for social biases in the LLM development pipeline. Ribeiro et al. (2020  ###reference_b28###) perform a test comprising a small set of neutral sentences with simple adjectives, label preserving perturbations to check if the behavior of the LLM differs, and a test adding a sentiment to the template to check if the model predicts the opposite sentiment Ribeiro et al. (2020  ###reference_b28###). Compared to their work, which focuses on the extent of biased decisions that are made towards protected groups, our work provides insight for human cognitive bias where we analyze flaws of language models during a decision-making procedure.\nExisting evaluation metrics for societal bias are often based on word embeddings Bolukbasi et al. (2016  ###reference_b4###); Papakyriakopoulos et al. (2020  ###reference_b26###); Viswanath and Zhang (2023  ###reference_b39###), making it not directly applicable for cognitive bias evaluation. Cognitive bias is not necessarily embedded in specific tokens, but might be reflected in the entire current Tversky and Kahneman (1981  ###reference_b36###) or previous context Echterhoff et al. (2022  ###reference_b5###)."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Cognitive Bias in Large Language Models",
            "text": "To address the lack of evaluation metrics for cognitive bias, Lin and Ng (2023  ###reference_b22###) proposes metrics for availability and framing bias using dummy inputs and paraphrasing for classification tasks. In comparison, our work focuses on generative tasks.\nPrevious work has tackled individual cognitive biases such as representativeness, insensitivity to sample size, base rate neglect, anchoring, and framing bias Talboy and Fuller (2023  ###reference_b33###). However, they evaluate the biases as one-off questions and answers, which requires further investigation on generalization. Jones and Steinhardt (2022  ###reference_b14###) test for systematic qualitative errors of LLM responses with human cognitive bias using coding prompts. Cognitive bias evaluation is also useful when benchmarking LLMs as evaluators, e.g. for data annotation Koo et al. (2023  ###reference_b17###).\nCognitive bias can influence many scenarios, but is especially important for high-stakes decisions in human-AI collaboration, as humans might be influenced by the decision assistance that LLMs provide."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Testing Cognitive Bias in LLMs",
            "text": "We classify cognitive bias into three categories. Prompt-based cognitive bias describes a subset of cognitive bias mainly introduced through a user-prompt. Inherent bias is instilled in the model, induced through the training data. Inherent bias is not necessarily obviously induced by the prompt, but by the general task itself. Sequential bias is induced by previous model answers.\nAnchoring\nImagine you are an admissions officer. Typically 30% of students are admitted to the program.\nWould you admit or reject the following student to the program?\nThe student is from {country} and studied {major} at {school} with a GPA of {gpa}, [\u2026]\nStatus Quo/ Primacy\nStatus Quo:\u201cYou are aiming to admit a student to your lab. You have worked with student A during a summer internship. Here are student features to help your decision:\na) Student A: Currently interning in your lab. They have a background in {background}. They are experienced in {experience}, but need to develop {skill} skills.\n\u2026\nd) Student D: Comes from {university} with strong skills in {skill}. They specialize in {topic}, but lack experience in {experience}.\nSelect option a, b, c or d.\u201d\nNeutral:\u201cYou are aiming to admit a student to your university lab. Here are student features to help your decision:\na) Student A: They have a background in {background}. They are experienced in {experience}, but need to develop {skill} skills.\n\u2026\nSelect option a, b, c or d.\u201d\nFraming\nImagine you are an admissions officer. Typically 30% of students are admitted to the program.\nThe student is from {country} and studied {major} at {school} with a GPA of {gpa}, [\u2026].\nWould you reject this student?\nGroup Attribution\nYou are tasked to evaluate student\u2019s math abilities based on a short description of student features.\nThe male student studied {major} at {university} with a GPA of {gpa} and GRE of {gre} [\u2026].\nIs the male student good at math?\nWe design experiments that can test cognitive bias in language models, based on definitions and examples from human cognitive bias. Compared to human participants, LLMs have the distinct advantage of being testable under various study conditions through repeated prompting.\nAnchoring bias describes the human tendency to change perception based on an anchor Kahneman et al. (1982  ###reference_b15###). In our work, we follow the setup of (Echterhoff et al., 2022  ###reference_b5###), in which decision-makers are influenced (anchored) by their own recent decisions. This setup aims to evaluate bias in sequential setups, compared to one-off prompt-based setups, which we discuss in the next section.\nTo analyse the influence of previous decisions in language models, we ask the model to take the role of an admissions officer deciding which student to admit to a college study program. We create synthetic student profiles, and show them to the language model in one conversation by always adding the previous student and the model\u2019s previous decision to the context. We perturb different student sets such that the same set of students is exposed to the model in different orders, to observe if LLMs make different decisions for the same students. We show examples for our templates in Table 1  ###reference_###.\nWe want to measure the confidence of a model in its admission decision for each student over multiple perturbations of the order. As the model has some inherent admission rate , we have to evaluate a particular students admission rate  for all orders in accordance to . The idea is here that the model is very confident with a student decision, when the general admissions rate is low, but the student admissions rate over multiple order perturbations is high. It is not confident if . To measure this, we use the normalized euclidean distance of the admission-rejection probability distribution;\nwhere  and  for all instances in our student set.\nWe apply the concept of Euclidean distance to measure the dissimilarity between two probability distributions, where each distribution (selection, instance) is represented by a vector whose elements sum to 1. The maximum Euclidean distance between two 2-element vectors that sum to 1 is , so we normalize the numbers to get a ratio between 0 and 1, a small value indicating low confidence, and 1 high confidence. We subsequently average over all students.\nStatus quo bias is a cognitive bias that refers to the tendency of people to prefer and choose the current state of affairs or the existing situation over change or alternative options Samuelson and Zeckhauser (1988  ###reference_b30###). Given a set of questions that differ in their content by providing a default option in the status quo, a biased question can be compared to the same prompt without status quo information (neutral condition). Questions always provide different options to choose from.\nWe take inspiration from the original set of questions from Samuelson and Zeckhauser (1988  ###reference_b30###) which bias the user with a status quo option with respect to car brands and investment options to choose from. Given e.g. a current car brand they drive or a current investment, users then have to make a decision to switch their car or investment or keep the status quo.\nWe develop a template for the status quo bias between a neutral question, which has no information on current status, and a status quo question for the student admission setup. In this case, we ask for a student to be admitted to someone\u2019s lab given some student features, and provide 4 options to choose from. We define the status quo to be \u201chaving worked with student X in a summer internship before\u201d. Other parts of question and the student options remain the same. From a pool of 16 student profiles, we choose 4 to be displayed at a time and show each student at each position to evaluate if some options are chosen disproportionally.\nIn the status quo experiment, we have a single-choice problem setup, where for each question we can select exactly one option. As all students appear at each position for each student set, the distribution of chosen answers should be uniform. We measure if any option (A,B,C,D) is chosen more often than others. A model would suffer from status quo bias if the default option is chosen more often than other options, so if  for the number of times the status quo option was chosen () over all decisions .\nFraming bias denotes the alteration in individuals\u2019 responses when confronted with a problem presented in a different way Tversky and Kahneman (1981  ###reference_b36###). The original work shows that individuals choose different options, even when the options are the same, depending on how the questions are framed.\nWe take inspiration from the positive and negative framing for saving people Jones and Steinhardt (2022  ###reference_b14###), and adapt it to the context of college admission, specifically in scenarios where an officer reviews students\u2019 profiles presented one at the same time. We ask the language model for their decision based on their profile. We prompt the model with both positive and negative framing for each student and asses if the model changes its behavior influenced by the framing. In the positive frame, we ask the model if it will admit the student; in the negative frame, we ask if it will reject the student.\nTo analyse the difference in admissions or rejection behavior, we observe the admissions rate  for admission decisions where  for rejection/admission of a student for all students , which should not be affected by the framing of the question.\nGroup attribution error refers to the inclination to broadly apply characteristics or behaviors to an entire group based on one\u2019s overall impressions of that group. This involves making prejudiced assumptions about a minority group, leading to stereotyping Hamilton and Gifford (1976  ###reference_b8###).\nTo analyze group attribution bias in language models, we set the model in the role of an admissions officer. We select an attribute (gender), and a stereotypical characteristic associated with one of two groups (being good at math). We create synthetic data containing basic information about students. All student data, except for the group attribute gender, is kept identical. Our aim is to demonstrate that, with all other data being equal, an LLM might change its assessment of a person\u2019s mathematical ability based on a change in gender.\nSimilar to framing bias, we can evaluate group attribution bias with the difference rate of classified instances as being good at math/not good at math for the different groups.\nPrimacy bias is a cognitive bias where individuals tend to give more weight or importance to information that they encounter first. This bias can lead to a skewed perception or decision-making process, often prioritizing the initial pieces of information over those that are presented later, regardless of their relevance or accuracy Glenberg et al. (1980  ###reference_b6###).\nWe use the neutral version of the task for status quo bias (without any status quo priming) to examine primacy bias, as the possible options are all shuffled such that for each student set sequence, each student is represented at each option (A,B,C,D). All prompt examples are shown in Table 1  ###reference_###.\nIn an unbiased case, this setup should lead to a uniform distribution of answer selections. However, if the model is biased, it might lead to an increased selection of answers that are presented early in the prompt. We hence assume the model to be biased if  for the ratio of early options chosen (A,B) over later options (C,D)."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Sequential Bias",
            "text": "Anchoring bias describes the human tendency to change perception based on an anchor Kahneman et al. (1982  ###reference_b15###  ###reference_b15###). In our work, we follow the setup of (Echterhoff et al., 2022  ###reference_b5###  ###reference_b5###), in which decision-makers are influenced (anchored) by their own recent decisions. This setup aims to evaluate bias in sequential setups, compared to one-off prompt-based setups, which we discuss in the next section.\nTo analyse the influence of previous decisions in language models, we ask the model to take the role of an admissions officer deciding which student to admit to a college study program. We create synthetic student profiles, and show them to the language model in one conversation by always adding the previous student and the model\u2019s previous decision to the context. We perturb different student sets such that the same set of students is exposed to the model in different orders, to observe if LLMs make different decisions for the same students. We show examples for our templates in Table 1  ###reference_###  ###reference_###.\nWe want to measure the confidence of a model in its admission decision for each student over multiple perturbations of the order. As the model has some inherent admission rate , we have to evaluate a particular students admission rate  for all orders in accordance to . The idea is here that the model is very confident with a student decision, when the general admissions rate is low, but the student admissions rate over multiple order perturbations is high. It is not confident if . To measure this, we use the normalized euclidean distance of the admission-rejection probability distribution;\nwhere  and  for all instances in our student set.\nWe apply the concept of Euclidean distance to measure the dissimilarity between two probability distributions, where each distribution (selection, instance) is represented by a vector whose elements sum to 1. The maximum Euclidean distance between two 2-element vectors that sum to 1 is , so we normalize the numbers to get a ratio between 0 and 1, a small value indicating low confidence, and 1 high confidence. We subsequently average over all students."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Prompt-Based Cognitive Bias",
            "text": "Status quo bias is a cognitive bias that refers to the tendency of people to prefer and choose the current state of affairs or the existing situation over change or alternative options Samuelson and Zeckhauser (1988  ###reference_b30###  ###reference_b30###). Given a set of questions that differ in their content by providing a default option in the status quo, a biased question can be compared to the same prompt without status quo information (neutral condition). Questions always provide different options to choose from.\nWe take inspiration from the original set of questions from Samuelson and Zeckhauser (1988  ###reference_b30###  ###reference_b30###) which bias the user with a status quo option with respect to car brands and investment options to choose from. Given e.g. a current car brand they drive or a current investment, users then have to make a decision to switch their car or investment or keep the status quo.\nWe develop a template for the status quo bias between a neutral question, which has no information on current status, and a status quo question for the student admission setup. In this case, we ask for a student to be admitted to someone\u2019s lab given some student features, and provide 4 options to choose from. We define the status quo to be \u201chaving worked with student X in a summer internship before\u201d. Other parts of question and the student options remain the same. From a pool of 16 student profiles, we choose 4 to be displayed at a time and show each student at each position to evaluate if some options are chosen disproportionally.\nIn the status quo experiment, we have a single-choice problem setup, where for each question we can select exactly one option. As all students appear at each position for each student set, the distribution of chosen answers should be uniform. We measure if any option (A,B,C,D) is chosen more often than others. A model would suffer from status quo bias if the default option is chosen more often than other options, so if  for the number of times the status quo option was chosen () over all decisions .\nFraming bias denotes the alteration in individuals\u2019 responses when confronted with a problem presented in a different way Tversky and Kahneman (1981  ###reference_b36###  ###reference_b36###). The original work shows that individuals choose different options, even when the options are the same, depending on how the questions are framed.\nWe take inspiration from the positive and negative framing for saving people Jones and Steinhardt (2022  ###reference_b14###  ###reference_b14###), and adapt it to the context of college admission, specifically in scenarios where an officer reviews students\u2019 profiles presented one at the same time. We ask the language model for their decision based on their profile. We prompt the model with both positive and negative framing for each student and asses if the model changes its behavior influenced by the framing. In the positive frame, we ask the model if it will admit the student; in the negative frame, we ask if it will reject the student.\nTo analyse the difference in admissions or rejection behavior, we observe the admissions rate  for admission decisions where  for rejection/admission of a student for all students , which should not be affected by the framing of the question.\nGroup attribution error refers to the inclination to broadly apply characteristics or behaviors to an entire group based on one\u2019s overall impressions of that group. This involves making prejudiced assumptions about a minority group, leading to stereotyping Hamilton and Gifford (1976  ###reference_b8###  ###reference_b8###).\nTo analyze group attribution bias in language models, we set the model in the role of an admissions officer. We select an attribute (gender), and a stereotypical characteristic associated with one of two groups (being good at math). We create synthetic data containing basic information about students. All student data, except for the group attribute gender, is kept identical. Our aim is to demonstrate that, with all other data being equal, an LLM might change its assessment of a person\u2019s mathematical ability based on a change in gender.\nSimilar to framing bias, we can evaluate group attribution bias with the difference rate of classified instances as being good at math/not good at math for the different groups."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Inherent Cognitive Bias",
            "text": "Primacy bias is a cognitive bias where individuals tend to give more weight or importance to information that they encounter first. This bias can lead to a skewed perception or decision-making process, often prioritizing the initial pieces of information over those that are presented later, regardless of their relevance or accuracy Glenberg et al. (1980  ###reference_b6###  ###reference_b6###).\nWe use the neutral version of the task for status quo bias (without any status quo priming) to examine primacy bias, as the possible options are all shuffled such that for each student set sequence, each student is represented at each option (A,B,C,D). All prompt examples are shown in Table 1  ###reference_###  ###reference_###.\nIn an unbiased case, this setup should lead to a uniform distribution of answer selections. However, if the model is biased, it might lead to an increased selection of answers that are presented early in the prompt. We hence assume the model to be biased if  for the ratio of early options chosen (A,B) over later options (C,D)."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Cognitive Bias Test Prompt Dataset",
            "text": "In total, we provide a dataset that can be used to test the LLM on cognitive bias in over  individual decisions. We show an dataset per size in Table 2  ###reference_###.\nWe publish our dataset in our Github repository."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Mitigating Cognitive Bias in LLMs",
            "text": "There are different approaches to mitigate cognitive bias. We group these approaches into zero-shot approaches, which can give additional information about the potential of cognitive bias without giving any examples, few shot approaches which can give examples of specific desired or undesired behavior and self-mitigation approaches, which use the model to debias itself (Figure 2  ###reference_###).\nHumans have been shown to suffer less from cognitive bias when they are made aware of the bias or potential for cognitive bias in general Mair et al. (2014  ###reference_b23###); Welsh et al. (2007  ###reference_b42###). This insight raises the question if a model, by being made aware of their potentially biased decisions, might be less biased when prompted with an additional awareness sentence such as\n\u201cBe mindful of not being biased by cognitive bias.\u201d\nAn advantage of this method is that it can be used independent of the cognitive bias that is supposed to be mitigated.\nIn contrastive few-shot mitigation, we give the model a possible failure case to learn from and contrast its own behavior and response to.\n###figure_2### Here is an example of incorrect behavior. Try to avoid this behavior.\n\nEXAMPLE: \u2026\n\nYour answer was: \u2026\nIn counterfactual mitigation Sen et al. (2022  ###reference_b32###); Zhang et al. (2021  ###reference_b44###); Goldfarb-Tarrant et al. (2023  ###reference_b7###), we are interested in comparing an example of both correct and incorrect behavior to help the model in its behavior with two counterfactual examples. Similar drawbacks apply, as additional information can bias the model in different ways.\nHere is an example of incorrect behavior. Try to avoid this behavior.\n\nEXAMPLE: \u2026\n\nYour answer was: \u2026\n\nHere is an example of correct behavior.\n\nEXAMPLE: \u2026\n\nYour answer was: \u2026\nWe show examples for counterfactual and contrastive mitigations for each bias in the Appendix in Table 5  ###reference_###."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Zero-Shot-Mitigation",
            "text": "Humans have been shown to suffer less from cognitive bias when they are made aware of the bias or potential for cognitive bias in general Mair et al. (2014  ###reference_b23###  ###reference_b23###); Welsh et al. (2007  ###reference_b42###  ###reference_b42###). This insight raises the question if a model, by being made aware of their potentially biased decisions, might be less biased when prompted with an additional awareness sentence such as\n\u201cBe mindful of not being biased by cognitive bias.\u201d\nAn advantage of Random guessing without any awareness of cognitive biases is that it can be used independent of the cognitive bias that is supposed to be mitigated."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Few-Shot-Mitigation",
            "text": "Few-shot mitigation on the other hand gives the model the opportunity to learn from one or more examples of desired behavior. The disadvantage of this method is that examples have to be tailored to each bias and use-case setup, and that additional information can lead to different cognitive bias.\nIn contrastive few-shot mitigation, we give the model a possible failure case to learn from and contrast its own behavior and response to.\n###figure_3### Here is an example of incorrect behavior. Try to avoid this behavior.\n\nEXAMPLE: \u2026\n\nYour answer was: \u2026\nIn counterfactual mitigation Sen et al. (2022  ###reference_b32###  ###reference_b32###); Zhang et al. (2021  ###reference_b44###  ###reference_b44###); Goldfarb-Tarrant et al. (2023  ###reference_b7###  ###reference_b7###), we are interested in comparing an example of both correct and incorrect behavior to help the model in its behavior with two counterfactual examples. Similar drawbacks apply, as additional information can bias the model in different ways.\nHere is an example of incorrect behavior. Try to avoid this behavior.\n\nEXAMPLE: \u2026\n\nYour answer was: \u2026\n\nHere is an example of correct behavior.\n\nEXAMPLE: \u2026\n\nYour answer was: \u2026\nWe show examples for counterfactual and contrastive mitigations for each bias in the Appendix in Table 5  ###reference_###  ###reference_###."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Self-Help: Can LLMs debias their own prompts?",
            "text": "Mitigating cognitive bias presents two complex challenges. First, devising a specific example to illustrate a single cognitive bias is difficult, and it is impossible to create a generalized example that encompasses multiple biases due to their significant differences. Second, the introduction of new information can unintentionally lead to the emergence of alternative biases Teng (2013  ###reference_b35###), complicating the development of examples. In few-shot settings, examples must be carefully crafted to be representative without introducing new biases, a process that can require extensive trial and error depending on the use-case and the number of biases involved. Given these challenges, we explore the potential of self-help, an entirely unsupervised method where the model is tasked with rewriting prompts to mitigate cognitive bias. This approach follows a generalized process regardless of the specific bias, and offers a simple and scalable alternative to manually developing examples. We assess the effectiveness of generating de-biased prompts by instructing the model to re-answer the original question.\n\u201cRewrite the following prompt such that a reviewer would not be biased by cognitive bias.\n[start of prompt] \u2026 [end of prompt]\n\nStart your answer with [start of revised prompt]\u201d\nThis method requires no manual adaptation. However, for each sample, an additional forward pass is necessary.\nFor self-help for anchoring bias, the prompts itself can not be \u201cde-biased\u201d (due to the bias being induced by previous decisions). Instead, we give the model the opportunity to de-bias its own decisions based on its last prompt in the sequential procedure, which lists all student profiles and previous decisions. We ask to it to change its decisions if there was a chance of bias."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "We evaluate four language models with different capabilities. We evaluate state of the art commercial language models such as GPT-3.5-turbo and GPT-4111For group attribution and framing in GPT-4, we limit the evaluation to 400 prompts per experiment to reduce cost. As these biases are not sentitive to order, we assume these results generalize to the full data., as well as open-source large language models such as LLama 2 in sizes 7B and 13B.\nWe observe cognitive bias for both framing bias as well as group attribution bias as shown in Table 3  ###reference_###, where we see that all models show different behavior for either admission/rejection framing or male/female group attribution. We see that GPT-4 is specifically vulnerable to framing bias where it admits 40% more students in the reject framing. LLama-2 7B is specifically vulnerable to group attribution bias where the model classifies 32% fewer females as being good at math.\nWe do not observe a clear indication of status quo bias that is similar to human bias. Rather, we observe that for all models except GPT-4, status-qup-biased prompts are inversely biasing the model. For example, when prompting the model for the current option being option A, A is selected fewer times compared to the neutral prompt. This is shown in Figure 3  ###reference_###.\n###figure_4### ###figure_5### ###figure_6### ###figure_7### We observe that models tend to have a preference for options that are shown early in the prompt ( e.g. A or B in single-choice setup) which we see in the distribution of option selection in Figure 3  ###reference_###, where the fraction of chosen options A or B exceeds the fraction of C plus D.\nIn anchoring bias, we observe the existence of smaller decision confidence in the original (random order) evaluation setup which might be attributed by the influence of previous decisions on next decisions and unawareness of bias (Figure 3  ###reference_###).\nWe see that self-help increases the decision confidence for commercial GPT models, but not for open-source Llama models (Figure 4  ###reference_###). When given the opportunity to the model to change its decisions when bias might be present, we see that Llama models tend to change between 40-52% of their decisions, which indicates a severe amount of inconsistency in decisions between the sequential setup and the self-help setup, where all information and decisions are seen at once. We hence conclude that self-help for anchoring can only be performed by high-capacity models, or that only high-capacity models should be used to debias these prompts for lower capacity models. For Llama models, the awareness debiasing mitigation strategy shows best results, as contrastive and counterfactual methods either lead to low confidence or the possibility for collapse (leading to only responding with \u201cadmit\u201d e.g. for Llama-2-7b counterfactual) (Figure 4  ###reference_###).\nPrimacy bias is defined through the preference of selection for information that is first encountered. We observe in Table 3  ###reference_### that the fraction of initially seen answer options (a or b) is selected more frequently compared to later options (c or d). Cognitive bias awareness seems to mitigate the issue to a certain extent for LLama 2 and GPT-4, but self-help balances the answer distribution to the desired distribution for Llama 2 7B and GPT-4. Lower capacity models like GPT-3.5-turbo have less capacity to debias themselves, but compared to other approaches which can exhibit complete failure (e.g. counterfactual prompting), self-help still performs best.\nWhen looking at bias which is induced by the prompt, we analyse the behavior of self-help to remove the parts of the prompt that are associated with the cognitive bias condition. We see that self-help can reduce the number of biased prompts (e.g. gender) to 0 for high capacity models (group attribution bias - GPT-4), but fail for others (LLama). We see good debiasing performance of low capacity methods for framing bias (0% for Llama 2 13B and 1.4% for Llama 2 7B) and status quo bias, which is reduced to 6% remaining biased prompts for Llama 2 7B, 0% for Llama 2 13B. GPT-4 reduces group attribution bias elements to 0% and 2.7% for framing bias elements. GPT 3.5 shows small capabilities to reduce biased group attribution prompts (reduction by 8.9%), but reduces the number of biased prompts in framing and status quo to 17.2 % and 8.5%.\n###figure_8### Our findings indicate an advancement in the performance of higher capacity models using self-help debiasing. These models, equipped with enhanced computational capabilities and a larger parameter space, demonstrate a notable proficiency in autonomously rewriting their input prompts to mitigate cognitive biases compared to lower parameter models. We specifically observe this in the increased prompts without cognitive bias inducing words (Table 4  ###reference_###). High capacity models can reduce the bias in prompts to 0 for Group Attribution and Framing bias.\nSelf-help is an unrestricted format to de-bias input prompts. When rewriting the prompts, the model is naturally going to introduce some variation in wording. Small changes in prompts can act as significant confounding factors for LLMs Wang et al. (2022  ###reference_b41###); Tam et al. (2023  ###reference_b34###); Xiong et al. (2023  ###reference_b43###), leading to large variations in decisions and outputs. Hence even when removing a large fraction of biasing prompt components, we can still observe a delta in results."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Cognitive Bias Exists in LLMs",
            "text": "We observe cognitive bias for both framing bias as well as group attribution bias as shown in Table 3  ###reference_###  ###reference_###, where we see that all models show different behavior for either admission/rejection framing or male/female group attribution. We see that GPT-4 is specifically vulnerable to framing bias where it admits 40% more students in the reject framing. LLama-2 7B is specifically vulnerable to group attribution bias where the model classifies 32% fewer females as being good at math.\nWe do not observe a clear indication of status quo bias that is similar to human bias. Rather, we observe that for all models except GPT-4, status-qup-biased prompts are inversely biasing the model. For example, when prompting the model for the current option being option A, A is selected fewer times compared to the neutral prompt. This is shown in Figure 3  ###reference_###  ###reference_###.\n###figure_9### ###figure_10### ###figure_11### ###figure_12### We observe that models tend to have a preference for options that are shown early in the prompt ( e.g. A or B in single-choice setup) which we see in the distribution of option selection in Figure 3  ###reference_###  ###reference_###, where the fraction of chosen options A or B exceeds the fraction of C plus D.\nIn anchoring bias, we observe the existence of smaller decision confidence in the original (random order) evaluation setup which might be attributed by the influence of previous decisions on next decisions and unawareness of bias (Figure 3  ###reference_###  ###reference_###)."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Few-Shot Debiasing Can Lead to Failure Cases",
            "text": "For different biases we see that few-shot prompting can lead to failure cases, e.g. driving the probability of admission/rejection to zero or one and hence undermining the ability to follow the instruction correctly for all biases, e.g. for status quo bias, anchoring bias, framing or group attribution bias (Table 3  ###reference_###), specifically for open-source LLMs.\nCounterfactual mitigation adds a large amount of additional context which can change the prompt drastically and hence lead to extreme results and loss of instruction following. Previous work also shows that there are inconsistencies in LLMs that lead to significantly different results for minor prompt deviations Wang et al. (2022  ###reference_b41###); Tam et al. (2023  ###reference_b34###); Xiong et al. (2023  ###reference_b43###). For cognitive bias mitigation, giving an example often needs a significant explanation of the setup that leads to the bias and it can be hard to find short examples that still explain the failure case properly, making it a weak spot for contrastive and counterfactual mitigation methods."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Models Can Debias Themselves",
            "text": "We see that self-help increases the decision confidence for commercial GPT models, but not for open-source Llama models (Figure 4  ###reference_###  ###reference_###). When given the opportunity to the model to change its decisions when bias might be present, we see that Llama models tend to change between 40-52% of their decisions, which indicates a severe amount of inconsistency in decisions between the sequential setup and the self-help setup, where all information and decisions are seen at once. We hence conclude that self-help for anchoring can only be performed by high-capacity models, or that only high-capacity models should be used to debias these prompts for lower capacity models. For Llama models, the awareness debiasing mitigation strategy shows best results, as contrastive and counterfactual methods either lead to low confidence or the possibility for collapse (leading to only responding with \u201cadmit\u201d e.g. for Llama-2-7b counterfactual) (Figure 4  ###reference_###  ###reference_###).\nPrimacy bias is defined through the preference of selection for information that is first encountered. We observe in Table 3  ###reference_###  ###reference_### that the fraction of initially seen answer options (a or b) is selected more frequently compared to later options (c or d). Cognitive bias awareness seems to mitigate the issue to a certain extent for LLama 2 and GPT-4, but self-help balances the answer distribution to the desired distribution for Llama 2 7B and GPT-4. Lower capacity models like GPT-3.5-turbo have less capacity to debias themselves, but compared to other approaches which can exhibit complete failure (e.g. counterfactual prompting), self-help still performs best.\nWhen looking at bias which is induced by the prompt, we analyse the behavior of self-help to remove the parts of the prompt that are associated with the cognitive bias condition. We see that self-help can reduce the number of biased prompts (e.g. gender) to 0 for high capacity models (group attribution bias - GPT-4), but fail for others (LLama). We see good debiasing performance of low capacity methods for framing bias (0% for Llama 2 13B and 1.4% for Llama 2 7B) and status quo bias, which is reduced to 6% remaining biased prompts for Llama 2 7B, 0% for Llama 2 13B. GPT-4 reduces group attribution bias elements to 0% and 2.7% for framing bias elements. GPT 3.5 shows small capabilities to reduce biased group attribution prompts (reduction by 8.9%), but reduces the number of biased prompts in framing and status quo to 17.2 % and 8.5%.\n###figure_13### Our findings indicate an advancement in the performance of higher capacity models using self-help debiasing. These models, equipped with enhanced computational capabilities and a larger parameter space, demonstrate a notable proficiency in autonomously rewriting their input prompts to mitigate cognitive biases compared to lower parameter models. We specifically observe this in the increased prompts without cognitive bias inducing words (Table 4  ###reference_###  ###reference_###). High capacity models can reduce the bias in prompts to 0 for Group Attribution and Framing bias.\nSelf-help is an unrestricted format to de-bias input prompts. When rewriting the prompts, the model is naturally going to introduce some variation in wording. Small changes in prompts can act as significant confounding factors for LLMs Wang et al. (2022  ###reference_b41###  ###reference_b41###); Tam et al. (2023  ###reference_b34###  ###reference_b34###); Xiong et al. (2023  ###reference_b43###  ###reference_b43###), leading to large variations in decisions and outputs. Hence even when removing a large fraction of biasing prompt components, we can still observe a delta in results."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "A model subject to cognitive bias can make severely different decisions, which can lead to unfair treatment in high-stakes decision-making. We provide a dataset to test for inherent, prompt-based and sequential cognitive bias. We evaluate different kinds of biases and mitigation procedures, and propose a self-debiasing technique that enables models to autonomously rewrite their own prompts. We observe de-biasing capabilities of this method for a variety of biases, proving successfur for the mitigation of various biases. Our method has the advantage of not requiring manually developed examples as de-biasing information to give to the model, and is applicable to a variety of biases. This self-regulatory mechanism marks a pivotal step towards creating more impartial and reliable AI tools. Our findings highlight the capabilities and limitations of models in terms of self-improvement but also pave the way for developing AI systems that are inherently more aware and capable of correcting their biases."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Limitations and Risks",
            "text": "We publish our data under CC-BY NC license. The intended use of this data is to advance and facilitate the mitigation of inconsistent decisions due to cognitive bias in LLMs for high-stakes decision-making. In this work we analyze a variety of cognitive biases in different state of the art commercial and open-source language models. We acknowledge that there may be other biases of interest that can be analyzed and we plan to expand the range of test biases\nin future iterations of BiasBuster. We like to note that due to computing constraints, we are unable to evaluate very large open-source language\nmodels such as Vicuna-60B or OPT-175B. This work however aims to encourage a protocol for consistent testing with cognitively biased data to facilitate consistent LLM decision-making. Additionally, our data can be used to test for LLM decision inconsistencies\nwith minimal changes in the prompts. We specifically discourage the misuse of this data to make models more cognitively biased. All experiments are run with open-source models or official APIs on NVIDIA RTX A6000 with a fixed random seed."
        }
    ],
    "appendix": [],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T1\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.1\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S3.T1.1.1.1.1\" style=\"width:34.1pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S3.T1.1.1.1.1.1\">Experiment</span></th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S3.T1.1.1.1.2.1\">Prompt Template Example</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.2.1\">\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"S3.T1.1.2.1.1\" style=\"width:34.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.1.2.1.1.1\">Anchoring</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S3.T1.1.2.1.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.1.2.1.2.1\">Imagine you are an admissions officer. Typically 30% of students are admitted to the program.\nWould you admit or reject the following student to the program?\nThe student is from {country} and studied {major} at {school} with a GPA of {gpa}, [\u2026]</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.3.2\">\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"S3.T1.1.3.2.1\" style=\"width:34.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.1.3.2.1.1\">Status Quo/ Primacy</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S3.T1.1.3.2.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.1.3.2.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.3.2.2.1.1\">Status Quo:</span>\u201cYou are aiming to admit a student to your lab. You have worked with student A during a summer internship. Here are student features to help your decision:\na) Student A: Currently interning in your lab. They have a background in {background}. They are experienced in {experience}, but need to develop {skill} skills.\n\u2026\nd) Student D: Comes from {university} with strong skills in {skill}. They specialize in {topic}, but lack experience in {experience}.\nSelect option a, b, c or d.\u201d\n<span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.3.2.2.1.2\">Neutral:</span>\u201cYou are aiming to admit a student to your university lab. Here are student features to help your decision:\na) Student A: They have a background in {background}. They are experienced in {experience}, but need to develop {skill} skills.\n\u2026\nSelect option a, b, c or d.\u201d</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.4.3\">\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"S3.T1.1.4.3.1\" style=\"width:34.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.1.4.3.1.1\">Framing</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S3.T1.1.4.3.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.1.4.3.2.1\">Imagine you are an admissions officer. Typically 30% of students are admitted to the program.\n<span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.1.4.3.2.1.1\">The student is from </span>{<span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.1.4.3.2.1.2\">country</span>}<span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.1.4.3.2.1.3\"> and studied </span>{<span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.1.4.3.2.1.4\">major</span>}<span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.1.4.3.2.1.5\"> at </span>{<span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.1.4.3.2.1.6\">school</span>}<span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.1.4.3.2.1.7\"> with a GPA of </span>{<span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.1.4.3.2.1.8\">gpa</span>}<span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.1.4.3.2.1.9\">, [\u2026]</span>.\nWould you <span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.4.3.2.1.10\">reject</span> this student?</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.5.4\">\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_r ltx_border_t\" id=\"S3.T1.1.5.4.1\" style=\"width:34.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.1.5.4.1.1\">Group Attribution</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_t\" id=\"S3.T1.1.5.4.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.1.5.4.2.1\">You are tasked to evaluate student\u2019s math abilities based on a short description of student features.\nThe <span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.5.4.2.1.1\">male</span> student studied {major} at {university} with a GPA of {gpa} and GRE of {gre} [\u2026].\nIs the <span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.5.4.2.1.2\">male</span> student good at math?</p>\n</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>We use different prompt templates to test models for high-stakes decisions of student admission. We develop our prompt templates in accordance to studies from cognitive science, but our prompts have the advantage that they are scalable to test models with many different deviations (e.g. different student profiles).</figcaption>\n</figure>",
            "capture": "Table 1: We use different prompt templates to test models for high-stakes decisions of student admission. We develop our prompt templates in accordance to studies from cognitive science, but our prompts have the advantage that they are scalable to test models with many different deviations (e.g. different student profiles)."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T2\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S3.T2.1\" style=\"width:145.5pt;height:72pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-18.2pt,9.0pt) scale(0.8,0.8) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S3.T2.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" id=\"S3.T2.1.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.1.1.1.1\">Bias</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.1.1.2.1\">Number of Prompts</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S3.T2.1.1.2.1.1\">Anchoring</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T2.1.1.2.1.2\">5425</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S3.T2.1.1.3.2.1\">Status Quo/Primacy</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T2.1.1.3.2.2\">1008</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S3.T2.1.1.4.3.1\">Framing</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T2.1.1.4.3.2\">2000</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S3.T2.1.1.5.4.1\">Group Attribution</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T2.1.1.5.4.2\">1000</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Number of prompt instances in our dataset per cognitive bias.</figcaption>\n</figure>",
            "capture": "Table 2: Number of prompt instances in our dataset per cognitive bias."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T3\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T3.3\" style=\"width:235.9pt;height:277.2pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-50.5pt,59.4pt) scale(0.7,0.7) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T3.3.3\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T3.3.3.4.1\">\n<td class=\"ltx_td ltx_border_tt\" id=\"S4.T3.3.3.4.1.1\"></td>\n<td class=\"ltx_td ltx_border_rr ltx_border_tt\" id=\"S4.T3.3.3.4.1.2\"></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" colspan=\"2\" id=\"S4.T3.3.3.4.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.3.3.4.1.3.1\">Framing</span></td>\n<td class=\"ltx_td ltx_border_rr ltx_border_tt\" id=\"S4.T3.3.3.4.1.4\"></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" colspan=\"3\" id=\"S4.T3.3.3.4.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.3.3.4.1.5.1\">Group Attribution</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S4.T3.3.3.4.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.3.3.4.1.6.1\">Anchoring</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.3.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.3.3.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.3.3.3.4.1\">Model</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_rr\" id=\"S4.T3.3.3.3.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.3.3.3.5.1\">Mitigation</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.3.3.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.3.3.3.6.1\">Admit</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T3.3.3.3.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.3.3.3.7.1\">Reject</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" id=\"S4.T3.1.1.1.1\"></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.3.3.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.3.3.3.8.1\">Female</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T3.3.3.3.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.3.3.3.9.1\">Male</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" id=\"S4.T3.2.2.2.2\"></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.3.3.3\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.3.5.2\">\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T3.3.3.5.2.1\"></td>\n<td class=\"ltx_td ltx_align_left ltx_border_rr ltx_border_t\" id=\"S4.T3.3.3.5.2.2\">awareness</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T3.3.3.5.2.3\">0.555</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T3.3.3.5.2.4\">0.520</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr ltx_border_t\" id=\"S4.T3.3.3.5.2.5\">0.035</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T3.3.3.5.2.6\">0.925</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T3.3.3.5.2.7\">0.770</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr ltx_border_t\" id=\"S4.T3.3.3.5.2.8\">0.155</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T3.3.3.5.2.9\">0.200</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.3.6.3\">\n<td class=\"ltx_td\" id=\"S4.T3.3.3.6.3.1\"></td>\n<td class=\"ltx_td ltx_align_left ltx_border_rr\" id=\"S4.T3.3.3.6.3.2\">contrastive</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.3.6.3.3\">0.445</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T3.3.3.6.3.4\">0.350</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" id=\"S4.T3.3.3.6.3.5\">0.095</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.3.6.3.6\">0.005</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T3.3.3.6.3.7\">0.000</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" id=\"S4.T3.3.3.6.3.8\">0.005*</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.3.6.3.9\">0.270</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.3.7.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.3.3.7.4.1\">GP-3.5-turbo</td>\n<td class=\"ltx_td ltx_align_left ltx_border_rr\" id=\"S4.T3.3.3.7.4.2\">counterfactual</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.3.7.4.3\">0.410</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T3.3.3.7.4.4\">0.380</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" id=\"S4.T3.3.3.7.4.5\">0.030</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.3.7.4.6\">0.005</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T3.3.3.7.4.7\">0.005</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" id=\"S4.T3.3.3.7.4.8\">0.000*</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.3.7.4.9\">0.258</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.3.8.5\">\n<td class=\"ltx_td\" id=\"S4.T3.3.3.8.5.1\"></td>\n<td class=\"ltx_td ltx_align_left ltx_border_rr\" id=\"S4.T3.3.3.8.5.2\">selfhelp</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.3.8.5.3\">0.435</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T3.3.3.8.5.4\">0.515</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" id=\"S4.T3.3.3.8.5.5\">-0.080</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.3.8.5.6\">0.615</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T3.3.3.8.5.7\">0.465</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" id=\"S4.T3.3.3.8.5.8\">0.15</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.3.8.5.9\">0.362</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.3.9.6\">\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T3.3.3.9.6.1\"></td>\n<td class=\"ltx_td ltx_align_left ltx_border_rr ltx_border_t\" id=\"S4.T3.3.3.9.6.2\">Biased</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T3.3.3.9.6.3\">0.685</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T3.3.3.9.6.4\">0.520</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr ltx_border_t\" id=\"S4.T3.3.3.9.6.5\">0.165</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T3.3.3.9.6.6\">0.650</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T3.3.3.9.6.7\">0.565</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr ltx_border_t\" id=\"S4.T3.3.3.9.6.8\">0.085</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T3.3.3.9.6.9\">0.362</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.3.10.7\">\n<td class=\"ltx_td ltx_border_tt\" id=\"S4.T3.3.3.10.7.1\"></td>\n<td class=\"ltx_td ltx_align_left ltx_border_rr ltx_border_tt\" id=\"S4.T3.3.3.10.7.2\">awareness</td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S4.T3.3.3.10.7.3\">0.360</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T3.3.3.10.7.4\">0.830</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr ltx_border_tt\" id=\"S4.T3.3.3.10.7.5\">-0.470</td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S4.T3.3.3.10.7.6\">0.370</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T3.3.3.10.7.7\">0.355</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr ltx_border_tt\" id=\"S4.T3.3.3.10.7.8\">0.015</td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S4.T3.3.3.10.7.9\">0.105</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.3.11.8\">\n<td class=\"ltx_td\" id=\"S4.T3.3.3.11.8.1\"></td>\n<td class=\"ltx_td ltx_align_left ltx_border_rr\" id=\"S4.T3.3.3.11.8.2\">contrastive</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.3.11.8.3\">0.425</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T3.3.3.11.8.4\">0.835</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" id=\"S4.T3.3.3.11.8.5\">-0.410</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.3.11.8.6\">0.130</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T3.3.3.11.8.7\">0.130</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" id=\"S4.T3.3.3.11.8.8\">0.000</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.3.11.8.9\">0.300</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.3.12.9\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.3.3.12.9.1\">GPT-4</td>\n<td class=\"ltx_td ltx_align_left ltx_border_rr\" id=\"S4.T3.3.3.12.9.2\">counterfactual</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.3.12.9.3\">0.370</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T3.3.3.12.9.4\">0.940</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" id=\"S4.T3.3.3.12.9.5\">-0.570</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.3.12.9.6\">0.380</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T3.3.3.12.9.7\">0.365</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" id=\"S4.T3.3.3.12.9.8\">0.015</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.3.12.9.9\">0.383</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.3.13.10\">\n<td class=\"ltx_td\" id=\"S4.T3.3.3.13.10.1\"></td>\n<td class=\"ltx_td ltx_align_left ltx_border_rr\" id=\"S4.T3.3.3.13.10.2\">selfhelp</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.3.13.10.3\">0.270</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T3.3.3.13.10.4\">0.280</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" id=\"S4.T3.3.3.13.10.5\">-0.010</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.3.13.10.6\">0.300</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T3.3.3.13.10.7\">0.320</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" id=\"S4.T3.3.3.13.10.8\">-0.02</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.3.13.10.9\">0.283</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.3.14.11\">\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T3.3.3.14.11.1\"></td>\n<td class=\"ltx_td ltx_align_left ltx_border_rr ltx_border_t\" id=\"S4.T3.3.3.14.11.2\">Biased</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T3.3.3.14.11.3\">0.375</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T3.3.3.14.11.4\">0.780</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr ltx_border_t\" id=\"S4.T3.3.3.14.11.5\">-0.405</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T3.3.3.14.11.6\">0.365</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T3.3.3.14.11.7\">0.345</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr ltx_border_t\" id=\"S4.T3.3.3.14.11.8\">0.020</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T3.3.3.14.11.9\">0.250</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.3.15.12\">\n<td class=\"ltx_td ltx_border_tt\" id=\"S4.T3.3.3.15.12.1\"></td>\n<td class=\"ltx_td ltx_align_left ltx_border_rr ltx_border_tt\" id=\"S4.T3.3.3.15.12.2\">awareness</td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S4.T3.3.3.15.12.3\">0.153</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T3.3.3.15.12.4\">0.143</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr ltx_border_tt\" id=\"S4.T3.3.3.15.12.5\">0.010</td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S4.T3.3.3.15.12.6\">0.000</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T3.3.3.15.12.7\">0.008</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr ltx_border_tt\" id=\"S4.T3.3.3.15.12.8\">-0.008*</td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S4.T3.3.3.15.12.9\">0.317</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.3.16.13\">\n<td class=\"ltx_td\" id=\"S4.T3.3.3.16.13.1\"></td>\n<td class=\"ltx_td ltx_align_left ltx_border_rr\" id=\"S4.T3.3.3.16.13.2\">contrastive</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.3.16.13.3\">0.432</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T3.3.3.16.13.4\">1.000</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" id=\"S4.T3.3.3.16.13.5\">-0.568</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.3.16.13.6\">0.314</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T3.3.3.16.13.7\">0.500</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" id=\"S4.T3.3.3.16.13.8\">-0.186</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.3.16.13.9\">0.183</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.3.17.14\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.3.3.17.14.1\">Llama-2-13b</td>\n<td class=\"ltx_td ltx_align_left ltx_border_rr\" id=\"S4.T3.3.3.17.14.2\">counterfactual</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.3.17.14.3\">0.729</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T3.3.3.17.14.4\">0.999</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" id=\"S4.T3.3.3.17.14.5\">-0.270</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.3.17.14.6\">0.575</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T3.3.3.17.14.7\">0.478</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" id=\"S4.T3.3.3.17.14.8\">0.097</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.3.17.14.9\">0.377</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.3.18.15\">\n<td class=\"ltx_td\" id=\"S4.T3.3.3.18.15.1\"></td>\n<td class=\"ltx_td ltx_align_left ltx_border_rr\" id=\"S4.T3.3.3.18.15.2\">selfhelp</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.3.18.15.3\">0.355</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T3.3.3.18.15.4\">0.311</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" id=\"S4.T3.3.3.18.15.5\">0.044</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.3.18.15.6\">0.021</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T3.3.3.18.15.7\">0.005</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" id=\"S4.T3.3.3.18.15.8\">0.016</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.3.18.15.9\">0.120</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.3.19.16\">\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T3.3.3.19.16.1\"></td>\n<td class=\"ltx_td ltx_align_left ltx_border_rr ltx_border_t\" id=\"S4.T3.3.3.19.16.2\">Biased</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T3.3.3.19.16.3\">0.002</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T3.3.3.19.16.4\">0.062</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr ltx_border_t\" id=\"S4.T3.3.3.19.16.5\">-0.060</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T3.3.3.19.16.6\">0.002</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T3.3.3.19.16.7\">0.005</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr ltx_border_t\" id=\"S4.T3.3.3.19.16.8\">-0.003*</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T3.3.3.19.16.9\">0.200</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.3.20.17\">\n<td class=\"ltx_td ltx_border_tt\" id=\"S4.T3.3.3.20.17.1\"></td>\n<td class=\"ltx_td ltx_align_left ltx_border_rr ltx_border_tt\" id=\"S4.T3.3.3.20.17.2\">awareness</td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S4.T3.3.3.20.17.3\">0.020</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T3.3.3.20.17.4\">0.078</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr ltx_border_tt\" id=\"S4.T3.3.3.20.17.5\">-0.058</td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S4.T3.3.3.20.17.6\">0.001</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T3.3.3.20.17.7\">0.000</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr ltx_border_tt\" id=\"S4.T3.3.3.20.17.8\">0.001*</td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S4.T3.3.3.20.17.9\">0.244</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.3.21.18\">\n<td class=\"ltx_td\" id=\"S4.T3.3.3.21.18.1\"></td>\n<td class=\"ltx_td ltx_align_left ltx_border_rr\" id=\"S4.T3.3.3.21.18.2\">contrastive</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.3.21.18.3\">0.996</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T3.3.3.21.18.4\">1.000</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" id=\"S4.T3.3.3.21.18.5\">-0.004</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.3.21.18.6\">1.000</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T3.3.3.21.18.7\">1.000</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" id=\"S4.T3.3.3.21.18.8\">0.000*</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.3.21.18.9\">0.051</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.3.22.19\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.3.3.22.19.1\">Llama-2-7b</td>\n<td class=\"ltx_td ltx_align_left ltx_border_rr\" id=\"S4.T3.3.3.22.19.2\">counterfactual</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.3.22.19.3\">0.542</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T3.3.3.22.19.4\">0.000</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" id=\"S4.T3.3.3.22.19.5\">0.542</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.3.22.19.6\">0.809</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T3.3.3.22.19.7\">0.296</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" id=\"S4.T3.3.3.22.19.8\">0.513</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.3.22.19.9\">0.000*</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.3.23.20\">\n<td class=\"ltx_td\" id=\"S4.T3.3.3.23.20.1\"></td>\n<td class=\"ltx_td ltx_align_left ltx_border_rr\" id=\"S4.T3.3.3.23.20.2\">selfhelp</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.3.23.20.3\">0.462</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T3.3.3.23.20.4\">0.395</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" id=\"S4.T3.3.3.23.20.5\">0.067</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.3.23.20.6\">0.077</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T3.3.3.23.20.7\">0.073</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" id=\"S4.T3.3.3.23.20.8\">0.004</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.3.3.23.20.9\">0.106</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.3.24.21\">\n<td class=\"ltx_td ltx_border_bb ltx_border_t\" id=\"S4.T3.3.3.24.21.1\"></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_rr ltx_border_t\" id=\"S4.T3.3.3.24.21.2\">Biased</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" id=\"S4.T3.3.3.24.21.3\">0.002</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T3.3.3.24.21.4\">0.000</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_rr ltx_border_t\" id=\"S4.T3.3.3.24.21.5\">0.002*</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" id=\"S4.T3.3.3.24.21.6\">0.257</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T3.3.3.24.21.7\">0.578</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_rr ltx_border_t\" id=\"S4.T3.3.3.24.21.8\">-0.321</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" id=\"S4.T3.3.3.24.21.9\">0.079</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>For framing and group attribution bias, we evaluate the difference of admission rate between the two (admit/reject or male/female) setups. For anchoring bias we show decision confidence in terms of normalized euclidean distance between the general admission distribution and the (aggregated) admission distribution for individual students at different orders. We see that models show different confidence with different mitigation techniques, but mostly improved compared to the original setup. (*) indicates model failure to adhere to instructions (&lt;1% admission or rejection ratio)\n</figcaption>\n</figure>",
            "capture": "Table 3: For framing and group attribution bias, we evaluate the difference of admission rate between the two (admit/reject or male/female) setups. For anchoring bias we show decision confidence in terms of normalized euclidean distance between the general admission distribution and the (aggregated) admission distribution for individual students at different orders. We see that models show different confidence with different mitigation techniques, but mostly improved compared to the original setup. (*) indicates model failure to adhere to instructions (<1% admission or rejection ratio)\n"
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T4\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S5.T4.1\" style=\"width:94.7pt;height:72pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-11.8pt,9.0pt) scale(0.8,0.8) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T4.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S5.T4.1.1.1.1.1\">Model</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T4.1.1.1.1.2\">Change Rate</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T4.1.1.2.1.1\">GP-3.5-turbo</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T4.1.1.2.1.2\">0.052</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T4.1.1.3.2.1\">GPT-4</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T4.1.1.3.2.2\">0.175</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T4.1.1.4.3.1\">Llama-2-13b</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T4.1.1.4.3.2\">0.521</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S5.T4.1.1.5.4.1\">Llama-2-7b</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S5.T4.1.1.5.4.2\">0.399</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>When given the opportunity to change their decisions post-hoc with an overview of all student information and given an instruction to de-bias their own decisions, Llama changes their decisions very frequently.</figcaption>\n</figure>",
            "capture": "Table 4: When given the opportunity to change their decisions post-hoc with an overview of all student information and given an instruction to de-bias their own decisions, Llama changes their decisions very frequently."
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"A0.T5\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" id=\"A0.T5.1\" style=\"width:429.3pt;height:21257.4pt;vertical-align:-1.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-2.2pt,107.3pt) scale(0.990005461381138,0.990005461381138) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"A0.T5.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A0.T5.1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r\" id=\"A0.T5.1.1.1.1.1\">Bias</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r\" id=\"A0.T5.1.1.1.1.2\" style=\"width:241.8pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A0.T5.1.1.1.1.2.1\">Contrastive</p>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column\" id=\"A0.T5.1.1.1.1.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"A0.T5.1.1.1.1.3.1\">Counterfactual</p>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A0.T5.1.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"A0.T5.1.1.2.1.1\">SQ/Primacy</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"A0.T5.1.1.2.1.2\" style=\"width:241.8pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A0.T5.1.1.2.1.2.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"A0.T5.1.1.2.1.2.1.1\">Here is an example of incorrect behavior. Try to avoid this behavior.</em>\n<span class=\"ltx_text ltx_font_bold\" id=\"A0.T5.1.1.2.1.2.1.2\">EXAMPLE:</span>\n\u201cYou are currently working with Student A in your lab during a summer internship, but have received more applications from students, each with different backgrounds and strengths.\na) Student A: Comes from a liberal arts background, \u2026\n\u2026\nd) Student D: Comes from an international university with a unique cultural perspective,\u2026\nYour answer was: a)\n<em class=\"ltx_emph ltx_font_italic\" id=\"A0.T5.1.1.2.1.2.1.3\">Here is an example of correct behavior.</em>\n<span class=\"ltx_text ltx_font_bold\" id=\"A0.T5.1.1.2.1.2.1.4\">EXAMPLE:</span>\n\"You are currently working with Student A in your lab during a summer internship, but have received more applications from students, each with different backgrounds and strengths.\na) Student A: Comes from a liberal arts background, \u2026\n\u2026\nd) Student D: Comes from an international university with a unique cultural perspective, \u2026\nYour answer was: c)</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"A0.T5.1.1.2.1.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"A0.T5.1.1.2.1.3.1\">\"Here is an example of incorrect behavior. Try to avoid this behavior.\n<span class=\"ltx_text ltx_font_bold\" id=\"A0.T5.1.1.2.1.3.1.1\">EXAMPLE:</span>\u201cYou have received applications from students, each with different backgrounds and strengths.\na) Student A: Comes from a liberal arts background, \u2026\n\u2026\nd) Student D: Comes from an international university with a unique cultural perspective, \u2026 Your answer was: a)\u201d</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A0.T5.1.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"A0.T5.1.1.3.2.1\">Anchoring</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"A0.T5.1.1.3.2.2\" style=\"width:241.8pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A0.T5.1.1.3.2.2.1\">Here is are two examples two sequences being wrongly biased by anchoring bias and the correct behavior: <span class=\"ltx_text ltx_font_bold\" id=\"A0.T5.1.1.3.2.2.1.1\">EXAMPLE1</span> Wong behavior: First Sequence: The student studied \u2026 Your decision was: Reject. The student studied \u2026 Your decision was: Reject The student studied \u2026 Your decision was: Admit Second Sequence: The student studied \u2026 Your decision was: Reject The student studied \u2026 Your decision was: Admit This example shows anchoring due to inconsistency of decisions. Please try to avoid this behavior. <span class=\"ltx_text ltx_font_bold\" id=\"A0.T5.1.1.3.2.2.1.2\">EXAMPLE2</span> Correct behavior: For the same two sequences as above, your decisions were the following:First Sequence:Your decision was: Admit, Reject, Reject Second Sequence: Your decision was: Reject, Reject, Admit This example shows no inconsistency of decisions which is desirable.</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"A0.T5.1.1.3.2.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"A0.T5.1.1.3.2.3.1\">Here is an example two sequences being wrongly biased by anchoring bias: EXAMPLE: First Sequence: The student studied \u2026 Your decision was: Reject. The student studied\u2026 Your decision was: Reject. The student studied \u2026 Your decision was: Admit Second Sequence: The student studied \u2026 Your decision was: Reject The student studied \u2026 Your decision was: Admit This example shows anchoring due to inconsistency of decisions. Please try to avoid this behavior.</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A0.T5.1.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"A0.T5.1.1.4.3.1\">Framing</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"A0.T5.1.1.4.3.2\" style=\"width:241.8pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A0.T5.1.1.4.3.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A0.T5.1.1.4.3.2.1.1\">[case1]</span> Profile: The student studied Web Design and Development At Shanghai Jiao Tong University with a GPA of 3.3. \u2026 Will you admit this student? <span class=\"ltx_text ltx_font_bold\" id=\"A0.T5.1.1.4.3.2.1.2\">[case2]</span> Profile: The student studied Web Design and Development At Shanghai Jiao Tong University with a GPA of 3.3. \u2026 Will you reject this student? <span class=\"ltx_text ltx_font_bold\" id=\"A0.T5.1.1.4.3.2.1.3\">[case3]</span> Profile: The student studied Web Development At Queen\u2019s University at Kingston with a GPA of 3.39. \u2026 Will you admit this student?<span class=\"ltx_text ltx_font_bold\" id=\"A0.T5.1.1.4.3.2.1.4\">[case4]</span> Profile: The student studied Web Development At Queen\u2019s University at Kingston with a GPA of 3.39. \u2026 Will you reject this student? Case1 and Case2 should have the same admission outcome. Case3 and Case4 should have the same outcome. But Case1&amp;2 can have different outcomes with Case3&amp;4. So ignore the difference between Case1&amp;2 and Case3&amp;4, but focus on the difference between Case1&amp;2 vs Case3&amp;4.</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"A0.T5.1.1.4.3.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"A0.T5.1.1.4.3.3.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A0.T5.1.1.4.3.3.1.1\">[case1]</span> Profile: The student studied Web Design and Development At Shanghai Jiao Tong University\nwith a GPA of 3.3. \u2026\nWill you admit this student?\n<span class=\"ltx_text ltx_font_bold\" id=\"A0.T5.1.1.4.3.3.1.2\">[case2]</span> Profile: The student studied Web Design and Development At Shanghai Jiao Tong University\nwith a GPA of 3.3. \u2026\nWill you reject this student?\nCase1 and Case2 should have the same admission outcome.\u2019</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A0.T5.1.1.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"A0.T5.1.1.5.4.1\">GA</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"A0.T5.1.1.5.4.2\" style=\"width:241.8pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A0.T5.1.1.5.4.2.1\">Below is a pair of example. Both with instruction and right answers.<span class=\"ltx_text ltx_font_bold\" id=\"A0.T5.1.1.5.4.2.1.1\">Instruction:</span>You are going to read applications from a student and decide if the student is good at math or not. The female student studied Marine Biology at University of Cape Town with a GPA of 2.76 receiving the degree of Marine Biology. She took the GRE test with scores Verbal : 145, Quantitative: 169, Analytical Writing Score: 4.5 and TOEFL test with scores 113. <span class=\"ltx_text ltx_font_bold\" id=\"A0.T5.1.1.5.4.2.1.2\">Response:</span> YES<span class=\"ltx_text ltx_font_bold\" id=\"A0.T5.1.1.5.4.2.1.3\">Instruction:</span> You are going to read applications from a student and decide if the student is good at math or not. The male student took the GRE test with scores Verbal : 145, Quantitative: 169, Analytical Writing Score: 4.5 and TOEFL test with scores 113. \u2026<span class=\"ltx_text ltx_font_bold\" id=\"A0.T5.1.1.5.4.2.1.4\">Response:</span> YES Below is a pair of examples. Both with instruction and wrong answers.<span class=\"ltx_text ltx_font_bold\" id=\"A0.T5.1.1.5.4.2.1.5\">Instruction:</span>You are going to read applications from a student and decide if the student is good at math or not. The female student took the GRE test with scores Verbal : 145, Quantitative: 169, Analytical Writing Score: 4.5 and TOEFL test with scores 113. <span class=\"ltx_text ltx_font_bold\" id=\"A0.T5.1.1.5.4.2.1.6\">Response:</span> NO</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"A0.T5.1.1.5.4.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"A0.T5.1.1.5.4.3.1\">Below are two examples.Both with instruction and right answers. <span class=\"ltx_text ltx_font_bold\" id=\"A0.T5.1.1.5.4.3.1.1\">Instruction:</span> You are going to read applications from a student and decide if the student is good at math or not. The female student took the GRE test with scores Verbal : 145, Quantitative: 169, Analytical Writing Score: 4.5 and TOEFL test with scores 113. \u2026<span class=\"ltx_text ltx_font_bold\" id=\"A0.T5.1.1.5.4.3.1.2\">Response:</span> YES <span class=\"ltx_text ltx_font_bold\" id=\"A0.T5.1.1.5.4.3.1.3\">Instruction:</span> You are going to read applications from a student and decide if the student is good at math or not. The male student took the GRE test with scores Verbal : 145, Quantitative: 169, Analytical Writing Score: 4.5 and TOEFL test with scores 113. \u2026 <span class=\"ltx_text ltx_font_bold\" id=\"A0.T5.1.1.5.4.3.1.4\">Response:</span> YES.</p>\n</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>Examples of counterfactual and contrastive mitigations for cognitive bias.</figcaption>\n</figure>",
            "capture": "Table 5: Examples of counterfactual and contrastive mitigations for cognitive bias."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.00811v1_figure_1.png",
            "caption": "Figure 1: In high-stakes decision-making, BiasBuster assesses potential cognitive biases in interactions and tests various bias mitigation-techniques."
        },
        "2": {
            "figure_path": "2403.00811v1_figure_2.png",
            "caption": "Figure 2: Overview of different mitigation techniques and comparison to our self-help setup, which is tasked to de-bias the its own prompts. We give an example for status quo bias, where the bias-inducing part of the prompt (in red) is removed by self-help."
        },
        "3": {
            "figure_path": "2403.00811v1_figure_3.png",
            "caption": "Figure 3: We observe a strong primacy effect, with first options (a, b) being selected more frequently than later ones (c,d), even though all options are equally likely. Counterfactual and contrastive methods lead to failure cases that disregard options of the answer set. Self help leads to a more balanced selection distribution. For status quo, we observe that the status quo prompting inversely biases the model to select the status quo option less frequently."
        },
        "4": {
            "figure_path": "2403.00811v1_figure_4.png",
            "caption": "Figure 3: We observe a strong primacy effect, with first options (a, b) being selected more frequently than later ones (c,d), even though all options are equally likely. Counterfactual and contrastive methods lead to failure cases that disregard options of the answer set. Self help leads to a more balanced selection distribution. For status quo, we observe that the status quo prompting inversely biases the model to select the status quo option less frequently."
        },
        "5": {
            "figure_path": "2403.00811v1_figure_5.png",
            "caption": "Figure 3: We observe a strong primacy effect, with first options (a, b) being selected more frequently than later ones (c,d), even though all options are equally likely. Counterfactual and contrastive methods lead to failure cases that disregard options of the answer set. Self help leads to a more balanced selection distribution. For status quo, we observe that the status quo prompting inversely biases the model to select the status quo option less frequently."
        },
        "6": {
            "figure_path": "2403.00811v1_figure_6.png",
            "caption": "Figure 3: We observe a strong primacy effect, with first options (a, b) being selected more frequently than later ones (c,d), even though all options are equally likely. Counterfactual and contrastive methods lead to failure cases that disregard options of the answer set. Self help leads to a more balanced selection distribution. For status quo, we observe that the status quo prompting inversely biases the model to select the status quo option less frequently."
        },
        "7": {
            "figure_path": "2403.00811v1_figure_7.png",
            "caption": "Figure 4: Ratio of biased prompts that were successfully de-biased, with bias-inducing parts removed in the self-help de-biased prompt. Higher Capacity Models experience greater self-help debiasing success for prompt-induced cognitive bias."
        }
    },
    "references": [
        {
            "1": {
                "title": "Persistent anti-muslim bias in large language models.",
                "author": "Abubakar Abid, Maheen Farooqi, and James Zou. 2021.",
                "venue": "In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, pages 298\u2013306.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Despite\" super-human\" performance, current llms are unsuited for decisions about ethics and safety.",
                "author": "Joshua Albrecht, Ellie Kitanidis, and Abraham J Fetterman. 2022.",
                "venue": "arXiv preprint arXiv:2212.06295.",
                "url": null
            }
        },
        {
            "3": {
                "title": "How cognitive biases affect xai-assisted decision-making: A systematic review.",
                "author": "Astrid Bertrand, Rafik Belloum, James R Eagan, and Winston Maxwell. 2022.",
                "venue": "In Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society, pages 78\u201391.",
                "url": null
            }
        },
        {
            "4": {
                "title": "Man is to computer programmer as woman is to homemaker? debiasing word embeddings.",
                "author": "Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. 2016.",
                "venue": "Advances in neural information processing systems, 29.",
                "url": null
            }
        },
        {
            "5": {
                "title": "Ai-moderated decision-making: Capturing and balancing anchoring bias in sequential decision tasks.",
                "author": "Jessica Maria Echterhoff, Matin Yarmand, and Julian McAuley. 2022.",
                "venue": "In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems, pages 1\u20139.",
                "url": null
            }
        },
        {
            "6": {
                "title": "A two-process account of long-term serial position effects.",
                "author": "Arthur M Glenberg, Margaret M Bradley, Jennifer A Stevenson, Thomas A Kraus, Marilyn J Tkachuk, Ann L Gretz, Joel H Fish, and BettyAnn M Turpin. 1980.",
                "venue": "Journal of Experimental Psychology: Human Learning and Memory, 6(4):355.",
                "url": null
            }
        },
        {
            "7": {
                "title": "Bias beyond English: Counterfactual tests for bias in sentiment analysis in four languages.",
                "author": "Seraphina Goldfarb-Tarrant, Adam Lopez, Roi Blanco, and Diego Marcheggiani. 2023.",
                "venue": "In Findings of the Association for Computational Linguistics: ACL 2023, pages 4458\u20134468, Toronto, Canada. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.findings-acl.272"
            }
        },
        {
            "8": {
                "title": "Illusory correlation in interpersonal perception: A cognitive basis of stereotypic judgments.",
                "author": "David L Hamilton and Robert K Gifford. 1976.",
                "venue": "Journal of Experimental Social Psychology, 12(4):392\u2013407.",
                "url": null
            }
        },
        {
            "9": {
                "title": "The evolution of cognitive bias.",
                "author": "Martie G Haselton, Daniel Nettle, and Paul W Andrews. 2015.",
                "venue": "The handbook of evolutionary psychology, pages 724\u2013746.",
                "url": null
            }
        },
        {
            "10": {
                "title": "Detect and perturb: Neutral rewriting of biased and sensitive text via gradient-based decoding.",
                "author": "Zexue He, Bodhisattwa Prasad Majumder, and Julian McAuley. 2021.",
                "venue": "In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4173\u20134181, Punta Cana, Dominican Republic. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.findings-emnlp.352"
            }
        },
        {
            "11": {
                "title": "Reducing sentiment bias in language models via counterfactual evaluation.",
                "author": "Po-Sen Huang, Huan Zhang, Ray Jiang, Robert Stanforth, Johannes Welbl, Jack Rae, Vishal Maini, Dani Yogatama, and Pushmeet Kohli. 2019.",
                "venue": "arXiv preprint arXiv:1911.03064.",
                "url": null
            }
        },
        {
            "12": {
                "title": "MathPrompter: Mathematical reasoning using large language models.",
                "author": "Shima Imani, Liang Du, and Harsh Shrivastava. 2023.",
                "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track), pages 37\u201342, Toronto, Canada. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.acl-industry.4"
            }
        },
        {
            "13": {
                "title": "Instructed to bias: Instruction-tuned language models exhibit emergent cognitive bias.",
                "author": "Itay Itzhak, Gabriel Stanovsky, Nir Rosenfeld, and Yonatan Belinkov. 2023.",
                "venue": "arXiv preprint arXiv:2308.00225.",
                "url": null
            }
        },
        {
            "14": {
                "title": "Capturing failures of large language models via human cognitive biases.",
                "author": "Erik Jones and Jacob Steinhardt. 2022.",
                "venue": "Advances in Neural Information Processing Systems, 35:11785\u201311799.",
                "url": null
            }
        },
        {
            "15": {
                "title": "Judgment under uncertainty: Heuristics and biases.",
                "author": "Daniel Kahneman, Paul Slovic, and Amos Tversky. 1982.",
                "venue": "Cambridge university press.",
                "url": null
            }
        },
        {
            "16": {
                "title": "Bias out-of-the-box: An empirical analysis of intersectional occupational biases in popular generative language models.",
                "author": "Hannah Rose Kirk, Yennie Jun, Filippo Volpin, Haider Iqbal, Elias Benussi, Frederic Dreyer, Aleksandar Shtedritski, and Yuki Asano. 2021.",
                "venue": "Advances in neural information processing systems, 34:2611\u20132624.",
                "url": null
            }
        },
        {
            "17": {
                "title": "Benchmarking cognitive biases in large language models as evaluators.",
                "author": "Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae Myung Kim, and Dongyeop Kang. 2023.",
                "venue": "arXiv preprint arXiv:2309.17012.",
                "url": null
            }
        },
        {
            "18": {
                "title": "Gender bias and stereotypes in large language models.",
                "author": "Hadas Kotek, Rikker Dockum, and David Q Sun. 2023.",
                "venue": "arXiv preprint arXiv:2308.14921.",
                "url": null
            }
        },
        {
            "19": {
                "title": "Prompted LLMs as chatbot modules for long open-domain conversation.",
                "author": "Gibbeum Lee, Volker Hartmann, Jongho Park, Dimitris Papailiopoulos, and Kangwook Lee. 2023.",
                "venue": "In Findings of the Association for Computational Linguistics: ACL 2023, pages 4536\u20134554, Toronto, Canada. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.findings-acl.277"
            }
        },
        {
            "20": {
                "title": "Pre-trained language models for interactive decision-making.",
                "author": "Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang, Ekin Aky\u00fcrek, Anima Anandkumar, et al. 2022.",
                "venue": "Advances in Neural Information Processing Systems, 35:31199\u201331212.",
                "url": null
            }
        },
        {
            "21": {
                "title": "Towards understanding and mitigating social biases in language models.",
                "author": "Paul Pu Liang, Chiyu Wu, Louis-Philippe Morency, and Ruslan Salakhutdinov. 2021.",
                "venue": "In International Conference on Machine Learning, pages 6565\u20136576. PMLR.",
                "url": null
            }
        },
        {
            "22": {
                "title": "Mind the biases: Quantifying cognitive biases in language model prompting.",
                "author": "Ruixi Lin and Hwee Tou Ng. 2023.",
                "venue": "In Findings of the Association for Computational Linguistics: ACL 2023, pages 5269\u20135281.",
                "url": null
            }
        },
        {
            "23": {
                "title": "Debiasing through raising awareness reduces the anchoring bias.",
                "author": "Carolyn Mair, Martin Shepperd, et al. 2014.",
                "venue": "-.",
                "url": null
            }
        },
        {
            "24": {
                "title": "Stereoset: Measuring stereotypical bias in pretrained language models.",
                "author": "Moin Nadeem, Anna Bethke, and Siva Reddy. 2020.",
                "venue": "arXiv preprint arXiv:2004.09456.",
                "url": null
            }
        },
        {
            "25": {
                "title": "Pipelines for social bias testing of large language models.",
                "author": "Debora Nozza, Federcio Bianchi, Dirk Hovy, et al. 2022.",
                "venue": "In Proceedings of BigScience Episode# 5\u2013Workshop on Challenges & Perspectives in Creating Large Language Models. Association for Computational Linguistics.",
                "url": null
            }
        },
        {
            "26": {
                "title": "Bias in word embeddings.",
                "author": "Orestis Papakyriakopoulos, Simon Hegelich, Juan Carlos Medina Serrano, and Fabienne Marco. 2020.",
                "venue": "In Proceedings of the 2020 conference on fairness, accountability, and transparency, pages 446\u2013457.",
                "url": null
            }
        },
        {
            "27": {
                "title": "Supporting human-ai collaboration in auditing llms with llms.",
                "author": "Charvi Rastogi, Marco Tulio Ribeiro, Nicholas King, Harsha Nori, and Saleema Amershi. 2023.",
                "venue": "In Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society, pages 913\u2013926.",
                "url": null
            }
        },
        {
            "28": {
                "title": "Beyond accuracy: Behavioral testing of nlp models with checklist.",
                "author": "Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. 2020.",
                "venue": "arXiv preprint arXiv:2005.04118.",
                "url": null
            }
        },
        {
            "29": {
                "title": "A survey of evaluation metrics used for nlg systems.",
                "author": "Ananya B Sai, Akash Kumar Mohankumar, and Mitesh M Khapra. 2022.",
                "venue": "ACM Computing Surveys (CSUR), 55(2):1\u201339.",
                "url": null
            }
        },
        {
            "30": {
                "title": "Status quo bias in decision making.",
                "author": "William Samuelson and Richard Zeckhauser. 1988.",
                "venue": "Journal of risk and uncertainty, 1:7\u201359.",
                "url": null
            }
        },
        {
            "31": {
                "title": "Large pre-trained language models contain human-like biases of what is right and wrong to do.",
                "author": "Patrick Schramowski, Cigdem Turan, Nico Andersen, Constantin A Rothkopf, and Kristian Kersting. 2022.",
                "venue": "Nature Machine Intelligence, 4(3):258\u2013268.",
                "url": null
            }
        },
        {
            "32": {
                "title": "Counterfactually augmented data and unintended bias: The case of sexism and hate speech detection.",
                "author": "Indira Sen, Mattia Samory, Claudia Wagner, and Isabelle Augenstein. 2022.",
                "venue": "In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4716\u20134726, Seattle, United States. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2022.naacl-main.347"
            }
        },
        {
            "33": {
                "title": "Challenging the appearance of machine intelligence: Cognitive bias in llms.",
                "author": "Alaina N Talboy and Elizabeth Fuller. 2023.",
                "venue": "arXiv preprint arXiv:2304.01358.",
                "url": null
            }
        },
        {
            "34": {
                "title": "Evaluating the factual consistency of large language models through news summarization.",
                "author": "Derek Tam, Anisha Mascarenhas, Shiyue Zhang, Sarah Kwan, Mohit Bansal, and Colin Raffel. 2023.",
                "venue": "In Findings of the Association for Computational Linguistics: ACL 2023, pages 5220\u20135255, Toronto, Canada. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.findings-acl.322"
            }
        },
        {
            "35": {
                "title": "Bias dilemma: de-biasing and the consequent introduction of new biases.",
                "author": "Jiulin Teng. 2013.",
                "venue": "HEC Paris Research Paper No. SPE-2013-1025.",
                "url": null
            }
        },
        {
            "36": {
                "title": "The framing of decisions and the psychology of choice.",
                "author": "Amos Tversky and Daniel Kahneman. 1981.",
                "venue": "science, 211(4481):453\u2013458.",
                "url": null
            }
        },
        {
            "37": {
                "title": "A study of implicit bias in pretrained language models against people with disabilities.",
                "author": "Pranav Narayanan Venkit, Mukund Srinath, and Shomir Wilson. 2022.",
                "venue": "In Proceedings of the 29th International Conference on Computational Linguistics, pages 1324\u20131332.",
                "url": null
            }
        },
        {
            "38": {
                "title": "Investigating gender bias in language models using causal mediation analysis.",
                "author": "Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and Stuart Shieber. 2020.",
                "venue": "Advances in neural information processing systems, 33:12388\u201312401.",
                "url": null
            }
        },
        {
            "39": {
                "title": "Fairpy: A toolkit for evaluation of social biases and their mitigation in large language models.",
                "author": "Hrishikesh Viswanath and Tianyi Zhang. 2023.",
                "venue": "arXiv preprint arXiv:2302.05508.",
                "url": null
            }
        },
        {
            "40": {
                "title": "Zero-shot cross-lingual summarization via large language models.",
                "author": "Jiaan Wang, Yunlong Liang, Fandong Meng, Beiqi Zou, Zhixu Li, Jianfeng Qu, and Jie Zhou. 2023.",
                "venue": "In Proceedings of the 4th New Frontiers in Summarization Workshop, pages 12\u201323.",
                "url": null
            }
        },
        {
            "41": {
                "title": "Self-consistency improves chain of thought reasoning in language models.",
                "author": "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022.",
                "venue": "arXiv preprint arXiv:2203.11171.",
                "url": null
            }
        },
        {
            "42": {
                "title": "Efficacy of bias awareness in debiasing oil and gas judgments.",
                "author": "Matthew B Welsh, Steve H Begg, and Reidar B Bratvold. 2007.",
                "venue": "In Proceedings of the Annual Meeting of the Cognitive Science Society, volume 29.",
                "url": null
            }
        },
        {
            "43": {
                "title": "Examining inter-consistency of large language models collaboration: An in-depth analysis via debate.",
                "author": "Kai Xiong, Xiao Ding, Yixin Cao, Ting Liu, and Bing Qin. 2023.",
                "venue": "In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 7572\u20137590, Singapore. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.findings-emnlp.508"
            }
        },
        {
            "44": {
                "title": "Double perturbation: On the robustness of robustness and counterfactual bias evaluation.",
                "author": "Chong Zhang, Jieyu Zhao, Huan Zhang, Kai-Wei Chang, and Cho-Jui Hsieh. 2021.",
                "venue": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3899\u20133916, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.naacl-main.305"
            }
        },
        {
            "45": {
                "title": "Gender bias in coreference resolution: Evaluation and debiasing methods.",
                "author": "Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. 2018.",
                "venue": "arXiv preprint arXiv:1804.06876.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.00811v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.4",
            "4",
            "4.1",
            "4.2",
            "4.3"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.1",
            "5.2",
            "5.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "1",
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.4",
            "4",
            "4.1",
            "4.2",
            "4.3",
            "5",
            "5.1",
            "5.2",
            "5.3"
        ]
    },
    "research_context": {
        "paper_id": "2403.00811v1",
        "paper_title": "Cognitive Bias in High-Stakes Decision-Making with LLMs",
        "research_background": "### Motivation\nThe motivation for this paper arises from the increasing use of large language models (LLMs) in day-to-day decision-making tasks. While LLMs have shown strong performance in tasks such as summarizing documents, answering math questions, and providing chat support, they still suffer from various biases. Existing research has addressed algorithmic and societal biases in LLMs, yet there is a gap in the study of cognitive bias, particularly in high-stakes decisions. Cognitive biases are systematic deviations from rationality which can adversely affect decision-making processes when humans interact with LLMs. Given the complexity and often hidden nature of cognitive biases, there is an urgent need for methods to identify and mitigate these biases in LLMs.\n\n### Research Problem\nThe main research problem addressed in this paper is the quantification and mitigation of cognitive biases in large language models, especially when they are used in high-stakes decision-making scenarios. Unlike societal bias, cognitive bias arises from information processing mechanisms and thus is difficult to detect and assess. The lack of robust assessment tools compounds this problem. The paper seeks to develop a novel approach, including new techniques for testing and reducing cognitive bias in LLMs through prompting strategies.\n\n### Relevant Prior Work\n1. **Capabilities of LLMs**: Prior research has demonstrated the strong performance of LLMs in various tasks:\n   - Summarizing documents (Wang et al., 2023).\n   - Answering math questions (Imani et al., 2023).\n   - Providing chat support (Lee et al., 2023).\n\n2. **Bias in LLMs**: Multiple studies have recognized that LLMs suffer from different types of biases:\n   - General algorithmic bias (Zhao et al., 2018; Nadeem et al., 2020; Liang et al., 2021; He et al., 2021).\n   - Societal bias arising from cultural and social backgrounds which influence model behavior (Schramowski et al., 2022).\n\n3. **Cognitive Bias**: Cognitive bias in both humans and machine learning interactions is a recognized issue:\n   - Systematic deviation from rationality due to subjective reality construction (Haselton et al., 2015; Kahneman et al., 1982).\n   - Requires different evaluation and mitigation procedures compared to societal bias (Sai et al., 2022).\n\nThe paper builds on this extensive body of work by proposing \"BiasBuster,\" a framework specifically designed to address cognitive biases in LLMs through evaluation and automated mitigation strategies.",
        "methodology": "### Proposed Methodology for Analyzing Cognitive Bias in High-Stakes Decision-Making with LLMs\n\n#### Overview\nThe methodology section details a structured approach to evaluate various cognitive biases within large language models (LLMs) in the context of high-stakes decision-making, such as student admissions. The key cognitive biases examined include prompt-based cognitive bias, inherent bias, sequential bias, anchoring bias, status quo/primacy bias, framing bias, and group attribution error. The research methodology involves designing specific experiments and leveraging templates to rigorously analyze these biases.\n\n#### Bias Classification:\n1. **Prompt-Based Cognitive Bias:** Induced through the user\u2019s prompt.\n2. **Inherent Bias:** Embedded within the model as a result of its training data, not directly linked to the user's prompt.\n3. **Sequential Bias:** Stemming from the model\u2019s previous responses within the same conversation.\n\n#### Experiment Designs:\n1. **Anchoring Bias:**\n   - **Setup:** Inspired by Echterhoff et al. (2022), the model emulates an admissions officer who continuously updates decisions. Synthetic student profiles are created and presented to the language model in various sequences.\n   - **Objective:** Observe if the model\u2019s decision changes based on different sequences of student profiles.\n   - **Confidence Measurement:** Uses normalized Euclidean distance to measure the distance between admission-rejection probability distributions over multiple order perturbations.\n   - **Metric:** Confidence of a model\u2019s decision, where higher dissimilarity (values near 1) indicates less confidence.\n\n2. **Status Quo/Primacy Bias:**\n   - **Setup:** Questions offering a default option (status quo) are compared to their neutral counterparts. Specifically, a scenario where a student has interned with the admissions officer before is used as a status quo indicator.\n   - **Objective:** Evaluate if the model disproportionately prefers options marked with status quo information.\n   - **Metric:** Distribution of chosen options; examining if the status quo option (A over B, C, D) is selected more frequently.\n\n3. **Framing Bias:**\n   - **Setup:** Templates presenting decision questions in positive or negative frames are used. Positive framing asks if a student will be admitted, and negative framing asks if the student will be rejected.\n   - **Objective:** Assess if the model's decision varies with different framing despite identical information content.\n   - **Metric:** Difference in admission rates, aiming for consistency irrespective of the framing approach.\n\n4. **Group Attribution Error:**\n   - **Setup:** Examines stereotypical judgments based on gender. Profiles only differ by the group attribute, and their evaluation focuses on a characteristic stereotypically associated with that group (e.g., math ability).\n   - **Objective:** Detects if the model\u2019s judgment of a student's ability changes based on gender, given otherwise identical information.\n   - **Metric:** Classification difference rate; the model should ideally show no variance in assessment between different gender groups.\n\n5. **Primacy Bias:**\n   - **Setup:** Uses the neutral template of status quo experiment with options shuffled; targets the sequence in which student profiles appear (i.e., early vs. later).\n   - **Objective:** Determine if the model favors options shown earlier in the sequence.\n   - **Metric:** Ratio of selections of early options (A, B) versus later ones (C, D); expectation is a uniform distribution if unbiased.\n\n#### Conclusion\nThe methodology employs a mix of synthetic data, predefined prompts, and controlled perturbation of input sequences to uncover and quantify biases within language models. Each bias has a tailored experiment design to closely simulate real-world decision scenarios. The research leverages metrics like normalized Euclidean distance and selection ratios to provide a quantifiable analysis of model behavior, aspiring to derive comprehensive insights into the susceptibility of LLMs to various cognitive biases.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n**Experiment Setup:**\n- **Models Evaluated**: Four language models were evaluated: GPT-3.5-turbo, GPT-4, LLama 2 7B, and LLama 2 13B.\n- **Bias Types Investigated**: Cognitive biases examined included framing bias, group attribution bias, status quo bias, anchoring bias, and primacy bias.\n- **Evaluation Metrics**: The primary metrics were the differences in behavior and decision-making across different prompt framings and group attributes. Evaluation metrics included the fraction of chosen options and the number of biased prompts before and after applying self-help debiasing strategies.\n- **Dataset**: Instances with 400 prompts were used for bias evaluation to balance practicality and cost where applicable.\n\n**Main Experimental Results:**\n- **Framing Bias**: GPT-4 demonstrated significant susceptibility to framing bias, admitting 40% more students in the reject framing. Llama-2 7B showed a considerable group attribution bias, classifying 32% fewer females as good at math.\n- **Status Quo Bias**: Status quo bias was not prevalent; instead, models displayed an inverse bias where the current option (e.g., option A) was selected fewer times than a neutral prompt (excluding GPT-4).\n- **Primacy Bias**: Primacy bias was observed as a preference for early options in the prompt (A or B) over later ones (C or D).\n- **Anchoring Bias**: Anchoring bias led to lower decision confidence in initial evaluations. \n\nIn summary, higher capacity models demonstrated superior capabilities in debiasing prompts autonomously. GPT-4 and Llama 2 13B were notably effective, reducing group attribution and framing biases to minimal levels. However, lower capacity models like GPT-3.5-turbo showed limited debiasing capability, indicating the necessity for more advanced models or alternate debiasing techniques for lower parameter models."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To analyze the influence of previous decisions in language models and evaluate anchoring bias in sequential setups.",
            "experiment_process": "The model is asked to take the role of an admissions officer deciding which student to admit to a college program. Synthetic student profiles are created and shown to the model in one conversation, successively adding previous students and decisions to the context. Different sets of students are shown in multiple orders to identify if the model's decisions differ based on the order. The confidence of admissions decisions is measured using normalized Euclidean distance between probability distributions of admission and rejection.",
            "result_discussion": "For anchoring bias, the study observed smaller decision confidence in the original evaluation setup, potentially influenced by previous decisions. Self-help increased decision confidence for commercial GPT models, but not for open-source Llama models. High-capacity models (e.g., GPT) were more effective at debiasing themselves than lower capacity models (e.g., Llama).",
            "ablation_id": "2403.00811v1.No1"
        },
        {
            "research_objective": "To evaluate the presence and impact of status quo bias in language models.",
            "experiment_process": "Inspired by Samuelson and Zeckhauser's research, the setup includes a neutral question without status information and a status quo question indicating a default option ('having worked with student X during a summer internship'). 16 student profiles are used, and 4 are shown at a time. The experiment measures if the status quo option is chosen more often than others by evaluating if the distribution of selections is deviated from uniformity.",
            "result_discussion": "Results show an inverse status quo bias in all models except GPT-4. When prompted with the status quo option (e.g., default being 'student A'), it was selected fewer times compared to the neutral prompt. Suggesting that LLMs may react oppositely to human-like status quo bias.",
            "ablation_id": "2403.00811v1.No2"
        },
        {
            "research_objective": "To analyze framing bias in language models based on positive and negative framing of questions.",
            "experiment_process": "The study is adapted from research on framing effects, specifically focusing on college admissions. The model is prompted under two conditions: positive framing (asking if a student will be admitted) and negative framing (asking if the student will be rejected). The admissions rate is assessed to determine if framing impacts decision-making.",
            "result_discussion": "For framing bias, the models, including GPT-4, displayed vulnerability, with GPT-4 admitting 40% more students under reject framing. The framing effect was evident across models, influencing their decision rates significantly.",
            "ablation_id": "2403.00811v1.No3"
        },
        {
            "research_objective": "To test group attribution bias in language models based on gender stereotypes.",
            "experiment_process": "The model takes the role of an admissions officer evaluating students' math abilities with all student data except gender being identical. The objective is to observe if the model's assessment of mathematical ability changes when the gender attribute is varied.",
            "result_discussion": "For group attribution bias, the findings indicated that models classified fewer females as good at math compared to males. LLama-2 7B was notably biased, classifying 32% fewer females as proficient in math.",
            "ablation_id": "2403.00811v1.No4"
        },
        {
            "research_objective": "To examine primacy bias in language models' decision-making.",
            "experiment_process": "Primacy bias is tested using a neutral task setup adapted from status quo bias tasks, without any priming information. Student options were shuffled to appear in different positions (A, B, C, D) across different prompts to see if early presented options were preferred over later ones.",
            "result_discussion": "The study found that for primacy bias, models showed a preference for options presented early (A or B) over later ones (C or D). Self-help debiasing improved distribution uniformity for higher capacity models (e.g., GPT-4), while cognitive bias awareness also mitigated bias to some extent in models like LLama 2.",
            "ablation_id": "2403.00811v1.No5"
        },
        {
            "research_objective": "To evaluate different cognitive bias mitigation strategies, specifically self-help debiasing.",
            "experiment_process": "The self-help method involves instructing the language model to rewrite prompts to reduce bias, followed by re-answering the revised prompt. This unsupervised method aims to provide a generalized approach for mitigating various cognitive biases without manual adaptation.",
            "result_discussion": "Self-help debiasing improved performance, particularly for high-capacity models, reducing biased prompt components to 0 for group attribution and framing biases. However, the method was less effective for lower capacity models like LLama. Self-help also avoided issues seen with contrastive and counterfactual explanations, which could lead to extreme and inconsistent outcomes.",
            "ablation_id": "2403.00811v1.No6"
        }
    ]
}