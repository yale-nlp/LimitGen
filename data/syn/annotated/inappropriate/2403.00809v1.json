{
    "title": "Abdelhak at SemEval-2024 Task 9 : Decoding Brainteasers, The Efficacy of Dedicated Models Versus ChatGPT",
    "abstract": "This study introduces a dedicated model aimed at solving the BRAINTEASER task 9 Jiang et al. (2023), a novel challenge designed to assess models\u2019 lateral thinking capabilities through sentence and word puzzles. Our model demonstrates remarkable efficacy, securing Rank 1 in sentence puzzle solving during the test phase with an overall score of 0.98. Additionally, we explore the comparative performance of ChatGPT, specifically analyzing how variations in temperature settings affect its ability to engage in lateral thinking and problem-solving. Our findings indicate a notable performance disparity between the dedicated model and ChatGPT, underscoring the potential of specialized approaches in enhancing creative reasoning in AI.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The BRAINTEASER task Jiang et al. (2023  ###reference_b5###) aims to challenge the lateral thinking abilities of models, setting it apart from traditional tasks focused on vertical logical reasoning. It introduces lateral thinking puzzles in the form of multiple-choice questions to test the models\u2019 ability to think creatively and challenge common sense associations. The goal is to identify the gap between human and model performances in creative thinking, highlighting the need for progress in AI\u2019s creative reasoning abilities. NLP (Natural Language Processing) transformer models have revolutionized text understanding and generation with their architecture capable of processing word sequences more efficiently. For multiple-choice questions, these models utilize their ability to understand context and language nuances to select the most appropriate answer from several options. Thanks to deep learning and attention mechanisms, they excel in various NLP tasks, significantly improving the accuracy and relevance of responses generated in complex contexts.\nThe integration of NLP transformer models into the BRAINTEASER task aims to explore their ability to solve lateral thinking puzzles in the form of multiple-choice questions. This approach highlights the challenges posed by deep language understanding and the creativity required to surpass traditional logical reasoning. It emphasizes the importance of advancing in the development of models capable of navigating beyond common sense associations, encouraging innovation in the interpretation and generation of complex and nuanced responses.\nIn our study, we will explore the ability of language models to handle this task, with the following main contributions of this paper :\nDevelopment of a dedicated model for this task with a good result for the sentence puzzle task (Rank 1 in the test phase).\nA comparative analysis with ChatGPT: Specifically, the relationship of temperature with lateral thinking and performance."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Shared Task Description",
            "text": "The BRAINTEASER Shared Task 9 Jiang et al. (2023  ###reference_b5###) is a Question Answering (QA) task based on evaluating the capacity of language models to engage in lateral thinking and to solve puzzles that require unconventional thinking. BRAINTEASER comprises two distinct subtasks: Sentence Puzzle and Word Puzzle, both of which involve defying commonsense \"defaults\" but through different methodologies.\nSentence Puzzle: Create sentence-based brain teasers where the challenge lies in interpreting sentence snippets in a way that goes against commonsense expectations.\nWord Puzzle: Design word-based brain teasers that require rethinking the default meanings of words, with a focus on the composition of letters in the target question.\nBoth tasks include an adversarial subset, created by manually modifying the original brain teasers without changing their latent reasoning path. They construct adversarial versions of the original data in two ways:\n(SR) Semantic Reconstruction rephrases the original question without changing the correct answer and the distractors.\n(CR) Context Reconstruction keeps the original reasoning path but changes both the question and the answer to describe a new situational context\nDistractors are generated by identifying the implicit and explicit premises of a puzzle and then manually overwriting these premises, ensuring they remain incorrect but challenging.\nThe BRAINTEASER Jiang et al. (2023  ###reference_b5###) paper reveals a significant gap between human performances and AI models, and underscores the need to enhance lateral reasoning in language models."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "The task of commonsense reasoning has long been a challenge for deep learning and has been the subject of research for several years, accompanied by various benchmarks such as Nie et al. (2020  ###reference_b6###), which introduces a new large-scale NLI benchmark dataset created through an adversarial process involving humans and models. This improves NLI models\u2019 performance on popular benchmarks and reveals their weaknesses, offering a dynamic framework for continuous improvement in natural language understanding. A study demonstrated a simple and unsupervised method for commonsense reasoning using language models trained on vast text corpora, significantly outperforming state-of-the-art methods on Pronoun Disambiguation Problems and the Winograd Schema Challenge without the need for annotated knowledge bases or manually engineered features Trinh and Le (2019  ###reference_b12###).\nTransformer models like BERT Devlin et al. (2019  ###reference_b2###), GPT Brown et al. (2020  ###reference_b1###), and their variants have revolutionized natural language understanding, including question answering Qu et al. (2019  ###reference_b8###). Their architecture captures semantic and contextual nuances Ethayarajh (2019  ###reference_b3###) Zhang et al. (2020  ###reference_b13###), proving exceptionally effective in comprehending and responding to complex inquiries. By training on extensive text corpora, they develop a deep understanding, enabling them to identify the most plausible answers among multiple choices Roy et al. (2023  ###reference_b10###) Ravi et al. (2023  ###reference_b9###).\nLarge pretrained language models (PLMs) can achieve near-human performance on commonsense reasoning tasks by generating contrastive explanations that highlight the key attributes needed to justify correct answers. This approach not only improves performance on commonsense reasoning benchmarks but also produces explanations judged by humans as more relevant and understandable Paranjape et al. (2021  ###reference_b7###)\nRecent studies reveal that ChatGPT has notable capabilities to effectively solve a variety of problems in several languages, including the task of answering questions. Moreover, its performance improves with each new version. ChatGPT excels in certain areas but also has its limitations in terms of consistency and complex reasoning tasks.Tan et al. (2023  ###reference_b11###)."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Proposed Approach",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Methodology",
            "text": "In our study, we have developed a model based on transformers for multiple-choice questions, where each option is combined with the question to form separate pairs. These pairs are then pre-processed as distinct inputs for the already pre-trained model. The preprocessing includes adding special tokens like [CLS] at the beginning and [SEP] to separate the question from the choice. Each pre-processed question-choice pair is passed through the transformer model, which encodes each pair using its unidirectional attention mechanism, allowing every word in the pair to capture the context of the entire sentence and the related choice. For each question-choice pair, the model generates a feature vector from the output associated with the [CLS] token, which serves as a summary of the information contained in the pair. This means that for a question with four answer choices, the model would be run four times (once for each question-choice pair). This process allows for the consideration of the full context of the question as well as that of each individual answer choice, which is crucial for understanding which choice best answers the question.\n###figure_1### ###figure_2### The feature vector for each question-choice pair is then passed through a dense (or fully connected) layer, which reduces the vector\u2019s dimensionality to a number corresponding to the number of classes or answer categories. After the dense layer, a softmax activation function is applied to convert the scores into probabilities. The softmax function is ideal for classification tasks because it transforms the scores into a set of probabilities that sum up to 1, making the scores directly interpretable as the probabilities that each choice is the correct answer. Figure 1 ###reference_### illustrates the prediction process described above.\nThe prediction formula can be expressed as follows in our model:\nEach question-choice pair is pre-processed to form an input sequence by concatenating the question with each choice and adding special tokens:\nThe transformer model processes each separately to encode the pair, utilizing its unidirectional attention mechanism. The output for each token in is obtained, but we are specifically interested in the output associated with the [CLS] token, , which captures the contextualized representation of the pair:\nThe feature vector is extracted from the transformer output associated with the [CLS] token for each question-choice pair:\nEach feature vector is passed through a dense layer to reduce its dimensionality to the number of classes , resulting in a reduced feature vector :\nThe softmax activation function is applied to to convert the scores into probabilities, indicating the likelihood that each choice is the correct answer:\nWhere:\nrepresents the question.\nrepresents the th answer choice.\nis the input sequence formed by concatenating and with special tokens.\nis the transformer output for the [CLS] token for the th question-choice pair.\nis the feature vector extracted from .\nis the reduced feature vector after passing through a dense layer.\nrepresents the probabilities that each choice is the correct answer, obtained after applying the softmax function to ."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Evaluation Method",
            "text": "The BRAINTEASER task proposes the following evaluation system, each system is evaluated based on the following two accuracy metrics:\nInstance-based Accuracy: They consider each question (original/adversarial) as a separate instance. They report accuracy for the original and its adversaries.\nGroup-based Accuracy: Each question and its associated adversarial instances form a group, and a system will only receive a score of 1 when it correctly solves all questions in the group.\nThe final score corresponds to the average of all the scores."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Results",
            "text": "We trained our model using the pre-trained language model DeBERTa-v3-base He et al. (2023  ###reference_b4###) over 5 learning epochs, with a learning rate of 5e-5 and a batch size of 16. The results obtained are presented in official Leaderboard of the task in the evaluation phase 2  ###reference_###.\nOur model stands out for its good performance in sentence-type puzzles, ranking first with with an average accuracy score of 0.98 ( leaderboard 2  ###reference_###) . This means it excels particularly in thinking challenges where the puzzle, often contrary to common sense, is based on sentence excerpts. On the other hand, for word-based puzzles, which require finding a solution that goes against the usual meaning of words by focusing on the letter composition of the posed question, our model shows lower performance. It ranks 16th with a total score of 0.61 . This performance difference suggests that, although our model is very skilled at solving puzzles involving the understanding and manipulation of sentences, it could benefit from improvement in the area of word-based puzzles. This indicates an opportunity to deepen our research and development efforts on word-type puzzles to enhance the versatility and overall effectiveness of our model."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "ChatGPT Analysis",
            "text": ""
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Zero-shot Predictions",
            "text": "Given that we are currently in the era of ChatGPT, it\u2019s challenging to approach our study without including a comparison to evaluate the role of this task in relation to ChatGPT. We crafted a simple and explicit prompt with ChatGPT turbo 3.5 on February 5, 2024, assessing ChatGPT\u2019s logical reasoning ability using various prompts in a qualitative manner. However, we faced challenges in determining the optimal prompt, as the same input does not always lead to the desired output. Hallucinations related to conversation history were resolved by initiating a new session for each iteration. In the end, we settled on the following prompt:\n\u201c\u201d\u201d\n\n\nQuestion ?\n\n\n\nliste of choices :\n\n\n1- Answer 1.\n\n\n2- Answer 2.\n\n\n3- Answer 3.\n\n\n4- Answer 4.\n\n\nResponse should be in json format :\n\n\n{ \u201canswer\u201d: Number of the choice }\n\n\n\u201c\u201d\u201d\nWe achieved a total score of 0.59 for the sentence-puzzle task and 0.27 for the word-puzzle task, scores that do not necessarily match the expected performance for a model like ChatGPT. This suggests that, although ChatGPT was not specifically trained for this task, it might not be able to compete with models that were specially designed for it. ChatGPT was trained on a vast dataset, but it is assumed that most of this data is well-structured and more aligned with linear thinking rather than lateral thinking, which explains its moderate performance in this area."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "The Effect of Temperature",
            "text": "The temperature parameter in language models for natural language processing is a hyperparameter used to control the diversity of predictions made by the model during text generation. Temperature adjusts the likelihood of predictions based on their calculated probability, thereby influencing the level of risk or surprise in the choice of generated words. Adjusting the temperature allows for control over the trade-off between creativity and safety in text generation. Finding the right temperature depends on the specific application, the domain of use, and preferences for the balance between innovation and reliability in the generated responses. A low temperature close to 0 produces more conservative and repetitive responses, while a high temperature close to 1 yields more varied and creative responses.\nIs there a relationship between temperature and lateral thinking ? Although the temperature setting in language models and lateral thinking operate in different domains, they share a common goal of fostering creativity and innovation by breaking conventions and exploring possibilities beyond those that are immediately obvious. Lateral thinking encourages questioning assumptions and considering a variety of different perspectives. Similarly, by adjusting the temperature to favor less likely word selections, a language model can \"think\" more laterally, exploring linguistic options that would not be considered at a lower temperature. Therefore, we will measure the performance of ChatGPT based on temperature, relationship between temperature and lateral thinking.We will launch several runs by increasing the temperature from 0 to 1.2\n###figure_3### ###figure_4### Sentence Puzzle : The graphic 3  ###reference_### represents four data series corresponding to different test scenarios for the sentence puzzle task: Overall, OR (Original), SR (Semantic Reconstruction), and CR (Context Reconstruction). \"Overall\" indicates a benchmark or an overall average of performance, while OR shows stable results, suggesting a consistent baseline. CR follows a trend similar to OR, indicating that contextual reconstruction performs comparably to the original. In contrast, SR shows a notable degradation in performance towards the end, which could suggest that the semantic reconstruction method is less stable or effective under certain conditions. The data set suggests that while OR and CR methods maintain a degree of consistency, SR might involve a riskier or more innovative approach, which could be likened to a \"higher temperature\" in the context of lateral thinking, leading to more varied and potentially less predictable outcomes. However, increasing the temperature does not allow the model to perform better on a task, on the contrary, performance decreases.\nWord puzzle : In the case of word puzzles 4  ###reference_###, it is difficult to conclude as there are no clear trends observed. However, for the overall general case, it is noted that performance increases very slightly with temperature, which stands out in comparison to the sentence puzzle task, potentially because word puzzles better illustrate lateral thinking. In this case, the focus is not on the sentence, which contains more semantic aspects. The answer in this task violates the default meaning of the word and focuses on the letter composition of the target question."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "Our research underscores the significance of dedicated models in advancing AI\u2019s capability to solve complex lateral thinking tasks, as exemplified by our model\u2019s top-ranking performance in the BRAINTEASER sentence puzzles. The comparative analysis with ChatGPT highlights the limitations of general-purpose models in specific creative reasoning challenges, despite their overall versatility. The study also reveals the nuanced role of temperature settings in modulating ChatGPT\u2019s performance, offering insights into optimizing AI models for enhanced creativity and lateral thinking. Future work should focus on bridging the gap in word puzzle performance and further refining the balance between creativity and logical reasoning in AI systems."
        }
    ],
    "appendix": [],
    "tables": {},
    "image_paths": {
        "1": {
            "figure_path": "2403.00809v1_figure_1.png",
            "caption": "Figure 1: The overall architecture for predicting BRAINTEASER"
        },
        "2": {
            "figure_path": "2403.00809v1_figure_2.png",
            "caption": "Figure 2: The Ranking Leaderboard Displaying Our Position"
        },
        "3": {
            "figure_path": "2403.00809v1_figure_3.png",
            "caption": "Figure 3: ChatGPT Performance Across Different Temperatures (Sentence puzzle)"
        },
        "4": {
            "figure_path": "2403.00809v1_figure_4.png",
            "caption": "Figure 4: ChatGPT Performance Across Different Temperatures (Word puzzle)"
        }
    },
    "references": [
        {
            "1": {
                "title": "Language models are few-shot learners.",
                "author": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020.",
                "venue": "Advances in neural information processing systems, 33:1877\u20131901.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Bert: Pre-training of deep bidirectional transformers for language understanding.",
                "author": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.",
                "venue": null,
                "url": "http://arxiv.org/abs/1810.04805"
            }
        },
        {
            "3": {
                "title": "How contextual are contextualized word representations? comparing the geometry of bert, elmo, and gpt-2 embeddings.",
                "author": "Kawin Ethayarajh. 2019.",
                "venue": "arXiv preprint arXiv:1909.00512.",
                "url": null
            }
        },
        {
            "4": {
                "title": "Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing.",
                "author": "Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2111.09543"
            }
        },
        {
            "5": {
                "title": "Brainteaser: Lateral thinking puzzles for large language model.",
                "author": "Yifan Jiang, Filip Ilievski, and Kaixin Ma. 2023.",
                "venue": "arXiv preprint arXiv:2310.05057.",
                "url": null
            }
        },
        {
            "6": {
                "title": "Adversarial NLI: A new benchmark for natural language understanding.",
                "author": "Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. 2020.",
                "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4885\u20134901, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2020.acl-main.441"
            }
        },
        {
            "7": {
                "title": "Prompting contrastive explanations for commonsense reasoning tasks.",
                "author": "Bhargavi Paranjape, Julian Michael, Marjan Ghazvininejad, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2021.",
                "venue": "In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4179\u20134192, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.findings-acl.366"
            }
        },
        {
            "8": {
                "title": "Bert with history answer embedding for conversational question answering.",
                "author": "Chen Qu, Liu Yang, Minghui Qiu, W Bruce Croft, Yongfeng Zhang, and Mohit Iyyer. 2019.",
                "venue": "In Proceedings of the 42nd international ACM SIGIR conference on research and development in information retrieval, pages 1133\u20131136.",
                "url": null
            }
        },
        {
            "9": {
                "title": "Vlc-bert: visual question answering with contextualized commonsense knowledge.",
                "author": "Sahithya Ravi, Aditya Chinchure, Leonid Sigal, Renjie Liao, and Vered Shwartz. 2023.",
                "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1155\u20131165.",
                "url": null
            }
        },
        {
            "10": {
                "title": "Analysis of community question-answering issues via machine learning and deep learning: State-of-the-art review.",
                "author": "Pradeep Kumar Roy, Sunil Saumya, Jyoti Prakash Singh, Snehasish Banerjee, and Adnan Gutub. 2023.",
                "venue": "CAAI Transactions on Intelligence Technology, 8(1):95\u2013117.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Evaluation of chatgpt as a question answering system for answering complex questions.",
                "author": "Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu, Yongrui Chen, and Guilin Qi. 2023.",
                "venue": "arXiv preprint arXiv:2303.07992.",
                "url": null
            }
        },
        {
            "12": {
                "title": "A simple method for commonsense reasoning.",
                "author": "Trieu H. Trinh and Quoc V. Le. 2019.",
                "venue": null,
                "url": "http://arxiv.org/abs/1806.02847"
            }
        },
        {
            "13": {
                "title": "Semantics-aware bert for language understanding.",
                "author": "Zhuosheng Zhang, Yuwei Wu, Hai Zhao, Zuchao Li, Shuailiang Zhang, Xi Zhou, and Xiang Zhou. 2020.",
                "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 9628\u20139635.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.00809v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "3"
        ],
        "methodology_sections": [
            "4.1"
        ],
        "main_experiment_and_results_sections": [
            "4.2",
            "4.3",
            "5",
            "5.1",
            "5.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "5",
            "5.1",
            "5.2"
        ]
    },
    "research_context": {
        "paper_id": "2403.00809v1",
        "paper_title": "Abdelhak at SemEval-2024 Task 9 : Decoding Brainteasers, The Efficacy of Dedicated Models Versus ChatGPT",
        "research_background": "### Paper's Motivation\n\nThe motivation behind this paper is to address the gap between human and model performances in terms of creative thinking, particularly through the use of lateral thinking puzzles presented as multiple-choice questions. This task challenges models to think creatively and go beyond traditional logical reasoning. The study seeks to explore and enhance the ability of NLP transformer models to navigate complex and nuanced responses, emphasizing the significance of innovating AI's creative reasoning capabilities.\n\n### Research Problem\n\nThe primary research problem the paper addresses is the efficacy of NLP transformer models, including specialized models and ChatGPT, in solving lateral thinking puzzles. The study aims to evaluate how well these models can handle creative reasoning tasks and to analyze their performance, particularly in terms of their capability to go beyond common sense associations. The research also focuses on comparing a dedicated model\u2019s performance with that of ChatGPT, with a special interest in how the temperature setting affects lateral thinking and overall model performance.\n\n### Relevant Prior Work\n\n1. **BRAINTEASER Task**: The foundational task by Jiang et al. (2023) ###reference_b5### forms the basis of this study. It introduces lateral thinking puzzles in the form of multiple-choice questions to challenge models' creative thinking.\n   \n2. **NLP Transformer Models**: The study builds on the extensive prior work on NLP transformer models, which have revolutionized text understanding and generation. These models utilize deep learning and attention mechanisms to process word sequences and understand context more efficiently, excelling in various NLP tasks by significantly improving the accuracy and relevance of responses in complex contexts.\n\nThese references highlight the advancements in NLP and the specific focus on creative reasoning, setting the stage for exploring new challenges in AI lateral thinking through a comparative analysis with advanced models like ChatGPT.",
        "methodology": "The paper details a sophisticated methodology for addressing multiple-choice questions using a transformer-based model. The key innovation lies in how the question and each answer choice are paired and processed, leveraging the bidirectional attention mechanism of transformers to grasp the full contextual relevance of each option in relation to the question. Here is a summarized description using the original wording and phrases as much as possible:\n\n### Methodology Summary\n\n1. **Transformers for Question-Choice Pairs**:\n   - **Pair Formation**: Each answer option is paired with the question, creating separate question-choice pairs.\n   - **Preprocessing**: These pairs are pre-processed by adding special tokens such as `[CLS]` at the beginning and `[SEP]` to separate the question from the choice.\n\n2. **Transformer Encoding**:\n   - **Running the Model**: Each question-choice pair is individually passed through the pre-trained transformer model.\n   - **Bidirectional Attention**: The transformer's bidirectional attention mechanism encodes each pair, allowing every word to capture the context of the entire sentence and the related choice.\n   - **Feature Vector Extraction**: From the transformer's output, the feature associated with the `[CLS]` token is extracted, serving as a summary of the pair's information.\n\n3. **Dimensionality Reduction and Classification**:\n   - **Dense Layer**: The extracted feature vector for each pair is passed through a dense layer to reduce its dimensionality to the number of answer categories.\n   - **Softmax Activation**: A softmax function is applied to these scores to convert them into probabilities, which sum up to 1 and can be interpreted as the likelihood of each choice being correct.\n\n### Model Calculation Steps\n\n1. **Input Pre-Processing**:\n   - **Sequence Formation**: Each question-choice pair \\( (Q, C_i) \\) is concatenated and special tokens are added to form the input sequence \\( S_i \\).\n   \n2. **Encoding with Transformers**:\n   - **Bidirectional Attention**: The sequence \\( S_i \\) is passed through the transformer, which processes it using its attention mechanism.\n   - **Feature Extraction**: The output for the `[CLS]` token ( \\( T_{[CLS]_i} \\) ) representing the entire input sequence is extracted to obtain the feature vector \\( o_i \\).\n\n3. **Dimensionality Reduction**:\n   - **Dense Layer Output**: Each feature vector \\( o_i \\) is passed through a dense layer to obtain a reduced vector \\( d_i \\), corresponding to the number of answer classes.\n\n4. **Softmax Application**:\n   - **Probability Computation**: The softmax function is applied to \\( d_i \\), yielding the probabilities \\( P(C_i | Q) \\) for each choice \\( C_i \\) being correct.\n\n### Key Components\n\n- **Transformers**: Utilizes pre-trained transformers and their bidirectional attention mechanism for encoding.\n- **Special Tokens**: The addition of `[CLS]` and `[SEP]` tokens during preprocessing to structure input sequences.\n- **Feature Vector**: Extraction of the contextualized representation from the `[CLS]` token's output.\n- **Dense Layer**: Applies dimensionality reduction to the feature vector.\n- **Softmax Function**: Converts reduced vectors into interpretable probability scores.\n\nThis methodology emphasizes an integrated approach combining sophisticated preprocessing steps with powerful transformer-based encoding and classification mechanisms, optimized for interpreting and answering multiple-choice questions accurately.",
        "main_experiment_and_results": "The main experiment setup for the BRAINTEASER task, as described in the paper \"Abdelhak at SemEval-2024 Task 9: Decoding Brainteasers, The Efficacy of Dedicated Models Versus ChatGPT,\" involves evaluating systems based on two accuracy metrics: instance-based accuracy and group-based accuracy.\n\n**Datasets:** The experiments are conducted using data comprising original brainteaser questions and their corresponding adversarial instances. \n\n**Baselines:** This section isn\u2019t explicitly detailed in the provided excerpt. Further review of the full paper would be necessary to identify the specific models and systems used as baselines in the experimental comparisons.\n\n**Evaluation Metrics:**\n1. **Instance-based Accuracy**: This metric considers each question (whether original or adversarial) independently, and reports the accuracy of answering these individual instances correctly.\n2. **Group-based Accuracy**: In this approach, each question along with its adversarial instances is treated as a group. A system receives a score of 1 only if it correctly answers all questions within a group. The final score is an average of the scores across all groups.\n\n**Main Experimental Results:**\nThe final performance of the systems is presented as the average scores derived from the instance-based and group-based accuracy metrics. Further specific results would include a comparison between dedicated models and ChatGPT on their ability to decode brainteasers, though detailed numerical and comparative results are not provided in the text excerpt.\n\nTo summarize, the primary setup involves evaluating and comparing the correctness of responses to brainteaser questions and their adversarial instances using the two aforementioned accuracy metrics, with the final performance being an average score. For more detailed results and comparisons, a full-text review would be necessary."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Evaluate the logical reasoning ability of ChatGPT in solving sentence-puzzles and word-puzzles as part of the BRAINTEASER task, and to compare its performance to a specialized model designed for lateral thinking tasks.",
            "experiment_process": "To assess ChatGPT's performance, a simple and explicit prompt was crafted and evaluated using ChatGPT turbo 3.5 on February 5, 2024. The prompt required ChatGPT to select an answer from a list of choices in json format. Each session was initiated anew to avoid hallucinations related to conversation history. The scores were 0.59 for the sentence-puzzle task and 0.27 for the word-puzzle task.",
            "result_discussion": "ChatGPT's performance scores of 0.59 and 0.27 in sentence-puzzle and word-puzzle tasks respectively indicate that it does not match the performance of the specialized model. This suggests that while ChatGPT is trained on a vast dataset, it may be more aligned with linear thinking rather than the lateral thinking required for these tasks.",
            "ablation_id": "2403.00809v1.No1"
        },
        {
            "research_objective": "Investigate how variations in the temperature parameter affect ChatGPT\u2019s ability to perform lateral thinking tasks such as sentence-puzzles and word-puzzles.",
            "experiment_process": "The temperature was incrementally increased from 0 to 1.2 to see its effect on ChatGPT's performance on lateral thinking tasks. The temperature controls the diversity of predictions made by the model during text generation, with lower temperatures producing more conservative responses and higher temperatures yielding more varied responses.",
            "result_discussion": "For sentence puzzles, the results showed that overall performance declined with increasing temperature, suggesting that higher temperature might lead to more varied but less effective outcomes in this context. Specifically, original and context reconstruction methods showed consistent performance, while semantic reconstruction deteriorated towards the end. For word puzzles, no clear trends were observed, but a very slight performance increase with temperature was noted, likely due to their alignment with lateral thinking, focusing more on letter composition rather than semantics.",
            "ablation_id": "2403.00809v1.No2"
        }
    ]
}