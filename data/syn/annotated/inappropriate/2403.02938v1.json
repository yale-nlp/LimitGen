{
    "title": "AIx Speed: Playback Speed Optimization Using Listening Comprehension of Speech Recognition Models",
    "abstract": "Since humans can listen to audio and watch videos at faster speeds than actually observed, we often listen to or watch these pieces of content at higher playback speeds to increase the time efficiency of content comprehension.\nTo further utilize this capability, systems that automatically adjust the playback speed according to the user\u2019s condition and the type of content to assist in more efficient comprehension of time-series content have been developed.\nHowever, there is still room for these systems to further extend human speed-listening ability by generating speech with playback speed optimized for even finer time units and providing it to humans.\nIn this study, we determine whether humans can hear the optimized speech and propose a system that automatically adjusts playback speed at units as small as phonemes while ensuring speech intelligibility.\nThe system uses the speech recognizer score as a proxy for how well a human can hear a certain unit of speech and maximizes the speech playback speed to the extent that a human can hear.\nThis method can be used to produce fast but intelligible speech.\nIn the evaluation experiment, we compared the speech played back at a constant fast speed and the flexibly speed-up speech generated by the proposed method in a blind test and confirmed that the proposed method produced speech that was easier to listen to.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1. INTRODUCTION",
            "text": "With the widespread use of video distribution services, people are increasingly watching videos for various purposes, including information gathering, learning, and entertainment.\nHumans are capable of understanding naturally observed phenomena at rates faster than the original.\nTherefore, when watching video or listening to audio, we often increase the playback speed of the content to understand more content in a shorter amount of time.\nExisting research has shown that when watching videos for learning, under certain conditions, differences in video playback speed do not affect learning effectiveness and may even improve performance (Nagahama and\nMorita, 2017  ###reference_b23###; Lang\net al., 2020  ###reference_b17###; Murphy et al., 2022  ###reference_b22###).\nIt has also been reported that 30% to 80% of users prefer to watch dramas at high speed, although this varies from country to country (Duan and Chen, 2019  ###reference_b7###).\nThere are many advantages to listening to video and audio at high speeds.\nAs a result, many methods have been proposed to automatically adjust the playback speed according to the structure of the content and the condition of the user, to further enhance the human ability to listen at high speeds.\nThey are widely studied as video summarization (Apostolidis et al., 2021  ###reference_b2###) and audio summarization (Vartakavi\net al., 2021  ###reference_b30###).\nThere are two basic strategies to these methods.\nThe first is to scan the user\u2019s intentions and behavior and leave only the parts that need to be viewed or adjust the playback speed in proportion to the user\u2019s concentration level (Kurihara, 2011  ###reference_b16###; Kawamura et al., 2014  ###reference_b14###).\nThe second is to speed up unnecessary parts of the content (i.e., parts that do not contain speech) or slow down parts that contain speech (Kayukawa et al., 2018  ###reference_b15###; Higuchi\net al., 2017  ###reference_b10###; Song\net al., 2015  ###reference_b27###; Zhang\net al., 2020  ###reference_b33###).\nHowever, these methods do not explicitly model whether the resulting speech at different playback speeds is intelligible to humans, so it is unclear whether a wide range of speech types can be made intelligible to users.\nIn addition, these systems vary the playback speed for each large chunk of speech, which leaves room for adjustment regarding playback speed for smaller units of time.\nTherefore, we propose AIx Speed, a system that adjusts audiovisual output speed while maintaining intelligibility by measuring speech intelligibility after playback speed is increased.\nAs shown in Fig. 1  ###reference_###, this system flexibly optimizes the playback speed in a video at the phoneme level.\nBy utilizing the listening ability of a neural network\u2013based speech recognition model, which is said to rival human performance (Xiong et al., 2016  ###reference_b32###), the system simultaneously maximizes the video playback speed and the speech recognition rate after changing the playback speed.\nIn this paper, the validity of using speech recognizers as a proxy for evaluating human listening performance was tested through the correlation of changes in human and speech listening performance when speed is increased.\nWe also whether speech where the playback speed is controlled at the phoneme level, as generated by the proposed method, or speech that is played at a constant speed and a high rate is easier for humans to listen to.\nThe results showed that the utterances generated by the proposed method were easier for humans to understand.\nFurthermore, the experimental results confirmed that the speech of non-native speakers can be transformed into speech that is easier to understand for native speakers by speeding up the speech with AIx Speed.\nIn summary, the proposed method not only supports the improvement of human speed\u2013listening ability, but also improves the intelligibility of speech by generating speech with adjustable playback speeds that consider the balance between playback speed and speech intelligibility.\nThe contributions of this paper are summarized as follows.\nDemonstrates that speech recognizers can be a substitute for human listening performance assessment.\nProposes a method to increase playback speed while maintaining speech intelligibility at the phoneme level.\nImproves the intelligibility of speech for non-native speakers by optimizing speech rate at the phoneme level.\n###figure_1###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2. RELATED WORK",
            "text": "There are two main methods for adjusting video playback speed to improve the time efficiency of video viewing.\nThe first is to remove unnecessary portions by focusing on the content, and the second is to retain the necessary portions based on user interaction.\nThe former removes portions that do not have audio, which is accomplished using systems such as CinemaGazer (Kurihara, 2011  ###reference_b16###), or portions of sports games that are not highlights of the game (Kawamura et al., 2014  ###reference_b14###).\nThe latter has been studied extensively, especially in the field of human\u2013computer interaction (HCI), and adjusts the playback speed based on the user\u2019s behavior.\nFor example, SmartPlayer (Cheng\net al., 2009  ###reference_b5###) learns the optimal playback speed based on a user\u2019s past viewing history.\nThere are also technologies that allow a user to make a rough selection in advance of what AIx Speed considers important and then fast-forward the rest of the video (Kayukawa et al., 2018  ###reference_b15###; Higuchi\net al., 2017  ###reference_b10###).\nOthers monitor the user\u2019s movements and adjust the playback speed according to the user\u2019s level of concentration (Song\net al., 2015  ###reference_b27###) and comprehension (Zhang\net al., 2020  ###reference_b33###; Nishida\net al., 2022  ###reference_b24###).\nThese technologies have the advantage of tracking the optimal playback speed for each user, but they cannot reflect important factors such as the intelligibility of the conversation in the video and its changes in the playback speed.\nThese methods do not explicitly model whether the resulting speech is intelligible to humans when the playback speed is varied.\nTherefore, speeding up the playback of various types of audio and video while making the audio understandable to the user is still an open problem.\nIn addition, these systems vary the playback speed for large chunks of speech, which leaves room for adjustment regarding the playback speed for smaller units of time.\nIn this respect, the proposed system can adjust the playback speed in finer phoneme units and can also generate speech that is easier for the user to understand.\nThe differences between the proposed system and the existing systems are shown in Fig. 2  ###reference_###.\n###figure_2### ###figure_3### ###figure_4### ###figure_5###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3. Pilot Survey",
            "text": "The purpose of this study is to automate the maximization of speed to the extent that speech is understandable.\nTo this end, we hypothesize that as the speed increases, the recognition performance of both humans and speech recognizers will decrease in the same way.\nIf this hypothesis is correct, we can evaluate how well a human can hear when playback speed is increased using a speech recognizer instead of a human.\nSeveral attempts have been made to evaluate human hearing with speech recognizers in this way. For example, it has been shown that the results of human mean opinion score (MOS) listening tests correlate with the results of the speech recognition-based MOS estimation method introduced in (Jiang and\nSchulzrinne, 2002  ###reference_b12###), and in (Fontan\net al., 2017  ###reference_b8###), the understanding and comprehension scores of a listener with simulated age-related hearing loss were highly correlated speech recognition-based system.\nA similar hypothesis has been used in speech learning support research to evaluate a learner\u2019s speech ability based on speech recognition performance (Tejedor-Garc\u00c3\u00ada et al., 2021  ###reference_b28###).\nIn other words, if a speech recognizer can recognize speech, it judges that the person speaks well.\nHowever, the relationship between the machine learning model and the ability to understand human speech when the playback speed is varied, which is the focus of this study, has not been evaluated. Therefore, we first investigated whether this hypothesis is true.\nThis study compared human listening performance and speech recognition performance for speech at 0.25, 0.5, 0.75, 1.0, 1.25, 1.5, 1.75, and 2.0x playback speeds.\nTo measure human listening performance, speech data of English sentences were prepared at each playback speed, and the subjects were asked to transcribe the data.\nThe target English sentences were selected from LibriSpeech (Panayotov et al., 2015  ###reference_b25###), a large English speech corpus created for the development and evaluation of automatic speech recognition systems.\nThis corpus contains over 1,000 hours of audiobook readings and transcriptions.\nThe participants were 140 English speakers who lived in the United States and had graduated from a US high school.\nAll participants were recruited from the Amazon Mechanical Turk and were compensated for their time.\nEach subject was given 15 English sentences that had been randomly sped up by 0.5, 0.75, 1.0, 1.25, 1.5, 1.75, or 2.0x and asked to transcribe them.\nThe speech recognizer transcription data was collected by inputting 15 English sentences at each playback speed into a Wav2Vec2-based speech recognition model, similar to the human performance evaluation (Baevski\net al., 2020  ###reference_b3###).\nFigure 3  ###reference_### shows a graph of the change in listening performance of the human and machine learning models when the playback speed was changed.\nIn these figures, the horizontal axis is the playback speed, and the vertical axis is the recognition performance.\nRecognition performance was evaluated using character error rate (CER) and word error rate (WER), which are commonly used in speech recognition (the lower these values are, the better).\nThe WER was calculated as follows:\nand CER was calculated as\nFor both the human and machine learning models, listening performance decreased as playback speed increased from 1.0x.\nFor playback speeds greater than 1.0x, the correlation coefficient between the change in listening performance for the human and machine learning models was 0.9977.\nOn the other hand, when the playback speed was slowed down from 1.0x, the machine learning model showed a decrease in recognition performance, but the decline barely observable for humans.\nIn particular, while the performance of WER decreased slowly, the performance of CER showed almost no decrease.\nThis indicates that human recognition performance on a character-by-character basis does not drop nearly as much when listening to slowed speech.\nThus, when the playback speed increased, the recognition performance of the human and machine learning models decreased similarly, but they exhibited different behavior when the playback speed was decreased.\nHowever, since this study focuses on increasing the playback speed of speech, the difference in behavior when the playback speed is slower than 1.0x has no effect.\nTherefore, by taking advantage of the fact that listening performance decreases as playback speed is increased for humans and speech recognition models alike, we replaced human listening performance with speech recognition performance to develop the desired system."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4. AIx Speed",
            "text": "AIx Speed increases the speed as much as possible, as long as the user can understand it.\nThis system allows users to watch videos in a time-efficient manner without having to adjust the playback speed for each video.\nIn addition, the system can automatically improve intelligibility by adjusting the speech speed to accommodate non-native speakers who are not proficient in the target language.\nThe working process of AIx Speed is illustrated in Fig. 4  ###reference_###.\nThe system first extracts the human voice from the target video.\nNext, it splits this voice into specified equal intervals.\nNext, using each segmented voice as input, the system calculates the optimal playback speed for each segment voice, taking into account the characteristics of the voice as a whole.\nFinally, the system changes each voice to the specified playback speed and combines them into a single voice.\nAt the same time, the combined voice is recognized by speech recognition to confirm that the resulting single voice is understandable.\nThis system consists of two mechanisms, as shown in Fig. 5  ###reference_###.\nOne is a playback speed adjuster (left), and the other is a speech recognizer (right).\nThe former is used to maximize the playback speed of the input speech, while the latter is used to evaluate how understandable the input speech is.\nBy training these two models simultaneously, it is possible to generate speech that plays back as fast as possible within the comprehension range.\nThe following subsections describe these two key features.\n###figure_6### ###figure_7### ###figure_8###"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1. Playback speed adjuster",
            "text": "In this study, we use a simple linear interpolation method for adjusting playback speed that does not consider intelligibility to optimize the playback speed. The playback speed controller is divided into a feature extractor layer and a linear layer, as shown in Fig. 5  ###reference_### (left). They are trained by pre-training through self-supervised representation learning on unlabeled speech data and regression learning, which outputs the playback speed based on the features after the representation learning. The pre-training method for the feature extractor layer is similar to masked language modeling, as exemplified by bidirectional encoder representations from transformers (BERT) (Devlin et al., 2019  ###reference_b6###) in natural language processing, where a portion of the input is masked and the corresponding utterance features are estimated from the remaining input. In this way, the model can learn good-quality features of the target language to which the rate of utterance should be adapted. Typically, these self-supervised learners are used to tackle tasks such as speech recognition and speaker identification by pre-training and then fine-tuning with a small amount of label data. For example, in speech recognition, we have added a projection layer and a connectionist temporal classification (CTC) layer (Graves et al., 2006  ###reference_b9###) to the output of self-supervised learners, such as Wav2Vec2 and HuBERT (Hsu et al., 2021  ###reference_b11###), to enable transcription from speech waveforms. Similar to these methods, we pre-train on unlabeled speech data and then connect and train a linear layer that outputs rates. In general speech processing tasks, such as speech recognition (Malik et al., 2021  ###reference_b19###) and speaker identification (Bai and Zhang, 2021  ###reference_b4###; Kabir et al., 2021  ###reference_b13###), the difference from the class label is minimized as an error function. On the other hand, the goal of the playback speed adjuster is to maximize the speed. Therefore, we designed the following function whose value decreases as the speed increases. where  is the playback speed for each segment obtained using a simple linear interpolation method and a linear layer with audio as input as follows:"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2. Speech recognizer",
            "text": "The speech recognizer transcribes the speech converted to the playback speed obtained by the playback speed adjuster (Fig. 5  ###reference_### (right)).\nWhen speech parts with different playback speeds are combined, noise is generated in the speech data and the sound quality is degraded.\nWe use voice separation technology (McFee et al., 2015  ###reference_b20###) to extract only the speaker\u2019s voice and reduce the effect of noise.\nThe resulting speech is then fed into a speech recognizer for speech recognition. The speech recognition process consists of the extraction of acoustic features from the speech waveform, estimation of the classes of acoustic features for each frame, and generation of hypotheses from the sequence of class probabilities.\nSince the speech recognizer can partially share the neural network with the playback speed adjuster, the overall network size can be reduced.\nAs shown in Fig. 5  ###reference_###, the dotted speech feature extractor is shared.\nThe speech features obtained by this mechanism are used as input to generate text in the projection layer.\nIn this process, the speech recognizer is trained to minimize CTC loss, as in normal speech recognition.\nIn summary, the entire model is trained to minimize the following error function, which is a combination of this error function and the error function of the playback speed adjuster.\nHere,  is a hyperparameter that adjusts the importance of the playback speed calculation and speech recognition."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5. Prototype",
            "text": "As a prototype of AIx Speed, an application that optimizes audio playback speed using English as the target language has been implemented.\nThis section describes the implementation and usage of the application."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "5.1. Implementation",
            "text": "Wav2Vec2 was used for the prototype\u2019s shared utterance learning model (the utterance learning part shared by the speech recognizer and the playback speed adjuster).\nFor pre-training, LibriSpeech (Panayotov et al., 2015  ###reference_b25###) was used as the dataset, train-clean-360 as the training data, and dev-clean as the validation data.\nThe dataset consist of clean speech data in LibriSpeech and were partitioned using the data partitioning method proposed in LibriSpeech for training/validation.\nThe pre-training did not require the corresponding transcribed text, only the speech data.\nWe then trained a speech recognizer and a playback speed adjuster using two sets of speech data, including the transcribed text.\nOne was LibriSpeech\u2019s train-clean-100, and the other was the English speech database read by Japanese students (UME-ERJ) (Minematsu et al., 2002  ###reference_b21###).\nThe latter is a dataset of English spoken by non-native Japanese speakers, with 202 speakers (100 males and 102 females) reading simple English sentences.\nThe playback speed adjuster and the speech recognizer were trained separately.\nFirst, the speech recognizer was trained using LibriSpeech and UME-ERJ, and then the playback speed adjuster was trained using the same data with fixed weights for the speech recognizer.\nAll speech files used for training were normalized to a sampling frequency of 16 kHz.\nThe Wav2Vec2 used the initial parameters implemented in PyTorch (Paszke\net al., 2019  ###reference_b26###), and the final layers of both the playback rate adjuster and the speech recognizer were -dimensional linear layers.\nThe hyperparameter  of the error function was set to , and training was performed for 5 epochs with a batch size of  using the AdamW (Loshchilov and\nHutter, 2019  ###reference_b18###) optimization algorithm."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "5.2. Usage of the application",
            "text": "Examples of using AIx Speed are shown in Fig. 6  ###reference_###.\nThis is an example of the prototype applied to a video uploaded to YouTube.\nThe horizontal axis is the playback time, and the vertical axis represents the playback speed output by the model for each playback time. The first is an example of speeding up a dialogue, movie, or lecture.\nTwo speakers appear in the video, and the optimal playback speed can be set for each speaker. Of particular interest is that the two speakers in the video speak at different speeds, so the average playback speed for the two speakers is different.\nIt can also be seen that the playback speed increases drastically from the moment when the dialog between the two speakers ends and there is no more speech from the person.\nAlthough we did not intend to design this feature, we can see that our system, like conventional playback speed controllers, can speed up the playback speed in the parts where there is no speech.\nThe second example is speeding up the speech of non-native speakers to make it easier to understand.\nSince the speech of non-native speakers is often slower than that of native speakers, moderately speeding up the speech makes it easier to understand.\nThe change in playback speed shows that the overall playback speed is faster than the speech between native speakers in the first video. This indicates that the model can speed up the speech more because the non-native speakers\u2019 speech is slower than that of the native speakers."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6. Evaluation",
            "text": ""
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "6.1. Technical evaluation",
            "text": "To demonstrate that the proposed method can optimize playback speed while maintaining content understanding, we compared the CER and WER values at the AIx Speed\u2013modified speech playback speed to those at a constant playback speed.\nWe compared the CER and WER with the speech playback speed modified by the AIx Speed to the CER and WER when the speech was simply played at a constant speed.\nThe speeds of the comparison targets were 1.0x, 1.5x and the average speed times the playback speed of AIx Speed.\nA standard Wav2Vec2 based speech recognition model, which was the speech recognizer used in our method, was used to compute the CER and WER for comparison.\nThe performance of the models is shown in Table. 1  ###reference_### (a) and 1  ###reference_### (b) for LibriSpeech and UME-ERJ, respectively.\nAIx Speed produces speech 1.30 times faster on average for LibriSpeech and 1.29 times faster on average for UME-ERJ.\nBoth results show that the playback speed optimized by AIx Speed has lower values for both CER and WER than the average constant speech speed at that playback speed.\nFrom these results, it can be said that the proposed model maximizes the playback speed while guaranteeing the recognition performance.\nIn addition, for UME-ERJ, the speech generated at AIx Speed shows better recognition performance in terms of WER than at 1.0x playback speed.\nTherefore, it is also suggested that the proposed method can be used to convert the speech of non-native speakers into more understandable speech."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "6.2. User evaluation",
            "text": "User experiments were conducted to confirm that the generated speech was understandable.\nThe quality of the speech generated by the proposed method was compared with that of the speech played at a constant speed, at the average playback speed of the speech.\nThe quality was evaluated using the mean opinion score, which is commonly used in speech synthesis research (van den Oord\net al., 2016  ###reference_b29###; Wang\net al., 2017  ###reference_b31###).\nThis measure rates speech quality on a five-point scale from 1 (poor) to 5 (excellent).\nParticipants were 50 US residents who used English on a daily basis.\n20 sentences were extracted from each of the LibriSpeech and UME-ERJ datasets, and half were converted to speech with the proposed speed-up, while the other half were converted to speech with a constant speed-up.\nParticipants were given a total of 40 sentences of audio and assisted to rate the quality of the audio.\nFigure 7  ###reference_### shows the quality of the generated speech at baseline and AIx Speed, LThe quality of LibreSpeech and UME-ERJ were 0.5 and 0.8 points higher at speeds generated by the proposed method, respectively.\n###figure_9###"
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7. Discussion",
            "text": ""
        },
        {
            "section_id": "7.1",
            "parent_section_id": "7",
            "section_name": "7.1. Evaluation results",
            "text": "The technical evaluation shows that the proposed method can produce speech that is easier to understand than that produced by simply increasing the playback speed in terms of speech recognition performance.\nThe user evaluation also shows that the proposed method can produce speech that is easier to understand for real users. These results show that the proposed method can produce speech with a high playback speed within a range that is easy for users to understand.\nThis allows users to watch videos at a reasonable speed without having to adjust the playback speed for each video.\nHowever, the improvement in MOS values by using the proposed method is by no means sufficient.\nIn the current model, the average conversion to a faster playback speed is about 1.3 times, but it is a future task to investigate whether it is possible to make this even faster.\nIn fact, many video playback services implement 1.5x and 2.0x playback speeds, and some people watch dramas and lectures at such speeds. Therefore, we expect that it will be possible to convert up to this speed and make the audio easy to understand.\nIn addition, since each user has a different preferred playback speed, personalizing the model so that it plays at the optimal playback speed for users is also a future issue."
        },
        {
            "section_id": "7.2",
            "parent_section_id": "7",
            "section_name": "7.2. Listening comprehension and speech recognition",
            "text": "As discussed in the preliminary research chapter, it can be seen that there is a relationship between speech recognition performance and human transcription ability.\nThus, we expect to build automated systems for various tasks and evaluations by replacing human speech comprehension ability with machine learning models, as in this system.\nOn the other hand, speech recognition performance based on playback speed does not perfectly match human speech comprehension.\nIn other words, as the playback speed increases, the dictation performance decreases in both cases, but the performance values are not exactly the same.\nThus, we anticipate that by training speech recognition models to match these relationships as closely as possible, it will be possible to use them more generally as alternatives to humans. Distillation, a technique that learns to approximate an output that matches existing results, will be the technical key."
        },
        {
            "section_id": "7.3",
            "parent_section_id": "7",
            "section_name": "7.3. Adjustment of non-native speakers\u2019 speech",
            "text": "Several suggestions can be made as to how increasing the playback speed by the proposed system improves the intelligibility of speech for non-native speakers. One of them is that when non-native speakers read English manuscripts, they may find it easier to understand if they speak naturally (slower from a native speaker\u2019s point of view) and then artificially speed up their speech, rather than forcing them to speak quickly like a native speaker. In fact, this study also began with the realization that it is easier to listen to a video of a non-native speaker speaking his or her native language when it is played at a high speed."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "8. Conclusion",
            "text": "This paper presents a system that applies a speech recognition model to automatically and flexibly adjust the playback speed of video and audio within the range of human comprehension.\nBy using this system, users can consume audiovisual content at optimal speeds without having to manually adjust the playback speed.\nExperiments have also confirmed that the system makes it easier for users to understand the speech of non-native speakers.\nIn the future, we expect the system to be used in a variety of applications, such as video distribution services and language learning tools."
        }
    ],
    "appendix": [],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T1\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S5.T1.2.1.1\" style=\"font-size:90%;\">Table 1</span>. </span><span class=\"ltx_text\" id=\"S5.T1.3.2\" style=\"font-size:90%;\">Performance comparison of models</span></figcaption>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S5.T1.4.1.1\" style=\"font-size:90%;\">(a)</span> </span><span class=\"ltx_text\" id=\"S5.T1.5.2\" style=\"font-size:90%;\">LibriSpeech</span></figcaption>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S5.T1.7.1.1\" style=\"font-size:90%;\">(b)</span> </span><span class=\"ltx_text\" id=\"S5.T1.8.2\" style=\"font-size:90%;\">UME-ERJ</span></figcaption><div class=\"ltx_flex_figure ltx_flex_table\">\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_flex_size_2 ltx_align_middle\" id=\"S5.T1.6\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T1.6.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T1.6.1.1.1\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T1.6.1.1.2\">Avg. Speed</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T1.6.1.1.3\">CER</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T1.6.1.1.4\">WER</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T1.6.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.6.2.1.1\">Wav2Vec2 (1.00x)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.6.2.1.2\">1.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.6.2.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.6.2.1.3.1\">4.38</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.6.2.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.6.2.1.4.1\">12.57</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.6.3.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.6.3.2.1\">Wav2Vec2 (1.30x)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.6.3.2.2\">1.30</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.6.3.2.3\">5.61</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.6.3.2.4\">14.58</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.6.4.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.6.4.3.1\">Wav2Vec2 (1.50x)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.6.4.3.2\">1.50</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.6.4.3.3\">6.83</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.6.4.3.4\">17.19</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.6.5.4\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.6.5.4.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.6.5.4.1.1\">AIx Speed</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.6.5.4.2\">1.30</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.6.5.4.3\"><span class=\"ltx_text ltx_framed_underline\" id=\"S5.T1.6.5.4.3.1\">5.21</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.6.5.4.4\"><span class=\"ltx_text ltx_framed_underline\" id=\"S5.T1.6.5.4.4.1\">12.96</span></td>\n</tr>\n</tbody>\n</table>\n</div>\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_flex_size_2 ltx_align_middle\" id=\"S5.T1.9\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T1.9.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T1.9.1.1.1\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T1.9.1.1.2\">Avg. Speed</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T1.9.1.1.3\">CER</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T1.9.1.1.4\">WER</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T1.9.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.9.2.1.1\">Wav2Vec2 (1.00x)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.9.2.1.2\">1.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.9.2.1.3\"><span class=\"ltx_text ltx_framed_underline\" id=\"S5.T1.9.2.1.3.1\">26.74</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.9.2.1.4\"><span class=\"ltx_text ltx_framed_underline\" id=\"S5.T1.9.2.1.4.1\">55.02</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.9.3.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.9.3.2.1\">Wav2Vec2 (1.29x)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.9.3.2.2\">1.29</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.9.3.2.3\">33.71</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.9.3.2.4\">63.90</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.9.4.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.9.4.3.1\">Wav2Vec2 (1.50x)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.9.4.3.2\">1.50</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.9.4.3.3\">36.93</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.9.4.3.4\">66.51</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.9.5.4\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.9.5.4.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.9.5.4.1.1\">AIx Speed</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.9.5.4.2\">1.29</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.9.5.4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.9.5.4.3.1\">26.45</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.9.5.4.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.9.5.4.4.1\">53.13</span></td>\n</tr>\n</tbody>\n</table>\n</div>\n</div>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S5.T1.7.1.1\" style=\"font-size:90%;\">(b)</span> </span><span class=\"ltx_text\" id=\"S5.T1.8.2\" style=\"font-size:90%;\">UME-ERJ</span></figcaption>\n</figure>",
            "capture": "Table 1. Performance comparison of models"
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.02938v1_figure_1.png",
            "caption": "Figure 1. AIx Speed optimizes the playback speed of a video in units as small as phonemes. The system maximizes the speed of audio to the extent that humans can hear it. Users can watch videos at a comfortable speed without having to adjust the playback speed."
        },
        "2": {
            "figure_path": "2403.02938v1_figure_2.png",
            "caption": "Figure 2. Differences between existing methods and ours. Existing methods speed up the playback speed of silent or unimportant parts of the audio source and slow down the playback speed of important parts of the audio source or when the user is not concentrating on them. Our method, on the other hand, adjusts the playback speed of the sound source at finer intervals according to the audibility of the speaker\u2019s speech in the sound source."
        },
        "3": {
            "figure_path": "2403.02938v1_figure_3.png",
            "caption": "(a) CER of ML models"
        },
        "4": {
            "figure_path": "2403.02938v1_figure_4.png",
            "caption": "(a) CER of ML models"
        },
        "5": {
            "figure_path": "2403.02938v1_figure_5.png",
            "caption": "(a) CER of ML models"
        },
        "6": {
            "figure_path": "2403.02938v1_figure_6.png",
            "caption": "(a) CER of ML models"
        },
        "7": {
            "figure_path": "2403.02938v1_figure_7.png",
            "caption": "Figure 4. The work process of AIx Speed: 1\u20dd Extract human voices from the target video. 2\u20dd Divide the voices into specified equal intervals. 3\u20dd Calculate the optimal playback speed for each divided voice. 4\u20dd Change each voice to the fixed playback speed and synthesize it into one voice. 5\u20dd Perform speech recognition on the synthesized voice to confirm that the resulting voice is understandable."
        },
        "8": {
            "figure_path": "2403.02938v1_figure_8.png",
            "caption": "Figure 5. AIx Speed architecture: Our system simultaneously optimizes the playback speed regulator (left) and the speech recognizer (right). By doing so, we can maximize the playback speed to the extent that the model can recognize."
        },
        "9": {
            "figure_path": "2403.02938v1_figure_9.png",
            "caption": "Figure 6. Application example. This graph shows how the playback speed changed when AIx Speed was applied to two videos on YouTube. As you can see, the playback speed changes flexibly according to the speaker\u2019s speech."
        },
        "10": {
            "figure_path": "2403.02938v1_figure_10.png",
            "caption": "Figure 7. Voice quality at baseline (constant playback speed increase throughout) and AIx Speed (flexible playback speed increase)"
        }
    },
    "references": [
        {
            "1": {
                "title": "Video summarization using deep neural networks: A\nsurvey.",
                "author": "Evlampios Apostolidis,\nEleni Adamantidou, Alexandros I Metsai,\nVasileios Mezaris, and Ioannis Patras.\n2021.",
                "venue": "Proc. IEEE 109,\n11 (2021), 1838\u20131863.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Wav2vec 2.0: A Framework for Self-Supervised\nLearning of Speech Representations. In Advances in\nNeural Information Processing Systems.",
                "author": "Alexei Baevski, Yuhao\nZhou, Abdelrahman Mohamed, and Michael\nAuli. 2020.",
                "venue": "",
                "url": null
            }
        },
        {
            "3": {
                "title": "Speaker recognition based on deep learning: An\noverview.",
                "author": "Zx Bai and Xiao-Lei\nZhang. 2021.",
                "venue": "Neural Networks (2021).",
                "url": null
            }
        },
        {
            "4": {
                "title": "SmartPlayer: User-Centric Video Fast-Forwarding.\nIn Proc. of the SIGCHI Conference on Human Factors\nin Computing Systems.",
                "author": "Kai-Yin Cheng, Sheng-Jie\nLuo, Bing-Yu Chen, and Hao-Hua Chu.\n2009.",
                "venue": "",
                "url": null
            }
        },
        {
            "5": {
                "title": "BERT: Pre-training of Deep Bidirectional\nTransformers for Language Understanding. In\nProceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Papers).\nAssociation for Computational Linguistics,\nMinneapolis, Minnesota, 4171\u20134186.",
                "author": "Jacob Devlin, Ming-Wei\nChang, Kenton Lee, and Kristina\nToutanova. 2019.",
                "venue": "",
                "url": null
            }
        },
        {
            "6": {
                "title": "Why College Students Watch Streaming Drama at\nHigher Playback Speed: The Uses and Gratifications Perspective. In\nInternational Joint Conference on Information,\nMedia and Engineering.",
                "author": "Songshuang Duan and\nXiaoqian Chen. 2019.",
                "venue": "",
                "url": null
            }
        },
        {
            "7": {
                "title": "Automatic speech recognition predicts speech\nintelligibility and comprehension for listeners with simulated age-related\nhearing loss.",
                "author": "Lionel Fontan, Isabelle\nFerran\u00e9, J\u00e9r\u00f4me Farinas,\nJulien Pinquier, Julien Tardieu,\nCynthia Magnen, Pascal Gaillard,\nXavier Aumont, and Christian\nF\u00fcllgrabe. 2017.",
                "venue": "Journal of Speech, Language, and Hearing\nResearch 60, 9 (2017),\n2394\u20132405.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Connectionist Temporal Classification: Labelling\nUnsegmented Sequence Data with Recurrent Neural Nets. In\nICML \u201906: Proceedings of the International\nConference on Machine Learning.",
                "author": "A. Graves, S. Fernandez,\nF. Gomez, and J. Schmidhuber.\n2006.",
                "venue": "",
                "url": null
            }
        },
        {
            "9": {
                "title": "EgoScanning: Quickly Scanning First-Person Videos\nwith Egocentric Elastic Timelines. In Proc. ACM\nConference on Human Factors in Computing Systems.",
                "author": "Keita Higuchi, Ryo\nYonetani, and Yoichi Sato.\n2017.",
                "venue": "",
                "url": null
            }
        },
        {
            "10": {
                "title": "HuBERT: Self-Supervised Speech Representation\nLearning by Masked Prediction of Hidden Units.",
                "author": "Wei-Ning Hsu, Benjamin\nBolte, Yao-Hung Hubert Tsai, Kushal\nLakhotia, Ruslan Salakhutdinov, and\nAbdelrahman Mohamed. 2021.",
                "venue": "(2021).",
                "url": null
            }
        },
        {
            "11": {
                "title": "Speech recognition performance as an effective\nperceived quality predictor. In IEEE 2002 Tenth\nIEEE International Workshop on Quality of Service (Cat. No.02EX564).\n269\u2013275.",
                "author": "Wenyu Jiang and H.\nSchulzrinne. 2002.",
                "venue": "",
                "url": null
            }
        },
        {
            "12": {
                "title": "A Survey of Speaker Recognition: Fundamental\nTheories, Recognition Methods and Opportunities.",
                "author": "Muhammad Mohsin Kabir,\nMuhammad Firoz Mridha, Jungpil Shin,\nIsrat Jahan, and Abu Quwsar Ohi.\n2021.",
                "venue": "IEEE Access 9\n(2021), 79236\u201379263.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Efficient Video Viewing System for Racquet Sports\nwith Automatic Summarization Focusing on Rally Scenes. In\nACM SIGGRAPH 2014 Posters.",
                "author": "Shunya Kawamura, Tsukasa\nFukusato, Tatsunori Hirai, and Shigeo\nMorishima. 2014.",
                "venue": "",
                "url": null
            }
        },
        {
            "14": {
                "title": "Dynamic Object Scanning: Object-Based Elastic\nTimeline for Quickly Browsing First-Person Videos. In\nExtended Abstracts of the 2018 CHI Conference on\nHuman Factors in Computing Systems.",
                "author": "Seita Kayukawa, Keita\nHiguchi, Ryo Yonetani, Masanori\nNakamura, Yoichi Sato, and Shigeo\nMorishima. 2018.",
                "venue": "",
                "url": null
            }
        },
        {
            "15": {
                "title": "CinemaGazer: A System for Watching Video at Very\nHigh Speed. In Proc. of the Workshop on Advanced\nVisual Interfaces AVI.",
                "author": "Kazutaka Kurihara.\n2011.",
                "venue": "",
                "url": null
            }
        },
        {
            "16": {
                "title": "Is Faster Better? A Study of Video Playback Speed.\nIn Proc. of the Tenth International Conference on\nLearning Analytics & Knowledge.",
                "author": "David Lang, Guanling\nChen, Kathy Mirzaei, and Andreas\nPaepcke. 2020.",
                "venue": "",
                "url": null
            }
        },
        {
            "17": {
                "title": "Decoupled Weight Decay Regularization. In\nInternational Conference on Learning\nRepresentations.",
                "author": "Ilya Loshchilov and\nFrank Hutter. 2019.",
                "venue": "",
                "url": null
            }
        },
        {
            "18": {
                "title": "Automatic speech recognition: A survey.",
                "author": "Mishaim Malik, Muhammad\nMalik, Khawar Mehmood, and Imran\nMakhdoom. 2021.",
                "venue": "Multimedia Tools and Applications\n(2021).",
                "url": null
            }
        },
        {
            "19": {
                "title": "Librosa: Audio and music signal analysis in\npython. In Proceedings of the 14th python in\nscience conference. Citeseer.",
                "author": "Brian McFee, Colin\nRaffel, Dawen Liang, Daniel P Ellis,\nMatt McVicar, Eric Battenberg, and\nOriol Nieto. 2015.",
                "venue": "",
                "url": null
            }
        },
        {
            "20": {
                "title": "English Speech Database Read by Japanese Learners\nfor CALL System Development.. In LREC. Citeseer.",
                "author": "Nobuaki Minematsu,\nYoshihiro Tomiyama, Kei Yoshimoto,\nKatsumasa Shimizu, Seiichi Nakagawa,\nMasatake Dantsuji, and Shozo Makino.\n2002.",
                "venue": "",
                "url": null
            }
        },
        {
            "21": {
                "title": "Learning in double time: The effect of lecture\nvideo speed on immediate and delayed comprehension.",
                "author": "Dillon H. Murphy, Kara M.\nHoover, Karina Agadzhanyan, Jesse C.\nKuehn, and Alan D. Castel.\n2022.",
                "venue": "Applied Cognitive Psychology\n36, 1 (2022),\n69\u201382.",
                "url": null
            }
        },
        {
            "22": {
                "title": "Effect Analysis of Playback Speed for Lecture Video\nIncluding Instructor Images.",
                "author": "Toru Nagahama and Yusuke\nMorita. 2017.",
                "venue": "International Journal for Educational Media\nand Technology (2017).",
                "url": null
            }
        },
        {
            "23": {
                "title": "Laugh at Your Own Pace: Basic Performance\nEvaluation of Language Learning Assistance by Adjustment of Video Playback\nSpeeds Based on Laughter Detection. In Proc. of\nthe Ninth ACM Conference on Learning @ Scale.",
                "author": "Naoto Nishida, Hinako\nNozaki, and Buntarou Shizuki.\n2022.",
                "venue": "",
                "url": null
            }
        },
        {
            "24": {
                "title": "Librispeech: An ASR corpus based on public domain\naudio books. In IEEE International Conference on\nAcoustics, Speech and Signal Processing.",
                "author": "Vassil Panayotov, Guoguo\nChen, Daniel Povey, and Sanjeev\nKhudanpur. 2015.",
                "venue": "",
                "url": null
            }
        },
        {
            "25": {
                "title": "PyTorch: An Imperative Style, High-Performance Deep\nLearning Library.",
                "author": "Adam Paszke, Sam Gross,\nFrancisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin,\nNatalia Gimelshein, Luca Antiga,\nAlban Desmaison, Andreas Kopf,\nEdward Yang, Zachary DeVito,\nMartin Raison, Alykhan Tejani,\nSasank Chilamkurthy, Benoit Steiner,\nLu Fang, Junjie Bai, and\nSoumith Chintala. 2019.",
                "venue": "In Advances in Neural Information\nProcessing Systems 32.",
                "url": null
            }
        },
        {
            "26": {
                "title": "Automatically Adjusting the Speed of E-Learning\nVideos. In Proc. of the 33rd Annual ACM Conference\nExtended Abstracts on Human Factors in Computing Systems.",
                "author": "Sunghyun Song, Jeong-ki\nHong, Ian Oakley, Jun Dong Cho, and\nAndrea Bianchi. 2015.",
                "venue": "",
                "url": null
            }
        },
        {
            "27": {
                "title": "Automatic Speech Recognition (ASR) Systems Applied\nto Pronunciation Assessment of L2 Spanish for Japanese Speakers.",
                "author": "Cristian Tejedor-Garc\u00c3\u00ada,\nValent\u00c3\u00adn Carde\u00c3\u00b1oso-Payo, and\nDavid Escudero-Mancebo. 2021.",
                "venue": "Applied Sciences 11,\n15 (2021).",
                "url": null
            }
        },
        {
            "28": {
                "title": "WaveNet: A Generative Model for Raw Audio. In\nArxiv.",
                "author": "A\u00c3\u00a4ron van den Oord,\nSander Dieleman, Heiga Zen,\nKaren Simonyan, Oriol Vinyals,\nAlexander Graves, Nal Kalchbrenner,\nAndrew Senior, and Koray Kavukcuoglu.\n2016.",
                "venue": "",
                "url": null
            }
        },
        {
            "29": {
                "title": "Audio Summarization for Podcasts. In\n2021 29th European Signal Processing Conference\n(EUSIPCO). IEEE, 431\u2013435.",
                "author": "Aneesh Vartakavi, Amanmeet\nGarg, and Zafar Rafii. 2021.",
                "venue": "",
                "url": null
            }
        },
        {
            "30": {
                "title": "Tacotron: Towards End-to-End Speech Synthesis. In\nINTERSPEECH. 4006\u20134010.",
                "author": "Yuxuan Wang, R. J.\nSkerry-Ryan, Daisy Stanton, Yonghui Wu,\nRon J. Weiss, Navdeep Jaitly,\nZongheng Yang, Ying Xiao,\nZhifeng Chen, Samy Bengio,\nQuoc V. Le, Yannis Agiomyrgiannakis,\nRob Clark, and Rif A. Saurous.\n2017.",
                "venue": "",
                "url": null
            }
        },
        {
            "31": {
                "title": "Achieving Human Parity in Conversational Speech\nRecognition.",
                "author": "Wayne Xiong, Jasha\nDroppo, Xuedong Huang, Frank Seide,\nMichael Seltzer, Andreas Stolcke,\nDong Yu, and Geoffrey Zweig.\n2016.",
                "venue": "IEEE/ACM Transactions on Audio, Speech, and\nLanguage Processing.",
                "url": null
            }
        },
        {
            "32": {
                "title": "WithYou: Automated Adaptive Speech Tutoring With\nContext-Dependent Speech Recognition. In Proc. of\nthe 2020 CHI Conference on Human Factors in Computing Systems.",
                "author": "Xinlei Zhang, Takashi\nMiyaki, and Jun Rekimoto.\n2020.",
                "venue": "",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.02938v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "4",
            "4.1",
            "4.2"
        ],
        "main_experiment_and_results_sections": [
            "6.1",
            "6.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "6.1",
            "6.2"
        ]
    },
    "research_context": {
        "paper_id": "2403.02938v1",
        "paper_title": "AIx Speed: Playback Speed Optimization Using Listening Comprehension of Speech Recognition Models",
        "research_background": "### Paper's Motivation\nThe paper is driven by the increasing prevalence of video distribution services and people's subsequent preference for consuming content at higher playback speeds to save time. It acknowledges the human capacity to comprehend information at a rate faster than the original and seeks to leverage this ability. Despite the benefits of fast playback, current methods that adjust playback speeds do not ensure that the altered speech remains intelligible to humans, leading to potential comprehension issues.\n\n### Research Problem\nThe main research problem addressed by the paper is optimizing playback speed to maximize content consumption efficiency while maintaining speech intelligibility. Specifically, the paper targets the shortcomings of existing methods that vary playback speeds of large chunks of speech content without considering intelligibility, proposing a solution that dynamically adjusts the speed at a more granular phoneme level.\n\n### Relevant Prior Work\n- **Learning and Playback Speed**: Past studies have demonstrated that differences in playback speed do not necessarily affect learning effectiveness and can even improve performance in specific contexts (Nagahama and Morita, 2017; Lang et al., 2020; Murphy et al., 2022).\n- **User Preferences**: Research indicates that a significant percentage of users prefer faster playback speeds for entertainment content, though preferences vary by country (Duan and Chen, 2019).\n- **Summarization Techniques**: Methods for automatically adjusting playback speeds have been explored in video and audio summarization domains (Apostolidis et al., 2021; Vartakavi et al., 2021).\n- **Playback Speed Adjustment Methods**: Two primary strategies are recognized:\n  - Scanning user intentions and adjusting playback based on concentration levels (Kurihara, 2011; Kawamura et al., 2014).\n  - Speeding up parts without speech and slowing down parts with speech (Kayukawa et al., 2018; Higuchi et al., 2017; Song et al., 2015; Zhang et al., 2020).\n  \nThe paper highlights the gap in current methods which fail to model the intelligibility of speech at various playback speeds and do not offer fine-grained adjustments necessary for optimal comprehension. \n\nBy proposing AIx Speed, the paper contributes a novel system that leverages neural network-based speech recognition to adjust playback speeds at the phoneme level, ensuring intelligibility while maximizing speed. The paper provides empirical evidence that this approach can improve human listening performance and intelligibility, particularly for non-native speakers.",
        "methodology": "**AIx Speed: Playback Speed Optimization Using Listening Comprehension of Speech Recognition Models**\n\n**Methodology:**\n\n**Overview:**\nAIx Speed is a system designed to optimize the playback speed of videos to maximize efficiency without compromising user comprehension. The core innovation here is the system's ability to adjust playback speeds dynamically, based on user understanding, which is particularly helpful for non-native speakers.\n\n**Key Components and Innovations:**\n\n1. **Automatic Speed Adjustment:**\n   - The system first extracts the human voice from the target video.\n   - The extracted voice is split into specified equal intervals.\n   - For each segmented voice interval, the system calculates the optimal playback speed, considering the voice's overall characteristics.\n   - Each segment is then adjusted to its calculated speed and recombined into a single audio track.\n   - This combined audio is processed by a speech recognition model to ensure comprehensibility.\n\n2. **Dual Mechanism System:**\n   - **Playback Speed Adjuster:** This component aims to maximize the speed of the input speech without sacrificing understandability.\n   - **Speech Recognizer:** This component evaluates whether the adjusted speech remains understandable.\n   - By training both models simultaneously, the system ensures that the speech plays back as quickly as possible within the user's comprehension range.\n\n**Detailed Process:**\n- The process begins with voice extraction from the video.\n- The extracted voice is evenly segmented.\n- Each voice segment undergoes speed optimization.\n- The optimized segments are recombined into a single audio track.\n- The recombined audio undergoes speech recognition to check comprehensibility.\n\nThis methodology guarantees that users can consume video content faster, improving time efficiency while maintaining, and even enhancing, understanding, especially for non-native speakers. The interplay between the playback speed adjuster and speech recognizer models, trained concurrently, ensures a balanced output that maximizes speed without losing clarity.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n**Objective:** \nThe main goal of the experiment was to demonstrate that the proposed AIx Speed method optimizes playback speed while maintaining or improving content understanding.\n\n**Datasets:**\n1. **LibriSpeech**\n2. **UME-ERJ**\n\n**Baselines:**\n1. **Constant Playback Speeds:** \n   - 1.0x speed\n   - 1.5x speed\n2. **AIx Speed Modified Playback:** \n   - Average speed times the playback speed of AIx Speed\n\n**Speech Recognition Model:**\nA standard Wav2Vec2 based speech recognition model was used for computing the Character Error Rate (CER) and Word Error Rate (WER).\n\n**Evaluation Metrics:**\n1. **Character Error Rate (CER)**\n2. **Word Error Rate (WER)**\n\n### Main Experimental Results\n\n- **Playback Speed Optimization:** \n   - AIx Speed produced speech at 1.30 times faster on average for LibriSpeech and 1.29 times faster on average for UME-ERJ.\n\n- **Performance Comparison:**\n   - **LibriSpeech dataset:** AIx Speed resulted in lower CER and WER values compared to the average constant speech speeds.\n   - **UME-ERJ dataset:** Similar to LibriSpeech, AIx Speed led to lower CER and WER values. Notably, WER at AIx Speed outperformed even the 1.0x constant playback speed.\n\n**Conclusion:**\nThe results suggest that the AIx Speed method successfully maximizes playback speed while ensuring, or even improving, recognition performance. For non-native speakers in the UME-ERJ dataset, the AIx Speed method significantly improved recognition performance, indicating its potential for enhancing the understandability of non-native speech."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To demonstrate that the proposed AIx Speed method can optimize playback speed while maintaining content understanding.",
            "experiment_process": "The study compared the Character Error Rate (CER) and Word Error Rate (WER) values at the AIx Speed-modified speech playback speed with those at constant playback speeds of 1.0x, 1.5x, and the average speed. A Wav2Vec2 based speech recognition model was used to compute these errors for the LibriSpeech and UME-ERJ datasets. The comparison focused on demonstrating the effectiveness of AIx Speed in optimizing playback speed while preserving speech intelligibility.",
            "result_discussion": "The results indicated that AIx Speed produced speech 1.30 times faster on average for LibriSpeech and 1.29 times faster for UME-ERJ, with both datasets showing lower CER and WER values than the average constant speed playback. Additionally, AIx Speed showed better recognition performance for non-native speakers' speech compared to 1.0x speed playback, suggesting that the proposed method converts speech into more understandable forms.",
            "ablation_id": "2403.02938v1.No1"
        },
        {
            "research_objective": "To confirm that the generated AIx Speed speech is understandable and of high quality compared to constant speed speech.",
            "experiment_process": "User experiments were conducted where 50 US residents who used English daily rated the quality of generated speech. The quality was measured using the mean opinion score (MOS) on a five-point scale. 20 sentences from the LibriSpeech and UME-ERJ datasets were either converted to AIx Speed-enhanced speech or constant speed-up and played to participants. Each participant rated 40 audio sentences in total.",
            "result_discussion": "The MOS scores showed that the quality of AIx Speed-generated speech was higher, with LibreSpeech and UME-ERJ scores being 0.5 and 0.8 points higher, respectively, compared to constant speed speech. This demonstrates that AIx Speed produces more intelligible and higher quality speech as perceived by users.",
            "ablation_id": "2403.02938v1.No2"
        }
    ]
}