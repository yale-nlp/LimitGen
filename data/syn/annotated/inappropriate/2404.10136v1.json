{
    "title": "Language Model Cascades: Token-level uncertainty and beyond",
    "abstract": "Recent advances in language models (LMs) have led to significant improvements in quality on complex NLP tasks,\nbut at the expense of increased inference costs.\nCascading offers a simple strategy to achieve more favorable cost-quality tradeoffs:\nhere,\na small model is invoked for most \u201ceasy\u201d instances, while\na few \u201chard\u201d instances are deferred to the large model.\nWhile the principles underpinning cascading are well-studied for classification tasks\n\u2014\nwith deferral based on predicted class uncertainty favored theoretically and practically\n\u2014\na similar understanding is lacking for generative LM tasks.\nIn this work, we initiate a systematic study of deferral rules for LM cascades.\nWe begin by examining\nthe natural extension of predicted class uncertainty to generative LM tasks,\nnamely,\nthe predicted sequence uncertainty.\nWe show that this measure suffers from the length bias problem, either over- or under-emphasizing outputs based on their lengths.\nThis is because LMs produce\na sequence of uncertainty values,\none for each output token;\nand moreover, the number of output tokens is variable across examples.\nTo mitigate this issue, we propose to exploit the richer token-level uncertainty information implicit in generative LMs.\nWe argue that na\u00efve predicted sequence uncertainty corresponds to a simple aggregation of these uncertainties.\nBy contrast, we show that incorporating token-level uncertainty through learned post-hoc deferral rules can significantly outperform such simple aggregation strategies, via experiments on a range of natural language benchmarks with FLAN-T5 models.\nWe further show that incorporating embeddings from the smaller model and intermediate layers of the larger model can give an additional boost in the overall cost-quality tradeoff.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "",
            "text": "Recent advances in generative language modeling have yielded a series of\nTransformer-based models\nwith remarkably improved quality on complex NLP tasks (Radford et al., 2018  ###reference_b56###; Raffel et al., 2020  ###reference_b57###; Brown et al., 2020  ###reference_b7###; Black et al., 2022  ###reference_b4###; Hoffmann et al., 2022  ###reference_b28###; Chowdhery et al., 2022  ###reference_b13###; Wei et al., 2022  ###reference_b86###; Chung et al., 2022  ###reference_b14###; Tay et al., 2023  ###reference_b72###; Anil et al., 2023  ###reference_b3###; Touvron et al., 2023  ###reference_b74###; Team et al., 2023  ###reference_b73###).\nUnfortunately, such models also involve significantly increased inference costs,\nwhich has motivated a series of efforts at reducing the same.\nThese span\ncareful infrastructure optimization (Chowdhery et al., 2022  ###reference_b13###; Pope et al., 2022  ###reference_b55###; Sheng et al., 2023  ###reference_b65###),\nrethinking the autoregressive decoding that underpin Transformers (Stern et al., 2018  ###reference_b69###; Leviathan et al., 2023  ###reference_b45###; Chen et al., 2023a  ###reference_b8###; Sun et al., 2023  ###reference_b70###),\nmodifications of the underlying model architecture (Dao et al., 2022  ###reference_b16###), and\nmodel compression strategies (Frantar & Alistarh, 2023  ###reference_b23###; Agarwal et al., 2023  ###reference_b2###).\nCascading is one simple strategy to achieve more favorable cost-quality tradeoffs\nvia adaptive inference.\nIn a two-model cascade,\na small model is invoked for most \u201ceasy\u201d instances, while\na few \u201chard\u201d instances\nare deferred to\na large model.\nCascades have been widely explored in the vision domain (Viola & Jones, 2001  ###reference_b79###; Trapeznikov & Saligrama, 2013  ###reference_b75###; Bolukbasi et al., 2017  ###reference_b6###; Huang et al., 2018  ###reference_b31###; Rawat et al., 2021  ###reference_b59###; Kag et al., 2023  ###reference_b37###; Jitkrittum et al., 2023  ###reference_b34###),\nand have seen increasing adoption within NLP (Mamou et al., 2022  ###reference_b47###; Varshney & Baral, 2022  ###reference_b77###; Khalili et al., 2022  ###reference_b39###; Dohan et al., 2022  ###reference_b18###; Chen et al., 2023b  ###reference_b9###, a  ###reference_b8###).\nImportantly, cascades can be implemented in a black-box fashion over existing models, and do not necessitate any additional training.\n\n###figure_1### The key challenge in cascading is to design a deferral rule which decides whether to defer an input to the larger model. The principles underpinning optimal deferral rules are well known for the classification setting, where the standard recipe is to employ the small model\u2019s prediction confidence,\nas canonically measured by its softmax probability output (Chow\u2019s rule (Chow, 1970  ###reference_b12###)). This simple deferral rule is remarkably hard to surpass in most natural settings (Jitkrittum et al., 2023  ###reference_b34###).\nHowever, the narrative is more complex for generative LMs.\nWhile one can na\u00efvely translate Chow\u2019s rule for such models\nbased on the softmax probability of the output sequence,\nthis suffers from a\nlength bias issue:\none tends to defer longer predictions, regardless of quality.\nFurther, simply normalizing the probability by sequence length tends to over-correct this bias, and defer shorter predictions.\nIntuitively, such na\u00efve translations of Chow\u2019s rule\nignore a key distinction between the classification and generative LM setting:\nthe former involves a single probability distribution over labels,\nwhile\nthe latter involves a sequence of distributions over multiple tokens of the LM output;\nmoreover, the number of output tokens differs across examples.\nThis variability complicates summarizing the sequence of uncertainty (or confidence) values into a single deferral score.\nTo mitigate the length bias and capture fine-grained information from the uncertainty vector over tokens, we propose to use quantiles over the vector.\nVia experiments on a range of NLP benchmarks and FLAN-T5 models, we show that these quantiles can capture rich and complementary sources of uncertainty information from the uncertainty sequence vector and do better than the simple aggregation schemes like sum and average.\nHowever, we observe that there is no fixed quantile value which works across all datasets. This motivates us to learn a deferral rule based on these quantile values as features, which can combine the strengths of these different quantile scores.\nWe show that our trained deferral rule is the most consistently performant method compared to all the natural baseline\naggregation strategies.\nWe further show that using embeddings from the smaller model and intermediate embeddings from the larger model can give further performance improvement.\nTo summarize, our contributions are:\nWe show that simple sequence-level LM confidence measures for deferral\ncan yield strongly sub-optimal cost-quality tradeoffs,\nowing to a length bias issue (\u00a74  ###reference_###).\nWe introduce token-level uncertainty in the form of distribution quantiles,\nand show that they\ncan yield to consistently more effective cost-quality tradeoffs,\nowing to their finer-grained measure of uncertainty.\nHowever, there is no fixed quantile which works well across all settings (\u00a74  ###reference_###).\nWe propose a post-hoc deferral rule trained on quantile features, and show it can outperform all other strategies on a range of NLP benchmarks for FLAN-T5 models\n(\u00a74.3  ###reference_###).\nWe further demonstrate that\nusing\nthe large model\u2019s intermediate embeddings\ncan significantly\nboost\nperformance."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "",
            "text": "In this section, we discuss the relevant background and set up the problem of LM cascades.\nLanguage models (LMs). Given a finite vocabulary \n(e.g., tokens derived from SentencePiece (Kudo & Richardson, 2018  ###reference_b40###)),\na language model (LM)\ndefines a distribution  over all possible tokens given any context .\nThis in turn defines a distribution over sequences  for any , with\n via the chain rule of probability.\nLMs based on Transformers (Vaswani et al., 2017  ###reference_b78###) have proven increasingly popular in recent years.\nSuch models are typically pre-trained on large corpora based on self-supervised objectives (Radford et al., 2018  ###reference_b56###; Devlin et al., 2019  ###reference_b17###; Raffel et al., 2020  ###reference_b57###; Brown et al., 2020  ###reference_b7###; Tay et al., 2022  ###reference_b71###; Anil et al., 2023  ###reference_b3###).\nThese objectives\ninvolve different (input, output) pair constructions\n\n(e.g., masking out the next token),\nupon which one minimizes the cross-entropy or log-loss, i.e., .\nAt inference time, given a trained LM and any input context ,\nit is common to perform either classification or generation.\nIn the former, given a set of predefined choices  (e.g., { yes, no }),\none scores each  and returns the highest scoring choice.\nIn the latter,\none performs sampling from  to produce a suitable output string response, e.g., by\ntemperature (Ficler & Goldberg, 2017  ###reference_b21###),\ntop- (Fan et al., 2018  ###reference_b20###),\nor nucleus sampling (Holtzman et al., 2020  ###reference_b29###).\nModel cascades. Cascades are a simple, generic strategy to improve the inference cost-quality tradeoff (Wang et al., 2022  ###reference_b83###).\nGiven a collection of models of varying inference cost,\nthe key idea is to perform\nadaptive inference:\n\u201ceasy\u201d samples are afforded less computation compared to \u201chard\u201d samples.\nConcretely, for any test input,\none first executes the lowest cost model,\nand uses a deferral rule to determine whether\nto terminate with its prediction, or\nto invoke the next cheapest model.\nCascades can reduce the average inference cost if only a small fraction of inputs are deferred.\nCascades\nhave a long history of usage in vision (Viola & Jones, 2001  ###reference_b79###; Huang et al., 2018  ###reference_b31###; Wang et al., 2018  ###reference_b84###),\nwhere they are\noften applied for classification problems.\nGiven an instance space  and label space , the classification problem seeks a classifier\n\nwith good average quality under some distribution ,\nas measured by\n\nfor\nsome .\nIn the simplest case,  measures the classifier accuracy.\nNow suppose we have two classifiers ,\nwith inference costs (e.g., latencies)\n.\nOperationally, a cascade first invokes the \u201csmall\u201d model , and then applies a deferral rule to decide whether to either defer prediction to the \u201clarge\u201d model , or terminate with \u2019s prediction.\nMore precisely,\nlet\n denote the deferral rule,\nwhere\n denotes that we wish to defer to the large model.\nThen, the cascaded classifier is (Kag et al., 2023  ###reference_b37###; Jitkrittum et al., 2023  ###reference_b34###):\nGiven an input ,\nthe corresponding cascade quality and cost are:\nIdeally, one seeks to maximize quality\ngiven a budget  on average inference cost:\nWe note that the average cost  is related to the deferral rate , via .\nIn practice, one may set  for suitable  and threshold .\nOne may choose  to satisfy the inference cost constraint.\nNow, we discuss cascades for generative LMs.\nThis largely follows the setup described above, except that we now consider probabilistic models over sequences.\nConcretely, suppose we have two language models\n,\nwith inference costs .\nSimilarly,\nsuppose \nis a measure of the quality of a given distribution over responses for a given prompt.\nA cascade\n of these models\nis parameterized by\na\ndeferral rule ,\nand is given by:\nGiven an input sequence  and target sequence ,\nan LM cascade results in quality and cost\nWith these, we may construct a similar constrained objective as in Equation 1  ###reference_###.\nSimilarly to the classification case, we may parameterize the deferral rule as  for a suitable deferral score function  (which may depend on the output of ). We will investigate and analyze different types of deferral score functions on different NLP tasks.\nRecently, Chen et al. (2023b  ###reference_b9###) introduced the FrugalGPT system to achieve efficient inference via multiple strategies, including LM cascades.\nThey also learn a deferral score to determine whether or not to terminate prediction;\nhowever, this\ndepends on the input prompt and the generated output text,\nand does not consider the model\u2019s token-level uncertainty as we shall explore subsequently. A few works have proposed to learn a router which can decide which model to use amongst a set of models depending upon the input prompt  (Shnitzer et al., 2023  ###reference_b66###; Hari & Thomson, 2023  ###reference_b26###). However, their settings do not necessarily consider models of increasing capacity and hence, their routers depend only on the input prompt not on the model confidence."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "",
            "text": "A key question in the design of cascades is the choice of deferral rule.\nIn this work, we seek to understand the behaviors of different types of deferral functions on NLP tasks. We start by discussing a few natural extensions of commonly used deferral rules for classification."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "",
            "text": "Chow-Sum. We start with the multi-class classification setting where the output space  and .\nIn the simplest case, one may defer\nif the confidence in the prediction  of the small model is sufficiently low.\nThere are several means of quantifying confidence in classification (Shafer & Vovk, 2008  ###reference_b64###; Guo et al., 2017  ###reference_b25###; Kendall & Gal, 2017  ###reference_b38###; Jiang et al., 2018  ###reference_b32###),\nbut arguably the simplest is\nthe predicted class probability (Huang et al., 2018  ###reference_b31###; Wang et al., 2022  ###reference_b83###; Jitkrittum et al., 2023  ###reference_b34###),\nwhich aligns with Chow\u2019s rule from the closely related problem (see Mozannar & Sontag (2020  ###reference_b49###); Narasimhan et al. (2022  ###reference_b51###)) of learning to reject (Chow, 1970  ###reference_b12###):\nwhere  denotes the predictive distribution over possible labels of the small model,\nand  denotes the predicted label.\nTo design a deferral rule for LM cascading,\na natural starting point is to mimic the predicted class probability (Equation 2  ###reference_###):\nwe may compute\nthe (log) probability of the model generated sequence\n,\nWe term this approach Chow-Sum, as it involves the sum of per-token log probabilities.\nAnalogous to the prediction rule for classification,\nwe may set\n,\ndenoting by  the set of all sequences.\nThis requires searching over an exponentially large set;\nhowever, efficient approximations via greedy or beam search are feasible.\nChow-Average.\nChow-Sum computes the aggregate sequence-level log-probability.\nA natural variant is the\naverage of the per-token log-probabilities.\nThis is equivalently\nthe length normalized log-probability,\nor\nthe log-perplexity (Chen et al., 1998  ###reference_b10###):\nNote that\n may be computed as above,\nwithout incorporating any length-normalization."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "",
            "text": "Given that Chow-Sum tracks closely with the well-established Equation 2  ###reference_###,\nit is tempting to conclude that this emphatically solves the LM cascade problem.\nHowever, LMs can be susceptible to the length bias problem (Murray & Chiang, 2018  ###reference_b50###; Adiwardana et al., 2020  ###reference_b1###):\nshorter, lower quality responses may receive a higher probability than longer, higher quality responses.\nThis may be seen as a consequence of the fact that each\n provides an imperfect estimate of a \u201ctrue\u201d probability ,\nand that errors in these estimates compound with sequence length.\nThe length-bias issue naturally suggests using an average instead of sum of log-probabilities.\nHowever, Chow-Average can over-correct for this length bias,\nand preferentially defer shorter predictions.\nWe will see concrete examples of this behavior in \u00a74  ###reference_###.\nMore fundamentally, both approaches\nare inherently limited in the way they aggregate token-level uncertainty.\nIn particular, computing the\nsum or average of per-token probabilities\nmay mask settings where individual tokens are highly uncertain,\neven if the entire sequence has reasonably high probability.\nSuch token-level uncertainty may be highly important in certain settings such as fact-answering:\nhere, an LM may be (correctly) highly confident on articles and other grammatical primitives, but these are of less interest than confidence on tokens corresponding to entities (say).\nThis observation has been previously noted and exploited to allow certain \u201ceasy\u201d tokens to be quickly decoded (Schuster et al., 2022  ###reference_b62###). This observation has also been exploited in knowledge distillation by using different teaching modes for \u201ceasy\u201d versus \u201chard\u201d tokens (Zhong et al., 2024  ###reference_b92###).\nFigure 2  ###reference_### presents an example of this phenomenon on the WMT FR  EN dataset (details in \u00a73.5  ###reference_###):\nthere can be cases where most tokens are highly predictable\n(i.e., ),\nbut a few tokens are less predictable\n(i.e., ).\nIn such cases, Chow-Sum can yield overly optimistic uncertainty estimates.\nThis motivates us to consider richer representations of uncertainty which can capture token-level uncertainty, instead of simply computing the sum or average over the sequence.\n###figure_2### Model output tokens:"
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "",
            "text": "The discussion in \u00a73.2  ###reference_### suggests there is value\nin considering the following generalization of the maximal sequence probability:\nwhere  is the most probable output sequence (or a suitable approximation thereof) under .\nHere,  computes the -quantile of the set of per-token log probabilities. For instance,  would correspond to taking the minimum per-token log probability as the deferral score.\nOne may regard quantiles as another way of converting the token-level uncertainty distribution into a single score, which are capable of capturing richer information from the token-level uncertainty distribution. For example, employing the maximal token uncertainty\n(i.e., the minimum of the per-token probabilities (Stengel-Eskin & Van Durme, 2022  ###reference_b68###))\ncan be useful in scenarios where most tokens are predictable, but a few important tokens are not (per Figure 2  ###reference_###).\nNext, we evaluate all the aforementioned approaches on multiple NLP tasks. For that, we describe the experimental setup used for the evaluation."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "",
            "text": "Models.\nWe employ\nFLAN-T5 (Chung et al., 2022  ###reference_b14###) models, which are T5 models (Raffel et al., 2020  ###reference_b57###) that have undergone instruction tuning (Wei et al., 2022  ###reference_b86###).\nThis family offers a range of models of different sizes,\nspanning Small (M parameters) to XXL (B parameters),\nand have demonstrated strong few-shot performance on a range of NLP benchmarks.\nIn the body, we primarily focus on a two-model cascade of FLAN-T5 Base and FLAN-T5 Large. Results for other models are included in the Appendix.\nWe employ these models with few-shot prompting and greedy decoding.\nEvaluation.\nWe summarize performance using the deferral curve.\nConsider a candidate deferral rule produced by thresholding\n\nvia .\nLet  denote the associated cascaded LM.\nFor a fixed threshold , we may compute the associated\ndeferral rate ,\nand the associated cascade quality .\nThe deferral curve is produced by\nplotting the trade-off between deferral rate and cascade quality\nas  is varied.\nAs a scalar summary,\nwe report\nthe area under the deferral curve (AUC-DF).\nFor a given dataset, higher AUC-DF values indicate better deferral curves.\nNote that the range of AUC-DF values vary across datasets, however.\nDatasets.\nIn the body, we show deferral curves for three different NLP tasks:\nMNLI (Williams et al., 2018  ###reference_b87###), a multi-class classification problem;\nTriviaQA (Joshi et al., 2017  ###reference_b35###), a closed-book question answering problem;\nand\nWMT DE  FR, a translation problem.\nWe report AUC-DF numbers for an expanded dataset pool.\nThese span\nClassification\n(IMDb (Maas et al., 2011  ###reference_b46###),\nSuperGLUE (Wang et al., 2019a  ###reference_b81###),\nMNLI (Williams et al., 2018  ###reference_b87###),\nANLI (Nie et al., 2020  ###reference_b52###));\nQuestion answering\n(TriviaQA (Joshi et al., 2017  ###reference_b35###),\nNaturalQA (Kwiatkowski et al., 2019  ###reference_b44###),\nTyDiQA { ID, SW, FI } (Clark et al., 2020  ###reference_b15###));\nReading comprehension\n(Lambada (Paperno et al., 2016  ###reference_b54###),\nSQuAD (Rajpurkar et al., 2016  ###reference_b58###));\nTranslation\n(WMT 14: EN  FR (Bojar et al., 2014  ###reference_b5###), WMT 19: DE  FR (Foundation,  ###reference_b22###), and WMT 14: FR  EN (Bojar et al., 2014  ###reference_b5###));\nand\nCommon-sense reasoning\n(Winogrande (Sakaguchi et al., 2021  ###reference_b61###)). Note that we treat all problems as finding a text to text mapping. So for classification tasks, we encode the classes as strings. For evaluation, we take the model\u2019s output text and perform a string comparison to the label. See Table 2  ###reference_### (Appendix) for more details."
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "",
            "text": "We now empirically validate the critiques in \u00a73.2  ###reference_###, demonstrating that using the standard sequence probability (Chow-Sum) to defer can result in overly penalizing longer sequences. Moreover, Chow-Average flips this bias and overly defers shorter sequences.\nWe then verify that the Chow-Quantile generalization proposed above can capture richer token-level uncertainty.\n###figure_3### ###figure_4### ###figure_5### ###figure_6### ###figure_7### Summary of results.\nFigure 3  ###reference_### plots the deferral curves for three datasets. We see that\n(for a particular choice of quantile),\nChow-Quantile consistently outperforms standard Chow-Sum and Chow-Average.\nAUC-DF values for all datasets are included in Table 4  ###reference_###. Looking at the table, we see that while a particular choice of quantile is able to do well,\nthere is no single consistent choice that performs well across tasks.\nNext, we discuss insights into the results by using WMT FREN as an example dataset.\nWhy can Chow-Sum and Chow-Average be sub-optimal?\n\nTo better understand the reason for the sub-optimality of Chow-Sum, Figure 4  ###reference_### studies the relation between the deferral rule and output length.\nSpecifically,\nfor each test prompt ,\nlet  denote the result of decoding via the small model in the cascade.\nFor each deferral rule,\nwe compute the corresponding score  and the length .\nFor ease of comparison across different rules, we convert each score into the corresponding quantile.\nFigure 4  ###reference_### reveals that Chow-Sum tends to defer prompts with larger output lengths:\nthe prompts with lowest scores have notably higher output length than those with higher scores. This makes us ask if it is all bad to defer prompts with longer outputs?\nWe observe that for the Base model on the WMT datasets,\neven the BLEURT (Sellam et al., 2020  ###reference_b63###) scores\ntend to have a non-zero negative correlation with output lengths\n(Table 3  ###reference_###, Appendix).\nA closer look at the model predictions shows that longer predictions tend to have repetitions, as shown in Figure 5  ###reference_### (Top) and hence, are good candidates for deferral.\n(The shown predictions are truncated for clarity.)\nThis shows that there is some signal in output length as a deferral score.\nHowever, Chow-Sum is overly biased towards deferring longer predictions and hence, can be sub-optimal. Interestingly, Chow-Average over-corrects this bias:\nit tends to overly defer prompts\nwith lower output length.\nWhy does Chow-Quantile help?\nAs discussed above, Chow-Quantile is able to capture rich information from the token-level uncertainty vector. We discuss below why Chow-Quantile-0 and Chow-Quantile-0.8 work well with respect to the WMT FREN dataset.\nChow-Quantile-0: The main insight is that the minimum token probability\nis able to capture repetitions and \u201c??\u201d (unknown tokens),\nas they generally tend to have lower probability values and are more uncertain.\nThis confirms our understanding that quantiles can capture richer token-level uncertainty.\nWe show two examples with the minimum Chow-Quantile-0 value for the WMT FREN dataset and FLAN-T5 Base in Figure 5  ###reference_### (Middle).\nChow-Quantile-0.8: Interestingly, Chow-Quantile-0.8 tends to defer shorter predictions. We show two examples with the minimum Chow-Quantile-0.8 value in Figure 5  ###reference_### (Bottom).\nTo understand this,\nFigure 4(c)  ###reference_sf3### shows the\naverage token probability as a function of the token index,\nfor the\nWMT EN  FR dataset and FLAN-T5 Base model.\nAs the token index increases, the average probability increases; i.e., the model tends to become more confident. Hence, the Chow-Quantile-0.8 is able to focus more on the shorter, uncertain outputs.\nIn summary, we have seen that Chow-Quantile-0 is able to focus more on identifying the presence of repetitions and unknown tokens \u201c??\u201d whereas Chow-Quantile-0.8 is able to capture the uncertainty in shorter predictions better. Thus, we conclude that different quantiles are able to capture richer and complementary measures of uncertainty. Moreover, we have already seen that there is no one quantile which works well across all datasets. Given this, a natural option is to learn how to combine various quantiles for a given dataset, which we consider next."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "",
            "text": "We show that training post-hoc deferral rules based on the mean of the probabilities, and (optionally) suitable embeddings from the small and large model, can significantly improve the cost-quality tradeoff."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "",
            "text": "The idea of learning when to defer in a cascade follows a recent line of work on classification (Narasimhan et al., 2022  ###reference_b51###; Kag et al., 2023  ###reference_b37###; Jitkrittum et al., 2023  ###reference_b34###). In a nutshell, for suitable feature mapping, we seek to learn a deferral score via a standard model class (e.g., a feedforward network). We then defer using . To construct the input features, we set  to be a fixed length vector comprising the mean of the probabilities from the small model. Additionally, we add the aggregate scores from Chow-Sum and Chow-Average (see Appendix C  ###reference_###). To fit the deferral scorer on a training set of input prompts , we minimize an empirical loss against a set of target labels. For tasks based on accuracy, we set  iff the large model is correct, and the small model is incorrect on the given example; i.e., it would benefit to defer to the larger model. We then fit the scorer with the binary logistic loss. For translation tasks, the target is the difference of BLEURT scores of the two models; we train with the square loss. We call this method Post-Hoc-Quantile (see Appendix A  ###reference_### for details)."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "",
            "text": "The above target labels exploit information from the large model during training. Importantly, we cannot directly use such information during inference, as it would require querying the large model (and thus defeat the point of cascading). Note, however, that in some settings it may be feasible to use intermediate information from the large model, e.g., token embeddings from an intermediate layer. Prior work has noted that such intermediate embeddings can often contain valuable information by themselves (Schuster et al., 2022 ###reference_b62###). Inspired by this, we thus study the viability of using such intermediate embeddings for training post-hoc deferral rules. For encoder-decoder models such as T5, such embeddings can be from either the encoder, decoder, or both. We study two methods - one which uses the final decoder embeddings of the smaller model averaged over all tokens. We call this method Post-Hoc-Embed-1. In the second method, we add the first token embedding from the first decoder layer of the large model as another input to the post-hoc rule. We call this method Post-Hoc-Embed-1+2. We remark that previous work (Ren et al., 2023 ###reference_b60###) has shown the value of mean of the probabilities for selective generation, and the related problem of out-of-distribution detection. We caution also that while not as expensive as querying the entire large model, even extracting intermediate decoder embeddings can involve a non-trivial cost. Nonetheless, in settings where some increase in cost is acceptable, it is of interest whether these embeddings offer significantly valuable information."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "",
            "text": "For the same experimental setup as the previous section, Table 4  ###reference_### summarizes the area under the deferral curve (AUC-DF) across various datasets. Numbers are averaged over 5 random runs. We see that the post-hoc deferral rule approach consistently performs the best; in scenarios where other methods are better, the difference is minimal. For a more fine-grained analysis, Figure 6  ###reference_### plots the full deferral curves on MNLI, TriviaQA, and WMT. In the plot, Chow-Mean-Best chooses the best method out of Chow-Mean-* based on the validation data. We see that post-hoc routing is generally on-par with the Chow-* family of (non-learned) deferral rules. We see that Post-Hoc-Embed-1 is able to improve upon Post-Hoc-Mean and Chow-Mean-* methods slightly but is slightly inferior compared to the Post-Hoc-Embed-1+2 method. This intuitively makes sense as this has more information compared to the Post-Hoc-Mean method but still does not include any information about model 2. Strikingly, further using the larger model\u2019s intermediate embeddings can lead to significant improvements across all deferral rates, particularly for classification tasks. Intuitively, the first token\u2019s intermediate embedding could have a lot of information for classification tasks, where the answers typically comprise of a single word and only a couple of tokens. However, for generation and translation tasks with longer outputs, the main token containing the answer could be present later in the sequence and thus, there may be limited use of using the first token embedding. One may wonder if we really need mean of the probabilities to train the post-hoc deferral rule. In Appendix B.1  ###reference_###, we show that na\u00efvely passing the probability vector with padded zeros performs poorly in many cases. ###figure_8###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "",
            "text": "We have seen that there is value in going beyond simple sequence level uncertainty and considering finer measures of token level uncertainty as deferral rules for cascades. Moreover, we have seen that\nintermediate embeddings from the larger model can further boost performance.\nThis work raises a number of interesting directions for future work.\nFirst, we have used a -layer MLP to train post-hoc deferral rules using the quantiles of the probability distribution. It would be interesting to see how well a -layer Transformer works for this task, to potentially better exploit the sequential nature of the token probability vector.\nWe have focused on FLAN-T5 instruction tuned Encoder-Decoder models in this work. We believe that the insights and methods should generalize to Decoder-only architectures. It would be interesting to evaluate the proposed approaches for such architectures. Moreover, it has been observed that models with RLHF finetuning become uncalibrated (Figure 8 in OpenAI (2023  ###reference_b53###)). It would be interesting to see how various finetuning steps affect the findings in this work and what consequences they have for designing efficient cascades.\nMultiple works have considered alternative notions of uncertainty using the generative abilities of LMs, for example, reprompting the model to ask how confident it is about the answer or output an additional confidence score as part of the outputs (Kadavath et al., 2022  ###reference_b36###). It would be interesting to evaluate how well these measures work for cascades which we discuss in the next section."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "",
            "text": "There has been a large body of work on uncertainty quantification for LMs. We discuss some of the approaches below. They can be broadly divided into the following categories.\nConsensus-based.\nOne limitation of Equation 3  ###reference_###\nis that it considers a single output sequence, e.g., the most likely one.\nHowever, as there are many sequences that have similar meaning, it is intuitively more reliable to consider drawing multiple sequences from .\nOne may then assess the consensus in resulting predictions to measure confidence (Wang et al., 2023  ###reference_b85###; Xiao et al., 2021  ###reference_b88###; Chen et al., 2023c  ###reference_b11###);\nthis can help distinguish between models that are locally diffused versus peaked around a candidate sequence. Recently, Yue et al. (2023  ###reference_b89###) explored the use of answer consistency to construct effective cascades.\nDeep ensembles and dropout.\nOne approach to measure confidence is to create an ensemble of different models, and suitably aggregate their predictions (e.g., based on disagreement) (Van Landeghem et al., 2022  ###reference_b76###; Wang et al., 2019b  ###reference_b82###; Gleave & Irving, 2022  ###reference_b24###). However, these uncertainty estimation procedures involve additional computation (e.g., multiple inferences with a single model in dropout-based approaches and single inference with multiple models in ensemble-based approaches) compared to simply using softmax probability outputs from a single network. Such approaches are less appealing for use in cascades, where the primary goal is to improve efficiency.\nPost-hoc calibration/Answer- and length-bias calibration.\nFor tasks involving question-answering with multiple choices (e.g., (A), (B), (C)),\nseveral works have demonstrated that LMs can have prior biases to certain answers, which can be identified and corrected (Zhao et al., 2021  ###reference_b91###; Holtzman et al., 2021  ###reference_b30###; Kumar, 2022  ###reference_b43###; Murray & Chiang, 2018  ###reference_b50###; Mielke et al., 2022  ###reference_b48###; Jiang et al., 2021  ###reference_b33###).\nSemantic entropy.\nAnother key challenge in measuring the uncertainty for natural language outputs is that there are a lot of semantic equivalent sentences and hence, the probability can be divided among multiple outputs which mean the exact same thing. Kuhn et al. (2023a  ###reference_b41###) proposes to mitigate this problem by sampling multiple outputs and then clustering semantically equivalent outputs together and combining their probability together. It would be interesting to understand how well this method can work for our setting.\nGenerative uncertainty.\nThe above has largely focussed on generalizing the standard maximum predictive probability (Equation 2  ###reference_###) from classification to the LM setting.\nWhile this by itself leads to a rich array of possible confidence measures,\nLMs intriguingly offer a wholly new possible means of assessing confidence:\none may directly probe the model to obtain how confident it is on the proposed answer  (Kadavath et al., 2022  ###reference_b36###). Kadavath et al. (2022  ###reference_b36###) discuss various ways of the input prompt format for this confidence probe. They also discuss the training of an additional head of the model to predict the model confidence but again, it is not clear how this compares with the standard probability output by the model without any additional finetuning. However, (Shrivastava et al., 2023  ###reference_b67###) found that the confidence measures generated linguistically give worse estimates of uncertainty compared to the classical softmax-based measures even when these softmax-based probabilities come from a different and weaker model. Moreover, they observed that two sources of uncertainty are complementary and it can be beneficial to combine them.\nOther work.\nZhao et al. (2023  ###reference_b90###) proposed sequence-level calibration as a means to improve the generative ability of LMs; such calibration could also be useful in improving methods such as Chow-Sum.\nKuhn et al. (2023b  ###reference_b42###) proposed to ask the model to detect ambiguous questions which the model is likely to get wrong and answer clarifying questions if the question is indeed ambiguous. Hendy et al. (2023  ###reference_b27###) proposed to use an exogeneous quality estimation model to decide how to route between two models.\n\u0160akota et al. (2023  ###reference_b80###) similarly proposed to train a meta-model to pick an appropriate model from a family. Fadeeva et al. (2023  ###reference_b19###) did a comprehensive experimental analysis of various uncertainty methods."
        }
    ],
    "url": "http://arxiv.org/html/2404.10136v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "4",
            "4.1",
            "4.2",
            "4.3"
        ],
        "main_experiment_and_results_sections": [
            "3.4",
            "3.5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.1",
            "3.2",
            "3.3",
            "3.4",
            "3.5",
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "research_context": {
        "paper_id": "2404.10136v1",
        "paper_title": "Language Model Cascades: Token-level uncertainty and beyond",
        "research_background": "## Paper's Motivation and Research Problem\nThe primary motivation of the paper is to address the significant inference costs associated with advanced generative language models, as recent improvements in the quality of such models have simultaneously increased the computational resources required for their inference. The research problem focuses on developing cost-efficient strategies for deploying these models, specifically by leveraging a cascading mechanism where a small model handles easier instances while deferring harder ones to a larger model.\n\nThe key challenge within this cascading framework is designing an effective deferral rule, particularly for generative language models, where na\u00efve adaptations of existing deferral mechanisms (originating from classification settings) fail due to biases introduced by the sequential nature of language modeling. This research aims to create more nuanced deferral rules that consider token-level uncertainties within generated sequences, seeking to improve the cost-quality tradeoff without substantial increases in complexity or training requirements.\n\n## Relevant Prior Work\nThe paper builds on significant prior work in several interconnected areas:\n1. **Generative Language Models:**\n    - **Transformer-based models**: Advances in models such as those discussed by Radford et al. (2018), Raffel et al. (2020), Brown et al. (2020), and more recent contributions (Black et al., 2022; Hoffmann et al., 2022; Chowdhery et al., 2022; Wei et al., 2022; Chung et al., 2022; Tay et al., 2023; Anil et al., 2023; Touvron et al., 2023; Team et al., 2023) demonstrate improvements in complex NLP tasks but also highlight increased inference costs.\n2. **Infrastructure Optimization:**\n    - Efforts to optimize the infrastructure to mitigate inference costs, such as those by Chowdhery et al. (2022), Pope et al. (2022), and Sheng et al. (2023).\n3. **Decoding Strategies:**\n    - Rethinking autoregressive decoding, exemplified in works by Stern et al. (2018), Leviathan et al. (2023), Chen et al. (2023a), and Sun et al. (2023).\n4. **Model Architecture and Compression:**\n    - Innovations in the underlying model architecture (Dao et al., 2022) and model compression strategies (Frantar & Alistarh, 2023; Agarwal et al., 2023).\n5. **Cascading Mechanisms:**\n    - The concept of cascading for efficiency, with roots in vision tasks (Viola & Jones, 2001; Trapeznikov & Saligrama, 2013; Bolukbasi et al., 2017; Huang et al., 2018; Rawat et al., 2021; Kag et al., 2023; Jitkrittum et al., 2023), and its growing adoption in NLP (Mamou et al., 2022; Varshney & Baral, 2022; Khalili et al., 2022; Dohan et al., 2022; Chen et al., 2023a, 2023b).\n6. **Deferral Rules for Cascading:**\n    - Traditional deferral rules like Chow\u2019s rule (Chow, 1970) that have shown effective yet imperfect adaptation for generative models, indicating the need for more sophisticated methods to handle the sequence-based nature of language models efficiently.\n\nThis paper proposes innovative methods to overcome limitations in these prior approaches, notably by introducing and learning deferral rules based on token-level uncertainty quantiles, and leveraging embeddings from both smaller and larger models to enhance overall performance.",
        "methodology": "In this work, we investigate various deferral functions and their behaviors in the context of NLP tasks, focusing on the design of language model cascades. The key question addressed is the choice of the deferral rule that decides when to pass the decision from one model to a subsequent one in the cascade. Here, we explore natural extensions of commonly employed deferral rules in classification settings.",
        "main_experiment_and_results": "**Main Experiment Setup and Results:**\n\n**Models:**\nWe utilized FLAN-T5 models, which are variants of T5 models enhanced through instruction tuning. The family includes models of varying sizes from Small to XXL. The main focus in the body of the paper is on a two-model cascade comprising FLAN-T5 Base and FLAN-T5 Large. These models were used in a few-shot prompting setup with greedy decoding.\n\n**Evaluation:**\nPerformance was summarized using the deferral curve, which captures the trade-off between deferral rate and cascade quality by varying a threshold. The deferral rate () and cascade quality () were computed for each threshold. As a scalar summary, the area under the deferral curve (AUC-DF) was reported, with higher AUC-DF values indicating better deferral curves. Note that the range of AUC-DF values varies across datasets.\n\n**Datasets:**\nThe main results were shown for three NLP tasks:\n1. MNLI (a multi-class classification problem),\n2. TriviaQA (a closed-book question answering problem), and \n3. WMT (DE to FR translation).\n\nAdditionally, AUC-DF numbers were reported for an expanded dataset pool across various tasks:\n- **Classification:** IMDb, SuperGLUE, MNLI, ANLI\n- **Question Answering:** TriviaQA, NaturalQA, TyDiQA (ID, SW, FI)\n- **Reading Comprehension:** Lambada, SQuAD\n- **Translation:** WMT 14 (EN to FR), WMT 19 (DE to FR), WMT 14 (FR to EN)\n- **Common-sense Reasoning:** Winogrande\n\nAll problems were treated as text-to-text mappings, encoding classes as strings for classification tasks. The model outputs were evaluated via string comparison to the labels. Details on the datasets are available in the Appendix."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To examine the effectiveness of sequence-level log-probability based deferral rules (Chow-Sum and Chow-Average) in improving LM cascades and to address the limitations arising from length bias in token-level uncertainty.",
            "experiment_process": "The study employs FLAN-T5 models with few-shot prompting and greedy decoding. Experimental setups include generating deferral curves to assess the trade-off between deferral rate and cascade quality across different thresholds. Evaluated datasets encompass tasks like MNLI, TriviaQA, and WMT DE-FR, among others. The primary measurements are the deferral rate and cascade quality, summarized by the area under the deferral curve (AUC-DF).",
            "result_discussion": "The results indicate that Chow-Sum tends to defer longer outputs excessively while Chow-Average over-corrects by deferring shorter outputs. The Chow-Quantile method, which generalizes the sequence probability using quantiles, emerges as superior in capturing richer token-level uncertainties. However, no single quantile consistently excels across all tasks.",
            "ablation_id": "2404.10136v1.No1"
        },
        {
            "research_objective": "To empirically validate the critiques on Chow-Sum and Chow-Average deferral rules and to evaluate the effectiveness of Chow-Quantile in capturing token-level uncertainties.",
            "experiment_process": "Deferral rules were evaluated using deferral curves and AUC-DF values on multiple datasets including MNLI, TriviaQA, and WMT DE-FR. The study specifically examined the relation between deferral scores and output lengths, analyzing BLEURT scores and model predictions for longer outputs.",
            "result_discussion": "Chow-Sum is found to defer longer outputs due to length bias, and Chow-Average tends to favor shorter outputs. Comparatively, Chow-Quantile better captures token-level uncertainties, demonstrating improved performance by focusing on uncertain and important tokens. Different quantiles are shown to be effective in specific contexts, emphasizing the need for a combined approach tailored to different datasets.",
            "ablation_id": "2404.10136v1.No2"
        },
        {
            "research_objective": "To investigate the effectiveness of Post-Hoc-Quantile and embedding-based deferral rules in learning optimal deferral strategies in LM cascades.",
            "experiment_process": "Post-Hoc-Quantile utilizes per-token probability quantiles and aggregate scores from Chow-Sum and Chow-Average as features, training a deferral scorer via logistic or square loss. Post-Hoc-Embed methods incorporate intermediate embeddings from both smaller and larger models. Models were evaluated by AUC-DF on datasets like MNLI, TriviaQA, and WMT DE-FR over five random runs.",
            "result_discussion": "Post-Hoc-Quantile consistently outperforms non-learned methods. Embedding-based methods, especially using intermediate embeddings from the large model, provide significant improvements, particularly for classification tasks. However, they offer limited benefits for generation/translation tasks where the critical information may occur later in the sequence. The study highlights the advantage of incorporating richer contextual embeddings to enhance deferral accuracy.",
            "ablation_id": "2404.10136v1.No3"
        }
    ]
}