{
    "title": "STaR-GATE: Teaching Language Models to Ask Clarifying Questions",
    "abstract": "When prompting language models to complete a task, users often leave important aspects unsaid. While asking questions could resolve this ambiguity (GATE; Li et al., 2023), models often struggle to ask good questions. We explore a language model\u2019s ability to self-improve (STaR; Zelikman et al., 2022) by rewarding the model for generating useful questions\u2014a simple method we dub STaR-GATE. We generate a synthetic dataset of 25,500 unique persona-task prompts to simulate conversations between a pretrained language model\u2014the Questioner\u2014and a Roleplayer whose preferences are unknown to the Questioner. By asking questions, the Questioner elicits preferences from the Roleplayer. The Questioner is iteratively finetuned on questions that increase the probability of high-quality responses to the task, which are generated by an Oracle with access to the Roleplayer\u2019s latent preferences. After two iterations of self-improvement, the Questioner asks better questions, allowing it to generate responses that are preferred over responses from the initial model on  \n\n\n72%\n of tasks. Our results indicate that teaching a language model to ask better questions leads to better personalized responses.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "###figure_1### When interacting with users who have different preferences, language models (LMs) encounter task ambiguity (Finn et al., 2018  ###reference_b6###; Tamkin et al., 2022  ###reference_b31###). Depending on the user, the same request might correspond to a different task. For example, consider a user who asks an LM for a pasta recipe (Figure 1  ###reference_###). If the model could elicit information about the user\u2019s dietary restrictions, favorite sauces, and preferred cooking methods, it could tailor the recipe to their specific needs and desires. The model might suggest a vegetarian pasta recipe for a user who is vegetarian, or propose a traditional lasagna recipe for a user with a passion for Neapolitan cuisine. However, if this information is not explicitly specified in the prompt, the model may generate a generic recipe that fails to account for the user\u2019s unique preferences and constraints. In high-stakes domains like healthcare or education, such task ambiguity can have significant consequences.\nOne approach to resolving task ambiguity is by asking targeted questions to elicit relevant information from users. Prompting closed-source LMs can yield useful questions (e.g., Li et al., 2023  ###reference_b16###; Piriyakulkij et al., 2023  ###reference_b21###). However, this approach is inflexible in guiding a model\u2019s questioning strategy and frequently generates queries that are ineffective or irrelevant for the task at hand.\nIndeed, it is likely that current alignment strategies\u2014such as RLHF\u2014specifically inhibit the ability to\ncarry out such dialog (Shaikh et al., 2023  ###reference_b27###). One recent effort addresses these limitations by combining elicitation with optimal experimental design methods (Handa et al., 2024  ###reference_b9###). However, this approach constrains questions to pairwise comparisons over a fixed set of features, substantially limiting the space of questions that can be used to probe user preferences. Another approach is to use offline reinforcement learning to encourage useful dialog (Hong et al., 2023  ###reference_b11###). This is promising but requires offline generation of high-quality dialog from an expert model, and has not targeted questions for preference elicitation specifically.\nIn this paper, we explore whether we can improve a LM\u2019s ability to ask useful questions by bootstrapping with a form of self-play (Silver et al., 2017  ###reference_b29###; Anthony et al., 2017  ###reference_b1###). We introduce STaR-GATE (Figure 2  ###reference_###), an iterative algorithm that combines active preference elicitation (GATE; Li et al., 2023  ###reference_b16###) with a self-improvement loop inspired by STaR (Zelikman et al., 2022  ###reference_b37###). We address several technical challenges: (1) We define a task setting for improving elicitation for which we generate a synthetic dataset of 25,500 unique persona-task prompts; (2) We define a reward function based on the log probability of gold responses generated by an oracle model (with access to the persona); and (3) We encourage the LM to use the elicited information while avoiding distribution shift through response regularization.\nWe find that questions asked by the finetuned model increase the probability of gold responses consistently across iterations (Figure 4  ###reference_###).\nMoreover, compared to responses generated by the initial model, responses generated by a STaR-GATE finetuned model have  \n\n\n72%\n win rates (Figure 3  ###reference_###a) after two iterations.\n###figure_2### In summary, we make the following contributions: (1) We introduce STaR-GATE, a simple algorithm that iteratively improves a LM\u2019s ability to elicit user preferences by asking questions. (2) We generate a synthetic dataset consisting of 25,500 unique persona-task prompts, each paired with a personalized, gold response. (3) We show that finetuning with STaR-GATE enables a LM to generate questions that significantly increase the probability of generating gold responses. (4) We show that adding response-regularization to STaR-GATE yields a fine-tuned model able to use the elicited preferences to generate better responses\u2014a high win rate against the initial model. (5) We show that the finetuned model generalizes beyond the roleplayer it was trained with."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "###figure_3###"
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Preference Optimization",
            "text": "Preference optimization algorithms, such as RLHF (Christiano et al., 2017  ###reference_b4###), DPO (Rafailov et al., 2024  ###reference_b25###), or KTO (Ethayarajh et al., 2024  ###reference_b5###), optimize LMs to provide single-turn dialog responses that reflect preferred or high-utility outcomes. As a result, these models learn distributions over responses that effectively generate answers to user queries without requiring additional information beyond the initial prompt. However, asking follow-up questions to elicit user preferences is essential for understanding their unique needs and desires, especially when faced with task ambiguity (Tamkin et al., 2022  ###reference_b31###; Li et al., 2023  ###reference_b16###). Despite the importance of follow-up questions for effective communication, recent research has shown that preference optimization algorithms can reduce a LM\u2019s ability to ask follow-up questions. Specifically, RLHF has been found to negatively correlate with a LM\u2019s attempts to ask follow-up questions or show acknowledgment (Shaikh et al., 2023  ###reference_b27###). This limitation can be problematic for high-stakes domains such as healthcare (Thirunavukarasu et al., 2023  ###reference_b32###) or education (Kasneci et al., 2023  ###reference_b15###) , where resolving task ambiguity through effective questioning is crucial for effective dialog."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Preference Elicitation with LMs",
            "text": "One way of resolving task ambiguity is by prompting a LM to ask questions or infer user preferences from observations (Li et al., 2023  ###reference_b16###; Piriyakulkij et al., 2023  ###reference_b21###; Lin et al., 2023  ###reference_b17###; Fr\u00e4nken et al., 2023  ###reference_b7###; Handa et al., 2024  ###reference_b9###). For example, Li et al. (2023  ###reference_b16###) used LMs themselves to elicit user preferences during interaction (GATE; Li et al., 2023  ###reference_b16###). In GATE (short for Generative Active Task Elicitation), a LM elicits and infers intended behavior through free-form, language-based interaction. Unlike non-interactive elicitation approaches, such as prompting (Brown et al., 2020  ###reference_b3###), which rely entirely on the user to specify their preferences, generative elicitation probes nuanced user preferences better. Across domains such as content recommendation and email verification, generative elicitation with LMs requires less effort than prompting while being comparable to or better than user-written prompts (for further details, see Section 5 in Li et al., 2023  ###reference_b16###). Building upon GATE, Handa et al. (2024  ###reference_b9###) introduced OPEN, a framework that combines LM-driven elicitation with Bayesian Optimal Experimental Design (BOED) to select informative questions and translate abstract queries into natural language. OPEN combines the advantages of LMs and Bayesian methods to recommend news articles. OPEN is better at eliciting human preferences than approaches that only use LMs or BOED. Piriyakulkij et al. (2023  ###reference_b21###) combine LMs with probabilistic reasoning to select informative questions that maximize information gain about user preferences in a simplified web shopping task (Yao et al., 2022  ###reference_b36###). Relatedly, Hong et al. (2023  ###reference_b11###) demonstrated that instead of eliciting information directly, it is also possible to prompt a large LM such as GPT-3.5 to simulate conversations between a human and an assistant, and then revise the simulated conversation with Constitutional AI (Bai et al., 2022  ###reference_b2###). This approach allowed the authors to finetune a much smaller GPT-2 model (Radford et al., 2019  ###reference_b24###) to become a capable conversationalist. While all of the above approaches have resulted in significant improvements, they rely on proprietary models for both elicitation and generation of synthetic data for downstream finetuning."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Self-Improving Reasoning",
            "text": "We are interested in training a LM to better elicit preferences using its own reasoning capabilities. To do so, we draw upon recent work showing that LMs can self-improve. For example, Self-Taught Reasoner (STaR; Zelikman et al., 2022  ###reference_b37###) demonstrated that a LM which was trained iteratively on its own reasoning traces for correct answers could solve increasingly difficult problems. By combining rationalization (i.e., reasoning backwards from an answer; see also Rajani et al., 2019  ###reference_b26###) with supervised finetuning on rationales leading to correct answers, a pretrained LM achieves strong performance on datasets such as CommonsenseQA (Talmor et al., 2018  ###reference_b30###). Recently, V-STaR (Hosseini et al., 2024  ###reference_b12###) extended this idea by using both correct and incorrect reasoning traces, essentially attempting to merge STaR with DPO. Relatedly, TRICE (Hoffman et al., 2024  ###reference_b10###) frames the process of generating better chains of thought as a latent-variable inference problem and maximizes the marginal log-likelihood of correct answers. Other relevant works include learning intermediate reasoning for mathematical statements (Poesia et al., 2023  ###reference_b22###), learning from reasoning mistakes (Shinn et al., 2024  ###reference_b28###; Zhang et al., 2024  ###reference_b39###), teaching LMs to reason in planning (Gandhi et al., 2023  ###reference_b8###; Qiao et al., 2024  ###reference_b23###), and Quiet-STaR (Zelikman et al., 2024  ###reference_b38###), a generalization of STaR which generates rationales at each token to explain future text. Inspired by these developments, we use self-improvement techniques to teach a LM to ask effective questions for eliciting user preferences."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "STaR-GATE",
            "text": "Overview. On a high level, STaR-GATE starts with persona-task prompts and generates gold responses with an Oracle that has access to both the persona and the task. Given this setup, we simulate conversations between a Questioner and a human Roleplayer that\u2014similar to the Oracle\u2014has access to the user persona which is unknown to the Questioner. The task of the Questioner is to elicit useful information from the Roleplayer, whereby usefulness is measured as the log probability of the gold response conditional on the questions asked by the Questioner and the preferences elicited from the Roleplayer (see Figure 2 ###reference_###).\nObjective. Let  denote the Questioner (i.e., the policy to train),  the Roleplayer model,  the set of tasks,  the set of user personas, and  the Oracle model. Given a task  and a persona , the Oracle  generates a gold response . The objective of STaR-GATE is to maximize the expected log probability that the pretrained model  assigns to the gold response , given the task  and a simulated conversation  between  and :\nHere  is a simulated conversation of questions  distributed according to  and answers distributed according to .\nOptimization. Equation 1 ###reference_### can be optimized in a variety of ways. Following Zelikman et al. (2022 ###reference_b37###), we use a simple variant of randomly selecting questions without any optimization or evaluation. On each overall iteration, , for each pair , we sample  trajectories of simulated conversations, , using the current . We then select the top- trajectories (here, ) based on the objective, and do supervised fine-tuning for this set from the initial .\nRegularization. An important failure mode of optimizing this objective is that by training the policy  to ask good questions it may forget how to respond and instead always ask questions. This behavior is not useful in practice, as we want a model that is not only good at asking questions to elicit user preferences but also one that uses the elicited preferences to give good responses. To address this issue, we add to Equation 1 ###reference_### a regularization term preventing the distribution of responses (not questions) from moving too far from the previous iteration:\n.\nIn practice this can be accomplished by simply sampling a response (at temperature ) from the previous policy  for each task and persona pair, conditioned on the best conversation history , then appending this response to the conversation history during fine-tuning.\nAlgorithm. We provide an outline of STaR-GATE in Algorithm 1 ###reference_###. We perform expert iteration, training the initial model   times ( for all experiments) on question-response pairs generated from each intermediate model . At each iteration , we alternate between task splits  and , as well as persona splits  and , to prevent generating new data for tasks or personas present during training. For each task  and user persona , we simulate  conversations ( for all experiments), each having a maximum of  total turns ( for all experiments). When generating  for all , we sample  at each turn  from the previous  and fixed . To achieve a roughly uniform distribution of conversation lengths and prevent overfitting on conversations of a single length, we set the termination point to be uniform across .\nAs indicated above, we select the best simulated conversations for finetuning the next iteration according to the objective .\nWe then fine-tune the initial model  on both the selected conversations  and the greedily sampled responses , ensuring that the model learns to ask informative questions and provide personalized responses. Critically, we mask the answers  from the loss, finetuning the question-generation and response-generation policy but not learning to imitate answers."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Elicitation Task",
            "text": "Overview. We evaluate STaR-GATE\u2019s ability to improve the Questioner\u2019s question-asking and response generation across diverse everyday tasks. We find that training with STaR-GATE increases both the log-probability of gold responses and win rates compared to the initial (pretrained and instruction-finetuned) model. Code to reproduce experiments is available at https://github.com/scandukuri/assistant-gate  ###reference_te###.\nTo cover a broad range of everyday life tasks, we selected the first 550 conversations from the open-source instruct-human-assistant-prompt-dataset111instruct-human-assistant-prompt  ###reference_truct-human-assistant-prompt?row=1### which we divided into two train splits (, ) each of  and one test split . Importantly, we only selected the human queries (not the assistant responses) for each conversation and used these as the tasks  to seed a given simulation. We selected instruct-human-assistant-prompt dataset as it covers a broad range of queries, from questions about food (e.g., \u201cWhat type of wine goes best with steak?\u201d), to career questions (e.g., \u201cI\u2019m having trouble finding the perfect job. What resources can help me?\u201d), and education (e.g., \u201cI\u2019m curious about quantum computing. Can you tell me the basics of how it works.\u201d). See Appendix A.2  ###reference_### for further details.\nPersona Generation. We generate personas  with GPT-4 by few-shot () prompting with randomly sampled personas from a set of  content-filtered personas222We hand-selected personas that did not contain references to violence, profanity, or content violations. from the PRODIGy dataset (Occhipinti et al., 2023  ###reference_b19###). We generated a total of  personas and split personas into two train splits (, ) each with  personas, and one test split of . Example personas and prompts are provided in Appendix A.3  ###reference_###.\nGold Responses. To generate a gold response  for each  pair, we prompted an Oracle (GPT-4). Specifically, we provided the persona  followed by the task , without any dialog history, and prompted the Oracle to generate a personalized response that completes the task with respect to the persona profile. This process resulted in a total of  task--persona--gold-response triples (250 x 50 + 250 x 50 + 50 x 10). Prompt details and examples are provided in Appendix A.4  ###reference_###."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Evaluation and Results",
            "text": "We evaluate the performance of the Questioner  at each iteration  using two metrics: log-probabilities of generating the gold responses and win rates.\nModels. We use mistral-7b-instruct as our Questioner. We chose mistral-7b-instruct, a 7B-parameter model, because its weights are openly available and it has been shown to be one of the best models for its size (Jiang et al., 2023  ###reference_b13###), outperforming larger models such as llama-13B-chat (Touvron et al., 2023  ###reference_b33###) on benchmarks like MT-Bench (Zheng et al., 2024  ###reference_b40###). We use GPT-4 (OpenAI, 2023  ###reference_b20###, gpt-4-0613 snapshot) as our Oracle, as at the time of generating our dataset, GPT-4 was the most capable model available. For the Roleplayer, we use mixtral-8x7b-instruct (Jiang et al., 2024  ###reference_b14###)."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Gold Log-probabilities",
            "text": "Our main training objective is learning to elicit information that increases the log probability of the gold responses according to the initial (pretrained and instruction-finetuned) model . For our evaluations, we thus first compute the log probability of gold responses , conditioned on simulated conversations  generated by the current model  and fixed roleplayer , for a held-out test set of tasks  and personas .\nWe calculate log probabilities for four conditions:\nNegative Control: , performance of the pretrained model without any information about the Roleplayer (persona or elicited),\nPositive Control: , performance of the pretrained model given oracular information about the persona,\nQ-Experimental: , evaluation of the STaR-GATE finetuned model,\nQ-Random: , a baseline that randomizes persona info used in answering elicitation questions ( indicates a random different test persona).\nIn prompting both Q-Random and Q-Experimental (the main condition), we repeat the task text  at the end of the conversation to prompt a final response instead of asking another elicitation question. The purpose of the Q-Random baseline is to isolate the relevance of persona-specific information from generally informative information elicited from the Roleplayer.\nOur results show that log probabilities of the gold response increase over iterations for the Q-Experimental condition (Figure 4  ###reference_###a). We observe a similar trend (however, with much lower log probabilities) for the Q-Random baseline. This result is expected as the random personas are not entirely orthogonal to the correct personas. For example, eliciting preferences from June---who is a small bistro owner and enjoys art and music---might also reveal information that is relevant to Reece---who enjoys vintage jazz and cooking (see \u00a7 A.3  ###reference_###). The additional increase in logprobs in the Q-Experimental condition over the Q-Random condition can be attributed to the persona-specific information. For the Q-Experimental/Random conditions, each data point for the log probabilities is calculated using 10 simulated conversations for each of the 10 x 50 persona-task prompts, resulting in a total of 5000 responses (which is why the error bars are small). See \u00a7 A.1  ###reference_### for figures including log probabilities for the positive control condition.\n###figure_4###"
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Win Rates",
            "text": "The primary goal of asking questions is to generate high-quality answers, not just to assign high probability to known, good answers. To evaluate this, we compared the responses from the STaR-GATE model, , to those from the initial model, . For each  pair, we used model  to generate a response  at temperature , conditioned on a randomly sampled conversation  at temperature . We then prompted GPT-4 to choose the more suitable response for task  and persona , following the evaluation protocol of Rafailov et al. (2024  ###reference_b25###). Specifically, GPT-4 was asked to select between  and  (see Figure 16  ###reference_###). To mitigate order effects (Wang et al., 2023  ###reference_b34###), we randomized the order of the responses. Due to the uniform sampling of turn lengths, each turn length has approximately 166  pairs in total. Consequently, each data point for the win rates is an average of 300 values.333GPT-4\u2019s content filter rejected 1-2% of requests, resulting in average values between 293 and 300 for each data point. Our results show that win rates for STaR-GATE increase over iterations (see Figure 3  ###reference_###b), reaching a maximum win rate of  \n\n\n72%\n after two iterations."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Ablations",
            "text": "We perform several ablations to study the effect of different design choices on the performance of STaR-GATE.\n###figure_5### Roleplayer Robustness. To investigate the effect of Roleplayer capability on the Questioner\u2019s performance, we conducted evaluations with different Roleplayer models: mistral-8x7b-instruct, mistral-7b-instruct, and gemma-7b-instruct (Mesnard et al., 2024  ###reference_b18###). This study aims to determine whether the Questioner can generalize beyond the Roleplayer it was trained against (mistral-8x7b-instruct). The robustness results show that when using mistral-7b-instruct as the Roleplayer, STaR-GATE achieves a slightly lower win rate of  \n\n\n65%\n after two iterations. When gemma-7b-instruct is used as the Roleplayer, the win rates peak at  \n\n\n62%\n after one iteration. This result shows that STaR-GATE can generalize to different Roleplayers, though with slightly lower performance (see Figure 3  ###reference_###b).\nTraining Ablations. To demonstrate the importance of regularization during training (i.e., sampling responses  and finetuning on these; for details see Algorithm 1  ###reference_###), we additionally run an ablation in which we only finetune on questions  but not responses  (see Figure 2  ###reference_###, for an illustration). We expect that this ablation (STaR-GATE w/o Regularization) decreases win rates, as the Questioner  might forget how to respond and instead always asks questions. Finally, we include an ablation in which we finetune on the gold responses  instead of the sampled responses . We expect this ablation to result in higher log probabilities, as the Questioner directly learns to generate the gold responses. However, we also expect this to lead to hallucination during the generation of responses, as the Questioner will have seen information from gold responses that was not present in the elicited preferences (since the gold responses come from an Oracle that sees the complete persona). We denote this ablation as STaR-GATE w/ Gold Response. Win rates for both STaR-GATE w/o Regularization and STaR-GATE w/ Gold Response are shown in Figure Figure 3  ###reference_###a. As expected, STaR-GATE w/o Regularization decreases win rates over iterations, as finetuning on questions alone yields a model that forgets how to respond (see \u00a7 A.5  ###reference_###, for an example). For STaR-GATE w/ Gold Response, win rates initially decrease and then converge to  \n\n\n50%\n. We attribute this to hallucination in responses that were not aligned with the elicited responses (see \u00a7 A.5  ###reference_###). Log probabilities for STaR-GATE w/o Regularization are slightly lower compared to STaR-GATE (Figure 4  ###reference_###b), while log probabilities for STaR-GATE w/ Gold Response were slightly higher (Figure 5  ###reference_###a).\nResponse Model. We finally run an additional win rate evaluation in which we report GPT-4 win rates for responses generated by model  conditional on conversation elicited from model  over responses generated by the initial model  conditional on information elicited by  (see Figure 5  ###reference_###b). The purpose of this condition was to understand whether the initial model would benefit from the conversation history in the same way the STaR-GATE finetuned model would. While we found a slight increase in win rates up to  \n\n\n57%\n after two iterations, win rates eventually reversed to 50% at iteration three. We attribute this result to the fact that unlike the STaR-GATE finetuned model, the initial model did not learn to utilize the conversation history as it was not trained to predict responses conditional on the conversation history."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Limitations and Future Work",
            "text": "One important limitation of our work is that it depends on gold responses (i.e., labels). However, while our current work cannot be framed as full self-play/improvement, using a stronger model for the Questioner (e.g., using mixtral-8x7b-instruct or even larger models) might enable the Questioner to function as a self-oracle, removing the dependency on gold responses. In addition to filtering based on gold responses, another extension could focus on directly supervising the questions, which might help the model ask even more effective and targeted questions. Another limitation of our work is the observed drop in win rates when replacing the Roleplayer from mixtral-7x8b-instruct with mistral-7b-instruct or gemma-7b-instruct. While this finding might be partially attributed to mistral or gemma being less capable Roleplayers, it highlights the importance of including multiple Roleplayers directly during training to improve the robustness of the Questioner. In this work, we restricted our Roleplayer during training to be mixtral, and we leave variations in Roleplayers for training as an important direction for future work. Finally, future work could also explore alternative ways to optimize our objective, such as using REINFORCE (Williams, 1992  ###reference_b35###) combined with variance reduction techniques as in Zelikman et al. (2024  ###reference_b38###) and Hoffman et al. (2024  ###reference_b10###)."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In summary, our results demonstrate that STaR-GATE can significantly enhance a model\u2019s ability to engage in effective dialog through targeted questioning. This finding is particularly relevant considering recent assessments suggesting that alignment strategies such as RLHF may inadvertently limit a model\u2019s capacity for engaging in effective dialog (Shaikh et al., 2023  ###reference_b27###). Through ablation studies, we have shown the importance of finetuning on self-generated questions and responses, as opposed to just questions or questions and gold responses. The superior performance of the model finetuned on both questions and self-generated responses highlights the significance of regularization in preventing the model from forgetting how to provide answers and avoiding hallucinations. Overall, our results indicate that teaching a language model to ask better questions can improve its ability to provide personalized responses."
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "Acknowledgements",
            "text": "We particularly thank Eric Zelikman, Ben Prystawski and Omar Shaikh for their helpful and detailed comments, as well as Rafael Rafailov, Michael Li, Violet Xiang, and Kanishk Gandhi for useful discussions. In addition, we would like to acknowledge that this work was supported by the Microsoft AFMR program and are grateful for Hassan Teimoori and the wider AFMR team\u2019s support."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A Appendix",
            "text": ""
        }
    ],
    "tables": {},
    "image_paths": {
        "1": {
            "figure_path": "2403.19154v2_figure_1.png",
            "caption": "Figure 1: Problem Illustration. When user preferences are unknown, language models may respond ineffectively. By asking questions, models can elicit information and provide more effective responses."
        },
        "2": {
            "figure_path": "2403.19154v2_figure_2.png",
            "caption": "Figure 2: Overview of STaR-GATE. A task is given to a Questioner who elicits preferences from a Roleplayer whose persona is unknown to the Questioner. The resulting conversations are then filtered based on the log probability of a gold response generated by an Oracle which has access to the Roleplayer\u2019s persona (omitted from the diagram for clarity). We then fine-tune the Questioner on the filtered questions. Moreover\u2014to avoid distribution shift\u2014we regularize the Questioner by additionally sampling responses conditioned on the filtered conversations. In our ablations, we contrast fine-tuning on sampled responses with fine-tuning on the gold responses."
        },
        "3": {
            "figure_path": "2403.19154v2_figure_3.png",
            "caption": "Figure 3: Win Rates Against Initial Model. [a] Complete method and ablations: w/o Reg. refers to finetuning on questions only, which decreases the model\u2019s ability to generate answers. w/ Gold Resp. refers to finetuning directly on the gold responses rather than self-generated responses, which leads to hallucinations in generated answers. [b] Roleplayer generalization results. We demonstrate that STaR-GATE generalizes beyond the roleplayer it was trained against (mixtral-8x7b). All three roleplayers correspond to the instruct version of their respective models. Error bars represent the standard error of the mean (\u00b1plus-or-minus\\pm\u00b1 SEM). We include 0.50.50.50.5 (chance) as a reference point for iteration t=0\ud835\udc610t=0italic_t = 0."
        },
        "4": {
            "figure_path": "2403.19154v2_figure_4.png",
            "caption": "Figure 4: Log Probability of Gold Responses. Log probabilities of gold responses increase over iterations for both [a] STaR-GATE and [b] STaR-GATE w/o Regularization. Error bars correspond to \u00b1plus-or-minus\\pm\u00b1 SEM calculated across held-out persona-task prompts."
        },
        "5": {
            "figure_path": "2403.19154v2_figure_5.png",
            "caption": "Figure 5: Additional Ablation Results. [a] Log probability of gold responses for STaR-GATE w/ gold response. [b] Win Rates for STaR-GATE using QB\u2062A\u2062S\u2062Esubscript\ud835\udc44\ud835\udc35\ud835\udc34\ud835\udc46\ud835\udc38Q_{BASE}italic_Q start_POSTSUBSCRIPT italic_B italic_A italic_S italic_E end_POSTSUBSCRIPT to generate responses at each iteration. Error bars correspond to \u00b1plus-or-minus\\pm\u00b1 SEM."
        },
        "6": {
            "figure_path": "2403.19154v2_figure_6.png",
            "caption": "Figure 6: Log Probability of Gold Responses Including Pos. Control. Log probabilities of gold responses increase over iterations for both [a] STaR-GATE and [b] STaR-GATE w/o Regularization. Error bars correspond to \u00b1plus-or-minus\\pm\u00b1 SEM calculated across held-out persona-task prompts."
        },
        "7": {
            "figure_path": "2403.19154v2_figure_7.png",
            "caption": "Figure 7: Log Probability of Gold Responses. STaR-GATE w/ Gold Response. Error bars correspond to \u00b1plus-or-minus\\pm\u00b1 SEM calculated across held-out persona-task prompts."
        }
    },
    "references": [
        {
            "1": {
                "title": "Thinking fast and slow with deep learning and tree search.",
                "author": "Thomas Anthony, Zheng Tian, and David Barber.",
                "venue": "Advances in neural information processing systems, 30, 2017.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Constitutional ai: Harmlessness from ai feedback.",
                "author": "Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al.",
                "venue": "arXiv preprint arXiv:2212.08073, 2022.",
                "url": null
            }
        },
        {
            "3": {
                "title": "Language models are few-shot learners.",
                "author": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.",
                "venue": "Advances in neural information processing systems, 33:1877--1901, 2020.",
                "url": null
            }
        },
        {
            "4": {
                "title": "Deep reinforcement learning from human preferences.",
                "author": "Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei.",
                "venue": "Advances in neural information processing systems, 30, 2017.",
                "url": null
            }
        },
        {
            "5": {
                "title": "Kto: Model alignment as prospect theoretic optimization.",
                "author": "Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela.",
                "venue": "arXiv preprint arXiv:2402.01306, 2024.",
                "url": null
            }
        },
        {
            "6": {
                "title": "Probabilistic model-agnostic meta-learning.",
                "author": "Chelsea Finn, Kelvin Xu, and Sergey Levine.",
                "venue": "Advances in neural information processing systems, 31, 2018.",
                "url": null
            }
        },
        {
            "7": {
                "title": "Social contract ai: Aligning ai assistants with implicit group norms.",
                "author": "Jan-Philipp Fr\u00e4nken, Sam Kwok, Peixuan Ye, Kanishk Gandhi, Dilip Arumugam, Jared Moore, Alex Tamkin, Tobias Gerstenberg, and Noah D Goodman.",
                "venue": "arXiv preprint arXiv:2310.17769, 2023.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Strategic reasoning with language models.",
                "author": "Kanishk Gandhi, Dorsa Sadigh, and Noah D Goodman.",
                "venue": "arXiv preprint arXiv:2305.19165, 2023.",
                "url": null
            }
        },
        {
            "9": {
                "title": "Bayesian preference elicitation with language models.",
                "author": "Kunal Handa, Yarin Gal, Ellie Pavlick, Noah Goodman, Jacob Andreas, Alex Tamkin, and Belinda Z Li.",
                "venue": "arXiv preprint arXiv:2403.05534, 2024.",
                "url": null
            }
        },
        {
            "10": {
                "title": "Training chain-of-thought via latent-variable inference.",
                "author": "Matthew Douglas Hoffman, Du Phan, David Dohan, Sholto Douglas, Tuan Anh Le, Aaron Parisi, Pavel Sountsov, Charles Sutton, Sharad Vikram, and Rif A Saurous.",
                "venue": "Advances in Neural Information Processing Systems, 36, 2024.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Zero-shot goal-directed dialogue via rl on imagined conversations.",
                "author": "Joey Hong, Sergey Levine, and Anca Dragan.",
                "venue": "arXiv preprint arXiv:2311.05584, 2023.",
                "url": null
            }
        },
        {
            "12": {
                "title": "V-star: Training verifiers for self-taught reasoners.",
                "author": "Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, and Rishabh Agarwal.",
                "venue": "arXiv preprint arXiv:2402.06457, 2024.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Mistral 7b.",
                "author": "Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al.",
                "venue": "arXiv preprint arXiv:2310.06825, 2023.",
                "url": null
            }
        },
        {
            "14": {
                "title": "Mixtral of experts.",
                "author": "Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al.",
                "venue": "arXiv preprint arXiv:2401.04088, 2024.",
                "url": null
            }
        },
        {
            "15": {
                "title": "Chatgpt for good? on opportunities and challenges of large language models for education.",
                "author": "Enkelejda Kasneci, Kathrin Se\u00dfler, Stefan K\u00fcchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan G\u00fcnnemann, Eyke H\u00fcllermeier, et al.",
                "venue": "Learning and individual differences, 103:102274, 2023.",
                "url": null
            }
        },
        {
            "16": {
                "title": "Eliciting human preferences with language models.",
                "author": "Belinda Z Li, Alex Tamkin, Noah Goodman, and Jacob Andreas.",
                "venue": "arXiv preprint arXiv:2310.11589, 2023.",
                "url": null
            }
        },
        {
            "17": {
                "title": "Decision-oriented dialogue for human-ai collaboration.",
                "author": "Jessy Lin, Nicholas Tomlin, Jacob Andreas, and Jason Eisner.",
                "venue": "arXiv preprint arXiv:2305.20076, 2023.",
                "url": null
            }
        },
        {
            "18": {
                "title": "Gemma: Open models based on gemini research and technology.",
                "author": "Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi\u00e8re, Mihir Sanjay Kale, Juliette Love, et al.",
                "venue": "arXiv preprint arXiv:2403.08295, 2024.",
                "url": null
            }
        },
        {
            "19": {
                "title": "Prodigy: a profile-based dialogue generation dataset, 2023.",
                "author": "Daniela Occhipinti, Serra Sinem Tekiroglu, and Marco Guerini.",
                "venue": null,
                "url": null
            }
        },
        {
            "20": {
                "title": "GPT-4 Technical Report.",
                "author": "OpenAI.",
                "venue": "arXiv preprint arXiv:2303.08774, 2023.",
                "url": null
            }
        },
        {
            "21": {
                "title": "Active preference inference using language models and probabilistic reasoning.",
                "author": "Top Piriyakulkij, Volodymyr Kuleshov, and Kevin Ellis.",
                "venue": "arXiv preprint arXiv:2312.12009, 2023.",
                "url": null
            }
        },
        {
            "22": {
                "title": "Certified deductive reasoning with language models.",
                "author": "Gabriel Poesia, Kanishk Gandhi, Eric Zelikman, and Noah Goodman.",
                "venue": "2023.",
                "url": null
            }
        },
        {
            "23": {
                "title": "Autoact: Automatic agent learning from scratch via self-planning.",
                "author": "Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou, Yuchen Eleanor Jiang, Chengfei Lv, and Huajun Chen.",
                "venue": "arXiv preprint arXiv:2401.05268, 2024.",
                "url": null
            }
        },
        {
            "24": {
                "title": "Language models are unsupervised multitask learners.",
                "author": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.",
                "venue": "OpenAI blog, 1(8):9, 2019.",
                "url": null
            }
        },
        {
            "25": {
                "title": "Direct preference optimization: Your language model is secretly a reward model.",
                "author": "Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn.",
                "venue": "Advances in Neural Information Processing Systems, 36, 2024.",
                "url": null
            }
        },
        {
            "26": {
                "title": "Explain yourself! leveraging language models for commonsense reasoning.",
                "author": "Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher.",
                "venue": "arXiv preprint arXiv:1906.02361, 2019.",
                "url": null
            }
        },
        {
            "27": {
                "title": "Grounding or guesswork? large language models are presumptive grounders.",
                "author": "Omar Shaikh, Kristina Gligori\u0107, Ashna Khetan, Matthias Gerstgrasser, Diyi Yang, and Dan Jurafsky.",
                "venue": "arXiv preprint arXiv:2311.09144, 2023.",
                "url": null
            }
        },
        {
            "28": {
                "title": "Reflexion: Language agents with verbal reinforcement learning.",
                "author": "Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao.",
                "venue": "Advances in Neural Information Processing Systems, 36, 2024.",
                "url": null
            }
        },
        {
            "29": {
                "title": "Mastering chess and shogi by self-play with a general reinforcement learning algorithm.",
                "author": "David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al.",
                "venue": "arXiv preprint arXiv:1712.01815, 2017.",
                "url": null
            }
        },
        {
            "30": {
                "title": "Commonsenseqa: A question answering challenge targeting commonsense knowledge.",
                "author": "Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant.",
                "venue": "arXiv preprint arXiv:1811.00937, 2018.",
                "url": null
            }
        },
        {
            "31": {
                "title": "Task ambiguity in humans and language models.",
                "author": "Alex Tamkin, Kunal Handa, Avash Shrestha, and Noah Goodman.",
                "venue": "arXiv preprint arXiv:2212.10711, 2022.",
                "url": null
            }
        },
        {
            "32": {
                "title": "Large language models in medicine.",
                "author": "Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and Daniel Shu Wei Ting.",
                "venue": "Nature medicine, 29(8):1930--1940, 2023.",
                "url": null
            }
        },
        {
            "33": {
                "title": "Llama 2: Open foundation and fine-tuned chat models.",
                "author": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.",
                "venue": "arXiv preprint arXiv:2307.09288, 2023.",
                "url": null
            }
        },
        {
            "34": {
                "title": "Large language models are not fair evaluators.",
                "author": "Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui.",
                "venue": "arXiv preprint arXiv:2305.17926, 2023.",
                "url": null
            }
        },
        {
            "35": {
                "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning.",
                "author": "Ronald J Williams.",
                "venue": "Machine learning, 8:229--256, 1992.",
                "url": null
            }
        },
        {
            "36": {
                "title": "Webshop: Towards scalable real-world web interaction with grounded language agents.",
                "author": "Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan.",
                "venue": "Advances in Neural Information Processing Systems, 35:20744--20757, 2022.",
                "url": null
            }
        },
        {
            "37": {
                "title": "Star: Bootstrapping reasoning with reasoning.",
                "author": "Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman.",
                "venue": "Advances in Neural Information Processing Systems, 35:15476--15488, 2022.",
                "url": null
            }
        },
        {
            "38": {
                "title": "Quiet-star: Language models can teach themselves to think before speaking.",
                "author": "Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah D Goodman.",
                "venue": "arXiv preprint arXiv:2403.09629, 2024.",
                "url": null
            }
        },
        {
            "39": {
                "title": "In-context principle learning from mistakes.",
                "author": "Tianjun Zhang, Aman Madaan, Luyu Gao, Steven Zheng, Swaroop Mishra, Yiming Yang, Niket Tandon, and Uri Alon.",
                "venue": "arXiv preprint arXiv:2402.05403, 2024.",
                "url": null
            }
        },
        {
            "40": {
                "title": "Judging llm-as-a-judge with mt-bench and chatbot arena.",
                "author": "Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al.",
                "venue": "Advances in Neural Information Processing Systems, 36, 2024.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.19154v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2",
            "2.3"
        ],
        "methodology_sections": [
            "3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "5",
            "5.1",
            "5.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "6"
        ]
    },
    "research_context": {
        "paper_id": "2403.19154v2",
        "paper_title": "STaR-GATE: Teaching Language Models to Ask Clarifying Questions",
        "research_background": "### Introduction:\n\n**Paper\u2019s Motivation:**\nWhen language models (LMs) interact with users who have diverse preferences, they often face task ambiguity. This ambiguity arises because the same request can correspond to different tasks depending on the user's preferences. High-stakes applications like healthcare or education magnify the need for personalized, preference-aware responses. \n\n**Research Problem:**\nThe core issue addressed by this paper is how to resolve task ambiguity in LMs to tailor their responses to individual user preferences. The challenge lies in guiding models to ask relevant clarifying questions when users' preferences are not explicitly specified in their prompts.\n\n### Related Work:\n1. **Prompt-based Question Generation:** Previous work like Li et al. (2023) and Piriyakulkij et al. (2023) has involved prompting closed-source LMs for useful questions. This approach, however, lacks flexibility and often results in ineffective or irrelevant questions.\n   \n2. **Alignment Strategies:** Strategies such as Reinforcement Learning from Human Feedback (RLHF) can hinder a model's ability to engage in effective dialog (Shaikh et al., 2023).\n\n3. **Optimal Experimental Design:** Handa et al. (2024) combined elicitation with experimental design, but their method was limited to pairwise comparisons, restricting the range of questions used.\n\n4. **Offline Reinforcement Learning:** Work by Hong et al. (2023) suggested using offline reinforcement learning to promote better dialog. However, this approach necessitated pre-generated high-quality dialog from expert models and did not focus on questions for preference elicitation.\n\n**Proposed Solution:**\nThis paper introduces STaR-GATE, an algorithm that iteratively improves a LM's capability to ask useful questions through a self-improvement loop inspired by active preference elicitation and self-play methods. \n\n**Contributions:**\n1. **Algorithm Introduction:** Introducing STaR-GATE, a method to enhance a LM's ability to elicit user preferences via iterative questioning.\n2. **Synthetic Dataset:** Creation of a dataset with 25,500 persona-task prompts paired with personalized, benchmark responses.\n3. **Performance Improvement:** Demonstration that STaR-GATE fine-tuning increases the likelihood of generating ideal responses.\n4. **Response Regularization:** Introduction of response-regularization within STaR-GATE to leverage elicited preferences for improved response quality.\n5. **Generalization:** Evidence that the fine-tuned model generalizes beyond the roleplayer it was trained with.",
        "methodology": "### STaR-GATE: Teaching Language Models to Ask Clarifying Questions\n\n#### Methodology\n\n#### Overview\nSTaR-GATE is a technique designed to train language models to ask clarifying questions in conversations. It starts with persona-task prompts and generates gold responses using an Oracle that has full access to both the persona and the task. The methodology involves simulating conversations between a Questioner, which needs to ask questions to gather useful information, and a human Roleplayer who has access to the persona but not the task. Usefulness is measured by the log probability of the gold response, given the questions asked by the Questioner and the preferences elicited from the Roleplayer.\n\n#### Objective\nDefine \\(Q\\) as the Questioner (policy to be trained), \\(R\\) as the Roleplayer model, \\(T\\) as the set of tasks, \\(P\\) as the set of user personas, and \\(O\\) as the Oracle model. For a given task \\(t \\in T\\) and a persona \\(p \\in P\\), the Oracle \\(O\\) generates a gold response \\(y_t^p\\). The STaR-GATE objective is to maximize the expected log probability that the pretrained model \\(M\\) assigns to the gold response \\(y_t^p\\), given the task \\(t\\) and a simulated conversation \\(h\\) between \\(Q\\) and \\(R\\):\n\n\\[ \\mathbb{E}[\\log P_M(y_t^p | t, h)] \\]\n\nwhere \\(h\\) is a conversation with questions distributed according to \\(Q\\) and answers according to \\(R\\).\n\n#### Optimization\nThe optimization of Equation 1 can be approached in multiple ways. The methodology follows a simple variant of Expert Iteration, as per Zelikman et al. (2022), where on each iteration \\(i\\), for each pair \\((t, p)\\), multiple conversation trajectories are sampled using the current \\(Q_i\\). The top-\\(\\kappa\\) trajectories are then selected for supervised fine-tuning from the initial \\(M\\).\n\n#### Regularization\nA notable failure mode in optimizing the stated objective is the model's tendency to forget how to respond while focusing on asking questions. To prevent this, a regularization term is introduced to restrict the distribution of responses from diverging too far from previous iterations:\n\n\\[ \\rho(h, Q_{i}) \\]\n\nThis regularization involves sampling a response at temperature \\(\\tau\\) from the previous policy \\(Q_{i-1}\\) for each task and persona pair, conditioned on the best conversation history \\(\\sigma_{i-1}\\), then incorporating this response into the fine-tuning phase.\n\n#### Algorithm\nThe outline of STaR-GATE includes the following steps:\n\n1. Perform expert iteration by training the initial model \\(M_0\\) for \\(N\\) iterations (set to 60 for all experiments) on question-response pairs generated from intermediate models \\(Q_i\\).\n2. Alternate between task splits \\(T_1, T_2\\) and persona splits \\(P_1, P_2\\) to avoid overlap in training data.\n3. For each task \\(t\\) and user persona \\(p\\), simulate \\(C\\) conversations (set to 128 for experiments), each with a maximum of \\(K\\) total turns (set to 5 turns for experiments).\n4. Sample \\(Q_i\\) at each turn \\(k\\) from the previous \\(Q_{i-1}\\) and a fixed Roleplayer \\(R\\).\n5. Distribute conversation lengths uniformly to prevent overfitting on a specific conversation length.\n6. Select the best simulated conversations based on the objective \\(\\mathbb{E}[\\log P_M(y_t^p | t, h)]\\) for fine-tuning the next iteration.\n\nFinally, fine-tune the initial model \\(M_0\\) on both the selected conversations \\(\\sigma_i'\\) and greedily sampled responses \\(G_{\\sigma_{i-1}}\\), ensuring the model learns to ask informative questions and provide tailored responses. Importantly, mask the answers provided by \\(R\\) from the loss, focusing the training on enhancing question and response generation policies.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n**Objective:**  \nEvaluate STaR-GATE's ability to enhance the Questioner's performance in generating clarifying questions and response generation across diverse everyday tasks.\n\n**Datasets:**  \n- **Instruct-human-assistant-prompt Dataset:**  \n  - **Scope:** Broad range of queries including topics such as food, career, and education.\n  - **Size and Splits:** First 550 conversations were selected, divided into two training splits and one test split.\n  - **Data Selection:** Only human queries were considered for simulations, excluding assistant responses.\n\n**Persona Generation:**  \n- **Method:** Generated personas using GPT-4 with a few-shot prompting technique.\n- **Source:** Sampled from the PRODIGy dataset, with hand-selection to filter out inappropriate content.\n- **Splits:** 500 personas in total, divided into two training splits and one test split.\n\n**Gold Responses:**  \n- **Process:** Prompted an Oracle (GPT-4) to generate personalized responses for each persona-task pair without any dialog history.\n- **Outcome:** Generated task-persona-gold-response triples based on the training and test splits.\n\n**Baselines:**  \n- **Initial Model:** The baseline model includes both a pretrained state and a version that is fine-tuned with instruction-based data.\n  \n**Evaluation Metrics:**  \n1. **Log-Probability of Gold Responses:** Measures the likelihood of the model generating the correct response.\n2. **Win Rates:** Comparison metric to determine how often the model's response is preferable over the baseline.\n\n### Main Experimental Results\n\n- **Increase in Log-Probability:** Training with STaR-GATE improves the log-probability of gold responses compared to the initial pretrained and instruction-fine-tuned model.\n- **Higher Win Rates:** STaR-GATE achieves better win rates, showing that it generates more preferred responses relative to the baseline models.\n\nThe results indicate that STaR-GATE is effective in enhancing the model's capability to ask clarifying questions and generate contextually appropriate responses. Full experimental details and replication code are available at the specified GitHub repository."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To investigate the effect of Roleplayer capability on the Questioner's performance and determine whether the Questioner can generalize beyond the Roleplayer it was trained against.",
            "experiment_process": "Evaluations were conducted with different Roleplayer models: mistral-8x7b-instruct, mistral-7b-instruct, and gemma-7b-instruct. The Questioner was trained against mistral-8x7b-instruct, and its performance was tested using the other Roleplayer models.",
            "result_discussion": "When using mistral-7b-instruct, STaR-GATE achieved a slightly lower win rate of 65% after two iterations. With gemma-7b-instruct, the win rates peaked at 62% after one iteration. This shows that STaR-GATE can generalize to different Roleplayers, albeit with slightly lower performance.",
            "ablation_id": "2403.19154v2.No1"
        },
        {
            "research_objective": "To demonstrate the importance of regularization during training by comparing performance when finetuning on both questions and responses versus questions alone or using gold responses.",
            "experiment_process": "An ablation study was carried out with three conditions: (1) standard STaR-GATE, (2) STaR-GATE without regularization (only finetuning on questions), and (3) STaR-GATE with gold responses instead of sampled responses. Win rates and log probabilities were measured for each setup over iterations.",
            "result_discussion": "STaR-GATE without regularization showed a decrease in win rates, indicating that finetuning on questions alone leads to a model that forgets how to respond. STaR-GATE with gold responses initially showed a decrease in win rates, which then converged to 50%, attributed to hallucination in the responses. Log probabilities were slightly lower for the regularization-omitted model and slightly higher for the gold response model.",
            "ablation_id": "2403.19154v2.No2"
        },
        {
            "research_objective": "To understand whether the initial model benefits from conversation history in the same way the STaR-GATE finetuned model does.",
            "experiment_process": "An evaluation was conducted where GPT-4 win rates were reported for responses generated by the model conditional on conversation elicited from the same model, compared to responses generated by the initial model conditional on information elicited by it. This was done over several iterations.",
            "result_discussion": "There was a slight increase in win rates up to 57% after two iterations, but win rates reversed to 50% at iteration three. This indicates that, unlike the STaR-GATE finetuned model, the initial model did not effectively learn to utilize conversation history as it was not trained for this purpose.",
            "ablation_id": "2403.19154v2.No3"
        }
    ]
}