{
    "title": "MetaIE: Distilling a Meta Model from LLM for All Kinds of Information Extraction Tasks",
    "abstract": "Information extraction (IE) is a fundamental area in natural language processing where prompting large language models (LLMs), even with in-context examples, cannot defeat small LMs tuned on very small IE datasets.\nWe observe that IE tasks, such as named entity recognition and relation extraction, all focus on extracting important information, which can be formalized as a label-to-span matching.\nIn this paper, we propose a novel framework MetaIE to build a small LM as meta-model by learning to extract \u201cimportant information\u201d, i.e., the meta-understanding of IE, so that this meta-model can be adapted to all kind of IE tasks effectively and efficiently.\nSpecifically, MetaIE obtains the small LM via a symbolic distillation from an LLM following the label-to-span scheme.\nWe construct the distillation dataset via sampling sentences from language model pre-training datasets (e.g., OpenWebText in our implementation) and prompting an LLM to identify the typed spans of \u201cimportant information\u201d.\nWe evaluate the meta-model under the few-shot adaptation setting.\nExtensive results on 13 datasets from 6 IE tasks\nconfirm that\nMetaIE can offer a better starting point for few-shot tuning on IE datasets\nand outperform other meta-models from\n(1) vanilla language model pre-training,\n(2) multi-IE-task pre-training with human annotations,\nand (3) single-IE-task symbolic distillation from LLM.\nMoreover, we provide comprehensive analyses of MetaIE, such as the size of the distillation dataset, the meta-model architecture, and the size of the meta-model.111Code, datasets, and model checkpoints: https://github.com/KomeijiForce/MetaIE.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large language models (LLMs), such as ChatGPT (OpenAI, 2023  ###reference_b31###), benefit from vast amount of training data and have demonstrated exceptional performance across various areas through in-context learning (ICL) (Dong et al., 2023  ###reference_b7###).\nHowever, when it comes to information extraction (IE), LLMs, even with ICL examples, struggle to compete with smaller LMs (e.g., BERT (Devlin et al., 2019  ###reference_b5###) and RoBERTa (Liu et al., 2019  ###reference_b24###)) fine-tuned on very small training sets (Peng et al., 2023  ###reference_b33###; Wadhwa et al., 2023  ###reference_b39###; Gao et al., 2024  ###reference_b13###).\nThis is usually regarded as a limitation of LLMs in following a specific extraction scheme (Xu et al., 2023  ###reference_b43###).\nMeanwhile, it is worth mentioning that conducting auto-regressive inference with LLMs is expensive and time-consuming, hindering their application in conducting IE over large corpora.\nWe observe that IE tasks, such as named entity recognition (NER) and relation extraction (RE), all focus on extracting important information, which can be formalized as label-to-span instructions.\nSpecifically, all IE tasks can be decomposed as several instructions such as \u201cgiven an IE label (), extract a span from the input text\u201d (Figure 1  ###reference_###), where  can be (1) Person, Location, Organization in NER to recognize entities or (2) Tom births at in RE to verify if there is a certain relation between two entities by checking the other entity can be recognized or not.\nFollowing these label-to-span instructions, LLMs can handle all kinds of IE tasks and return imperfect yet semantically reasonable answers.\nTo this end, we argue that LLMs can be distilled into meta-models for IE which can quickly fine-tuned on few-shot training sets for better task-specific performance.\n###figure_1### In this paper, we propose a novel framework MetaIE to build a small LM as a meta-model by learning to extract \u201cimportant information\u201d, i.e., the meta-understanding of IE, and we show that this meta-model can be adapted to all kind of IE tasks effectively and efficiently.\nSome prior work have built meta-models for a specific IE tasks, e.g., UniversalNER (Zhou et al., 2023  ###reference_b46###) explores the potential of building a meta-model for NER tasks.\nOur work is more ambitious at a larger scope for all IE tasks.\nMetaIE obtains the small LM via a symbolic distillation (West et al., 2022  ###reference_b41###) from an LLM following the label-to-span scheme.\nWe construct the distillation dataset via sampling sentences from language model pre-training datasets and prompting an LLM to identify the typed spans of \u201cimportant information\u201d.\nIn particular, we implement this idea with  sentences from the OpenWebText corpus (Gokaslan & Cohen, 2019  ###reference_b14###), which contains various webpage texts and is also a subset of the popular language model pre-training dataset.\nWe feed these sentences to GPT-3.5-turbo for identifying \u201cimportant information\u201d, which is then used to distill small LMs.\nIt is worth mentioning that MetaIE is applicable to all types of small LMs and one only needs to convert the label-span pairs following the corresponding labeling scheme (e.g., BIO sequence labeling for encoders like RoBERTa, seq2seq labeling for encoder-decoders like BART).\nOur evaluation focuses on the few-shot learning ability of the meta-model for different IE tasks.\nWe mainly compare MetaIE with meta-models from (1) vanilla language model pre-training, (2) multi-IE-task pre-training with human annotations, and (3) single-IE-task symbolic distillation from LLM.\nLarge-scale datasets for NER, RE, and event extraction (EE) tasks are used in single-IE-task and multi-IE-task pre-training, therefore, these datasets shall be considered as in-task-distributional for these two methods.\nFor a more comprehensive evaluation, we further include out-of-task-distributional datasets from (1) semantic role labeling (SRL) (Carreras & M\u00e0rquez, 2005  ###reference_b3###), (2) aspect-based sentiment analysis (ABSA) (Pontiki et al., 2014  ###reference_b34###), and (3) aspect-sentiment triplet extraction (ASTE) (Xu et al., 2020  ###reference_b44###), totaling 13 datasets across 6 IE tasks.\nIn our experiments,\nMetaIE generally achieves the best performance, only very occasionally losing to task-specific distillation on some in-task-distributional datasets.\nThis demonstrates that MetaIE is a strong and efficient method to distill the meta-understanding of IE from LLMs into small LMs.\nRemarkably, distilling from the LLM-produced dataset following the traditional human annotation schemes performs poorly.\nTherefore, the success of MetaIE, rather than from purely using LLMs, shall also come from our label-to-span scheme.\nWe have conducted comprehensive analyses of MetaIE.\nWe study the scaling-up rules to investigate the model and dataset size boundaries in obtaining the meta-understanding of IE.\nWe showcase the diversity of the types of important information in the MetaIE distillation dataset.\nWe show that the RoBERTa with sequence labeling framework is the best meta-model architecture compared with sequence-to-sequence and decoder-only models, at a similar scale.\nOur contributions are three-fold:\nWe are the first to build a small LM as a meta-model for all kinds of IE tasks.\nWe propose a novel label-to-span scheme that unifies all IE tasks and applies symbolic distillation to distill the meta-understanding from an LLM to a small LM.\nWe have a rigorous experiment design, which covers various IE tasks and meta-model methods. Comprehensive experiment results support the intuitive expectation and advantage of our MetaIE."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Works",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Information Extraction",
            "text": "Information extraction (IE) is one of the most popular and vital domains in natural language processing. Early IE systems are generally developed for a single IE dataset like NER (dos Santos & Guimar\u00e3es, 2015  ###reference_b8###), RE (Katiyar & Cardie, 2016  ###reference_b20###), or EE (Chen et al., 2015  ###reference_b4###). Due to the gap between the label sets and annotation styles of different IE datasets, few-shot IE frameworks (Ding et al., 2021  ###reference_b6###; Han et al., 2018  ###reference_b17###; Ma et al., 2023  ###reference_b28###) are proposed to quickly learn models on new datasets. The IE models are pre-trained on a large scale of IE labels and then transferred to the target domain by fine-tuning on few examples. With the emergence of LLMs, researchers have started to train LMs on multiple IE tasks with unified formats (Lu et al., 2022  ###reference_b27###; Paolini et al., 2021  ###reference_b32###). LLMs fine-tuned for general purpose (OpenAI, 2023  ###reference_b31###; Touvron et al., 2023  ###reference_b37###) have also shown strong potential to understand new IE tasks with their instruction-following ability. However, these LLMs still lag behind supervised models (Xu et al., 2023  ###reference_b43###), potentially due to the difficulty of specifying the required pattern for extraction in different datasets. Moreover, the cost of LLMs limits their application to IE on a large corpus. This paper aims to transfer the meta-understanding of IE from LLMs to lighter-weight models, which produce a flexible model with high adaptability to any target IE task."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Model Distillation",
            "text": "Model distillation (Hinton et al., 2015  ###reference_b18###; Gou et al., 2021  ###reference_b15###) is the process of transferring knowledge from large models (teacher models) to small ones (student models). Traditional distillation optimizes the similarity between logits produced by the teacher and student models (Hinton et al., 2015  ###reference_b18###; Kim et al., 2019  ###reference_b21###; Mirzadeh et al., 2020  ###reference_b29###). Symbolic distillation (West et al., 2022  ###reference_b41###; Li et al., 2023  ###reference_b23###; West et al., 2023  ###reference_b42###) for language models learns a student model on texts generated by the teacher model. In comparison with traditional distillation, symbolic distillation allows the student model to focus on one aspect of the teacher model (West et al., 2022  ###reference_b41###), which can be some high-level ability, such as chain-of-thought reasoning (Li et al., 2023  ###reference_b23###), with much smaller model size. For IE, symbolic model distillation has been successfully applied for an IE subtask, NER (Zhou et al., 2023  ###reference_b46###), which distills an NER model that can extract entities in a broad domain. This paper aims to distill the cross-IE task ability of LLMs, i.e., meta-understanding of IE and proposes a meta-model that can effectively learn IE tasks with few examples."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Meta Learning",
            "text": "Meta-learning (Finn et al., 2017b  ###reference_b11###) enables the models to learn new tasks better, i.e., stronger transfer learning ability. MAML (Finn et al., 2017a  ###reference_b10###) proposes a framework to learn a better starting point for few-shot learning by utilizing multiple datasets for loss updating. Reptile (Nichol et al., 2018  ###reference_b30###), similar to MAML, simplifies the meta-learning algorithm by performing stochastic gradient descent not only within each task but also across tasks, making it more efficient and easier to implement. The Prototypical Networks method (Snell et al., 2017  ###reference_b36###) employs a distance-based classification approach, where it learns a metric space in which classification can be performed by computing distances to prototype representations of each class. While most meta-learning methods are experimented on classification tasks, pre-training on multiple datasets (Ding et al., 2021  ###reference_b6###) and prototypical networks (Ji et al., 2022  ###reference_b19###) have been applied for IE. While these methods focus on specific IE tasks like NER, we aim to optimize a starting point for general IE tasks by distilling from LLMs."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Our MetaIE Framework",
            "text": "We select the paragraphs from OpenWebText (Gokaslan & Cohen, 2019  ###reference_b14###), Since OpenWebText it is a popular dataset used in language model pre-training, we are not introducing new texts.\nWe split the paragraphs by sentences and only use the first sentence of each paragraph for a higher diversity and to avoid the ambiguity caused by coreference.\nThe LLM is instructed to formalize all  pairs in the prompting output as \u201c- Place : New York \u201d, which are extracted by regular expression matching.\nConsidering there might be multiple spans returned for , we split the span by conjunctions like comma.\nTable 1  ###reference_### shows some statistics and example results of the labels returned by the LLM, illustrating a broad spectrum of IE domains, ranging from simple entities and events to complex relationships and contexts.\nThe diversity in the -gram categories\nshowcases the model\u2019s ability to capture a wide array of query types.\nThis variety underscores the comprehensive coverage and nuanced understanding that LLMs bring to the task of generating queries across different facets of the IE domain."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Label-to-span Scheme",
            "text": "We formalize the IE task as given an IE label  (e.g., Person in NER), extracting a span  from a sentence .\nThe span  can be represented as  including the words from -th to -th.\nDenoting the IE process as a mapping , it can be represented as . Machine learning-based methods aim to learn the mapping by optimizing a model  with parameter .\nFor a specific IE task (e.g., NER), the IE label set  will contain  falling inside the task label, i.e., .\nBased on the general definition of IE, the general IE label set  can be any textual description, thus .\nIn this paper, we aim to learn a meta-model that can be easily adapted to different IE tasks.\nIn the current practice of IE, the \u201cmeta-model\u201d is generally pre-trained in a single IE task with a large number of labels ().\nThen, the meta-model can be fine-tuned on few-shot examples to quickly adapt to different downstream IE datasets in the same task, such that .\nWe expand this learning scheme to a general meta-model that works for all existing and potentially new IE tasks.\nTo achieve this goal, our intuition is to pre-train the model to learn the label-to-span mapping with the label set approximating the general IE label distribution .\nAs the label sets of all IE tasks are subsets of , our meta-model will enjoy an efficient transfer to all IE tasks."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Distillation Dataset Construction",
            "text": "###figure_2### To apply a symbolic distillation of the meta-understanding of IE from LLMs, we prompt LLMs to create data for distillation by querying them to extract \u201cimportant information\u201d from texts as shown in Figure 2  ###reference_###.\nOur expectation for the dataset is to cover as many  as possible to approximate the broad  set to better distill the meta-model for all kinds of IE tasks.\nWe query LLMs to annotate some raw corpora  to build the MetaIE dataset.\nGiven each , the LLM is instructed to generate a series of  pairs.\nWe do not set any limitation to  to better approximate the broad  set.\nWe select the paragraphs from OpenWebText (Gokaslan & Cohen, 2019  ###reference_b14###  ###reference_b14###), Since OpenWebText it is a popular dataset used in language model pre-training, we are not introducing new texts.\nWe split the paragraphs by sentences and only use the first sentence of each paragraph for a higher diversity and to avoid the ambiguity caused by coreference.\nThe LLM is instructed to formalize all  pairs in the prompting output as \u201c- Place : New York \u201d, which are extracted by regular expression matching.\nConsidering there might be multiple spans returned for , we split the span by conjunctions like comma.\nTable 1  ###reference_###  ###reference_### shows some statistics and example results of the labels returned by the LLM, illustrating a broad spectrum of IE domains, ranging from simple entities and events to complex relationships and contexts.\nThe diversity in the -gram categories\nshowcases the model\u2019s ability to capture a wide array of query types.\nThis variety underscores the comprehensive coverage and nuanced understanding that LLMs bring to the task of generating queries across different facets of the IE domain."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Distillation Framework",
            "text": "We illustrate the distillation with manual labeling by human annotators (dos Santos & Guimar\u00e3es, 2015  ###reference_b8###) that suits well for encoder-based language models (e.g., RoBERTa (Liu et al., 2019  ###reference_b24###)).\nGiven a sequence of words, the manual labeling by human annotators will tag each word by outputting.\nFollowing the traditional BIO labeling scheme, will be (begin), (inner), and (none). The annotators are trained on word tagging and the tags are decoded into spans by searching sequences that begin with and continue by. In traditional manual labeling, the and tags generally consist of label information such as -place or -person.\nIn our case, we formalize the tagging in a query-dependent way since the annotators need to handle arbitrary queries.\nWe attach the label information as a prefix like \u201cplace: \u201d to the beginning of the input text.\nThe input text is then labeled by the BIO scheme, where the span label is indicated in the prefix.\nFinally, the BIO sequences are used to fine-tune the manual labeling process.\nThis distillation process can also be adapted to Seq2Seq encoder-decoder models and Causal LM-based decoder-only models.\nWe use manual labeling by human annotators for the main experiment based on their empirical advantage in IE tasks, which we also empirically find support in the analysis in Section 5.2  ###reference_###."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "IE Tasks and Datasets",
            "text": "To deeply delve into the differences between different model distillation or meta-learning methods, we include a wide variety of tasks:\nNamed Entity Recognition (NER) extracts named entities with their labels from texts.\nWe include  NER datasets that was studied in Ushio & Camacho-Collados (2021  ###reference_b38###), i.e., (1) CoNLL2003, (2) BioNLP2004, (3) WNUT2017, (4) MIT-Movie, (5) MIT-Restaurant, (6) BC5CDR, which covers various domains: news, medical, social media, and reviews.\nRelation Extraction (RE) extracts named entities, and in addition, identifies the relationships between them.\nWe include  popular datasets, (1) ADE (Gurulingappa et al., 2012  ###reference_b16###) and (2) CoNLL2004 (Carreras & M\u00e0rquez, 2004  ###reference_b2###) representing RE on medical and news domain.\nWe evaluate the performance of RE models on both relation detection and the detection of entities involved in the relations.\nEvent Extraction (EE) extracts event triggers and their arguments. We use the standard ACE2005 dataset (Walker et al., 2006  ###reference_b40###) for EE evaluation.\nWe compare the model performance on both event trigger detection (T) evaluation task and trigger-augment pair detection (A) evaluation task.\nSemantic Role Labeling (SRL) extracts predicates (verbs) and their arguments.\nWe select the CoNLL2005 (Carreras & M\u00e0rquez, 2005  ###reference_b3###) dataset for SRL.\nWe follow previous works to learn backbone LMs on samples from the Brown training dataset and then test them on Brown and WSJ test datasets.\nAspect-based Sentiment Analysis (ABSA) extracts aspect terms and the sentiment polarity towards them.\nWe select SemEval2014 (Pontiki et al., 2014  ###reference_b34###) as the dataset for ABSA, with its two subsets: 14res and 14lap including reviews about restaurants and laptops.\nAspect Sentiment Triplet Extraction (ASTE) extracts aspect terms and the corresponding opinion terms that contain the sentiment polarity towards them.\nWe use the same SemEval2014 dataset as for ABSA, on which aspect-sentiment triplets are further annotated by Xu et al. (2020  ###reference_b44###).\nFor a fair comparison, we formalize all those tasks as , which can be found in the Appendix A  ###reference_###.\nFor each task, we query each possible label to extract  pairs.\nFor spans conflicting with each other, as we run label-wise extractions, we only keep the one with a higher BI sequence probability.\nFor tasks that extractions are dependent on each other (e.g., RE, EE, SRL, ASTE), we follow Paolini et al. (2021  ###reference_b32###) to run multi-stage extractions for these tasks.\nAs ACE2005 involves too many labels, we report the unlabeled performance on detecting the triggers and arguments for all methods for comparison."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Evaluation Metric: Few-shot Fine-tuning Performance",
            "text": "We use the few-shot fine-tuning performance on all IE tasks to evaluate the meta-model\u2019s quality.\nSpecifically, all methods in our evaluation will provide us a backbone LM.\nWe then conduct few-shot fine-tuning from the training dataset for fine-tuning with sample details in Appendix B  ###reference_###.\nFinally, we evaluate them on the test dataset using the micro F1 score as the evaluation metric.\nFor multi-task pre-training baselines, tasks without large-scale annotations (SRL, ABSA, ASTE) are out-of-distribution tasks.\nThe default backbone LM we used for fine-tuning is RoBERTa-Large (Liu et al., 2019  ###reference_b24###), which is a traditional bidirectional encoder used for learning IE tasks formalized as sequence tagging.\nThe learning rate is set to  with AdamW (Loshchilov & Hutter, 2019  ###reference_b26###) as the optimizer and a cosine annealing learning rate scheduler (Loshchilov & Hutter, 2017  ###reference_b25###).\nWe fine-tune the backbone LM with batch size  for a single epoch to avoid overfitting."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Compared Methods",
            "text": "We first include a comparison with the teacher model GPT-3.5-turbo via LLM Prompting with in-context learning (ICL).\nFor ICL, we provide  examples in the prompt of our query.\nBased on previous discoveries on LLM-based IE (Peng et al., 2023  ###reference_b33###; Wadhwa et al., 2023  ###reference_b39###; Gao et al., 2024  ###reference_b13###), we shall expect that fine-tuned small LMs work better than the LLM.\nWe compare our MetaIE with a variety of methods from the following three categories\nVanilla LM fine-tuning (FT), i.e., directly using the vanilla pre-trained LM as the backbone LM in fine-tuning.\nTask-level Meta-learning (ML)+FT. It is expected to have a strong performance to other datasets in the same IE task but poor generalization to other IE tasks.\nTransfer (Human) is a baseline that trains the backbone LM on large-scale human annotations of a specific IE task. Specifically, we use FewNerd (Ding et al., 2021  ###reference_b6###) for NER, FewRels (Han et al., 2018  ###reference_b17###) for RE, and FewEvents (Ma et al., 2023  ###reference_b28###) for EE.\nTransfer (LLM) uses the same datasets in Transfer (Human) but queries the LLM to annotate them following the human workflow. This baseline aims to compare the quality of annotation from humans and LLMs following the conventional annotation schema.\nTask Distillation distills from LLMs by querying answers for specific IE tasks. We implement this by providing in-context task-specific examples to control the LLM-produced data similar to the label IE task. The input texts are set to be the same as MetaIE to avoid bias.\nNER Distillation applies the model distilled following Task Distillation but tests them on non-NER tasks to evaluate its cross-task transferability.\nIE-level Meta-learning (ML)+FT aims to learn an IE model with strong transferability to all IE tasks. Our MetaIE also falls into this category.\nMultiIE merges the multiple human-annotated IE datasets (FewNerd, FewRels, FewEvents) to train a backbone LM, which represents a multi-task baseline with human annotations.\nMAML (Finn et al., 2017a  ###reference_b10###) is a traditional meta-learning baseline that merges gradients on different datasets to build a model that can be quickly transferred to these datasets. We use the datasets in MultiIE for MAML in the experiment.\nFor all baselines, the data number for meta-learning is controlled to the same as MetaIE by sampling towards a fair comparison."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Result",
            "text": "The result from our experiments is presented in Table 2  ###reference_###. The vanilla model is poorly transferred by fine-tuning to all kinds of IE tasks. The model with meta-learning on a single IE task, NER, is only well-transferred to other NER datasets but poorly-transferred to other IE tasks. Among IE-level meta-learning methods, the MultiIE model can be transferred to in-domain IE tasks with outstanding performance but still fails to be transferred to out-of-domain IE tasks, either with regular pre-training or meta-learning frameworks like MAML. In contrast to all these baselines, our MetaIE shows a strong transferability to all IE tasks, especially on out-of-domain tasks for MultiIE. Thus, the experiment results are highly consistent with our claim in IE task transferability that wider pre-training label set  will enable macro transferability of the model to all IE tasks.\nBesides the main discovery, we can also observe that LLM-based meta-learning outperforms the pre-training on human annotation. Take NER as an instance, while both label sets satisfy , the  proposed by LLMs is much more diverse than the fixed set in human annotated datasets, which again verifies the importance of the label distribution, even in task-specific distillation.\nThe comparison with the teacher model also shows the student model generally outperforming the teacher model under few-shot supervision. Thus, we conclude fine-tuning a distilled student IE model to perform better than inference by the teacher LLMs with few-shot in-context examples. This further verifies the advantage of model distillation in meta-learning which enables more efficient and effective transfer."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Further Analysis",
            "text": ""
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Size Analysis",
            "text": "We explore how the scale of the student model or the data number affects the distillation quality. For the model scale, we compare among RoBERTa-Small, RoBERTa-Base, and RoBERTa-Large. For the data scale, we increase the sampling size to  and pre-train the student model with different amounts of data.\n###figure_3### ###figure_4### The analysis of model size is presented in Figure 3  ###reference_###, we can observe the performance of a student model can be scaled up by more parameters. Also, for simple tasks (like NER) with a general domain (like CoNLL2004), a tiny student model is competent for the distillation. However, for specific domains or complex tasks, the student model needs more parameters for generalization.\nThe analysis of data size is presented in Figure 4  ###reference_###, we observe the existence of a threshold between  to endow the student model with the meta-understanding of IE. Also, a small amount of meta data like  can significantly benefit the transferring."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Distillation Framework Comparison",
            "text": "We compare student models following different distillation frameworks (because of their architectures) to investigate how this factor affects the distillation effectiveness.\nSeq2Seq implements the distillation by learning to extract a group of spans based on the IE label as in the distillation dataset. We include two Seq2Seq models: BART-Large (Lewis et al., 2020  ###reference_b22###) and T5-Base (Raffel et al., 2020  ###reference_b35###), which contain the same scale of parameters as in the RoBERTa-Large in our previous experiments.\nCausalLM is similar to Seq2Seq but only uses the decoder model instead of the encoder-decoder as in Seq2Seq. We also include two CausalLM-based models with similar parameter scales: GPT2-Medium (Brown et al., 2020  ###reference_b1###) and OPT-350M (Zhang et al., 2022  ###reference_b45###).\nWe also include another sequence labeling model BERT-Large-Cased (Devlin et al., 2019  ###reference_b5###) as a baseline to explore the influence of the backbone model quality on the learning performance. For all models, we pre-train them using our MetaIE dataset with the same hyperparameters.\nWe compare the performance of different distillation frameworks on NER as an example and the result is demonstrated in Table 3  ###reference_###. Sequence labeling models perform the best in few-shot transfer learning, which indicates their advantage in the distillation of meta-understanding of IE. This can be attributed to the consistency of sequence labeling with the extraction nature. We thus conclude distilling IE knowledge to a traditional sequence labeling model is better than those popular generative models. Between sequence labeling models, RoBERTa outperforms BERT, showing a better student model also benefits the distillation procedure."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Limitation Discussion",
            "text": "Efficiency The efficiency of the unified label-to-span will be , which is lower than the traditional  (number of LM forwarding) BIO sequence labeler with label information in the labeling result. This will limit the application of our model to cases where  is large. This efficiency is a trade-off for the ability to process any IE label, which enables the fast transfer of the BIO model to different IE tasks.\nBias in LLM-proposed labels As pointed out in previous works (Gallegos et al., 2023  ###reference_b12###; Fang et al., 2023  ###reference_b9###), LLMs have biases in their responses. This can also be observed in the statistics of our distillation dataset. Thus, the small meta-model might also inherit the bias and have better transferability to labels that LLMs prefer than others."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusions and Future Work",
            "text": "This paper presents a novel approach for distilling the meta-understanding of IE from LLMs into more efficient, smaller language models through a synthesized dataset, MetaIE. Our findings indicate that this method not only enhances the adaptability and efficiency of smaller models but also outperforms existing single-task and multi-task distillation methods in various IE tasks. The success of MetaIE underscores the potential of leveraging LLM\u2019s meta-understanding to improve the performance and versatility of smaller models in complex tasks, offering a promising direction for future research in model distillation and IE. Future work will explore a better way for meta-learning by distilling from LLMs and other meta-tasks can be trained based on distillation."
        }
    ],
    "url": "http://arxiv.org/html/2404.00457v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2.1",
            "2.2",
            "2.3"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3"
        ],
        "main_experiment_and_results_sections": [
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "5.1",
            "5.2"
        ]
    },
    "research_context": {
        "paper_id": "2404.00457v1",
        "paper_title": "MetaIE: Distilling a Meta Model from LLM for All Kinds of Information Extraction Tasks",
        "research_background": "### Paper's Motivation\n\nThe motivation behind this paper is to address the limitations of large language models (LLMs) like ChatGPT in information extraction (IE) tasks despite their exceptional performance in other areas through in-context learning (ICL). The authors note that even with ICL examples, LLMs struggle to match the performance of smaller, fine-tuned models such as BERT or RoBERTa in IE tasks. Additionally, conducting auto-regressive inference with LLMs is costly and time-consuming, hindering their scalability for large corpora.\n\n### Research Problem\n\nThe research problem this paper tackles lies in overcoming the limitations of LLMs in executing IE tasks effectively and efficiently. The authors aim to create a small language model (LM) that can fully leverage the meta-understanding of IE from LLMs and adapt it to various specific IE tasks, thereby mitigating the need for extensive, costly training and inference procedures typical of LLMs.\n\n### Relevant Prior Work\n\n1. **Performance Comparison**: The authors note the superior performance of smaller LMs fine-tuned on small training sets over LLMs for IE, referencing works like Peng et al. (2023), Wadhwa et al. (2023), and Gao et al. (2024).\n\n2. **Limitations of LLMs**: It's cited that LLMs face limitations in following specific extraction schemes (Xu et al., 2023).\n\n3. **Existing Meta-Models**: Meta-models for specific IE tasks have previously been explored, such as UniversalNER developed by Zhou et al. (2023) for NER tasks.\n\n4. **Symbolic Distillation**: The paper employs symbolic distillation techniques as discussed by West et al. (2022).\n\n5. **Dataset Sources**: The OpenWebText corpus (Gokaslan & Cohen, 2019) is used as a source for constructing the distillation dataset.\n\n6. **Evaluation Benchmarks**: The MetaIE framework is evaluated against a variety of existing methods and tasks, including semantic role labeling (Carreras & M\u00e0rquez, 2005), aspect-based sentiment analysis (Pontiki et al., 2014), and aspect-sentiment triplet extraction (Xu et al., 2020).\n\n7. **Experiment Design**: The significance of experimenting rigorously across diverse IE tasks and employing meta-model methods.\n\nThis comprehensive incorporation of prior work and methodologies underscores the robust foundation upon which the innovative MetaIE framework is built.",
        "methodology": "**MetaIE: Distilling a Meta Model from LLM for All Kinds of Information Extraction Tasks**\n\n**Methodology:**\n1. **Data Selection and Preparation:**\n   - **Dataset Choice:** We utilize the OpenWebText dataset (Gokaslan & Cohen, 2019), a well-known text corpus used in language model pre-training. By opting for this existing dataset, we avoid introducing new texts.\n   - **Sentence Selection:** Each paragraph from OpenWebText is split by sentences, with only the first sentence selected from each paragraph. This choice is made to enhance diversity and minimize the ambiguity that coreferencing might cause.\n\n2. **Data Formatting:**\n   - **Standardized Pair Formalization:** The Large Language Model (LLM) is directed to format all entity pairs in the output prompt as \u201c- Place : New York\u201d. These pairs are extracted using regular expression matching.\n   - **Span Splitting:** In instances where multiple spans are identified, they are divided by conjunctions such as commas to manage the extraction effectively.\n\n3. **Task Variety and Model Versatility:**\n   - **IE Domain Coverage:** Statistical details and sample results of the labels generated by the LLM highlight a wide spectrum of information extraction domains. These range from simple entities and events to more complex relationships and contexts.\n   - **N-Gram Diversity:** The variety in n-gram categories captured by the model demonstrates its capability to understand and generate a diverse array of query types, emphasizing the model's extensive coverage and intricate comprehension of the IE domain.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n#### Tasks and Datasets\nThe main experiment involves a diverse set of Information Extraction (IE) tasks, each offering distinct challenges and domain-specific data:\n\n1. **Named Entity Recognition (NER)**\n   - **Datasets**: CoNLL2003, BioNLP2004, WNUT2017, MIT-Movie, MIT-Restaurant, BC5CDR.\n   - **Domains**: News, medical, social media, reviews.\n\n2. **Relation Extraction (RE)**\n   - **Datasets**: ADE, CoNLL2004.\n   - **Domains**: Medical, news.\n\n3. **Event Extraction (EE)**\n   - **Dataset**: ACE2005.\n   - **Evaluation Tasks**: Event trigger detection (T), trigger-augment pair detection (A).\n\n4. **Semantic Role Labeling (SRL)**\n   - **Dataset**: CoNLL2005.\n   - **Training & Evaluation**: Trained on Brown dataset, tested on Brown and WSJ test datasets.\n\n5. **Aspect-based Sentiment Analysis (ABSA)**\n   - **Dataset**: SemEval2014 (with subsets 14res and 14lap).\n   - **Domains**: Restaurant and laptop reviews.\n\n6. **Aspect Sentiment Triplet Extraction (ASTE)**\n   - **Dataset**: The same SemEval2014 dataset used for ABSA, with additional annotations by Xu et al. (2020).\n\n#### Evaluation Metrics\nEach of these tasks is evaluated based on appropriate performance indicators:\n- **NER, RE, EE, and ASTE Tasks**: Evaluation criteria typically involve precision, recall, and F1-score for both entity detection and relation/event/trigger extraction.\n- **ABSA & SRL Tasks**: Performance measurements include precision, recall, and F1-score for aspect and sentiment analysis, as well as argument and predicate detection in SRL.\n\n#### Experimental Details\nFor fair comparisons:\n- All tasks are formalized uniformly as token pair extraction, details of which are provided in Appendix A.\n- For overlapping spans, the extraction with a higher BI sequence probability is retained.\n- Multi-stage extractions are executed for dependent extraction tasks (RE, EE, SRL, ASTE) following the method by Paolini et al. (2021).\n- Due to numerous labels in ACE2005, unlabeled performance on trigger and argument detection is reported for all methodologies.\n\n### Main Experimental Results\nThe key findings illustrate the effectiveness of the meta model distillation approach across diverse IE tasks. The comparative performance results substantiate the model's efficacy. Specific performance metrics (precision, recall, and F1-score) for each task and dataset showcase superior or competitive results, demonstrating the robustness and generalizability of the proposed approach. The detailed performance differences, analysis, and comparisons with baseline methods are documented in the corresponding tables within the paper."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To explore how the scale of the student model or the data number affects the distillation quality in the MetaIE framework.",
            "experiment_process": "The experiment compared different sizes of student models: RoBERTa-Small, RoBERTa-Base, and RoBERTa-Large. It also varied the sampling size for the distillation dataset, pre-training the student models with different amounts of data. The performance was evaluated in terms of how well each configuration could scale up in terms of parameters and handle general and specific domain tasks effectively.",
            "result_discussion": "The results, presented in Figures 3 and 4, indicated that the performance of a student model improves with more parameters, especially for specific domains or complex tasks. A tiny student model is competent for simple tasks in general domains. There exists a threshold data size that endows the student model with the meta-understanding of IE, and even a small amount of meta data can significantly benefit the transfer.",
            "ablation_id": "2404.00457v1.No1"
        },
        {
            "research_objective": "To investigate how different distillation frameworks affect the distillation effectiveness of MetaIE.",
            "experiment_process": "The experiment compared various student models based on different distillation frameworks, including Seq2Seq (BART-Large and T5-Base), CausalLM (GPT2-Medium and OPT-350M), and sequence labeling model BERT-Large-Cased. All models were pre-trained using the MetaIE dataset with the same hyperparameters and their performance on NER (Named Entity Recognition) was evaluated.",
            "result_discussion": "The results, shown in Table 3, demonstrated that sequence labeling models perform the best in few-shot transfer learning, indicating their advantage in distilling the meta-understanding of IE. This consistency aligns with the extraction nature of IE. Among sequence labeling models, RoBERTa outperformed BERT, highlighting that a better student model benefits the distillation procedure.",
            "ablation_id": "2404.00457v1.No2"
        }
    ]
}