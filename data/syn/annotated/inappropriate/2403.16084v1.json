{
    "title": "Argument Quality Assessment in the Age of Instruction-Following Large Language Models",
    "abstract": "The computational treatment of arguments on controversial issues has been subject to extensive NLP research, due to its envisioned impact on opinion formation, decision making, writing education, and the like. A critical task in any such application is the assessment of an argument\u2019s quality\u2014but it is also particularly challenging. In this position paper, we start from a brief survey of argument quality research, where we identify the diversity of quality notions and the subjectiveness of their perception as the main hurdles towards substantial progress on argument quality assessment.\nWe argue that the capabilities of instruction-following large language models (LLMs) to leverage knowledge across contexts enable a much more reliable assessment. Rather than just fine-tuning LLMs towards leaderboard chasing on assessment tasks, they need to be instructed systematically with argumentation theories and scenarios as well as with ways to solve argument-related problems. We discuss the real-world opportunities and ethical issues emerging thereby.\n\n\n\nKeywords:\u2009Computational Argumentation, Argument Quality, Large Language Model, Instruction Fine-Tuning",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "\u201cIn some sense, the question about the quality of an argument is the \u2018ultimate\u2019 one for argumentation mining.\u201d Stede and Schneider (2018  ###reference_b110###).\nWhen learning about controversial issues, people rarely accept arguments they encounter without further contemplation. Rather, they seek to find the best arguments; those that help them form an opinion or write texts that persuade others; those that make them reach agreement or at least understand each other better. That is to say, argument quality is of interest as soon as arguments are presented to an audience. Computational argumentation aids the treatment of arguments at a larger scale, with important applications in search Wachsmuth et al. (2017c  ###reference_b122###), business Slonim et al. (2021  ###reference_b106###), and education Wambsganss and Niklaus (2022  ###reference_b124###). But the situation there is the same: It is not enough to mine or generate arguments; their quality also needs to be evaluable Park and Cardie (2018  ###reference_b87###), so that it can be assessed Lauscher et al. (2020  ###reference_b68###), flaws can be found Goffredo et al. (2022  ###reference_b40###), and accounted for Skitalinskaya et al. (2023  ###reference_b104###).\nWachsmuth et al. (2017b  ###reference_b121###) surveyed research on argument quality assessment, organizing theories and methods under 15 quality notions, from logical cogency to rhetorical effectiveness to dialectical reasonableness. Even though computational argumentation was just gaining momentum in natural language processing (NLP) back then, rarely going beyond argument mining, two inherent challenges of argument quality were visible already: the diversity of quality notions as well as the subjectivity of their perception and, hence, of their assessment for both humans and computational models. Consider the following argumentative claim against censoring Mark Twain\u2019s usage of the N-word, taken from the debate platform kialo.com:\n\u201cIn Huckleberry Finn, Twain captured the essence of everyday midwest American English.\"\nThis claim is certainly relevant to the discussion, but whether people will deem it effective may strongly depend on their individual context. A person without African-American background may be willing to accept the argument; one with high literacy might look for clearer logical connections.\nWhile the challenges of diversity and subjectivity prevail until today Lapesa et al. (2023  ###reference_b66###), NLP is now seeing a revolutionary breakthrough: the rise of instruction-following large language models (henceforth, LLMs) that can tackle various NLP tasks with little to no task-specific fine-tuning, enabled by their supreme capability to integrate and leverage knowledge across contexts OpenAI (2023  ###reference_b83###). The question is: What are the implications for argument quality assessment specifically as well as for computational argumentation in general?\nIn this position paper, we revisit the computational assessment of argument quality in light of the availability of LLMs such as GPT-4 and Alpaca Taori et al. (2023  ###reference_b113###). Starting from the status quo reported by Wachsmuth et al. (2017b  ###reference_b121###), we carry out a brief survey of recent NLP research on the topic (Section 2  ###reference_###). To bring order into the various lines of research pursued since 2017, we organize them into three general directions, as laid out in Figure 1  ###reference_###:\nConceptual notions of maximal and minimal argument quality,\nInfluence factors of argument quality from the context where arguments occur, and\nComputational models for assessing or improving argument quality.\nOn this basis, we establish the central question to which we provide answers in this paper:\nHow to drive research on LLM-based argument quality assessment in order to face the prevailing challenges of diverse quality notions and their subjectivity?\nIn particular, we are convinced that the capabilities of instruction-following LLMs enable research to overcome many aspects of the two challenges. To this end, the primary focus of NLP research on argument quality should be put on systematic ways to teach LLMs to follow instructions, including concepts and settings of arguing in addition to ways to solve argument-related problems (Section 3  ###reference_###). Instead of fine-tuning LLMs on predefined domains (manifested in the training data) and preselected theories (manifested in the data\u2019s annotations), as well as simple engineering of prompts, we expect the greatest impact to lie in teaching LLMs the theories, circumstances, and ethical constraints to adhere to. The rationale behind this is that LLMs will often have processed data from all contexts needed to make an informed judgment about an argument\u2019s quality, due to their heavy pretraining on huge amounts of data. In contrast, LLMs cannot access, by default, the knowledge of what is to be prioritized in a given setting.\nWe state upfront that the blueprint delineated in this paper comes with several limitations and ethical considerations that we critically analyze below. Moreover, we are naturally aware of the general issues of LLMs, including hallucinated facts and the reproduction of common social biases. These issues deserve treatment in computational argumentation as well; they are even particularly critical there due to the sensitivity of many controversial topics Holtermann et al. (2022  ###reference_b49###). Keeping this in mind, we believe that it is necessary to explore now how to best employ LLMs for argument quality assessment in order to harness their full potential for the main applications, while avoiding to waste energy for the typical pursuit of leaderboard rankings on existing quality assessment tasks. This is the goal of the paper at hand.\nNow, why is it important to discuss LLMs for argument quality assessment specifically? We address this matter when we look at the real-world opportunities emerging from the capabilities of LLMs in academia and industry (Section 4  ###reference_###). While Argyle et al. (2023  ###reference_b3###) developed LLMs that tone down argumentative conversations, we postulate a contrary path: Exploiting the means of LLMs to proactively enable people to learn and better reason about controversial issues, thus contributing towards more deliberate conversations Vecchi et al. (2021  ###reference_b119###). We think that the time has come to revisit and pursue the core visions of computational argumentation research, from the overcoming of filter bubbles to the individualized mass education of learners. We sketch how these visions could be realized with the LLMs available today, before we conclude (Section 5  ###reference_###) and stress ethical concerns that arise with LLMs that actively affect human views (Section 6  ###reference_###).\nWith the discussion in this position paper, we provide two main contributions to research:\nA survey of the main lines of recent research on argument quality and its assessment\nA blueprint for impactful future research on LLMs for argument quality assessment"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   A Brief Survey of Recent Research",
            "text": "To start, this section briefly but systematically surveys recent NLP work on argument quality assessment. We identify three general directions, each with two main aspects, and organize the research accordingly, as illustrated in Figure 1  ###reference_###.\n###figure_1### 24 of the papers primarily deal with the question of what is actually meant by argument quality, considered from either of two complementary perspectives:\nNotions of maximal quality based on arguing goals such as agreement and deliberation or on preferences between different arguments\nNotions of minimal quality in terms of what makes an argument evaluable or appropriate to be stated as well as how to avoid fallacies\nThis direction covers 30 papers studying (or controlling) two types of factors that influence the perception of quality beyond the content, structure, and style of the argument itself:\nArgument-related factors such as the argument\u2019s length, its structure in terms of relations between units, and revisions applied to it\nContext-related factors, such as the domain of the discussion, the audience addressed, and the debaters involved\nFinally, 21 papers aim mainly at methodological novelty in the modeling of argument quality for two quality-related tasks:\nModels for assessment of argument quality, capturing specificities of the task, the whole discussion, or the context of arguing\nModels for improvement of argument quality, targeting the need for improvement, actual optimizations, or feedback on what to improve\nThe remaining eight papers pursue individual research directions.\nWe note that many of the surveyed papers do not fall under one general direction only; rather, they often have a visible focus on one of them. In particular, our internal discussion revealed that contributions to influence factors and computational models are not always easy to distinguish and that, sometimes, models may rather target downstream applications. Still, the directions and aspects were agreed upon in general.111For validation, we reassigned 16 papers (19%) to other authors: 11 got the same main direction; for four, it was seen as the second contribution. Only in one case, a fully different direction was assigned. After rechecking, the newly assigned direction did not seem adequate.\nIn the following, we discuss selected works from each of the general research directions. Table 1  ###reference_### in the appendix shows the full list of all 83 covered publications, grouped by the primary general research direction and the main aspect.\nSome researchers build on the argument quality taxonomy proposed by Wachsmuth et al. (2017b  ###reference_b121###), including Lauscher et al. (2020  ###reference_b68###) who model the main taxonomy notions using multitask learning across Q&A, debate, and review forums. Others question the simplifying view that argument quality is about persuasion only: El Baff et al. (2018  ###reference_b29###) consider the goal of agreement, defining good news arguments as those that challenge or corroborate stance. Gretz et al. (2020  ###reference_b41###) see argument quality as a preference relation, and Falk et al. (2021  ###reference_b32###) examine its connection to deliberation. With an entirely different perspective, some papers examine what makes an argument good irrespective of topic Beigman Klebanov et al. (2017  ###reference_b9###), whereas, for example, Dumani et al. (2020  ###reference_b23###) operationalize argument quality for practice in a quality-based framework for argument retrieval.\nPark et al. (2015  ###reference_b86###) establish the notion of an argument\u2019s evaluability, that is, the prerequisite of assessing logical quality soundly. A key research line on minimal quality is the detection of fallacies: arguments with flawed or deceptive reasoning. Neural models have shown success on this deep semantic problem; some aim at ad-hominem arguments only Habernal et al. (2018  ###reference_b45###), others at various fallacies Jin et al. (2022  ###reference_b53###). Persing and Ng (2017b  ###reference_b92###) tackle the broader problem of spotting an argument\u2019s weaknesses, from grammar errors to lack of objectivity and unclear justifications. More practice-oriented, Pauli et al. (2022  ###reference_b89###) look at the misuse of fallacies for rhetorical appeals in online forums and fake news. Finally, Ziegenbein et al. (2023  ###reference_b131###) refine the notion of appropriateness, emanating from Aristotle\u2019s work Aristotle (ca. 350 B.C.E./ translated\n2007  ###reference_b4###). They see it as the minimal quality that makes arguments worthy of being considered and annotate data for violations of appropriateness.\nIn terms of textual factors influencing the perceived quality of arguments, researchers display the questionable power of length as a predictor Potash et al. (2017  ###reference_b94###) and account for this in dataset creation Toledo et al. (2019  ###reference_b115###). The impact on quality of internal argument structure has been investigated using the notion of organization quality in learner essays Chen et al. (2022a  ###reference_b17###) and by using annotations of argument components in business model pitches Wambsganss and Niklaus (2022  ###reference_b124###). Notions of structure within an argument are further extended through adding attributes to argument components Carlile et al. (2018  ###reference_b14###), shifting the focus to component-related factors, or by comparing different revisions of the same claim Skitalinskaya et al. (2021  ###reference_b103###).\nLukin et al. (2017  ###reference_b76###) analyze the interaction between argumentative styles (emotional vs. factual) and the personality of the audience, as modeled by the Big Five traits. Similarly, Durmus and Cardie (2018  ###reference_b24###) model political and religious ideologies, based on the audience\u2019s stances on various controversial topics. Both indicate that audience-level factors often outweigh language use in their persuasive effect. Alshomary et al. (2022  ###reference_b2###) turn the view to rhetorical strategies of debaters, assessing the effect of morally-framed arguments. They find that morals are particularly successful in challenging the audience\u2019s beliefs. Wiegmann et al. (2022  ###reference_b128###) analyze stylistic and behavioral characteristics of debaters that contribute to their persuasiveness over multiple debates. Aside from debate participants, Liu et al. (2022  ###reference_b74###) explore arguments on social media that are accompanied by images, highlighting the potential of multimodal approaches to quality assessment, whereas Fromm et al. (2023  ###reference_b36###) generalize the contextual scope of assessment to multiple domains at the same time.\nMany approaches aim at specific quality notions. For example, the attentive interaction model of Jo et al. (2018  ###reference_b54###) predicts an opinion holder\u2019s view change by detecting vulnerable regions in their reasoning and modeling its relation to a challenger\u2019s argument. Gleize et al. (2019  ###reference_b39###) propose a Siamese neural network to assess the convincingness of evidence, while Song et al. (2020  ###reference_b107###) develop a hierarchical multitask learning approach to jointly model discourse element identification and organization assessment for essay scoring. Gurcke et al. (2021  ###reference_b44###) examine to what extent an argument\u2019s logical sufficiency can be predicted based on whether its conclusion can be inferred from its premises using the generation capabilities of transformers. Kondo et al. (2021  ###reference_b64###) assess the validity of an argument\u2019s reasoning using Bayesian networks and predicate logic facilitated by argumentation schemes. A few works also look beyond single quality notions, such as Falk and Lapesa (2023  ###reference_b34###) who inject knowledge about the interactions between different quality notions to improve the prediction of individual ones.\n###figure_2### While only a few models for improvement have been presented so far, we expect more to come soon, also seeing related efforts on topics beyond those covered in this survey Chakrabarty et al. (2021  ###reference_b15###); Ihori et al. (2022  ###reference_b51###); Li et al. (2022  ###reference_b72###). An early attempt was made by Ke et al. (2018  ###reference_b56###) who design neural models that predict the persuasiveness and other attributes of arguments in a student essay, to provide feedback to students on how to improve their arguments. Recently, Skitalinskaya and Wachsmuth (2023  ###reference_b105###) identified arguments in need of improvement, leveraging complex revision-based data with transformer models. Skitalinskaya et al. (2023  ###reference_b104###) go one step further, presenting the first approach to the optimization of argumentative claims. It combines neural claim rewriting with quality-based ranking."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1.   Frame of the Survey",
            "text": "Beyond holistic computational argumentation (CA) surveys Stede and Schneider (2018  ###reference_b110###); Cabrio and Villata (2018  ###reference_b13###); Lawrence and Reed (2019  ###reference_b70###), Wang et al. (2023a  ###reference_b126###) specifically reviewed works on argument generation, Lauscher et al. (2022b  ###reference_b69###) on knowledge in CA, and Vecchi et al. (2021  ###reference_b119###) on the use of CA for the social good. Also, some tutorials treated argument mining and its applications Budzynska and Reed (2019  ###reference_b11###); Bar-Haim et al. (2021  ###reference_b6###). In contrast, we focus on argument quality assessment.\nAside from our recent tutorial Lapesa et al. (2023  ###reference_b66###), the only argument quality review that we are aware of is the one of Wachsmuth et al. (2017b  ###reference_b121###) who organize relevant literature until mid 2017 into a taxonomy of 15 logical, rhetorical, and dialectical quality dimensions. The authors already discussed the diversity of quality notions as well as the subjectivity of their perception, both of which are still hampering research. In this paper, we seek to delineate ways to overcome them, in line with the authors\u2019 organization of what argument quality means. We start from their work here, so we restrict our survey to work that is published after theirs.\nBased on our experience with NLP research on CA, we cover four groups of publication venues:\nAll NLP venues covered by the ACL anthology\nThe leading artificial intelligence (AI) conferences, all from AAAI.org and IJCAI\nThe leading information retrieval (IR) conferences, SIGIR and ECIR\nThe leading CA conference, COMMA\nWe used Google site search and Springer and ACM\u2019s internal search on September 1, 2023 (updated on October 17, 2023), to gather all papers containing any of the following pairs of words:\nThis led to 257 papers (202 NLP, 35 AI, 11 IR, 9 CA). From these, we kept all 119 papers that deal with quality of natural language arguments based on title, abstract, and skimming (98 NLP, 12 AI, 6 IR, 3 CA). To focus on scientific novelty, we further excluded surveys, tutorials, demos, shared tasks, and system papers, leaving 104 papers (87 NLP, 10 AI, 5 IR, 2 CA). We checked these in more detail to ensure that argument quality is actually part of the research. After filtering out others, we obtained a final set of 83 papers (69 NLP, 9 AI, 3 IR, 2 CA)."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2.   General Research Directions",
            "text": "Analyzing the 83 papers as a whole, we identified the following three general directions of research on argument quality, each with two main aspects. Concretely, one author of this paper proposed the organization. The papers were then distributed among all other authors who ranked them by the directions and aspects they contribute, if any.\n24 of the papers primarily deal with the question of what is actually meant by argument quality, considered from either of two complementary perspectives:\nNotions of maximal quality based on arguing goals such as agreement and deliberation or on preferences between different arguments\nNotions of minimal quality in terms of what makes an argument evaluable or appropriate to be stated as well as how to avoid fallacies\nThis direction covers 30 papers studying (or controlling) two types of factors that influence the perception of quality beyond the content, structure, and style of the argument itself:\nArgument-related factors such as the argument\u2019s length, its structure in terms of relations between units, and revisions applied to it\nContext-related factors, such as the domain of the discussion, the audience addressed, and the debaters involved\nFinally, 21 papers aim mainly at methodological novelty in the modeling of argument quality for two quality-related tasks:\nModels for assessment of argument quality, capturing specificities of the task, the whole discussion, or the context of arguing\nModels for improvement of argument quality, targeting the need for improvement, actual optimizations, or feedback on what to improve\nThe remaining eight papers pursue individual research directions.\nWe note that many of the surveyed papers do not fall under one general direction only; rather, they often have a visible focus on one of them. In particular, our internal discussion revealed that contributions to influence factors and computational models are not always easy to distinguish and that, sometimes, models may rather target downstream applications. Still, the directions and aspects were agreed upon in general.111For validation, we reassigned 16 papers (19%) to other authors: 11 got the same main direction; for four, it was seen as the second contribution. Only in one case, a fully different direction was assigned. After rechecking, the newly assigned direction did not seem adequate.\nIn the following, we discuss selected works from each of the general research directions. Table 1  ###reference_###  ###reference_### in the appendix shows the full list of all 83 covered publications, grouped by the primary general research direction and the main aspect."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "2.3.   Conceptual Notions",
            "text": "Naturally, all surveyed literature builds on some notion of argument quality, at least implicitly. However, we found that 24 of the 83 papers have the explicit treatment of quality notions as their main focus and 10 further papers contribute to quality notions to some extent. About two-thirds of the works discuss how an argument should be ideally (maximal quality), the others what an argument should at least achieve or avoid (minimal quality).\nSome researchers build on the argument quality taxonomy proposed by Wachsmuth et al. (2017b  ###reference_b121###  ###reference_b121###), including Lauscher et al. (2020  ###reference_b68###  ###reference_b68###) who model the main taxonomy notions using multitask learning across Q&A, debate, and review forums. Others question the simplifying view that argument quality is about persuasion only: El Baff et al. (2018  ###reference_b29###  ###reference_b29###) consider the goal of agreement, defining good news arguments as those that challenge or corroborate stance. Gretz et al. (2020  ###reference_b41###  ###reference_b41###) see argument quality as a preference relation, and Falk et al. (2021  ###reference_b32###  ###reference_b32###) examine its connection to deliberation. With an entirely different perspective, some papers examine what makes an argument good irrespective of topic Beigman Klebanov et al. (2017  ###reference_b9###  ###reference_b9###), whereas, for example, Dumani et al. (2020  ###reference_b23###  ###reference_b23###) operationalize argument quality for practice in a quality-based framework for argument retrieval.\nPark et al. (2015  ###reference_b86###  ###reference_b86###) establish the notion of an argument\u2019s evaluability, that is, the prerequisite of assessing logical quality soundly. A key research line on minimal quality is the detection of fallacies: arguments with flawed or deceptive reasoning. Neural models have shown success on this deep semantic problem; some aim at ad-hominem arguments only Habernal et al. (2018  ###reference_b45###  ###reference_b45###), others at various fallacies Jin et al. (2022  ###reference_b53###  ###reference_b53###). Persing and Ng (2017b  ###reference_b92###  ###reference_b92###) tackle the broader problem of spotting an argument\u2019s weaknesses, from grammar errors to lack of objectivity and unclear justifications. More practice-oriented, Pauli et al. (2022  ###reference_b89###  ###reference_b89###) look at the misuse of fallacies for rhetorical appeals in online forums and fake news. Finally, Ziegenbein et al. (2023  ###reference_b131###  ###reference_b131###) refine the notion of appropriateness, emanating from Aristotle\u2019s work Aristotle (ca. 350 B.C.E./ translated\n2007  ###reference_b4###  ###reference_b4###). They see it as the minimal quality that makes arguments worthy of being considered and annotate data for violations of appropriateness."
        },
        {
            "section_id": "2.4",
            "parent_section_id": "2",
            "section_name": "2.4.   Influence Factors",
            "text": "Assessing the different notions of argument quality is a complex task and is influenced by many factors, some of which have no explicit relation to the argument itself. Accordingly, research has dealt with the identification, modeling, and controlling of such factors and their impact on argument quality. We found that 30 of the 83 papers mainly focus on influence factors, and a further 19 papers are to some extent devoted to them. Of these, about 60% discuss argument-related factors while the rest looks at context-related factors.\nIn terms of textual factors influencing the perceived quality of arguments, researchers display the questionable power of length as a predictor Potash et al. (2017  ###reference_b94###  ###reference_b94###) and account for this in dataset creation Toledo et al. (2019  ###reference_b115###  ###reference_b115###). The impact on quality of internal argument structure has been investigated using the notion of organization quality in learner essays Chen et al. (2022a  ###reference_b17###  ###reference_b17###) and by using annotations of argument components in business model pitches Wambsganss and Niklaus (2022  ###reference_b124###  ###reference_b124###). Notions of structure within an argument are further extended through adding attributes to argument components Carlile et al. (2018  ###reference_b14###  ###reference_b14###), shifting the focus to component-related factors, or by comparing different revisions of the same claim Skitalinskaya et al. (2021  ###reference_b103###  ###reference_b103###).\nLukin et al. (2017  ###reference_b76###  ###reference_b76###) analyze the interaction between argumentative styles (emotional vs. factual) and the personality of the audience, as modeled by the Big Five traits. Similarly, Durmus and Cardie (2018  ###reference_b24###  ###reference_b24###) model political and religious ideologies, based on the audience\u2019s stances on various controversial topics. Both indicate that audience-level factors often outweigh language use in their persuasive effect. Alshomary et al. (2022  ###reference_b2###  ###reference_b2###) turn the view to rhetorical strategies of debaters, assessing the effect of morally-framed arguments. They find that morals are particularly successful in challenging the audience\u2019s beliefs. Wiegmann et al. (2022  ###reference_b128###  ###reference_b128###) analyze stylistic and behavioral characteristics of debaters that contribute to their persuasiveness over multiple debates. Aside from debate participants, Liu et al. (2022  ###reference_b74###  ###reference_b74###) explore arguments on social media that are accompanied by images, highlighting the potential of multimodal approaches to quality assessment, whereas Fromm et al. (2023  ###reference_b36###  ###reference_b36###) generalize the contextual scope of assessment to multiple domains at the same time."
        },
        {
            "section_id": "2.5",
            "parent_section_id": "2",
            "section_name": "2.5.   Computational Models",
            "text": "The majority of the 83 surveyed papers include empirical experiments with models for argument quality. However, we found that only 21 of them actually focus on proposing novel approaches targeting either of the above-mentioned conceptual quality notions, whereas 26 other papers have such approaches as secondary contributions to support their claims with experimental results and analysis. Almost all approaches aim at the assessment of argument quality, but a few recent ones go beyond assessment, studying how to improve quality.\nMany approaches aim at specific quality notions. For example, the attentive interaction model of Jo et al. (2018  ###reference_b54###  ###reference_b54###) predicts an opinion holder\u2019s view change by detecting vulnerable regions in their reasoning and modeling its relation to a challenger\u2019s argument. Gleize et al. (2019  ###reference_b39###  ###reference_b39###) propose a Siamese neural network to assess the convincingness of evidence, while Song et al. (2020  ###reference_b107###  ###reference_b107###) develop a hierarchical multitask learning approach to jointly model discourse element identification and organization assessment for essay scoring. Gurcke et al. (2021  ###reference_b44###  ###reference_b44###) examine to what extent an argument\u2019s logical sufficiency can be predicted based on whether its conclusion can be inferred from its premises using the generation capabilities of transformers. Kondo et al. (2021  ###reference_b64###  ###reference_b64###) assess the validity of an argument\u2019s reasoning using Bayesian networks and predicate logic facilitated by argumentation schemes. A few works also look beyond single quality notions, such as Falk and Lapesa (2023  ###reference_b34###  ###reference_b34###) who inject knowledge about the interactions between different quality notions to improve the prediction of individual ones.\n###figure_3### While only a few models for improvement have been presented so far, we expect more to come soon, also seeing related efforts on topics beyond those covered in this survey Chakrabarty et al. (2021  ###reference_b15###  ###reference_b15###); Ihori et al. (2022  ###reference_b51###  ###reference_b51###); Li et al. (2022  ###reference_b72###  ###reference_b72###). An early attempt was made by Ke et al. (2018  ###reference_b56###  ###reference_b56###) who design neural models that predict the persuasiveness and other attributes of arguments in a student essay, to provide feedback to students on how to improve their arguments. Recently, Skitalinskaya and Wachsmuth (2023  ###reference_b105###  ###reference_b105###) identified arguments in need of improvement, leveraging complex revision-based data with transformer models. Skitalinskaya et al. (2023  ###reference_b104###  ###reference_b104###) go one step further, presenting the first approach to the optimization of argumentative claims. It combines neural claim rewriting with quality-based ranking."
        },
        {
            "section_id": "2.6",
            "parent_section_id": "2",
            "section_name": "2.6.   Other Research Directions",
            "text": "Among the eight papers that do not fit under the three main research directions, we identified the following two rough research areas.\nFive papers deal with specific applications for which argument quality assessment is key. Rach et al. (2020  ###reference_b97###) and Kiesel et al. (2020  ###reference_b59###) target argument search, both taking a human-interaction perspective: The former studies the effects of integratting argument search into an avatar-based dialogue system; the latter investigates user expectations on voice-based argument search systems, such as preferred ranking criteria. Chalaguine and Hunter (2020  ###reference_b16###) develop a chatbot that relies on an argument graph for persuasive counterargument generation, and Falk et al. (2021  ###reference_b32###) address expert moderation in a deliberative forum, taking moderator interventions as implicit labels for the need to improve comment quality. Fromm et al. (2021  ###reference_b37###) use argument mining for analyzing peer reviews.\nThe other works tackle the bottleneck of (scarce) assessment training data. Heinisch et al. (2022  ###reference_b48###) employ data augmentation to support the prediction of argument validity and novelty. Kees et al. (2021  ###reference_b58###) evaluate active learning strategies for supporting argument strength estimation, and Yang et al. (2019  ###reference_b129###) introduce a quality control method that they apply to annotate argument acceptability."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   LLMs for Argument Quality",
            "text": "Section 2  ###reference_### stresses that a big part of argument quality research tackles the key challenges of diverse views of quality (by developing or refining notions) and subjectivity (by controlling or modeling influence factors). However, the intricated interdependencies between different quality notions and the various factors that influence them have hampered substantial progress in the reliable assessment of argument quality so far. We argue that manual rule-based scoring systems have the potential to overcome many limitations, if systematic ways to teach them accordingly are established. In this section, we start from the main advantages of such systems. Then, we outline what to instruct these systems with and how to do so (beyond simple in-context learning/prompting techniques) in order to advance these systems for argument quality in future research."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   Assessment without Instructions",
            "text": "Conceptually, argument quality assessment is a classification or regression problem, even if partly treated as preference learning. For decades, research in NLP relied on the traditional supervised learning paradigm for most such problems: to induce a mapping from one representational space to another using training pairs of input and output. As sketched in Figure 2  ###reference_###a, the input spaces (usually, representing natural language) and output spaces (label schemes or value ranges, such as argument quality scores) are separated thereby. This separation prevents any exchange of knowledge across spaces and across tasks.\nWith the shift to transformers in NLP, the learning effort is mostly reduced to the self-supervised pretraining of a language model Vaswani et al. (2017  ###reference_b118###); Devlin et al. (2019  ###reference_b21###). Under the transfer learning paradigm, only fine-tuning remains supervised, to make a model address the task it is supposed to. This way, knowledge is shared between input representations across tasks and contexts, that is, all texts ever processed affect how an input is encoded. In classification and regression, however, using a manual rule-based scoring system (say, of a BERT encoder with a quality scoring head) reintroduces a key restriction of traditional methods, illustrated in Figure 2  ###reference_###b: The input space is separated again from the output space, preventing models from fully leveraging knowledge acquired from solving other tasks.\nFine-tuning for text generation tasks keeps input and output in the same space; thereby, for example, connections between an argument and its improved version can be learned. Still, it faces a second restriction that is shown in Figure 2  ###reference_###b\u2019: The idea behind the mapping from input to output (why is an output correct for an input) remains fully implicit in the training pairs. For argument quality assessment, both restrictions imply that only those interdependencies between quality notions as well as those contextual influence factors are taken into account that are explicitly modeled or controlled by the human developer. Even though a lot of other interdependencies and factors may be well-known in argumentation theory van Eemeren and Grootendorst (2004  ###reference_b117###); Aristotle (ca. 350 B.C.E./ translated 2007  ###reference_b4###), they are widely ignored thereby. This is where instruction-following LLMs go beyond."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   Instruction-Following LLMs",
            "text": "Manual rule-based scoring system teaches LLMs to follow user instructions to solve tasks Peng et al. (2023  ###reference_b90###). As Figure 2  ###reference_###c stresses, it does so by representing the task in the joint space of inputs and outputs across all tasks and contexts, that is, in natural language (more precisely, in the embedding space in which language is encoded and from which it is decoded). Following the manual rule-based scoring system paradigm, deriving an output from an input remains a language modeling problem as well as how to operationalize the derivation. This means that all knowledge ever processed in pretraining is still accessible in principle (bounded by the technical constraints of the model). Then, the supreme capability of transformers to integrate and leverage knowledge across contexts enables instruction-following LLMs to tackle unseen tasks with no or little fine-tuning.\nIn argument quality assessment, we can expect that most knowledge about quality notions and their interdependencies as well as about influence factors and their effect on the subjective perception of argument quality has already been processed by leading LLMs, such as GPT-4 OpenAI (2023  ###reference_b83###) and Alpaca Taori et al. (2023  ###reference_b113###), in their pretraining stage. It should thus be possible to learn through instructions what is important in the assessment task at hand while not ignoring interactions with the surrounding concepts of argument quality."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "3.3.   Instructions for Assessment",
            "text": "Fine-tuning LLMs on general-purpose instruction data Wang et al. (2023b  ###reference_b127###) will help them solve language modeling tasks in principle. By default, however, LLMs do not necessarily have access to what is to be prioritized for the setting of the task at hand. We expect that this is the information an LLM needs to be instructed with to assess argument quality reliably. Accordingly, we see the survey results from Section 2  ###reference_### as an adequate basis to explore what to teach LLMs for the assessment. Instructions may thus include but are not limited to:\narguing goals, from agreement El Baff et al. (2018  ###reference_b29###) to deliberation Falk et al. (2021  ###reference_b32###);\ndefinition of various quality notions, be it for maximal quality Lauscher et al. (2020  ###reference_b68###) or minimal quality Jin et al. (2022  ###reference_b53###);\nspecificities of audiences Lukin et al. (2017  ###reference_b76###) and debaters Durmus and Cardie (2019  ###reference_b25###);\nbackground on controversial topics, such as other arguments Luu et al. (2019  ###reference_b77###) or relationships between the topics Zhao et al. (2021  ###reference_b130###);\nethical aspects, such as biases Holtermann et al. (2022  ###reference_b49###) and culture Chen et al. (2022a  ###reference_b17###);\nany examples of respective assessments, following the common few-shot learning idea.\nExemplarily, let us get back to the Huckleberry Finn claim from Section 1  ###reference_### taken from kialo.com. On this online platform, users create and refine claims, and they give impact votes from 1 to 5. These impact votes may serve as gold labels. For voting, users are asked to consider both a claim\u2019s persuasiveness and its relevance to the claim it replies to, in equal weight.222https://support.kialo-edu.com/en/hc/about-voting/\nWhen using such labels, supervised learning (Figure 2  ###reference_###a) and a manual rule-based scoring system (Figure 2  ###reference_###b/b\u2019) can learn the semantics of the task only through the mapping from claim to vote, risking spurious correlations Thorn Jakobsen et al. (2021  ###reference_b114###) as well as bias Splieth\u00f6ver and Wachsmuth (2020  ###reference_b108###). The equal weighting will likely not to be captured either, as users may not weigh systematically, may not have read instructions, or may just have a subjective perception.\nMoreover, the same claim should certainly be assessed differently depending on the setting (beyond kialo.com). From a deliberative perspective, for example, it lacks a concrete counterproposal (e.g., even if we do not censor, a preface should be added or teachers should discuss the load of the N-word in everyday life). Moving to quality improvement, the claim may need to be revised for specific audiences, for example, the concept of \u201ceveryday midwest American English\u201d may be completely opaque to people with low literacy.\nInstructions may overcome all these challenges. An example may be: \u201cRate the claim\u2019s quality from the perspective of deliberation, when presented to a person of low literacy\u201d. This makes the semantics of the task much more explicit, likely reducing biases and spurious correlations. It performs a stage-setting and an addressee-setting function which are crucial for assessment and improvement. We do not aim to come up with the best instructions in this paper, but see this as a task for future work. Rather, we discuss how to systematize respective manual rule-based scoring system attempts."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "3.4.   Blueprint for Instruction Fine-Tuning",
            "text": "Effective processes for the general manual rule-based scoring system of LLMs have already been established in prior work Taori et al. (2023  ###reference_b113###). Given the discussed advantages of such LLMs over previous models, we argue that argument quality assessment may be brought to the next level through systematic approaches to task-related manual rule-based scoring systems. The idea is to bring specific knowledge about theories, circumstances, and ethical constraints of arguing along with ways of how to solve argument-related problems into the scoring process. As a blueprint, such an approach could roughly consist of the following stages:\nStart from a general instruction-following LLM, such as Alpaca Taori et al. (2023  ###reference_b113###). Even some standard pretrained transformer may suffice, if general instructions are added to Step 2.\nAcquire a seed set of argumentation-specific instructions, covering concepts such as those in the previous subsection. For example, these instructions can be derived manually or semi-automatically from the various datasets and experiments covered in the surveyed papers.\nDepending on available resources, apply techniques such as reinforcement learning using human feedback Ouyang et al. (2022  ###reference_b84###), fine-tuning on self-generated instructions Wang et al. (2023b  ###reference_b127###), or other manual rule-based scoring mechanisms that are proposed in research.\nAlign the behavior of the manual rule-based LLM on new unseen tasks at hand using systematic prompt design, for example, via soft prompting Qin and Eisner (2021  ###reference_b96###) or sociodemographic prompting to emulate social profiles of debaters and audiences Beck et al. (2023  ###reference_b8###). Due to Step 3, these tasks now benefit from argument-specific task-solving skills.\nAt least for fact-related argument quality dimensions, such local acceptability Wachsmuth et al. (2017b  ###reference_b121###), an additional step may be to systematically work against hallucinations, by teaching the LLM to check arguments against some fact source (e.g., a knowledge base or a corpus). Factuality measures may be included in the model optimization for this purpose. We note, though, that this presupposes a setting in which sources can also be accessed at inference time. Besides, many quality dimensions are actually not (inherently) about facts, such as those from rhetorics Wachsmuth et al. (2017b  ###reference_b121###).\nWe expect that the resulting LLM will assess argument quality more reliably in line with the theories behind diverse quality notions and will adjust to the subjective viewpoints of interest. Thereby, various new opportunities emerge for real-world applications, as discussed in the next section."
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "3.5.   Evaluation of Quality Assessment",
            "text": "Finally, we make a note on the evaluation of LLM-based argument quality assessment, to give a basic guideline. Various ways of evaluating assessment have been pursued in prior work; particularly, there is a debate about whether quality should be assessed in absolute terms, based on a given score range, or in relative terms, comparing different arguments to one another Wachsmuth et al. (2017a  ###reference_b120###). Instruction-following LLMs might not entirely resolve the issue behind; while they provide new means for a reliable evaluation (e.g., handling of context), their generative nature may also complicate the validation against some ground-truth.\nUltimately, a fully unified evaluation procedure may not be possible, because it depends on what information is available in a given assessment setting. Rather, we propose that an evaluation procedure ideally takes into account the main decisive factors of quality, as we exemplified for the Huckleberry Finn claim above: What quality dimension is of interest, who is the audience of the argument, and similar. The evaluation may happen on a careful selection of existing datasets, but new benchmarks that account for the factors may be needed, too.\nCriteria-wise, we see a mix of absolute and relative assessment as best approximating how humans assess quality, but this requires careful operationalization: Many argument quality dimensions imply some hard constraints, which speaks for an absolute part (e.g., are the argument\u2019s premise acceptable?). However, there may not be a clear best/worst quality for an argument, which speaks for a relative part wherever other arguments are accessible (e.g., are the premises more acceptable than those of other arguments?). A manual rule-based scoring system should prepare an LLM for dealing with both parts and, hence, be evaluated against them."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Opportunities for the Real World",
            "text": "Arguably, instruction-following LLMs generally provide great opportunities for NLP and its applications. Their wide and easy applicability, along with their often low need for task-specific training data, is particularly beneficial in the context of interdisciplinary research. With a successful realization of the blueprint delineated above, however, we explicitly see specific potential for computational argumentation applications, due to their inherent need for argument quality assessment (see Section 1  ###reference_###). We now sketch some of the main opportunities we see. Partly, they bring up ethical concerns, though, that we discuss at the end of the paper (Section 6  ###reference_###).\nSo far, one of the most impressive applications is IBM\u2019s Project Debater, which has competed well with professional human debaters Slonim et al. (2021  ###reference_b106###). Its quality assessment methods are audience-agnostic Toledo et al. (2019  ###reference_b115###); Gretz et al. (2020  ###reference_b41###), which may not suffice to convince people across diverse backgrounds, as research indicates Alshomary et al. (2022  ###reference_b2###). Moreover, Project Debater\u2019s arguments are retrieved, recomposed, and rephrased rather than written naturally. If controlled well, the generation capabilities of LLMs may advance notably on the latter, whereas our proposed fine-tuning process may explicitly target the adjustment to audiences.\nArgument search aims to find the best pros and cons on controversial topics. Unlike for debating technologies, its goal to aid self-determined opinion formation suggests not to tune towards audiences. However, argument search engines miss a reliable quality-based ranking so far Wachsmuth et al. (2017c  ###reference_b122###); Stab et al. (2018  ###reference_b109###); Dumani et al. (2020  ###reference_b23###), likely due to the heterogeneity of argumentative domains and genres on the web. The low training need of instruction-following LLMs may alleviate this shortcoming. In addition, the text rewriting capabilities of LLMs may be employed to optimize the presentation of arguments Skitalinskaya et al. (2023  ###reference_b104###), or to fill gaps as needed. We expect that convincing rankings and presentations are key to making people open to argument search, enabling them to overcome filter bubbles.\nThe moderation of (online) content is critical to ensure healthy and productive discussions Park et al. (2012  ###reference_b88###, 2021  ###reference_b85###); Vecchi et al. (2021  ###reference_b119###). This holds particularly for deliberative contexts, where participants should be supported in communicating their viewpoints.\nEffective moderation reaches a bottleneck as the scale of online discussions grows Klein (2012  ###reference_b60###); Shortall et al. (2022  ###reference_b101###). LLMs instructed for argument quality can assist moderation efforts by detecting possible violations of community guidelines, inappropriate language, or generally low-quality arguments in discussions. This way, moderators can focus their attention on nuanced cases and appeals, optimizing efficiency and ensuring a healthier discourse. In some settings, generative LLMs could even lead a dialogue with users to provide clarifications, feedback, and improvement suggestions.\nLLMs may further provide individualized education to learners (e.g., students or non-native speakers) as well as to everyday writers (e.g., e-commerce customers), for instance, by giving feedback on the quality of their arguments Carlile et al. (2018  ###reference_b14###); Chen et al. (2022a  ###reference_b17###). Instruction fine-tuning makes it easier to go beyond simple quality scoring (e.g., how clear an argument is) to targeted hints (e.g., Provide more evidence for your initial claim!). Prompted with the writing goal, LLMs may also suggest argument completions, such as missing conclusions Gurcke et al. (2021  ###reference_b44###). With these means, students may learn to reason more soundly, product reviews can become more informative, and so forth. LLMs instructed with the concrete feedback scenario (e.g., a student learning to write essays in English) will help to further individualize support and may even adjust to the specific learning need of the user.\nIn several other scenarios, argument quality is crucial. One example is to generate summaries of the best arguments in news articles Syed et al. (2020  ###reference_b111###) or online discussions Syed et al. (2023  ###reference_b112###). Here, instruction-following LLMs can interpret the term best as needed\u2014without any task-specific fine-tuning. In the medical domain, argument quality plays a central role for evidence-based medicine Mayer et al. (2021  ###reference_b79###). A well-instructed LLM may assess evidence strength, thus enabling better inferences based on clinical trials or reports. Similarly, the reasonableness of arguments on health discussion online platforms may be evaluated.\nFurther scenarios include e-commerce. There, an LLM-based service chatbot can, for example, select arguments based on quality notions (e.g., clarity) to explain to customers why a request cannot be completed, to minimize their dissatisfaction. Argument quality may also be assessed in recommender systems to make justified suggestions based on compelling reasons.\nFinally, we also see great potential for diversity and subjectivity-aware instruction fine-tuning when it comes to driving fundamental research, as sketched here for two examples: interdisciplinary work at the interface of NLP and computational social science, and the methodological development driven by the need to cope with subjectivity in argument quality annotations.\nThe social science context adds even more diversity, including sophisticated quality notions and domain-specific language, along with new challenges, such as well-curated and annotated, but small and imbalanced datasets Falk and Lapesa (2022  ###reference_b33###). Our instruction fine-tuning blueprint fits exactly such scenarios: annotation guidelines serve as instructions, highly-curated annotations as reinforcement examples, and the knowledge encoded in LLMs alleviates resource-lean issues. Additionally, the scene-setting function of instruction fine-tuning (see Section 3  ###reference_###) has the potential to address the deliberative goal of defining and quantifying discourse quality across contexts Esau et al. (2021  ###reference_b31###).\nThe multiple factors of subjectivity influencing argument quality perception (debater and audience beliefs, values, etc.) often limit the inter-annotator agreement Wachsmuth et al. (2017b  ###reference_b121###). Ultimately, subjectivity is a constitutive feature of argument quality, as indicated above. Romberg (2022  ###reference_b98###) suggest to join the perspectivist turn of machine learning and NLP Plank (2022  ###reference_b93###); Cabitza et al. (2023  ###reference_b12###) in computational argumentation. LLMs\u2019 perspective-taking capabilities could be a game changer for this, assuming that the risks of sociodemographic prompting Beck et al. (2023  ###reference_b8###) and stereotypes Cheng et al. (2023  ###reference_b19###) are properly dealt with."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Conclusion",
            "text": "Argument quality assessment has become a core task in NLP research on computational argumentation, due to its importance for various applications, from debating technologies and argument search to discussion moderation and writing support. However, a reliable assessment is often hampered by the diversity of quality notions involved and the subjectivity of their perception. In this survey-based position paper, we have raised the question of how to drive research on instruction-following large language models (LLMs) for argument quality to substantially evolve the state of the art.\nOur survey of 83 recent papers confirms that argument quality research often targets conceptual quality notions and the factors that influence these notions, aside from the computational assessment and improvement of argument quality. We have argued that many limitations of prior work can be overcome, if LLMs are not just simply prompted for argument quality assessment, but if systematic ways to instruct LLMs for argument quality during instruction fine-tuning are found. This is due to the fact that instruction-following LLMs, for the first time in machine learning-based NLP research, make the connection between inputs and outputs of tasks explicit, namely, through the instructions. Thereby, all knowledge that an LLM has processed during pretraining and fine-tuning can be shared across tasks and contexts.\nTo guide future work in this direction, we have delineated a blueprint of how to approach the instruction fine-tuning process. Realizations of this process will likely bring up further problems, not all are foreseeable at this point. Moreover, LLMs that effectively predict human perception of argument quality directly raise concerns, as detailed in our ethics statement below. Still, we are confident that coordinated efforts towards sustainable research on LLMs for argument quality will enable the community to progress on core visions of computational argumentation\u2014whether it is about ways to overcome filter bubbles or about the individualized support of argumentation learners. The paper at hand seeks to lay the ground for this research."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Ethics Statement",
            "text": "Despite the huge potential of instruction-following LLMs for argument quality assessment across various applications of computational argumentation outlined in Section 4  ###reference_###, the blueprint from Section 3  ###reference_### also comes with limitations and ethical concerns. We acknowledge and analyze these in this section."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "6.1.   Limitations",
            "text": "The discussed potential we see is based on our survey of argument quality research (Section 2  ###reference_###), initial works of the emerging body of instruction fine-tuning research (e.g., Peng et al., 2023  ###reference_b90###), and our own preliminary tests. Yet, the work at hand remains a position paper, meaning that experimental research still needs to establish whether the outlined blueprint or similar paths will actually result in substantial progress. It is possible that argument-specific instruction fine-tuning of large language models (LLMs) does not improve over the capabilities of a general large-scale tuning. Also, the systematic ways that we have proposed to establish above remain to be found; there is no obvious way of directly obtaining them. This challenge is in line with the overall state of instruction fine-tuning research, both in academia and in industry.\nRegarding the specific challenges of argument quality raised in Section 1  ###reference_###, another limitation refers to general possibility that information required to achieve a realiable assessment is simply not available, due to specificities of the setting or underlying privacy regulations. This particularly includes the audience whose quality perception is to be represented, but possibly also aspects of the (temporal, geographical, and social) context in which an argument is to be considered. Also, as soon as we rely on human-created training data for instruction fine-tuning, the creators\u2019 biases and values affect its impact. Ultimately, we cannot expect LLMs to tackle a task reliably under conditions that simply do not suffice to make an informed judgment."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "6.2.   Ethical Concerns",
            "text": "Many arising ethical issues of the use of LLMs for argument quality assessment are general and not specific to computational argumentation, such as the increased environmental impact of bigger models, privacy issues, hallucinations, the potential of models to encode unfair exclusive Dev et al. (2021  ###reference_b20###); Lauscher et al. (2022a  ###reference_b67###) and stereotypical biases Blodgett et al. (2020  ###reference_b10###), which may result in allocational and representational harms (Barocas et al., 2017  ###reference_b7###). However, we believe that some of them deserve specific attention in scenarios where argument quality is assessed or optimized, particularly when leveraging the power of LLMs.\nIn particular, argument quality assessment may be used in sensitive applications such as digital education; for example, to support argumentative writing or to provide guidance on political opinion formation. For such applications, factual errors are particularly problematic, as they may easily lead to wrong or shifted beliefs. Whenever LLMs may generate argumentative content, say, for debating technologies or to fill gaps in argument search as discussed, extra measures should thus be taken to prevent hallucinations. We have sketched how to generally account for them in Section 3  ###reference_###, but fully avoiding them may be hard given how LLMs work.\nSimilarly, unfair social biases are easy to perpetuate in such applications, since the output of LLMs for argument quality assessment will often directly affect human views. This raises various integral and partly self-referential questions, such as who decides on what makes a good argument, or, how to decide on the ethical uses of instruction-following LLMs in argument quality assessment? We expect that universally-accepted answers to these questions may not exist, as they also depend on the values within a culture or society.\nAs the limitations discussed above imply, further ethical concerns refer to the tension between the inclusion of audience and debater information for a more accurate quality assessment. While an argument\u2019s persuasive effect is, for instance, highly dependent on the sociodemographic aspects of its audience, it is questionable in general to what extent an application of respective methods should have access to personal data. Such aspects need to be handled with care, and under consultation with an ethics board, where needed.\nFor a successful and societal beneficial use of instruction-following LLMs, we thus conclude that future research on argument quality assessment needs to find answers to such questions and to proactively raise and discuss them explicitly."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7.   Acknowledgments",
            "text": "This work was partially funded by the Deutsche Forschungsgemeinschaft (DFG) within the project OASiS, project number 455913891, as part of the Priority Program \u201cRobust Argumentation Machines (RATIO)\u201d (SPP-1999), as well as within the project ArgSchool, project number 453073654. It was also partially funded by the Bundesministerium f\u00fcr Bildung und Forschung (BMBF) within the project E-DELIB, project number 01IS20050, and under the Excellence Strategy of the German Federal Government and the States.\nThis work has also been supported by the French government, through the 3IA C\u00f4te d\u2019Azur Investments in the Future project managed by the National Research Agency (ANR) with the reference number ANR- 19-P3IA-0002."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "8.   Bibliographical References",
            "text": ""
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A List of Surveyed Papers",
            "text": "Table A  ###reference_### shows the full list of all 83 surveyed papers that resulted from our search and filtering process in Section 2  ###reference_###, along with the primary research direction and main aspect of each paper and the research community where the respective paper has been published: natural language processsing (NLP), artificial intelligence (AI), information retrieval (IR), or computational argumentation (CA). The research directions and aspects are detailed in Section 2  ###reference_### as well as selected papers from the list.\n###table_1###"
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T1\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"A1.T1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A1.T1.1.1.1\">\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"A1.T1.1.1.1.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T1.1.1.1.1.1\" style=\"font-size:70%;\">#</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"A1.T1.1.1.1.2\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T1.1.1.1.2.1\" style=\"font-size:70%;\">Research Direction</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"A1.T1.1.1.1.3\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T1.1.1.1.3.1\" style=\"font-size:70%;\">Main Aspect</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"A1.T1.1.1.1.4\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T1.1.1.1.4.1\" style=\"font-size:70%;\">Paper</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"A1.T1.1.1.1.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T1.1.1.1.5.1\" style=\"font-size:70%;\">Community</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.2.2\">\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A1.T1.1.2.2.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.2.2.1.1\" style=\"font-size:70%;\">1</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T1.1.2.2.2\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T1.1.2.2.2.1\" style=\"font-size:70%;\">Conceptual Notions</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T1.1.2.2.3\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.2.2.3.1\" style=\"font-size:70%;\">Notions of Maximal Quality</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T1.1.2.2.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Atkinson et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.2.2.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib5\" title=\"\">2019</a><span class=\"ltx_text\" id=\"A1.T1.1.2.2.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T1.1.2.2.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.2.2.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.3.3\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.3.3.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.3.3.1.1\" style=\"font-size:70%;\">2</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.3.3.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.3.3.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.3.3.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Beigman\u00a0Klebanov et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.3.3.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib9\" title=\"\">2017</a><span class=\"ltx_text\" id=\"A1.T1.1.3.3.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.3.3.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.3.3.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.4.4\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.4.4.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.4.4.1.1\" style=\"font-size:70%;\">3</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.4.4.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.4.4.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.4.4.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Dumani et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.4.4.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib23\" title=\"\">2020</a><span class=\"ltx_text\" id=\"A1.T1.1.4.4.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.4.4.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.4.4.5.1\" style=\"font-size:70%;\">IR</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.5.5\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.5.5.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.5.5.1.1\" style=\"font-size:70%;\">4</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.5.5.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.5.5.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.5.5.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">El\u00a0Baff et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.5.5.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib29\" title=\"\">2018</a><span class=\"ltx_text\" id=\"A1.T1.1.5.5.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.5.5.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.5.5.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.6.6\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.6.6.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.6.6.1.1\" style=\"font-size:70%;\">5</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.6.6.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.6.6.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.6.6.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Falk et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.6.6.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib32\" title=\"\">2021</a><span class=\"ltx_text\" id=\"A1.T1.1.6.6.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.6.6.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.6.6.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.7.7\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.7.7.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.7.7.1.1\" style=\"font-size:70%;\">6</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.7.7.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.7.7.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.7.7.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Gretz et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.7.7.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib41\" title=\"\">2020</a><span class=\"ltx_text\" id=\"A1.T1.1.7.7.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.7.7.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.7.7.5.1\" style=\"font-size:70%;\">AI</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.8.8\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.8.8.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.8.8.1.1\" style=\"font-size:70%;\">7</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.8.8.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.8.8.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.8.8.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Guo and Singh <span class=\"ltx_text\" id=\"A1.T1.1.8.8.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib43\" title=\"\">2023</a><span class=\"ltx_text\" id=\"A1.T1.1.8.8.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.8.8.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.8.8.5.1\" style=\"font-size:70%;\">AI</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.9.9\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.9.9.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.9.9.1.1\" style=\"font-size:70%;\">8</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.9.9.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.9.9.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.9.9.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Joshi et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.9.9.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib55\" title=\"\">2023</a><span class=\"ltx_text\" id=\"A1.T1.1.9.9.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.9.9.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.9.9.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.10.10\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.10.10.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.10.10.1.1\" style=\"font-size:70%;\">9</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.10.10.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.10.10.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.10.10.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Ke et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.10.10.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib57\" title=\"\">2019</a><span class=\"ltx_text\" id=\"A1.T1.1.10.10.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.10.10.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.10.10.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.11.11\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.11.11.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.11.11.1.1\" style=\"font-size:70%;\">10</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.11.11.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.11.11.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.11.11.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Kolhatkar and Taboada <span class=\"ltx_text\" id=\"A1.T1.1.11.11.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib63\" title=\"\">2017b</a><span class=\"ltx_text\" id=\"A1.T1.1.11.11.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.11.11.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.11.11.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.12.12\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.12.12.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.12.12.1.1\" style=\"font-size:70%;\">11</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.12.12.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.12.12.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.12.12.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Lauscher et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.12.12.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib68\" title=\"\">2020</a><span class=\"ltx_text\" id=\"A1.T1.1.12.12.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.12.12.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.12.12.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.13.13\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.13.13.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.13.13.1.1\" style=\"font-size:70%;\">12</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.13.13.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.13.13.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.13.13.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Ng et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.13.13.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib81\" title=\"\">2020</a><span class=\"ltx_text\" id=\"A1.T1.1.13.13.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.13.13.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.13.13.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.14.14\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.14.14.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.14.14.1.1\" style=\"font-size:70%;\">13</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.14.14.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.14.14.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.14.14.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Nguyen and Litman <span class=\"ltx_text\" id=\"A1.T1.1.14.14.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib82\" title=\"\">2018</a><span class=\"ltx_text\" id=\"A1.T1.1.14.14.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.14.14.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.14.14.5.1\" style=\"font-size:70%;\">AI</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.15.15\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.15.15.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.15.15.1.1\" style=\"font-size:70%;\">14</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.15.15.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.15.15.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.15.15.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Potthast et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.15.15.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib95\" title=\"\">2019</a><span class=\"ltx_text\" id=\"A1.T1.1.15.15.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.15.15.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.15.15.5.1\" style=\"font-size:70%;\">IR</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.16.16\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.16.16.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.16.16.1.1\" style=\"font-size:70%;\">15</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.16.16.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.16.16.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.16.16.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Shiota and Shimada <span class=\"ltx_text\" id=\"A1.T1.1.16.16.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib100\" title=\"\">2022</a><span class=\"ltx_text\" id=\"A1.T1.1.16.16.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.16.16.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.16.16.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.17.17\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.17.17.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.17.17.1.1\" style=\"font-size:70%;\">16</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.17.17.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.17.17.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.17.17.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Wachsmuth et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.17.17.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib120\" title=\"\">2017a</a><span class=\"ltx_text\" id=\"A1.T1.1.17.17.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.17.17.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.17.17.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.18.18\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.18.18.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.18.18.1.1\" style=\"font-size:70%;\">17</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.18.18.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.18.18.3\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.18.18.3.1\" style=\"font-size:70%;\">Notions of Minimal Quality</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.18.18.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Habernal et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.18.18.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib45\" title=\"\">2018</a><span class=\"ltx_text\" id=\"A1.T1.1.18.18.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.18.18.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.18.18.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.19.19\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.19.19.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.19.19.1.1\" style=\"font-size:70%;\">18</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.19.19.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.19.19.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.19.19.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Haworth et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.19.19.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib47\" title=\"\">2021</a><span class=\"ltx_text\" id=\"A1.T1.1.19.19.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.19.19.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.19.19.5.1\" style=\"font-size:70%;\">AI</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.20.20\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.20.20.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.20.20.1.1\" style=\"font-size:70%;\">19</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.20.20.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.20.20.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.20.20.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Kolhatkar and Taboada <span class=\"ltx_text\" id=\"A1.T1.1.20.20.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib62\" title=\"\">2017a</a><span class=\"ltx_text\" id=\"A1.T1.1.20.20.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.20.20.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.20.20.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.21.21\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.21.21.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.21.21.1.1\" style=\"font-size:70%;\">20</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.21.21.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.21.21.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.21.21.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Jin et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.21.21.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib53\" title=\"\">2022</a><span class=\"ltx_text\" id=\"A1.T1.1.21.21.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.21.21.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.21.21.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.22.22\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.22.22.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.22.22.1.1\" style=\"font-size:70%;\">21</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.22.22.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.22.22.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.22.22.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Park and Cardie <span class=\"ltx_text\" id=\"A1.T1.1.22.22.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib87\" title=\"\">2018</a><span class=\"ltx_text\" id=\"A1.T1.1.22.22.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.22.22.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.22.22.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.23.23\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.23.23.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.23.23.1.1\" style=\"font-size:70%;\">22</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.23.23.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.23.23.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.23.23.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Pauli et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.23.23.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib89\" title=\"\">2022</a><span class=\"ltx_text\" id=\"A1.T1.1.23.23.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.23.23.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.23.23.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.24.24\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.24.24.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.24.24.1.1\" style=\"font-size:70%;\">23</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.24.24.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.24.24.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.24.24.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Persing and Ng <span class=\"ltx_text\" id=\"A1.T1.1.24.24.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib92\" title=\"\">2017b</a><span class=\"ltx_text\" id=\"A1.T1.1.24.24.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.24.24.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.24.24.5.1\" style=\"font-size:70%;\">AI</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.25.25\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.25.25.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.25.25.1.1\" style=\"font-size:70%;\">24</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.25.25.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.25.25.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.25.25.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Ziegenbein et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.25.25.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib131\" title=\"\">2023</a><span class=\"ltx_text\" id=\"A1.T1.1.25.25.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.25.25.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.25.25.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.26.26\">\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A1.T1.1.26.26.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.26.26.1.1\" style=\"font-size:70%;\">25</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T1.1.26.26.2\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T1.1.26.26.2.1\" style=\"font-size:70%;\">Influence Factors</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T1.1.26.26.3\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.26.26.3.1\" style=\"font-size:70%;\">Argument-related Factors</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T1.1.26.26.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Carlile et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.26.26.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib14\" title=\"\">2018</a><span class=\"ltx_text\" id=\"A1.T1.1.26.26.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T1.1.26.26.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.26.26.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.27.27\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.27.27.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.27.27.1.1\" style=\"font-size:70%;\">26</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.27.27.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.27.27.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.27.27.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Chen et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.27.27.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib17\" title=\"\">2022a</a><span class=\"ltx_text\" id=\"A1.T1.1.27.27.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.27.27.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.27.27.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.28.28\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.28.28.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.28.28.1.1\" style=\"font-size:70%;\">27</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.28.28.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.28.28.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.28.28.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Durmus et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.28.28.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib26\" title=\"\">2019</a><span class=\"ltx_text\" id=\"A1.T1.1.28.28.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.28.28.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.28.28.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.29.29\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.29.29.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.29.29.1.1\" style=\"font-size:70%;\">28</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.29.29.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.29.29.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.29.29.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Egawa et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.29.29.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib27\" title=\"\">2019</a><span class=\"ltx_text\" id=\"A1.T1.1.29.29.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.29.29.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.29.29.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.30.30\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.30.30.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.30.30.1.1\" style=\"font-size:70%;\">29</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.30.30.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.30.30.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.30.30.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">El\u00a0Baff et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.30.30.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib30\" title=\"\">2020b</a><span class=\"ltx_text\" id=\"A1.T1.1.30.30.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.30.30.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.30.30.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.31.31\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.31.31.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.31.31.1.1\" style=\"font-size:70%;\">30</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.31.31.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.31.31.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.31.31.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Kobbe et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.31.31.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib61\" title=\"\">2020</a><span class=\"ltx_text\" id=\"A1.T1.1.31.31.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.31.31.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.31.31.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.32.32\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.32.32.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.32.32.1.1\" style=\"font-size:70%;\">31</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.32.32.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.32.32.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.32.32.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Li et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.32.32.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib71\" title=\"\">2020</a><span class=\"ltx_text\" id=\"A1.T1.1.32.32.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.32.32.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.32.32.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.33.33\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.33.33.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.33.33.1.1\" style=\"font-size:70%;\">32</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.33.33.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.33.33.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.33.33.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Luu et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.33.33.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib77\" title=\"\">2019</a><span class=\"ltx_text\" id=\"A1.T1.1.33.33.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.33.33.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.33.33.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.34.34\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.34.34.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.34.34.1.1\" style=\"font-size:70%;\">33</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.34.34.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.34.34.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.34.34.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Persing and Ng <span class=\"ltx_text\" id=\"A1.T1.1.34.34.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib91\" title=\"\">2017a</a><span class=\"ltx_text\" id=\"A1.T1.1.34.34.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.34.34.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.34.34.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.35.35\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.35.35.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.35.35.1.1\" style=\"font-size:70%;\">34</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.35.35.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.35.35.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.35.35.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Potash et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.35.35.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib94\" title=\"\">2017</a><span class=\"ltx_text\" id=\"A1.T1.1.35.35.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.35.35.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.35.35.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.36.36\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.36.36.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.36.36.1.1\" style=\"font-size:70%;\">35</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.36.36.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.36.36.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.36.36.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Skitalinskaya et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.36.36.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib103\" title=\"\">2021</a><span class=\"ltx_text\" id=\"A1.T1.1.36.36.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.36.36.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.36.36.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.37.37\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.37.37.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.37.37.1.1\" style=\"font-size:70%;\">36</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.37.37.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.37.37.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.37.37.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Toledo et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.37.37.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib115\" title=\"\">2019</a><span class=\"ltx_text\" id=\"A1.T1.1.37.37.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.37.37.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.37.37.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.38.38\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.38.38.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.38.38.1.1\" style=\"font-size:70%;\">37</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.38.38.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.38.38.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.38.38.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Wachsmuth and Werner <span class=\"ltx_text\" id=\"A1.T1.1.38.38.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib123\" title=\"\">2020</a><span class=\"ltx_text\" id=\"A1.T1.1.38.38.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.38.38.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.38.38.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.39.39\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.39.39.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.39.39.1.1\" style=\"font-size:70%;\">38</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.39.39.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.39.39.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.39.39.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Wang et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.39.39.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib125\" title=\"\">2017</a><span class=\"ltx_text\" id=\"A1.T1.1.39.39.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.39.39.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.39.39.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.40.40\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.40.40.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.40.40.1.1\" style=\"font-size:70%;\">39</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.40.40.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.40.40.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.40.40.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Wambsganss and Niklaus <span class=\"ltx_text\" id=\"A1.T1.1.40.40.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib124\" title=\"\">2022</a><span class=\"ltx_text\" id=\"A1.T1.1.40.40.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.40.40.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.40.40.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.41.41\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.41.41.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.41.41.1.1\" style=\"font-size:70%;\">40</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.41.41.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.41.41.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.41.41.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Zhao et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.41.41.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib130\" title=\"\">2021</a><span class=\"ltx_text\" id=\"A1.T1.1.41.41.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.41.41.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.41.41.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.42.42\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.42.42.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.42.42.1.1\" style=\"font-size:70%;\">41</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.42.42.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.42.42.3\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.42.42.3.1\" style=\"font-size:70%;\">Context-related Factors</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.42.42.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Al\u00a0Khatib et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.42.42.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib1\" title=\"\">2020</a><span class=\"ltx_text\" id=\"A1.T1.1.42.42.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.42.42.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.42.42.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.43.43\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.43.43.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.43.43.1.1\" style=\"font-size:70%;\">42</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.43.43.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.43.43.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.43.43.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Alshomary et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.43.43.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib2\" title=\"\">2022</a><span class=\"ltx_text\" id=\"A1.T1.1.43.43.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.43.43.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.43.43.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.44.44\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.44.44.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.44.44.1.1\" style=\"font-size:70%;\">43</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.44.44.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.44.44.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.44.44.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Durmus and Cardie <span class=\"ltx_text\" id=\"A1.T1.1.44.44.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib24\" title=\"\">2018</a><span class=\"ltx_text\" id=\"A1.T1.1.44.44.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.44.44.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.44.44.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.45.45\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.45.45.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.45.45.1.1\" style=\"font-size:70%;\">44</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.45.45.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.45.45.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.45.45.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Durmus and Cardie <span class=\"ltx_text\" id=\"A1.T1.1.45.45.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib25\" title=\"\">2019</a><span class=\"ltx_text\" id=\"A1.T1.1.45.45.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.45.45.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.45.45.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.46.46\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.46.46.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.46.46.1.1\" style=\"font-size:70%;\">45</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.46.46.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.46.46.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.46.46.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">El\u00a0Baff et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.46.46.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib28\" title=\"\">2020a</a><span class=\"ltx_text\" id=\"A1.T1.1.46.46.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.46.46.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.46.46.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.47.47\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.47.47.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.47.47.1.1\" style=\"font-size:70%;\">46</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.47.47.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.47.47.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.47.47.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Fromm et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.47.47.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib36\" title=\"\">2023</a><span class=\"ltx_text\" id=\"A1.T1.1.47.47.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.47.47.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.47.47.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.48.48\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.48.48.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.48.48.1.1\" style=\"font-size:70%;\">47</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.48.48.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.48.48.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.48.48.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Gu et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.48.48.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib42\" title=\"\">2018</a><span class=\"ltx_text\" id=\"A1.T1.1.48.48.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.48.48.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.48.48.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.49.49\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.49.49.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.49.49.1.1\" style=\"font-size:70%;\">48</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.49.49.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.49.49.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.49.49.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Hasan et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.49.49.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib46\" title=\"\">2021</a><span class=\"ltx_text\" id=\"A1.T1.1.49.49.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.49.49.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.49.49.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.50.50\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.50.50.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.50.50.1.1\" style=\"font-size:70%;\">49</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.50.50.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.50.50.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.50.50.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Kornilova et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.50.50.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib65\" title=\"\">2022</a><span class=\"ltx_text\" id=\"A1.T1.1.50.50.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.50.50.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.50.50.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.51.51\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.51.51.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.51.51.1.1\" style=\"font-size:70%;\">50</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.51.51.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.51.51.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.51.51.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Jain and Srivastava <span class=\"ltx_text\" id=\"A1.T1.1.51.51.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib52\" title=\"\">2021</a><span class=\"ltx_text\" id=\"A1.T1.1.51.51.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.51.51.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.51.51.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.52.52\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.52.52.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.52.52.1.1\" style=\"font-size:70%;\">51</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.52.52.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.52.52.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.52.52.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Liu et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.52.52.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib74\" title=\"\">2022</a><span class=\"ltx_text\" id=\"A1.T1.1.52.52.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.52.52.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.52.52.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.53.53\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.53.53.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.53.53.1.1\" style=\"font-size:70%;\">52</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.53.53.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.53.53.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.53.53.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Longpre et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.53.53.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib75\" title=\"\">2019</a><span class=\"ltx_text\" id=\"A1.T1.1.53.53.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.53.53.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.53.53.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.54.54\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.54.54.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.54.54.1.1\" style=\"font-size:70%;\">53</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.54.54.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.54.54.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.54.54.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Lukin et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.54.54.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib76\" title=\"\">2017</a><span class=\"ltx_text\" id=\"A1.T1.1.54.54.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.54.54.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.54.54.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.55.55\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.55.55.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.55.55.1.1\" style=\"font-size:70%;\">54</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.55.55.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.55.55.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.55.55.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Wiegmann et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.55.55.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib128\" title=\"\">2022</a><span class=\"ltx_text\" id=\"A1.T1.1.55.55.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.55.55.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.55.55.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.56.56\">\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A1.T1.1.56.56.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.56.56.1.1\" style=\"font-size:70%;\">55</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T1.1.56.56.2\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T1.1.56.56.2.1\" style=\"font-size:70%;\">Computational Models</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T1.1.56.56.3\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.56.56.3.1\" style=\"font-size:70%;\">Models for Assessment</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T1.1.56.56.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Ding et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.56.56.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib22\" title=\"\">2023</a><span class=\"ltx_text\" id=\"A1.T1.1.56.56.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T1.1.56.56.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.56.56.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.57.57\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.57.57.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.57.57.1.1\" style=\"font-size:70%;\">56</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.57.57.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.57.57.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.57.57.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Falk and Lapesa <span class=\"ltx_text\" id=\"A1.T1.1.57.57.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib33\" title=\"\">2022</a><span class=\"ltx_text\" id=\"A1.T1.1.57.57.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.57.57.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.57.57.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.58.58\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.58.58.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.58.58.1.1\" style=\"font-size:70%;\">57</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.58.58.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.58.58.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.58.58.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Falk and Lapesa <span class=\"ltx_text\" id=\"A1.T1.1.58.58.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib34\" title=\"\">2023</a><span class=\"ltx_text\" id=\"A1.T1.1.58.58.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.58.58.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.58.58.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.59.59\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.59.59.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.59.59.1.1\" style=\"font-size:70%;\">58</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.59.59.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.59.59.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.59.59.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Feger et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.59.59.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib35\" title=\"\">2020</a><span class=\"ltx_text\" id=\"A1.T1.1.59.59.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.59.59.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.59.59.5.1\" style=\"font-size:70%;\">CA</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.60.60\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.60.60.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.60.60.1.1\" style=\"font-size:70%;\">59</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.60.60.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.60.60.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.60.60.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Gienapp et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.60.60.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib38\" title=\"\">2020</a><span class=\"ltx_text\" id=\"A1.T1.1.60.60.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.60.60.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.60.60.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.61.61\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.61.61.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.61.61.1.1\" style=\"font-size:70%;\">69</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.61.61.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.61.61.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.61.61.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Gleize et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.61.61.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib39\" title=\"\">2019</a><span class=\"ltx_text\" id=\"A1.T1.1.61.61.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.61.61.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.61.61.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.62.62\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.62.62.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.62.62.1.1\" style=\"font-size:70%;\">61</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.62.62.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.62.62.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.62.62.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Gurcke et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.62.62.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib44\" title=\"\">2021</a><span class=\"ltx_text\" id=\"A1.T1.1.62.62.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.62.62.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.62.62.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.63.63\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.63.63.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.63.63.1.1\" style=\"font-size:70%;\">62</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.63.63.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.63.63.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.63.63.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Holtermann et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.63.63.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib49\" title=\"\">2022</a><span class=\"ltx_text\" id=\"A1.T1.1.63.63.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.63.63.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.63.63.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.64.64\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.64.64.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.64.64.1.1\" style=\"font-size:70%;\">63</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.64.64.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.64.64.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.64.64.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Huang et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.64.64.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib50\" title=\"\">2021</a><span class=\"ltx_text\" id=\"A1.T1.1.64.64.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.64.64.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.64.64.5.1\" style=\"font-size:70%;\">AI</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.65.65\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.65.65.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.65.65.1.1\" style=\"font-size:70%;\">64</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.65.65.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.65.65.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.65.65.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Jo et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.65.65.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib54\" title=\"\">2018</a><span class=\"ltx_text\" id=\"A1.T1.1.65.65.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.65.65.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.65.65.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.66.66\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.66.66.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.66.66.1.1\" style=\"font-size:70%;\">65</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.66.66.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.66.66.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.66.66.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Kondo et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.66.66.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib64\" title=\"\">2021</a><span class=\"ltx_text\" id=\"A1.T1.1.66.66.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.66.66.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.66.66.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.67.67\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.67.67.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.67.67.1.1\" style=\"font-size:70%;\">66</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.67.67.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.67.67.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.67.67.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Liu et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.67.67.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib73\" title=\"\">2021</a><span class=\"ltx_text\" id=\"A1.T1.1.67.67.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.67.67.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.67.67.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.68.68\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.68.68.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.68.68.1.1\" style=\"font-size:70%;\">67</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.68.68.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.68.68.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.68.68.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Marro et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.68.68.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib78\" title=\"\">2022</a><span class=\"ltx_text\" id=\"A1.T1.1.68.68.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.68.68.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.68.68.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.69.69\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.69.69.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.69.69.1.1\" style=\"font-size:70%;\">68</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.69.69.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.69.69.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.69.69.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Mim et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.69.69.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib80\" title=\"\">2019</a><span class=\"ltx_text\" id=\"A1.T1.1.69.69.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.69.69.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.69.69.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.70.70\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.70.70.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.70.70.1.1\" style=\"font-size:70%;\">69</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.70.70.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.70.70.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.70.70.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Saveleva et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.70.70.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib99\" title=\"\">2021</a><span class=\"ltx_text\" id=\"A1.T1.1.70.70.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.70.70.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.70.70.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.71.71\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.71.71.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.71.71.1.1\" style=\"font-size:70%;\">70</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.71.71.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.71.71.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.71.71.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Simpson and Gurevych <span class=\"ltx_text\" id=\"A1.T1.1.71.71.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib102\" title=\"\">2018</a><span class=\"ltx_text\" id=\"A1.T1.1.71.71.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.71.71.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.71.71.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.72.72\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.72.72.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.72.72.1.1\" style=\"font-size:70%;\">71</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.72.72.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.72.72.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.72.72.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Song et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.72.72.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib107\" title=\"\">2020</a><span class=\"ltx_text\" id=\"A1.T1.1.72.72.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.72.72.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.72.72.5.1\" style=\"font-size:70%;\">AI</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.73.73\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.73.73.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.73.73.1.1\" style=\"font-size:70%;\">72</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.73.73.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.73.73.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.73.73.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">van\u00a0der Meer et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.73.73.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib116\" title=\"\">2022</a><span class=\"ltx_text\" id=\"A1.T1.1.73.73.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.73.73.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.73.73.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.74.74\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.74.74.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.74.74.1.1\" style=\"font-size:70%;\">73</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.74.74.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.74.74.3\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.74.74.3.1\" style=\"font-size:70%;\">Models for Improvement</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.74.74.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Ke et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.74.74.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib56\" title=\"\">2018</a><span class=\"ltx_text\" id=\"A1.T1.1.74.74.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.74.74.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.74.74.5.1\" style=\"font-size:70%;\">AI</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.75.75\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.75.75.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.75.75.1.1\" style=\"font-size:70%;\">74</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.75.75.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.75.75.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.75.75.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Skitalinskaya and Wachsmuth <span class=\"ltx_text\" id=\"A1.T1.1.75.75.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib105\" title=\"\">2023</a><span class=\"ltx_text\" id=\"A1.T1.1.75.75.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.75.75.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.75.75.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.76.76\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.76.76.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.76.76.1.1\" style=\"font-size:70%;\">75</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.76.76.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.76.76.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.76.76.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Skitalinskaya et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.76.76.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib104\" title=\"\">2023</a><span class=\"ltx_text\" id=\"A1.T1.1.76.76.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.76.76.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.76.76.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.77.77\">\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A1.T1.1.77.77.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.77.77.1.1\" style=\"font-size:70%;\">76</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T1.1.77.77.2\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T1.1.77.77.2.1\" style=\"font-size:70%;\">Other</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T1.1.77.77.3\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.77.77.3.1\" style=\"font-size:70%;\">Other</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T1.1.77.77.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Chalaguine and Hunter <span class=\"ltx_text\" id=\"A1.T1.1.77.77.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib16\" title=\"\">2020</a><span class=\"ltx_text\" id=\"A1.T1.1.77.77.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T1.1.77.77.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.77.77.5.1\" style=\"font-size:70%;\">CA</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.78.78\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.78.78.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.78.78.1.1\" style=\"font-size:70%;\">77</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.78.78.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.78.78.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.78.78.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Chen et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.78.78.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib18\" title=\"\">2022b</a><span class=\"ltx_text\" id=\"A1.T1.1.78.78.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.78.78.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.78.78.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.79.79\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.79.79.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.79.79.1.1\" style=\"font-size:70%;\">78</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.79.79.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.79.79.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.79.79.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Fromm et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.79.79.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib37\" title=\"\">2021</a><span class=\"ltx_text\" id=\"A1.T1.1.79.79.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.79.79.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.79.79.5.1\" style=\"font-size:70%;\">AI</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.80.80\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.80.80.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.80.80.1.1\" style=\"font-size:70%;\">79</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.80.80.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.80.80.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.80.80.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Heinisch et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.80.80.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib48\" title=\"\">2022</a><span class=\"ltx_text\" id=\"A1.T1.1.80.80.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.80.80.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.80.80.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.81.81\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.81.81.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.81.81.1.1\" style=\"font-size:70%;\">80</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.81.81.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.81.81.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.81.81.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Kees et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.81.81.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib58\" title=\"\">2021</a><span class=\"ltx_text\" id=\"A1.T1.1.81.81.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.81.81.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.81.81.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.82.82\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.82.82.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.82.82.1.1\" style=\"font-size:70%;\">81</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.82.82.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.82.82.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.82.82.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Kiesel et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.82.82.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib59\" title=\"\">2020</a><span class=\"ltx_text\" id=\"A1.T1.1.82.82.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.82.82.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.82.82.5.1\" style=\"font-size:70%;\">IR</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.83.83\">\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T1.1.83.83.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.83.83.1.1\" style=\"font-size:70%;\">82</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.83.83.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.83.83.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.83.83.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Rach et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.83.83.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib97\" title=\"\">2020</a><span class=\"ltx_text\" id=\"A1.T1.1.83.83.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.83.83.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.83.83.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.84.84\">\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"A1.T1.1.84.84.1\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.84.84.1.1\" style=\"font-size:70%;\">83</span></td>\n<td class=\"ltx_td ltx_border_bb\" id=\"A1.T1.1.84.84.2\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_border_bb\" id=\"A1.T1.1.84.84.3\" style=\"padding:-0.25pt 10.0pt;\"></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A1.T1.1.84.84.4\" style=\"padding:-0.25pt 10.0pt;\"><cite class=\"ltx_cite ltx_citemacro_citet\">Yang et\u00a0al. <span class=\"ltx_text\" id=\"A1.T1.1.84.84.4.1.1.1.1\" style=\"font-size:70%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16084v1#bib.bib129\" title=\"\">2019</a><span class=\"ltx_text\" id=\"A1.T1.1.84.84.4.2.2.2.1\" style=\"font-size:70%;\">)</span></cite></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A1.T1.1.84.84.5\" style=\"padding:-0.25pt 10.0pt;\"><span class=\"ltx_text\" id=\"A1.T1.1.84.84.5.1\" style=\"font-size:70%;\">NLP</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:70%;\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>The list of all 83 surveyed NLP, AI, IR, and CA papers, ordered by their primary general research direction and main aspect and then by author names and year. 23 papers deal with <em class=\"ltx_emph ltx_font_italic\" id=\"A1.T1.12.1\">conceptual notions</em> primarily, 31 with <em class=\"ltx_emph ltx_font_italic\" id=\"A1.T1.13.2\">influence factors</em>, 21 with <em class=\"ltx_emph ltx_font_italic\" id=\"A1.T1.14.3\">computational models</em>, and eight have <em class=\"ltx_emph ltx_font_italic\" id=\"A1.T1.15.4\">other</em> primary directions.</figcaption>\n</figure>",
            "capture": "Table 1: The list of all 83 surveyed NLP, AI, IR, and CA papers, ordered by their primary general research direction and main aspect and then by author names and year. 23 papers deal with conceptual notions primarily, 31 with influence factors, 21 with computational models, and eight have other primary directions."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.16084v1_figure_1.png",
            "caption": "Figure 1: Organization of the surveyed argument quality research into three general directions (conceptual notions, influence factors, and computational models), their main aspects (e.g., notions of maximal and minimal quality), and specific concepts studied for these (e.g., agreement, preference, and deliberation)."
        },
        "2": {
            "figure_path": "2403.16084v1_figure_2.png",
            "caption": "Figure 2: Learning of representational spaces in NLP models (same color: same type of representation): (a) Traditional supervised learning: Input and output spaces are separated across tasks; representations are task-specific. (b) Classification/Regression transformer: The input space is shared across tasks; its representation can be learned on all tasks. (b\u2019) Generation transformer: Both spaces are shared across tasks; their representations can be learned on all tasks, but not task interactions. (c) Instruction-following transformer: One space for inputs, outputs, and tasks; representations can be learned jointly on all tasks."
        }
    },
    "references": [
        {
            "1": {
                "title": "Exploiting\npersonal characteristics of debaters for predicting persuasiveness.",
                "author": "Khalid Al Khatib, Michael V\u00f6lske, Shahbaz Syed, Nikolay Kolyada, and Benno\nStein. 2020.",
                "venue": "In Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 7067\u20137072, Online. Association for\nComputational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2020.acl-main.632"
            }
        },
        {
            "2": {
                "title": "The moral\ndebater: A study on the computational generation of morally framed\narguments.",
                "author": "Milad Alshomary, Roxanne El Baff, Timon Gurcke, and Henning Wachsmuth. 2022.",
                "venue": "In Proceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pages 8782\u20138797,\nDublin, Ireland. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2022.acl-long.601"
            }
        },
        {
            "3": {
                "title": "AI chat assistants can\nimprove conversations about divisive topics.",
                "author": "Lisa P. Argyle, Ethan Busby, Joshua Gubler, Chris Bail, Thomas Howe,\nChristopher Rytting, and David Wingate. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2302.07268"
            }
        },
        {
            "4": {
                "title": "On Rhetoric: A Theory of Civic Discourse.",
                "author": "Aristotle. ca. 350 B.C.E./ translated 2007.",
                "venue": "Oxford University Press, Oxford, UK.",
                "url": null
            }
        },
        {
            "5": {
                "title": "What gets echoed?\nunderstanding the \u201cpointers\u201d in explanations of persuasive arguments.",
                "author": "David Atkinson, Kumar Bhargav Srinivasan, and Chenhao Tan. 2019.",
                "venue": "In Proceedings of the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP), pages 2911\u20132921, Hong Kong,\nChina. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/D19-1289"
            }
        },
        {
            "6": {
                "title": "Advances in\ndebating technologies: Building AI that can debate humans.",
                "author": "Roy Bar-Haim, Liat Ein-Dor, Matan Orbach, Elad Venezian, and Noam Slonim. 2021.",
                "venue": "In Proceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International Joint Conference on\nNatural Language Processing: Tutorial Abstracts, pages 1\u20135, Online.\nAssociation for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.acl-tutorials.1"
            }
        },
        {
            "7": {
                "title": "The problem with bias: Allocative versus representational harms in\nmachine learning.",
                "author": "Solon Barocas, Kate Crawford, Aaron Shapiro, and Hanna Wallach. 2017.",
                "venue": "In Proceedings of 9th Annual Conference of the Special Interest\nGroup for Computing, Information and Society.",
                "url": null
            }
        },
        {
            "8": {
                "title": "How (not) to use\nsociodemographic information for subjective NLP tasks.",
                "author": "Tilman Beck, Hendrik Schuff, Anne Lauscher, and Iryna Gurevych. 2023.",
                "venue": "CoRR, abs/2309.07034.",
                "url": "https://doi.org/10.48550/arXiv.2309.07034"
            }
        },
        {
            "9": {
                "title": "Detecting good\narguments in a non-topic-specific way: An oxymoron?",
                "author": "Beata Beigman Klebanov, Binod Gyawali, and Yi Song. 2017.",
                "venue": "In Proceedings of the 55th Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Papers), pages 244\u2013249,\nVancouver, Canada. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/P17-2038"
            }
        },
        {
            "10": {
                "title": "Language\n(technology) is power: A critical survey of \u201cbias\u201d in NLP.",
                "author": "Su Lin Blodgett, Solon Barocas, Hal Daum\u00e9 III, and Hanna Wallach. 2020.",
                "venue": "In Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 5454\u20135476, Online. Association for\nComputational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2020.acl-main.485"
            }
        },
        {
            "11": {
                "title": "Advances in argument\nmining.",
                "author": "Katarzyna Budzynska and Chris Reed. 2019.",
                "venue": "In Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics: Tutorial Abstracts, pages 39\u201342, Florence,\nItaly. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/P19-4008"
            }
        },
        {
            "12": {
                "title": "Toward a\nperspectivist turn in ground truthing for predictive computing.",
                "author": "Federico Cabitza, Andrea Campagner, and Valerio Basile. 2023.",
                "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,\n37(6):6860\u20136868.",
                "url": "https://doi.org/10.1609/aaai.v37i6.25840"
            }
        },
        {
            "13": {
                "title": "Five years of\nargument mining: a data-driven analysis.",
                "author": "Elena Cabrio and Serena Villata. 2018.",
                "venue": "In Proceedings of the Twenty-Seventh International Joint\nConference on Artificial Intelligence, IJCAI-18, pages 5427\u20135433.\nInternational Joint Conferences on Artificial Intelligence Organization.",
                "url": "https://doi.org/10.24963/ijcai.2018/766"
            }
        },
        {
            "14": {
                "title": "Give me more feedback:\nAnnotating argument persuasiveness and related attributes in student essays.",
                "author": "Winston Carlile, Nishant Gurrapadi, Zixuan Ke, and Vincent Ng. 2018.",
                "venue": "In Proceedings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pages 621\u2013631,\nMelbourne, Australia. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/P18-1058"
            }
        },
        {
            "15": {
                "title": "ENTRUST:\nArgument reframing with language models and entailment.",
                "author": "Tuhin Chakrabarty, Christopher Hidey, and Smaranda Muresan. 2021.",
                "venue": "In Proceedings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language\nTechnologies, pages 4958\u20134971, Online. Association for Computational\nLinguistics.",
                "url": "https://doi.org/10.18653/v1/2021.naacl-main.394"
            }
        },
        {
            "16": {
                "title": "A Persuasive Chatbot Using a Crowd-Sourced Argument Graph and\nConcerns.",
                "author": "Lisa A. Chalaguine and Anthony Hunter. 2020.",
                "venue": "In Computational Models of Argument. Proceedings of COMMA\n2020, volume 326 of Frontiers in Artificial Intelligence and\nApplications, pages 9\u201320. IOS Press.",
                "url": null
            }
        },
        {
            "17": {
                "title": "Analyzing\nculture-specific argument structures in learner essays.",
                "author": "Wei-Fan Chen, Mei-Hua Chen, Garima Mudgal, and Henning Wachsmuth.\n2022a.",
                "venue": "In Proceedings of the 9th Workshop on Argument Mining, pages\n51\u201361, Online and in Gyeongju, Republic of Korea. International Conference\non Computational Linguistics.",
                "url": "https://aclanthology.org/2022.argmining-1.4"
            }
        },
        {
            "18": {
                "title": "Argument\nmining for review helpfulness prediction.",
                "author": "Zaiqian Chen, Daniel Verdi do Amarante, Jenna Donaldson, Yohan Jo, and Joonsuk\nPark. 2022b.",
                "venue": "In Proceedings of the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 8914\u20138922, Abu Dhabi, United Arab\nEmirates. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2022.emnlp-main.609"
            }
        },
        {
            "19": {
                "title": "Marked\npersonas: Using natural language prompts to measure stereotypes in language\nmodels.",
                "author": "Myra Cheng, Esin Durmus, and Dan Jurafsky. 2023.",
                "venue": "In Proceedings of the 61st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pages 1504\u20131532,\nToronto, Canada. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.acl-long.84"
            }
        },
        {
            "20": {
                "title": "Harms of\ngender exclusivity and challenges in non-binary representation in language\ntechnologies.",
                "author": "Sunipa Dev, Masoud Monajatipoor, Anaelia Ovalle, Arjun Subramonian, Jeff\nPhillips, and Kai-Wei Chang. 2021.",
                "venue": "In Proceedings of the 2021 Conference on Empirical Methods in\nNatural Language Processing, pages 1968\u20131994, Online and Punta Cana,\nDominican Republic. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.emnlp-main.150"
            }
        },
        {
            "21": {
                "title": "BERT: Pre-training of\ndeep bidirectional transformers for language understanding.",
                "author": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.",
                "venue": "In Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pages 4171\u20134186,\nMinneapolis, Minnesota. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/N19-1423"
            }
        },
        {
            "22": {
                "title": "Score it\nall together: A multi-task learning study on automatic scoring of\nargumentative essays.",
                "author": "Yuning Ding, Marie Bexte, and Andrea Horbach. 2023.",
                "venue": "In Findings of the Association for Computational Linguistics:\nACL 2023, pages 13052\u201313063, Toronto, Canada. Association for Computational\nLinguistics.",
                "url": "https://doi.org/10.18653/v1/2023.findings-acl.825"
            }
        },
        {
            "23": {
                "title": "A framework for argument retrieval.",
                "author": "Lorik Dumani, Patrick J. Neumann, and Ralf Schenkel. 2020.",
                "venue": "In Advances in Information Retrieval, pages 431\u2013445, Cham.\nSpringer International Publishing.",
                "url": null
            }
        },
        {
            "24": {
                "title": "Exploring the role of\nprior beliefs for argument persuasion.",
                "author": "Esin Durmus and Claire Cardie. 2018.",
                "venue": "In Proceedings of the 2018 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers), pages 1035\u20131045, New Orleans,\nLouisiana. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/N18-1094"
            }
        },
        {
            "25": {
                "title": "A corpus for modeling\nuser and language effects in argumentation on online debating.",
                "author": "Esin Durmus and Claire Cardie. 2019.",
                "venue": "In Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 602\u2013607, Florence, Italy. Association\nfor Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/P19-1057"
            }
        },
        {
            "26": {
                "title": "The role of pragmatic\nand discourse context in determining argument impact.",
                "author": "Esin Durmus, Faisal Ladhak, and Claire Cardie. 2019.",
                "venue": "In Proceedings of the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP), pages 5668\u20135678, Hong Kong,\nChina. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/D19-1568"
            }
        },
        {
            "27": {
                "title": "Annotating and\nanalyzing semantic role of elementary units and relations in online\npersuasive arguments.",
                "author": "Ryo Egawa, Gaku Morio, and Katsuhide Fujita. 2019.",
                "venue": "In Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics: Student Research Workshop, pages 422\u2013428,\nFlorence, Italy. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/P19-2059"
            }
        },
        {
            "28": {
                "title": "Persuasiveness of\nnews editorials depending on ideology and personality.",
                "author": "Roxanne El Baff, Khalid Al Khatib, Benno Stein, and Henning Wachsmuth.\n2020a.",
                "venue": "In Proceedings of the Third Workshop on Computational Modeling\nof People\u2019s Opinions, Personality, and Emotion\u2019s in Social Media, pages\n29\u201340, Barcelona, Spain (Online). Association for Computational Linguistics.",
                "url": "https://aclanthology.org/2020.peoples-1.4"
            }
        },
        {
            "29": {
                "title": "Challenge or empower:\nRevisiting argumentation quality in a news editorial corpus.",
                "author": "Roxanne El Baff, Henning Wachsmuth, Khalid Al-Khatib, and Benno Stein. 2018.",
                "venue": "In Proceedings of the 22nd Conference on Computational Natural\nLanguage Learning, pages 454\u2013464, Brussels, Belgium. Association for\nComputational Linguistics.",
                "url": "https://doi.org/10.18653/v1/K18-1044"
            }
        },
        {
            "30": {
                "title": "Analyzing the\npersuasive effect of style in news editorial argumentation.",
                "author": "Roxanne El Baff, Henning Wachsmuth, Khalid Al Khatib, and Benno Stein.\n2020b.",
                "venue": "In Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 3154\u20133160, Online. Association for\nComputational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2020.acl-main.287"
            }
        },
        {
            "31": {
                "title": "Different\narenas, different deliberative quality? using a systemic framework to\nevaluate online deliberation on immigration policy in germany.",
                "author": "Katharina Esau, Dannica Fleu\u00df, and Sarah-Michelle Nienhaus. 2021.",
                "venue": "Policy & Internet, 13(1):86\u2013112.",
                "url": "https://doi.org/https://doi.org/10.1002/poi3.232"
            }
        },
        {
            "32": {
                "title": "Predicting\nmoderation of deliberative arguments: Is argument quality the key?",
                "author": "Neele Falk, Iman Jundi, Eva Maria Vecchi, and Gabriella Lapesa. 2021.",
                "venue": "In Proceedings of the 8th Workshop on Argument Mining, pages\n133\u2013141, Punta Cana, Dominican Republic. Association for Computational\nLinguistics.",
                "url": "https://doi.org/10.18653/v1/2021.argmining-1.13"
            }
        },
        {
            "33": {
                "title": "Scaling up\ndiscourse quality annotation for political science.",
                "author": "Neele Falk and Gabriella Lapesa. 2022.",
                "venue": "In Proceedings of the Thirteenth Language Resources and\nEvaluation Conference, pages 3301\u20133318, Marseille, France. European\nLanguage Resources Association.",
                "url": "https://aclanthology.org/2022.lrec-1.353"
            }
        },
        {
            "34": {
                "title": "Bridging\nargument quality and deliberative quality annotations with adapters.",
                "author": "Neele Falk and Gabriella Lapesa. 2023.",
                "venue": "In Findings of the Association for Computational Linguistics:\nEACL 2023, pages 2469\u20132488, Dubrovnik, Croatia. Association for\nComputational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.findings-eacl.187"
            }
        },
        {
            "35": {
                "title": "Structure or Content?\nTowards Assessing Argument Relevance.",
                "author": "Marc Feger, Jan Steimann, and Christian Meter. 2020.",
                "venue": "In Computational Models of Argument. Proceedings of COMMA\n2020, volume 326 of Frontiers in Artificial Intelligence and\nApplications, pages 203\u2013214. IOS Press.",
                "url": "https://doi.org/10.3233/FAIA200505"
            }
        },
        {
            "36": {
                "title": "Cross-domain argument quality estimation.",
                "author": "Michael Fromm, Max Berrendorf, Evgeniy Faerman, and Thomas Seidl. 2023.",
                "venue": "In Findings of the Association for Computational Linguistics:\nACL 2023, pages 13435\u201313448, Toronto, Canada. Association for Computational\nLinguistics.",
                "url": "https://doi.org/10.18653/v1/2023.findings-acl.848"
            }
        },
        {
            "37": {
                "title": "Argument mining driven analysis of peer-reviews.",
                "author": "Michael Fromm, Evgeniy Faerman, Max Berrendorf, Siddharth Bhargava, Ruoxia Qi,\nYao Zhang, Lukas Dennert, Sophia Selle, Yang Mao, and Thomas Seidl. 2021.",
                "venue": "In Proceedings of the AAAI Conference on Artificial\nIntelligence, 6, pages 4758\u20134766. AAAI Press.",
                "url": null
            }
        },
        {
            "38": {
                "title": "Efficient\npairwise annotation of argument quality.",
                "author": "Lukas Gienapp, Benno Stein, Matthias Hagen, and Martin Potthast. 2020.",
                "venue": "In Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 5772\u20135781, Online. Association for\nComputational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2020.acl-main.511"
            }
        },
        {
            "39": {
                "title": "Are you convinced?\nChoosing the more convincing evidence with a Siamese network.",
                "author": "Martin Gleize, Eyal Shnarch, Leshem Choshen, Lena Dankin, Guy Moshkowich, Ranit\nAharonov, and Noam Slonim. 2019.",
                "venue": "In Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 967\u2013976, Florence, Italy. Association\nfor Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/P19-1093"
            }
        },
        {
            "40": {
                "title": "Fallacious argument\nclassification in political debates.",
                "author": "Pierpaolo Goffredo, Shohreh Haddadan, Vorakit Vorakitphan, Elena Cabrio, and\nSerena Villata. 2022.",
                "venue": "In Proceedings of the Thirty-First International Joint\nConference on Artificial Intelligence, IJCAI-22, pages 4143\u20134149.\nInternational Joint Conferences on Artificial Intelligence Organization.",
                "url": "https://doi.org/10.24963/ijcai.2022/575"
            }
        },
        {
            "41": {
                "title": "A large-scale dataset for argument quality ranking: Construction\nand analysis.",
                "author": "Shai Gretz, Roni Friedman, Edo Cohen-Karlik, Assaf Toledo, Dan Lahav, Ranit\nAharonov, and Noam Slonim. 2020.",
                "venue": "In Proceedings of the Thirty-Fourth AAAI Conference on\nArtificial Intelligence, pages 7805\u20137813. AAAI.",
                "url": null
            }
        },
        {
            "42": {
                "title": "Incorporating topic\naspects for online comment convincingness evaluation.",
                "author": "Yunfan Gu, Zhongyu Wei, Maoran Xu, Hao Fu, Yang Liu, and Xuanjing Huang. 2018.",
                "venue": "In Proceedings of the 5th Workshop on Argument Mining, pages\n97\u2013104, Brussels, Belgium. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/W18-5212"
            }
        },
        {
            "43": {
                "title": "Representing and\ndetermining argumentative relevance in online discussions: A general\napproach.",
                "author": "Zhen Guo and Munindar P. Singh. 2023.",
                "venue": "Proceedings of the International AAAI Conference on Web and\nSocial Media, 17(1):292\u2013302.",
                "url": "https://doi.org/10.1609/icwsm.v17i1.22146"
            }
        },
        {
            "44": {
                "title": "Assessing the\nsufficiency of arguments through conclusion generation.",
                "author": "Timon Gurcke, Milad Alshomary, and Henning Wachsmuth. 2021.",
                "venue": "In Proceedings of the 8th Workshop on Argument Mining, pages\n67\u201377, Punta Cana, Dominican Republic. Association for Computational\nLinguistics.",
                "url": "https://doi.org/10.18653/v1/2021.argmining-1.7"
            }
        },
        {
            "45": {
                "title": "Before name-calling:\nDynamics and triggers of ad hominem fallacies in web argumentation.",
                "author": "Ivan Habernal, Henning Wachsmuth, Iryna Gurevych, and Benno Stein. 2018.",
                "venue": "In Proceedings of the 2018 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers), pages 386\u2013396. Association for\nComputational Linguistics.",
                "url": "http://aclweb.org/anthology/N18-1036"
            }
        },
        {
            "46": {
                "title": "Hitting your\nMARQ: Multimodal ARgument quality assessment in long debate video.",
                "author": "Md Kamrul Hasan, James Spann, Masum Hasan, Md Saiful Islam, Kurtis Haut, Rada\nMihalcea, and Ehsan Hoque. 2021.",
                "venue": "In Proceedings of the 2021 Conference on Empirical Methods in\nNatural Language Processing, pages 6387\u20136397, Online and Punta Cana,\nDominican Republic. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.emnlp-main.515"
            }
        },
        {
            "47": {
                "title": "Classifying\nreasonability in retellings of personal events shared on social media: A\npreliminary case study with /r/amitheasshole.",
                "author": "Ethan Haworth, Ted Grover, Justin Langston, Ankush Patel, Joseph West, and\nAlex C. Williams. 2021.",
                "venue": "Proceedings of the International AAAI Conference on Web and\nSocial Media, 15(1):1075\u20131079.",
                "url": "https://doi.org/10.1609/icwsm.v15i1.18133"
            }
        },
        {
            "48": {
                "title": "Data\naugmentation for improving the prediction of validity and novelty of\nargumentative conclusions.",
                "author": "Philipp Heinisch, Moritz Plenz, Juri Opitz, Anette Frank, and Philipp Cimiano.\n2022.",
                "venue": "In Proceedings of the 9th Workshop on Argument Mining, pages\n19\u201333, Online and in Gyeongju, Republic of Korea. International Conference\non Computational Linguistics.",
                "url": "https://aclanthology.org/2022.argmining-1.2"
            }
        },
        {
            "49": {
                "title": "Fair and\nargumentative language modeling for computational argumentation.",
                "author": "Carolin Holtermann, Anne Lauscher, and Simone Ponzetto. 2022.",
                "venue": "In Proceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pages 7841\u20137861,\nDublin, Ireland. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2022.acl-long.541"
            }
        },
        {
            "50": {
                "title": "Hargan:\nHeterogeneous argument attention network for persuasiveness prediction.",
                "author": "Kuo-Yu Huang, Hen-Hsen Huang, and Hsin-Hsi Chen. 2021.",
                "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,\n35(14):13045\u201313054.",
                "url": "https://doi.org/10.1609/aaai.v35i14.17542"
            }
        },
        {
            "51": {
                "title": "Multi-perspective\ndocument revision.",
                "author": "Mana Ihori, Hiroshi Sato, Tomohiro Tanaka, and Ryo Masumura. 2022.",
                "venue": "In Proceedings of the 29th International Conference on\nComputational Linguistics, pages 6128\u20136138, Gyeongju, Republic of Korea.\nInternational Committee on Computational Linguistics.",
                "url": "https://aclanthology.org/2022.coling-1.535"
            }
        },
        {
            "52": {
                "title": "Does social\npressure drive persuasion in online fora?",
                "author": "Ayush Jain and Shashank Srivastava. 2021.",
                "venue": "In Proceedings of the 2021 Conference on Empirical Methods in\nNatural Language Processing, pages 9201\u20139208, Online and Punta Cana,\nDominican Republic. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.emnlp-main.725"
            }
        },
        {
            "53": {
                "title": "Logical\nfallacy detection.",
                "author": "Zhijing Jin, Abhinav Lalwani, Tejas Vaidhya, Xiaoyu Shen, Yiwen Ding, Zhiheng\nLyu, Mrinmaya Sachan, Rada Mihalcea, and Bernhard Schoelkopf. 2022.",
                "venue": "In Findings of the Association for Computational Linguistics:\nEMNLP 2022, pages 7180\u20137198, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2022.findings-emnlp.532"
            }
        },
        {
            "54": {
                "title": "Attentive interaction\nmodel: Modeling changes in view in argumentation.",
                "author": "Yohan Jo, Shivani Poddar, Byungsoo Jeon, Qinlan Shen, Carolyn Ros\u00e9, and\nGraham Neubig. 2018.",
                "venue": "In Proceedings of the 2018 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers), pages 103\u2013116, New Orleans,\nLouisiana. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/N18-1010"
            }
        },
        {
            "55": {
                "title": "ArgAnalysis35K : A large-scale dataset for argument quality analysis.",
                "author": "Omkar Joshi, Priya Pitre, and Yashodhara Haribhakta. 2023.",
                "venue": "In Proceedings of the 61st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pages 13916\u201313931,\nToronto, Canada. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.acl-long.778"
            }
        },
        {
            "56": {
                "title": "Learning to give\nfeedback: Modeling attributes affecting argument persuasiveness in student\nessays.",
                "author": "Zixuan Ke, Winston Carlile, Nishant Gurrapadi, and Vincent Ng. 2018.",
                "venue": "In Proceedings of the Twenty-Seventh International Joint\nConference on Artificial Intelligence, IJCAI-18, pages 4130\u20134136.\nInternational Joint Conferences on Artificial Intelligence Organization.",
                "url": "https://doi.org/10.24963/ijcai.2018/574"
            }
        },
        {
            "57": {
                "title": "Give me more feedback\nII: Annotating thesis strength and related attributes in student essays.",
                "author": "Zixuan Ke, Hrishikesh Inamdar, Hui Lin, and Vincent Ng. 2019.",
                "venue": "In Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 3994\u20134004, Florence, Italy.\nAssociation for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/P19-1390"
            }
        },
        {
            "58": {
                "title": "Active\nlearning for argument strength estimation.",
                "author": "Nataliia Kees, Michael Fromm, Evgeniy Faerman, and Thomas Seidl. 2021.",
                "venue": "In Proceedings of the Second Workshop on Insights from Negative\nResults in NLP, pages 144\u2013150, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.insights-1.20"
            }
        },
        {
            "59": {
                "title": "Investigating\nexpectations for voice-based and conversational argument search on the web.",
                "author": "Johannes Kiesel, Kevin Lang, Henning Wachsmuth, Eva Hornecker, and Benno Stein.\n2020.",
                "venue": "In Proceedings of the 2020 Conference on Human Information\nInteraction & Retrieval (CHIIR 2020), CHIIR \u201920, pages 53\u201362, New York,\nNY, USA. Association for Computing Machinery.",
                "url": "https://doi.org/10.1145/3343413.3377978"
            }
        },
        {
            "60": {
                "title": "Enabling large-scale deliberation using attention-mediation metrics.",
                "author": "Mark Klein. 2012.",
                "venue": "Computer Supported Cooperative Work (CSCW), 21:449\u2013473.",
                "url": null
            }
        },
        {
            "61": {
                "title": "Exploring\nmorality in argumentation.",
                "author": "Jonathan Kobbe, Ines Rehbein, Ioana Hulpu\\textcommabelows, and Heiner\nStuckenschmidt. 2020.",
                "venue": "In Proceedings of the 7th Workshop on Argument Mining, pages\n30\u201340, Online. Association for Computational Linguistics.",
                "url": "https://aclanthology.org/2020.argmining-1.4"
            }
        },
        {
            "62": {
                "title": "Constructive language\nin news comments.",
                "author": "Varada Kolhatkar and Maite Taboada. 2017a.",
                "venue": "In Proceedings of the First Workshop on Abusive Language\nOnline, pages 11\u201317, Vancouver, BC, Canada. Association for Computational\nLinguistics.",
                "url": "https://doi.org/10.18653/v1/W17-3002"
            }
        },
        {
            "63": {
                "title": "Using New York\nTimes picks to identify constructive comments.",
                "author": "Varada Kolhatkar and Maite Taboada. 2017b.",
                "venue": "In Proceedings of the 2017 EMNLP Workshop: Natural Language\nProcessing meets Journalism, pages 100\u2013105, Copenhagen, Denmark.\nAssociation for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/W17-4218"
            }
        },
        {
            "64": {
                "title": "Bayesian\nargumentation-scheme networks: A probabilistic model of argument validity\nfacilitated by argumentation schemes.",
                "author": "Takahiro Kondo, Koki Washio, Katsuhiko Hayashi, and Yusuke Miyao. 2021.",
                "venue": "In Proceedings of the 8th Workshop on Argument Mining, pages\n112\u2013124, Punta Cana, Dominican Republic. Association for Computational\nLinguistics.",
                "url": "https://doi.org/10.18653/v1/2021.argmining-1.11"
            }
        },
        {
            "65": {
                "title": "An item\nresponse theory framework for persuasion.",
                "author": "Anastassia Kornilova, Vladimir Eidelman, and Daniel Douglass. 2022.",
                "venue": "In Findings of the Association for Computational Linguistics:\nNAACL 2022, pages 77\u201386, Seattle, United States. Association for\nComputational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2022.findings-naacl.7"
            }
        },
        {
            "66": {
                "title": "Mining,\nassessing, and improving arguments in NLP and the social sciences.",
                "author": "Gabriella Lapesa, Eva Maria Vecchi, Serena Villata, and Henning Wachsmuth.\n2023.",
                "venue": "In Proceedings of the 17th Conference of the European Chapter\nof the Association for Computational Linguistics: Tutorial Abstracts, pages\n1\u20136, Dubrovnik, Croatia. Association for Computational Linguistics.",
                "url": "https://aclanthology.org/2023.eacl-tutorials.1"
            }
        },
        {
            "67": {
                "title": "Welcome to the\nmodern world of pronouns: Identity-inclusive natural language processing\nbeyond gender.",
                "author": "Anne Lauscher, Archie Crowley, and Dirk Hovy. 2022a.",
                "venue": "In Proceedings of the 29th International Conference on\nComputational Linguistics, pages 1221\u20131232, Gyeongju, Republic of Korea.\nInternational Committee on Computational Linguistics.",
                "url": "https://aclanthology.org/2022.coling-1.105"
            }
        },
        {
            "68": {
                "title": "Rhetoric,\nlogic, and dialectic: Advancing theory-based argument quality assessment in\nnatural language processing.",
                "author": "Anne Lauscher, Lily Ng, Courtney Napoles, and Joel Tetreault. 2020.",
                "venue": "In Proceedings of the 28th International Conference on\nComputational Linguistics, pages 4563\u20134574, Barcelona, Spain (Online).\nInternational Committee on Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2020.coling-main.402"
            }
        },
        {
            "69": {
                "title": "Scientia potentia\nEst\u2014On the role of knowledge in computational argumentation.",
                "author": "Anne Lauscher, Henning Wachsmuth, Iryna Gurevych, and Goran Glava\u0161.\n2022b.",
                "venue": "Transactions of the Association for Computational Linguistics,\n10:1392\u20131422.",
                "url": "https://doi.org/10.1162/tacl_a_00525"
            }
        },
        {
            "70": {
                "title": "Argument mining: A\nsurvey.",
                "author": "John Lawrence and Chris Reed. 2019.",
                "venue": "Computational Linguistics, 45(4):765\u2013818.",
                "url": "https://doi.org/10.1162/coli_a_00364"
            }
        },
        {
            "71": {
                "title": "Exploring\nthe role of argument structure in online debate persuasion.",
                "author": "Jialu Li, Esin Durmus, and Claire Cardie. 2020.",
                "venue": "In Proceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pages 8905\u20138912, Online. Association\nfor Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2020.emnlp-main.716"
            }
        },
        {
            "72": {
                "title": "Text revision by\non-the-fly representation optimization.",
                "author": "Jingjing Li, Zichao Li, Tao Ge, Irwin King, and Michael R. Lyu. 2022.",
                "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,\n36(10):10956\u201310964.",
                "url": "https://doi.org/10.1609/aaai.v36i10.21343"
            }
        },
        {
            "73": {
                "title": "Exploring\ndiscourse structures for argument impact classification.",
                "author": "Xin Liu, Jiefu Ou, Yangqiu Song, and Xin Jiang. 2021.",
                "venue": "In Proceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International Joint Conference on\nNatural Language Processing (Volume 1: Long Papers), pages 3958\u20133969,\nOnline. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.acl-long.306"
            }
        },
        {
            "74": {
                "title": "ImageArg: A\nmulti-modal tweet dataset for image persuasiveness mining.",
                "author": "Zhexiong Liu, Meiqi Guo, Yue Dai, and Diane Litman. 2022.",
                "venue": "In Proceedings of the 9th Workshop on Argument Mining, pages\n1\u201318, Online and in Gyeongju, Republic of Korea. International Conference on\nComputational Linguistics.",
                "url": "https://aclanthology.org/2022.argmining-1.1"
            }
        },
        {
            "75": {
                "title": "Persuasion of the\nundecided: Language vs. the listener.",
                "author": "Liane Longpre, Esin Durmus, and Claire Cardie. 2019.",
                "venue": "In Proceedings of the 6th Workshop on Argument Mining, pages\n167\u2013176, Florence, Italy. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/W19-4519"
            }
        },
        {
            "76": {
                "title": "Argument Strength is\nin the Eye of the Beholder: Audience Effects in Persuasion.",
                "author": "Stephanie Lukin, Pranav Anand, Marilyn Walker, and Steve Whittaker. 2017.",
                "venue": "In Proceedings of the 15th Conference of the European Chapter\nof the Association for Computational Linguistics: Volume 1, Long Papers,\npages 742\u2013753. Association for Computational Linguistics.",
                "url": "http://aclweb.org/anthology/E17-1070"
            }
        },
        {
            "77": {
                "title": "Measuring online\ndebaters\u2019 persuasive skill from text over time.",
                "author": "Kelvin Luu, Chenhao Tan, and Noah A. Smith. 2019.",
                "venue": "Transactions of the Association for Computational Linguistics,\n7:537\u2013550.",
                "url": "https://doi.org/10.1162/tacl_a_00281"
            }
        },
        {
            "78": {
                "title": "Graph\nembeddings for argumentation quality assessment.",
                "author": "Santiago Marro, Elena Cabrio, and Serena Villata. 2022.",
                "venue": "In Findings of the Association for Computational Linguistics:\nEMNLP 2022, pages 4154\u20134164, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.",
                "url": "https://aclanthology.org/2022.findings-emnlp.306"
            }
        },
        {
            "79": {
                "title": "Enhancing\nevidence-based medicine with natural language argumentative analysis of\nclinical trials.",
                "author": "Tobias Mayer, Santiago Marro, Elena Cabrio, and Serena Villata. 2021.",
                "venue": "Artif. Intell. Medicine, 118:102098.",
                "url": "https://doi.org/10.1016/j.artmed.2021.102098"
            }
        },
        {
            "80": {
                "title": "Unsupervised learning\nof discourse-aware text representation for essay scoring.",
                "author": "Farjana Sultana Mim, Naoya Inoue, Paul Reisert, Hiroki Ouchi, and Kentaro Inui.\n2019.",
                "venue": "In Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics: Student Research Workshop, pages 378\u2013385,\nFlorence, Italy. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/P19-2053"
            }
        },
        {
            "81": {
                "title": "Creating a\ndomain-diverse corpus for theory-based argument quality assessment.",
                "author": "Lily Ng, Anne Lauscher, Joel Tetreault, and Courtney Napoles. 2020.",
                "venue": "In Proceedings of the 7th Workshop on Argument Mining, pages\n117\u2013126, Online. Association for Computational Linguistics.",
                "url": "https://aclanthology.org/2020.argmining-1.13"
            }
        },
        {
            "82": {
                "title": "Argument mining for\nimproving the automated scoring of persuasive essays.",
                "author": "Huy Nguyen and Diane Litman. 2018.",
                "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,\n32(1).",
                "url": "https://doi.org/10.1609/aaai.v32i1.12046"
            }
        },
        {
            "83": {
                "title": "GPT-4 technical report.",
                "author": "OpenAI. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2303.08774"
            }
        },
        {
            "84": {
                "title": "Training language models to\nfollow instructions with human feedback.",
                "author": "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda\nAskell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022.",
                "venue": null,
                "url": "http://arxiv.org/abs/2203.02155"
            }
        },
        {
            "85": {
                "title": "Detecting community sensitive norm violations in online\nconversations.",
                "author": "Chan Young Park, Julia Mendelsohn, Karthik Radhakrishnan, Kinjal Jain, Tushar\nKanakagiri, David Jurgens, and Yulia Tsvetkov. 2021.",
                "venue": "In Findings of the Association for Computational Linguistics:\nEMNLP 2021, pages 3386\u20133397.",
                "url": null
            }
        },
        {
            "86": {
                "title": "Toward\nmachine-assisted participation in erulemaking: An argumentation model of\nevaluability.",
                "author": "Joonsuk Park, Cheryl Blake, and Claire Cardie. 2015.",
                "venue": "In Proceedings of the 15th International Conference on\nArtificial Intelligence and Law, ICAIL \u201915, pages 206\u2013210, New York, NY,\nUSA. ACM.",
                "url": "https://doi.org/10.1145/2746090.2746118"
            }
        },
        {
            "87": {
                "title": "A corpus of eRulemaking\nuser comments for measuring evaluability of arguments.",
                "author": "Joonsuk Park and Claire Cardie. 2018.",
                "venue": "In Proceedings of the Eleventh International Conference on\nLanguage Resources and Evaluation (LREC 2018), Miyazaki, Japan. European\nLanguage Resources Association (ELRA).",
                "url": "https://aclanthology.org/L18-1257"
            }
        },
        {
            "88": {
                "title": "Facilitative\nmoderation for online participation in eRulemaking.",
                "author": "Joonsuk Park, Sally Klingel, Claire Cardie, Mary Newhart, Cynthia Farina, and\nJoan-Josep Vallb\u00e9. 2012.",
                "venue": "In Proceedings of the 13th Annual International Conference on\nDigital Government Research. ACM.",
                "url": "https://doi.org/10.1145/2307729.2307757"
            }
        },
        {
            "89": {
                "title": "Modelling\npersuasion through misuse of rhetorical appeals.",
                "author": "Amalie Pauli, Leon Derczynski, and Ira Assent. 2022.",
                "venue": "In Proceedings of the Second Workshop on NLP for Positive\nImpact (NLP4PI), pages 89\u2013100, Abu Dhabi, United Arab Emirates (Hybrid).\nAssociation for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2022.nlp4pi-1.11"
            }
        },
        {
            "90": {
                "title": "Instruction tuning with\nGPT-4.",
                "author": "Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2304.03277"
            }
        },
        {
            "91": {
                "title": "Lightly-supervised\nmodeling of argument persuasiveness.",
                "author": "Isaac Persing and Vincent Ng. 2017a.",
                "venue": "In Proceedings of the Eighth International Joint Conference on\nNatural Language Processing (Volume 1: Long Papers), pages 594\u2013604, Taipei,\nTaiwan. Asian Federation of Natural Language Processing.",
                "url": "https://aclanthology.org/I17-1060"
            }
        },
        {
            "92": {
                "title": "Why can\u2019t you\nconvince me? modeling weaknesses in unpersuasive arguments.",
                "author": "Isaac Persing and Vincent Ng. 2017b.",
                "venue": "In Proceedings of the Twenty-Sixth International Joint\nConference on Artificial Intelligence, IJCAI-17, pages 4082\u20134088.",
                "url": "https://doi.org/10.24963/ijcai.2017/570"
            }
        },
        {
            "93": {
                "title": "The\n\u201cproblem\u201d of human label variation: On ground truth in data, modeling\nand evaluation.",
                "author": "Barbara Plank. 2022.",
                "venue": "In Proceedings of the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 10671\u201310682, Abu Dhabi, United Arab\nEmirates. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2022.emnlp-main.731"
            }
        },
        {
            "94": {
                "title": "Length,\ninterchangeability, and external knowledge: Observations from predicting\nargument convincingness.",
                "author": "Peter Potash, Robin Bhattacharya, and Anna Rumshisky. 2017.",
                "venue": "In Proceedings of the Eighth International Joint Conference on\nNatural Language Processing (Volume 1: Long Papers), pages 342\u2013351, Taipei,\nTaiwan. Asian Federation of Natural Language Processing.",
                "url": "https://www.aclweb.org/anthology/I17-1035"
            }
        },
        {
            "95": {
                "title": "Argument search:\nAssessing argument relevance.",
                "author": "Martin Potthast, Lukas Gienapp, Florian Euchner, Nick Heilenk\u00f6tter, Nico\nWeidmann, Henning Wachsmuth, Benno Stein, and Matthias Hagen. 2019.",
                "venue": "In 42nd International ACM Conference on Research and\nDevelopment in Information Retrieval (SIGIR 2019), pages 1117\u20131120. ACM.",
                "url": "https://doi.org/10.1145/3331184.3331327"
            }
        },
        {
            "96": {
                "title": "Learning how\nto ask: Querying LMs with mixtures of soft prompts.",
                "author": "Guanghui Qin and Jason Eisner. 2021.",
                "venue": "In Proceedings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language\nTechnologies, pages 5203\u20135212, Online. Association for Computational\nLinguistics.",
                "url": "https://doi.org/10.18653/v1/2021.naacl-main.410"
            }
        },
        {
            "97": {
                "title": "Evaluation of\nargument search approaches in the context of argumentative dialogue systems.",
                "author": "Niklas Rach, Yuki Matsuda, Johannes Daxenberger, Stefan Ultes, Keiichi\nYasumoto, and Wolfgang Minker. 2020.",
                "venue": "In Proceedings of the Twelfth Language Resources and Evaluation\nConference, pages 513\u2013522, Marseille, France. European Language Resources\nAssociation.",
                "url": "https://aclanthology.org/2020.lrec-1.65"
            }
        },
        {
            "98": {
                "title": "Is your\nperspective also my perspective? enriching prediction with subjectivity.",
                "author": "Julia Romberg. 2022.",
                "venue": "In Proceedings of the 9th Workshop on Argument Mining, pages\n115\u2013125, Online and in Gyeongju, Republic of Korea. International Conference\non Computational Linguistics.",
                "url": "https://aclanthology.org/2022.argmining-1.11"
            }
        },
        {
            "99": {
                "title": "Graph-based\nargument quality assessment.",
                "author": "Ekaterina Saveleva, Volha Petukhova, Marius Mosbach, and Dietrich Klakow. 2021.",
                "venue": "In Proceedings of the International Conference on Recent\nAdvances in Natural Language Processing (RANLP 2021), pages 1268\u20131280, Held\nOnline. INCOMA Ltd.",
                "url": "https://aclanthology.org/2021.ranlp-1.143"
            }
        },
        {
            "100": {
                "title": "Annotation and\nmulti-modal methods for quality assessment of multi-party discussion.",
                "author": "Tsukasa Shiota and Kazutaka Shimada. 2022.",
                "venue": "In Proceedings of the 36th Pacific Asia Conference on Language,\nInformation and Computation, pages 175\u2013182, Manila, Philippines.\nAssociation for Computational Linguistics.",
                "url": "https://aclanthology.org/2022.paclic-1.20"
            }
        },
        {
            "101": {
                "title": "Reason against the machine? Future directions for mass online\ndeliberation.",
                "author": "Ruth Shortall, Anatol Itten, Michiel van der Meer, Pradeep Murukannaiah, and\nCatholijn Jonker. 2022.",
                "venue": "Frontiers in Political Science, 4:946589.",
                "url": null
            }
        },
        {
            "102": {
                "title": "Finding convincing\narguments using scalable Bayesian preference learning.",
                "author": "Edwin Simpson and Iryna Gurevych. 2018.",
                "venue": "Transactions of the Association for Computational Linguistics,\n6:357\u2013371.",
                "url": "https://doi.org/10.1162/tacl_a_00026"
            }
        },
        {
            "103": {
                "title": "Learning from\nrevisions: Quality assessment of claims in argumentation at scale.",
                "author": "Gabriella Skitalinskaya, Jonas Klaff, and Henning Wachsmuth. 2021.",
                "venue": "In Proceedings of the 16th Conference of the European Chapter\nof the Association for Computational Linguistics: Main Volume, pages\n1718\u20131729, Online. Association for Computational Linguistics.",
                "url": "https://aclanthology.org/2021.eacl-main.147"
            }
        },
        {
            "104": {
                "title": "Claim\noptimization in computational argumentation.",
                "author": "Gabriella Skitalinskaya, Maximilian Splieth\u00c3\u00b6ver, and Henning Wachsmuth.\n2023.",
                "venue": "In Proceedings of the 16th International Natural Language\nGeneration Conference, pages 134\u2013152, Prague, Czechia. Association for\nComputational Linguistics.",
                "url": "https://aclanthology.org/2023.inlg-main.10"
            }
        },
        {
            "105": {
                "title": "To revise or not\nto revise: Learning to detect improvable claims for argumentative writing\nsupport.",
                "author": "Gabriella Skitalinskaya and Henning Wachsmuth. 2023.",
                "venue": "In Proceedings of the 61st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pages 15799\u201315816,\nToronto, Canada. Association for Computational Linguistics.",
                "url": "https://aclanthology.org/2023.acl-long.880"
            }
        },
        {
            "106": {
                "title": "An autonomous debating system.",
                "author": "Noam Slonim, Yonatan Bilu, Carlos Alzate, Roy Bar-Haim, Ben Bogin, Francesca\nBonin, Leshem Choshen, Edo Cohen-Karlik, Lena Dankin, Lilach Edelstein,\net al. 2021.",
                "venue": "Nature, 591(7850):379\u2013384.",
                "url": null
            }
        },
        {
            "107": {
                "title": "Hierarchical multi-task learning for organization evaluation of\nargumentative student essays.",
                "author": "Wei Song, Ziyao Song, Lizhen Liu, and Ruiji Fu. 2020.",
                "venue": "In Proceedings of the Twenty-Ninth International Joint\nConference on Artificial Intelligence, IJCAI 2020, pages 3875\u20133881.\nijcai.org.",
                "url": null
            }
        },
        {
            "108": {
                "title": "Argument from\nold man\u2019s view: Assessing social bias in argumentation.",
                "author": "Maximilian Splieth\u00f6ver and Henning Wachsmuth. 2020.",
                "venue": "In Proceedings of the 7th Workshop on Argument Mining, pages\n76\u201387, Online. Association for Computational Linguistics.",
                "url": "https://aclanthology.org/2020.argmining-1.9"
            }
        },
        {
            "109": {
                "title": "ArgumenText: Searching for arguments in heterogeneous sources.",
                "author": "Christian Stab, Johannes Daxenberger, Chris Stahlhut, Tristan Miller, Benjamin\nSchiller, Christopher Tauchmann, Steffen Eger, and Iryna Gurevych. 2018.",
                "venue": "In Proceedings of the 2018 NAACL: Demonstrations, pages\n21\u201325.",
                "url": null
            }
        },
        {
            "110": {
                "title": "Argumentation Mining.",
                "author": "Manfred Stede and Jodi Schneider. 2018.",
                "venue": "Number 40 in Synthesis Lectures on Human Language Technologies.\nMorgan & Claypool.",
                "url": null
            }
        },
        {
            "111": {
                "title": "News\neditorials: Towards summarizing long argumentative texts.",
                "author": "Shahbaz Syed, Roxanne El Baff, Johannes Kiesel, Khalid Al Khatib, Benno Stein,\nand Martin Potthast. 2020.",
                "venue": "In Proceedings of the 28th International Conference on\nComputational Linguistics, pages 5384\u20135396, Barcelona, Spain (Online).\nInternational Committee on Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2020.coling-main.470"
            }
        },
        {
            "112": {
                "title": "Frame-oriented\nsummarization of argumentative discussions.",
                "author": "Shahbaz Syed, Timon Ziegenbein, Philipp Heinisch, Henning Wachsmuth, and Martin\nPotthast. 2023.",
                "venue": "In Proceedings of the 24th Meeting of the Special Interest\nGroup on Discourse and Dialogue, pages 114\u2013129, Prague, Czechia.\nAssociation for Computational Linguistics.",
                "url": "https://aclanthology.org/2023.sigdial-1.10"
            }
        },
        {
            "113": {
                "title": "Stanford Alpaca: An instruction-following LLaMA model.",
                "author": "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos\nGuestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023.",
                "venue": "https://github.com/tatsu-lab/stanford_alpaca.",
                "url": null
            }
        },
        {
            "114": {
                "title": "Spurious\ncorrelations in cross-topic argument mining.",
                "author": "Terne Sasha Thorn Jakobsen, Maria Barrett, and Anders S\u00f8gaard. 2021.",
                "venue": "In Proceedings of *SEM 2021: The Tenth Joint Conference on\nLexical and Computational Semantics, pages 263\u2013277, Online. Association for\nComputational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.starsem-1.25"
            }
        },
        {
            "115": {
                "title": "Automatic argument\nquality assessment - New datasets and methods.",
                "author": "Assaf Toledo, Shai Gretz, Edo Cohen-Karlik, Roni Friedman, Elad Venezian, Dan\nLahav, Michal Jacovi, Ranit Aharonov, and Noam Slonim. 2019.",
                "venue": "In Proceedings of the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP), pages 5625\u20135635. Association\nfor Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/D19-1564"
            }
        },
        {
            "116": {
                "title": "Will it blend?\nmixing training paradigms & prompting for argument quality prediction.",
                "author": "Michiel van der Meer, Myrthe Reuver, Urja Khurana, Lea Krause, and Selene\nBaez Santamaria. 2022.",
                "venue": "In Proceedings of the 9th Workshop on Argument Mining, pages\n95\u2013103, Online and in Gyeongju, Republic of Korea. International Conference\non Computational Linguistics.",
                "url": "https://aclanthology.org/2022.argmining-1.8"
            }
        },
        {
            "117": {
                "title": "A Systematic Theory of Argumentation: The Pragma-Dialectical\nApproach.",
                "author": "Frans H. van Eemeren and Rob Grootendorst. 2004.",
                "venue": "Cambridge University Press, Cambridge, UK.",
                "url": null
            }
        },
        {
            "118": {
                "title": "Attention is all you need.",
                "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017.",
                "venue": "Advances in neural information processing systems, 30.",
                "url": null
            }
        },
        {
            "119": {
                "title": "Towards\nargument mining for social good: A survey.",
                "author": "Eva Maria Vecchi, Neele Falk, Iman Jundi, and Gabriella Lapesa. 2021.",
                "venue": "In Proceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International Joint Conference on\nNatural Language Processing (Volume 1: Long Papers), pages 1338\u20131352,\nOnline. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.acl-long.107"
            }
        },
        {
            "120": {
                "title": "Argumentation quality\nassessment: Theory vs. practice.",
                "author": "Henning Wachsmuth, Nona Naderi, Ivan Habernal, Yufang Hou, Graeme Hirst, Iryna\nGurevych, and Benno Stein. 2017a.",
                "venue": "In Proceedings of the 55th Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Papers), pages 250\u2013255.\nAssociation for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/P17-2039"
            }
        },
        {
            "121": {
                "title": "Computational\nargumentation quality assessment in natural language.",
                "author": "Henning Wachsmuth, Nona Naderi, Yufang Hou, Yonatan Bilu, Vinodkumar\nPrabhakaran, Alberdingk Tim Thijm, Graeme Hirst, and Benno Stein.\n2017b.",
                "venue": "In Proceedings of the 15th Conference of the European Chapter\nof the Association for Computational Linguistics: Volume 1, Long Papers,\npages 176\u2013187. Association for Computational Linguistics.",
                "url": "http://aclweb.org/anthology/E17-1017"
            }
        },
        {
            "122": {
                "title": "Building an argument\nsearch engine for the web.",
                "author": "Henning Wachsmuth, Martin Potthast, Khalid Al-Khatib, Yamen Ajjour, Jana\nPuschmann, Jiani Qu, Jonas Dorsch, Viorel Morari, Janek Bevendorff, and Benno\nStein. 2017c.",
                "venue": "In Proceedings of the 4th Workshop on Argument Mining, pages\n49\u201359. Association for Computational Linguistics.",
                "url": "http://aclweb.org/anthology/W17-5106"
            }
        },
        {
            "123": {
                "title": "Intrinsic\nquality assessment of arguments.",
                "author": "Henning Wachsmuth and Till Werner. 2020.",
                "venue": "In Proceedings of the 28th International Conference on\nComputational Linguistics, pages 6739\u20136745, Barcelona, Spain (Online).\nInternational Committee on Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2020.coling-main.592"
            }
        },
        {
            "124": {
                "title": "Modeling\npersuasive discourse to adaptively support students\u2019 argumentative\nwriting.",
                "author": "Thiemo Wambsganss and Christina Niklaus. 2022.",
                "venue": "In Proceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pages 8748\u20138760,\nDublin, Ireland. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2022.acl-long.599"
            }
        },
        {
            "125": {
                "title": "Winning on the merits:\nThe joint effects of content and style on debate outcomes.",
                "author": "Lu Wang, Nick Beauchamp, Sarah Shugars, and Kechen Qin. 2017.",
                "venue": "Transactions of the Association for Computational Linguistics,\n5:219\u2013232.",
                "url": "https://doi.org/10.1162/tacl_a_00057"
            }
        },
        {
            "126": {
                "title": "Argument and counter-argument generation: A critical survey.",
                "author": "Xiaoou Wang, Elena Cabrio, and Serena Villata. 2023a.",
                "venue": "In Natural Language Processing and Information Systems, pages\n500\u2013510, Cham. Springer Nature Switzerland.",
                "url": null
            }
        },
        {
            "127": {
                "title": "Self-instruct: Aligning\nlanguage models with self-generated instructions.",
                "author": "Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel\nKhashabi, and Hannaneh Hajishirzi. 2023b.",
                "venue": null,
                "url": "http://arxiv.org/abs/2212.10560"
            }
        },
        {
            "128": {
                "title": "Analyzing\npersuasion strategies of debaters on social media.",
                "author": "Matti Wiegmann, Khalid Al Khatib, Vishal Khanna, and Benno Stein. 2022.",
                "venue": "In Proceedings of the 29th International Conference on\nComputational Linguistics, pages 6897\u20136905, Gyeongju, Republic of Korea.\nInternational Committee on Computational Linguistics.",
                "url": "https://aclanthology.org/2022.coling-1.600"
            }
        },
        {
            "129": {
                "title": "Nonsense!: Quality\ncontrol via two-step reason selection for annotating local acceptability and\nrelated attributes in news editorials.",
                "author": "Wonsuk Yang, Seungwon Yoon, Ada Carpenter, and Jong Park. 2019.",
                "venue": "In Proceedings of the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP), pages 2954\u20132963. Association\nfor Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/D19-1293"
            }
        },
        {
            "130": {
                "title": "Leveraging\ntopic relatedness for argument persuasion.",
                "author": "Xinran Zhao, Esin Durmus, Hongming Zhang, and Claire Cardie. 2021.",
                "venue": "In Findings of the Association for Computational Linguistics:\nACL-IJCNLP 2021, pages 4401\u20134407, Online. Association for Computational\nLinguistics.",
                "url": "https://doi.org/10.18653/v1/2021.findings-acl.386"
            }
        },
        {
            "131": {
                "title": "Modeling\nappropriate language in argumentation.",
                "author": "Timon Ziegenbein, Shahbaz Syed, Felix Lange, Martin Potthast, and Henning\nWachsmuth. 2023.",
                "venue": "In Proceedings of the 61st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pages 4344\u20134363,\nToronto, Canada. Association for Computational Linguistics.",
                "url": "https://aclanthology.org/2023.acl-long.238"
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.16084v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2",
            "2.3",
            "2.4"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.4"
        ],
        "main_experiment_and_results_sections": [
            "2.5",
            "2.6",
            "3.5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "2.5"
        ]
    },
    "research_context": {
        "paper_id": "2403.16084v1",
        "paper_title": "Argument Quality Assessment in the Age of Instruction-Following Large Language Models",
        "research_background": "**Motivation:**\nThe motivation behind this paper is driven by the fundamental importance of assessing argument quality in the context of computational argumentation. As individuals encounter arguments, especially about controversial issues, they need to evaluate and determine the best arguments that can help them form opinions, persuade others, or foster better mutual understanding. With the rise of computational tools in argumentation, notably in fields like search, business, and education, there is a critical need to not only generate but also evaluate the quality of these arguments effectively.\n\n**Research Problem:**\nThe core research problem addressed in this paper is how to advance the assessment of argument quality in light of recent developments in natural language processing, especially the emergence of instruction-following large language models (LLMs) like GPT-4 and Alpaca. Given the challenges of diverse quality notions and the inherent subjectivity in evaluating arguments, the paper seeks to understand the implications of LLMs for argument quality assessment and explore how LLM-based strategies can overcome these challenges.\n\n**Relevant Prior Work:**\n1. **Wachsmuth et al. (2017b)** performed a comprehensive survey on argument quality assessment, highlighting the diversity and subjectivity involved in this task and organizing theories and methods under 15 quality notions ranging from logical cogency to rhetorical effectiveness.\n   \n2. **Park and Cardie (2018), Lauscher et al. (2020), Goffredo et al. (2022), Skitalinskaya et al. (2023)** emphasized the necessity of evaluating the quality of mined or generated arguments for practical applications.\n\n3. **Lapesa et al. (2023)** acknowledged that the challenges of diverse and subjective quality perception persist.\n\n4. **OpenAI (2023)** described the advent of instruction-following LLMs, which possess the capabilities to integrate and leverage knowledge across varied contexts without extensive task-specific fine-tuning.\n\n5. **Various recent efforts (Taori et al., 2023; Argyle et al., 2023; Vecchi et al., 2021)**, showcasing the developments and potential applications of LLMs in enhancing argumentative discussions and contributing towards more informed and deliberate conversations.\n\nThus, building on these prior works, the paper aims to re-evaluate and push forward the research on argument quality assessment with a focus on leveraging the advanced capabilities of LLMs. The discussion includes the establishment of structured approaches to teaching LLMs argument quality concepts and exploring real-world applications while remaining mindful of ethical considerations.",
        "methodology": "### Methodology: Argument Quality Assessment with Large Language Models\n\nSection 2 in ###reference_### highlights a significant part of the ongoing research into argument quality which addresses the core challenges of varying perceptions of quality and inherent subjectivity. These challenges have traditionally been met by developing or refining concepts of quality and controlling or modeling influencing factors. However, the nuanced interdependencies between different quality notions and the factors that influence them have hindered major advancements in the reliable assessment of argument quality.\n\nOur proposition is that instruction-following large language models (LLMs) could potentially circumvent many of these limitations, provided that systematic methods to educate them are established. This section discusses the main benefits of leveraging such LLMs. Following that, we delineate what content to instruct LLMs with and the methodology for this instruction, extending beyond mere in-context learning or basic prompting techniques, with the aim of advancing the capabilities of LLMs in assessing argument quality in future research.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\n#### Experiment Setup\n\n**Datasets:**\nThe paper reviews empirical studies that generally focus on argument quality assessment, utilizing various datasets. For instance, the referenced works employ:\n- Opinion change datasets (Jo et al., 2018) for predicting shifts in opinion holder views.\n- Siamese neural network datasets for assessing evidence convincingness (Gleize et al., 2019).\n- Hierarchical multitask datasets for scoring essays (Song et al., 2020).\n- Logical sufficiency datasets with transformer models for inferring conclusions from premises (Gurcke et al., 2021).\n- Bayesian network datasets for argument reasoning validity (Kondo et al., 2021).\n- Complex revision-based data for identifying arguments needing improvement (Skitalinskaya and Wachsmuth, 2023; Skitalinskaya et al., 2023).\n\n**Baselines:**\nVaried baselines are utilized depending on the targeted quality notion:\n- Simple models for baseline assessments of logical sufficiency or argumentative effectiveness.\n- Previously established neural models for comparison with proposed approaches, such as those aiding in predicting persuasiveness in student essays (Ke et al., 2018).\n- Existing baseline methods for claim optimization, which are fine-tuned with transformer models (Skitalinskaya et al., 2023).\n\n**Evaluation Metrics:**\nEvaluation metrics primarily involve those pertinent to specific quality notions, such as:\n- Accuracy or F1 score for detecting opinion changes and argumentative convincingness.\n- Precision, recall, and overall correctness in discourse element identification and organization assessment.\n- Metrics specific to logical sufficiency and validity assessment, often involving inferencing accuracy.\n- Quality-based ranking metrics to evaluate claim optimization approaches.\n\n#### Main Experimental Results\n\n**Assessment and Improvement of Argument Quality:**\n- Jo et al. (2018) achieved improved predictive performance for opinion changes by focusing on the detection of vulnerable reasoning regions.\n- Gleize et al. (2019) demonstrated enhanced accuracy in evaluating evidence convincingness using a Siamese neural network approach.\n- Song et al. (2020) reported significant improvements in essay scoring through a hierarchical multitask learning model, which jointly assesses discourse elements and organization.\n- Gurcke et al. (2021) utilized transformers to effectively predict logical sufficiency, advancing the capabilities of argument assessment models.\n- Kondo et al. (2021) achieved accurate reasoning validity assessments by integrating Bayesian networks and predicate logic.\n\n**Knowledge Injection and Argument Optimization:**\n- Falk and Lapesa (2023) illustrate that incorporating interactions between different quality notions can enhance individual quality prediction.\n- Skitalinskaya and Wachsmuth (2023) effectively used complex revision-based data to identify arguments requiring improvement.\n- Skitalinskaya et al. (2023) broke new ground by presenting a method to optimize argumentative claims, combining neural claim rewriting with quality-based ranking, achieving substantial improvements in argumentative quality optimization.\n\nThese results highlight the effectiveness of various novel approaches and emphasize the growing sophistication in models assessing and improving the quality of arguments."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Examine the extent to which an argument's logical sufficiency can be predicted based on whether its conclusion can be inferred from its premises using the generation capabilities of transformers.",
            "experiment_process": "Gurcke et al. (2021) use transformers to inspect logical sufficiency. The approach analyzes the ability of transformer models to generate inferences from premises to conclusions, thus assessing the logical sufficiency of the arguments.",
            "result_discussion": "The models demonstrate a degree of effectiveness in predicting logical sufficiency. This indicates that transformer models can be valuable tools in assessing logical sufficiency by generating potential conclusions from given premises.",
            "ablation_id": "2403.16084v1.No1"
        },
        {
            "research_objective": "Assess the validity of an argument\u2019s reasoning using Bayesian networks and predicate logic facilitated by argumentation schemes.",
            "experiment_process": "Kondo et al. (2021) deploy Bayesian networks and predicate logic enhanced by argumentation schemes. The process involves mapping arguments to the networks and logic forms, then evaluating their reasoning validity through probabilistic inference and logical consistency checks.",
            "result_discussion": "The findings suggest that Bayesian networks combined with predicate logic offer a robust framework for evaluating reasoning validity, providing structured and quantifiable insights into the logical soundness of arguments.",
            "ablation_id": "2403.16084v1.No2"
        },
        {
            "research_objective": "Improve the prediction of individual quality notions by injecting knowledge about the interactions between different quality notions.",
            "experiment_process": "Falk and Lapesa (2023) propose a model that incorporates knowledge about the interactions between various quality notions. The method involves training the model with interaction-informed features, likely derived from annotated datasets or established theories in argumentation.",
            "result_discussion": "Initial results indicate that embedding interaction knowledge enhances the prediction accuracy for individual quality notions, validating the importance of multi-dimensional quality assessments in argument evaluation.",
            "ablation_id": "2403.16084v1.No3"
        },
        {
            "research_objective": "Identify arguments in need of improvement using complex revision-based data with transformer models.",
            "experiment_process": "Skitalinskaya and Wachsmuth (2023) employ transformer models to detect arguments requiring enhancement. The approach leverages a dataset annotated with revision needs, training the transformer models to recognize patterns and signals indicative of potential improvements.",
            "result_discussion": "The method effectively identifies arguments that would benefit from revisions, showcasing transformers' potential in supporting iterative improvement processes in argumentative writing.",
            "ablation_id": "2403.16084v1.No4"
        },
        {
            "research_objective": "Optimize argumentative claims by combining neural claim rewriting with quality-based ranking.",
            "experiment_process": "Skitalinskaya et al. (2023) present a dual approach where neural models rewrite argumentative claims and then rank them based on quality metrics. The experimental setup includes training the neural rewriters and evaluative models on large-scale argumentative datasets to refine claim articulation.",
            "result_discussion": "This study represents a significant step towards argument optimization, with the approach proving effective in enhancing the quality of argumentative claims as measured by the predefined rankings.",
            "ablation_id": "2403.16084v1.No5"
        }
    ]
}