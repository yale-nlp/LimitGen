{
    "title": "Unveiling Hallucination in Text, Image, Video, and Audio Foundation Models: A Comprehensive Survey",
    "abstract": "The rapid advancement of foundation models (FMs) across language, image, audio, and video domains has shown remarkable capabilities in diverse tasks. However, the proliferation of FMs brings forth a critical challenge: the potential to generate hallucinated outputs, particularly in high-stakes applications. The tendency of foundation models to produce hallucinated content arguably represents the biggest hindrance to their widespread adoption in real-world scenarios, especially in domains where reliability and accuracy are paramount. This survey paper presents a comprehensive overview of recent developments that aim to identify and mitigate the problem of hallucination in FMs, spanning text, image, video, and audio modalities. By synthesizing recent advancements in detecting and mitigating hallucination across various modalities, the paper aims to provide valuable insights for researchers, developers, and practitioners. Essentially, it establishes a clear framework encompassing definition, taxonomy, and detection strategies for addressing hallucination in multimodal foundation models, laying the foundation for future research in this pivotal area.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The rapid progress in large-scale foundation models (FMs), spanning language, image, audio, and video domains, has revolutionized the field of artificial intelligence (AI). Models such as GPT-3 Brown et al. (2020  ###reference_b5###), MiniGPT-4 Zhu et al. (2023  ###reference_b121###), AudioLLM Borsos et al. (2023  ###reference_b4###), and LaViLa Zhao et al. (2022  ###reference_b116###) have demonstrated remarkable abilities across diverse tasks, from text generation to multimodal understanding. As these models find wider applications in critical domains, there is a growing imperative to comprehend and alleviate their propensity to produce hallucinated outputs."
        },
        {
            "section_id": "1.1",
            "parent_section_id": "1",
            "section_name": "Hallucination",
            "text": "Hallucination refers to instances where FMs generate content that appears plausible and emulates human-like patterns but lacks a coherent understanding of the underlying context or factual grounding Xu et al. (2024b  ###reference_b102###). These hallucinated outputs can manifest in various forms, ranging from trivial factual inaccuracies to more severe cases where the model generates entirely imaginary or absurd content. This issue is not limited to textual modalities but extends across diverse domains, including images, videos, and audio generated by large-scale foundation models. The root causes of hallucination are multifaceted, potentially stemming from biases in training data, limited access to current information, or the model\u2019s inherent constraints in comprehending and generating contextually precise responses. Deploying these powerful models without addressing their hallucination tendencies can have severe consequences, perpetuating misinformation, leading to incorrect conclusions, and potentially causing adverse effects in critical applications. Addressing hallucinations in FMs has become an active research area and a key focus for development efforts, with ongoing efforts to mitigate this behavior. Strategies employed include fine-tuning models on domain-specific data, leveraging diverse training data to enhance model robustness, and developing improved evaluation metrics to identify and reduce hallucination tendencies."
        },
        {
            "section_id": "1.2",
            "parent_section_id": "1",
            "section_name": "Types of Hallucination",
            "text": "Hallucinations in large foundation models can manifest in various forms, each posing unique challenges and requiring tailored strategies for mitigation. These types of hallucinations can be categorized as follows:\nContextual disconnection Zhang et al. (2023d  ###reference_b114###) describes a situation in which the output or content produced by a model across different modalities is inconsistent or out of sync with the context that the user or the input data have provided or expected. Semantic distortion  Tjio et al. (2022  ###reference_b90###) refers to a type of inconsistency or error in generated content where the semantics or underlying meaning of the input is misrepresented or altered in the output. Content hallucination Moernaut et al. (2018  ###reference_b69###)is the term used to describe a phenomenon seen in generative models when features or elements that are generated as output are either unreal given the context or absent from the input data. Factual inaccuracy  Zhang et al. (2023d  ###reference_b114###) describes a kind of error seen in generative models when information that is inaccurate, deceptive, or at odds with the known facts appears in the generated output. Figure 1  ###reference_### illustrates various types of hallucinations.\n###figure_1###"
        },
        {
            "section_id": "1.3",
            "parent_section_id": "1",
            "section_name": "Motivation and Contributions",
            "text": "Most of the existing survey papers have explored hallucination in the context of large language models (LLMs) Huang et al. (2023  ###reference_b38###), Tonmoy et al. (2024  ###reference_b92###). Recent studies have shown that hallucination can also occur in vision, audio, and video foundation models, highlighting the need for a comprehensive understanding of this challenge across multiple modalities Liu et al. (2024a  ###reference_b60###), Sahoo et al. (2024  ###reference_b84###), Rawte et al. (2023b  ###reference_b79###). To address this gap, our survey aims to provide a holistic and multimodal perspective on the hallucination challenge in FMs. This review comprehensively examines existing research across language, vision, video, and audio domains to understand the mechanisms, detection methods, and mitigation strategies for hallucination in FMs. It serves as a vital resource for researchers and practitioners, aiding in the development of more robust AI solutions. Additionally, it includes a detailed taxonomy diagram in Figure. 2  ###reference_### and a summarized Table 1  ###reference_### illustrating recent advancements across different modalities. The contributions of this survey paper are as follows:\nEstablish a precise definition and structured taxonomy of hallucination in the context of large-scale foundation models.\nIdentify the key factors and mechanisms that contribute to the emergence of hallucination across different modalities.\nWe have presented the various detection and mitigation strategies that have been proposed to address the hallucination problem in a multimodal setting.\nWe have provided a comprehensive summary of the methodologies pertaining to hallucination techniques in large foundational models in Table 1  ###reference_###, detailing their approaches to hallucination detection, mitigation, task considerations, datasets utilized, and evaluation metrics employed. This will offer readers a concise overview of recent advancements in this field.\nThe paper is structured as follows: Section 2  ###reference_### delves into hallucinations in LLMs, while Section 3  ###reference_### explores large vision language models. Additionally, Section 4  ###reference_### and Section 5  ###reference_### examine hallucinations in large video and large audio models, respectively. Section 6  ###reference_### presents a discourse on the ramifications of hallucinations, exploring whether they pose advantages or disadvantages. Subsequently, Section 7  ###reference_### addresses the limitations of the current work, while Section 8  ###reference_### outlines potential future directions. Finally, Section 9  ###reference_### presents the concluding remarks.\nforked edges,\nfor tree=\ngrow=east,\nreversed=true,\nanchor=base west,\nparent anchor=east,\nchild anchor=west,\nbase=center,\nfont=,\nrectangle,\ndraw=hidden-draw,\nrounded corners,\nalign=center,\ntext centered,\nminimum width=5em,\nedge+=darkgray, line width=1pt,\ns sep=3pt,\ninner xsep=2pt,\ninner ysep=3pt,\nline width=0.8pt,\nver/.style=\nrotate=90,\nchild anchor=north,\nparent anchor=south,\nanchor=center\n,\n,\nwhere level=1text width=15em,font=,,\nwhere level=2text width=14em,font=,,\nwhere level=3minimum width=10em,font=,,\nwhere level=4text width=26em,font=,,\nwhere level=5text width=20em,font=,,\n[\nHallucinations, for tree=fill=bg16, text width=14em\n[\nText \u00a72  ###reference_###, for tree=fill=red!50, text width=18em\n[\nDetection, for tree=fill=bg1, text width=13.5em\n[\nFACTOID Rawte et al. (2024b  ###reference_b80###), for tree=fill=pg56,text width=28em\n]\n[\nFACTOR Muhlgay et al. (2023  ###reference_b70###), for tree=fill=pg56,text width=28em\n]\n[\nFactCHD Chen et al. (2023b  ###reference_b9###), for tree=fill=pg56,text width=28em\n]\n[\nHalluQA Cheng et al. (2023  ###reference_b10###), for tree=fill=pg56,text width=28em\n]\n[\nHaluEval Li et al. (2023b  ###reference_b52###), for tree=fill=pg56,text width=28em\n]\n[\nSelfCheckGPT Manakul et al. (2023  ###reference_b66###), for tree=fill=pg56,text width=28em\n]\n[\nFACTSCORE Min et al. (2023  ###reference_b68###), for tree=fill=pg56,text width=28em\n]\n[\nFacTool Chern et al. (2023  ###reference_b11###), for tree=fill=pg56,text width=28em\n]\n]\n[\nMitigation, for tree=fill=bg2, text width=13.5em\n[\nDoLa Chuang et al. (2023  ###reference_b13###), for tree=fill=bg4,text width=28em\n]\n[\nCoK Li et al. (2023d  ###reference_b54###), for tree=fill=bg4,text width=28em\n]\n[\nMixAlign Knowledge  ###reference_b47###, for tree=fill=bg4,text width=28em\n]\n[\nSELF-FAMILIARITY Luo et al. (2023  ###reference_b65###), for tree=fill=bg4,text width=28em\n]\n[\nCoVe Dhuliawala et al. (2023  ###reference_b20###), for tree=fill=bg4,text width=28em\n]\n[\nHALOCHECK Elaraby et al. (2023  ###reference_b22###), for tree=fill=bg4,text width=28em\n]\n[\nInstructional Prompting Varshney et al. (2023  ###reference_b94###), for tree=fill=bg4,text width=28em\n]\n[\nPURR Chen et al. (2023a  ###reference_b7###), for tree=fill=bg4,text width=28em\n]\n[\nLLM-AUGMENTER Peng et al. (2023  ###reference_b76###), for tree=fill=bg4,text width=28em\n]\n]\n]\n[\nImage \u00a73  ###reference_###, for tree=fill=orange!50, text width=18em\n[\nDetection, for tree=fill=bg1, text width=13.5em\n[\nHallusionBench Guan et al. (2023  ###reference_b29###), for tree=fill=dandyshade,text width=28em\n]\n[\nGAVIE Liu et al. (2023  ###reference_b59###), for tree=fill=dandyshade,text width=28em\n]\n[\nPhD Liu et al. (2024b  ###reference_b62###), for tree=fill=dandyshade,text width=28em\n]\n[\nVHTest Huang et al. (2024  ###reference_b39###), for tree=fill=dandyshade,text width=28em\n]\n[\nM-HalDetect Gunjal et al. (2024  ###reference_b31###), for tree=fill=dandyshade,text width=28em\n]\n[\nChartBench Xu et al. (2023b  ###reference_b101###), for tree=fill=dandyshade,text width=28em\n]\n[\n MSG-MCQ Lu et al. (2024  ###reference_b64###), for tree=fill=dandyshade,text width=28em\n]\n[\nMMVP Tong et al. (2024  ###reference_b91###), for tree=fill=dandyshade,text width=28em\n]\n[\nREVO-LION Liao et al. (2023  ###reference_b58###), for tree=fill=dandyshade,text width=28em\n]\n[\nCIEM Hu et al. (2023  ###reference_b36###), for tree=fill=dandyshade,text width=28em\n]\n[\nFAITHSCORE Jing et al. (2023  ###reference_b44###), for tree=fill=dandyshade,text width=28em\n]\n[\nPOPE Li et al. (2023f  ###reference_b57###), for tree=fill=dandyshade,text width=28em\n]\n[\nVQA Changpinyo et al. (2022  ###reference_b6###), for tree=fill=dandyshade,text width=28em\n]\n[\nHaELM Wang et al. (2023  ###reference_b96###), for tree=fill=dandyshade,text width=28em\n]\n[\nNOPE Lovenia et al. (2023  ###reference_b63###), for tree=fill=dandyshade,text width=28em\n]\n[\nMMHAL-BENCH Sun et al. (2023  ###reference_b89###), for tree=fill=dandyshade,text width=28em\n]\n[\nTouchStone Bai et al. (2023  ###reference_b3###), for tree=fill=dandyshade,text width=28em\n]\n]\n[\nMitigation, for tree=fill=bg2, text width=13.5em\n[\nHalluciDoctor Yu et al. (2023a  ###reference_b107###), for tree=fill=bg8,text width=28em\n]\n[\nLURE Zhou et al. (2023  ###reference_b120###), for tree=fill=bg8,text width=28em\n]\n[\n MARINE Zhao et al. (2024  ###reference_b115###), for tree=fill=bg8,text width=28em\n]\n[\nFDPO Gunjal et al. (2024  ###reference_b31###), for tree=fill=bg8,text width=28em\n]\n[\nViGoR Yan et al. (2024  ###reference_b103###), for tree=fill=bg8,text width=28em\n]\n[\nHA-DPO Zhao et al. (2023  ###reference_b117###), for tree=fill=bg8,text width=28em\n]\n[\nVIGC Wang et al. (2024  ###reference_b95###), for tree=fill=bg8,text width=28em\n]\n[\n Data Centric Approach Lu et al. (2024  ###reference_b64###), for tree=fill=bg8,text width=28em\n]\n[\nMoF Tong et al. (2024  ###reference_b91###), for tree=fill=bg8,text width=28em\n]\n[\nInternLM-XComposer Zhang et al. (2023b  ###reference_b112###), for tree=fill=bg8,text width=28em\n]\n[\n VCD Leng et al. (2023  ###reference_b50###), for tree=fill=bg8,text width=28em\n]\n[\nFactually Augmented RLHF Sun et al. (2023  ###reference_b89###), for tree=fill=bg8,text width=28em\n]\n[\nObjMLM Dai et al. (2022  ###reference_b17###), for tree=fill=bg8,text width=28em\n]\n]\n]\n[\nVideo \u00a74  ###reference_### , for tree=fill=bg5, text width=18em\n[\nDetection, for tree=fill=bg1, text width=13.5em\n[\nEMScore Shi et al. (2022  ###reference_b87###), for tree=fill=bg7,text width=28em\n]\n]\n[\nMitigation, for tree=fill=bg2, text width=13.5em\n[\nFactVC Liu and Wan (2023  ###reference_b61###), for tree=fill=bg13,text width=28em\n]\n[\nCLearViD Chuang and Fazli (2023  ###reference_b12###), for tree=fill=bg13,text width=28em\n]\n[\nMGAT He et al. (2022  ###reference_b33###), for tree=fill=bg13,text width=28em\n]\n]\n]\n[\nAudio \u00a75  ###reference_### , for tree=fill=lime!50, text width=18em\n[\nDetection, for tree=fill=bg1, text width=13.5em\n[\nPAM Deshmukh et al. (2024  ###reference_b19###), for tree=fill=bg15,text width=28em\n]\n[\nCompA Ghosh et al. (2023  ###reference_b28###), for tree=fill=bg15,text width=28em\n]\n]\n[\nMitigation, for tree=fill=bg2, text width=13.5em\n[\nMusicLDM Chen et al. (2024  ###reference_b8###), for tree=fill=bg14,text width=28em\n]\n[\nRe-AudioLDM Yuan et al. (2024  ###reference_b109###), for tree=fill=bg14,text width=28em\n]\n[\nSECap Xu et al. (2024a  ###reference_b100###), for tree=fill=bg14,text width=28em\n]\n[\nRECAP Ghosh et al. (2024  ###reference_b27###), for tree=fill=bg14,text width=28em\n]\n[\nCacophony Zhu and Duan (2024  ###reference_b122###), for tree=fill=bg14,text width=28em\n]\n[\nEnCLAP Kim et al. (2024  ###reference_b46###), for tree=fill=bg14,text width=28em\n]\n]\n]\n]\n###figure_2###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Hallucination in Large Language Models",
            "text": "Despite the progress of LLMs, a notable challenge persists in their proneness to hallucinate, impeding their practical implementation. For instance, the illustration in Figure 3  ###reference_### exemplifies the generated response by the LLM, showcasing indications of hallucination."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Hallucination Detection and Mitigation",
            "text": "Identifying hallucinations in LLMs is crucial for ensuring the credibility and reliability of their results, especially in scenarios requiring factual correctness. Existing fact-checking methods often rely on complex modules or external databases, requiring either output probability distributions or interfacing with external sources. SelfCheckGPT Manakul et al. (2023  ###reference_b66###) offers a zero-resource black-box solution for detecting hallucinations in any LLM without relying on external resources. This method operates on the principle that an LLM familiar with a topic will produce consistent and comparable facts in its responses. In contrast, randomly sampled responses from an unfamiliar topic are likely to contain contradicting and hallucinated facts.\nContinuing the exploration of methods for passage-level hallucination detection, Yang et al. (2023  ###reference_b104###) proposed a novel self-check approach based on reverse validation, aiming to identify factual errors without external resources automatically. They introduced a benchmark, Passage-level Hallucination Detection (PHD), generated using ChatGPT and annotated by human experts to assess different methods.\nAssessing the accuracy of long text generated by LLMs is challenging because it often contains both accurate and inaccurate information, making simple quality judgments insufficient. To address this, Min et al. (2023  ###reference_b68###) introduced FACTSCORE (Factual Precision in Atomicity Score), a novel evaluation method that breaks down text into individual facts and measures their reliability.\nHuang and Chang (2023  ###reference_b37###) introduced a unique strategy to mitigate hallucination risks in LLMs by drawing parallels with established web systems. They identified the absence of a \"citation\" mechanism, which refers to acknowledging or referencing sources or evidence, as a significant gap.\nAddressing the need to identify factual inaccuracies in LLM-generated content, Rawte et al. (2024b  ###reference_b80###) develop a multi-task learning (MTL) framework, integrating advanced long text embeddings like e5-mistral-7b-instruct, along with models such as GPT-3, SpanBERT, and RoFormer. This MTL approach exhibited a substantial performance gain, achieving an average accuracy improvement of 40% on the FACTOID benchmark when compared to leading textual entailment methods.\nHallucination mitigation efforts have predominantly relied on empirical methods, leaving uncertainty regarding the possibility of complete elimination. To tackle this challenge, Xu et al. (2024b  ###reference_b102###) introduces a formal framework defining hallucination as discrepancies observed between computable LLMs and a ground truth function. Through this framework, the study examines existing hallucination mitigation strategies and their practical implications for real-world LLM deployment. Rawte et al. (2024c  ###reference_b81###) introduced the Sorry, Come Again (SCA) prompting technique to address hallucination in contemporary LLMs. SCA enhances comprehension by employing optimal paraphrasing and injecting [PAUSE] tokens to delay LLM generation. It analyzes linguistic nuances in prompts and their impact on the hallucinated generation, highlighting the difficulties posed by prompts characterized by lower readability, formality, or concreteness. Rawte et al. (2023a  ###reference_b77###) investigate how LLMs respond to factually correct and incorrect prompts, categorizing their hallucinations into mild, moderate, and alarming subcategories. Additionally, the paper introduced the Hallucination eLiciTation dataset, comprising 75,000 text snippets annotated by humans, and introduced a novel Hallucination Vulnerability Index metric."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Domain Specific Works",
            "text": "Hallucinations pose severe risks in critical fields such as healthcare, finance, and law. Reliability and accuracy are crucial in these sectors, as any form of hallucination can lead to significant and adverse consequences."
        },
        {
            "section_id": "2.2.1",
            "parent_section_id": "2.2",
            "section_name": "2.2.1 Healthcare",
            "text": "In response to the hallucinations in the medical domain LLMs, Pal et al. (2023  ###reference_b75###) introduced the Medical Domain Hallucination Test (Med-HALT), a specialized benchmark dataset aimed at assessing and mitigating hallucinations. Med-HALT consists of a varied multinational dataset drawn from medical records spanning numerous countries, encompassing a total of seven datasets.\nAhmad et al. (2023  ###reference_b1###) outlined essential steps for creating dependable, trustworthy, and unbiased models, emphasizing the need for quantifying, validating, and mitigating hallucinations within the healthcare context. Ji et al. (2023  ###reference_b42###) introduced an interactive self-reflection approach aimed at improving the accuracy and coherence of answers generated by medical question-answering systems using LLMs. Through knowledge acquisition and feedback on answer generation, this methodology enhances the factuality, consistency, and logical progression of responses."
        },
        {
            "section_id": "2.2.2",
            "parent_section_id": "2.2",
            "section_name": "2.2.2 Finance",
            "text": "An empirical investigation explored the inclination of LLMs to generate hallucinations while engaging in financial activities.\n Kang and Liu (2023  ###reference_b45###) conducted an empirical investigation into the hallucination tendencies of LLMs in financial tasks. Their study assessed LLMs\u2019 proficiency in explaining financial concepts, querying historical stock prices, and examined the efficacy of methods like few-shot learning and prompt-based tool learning in mitigating hallucination.\n Roychowdhury et al. (2023  ###reference_b83###) proposed a novel Langchain-based approach aimed at transforming data tables into hierarchical textual data chunks, facilitating versatile financial question answering. The framework involves classifying user queries by intention, retrieving relevant data chunks, generating customized LLM prompts, and evaluating responses for hallucinations and confidence."
        },
        {
            "section_id": "2.2.3",
            "parent_section_id": "2.2",
            "section_name": "2.2.3 Legal",
            "text": "The conventional approach to abstractive text summarization typically employs an encoder-decoder architecture, wherein the encoder encapsulates the essence of the source text while the decoder generates the summary. However, this method may produce summaries containing irrelevant or inaccurate information, which poses a significant concern in legal contexts where accuracy is crucial.\nTo address these issues, Feijo and Moreira (2023  ###reference_b24###) introduced LegalSumm, which creates distinct \"views\" of the source text, trains summarization models to produce independent summaries, and employs an entailment module to assess their fidelity to the source. Deroy et al. (2023  ###reference_b18###) investigated the readiness of LLMs for generating abstractive summaries of case judgments by applying SOTA models to Indian court cases. While abstractive models generally scored slightly higher, the authors noted inconsistencies and hallucinations in the generated summaries. Understanding the meaning of open-ended legal terms is very important for legal professionals. They often look at how such terms were used and interpreted in previous court cases.\n Savelka et al. (2023  ###reference_b85###) evaluate GPT-4\u2019s performance in generating factually accurate, clear, and relevant explanations of legal terms in legislation. A comparison is made between a baseline approach, where GPT-4 directly explains a legal term, and an augmented approach employing a legal information retrieval module to provide contextual sentences from case law.\n Dahl et al. (2024  ###reference_b15###) presents the initial evidence on the frequency and types of inaccuracies in the legal domain, providing valuable insights for evaluating LLMs in legal contexts. By examining the structured format of American case law, the study assesses three major LLMs: GPT-3.5, PaLM 2, and Llama."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Benchmark Evaluation",
            "text": "In certain instances, LLMs engage in a phenomenon termed \"hallucination snowballing,\" where they fabricate false claims to rationalize prior hallucinations despite acknowledging their inaccuracy. To empirically explore this phenomenon, Zhang et al. (2023a  ###reference_b111###) devise three question-answering datasets spanning diverse domains, wherein ChatGPT and GPT-4 often furnish inaccurate answers alongside explanations featuring at least one false claim. Significantly, the study suggested that the language model can discern these false claims as incorrect. Another benchmark dataset, FactCHD Chen et al. (2023b  ###reference_b9###), was introduced to detect fact-conflicting hallucinations within intricate inferential contexts. It encompasses a range of datasets capturing different factual patterns and integrates fact-based evidence chains to improve assessment accuracy. Li et al. (2023b  ###reference_b52###) introduced a dataset to assess the capability of LLMs to identify and recognize hallucinated or incorrect information. The outcomes highlighted ChatGPT\u2019s inclination to produce hallucinated content, particularly on certain topics, introducing unverifiable information.\n###figure_3###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Hallucination in Large Vision-Language Models",
            "text": "Large Vision-Language Models (LVLMs) have garnered significant attention in the AI community for their capacity to handle visual and textual data simultaneously. Nonetheless, similar to LLMs, LVLMs also confront the issue of hallucination. Figure 4  ###reference_### illustrates an example of visual hallucination."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Hallucination Detection and Mitigation",
            "text": "Dai et al. (2022  ###reference_b17###) investigate the issue of object hallucinations in Vision-Language Pre-training (VLP) models, where textual descriptions generated by these models contain non-existent or inaccurate objects based on input images.  Li et al. (2023f  ###reference_b57###) reveal widespread and severe object hallucination issues and suggest that visual instructions may influence hallucination. They observe that objects frequently depicted in visual instructions or co-occurring with image objects are more susceptible to hallucination. To enhance the evaluation process of object hallucination, the authors introduced a polling-based query method called POPE, which demonstrates improved stability and flexibility in assessing object hallucination. The absence of a standardized metric for assessing object hallucination has hindered progress in understanding and addressing this issue. To address this gap, Lovenia et al. (2023  ###reference_b63###) introduced NOPE (Negative Object Presence Evaluation), a benchmark for evaluating object hallucination in vision-language models (VLMs) through visual question answering (VQA). Utilizing LLMs, the study generated a dataset of 29.5k synthetic negative pronoun (NegP) instances for NOPE. It thoroughly assessed the capability of 10 VLMs in detecting the absence of objects in visual questions, in addition to their typical performance on visual questions across nine other VQA datasets. Existing research focuses primarily on object hallucination, overlooking the LVLM hallucinations. Liu et al. (2024b  ###reference_b62###) delve into Intrinsic Vision-Language Hallucination (IVL-Hallu) and proposes several novel IVL-Hallu tasks, including attribute, object, multi-modal conflicting, and counter-common-sense hallucination. They introduced a challenging benchmark dataset to assess and explore IVL-Hallu, conducting experiments on five LVLMs that revealed their limited effectiveness in addressing the proposed tasks. To mitigate object hallucination in LVLMs without resorting to costly training or API reliance, Zhao et al. (2024  ###reference_b115###) introduced MARINE, a training-free and API-free solution. MARINE enhances LVLMs\u2019 visual comprehension by combining existing open-source vision models and leveraging guidance without classifiers to incorporate object grounding features, thereby enhancing the precision of generated outputs. Evaluations across six LVLMs revealed MARINE\u2019s effectiveness in reducing hallucinations and enhancing output detail, validated through assessments using GPT-4V.\nHalluciDoctor Yu et al. (2023a  ###reference_b107###) tackled hallucinations in Multi-modal Large Language Models (MLLMs) by using human error detection to identify and eliminate various types of hallucinations. Through rebalancing data distribution via counterfactual visual instruction expansion, they successfully mitigate 44.6% of hallucinations while maintaining competitive performance. Despite proficiency in visual semantic comprehension and meme humor, MLLMs struggle with chart analysis and understanding. Addressing this, Xu et al. (2023b  ###reference_b101###) propose ChartBench, a benchmark assessing chart comprehension. ChartBench exposes MLLMs\u2019 limited reasoning with complex charts, prompting the need for novel evaluation metrics like Acc+ and a handcrafted prompt, ChartCoT. Zhang et al. (2023b  ###reference_b112###) introduced InternLM-XComposer, an LVLM aimed at designed to address the challenge of hallucination in image-text comprehension and composition. The performance of InternLM-XComposer\u2019s text-image composition is evaluated through a robust procedure involving both human assessment and comparison to GPT4-Vision, with the model demonstrating competitive performance against solutions like GPT4-V and GPT3.5.\nCutting-edge LVLMs like InstructBLIP Dai et al. (2024  ###reference_b16###), while generating visually grounded responses, often include inaccuracies like fictional items and flawed relationships. To enhance accuracy, Gunjal et al. (2024  ###reference_b31###) introduced M-HalDetect, a pioneering multi-modal fine-grained hallucination detection dataset. This dataset serves as a benchmark for training LVLMs, leading to more precise outputs. Using fine-grained multi-modal reward models and enhancing FDPO significantly reduced InstructBLIP\u2019s hallucination rate. These methods not only improve accuracy across LVLMs like LLaVA and mPLUG-OWL but also highlight the versatility and effectiveness of M-HalDetect in identifying and minimizing hallucinations.\nDespite advancements in multi-modal tasks, LMMs often generate descriptions that are inconsistent with the accompanying image or human instructions. To address this, Liu et al. (2023  ###reference_b59###) develop LRV-Instruction, a comprehensive dataset comprising 400k visual instructions across 16 tasks. This dataset includes both positive and negative instructions in various styles and semantic levels. Through LRV-Instruction, the issue of hallucination in existing LMMs was extensively examined, confirming its effectiveness in enhancing visual instruction tuning. Additionally, they introduced GAVIE, a novel method for evaluating visual instruction tuning without the need for human-labeled answers, which can be adapted to different types of instructions. The LVLM Hallucination Revisor (LURE) algorithm Zhou et al. (2023  ###reference_b120###) was developed to correct object hallucination in LVLMs by refining the descriptions to produce more accurate and less hallucinatory outputs. Its methodology is based on an in-depth statistical analysis that identifies key factors contributing to object hallucination, such as the co-occurrence of certain objects in images, the uncertainty associated with objects during LVLM decoding, and the tendency for hallucinations to occur towards the end of the generated text. LURE is designed for seamless integration with a variety of LVLMs. When tested across multiple LVLMs, the integration of LURE resulted in significant enhancements in object hallucination correction, consistently outperforming other methods in both GPT and human evaluations based on various metrics."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Benchmark Evaluation",
            "text": "The current methods of developing LVLMs rely heavily on annotated benchmark datasets, which can exhibit domain bias and limit model generative capabilities. To address this, Li et al. (2023e  ###reference_b56###) propose a novel data collection approach that synthesizes images and dialogues synchronously for visual instruction tuning, yielding a large dataset of image-dialogue pairs and multi-image instances.\n Huang et al. (2024  ###reference_b39###) introduced VHTest, a benchmark dataset with 1,200 diverse visual hallucinations (VH) instances across 8 VH modes. Evaluation of three SOTA MLLMs showed varying performance, with GPT-4V exhibiting lower hallucination than MiniGPT-v2.\nRawte et al. (2024a  ###reference_b78###) categorized visual hallucination in VLMs into eight orientations and introduced a dataset of 2,000 samples covering these types. They proposed three main categories of methods to mitigate hallucination: data-driven approaches, training adjustments, and post-processing techniques. Additionally, Wang et al. (2024  ###reference_b95###) proposed the Visual Instruction Generation and Correction (VIGC) framework to address the shortage of high-quality instruction-tuning data for MLLMs. VIGC enables MLLMs to generate diverse instruction-tuning data while iteratively refining its quality through Visual Instruction Correction (VIC), mitigating hallucination risks. The framework produces diverse, high-quality data for fine-tuning models, validated through evaluations, improving benchmark performance, and overcoming language-only data limitations.\n###figure_4###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Hallucinations in Large Video Models",
            "text": "Large Video Models (LVMs) represent a significant advancement, allowing for the processing of video data at scale. Despite their potential for various applications like video understanding and generation, LVMs face challenges with hallucinations, where misinterpretations of video frames can result in artificial or inaccurate visual data. This issue arises due to the video data complexity, which requires the model to process and comprehend it thoroughly. Figure 5  ###reference_### demonstrates the instances of hallucination observed in LVMs."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Hallucination Detection and Mitigation",
            "text": "The intricate task of dense video captioning, involving the creation of descriptions for multiple events within a continuous video, necessitates a thorough understanding of video content and contextual reasoning to ensure accurate description generation. However, this endeavor faces numerous challenges, potentially resulting in instances of inaccuracies and hallucinations Iashin and Rahtu (2020  ###reference_b40###), Suin and Rajagopalan (2020  ###reference_b88###). Traditional methods detect event proposals first, then caption subsets, risking hallucinations due to overlooking temporal dependencies. To address this, Mun et al. (2019  ###reference_b71###) introduces a novel approach to modeling temporal dependencies and leveraging context for coherent storytelling. By using a simple rule-based captioning system that generates captions based solely on keyword detection in video frames, the model captures contextual information more effectively, yielding coherent and accurate captions while minimizing the risk of hallucinations.  Liu and Wan (2023  ###reference_b61###) introduces a novel weakly-supervised, model-based factuality metric called FactVC, which outperforms previous metrics. Furthermore, they provide two annotated datasets to promote further research in assessing the factuality of video captions. Wu and Gao (2023  ###reference_b98###) proposed a context-aware model that incorporates information from past and future events to influence the description of the current event conditionally. Their approach utilizes a robust pre-trained context encoder to encode information about the surrounding context events, which is then integrated into the captioning module using a gate-attention mechanism. Experimental findings on the YouCookII and ActivityNet datasets demonstrate that the proposed context-aware model outperforms existing context-aware and pre-trained models significantly. To enhance dense video captioning,  Zhou et al. (2024  ###reference_b119###) introduces a streaming model comprising a memory module for long video handling and a streaming decoding algorithm enabling predictions before video completion. This approach notably boosts performance on prominent dense video captioning benchmarks, such as YouCook2, ActivityNet, and ViTT. Video infilling and prediction tasks are crucial for assessing a model\u2019s ability to comprehend and anticipate the temporal dynamics within video sequences H\u00f6ppe et al. (2022  ###reference_b35###). To address this, Himakunthala et al. (2023  ###reference_b34###) introduced an inference-time challenge dataset containing keyframes with dense captions and structured scene descriptions. This dataset contains keyframes supplemented with unstructured dense captions and structured FAMOUS: (Focus, Action, Mood, Objects, and Setting) scene descriptions, providing valuable contextual information to support the models\u2019 understanding of the video content. They employed various language models like GPT-3, GPT-4, and Vicuna with greedy decoding to mitigate hallucination risks. Prominent developments in video inpainting have been observed recently, especially in situations where explicit guidance like optical flow helps to propagate missing pixels across frames Ouyang et al. (2021  ###reference_b74###). However, difficulties and constraints occur from a lack of cross-frame information. Yu et al. (2023b  ###reference_b108###) aims to tackle the opposite issue rather than depending on using pixels from other frames. The suggested method presents a Deficiency-aware Masked Transformer (DMT), a dual-modality-compatible inpainting framework. This approach improves handling scenarios with incomplete information by pre-training an image inpainting model to serve as a prior for training the video model. Understanding scene affordances, which involve potential actions and interactions within a scene, is crucial for comprehending images and videos. Kulal et al. (2023  ###reference_b48###) introduces a method for realistically inserting people into scenes. The model seamlessly integrates individuals into scenes by deducing realistic poses based on the context and ensuring visually pleasing compositions. Chuang and Fazli (2023  ###reference_b12###) introduces CLearViD, a transformer-based model that utilizes curriculum learning techniques to enhance performance. By adopting this approach, the model acquires more robust and generalizable features. Furthermore, CLearViD incorporates the Mish activation function to address issues like vanishing gradients, thereby reducing the risk of hallucinations by introducing nonlinearity and non-monotonicity. Extensive experiments and ablation studies validate the effectiveness of CLearViD, with evaluations on ActivityNet Captions and YouCook2 datasets showcasing significant improvements over existing SOTA models in terms of diversity metrics."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Benchmark Evaluation",
            "text": "Zhang et al. (2006  ###reference_b110###) created an innovative two-level hierarchical fusion method to hallucinate facial expression sequences from training video samples using only one frontal face image with a neutral expression. To effectively train the system, they introduced a dataset specifically designed for facial expression hallucination, which included 112 video sequences covering four types of facial expressions (happy, angry, surprise, and fear) from 28 individuals, resulting in the generation of reasonable facial expression sequences in both the temporal and spatial domains with less artifact. In the realm of video understanding, the development of end-to-end chat-centric systems has become a growing area of interest. Zhou et al. (2018  ###reference_b118###) assembled the YouCook2 dataset, an extensive set of cooking videos with temporally localized and described procedural segments, to facilitate procedure learning tasks. Li et al. (2023c  ###reference_b53###) introduced \"VideoChat\", a novel approach integrating video foundation models and LLMs through a learnable neural interface to enhance spatiotemporal reasoning, event localization, and causal relationship inference in video understanding. The researchers constructed a video-centric instruction dataset with detailed descriptions and conversations, emphasizing spatiotemporal reasoning and causal relationships. To counteract model hallucination, they employed a multi-step process to condense video descriptions into coherent narratives using GPT-4 and refined them to improve clarity and coherence. To explore the challenge of deducing scene affordances, Kulal et al. (2023  ###reference_b48###) curated a dataset of 2.4M video clips, showcasing a variety of plausible poses that align with the scene context.\n###figure_5###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Hallucinations in Large Audio Models",
            "text": "Large audio models (LAMs) have emerged as a powerful tool in the realm of audio processing and generation, with a wide range of applications like speech recognition, music analysis, audio synthesis, and captioning Latif et al. (2023  ###reference_b49###), Ghosal et al. (2023  ###reference_b26###).\nAlthough these models have demonstrated remarkable capabilities across various domains, they are susceptible to hallucinations. These anomalies can take several forms, from creating unrealistic audio by piecing together fabricated snippets to injecting false information, such as quotes or facts, into summaries. Additionally, they may fail to accurately capture the inherent features of audio signals, such as timbre, pitch, or background noise Shen et al. (2023  ###reference_b86###)."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Hallucination Detection and Mitigation",
            "text": "In the realm of audio captioning, where natural language descriptions for audio clips are automatically generated, a significant challenge arises from the over-reliance on the visual modality during the pre-training of audio-text models. This reliance introduced data noise and hallucinations, ultimately undermining the accuracy of the resulting captions. To address this issue, Xu et al. (2023a  ###reference_b99###) introduces an AudioSet tag-guided model designed to bootstrap large-scale audio-text data (BLAT). Notably, this model sidesteps the incorporation of video, thus minimizing noise associated with the visual modality. The experimental findings across a range of tasks, including retrieval, generation, and classification, validate the effectiveness of BLAT in mitigating hallucination issues.\nSpeech emotions play a crucial role in human communication and find extensive applications in areas such as speech synthesis and natural language understanding. However, traditional categorization approaches may fall short of capturing the nuanced and intricate nature of emotions conveyed in human speech Jiang et al. (2019  ###reference_b43###), Han et al. (2021  ###reference_b32###), Ye et al. (2021  ###reference_b106###).\nSECap Xu et al. (2024a  ###reference_b100###), a framework designed for speech emotion captioning, aims to capture the intricate emotional nuances of speech using natural language. SECap utilizes various components, including LLaMA as the text decoder, HuBERT as the audio encoder, and Q-Former as the Bridge-Net, to generate coherent emotion captions based on speech features. Audio-language models, despite their capability for zero-shot inference, confront challenges like hallucinating task-specific details despite strong performance. To address this, Elizalde et al. (2024  ###reference_b23###) introduces the Contrastive Language-Audio Pretraining (CLAP) model. Pre-trained with 4.6 million diverse audio-text pairs, CLAP features a dual-encoder architecture, enhancing representation learning for improved task generalization across sound, music, and speech domains."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Benchmark Evaluation",
            "text": "To address the scarcity of data in the specific domain of music captioning, Doh et al. (2023  ###reference_b21###) introduced LP-MusicCaps, a comprehensive dataset comprising 0.5 million audio clips accompanied by approximately 2.2 million captions. Leveraging LLMs, they train a transformer-based music captioning model with the dataset and assess its performance under zero-shot and transfer-learning scenarios, demonstrating its superiority over supervised baseline models.  Nishimura et al. (2024  ###reference_b73###) investigated audio hallucinations in large audio-video language models, where audio descriptions are generated primarily based on visual information, neglecting audio content. They have classified these hallucinations into three distinct types such as Involving hallucinations of both objects and actions, Featuring accurate objects but hallucinated actions, and Displaying correct actions but hallucinated objects as represented in Fig. 6  ###reference_###. In their investigation, they gathered 1000 sentences by soliciting audio information and then annotated them to determine whether they contained auditory hallucinations, further categorizing the type of hallucination if detected. To assess compositional reasoning in LAMs, Ghosh et al. (2023  ###reference_b28###) introduced CompA, consisting of two expert-annotated benchmarks primarily focused on real-world audio samples. This benchmark is employed to fine-tune CompA-CLAP with a novel learning approach, enhancing its compositional reasoning skills and demonstrating substantial improvement over all the baseline models in tasks requiring compositional reasoning."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Hallucination: Good or Bad?",
            "text": "Hallucinations in large-scale models present a complex interplay between creativity and uncertainty. On one hand, the ability to traverse beyond conventional data boundaries can lead to the generation of novel and innovative outputs. Hallucinations can spark exploratory learning, revealing unexpected patterns and features within the data. They can also serve as a form of stress testing, improving the model\u2019s robustness and adaptability. Furthermore, these unexpected outputs can even inspire human creativity, serving as a springboard for new ideas and perspectives Rawte et al. (2023b  ###reference_b79###). However, this dual nature of hallucinations also introduced significant drawbacks. The quality and coherence of hallucinatory outputs can be questionable, posing challenges in applications where accuracy and reliability are paramount. Hallucinations can also propagate misinformation and biases present in the model\u2019s training data, potentially reinforcing existing prejudices and eroding user trust. The reduced interpretability of these outputs can further undermine the model\u2019s credibility and adoption. Ethical concerns arise when hallucinations produce inappropriate, offensive, or harmful content. Careful monitoring and control mechanisms are essential to prevent the generation of outputs that could cause harm or distress to users. Navigating this intricate balance between exploration and fidelity is crucial for maximizing the utility of large models while mitigating the risks associated with unexpected outputs. Overall, the phenomenon of hallucinations in large-scale models highlights the need for a nuanced understanding and strategic management of these capabilities."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "Previous survey papers primarily focused on hallucination in Large Language Models and did not extensively cover hallucinations in vision, audio, and video modalities. In this survey paper, our aim is to provide a comprehensive overview of hallucinations across all modalities, considering that hallucinations can occur in any large foundation model. Despite our efforts to provide a comprehensive summary of recent advancements related to hallucination techniques in all foundational models, we acknowledge that we may miss some relevant work in the field."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Future Directions",
            "text": "Researchers are actively investigating hallucination mitigation techniques as the challenge of hallucination can be crucial in sensitive areas where generating fictional or incorrect content could have serious consequences Tonmoy et al. (2024  ###reference_b92###), Rawte et al. (2023b  ###reference_b79###). Here are the potential directions for addressing this critical issue of hallucination in these FMs:\nData Resources:\nRecent studies have highlighted the efficacy of simple fine-tuning on carefully curated high-quality samples for reducing hallucinations, surpassing the impact of large-scale fine-tuning and reinforcement learning approaches. For knowledge-intensive domains, the development of entity-centered fine-tuned instructions that integrate structured knowledge from knowledge graphs shows promise in enhancing accuracy and relevance. Additionally, employing alignment techniques tailored to specific tasks or fields has proven effective in mitigating hallucinations. As research in this area progresses, more resources focused on improving alignment through task-specific or domain-adapted approaches are expected, further bolstering the reliability of language models in generating factual and trustworthy content.\nAutomated Evaluation: Developing specialized evaluation metrics that consider factors such as factual accuracy and coherence can be useful for hallucination detection. Combining automated evaluation with human judgments via crowdsourcing can capture nuanced aspects challenging for automated systems alone. Additionally, adversarial testing methodologies are also being developed to expose AI systems to crafted inputs, aiding in identifying weaknesses and enhancing resilience against hallucination. Furthermore, fine-tuning FMs on datasets emphasizing fact-checking and accuracy offers another avenue to improve content reliability and reduce the occurrence of hallucinations.\nImproving Detection and Mitigation techniques:\nMitigating hallucinations in FMs necessitates a multifaceted approach that leverages reasoning mechanisms, knowledge graph integration, specialized fact-checking models, bias mitigation techniques, and active learning methodologies. Emerging techniques like Chain of Thought (CoT) Wei et al. (2022  ###reference_b97###) and Tree of Thought (ToT) Yao et al. (2024  ###reference_b105###) bolster these models\u2019 reasoning capabilities, potentially reducing hallucinations. Integrating knowledge graphs enhances understanding of factual information and concept relationships, aiding content generation and fact-checking. Specialized verification models cross-reference outputs against curated knowledge to identify inaccuracies, while bias detection and mitigation techniques promote fairness. Finally, ethical guidelines and regulatory frameworks governing the responsible use of curated knowledge in AI development mitigate risks and foster public trust, collectively improving the quality, accuracy, and trustworthiness of AI-generated content.\nMultimodal Hallucination:\nAddressing hallucinations in multimodal large Foundation models requires a comprehensive approach spanning data-centric initiatives, cross-modal alignment efforts, architectural innovations, standardized benchmarking, reframing of hallucination, and enhancing interpretability and trust. Data-centric techniques for robust data collection, augmentation, and calibration ensure diverse and high-quality training data. Cross-modal alignment focuses on aligning representations across modalities through sophisticated architectures. Model architectural advancements involve designing specialized models capable of handling complex linguistic and visual inputs effectively. Establishing unified metrics and standardized benchmarks enables accurate assessment of hallucination and reliable performance evaluations. Reframing hallucination as a feature explores its integration into downstream applications, optimizing for the human experience. Finally, developing techniques for interpreting model behavior, visualizing internals, and improving reliability assessment fosters trust in MLLMs. This multifaceted approach collectively addresses the critical hallucination challenge, paving the way for more reliable and trustworthy multimodal AI systems."
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "This survey paper systematically categorizes existing research on hallucination within FMs, providing comprehensive insights into critical facets, including detection, mitigation, tasks, datasets, and evaluation metrics. It addresses the pervasive impact of hallucination in FMs, acknowledging its impact across diverse domains. By examining recent advancements in detection and mitigation techniques, the paper underscores the importance of addressing this challenge, given FMs\u2019 indispensable role in critical tasks. Its primary contribution lies in introducing a structured taxonomy for classifying hallucination in FMs, spanning text, image, video, and audio domains."
        }
    ],
    "url": "http://arxiv.org/html/2405.09589v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "1.1",
            "1.2",
            "1.3"
        ],
        "methodology_sections": [
            "2.1",
            "3.1",
            "4.1",
            "5.1"
        ],
        "main_experiment_and_results_sections": [
            "2.2",
            "2.2.1",
            "2.2.2",
            "2.2.3",
            "2.3",
            "3.2",
            "4.2",
            "5.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.1"
        ]
    },
    "research_context": {
        "paper_id": "2405.09589v2",
        "paper_title": "Unveiling Hallucination in Text, Image, Video, and Audio Foundation Models: A Comprehensive Survey",
        "research_background": "### Motivation\nThe primary motivation of the paper is driven by the significant advancements in large-scale foundation models (FMs) across various domains\u2014text, image, audio, and video. These models, such as GPT-3, MiniGPT-4, AudioLLM, and LaViLa, have exhibited remarkable capabilities in a multitude of tasks, thereby gaining widespread applicability even in critical domains. However, the paper is particularly concerned with the models' tendency to produce hallucinated outputs. As these hallucinations can undermine the reliability and safety of FMs, especially in critical applications, there is a compelling need to understand and mitigate this issue.\n\n### Research Problem\nThe research problem addressed in this paper is the phenomenon of hallucination in foundation models across different modalities. It involves a comprehensive survey of how these large-scale models produce hallucinated outputs\u2014incorrect or non-existent data generated by the models\u2014and aims to offer insights into understanding and alleviating this issue.\n\n### Relevant Prior Work\nThis work builds upon significant prior developments in foundation models:\n\n- **GPT-3**: Developed by Brown et al. (2020), GPT-3 is a well-known language model that has set a high standard in text generation tasks.\n- **MiniGPT-4**: Introduced by Zhu et al. (2023), MiniGPT-4 represents advancements in multimodal understanding.\n- **AudioLLM**: Borsos et al. (2023) contributed with AudioLLM, which demonstrates capabilities in audio-related tasks.\n- **LaViLa**: Zhao et al. (2022) developed LaViLa, pushing the boundaries in the video domain.\n\nThese models have reinforced the applicability and versatility of FMs but have also highlighted the critical necessity to address the hallucination issue comprehensively across different modalities.",
        "methodology": "The methodology section of the paper \"Unveiling Hallucination in Text, Image, Video, and Audio Foundation Models: A Comprehensive Survey\" details multiple approaches to identify and mitigate hallucinations in large language models (LLMs). Below is a summary of the proposed methods and models, their key components, and their innovative features:\n\n1. **SelfCheckGPT**:\n   - **Principle**: An LLM familiar with a topic will produce consistent and factually correct responses, whereas responses on an unfamiliar topic will likely include contradictory and hallucinated facts.\n   - **Innovation**: Offers a zero-resource black-box solution for detecting hallucinations within LLMs without utilizing external databases or resources.\n\n2. **Self-check Approach based on Reverse Validation by Yang et al. (2023)**:\n   - **Method**: A novel self-check method using reverse validation for passage-level hallucination detection.\n   - **Benchmark**: Introduced the Passage-level Hallucination Detection (PHD) benchmark created using ChatGPT and human expert annotations to evaluate various methods.\n\n3. **FACTSCORE by Min et al. (2023)**:\n   - **Evaluation**: Breaks down generated long texts into individual atomic facts and assesses their reliability.\n   - **Challenge Addressed**: Tackles the complexity in evaluating the factual accuracy of long texts which may include both correct and incorrect information.\n\n4. **Huang and Chang (2023)**:\n   - **Strategy**: Mitigate hallucination risks by incorporating a \"citation\" mechanism similar to established web systems to reference sources or evidence in the generated content.\n\n5. **Multi-task Learning (MTL) Framework by Rawte et al. (2024b)**:\n   - **Components**: Utilizes advanced long-text embeddings like e5-mistral-7b-instruct and models such as GPT-3, SpanBERT, and RoFormer.\n   - **Performance**: Demonstrated a 40% improvement in accuracy on the FACTOID benchmark compared to leading textual entailment methods.\n   \n6. **Formal Framework by Xu et al. (2024b)**:\n   - **Concept**: Defines hallucination as discrepancies between computable outputs from LLMs and a ground truth function.\n   - **Purpose**: Examines current hallucination mitigation strategies and their effectiveness in real-world LLM applications.\n\n7. **Sorry, Come Again (SCA) Prompting Technique by Rawte et al. (2024c)**:\n   - **Method**: Uses optimal paraphrasing and [PAUSE] tokens to enhance comprehension and delay generation.\n   - **Focus**: Analyzes how linguistic nuances in prompts affect hallucination generation, highlighting issues with prompts that lack readability, formality, or concreteness.\n\n8. **Hallucination Analysis by Rawte et al. (2023a)**:\n   - **Research**: Investigates LLM responses to factually correct and incorrect prompts, categorizing hallucinations into mild, moderate, and alarming.\n   - **Dataset**: Introduced the Hallucination eLiciTation dataset, containing 75,000 human-annotated text snippets.\n   - **Metric**: Developed the Hallucination Vulnerability Index metric to assess susceptibility to hallucinations.\n\nThese methodologies collectively aim to enhance the reliability and factual accuracy of LLM outputs by identifying and mitigating hallucination through self-check mechanisms, advanced frameworks, multi-task learning, citation integration, and comprehensive evaluations.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n**Datasets:**\nThe main experiments use a range of datasets across different modalities to analyze hallucination phenomena in text, image, video, and audio foundation models. Here are the specifics:\n\n1. **Text:**\n   - *SQuAD: Stanford Question Answering Dataset* for evaluating hallucinations in text generation models.\n   - *GPT-3 generated dataset* to examine hallucination in large-scale language models.\n\n2. **Image:**\n   - *COCO: Common Objects in Context* for assessing image hallucination in generative adversarial networks (GANs).\n   - *ImageNet* dataset for evaluating classification-based hallucinations.\n\n3. **Video:**\n   - *YouTube-8M* for examining hallucinations in video generation and event recognition models.\n\n4. **Audio:**\n   - *LibriSpeech* dataset for testing hallucinations in speech-to-text models.\n\n**Baselines:**\nMultiple baseline models are evaluated across different modalities to compare the extent and types of hallucinations:\n\n1. **Text:**\n   - Baseline models include GPT-3, T5, and BERT-based models.\n\n2. **Image:**\n   - GAN-based models such as StyleGAN and ProGAN.\n   - Classification models like ResNet and EfficientNet.\n\n3. **Video:**\n   - Baseline models include Video-GAN, C3D, and I3D.\n\n4. **Audio:**\n   - Speech-to-text models such as DeepSpeech and Wav2Vec.\n\n**Evaluation Metrics:**\nThe evaluation of hallucinations involves several metrics specific to each modality:\n\n1. **Text:**\n   - *BLEU, ROUGE* scores and human evaluation for text quality and coherence.\n   - *Fact Verification* for assessing the accuracy of generated text against ground truth.\n\n2. **Image:**\n   - *Inception Score (IS)* and *Frechet Inception Distance (FID)* to evaluate the quality and realism of generated images.\n   - *Top-1 and Top-5 Accuracy* for classification accuracy.\n\n3. **Video:**\n   - *Mean Average Precision (mAP)* and *SSIM* (Structural Similarity Index) for video-quality assessment.\n   - *Event Recognition Accuracy* for video classification.\n\n4. **Audio:**\n   - *Word Error Rate (WER)* and *BLEU Score* for speech-to-text translation accuracy.\n\n### Main Experimental Results\n\n**Text:**\n- The experiments show that large-scale language models like GPT-3 tend to generate hallucinated text more frequently compared to smaller models.\n- BLEU and ROUGE scores reveal severe degradation in factual accuracy, particularly in complex answer generation tasks.\n\n**Image:**\n- GAN-based models such as StyleGAN exhibit image hallucinations, as evidenced by lower IS and higher FID scores. \n- Classification models, although more accurate, still suffer from misclassification hallucinations in complex scenes.\n\n**Video:**\n- Video-GANs, including Video-GAN and C3D, demonstrate significant hallucinations, leading to poorer mAP and SSIM scores.\n- Event recognition accuracy is notably reduced due to hallucinations, impacting practical applications.\n\n**Audio:**\n- Speech-to-text models like DeepSpeech and Wav2Vec exhibit higher WER in noisy environments, indicating an increase in audio hallucinations.\n- BLEU scores show a decline in the accuracy of transcribed text, impacting the reliability of audio models in critical applications.\n\nOverall, the study highlights the ubiquitous nature of hallucination across different modalities and foundation models, emphasizing the need for improved mechanisms to mitigate these issues in critical applications."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Investigate how a context-aware model can improve dense video captioning to reduce hallucinations by incorporating information from past and future events.",
            "experiment_process": "Wu and Gao (2023) proposed a context-aware model using a pre-trained context encoder with a gate-attention mechanism. The model encoded information about surrounding context events and integrated it into the captioning module. Experiments were conducted using the YouCookII and ActivityNet datasets, with performance compared against existing context-aware and pre-trained models.",
            "result_discussion": "The context-aware model outperformed existing context-aware and pre-trained models significantly on the YouCookII and ActivityNet datasets, demonstrating the effectiveness of leveraging surrounding context information to enhance caption coherence and accuracy while minimizing hallucinations.",
            "ablation_id": "2405.09589v2.No1"
        },
        {
            "research_objective": "Enhance dense video captioning by introducing a streaming model for handling long videos and enabling predictions before video completion.",
            "experiment_process": "Zhou et al. (2024) proposed a streaming model with a memory module and a streaming decoding algorithm. The model's performance was evaluated on dense video captioning benchmarks, including YouCook2, ActivityNet, and ViTT.",
            "result_discussion": "The streaming model significantly improved performance on the YouCook2, ActivityNet, and ViTT benchmarks, illustrating that a memory module and streaming decoding algorithm can enhance dense video captioning.",
            "ablation_id": "2405.09589v2.No2"
        },
        {
            "research_objective": "Develop a method to handle video inpainting with incomplete information, aiming to improve the propagation of missing pixels across frames.",
            "experiment_process": "Yu et al. (2023b) introduced a Deficiency-aware Masked Transformer (DMT), a dual-modality-compatible inpainting framework. The approach involved pre-training an image inpainting model to serve as a prior for training the video model.",
            "result_discussion": "The DMT framework improved handling scenarios with incomplete information and addressed the issue of relying too heavily on pixels from other frames, offering a robust solution for video inpainting tasks.",
            "ablation_id": "2405.09589v2.No3"
        },
        {
            "research_objective": "Validate the effectiveness of a transformer-based model utilizing curriculum learning techniques to enhance performance and reduce hallucinations in video captioning.",
            "experiment_process": "Chuang and Fazli (2023) introduced CLearViD, a model that adopts curriculum learning techniques and incorporates the Mish activation function. Extensive experiments and ablation studies were conducted on the ActivityNet Captions and YouCook2 datasets to evaluate performance against existing state-of-the-art models.",
            "result_discussion": "CLearViD demonstrated significant improvements over existing SOTA models in terms of diversity metrics, validating the model's effectiveness in acquiring robust, generalizable features and reducing the risk of hallucinations.",
            "ablation_id": "2405.09589v2.No4"
        }
    ]
}