{
    "title": "Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A Brain-Inspired Method for Parameter-Efficient Fine-Tuning",
    "abstract": "Fine-tuning techniques based on Large Pretrained Language Models (LPLMs) have been proven to significantly enhance model performance on a variety of downstream tasks and effectively control the output behaviors of LPLMs. Recent studies have proposed numerous methods for fine-tuning a small number of parameters based on open-source LPLMs, reducing the demand for computational and storage resources. Among these, reparameterization fine-tuning methods represented by LoRA (Low-Rank Adaptation) have gained popularity. We find that although these methods perform well in many aspects, there is still considerable room for improvement in terms of complex task adaptability, performance, stability, and algorithm complexity.\nIn response to this, inspired by the idea that the functions of the brain are shaped by its geometric structure, this paper integrates this idea into LoRA technology and proposes a new matrix transformation-based reparameterization method for efficient fine-tuning, named Matrix-Transformation based Low-Rank Adaptation (MTLoRA). The spatiotemporal patterns of brain neural activity are the excitation of different wavelength characteristic patterns of its geometric structure. MTLoRA aims to dynamically alter its spatial geometric structure by applying a transformation-matrix  to perform linear transformations, such as rotation, scaling, and translation, on the task-specific parameter matrix, generating new matrix feature patterns (eigenvectors) to mimic the fundamental influence of complex geometric structure feature patterns in the brain on functions, thereby enhancing the model\u2019s performance in downstream tasks. The transformation-matrix  contains four different structures, each designed to simulate the geometric feature patterns of the brain at different levels. In Natural Language Understanding (NLU) tasks, it is evaluated using the GLUE benchmark test, and the results reveal that MTLoRA achieves an overall performance increase of about 1.0% across eight tasks and reduces the standard deviation by 0.7% in the Corpus of Linguistic Acceptability (CoLA) task; in Natural Language Generation (NLG) tasks, MTLoRA improves performance by an average of 0.95% and 0.56% in the DART and WebNLG tasks, respectively.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In recent years, with the rapid development of large pre-trained language models (LPLMs) such as BERT (Devlin et al., 2019  ###reference_b9###) and GPT-3 (Brown et al., 2020  ###reference_b3###), these models have shown exceptional performance in many downstream tasks of natural language processing (NLP), including text generation, machine translation, sentiment analysis, and question-answering systems (Radford et al., 2019  ###reference_b34###; He et al., 2020  ###reference_b17###; Raffel et al., 2020  ###reference_b35###; Devlin et al., 2019  ###reference_b9###; Liu et al., 2019  ###reference_b22###; Peters et al., 2018  ###reference_b32###; Brown et al., 2020  ###reference_b3###). However, training and deploying LPLMs from scratch face significant resource challenges, as training a GPT-3 model with 175 billion parameters requires running hundreds of NVIDIA A100 40GB GPUs for approximately 200 days. Such a level of resource investment is unaffordable for most research institutions and enterprises.\nFine-tuning techniques based on LPLMs have been proven to effectively enhance model performance (Ouyang et al., 2022  ###reference_b30###; Wei et al., 2021  ###reference_b48###; Min et al., 2021  ###reference_b25###; Wang et al., 2022  ###reference_b46###; Liu et al., 2022  ###reference_b21###), enabling the model to acquire desired capabilities and discard unwanted ones (Ouyang et al., 2022  ###reference_b30###; Askell et al., 2021  ###reference_b1###). The rise of open-source LPLMs globally, such as GPT (Radford et al., 2019  ###reference_b34###), BERT (Devlin et al., 2019  ###reference_b9###), GLM (Du et al., 2022  ###reference_b11###; Zeng et al., 2022  ###reference_b52###), LLaMA (Touvron et al., 2023a  ###reference_b41###), and LLaMA2 (Touvron et al., 2023b  ###reference_b42###), offers new opportunities for a wide range of research organizations and enterprises. These entities can use these open-source LPLMs as a foundation, combined with their industry knowledge and data, for full-parameter fine-tuning (Full Fine-Tuning) (Qiu et al., 2020  ###reference_b33###; Raffel et al., 2020  ###reference_b35###). However, even full-parameter fine-tuning based on open-source LPLMs may require significant computational and storage resources. For instance, according to a study by Dettmers et al. (2024  ###reference_b8###), full-parameter fine-tuning of the LLaMA 65B model could require up to 20 NVIDIA A100 40GB GPUs, which remains prohibitively expensive for many organizations. Moreover, considering the diversity of application scenarios, each downstream application needs to be trained separately, occupying the same space as the original LPLM, which not only increases deployment costs but also challenges the reliability and stability of online services.\nRecent research advancements have proposed many parameter-efficient fine-tuning methods based on LPLMs, aimed at reducing the demand for GPU computational power and storage resources during the training of downstream tasks. In the full-parameter fine-tuning process, all trainable parameters of the LPLM need to be updated. However, during the training process of parameter-efficient fine-tuning methods, the original parameters of the LPLM are frozen, and only a small number of trainable parameters are updated through gradients. This change is comparable in performance to full-parameter fine-tuning and significantly reduces the demand for computational power and storage.\nIn the field of parameter-efficient fine-tuning technology research for LPLMs, there are mainly three types of methods: First, Addition-based methods insert small trainable extension structures into the layers of the LPLM. During fine-tuning, only the trainable parameters within the extension structures are updated, while the original model\u2019s structure and parameters remain unchanged. A representative work in this category is Adapter Tuning (Houlsby et al., 2019  ###reference_b18###; Rebuffi et al., 2017  ###reference_b37###; He et al., 2022  ###reference_b16###), which may increase the model\u2019s depth and complexity. Second, Specification-based methods designate certain parameters within the LPLM as trainable and freeze the rest. BitFit, for instance, updates only the model\u2019s bias parameters during fine-tuning (Zaken et al., 2021  ###reference_b51###). Lastly, Reparameterization-based methods transform existing parameters into trainable ones. A pivotal work is LoRA (Low-Rank Adaptation), proposed by Hu et al. (2022  ###reference_b19###). LoRA approximates the increment matrix  of the pre-trained model parameters  through the product of low-rank decomposition matrices  and . Since the low-rank dimension  of  and  is much smaller than the dimension of , and the parameter matrix  is frozen during training, updating only the  and  parameter matrices significantly reduces the amount of trainable parameters. For instance, fine-tuning the GPT-3 175B model with LoRA can reduce the number of training parameters by a factor of ten thousand and the GPU memory requirement by a factor of three. There are mainly two types of methods based on LoRA improvements: one is performance optimization, such as AdaLoRA (Zhang et al., 2023b  ###reference_b54###), IncreLoRA (Zhang et al., 2023a  ###reference_b53###), and DELTA-LoRA (Zi et al., 2023  ###reference_b55###), which optimize LoRA in various aspects to enhance model performance. The other is functional expansion, like LongLoRA (Chen et al., 2023  ###reference_b6###) and QLoRA  (Dettmers et al., 2024  ###reference_b8###), which expand the model\u2019s applicability in long contexts and quantization based on LoRA as a foundational component. The LoRA fine-tuning technique, with its advantages of excellent performance, simple structure, efficient training, and no inference delay, is expected to continue to drive the development and industrial application of large models.\nDespite significant progress in fine-tuning with reparameterization methods (Hu et al., 2022  ###reference_b19###; Chen et al., 2023  ###reference_b6###; Zhang et al., 2023b  ###reference_b54###; Zi et al., 2023  ###reference_b55###) represented by LoRA, it still faces several key challenges: First, the structure of the parameter decomposition matrix is too simple and singular, making it difficult to dynamically represent various semantically complex downstream tasks. Second, performance fluctuation is significant. According to experiments by Hu et al. (2022  ###reference_b19###), LoRA has high performance fluctuation in the Corpus of Linguistic Acceptability (CoLA) task, with a standard deviation reaching 1.2%. Lastly, there is the issue of computational complexity. AdaLoRA (Zhang et al., 2023b  ###reference_b54###) and IncreLoRA (Zhang et al., 2023a  ###reference_b53###) optimize the size of the parameter incremental matrix rank () through a mechanism that measures the importance scores of the singular value decomposition triplets of the parameter incremental matrix, allowing for different amounts of trainable parameters in incremental matrices at different positions. However, this requires iterative calculations of the importance scores of the triplets in more parameter decomposition matrices to dynamically adjust the size of the incremental matrix rank (), significantly increasing the computational complexity. These challenges limit the application of LPLMs in a broader domain, and addressing these issues is crucial for advancing the development of large language models.\nThis study is inspired by the idea that the brain\u2019s functionality is shaped by its geometric structure (Pang et al., 2023  ###reference_b31###) and integrates this concept into LoRA technology. We propose a novel matrix transformation-based reparameterization method for efficient fine-tuning named MTLoRA. The spatiotemporal patterns of brain neural activity are the excitation of different wavelength characteristic modes of its geometric structure (Pang et al., 2023  ###reference_b31###). MTLoRA employs a transformation-matrix  to perform linear transformations on task-specific parameter matrices, such as rotation, scaling, and translation, dynamically altering their spatial geometric structure to generate new matrix feature patterns (eigenvectors). This mimics the fundamental impact of complex geometric structure characteristic patterns in the brain on functionality, enhancing the performance of the fine-tuned model, thereby alleviating the aforementioned issues. The  transformation matrix contains four different structures, each designed to simulate the characteristic patterns of geometric structures at different levels of the brain.\nIn this paper, through empirical experiments on two categories of 11 tasks in NLU and NLG, the effectiveness of the MTLoRA method is verified. Compared to the LoRA method, it is evident from the experimental results table 1  ###reference_### that MTLoRA improves performance by about 1.54% (=0.1%) on the CoLA (Warstadt et al., 2019  ###reference_b47###) task, effectively reducing the standard deviation; it enhances performance by about 3.61% (=0.8%) on the RTE (Dagan et al., 2005  ###reference_b7###; Haim et al., 2006  ###reference_b15###; Giampiccolo et al., 2007  ###reference_b14###; Bentivogli et al., 2009  ###reference_b2###) task; it increases performance by about 2.45% (=0.3%) on the MRPC (Dolan and Brockett, 2005  ###reference_b10###) task; and it boosts performance by about 0.88% (=0.0%) on the QQP (Quora Question Pairs) task. From the experimental results table 2  ###reference_###, it is known that MTLoRA achieves an average performance increase of about 0.95% (=0.1%) on the DART (Nan et al., 2020  ###reference_b26###) task and about 0.56% (=0.1%) on the WebNLG (Gardent et al., 2017  ###reference_b13###) task.\nOverall, the MTLoRA fine-tuning method proposed in this paper demonstrates significant advantages in downstream tasks based on LPLMs, which include:\n1) The rich matrix transformation structure significantly enhances model performance in many tasks while maintaining the simplicity of the algorithm;\n2) Effectively reduces performance fluctuations in certain tasks;\n3) After training is completed, the incremental parameter matrix  can be merged with the original parameter matrix , achieving no additional latency during the inference stage;\n4) Compared to full fine-tuning, the MTLoRA method significantly increases training speed, drastically reduces the amount of trainable parameters by 99%, and lowers the hardware requirements by several times.\n###figure_1###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related work",
            "text": "In recent years, large language models based on the Transformer (Vaswani et al., 2017  ###reference_b43###) architecture have made significant advancements in NLP field. Among these, BERT (Bidirectional Encoder Representations from Transformers) and its derivatives such as RoBERTa (Robustly Optimized BERT Approach) and DeBERTa (Decoding-enhanced BERT with Disentangled Attention) primarily utilize the Encoder part of the Transformer architecture (Devlin et al., 2019  ###reference_b9###; Liu et al., 2019  ###reference_b22###; He et al., 2020  ###reference_b17###). In contrast, the GPT (Generative Pre-trained Transformer) series, including GPT-2 and GPT-3, mainly rely on the Decoder part (Radford et al., 2019  ###reference_b34###; Brown et al., 2020  ###reference_b3###). The core feature of these models is the Multi-Head Self-Attention mechanism, which can effectively capture the dependencies between different positions in a sequence. The basic algorithm of this mechanism can be represented by the following formula (Vaswani et al., 2017  ###reference_b43###):\nwhere , , and  represent the Query, Key, and Value vectors, respectively; , , and  are the corresponding linear transformation matrices;  is the output transformation matrix; and  is the dimension of the key vector, used to scale the dot product to prevent the softmax function from entering the saturation region.\nThe relationship between model performance and the amount of parameters has attracted the attention of numerous scholars. For instance, the GPT-2 model presented by Radford et al. (2019  ###reference_b34###), which has 1.5 billion parameters, demonstrated a significant improvement in the quality and consistency of language generation compared to its predecessor, GPT. Furthermore, the GPT-3 model introduced by Brown et al. in 2020, with its 175 billion parameters, exhibited unprecedented language understanding and generation capabilities (Brown et al., 2020  ###reference_b3###).\nFine-tuning large-scale models is a crucial process that enables pre-trained language models to adapt to specific downstream tasks, thereby playing a more significant role across various application scenarios (Ouyang et al., 2022  ###reference_b30###; Wei et al., 2021  ###reference_b48###; Min et al., 2021  ###reference_b25###; Wang et al., 2022  ###reference_b46###; Liu et al., 2022  ###reference_b21###). Although traditional fine-tuning methods have achieved certain successes in performance, they usually require updating a large number of parameters, which not only increases computational and storage burdens but may also lead to overfitting, especially in scenarios with scarce data (Qiu et al., 2020  ###reference_b33###; Raffel et al., 2020  ###reference_b35###). Against this backdrop, the emergence of parameter-efficient fine-tuning methods provides a new direction for model fine-tuning by updating a small number of trainable parameters, achieving more efficient and flexible fine-tuning.\nLoRA (Low-Rank Adaptation) is a novel reparameterization fine-tuning technique proposed by Hu et al. (2022  ###reference_b19###). It is designed to facilitate fine-tuning training for adapting large pre-trained language models to downstream tasks with lower computational and storage resources. LoRA approximates the incremental matrix  of the pre-trained model parameter matrix  by the product of two low-rank parameter matrices  and , where  and , and the rank () is much smaller than the dimensions  or  in . During gradient updates, only the  and  parameter matrices are updated, with the  matrix being frozen, significantly reducing the number of trainable parameters and improving the performance of fine-tuning training. Specifically, the mathematical expression of LoRA during forward propagation is:\nThe  matrix is initialized with random Gaussian values, while the  matrix is initialized as a zero matrix. This initialization strategy is very important for maintaining the stability of the model at the beginning of training.\nAdaLoRA, a fine-tuning method based on LoRA, was proposed by Zhang et al. (2023b  ###reference_b54###). This method aims to find the optimal rank () size of the incremental parameter matrices at different positions in LPLMs to improve performance and prevent overfitting. The study found that parameter matrices at different positions, such as the word embedding projection matrix (), the query/key/value projection matrices (, , ), and the output projection matrix () as well as the two weight matrices in the two layers of FFNs (, ), have different impacts on model performance across various tasks.\nAdaLoRA approximates the incremental matrix  of the pre-trained model parameter matrix using singular value decomposition , where  is a diagonal matrix containing  singular values represented by a one-dimensional vector, , and . To improve performance, AdaLoRA introduced a metric strategy based on the importance scores of the singular value decomposition triplets (left singular vector, singular value, right singular vector), allowing important triplets with high scores to be retained and unimportant triplets with scores below a threshold to be discarded, thereby effectively controlling the amount of trainable parameters in different incremental matrices and preventing overfitting. They found that for the NLP task SQuAD 2.0, AdaLoRA could improve the model\u2019s F1 score by 1.2%. However, AdaLoRA generally requires finding the appropriate rank () in all parameter matrices, which brings higher computational costs.\nDELTA-LoRA, proposed by Zi et al. (2023  ###reference_b55###), is an improved technique based on the LoRA. This technique enhances the model\u2019s expressive capability by updating more model parameters, effectively adapting to and handling complex downstream tasks. Compared to LoRA, DELTA-LoRA mainly differs in two aspects:\nfirstly, during the training process, in addition to updating the low-rank adaptation parameter matrices  and , it is also necessary to update the pre-trained model parameters , with the update expression for  as follows:\nwhere the hyperparameters , , and  represent the update rate, scaling factor, and rank size, respectively.\nAdditionally, DELTA-LoRA removes the Dropout layer.\nQLoRA is a quantization fine-tuning method based on LoRA, proposed by Dettmers et al. (2024  ###reference_b8###). This method focuses on how to effectively reduce memory usage during the training phase of LPLMs while maintaining model performance. Unlike traditional quantization techniques applied only at the model deployment and inference stage, QLoRA proposes a new solution for memory optimization during the model training phase.\nQuantization techniques typically reduce memory usage by lowering the numerical precision of the model, but this may sacrifice model performance. However, QLoRA, through NormalFloat, Double Quantization, and Paged Optimizers techniques, combined with the LoRA fine-tuning method, effectively resolves the contradiction between memory saving and performance maintenance. Specifically, first, the parameter matrices in LPLMs are quantized to 4-bit representations through NormalFloat, then, the quantization constants are further reduced through Double Quantization, reducing memory overhead. Paged Optimizers address potential memory shortages during GPU operation using NVIDIA\u2019s unified memory feature.\nAccording to Dettmers et al. (2024  ###reference_b8###), full fine-tuning a 65B model requires 780GB of GPU memory, while using QLoRA technology can reduce the GPU memory capacity requirement to 48GB per card, without sacrificing runtime and predictive performance.\nLongLoRA, proposed by Chen et al. (2023  ###reference_b6###), is an extension technique based on the LoRA fine-tuning method, aimed at optimizing existing large language models to handle longer contexts. The LongLoRA method has two main innovations: (1) applying the LoRA method to the Embedding layer and Normalization layer; (2) proposing a new shifted short attention mechanism. LongLoRA significantly extends the model\u2019s long context processing capability.\nAccording to Chen et al. (2023  ###reference_b6###), after applying LongLoRA fine-tuning to the LLaMA2-7B model (Touvron et al., 2023b  ###reference_b42###), the model\u2019s context processing length could be expanded from 4096 to 100k. And after fine-tuning the LLaMA2-70B model on 8 NVIDIA A100 GPUs, its context length capability could be enhanced to 32k."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "The Method",
            "text": "Our proposed MTLoRA fine-tuning approach is inspired by the idea that the brain\u2019s functionality is shaped by its geometric structure (Pang et al., 2023  ###reference_b31###). Specifically, the neural activities of the brain are incited by the inherent resonance modes of its geometric topological structure, and the task-induced activations are the excitation of whole-brain modes, with wavelengths exceeding 60mm. Furthermore, the dominant role of wave-like activity explains the close connection between the brain\u2019s geometric structure and its function, and wave dynamics are utilized to reconstruct the spatiotemporal characteristics of spontaneous and task-induced brain activity recordings.\nThe intrinsic resonance modes of the geometric structure of the brain\u2019s neocortex can be characterized by its geometric feature patterns (Melrose and McPhedran, 1991  ###reference_b24###; Nozari et al., 2020  ###reference_b29###). These geometric feature patterns can be obtained by solving the eigenvalue problem of the Laplace\u2013Beltrami Operator (LBO) (Chavel, 1984  ###reference_b5###; Seo and Chung, 2011  ###reference_b39###) constructed on the basis of the group average template mesh representation (Fischl et al., 1999  ###reference_b12###) of the neocortex. The geometric feature patterns obtained in this manner include physical properties such as the curvature of the neocortex surface and the spatial relationships between vertices in the mesh (Wachinger et al., 2015  ###reference_b44###), analogous to how the resonance frequencies of a violin string are determined by the string\u2019s density, tension, and length. In essence, geometric feature patterns represent the vibrational modes of the system\u2019s dynamics (L\u00e9vy, 2006  ###reference_b20###), and the spatiotemporal patterns of neural activity in the neocortex are the excitations of these geometric feature patterns, just as the harmonics produced by plucking a violin string are vibrations of its own resonance modes.\nThe spatiotemporal patterns of neural activity in the neocortex can be decomposed into a weighted sum of geometric feature patterns of different wavelengths (Nowack, 1995  ###reference_b28###; Robinson et al., 2016  ###reference_b38###). According to the experimental results of  (Pang et al., 2023  ###reference_b31###), this decomposition can reconstruct the functional magnetic resonance imaging (fMRI) experimental data of neocortical activity obtained under spontaneous and task-induced conditions with more than 80% accuracy, thereby confirming that the brain\u2019s geometric structure shapes its function. This relationship between the brain\u2019s geometric structure and function can also be extended to subcortical activities, such as those in the thalamus, striatum, and hippocampus, indicating that the close connection between geometric structure and function is a ubiquitous presence in the brain.\nWe believe LPLMs should also possess rich and complex spatial geometric structure feature patterns. We aim to find a fine-tuning method that can flexibly change the geometric structure of parameter matrices based on the characteristics of downstream tasks, generating new matrix feature patterns (eigenvectors), to simulate the impact of complex geometric structure feature patterns in the brain on function, thereby improving model performance.\nThis section will introduce MTLoRA fine-tuning method in detail, using large pre-trained language models based on the Transformer architecture as an example. Transformer-based models, such as the RoBERTa model, contain multiple Blocks, each with numerous dense parameter matrices, including word embedding projection matrices (), query/key/value projection matrices (, , ), intermediate layer projection matrices (), output layer projection matrices (), and weight matrices in the MLP layer (), among others. These parameter matrices carry rich semantic information, which is our optimization target.\nMTLoRA aims to use a transformation-matrix  to perform linear transformations on parameter matrices specific to a task, changing their spatial geometric structure, and generating new matrix feature patterns, to mimic the fundamental impact of complex geometric structure feature patterns in the brain on function, thereby improving the model\u2019s performance after fine-tuning training. Specifically, MTLoRA uses the product of low-rank parameter increment matrices , , and , i.e., , to approximate the increment matrix  of the pre-trained parameter matrix  in the LPLM, where rank  <<  or . During training,  is frozen, and  performs task-specific adaptive linear transformations on  and , including scaling, rotation, translation, reflection, skewing, and projection.  contains four different structures, each designed to simulate different levels of geometric feature patterns in the brain, where structure 1 is the most basic and can handle most downstream application scenarios, while the other structures are more inclined to handle specific application scenarios with scarce or abundant corpora:\nStructure 1 SHIM (Spatial Harmonic Integration Matrix): This structure is designed to integrate different spatial feature transformations, similar to how various frequencies of harmonics combine in music to produce rich and complex timbres. By introducing the transformation matrix , it applies linear transformations such as spatial rotation, scaling, translation, and shearing to the  and  parameter matrices based on specific tasks, forming a comprehensive feature representation. This integration not only enhances the model\u2019s adaptability to different tasks but also provides a richer expression capability for adjusting model parameters, enabling the model to better adapt and optimize the processing effects of specific tasks. Its forward propagation mathematical expression in the model is shown as 4  ###reference_###:\nwhere .  represents the input vector, and  represents the output vector. Because  <<  or , the number of parameters in the  transformation matrix is very small, adding minimal overhead. The  and  matrices are initialized using random Gaussian initialization, while the  matrix is initialized as a zero matrix, as illustrated in Figure 1  ###reference_### (A).\nStructure 2 ICFM (Intrinsic Correlation Feature Matrix): This structure, by introducing the transformation matrix  and its transpose , results in a positive semi-definite matrix after multiplication, which can simulate a covariance matrix capturing the intrinsic correlation structure within the feature space. This positive semi-definite matrix applies linear transformations such as rotation and scaling to the  and  parameter matrices, highlighting task-specific features within the feature space, thus enhancing the model\u2019s representational ability by increasing the differentiation between patterns. This structure shares similarities with the covariance matrix in Mahalanobis distance and spatial rotation in Principal Component Analysis (PCA) (Wold et al., 1987  ###reference_b50###). Its forward propagation mathematical expression in the model is shown as 5  ###reference_###:\nwhere , , representing the transpose of matrix , , as illustrated in Figure 1  ###reference_### (B).\nStructure 3 CTCM (Composite Transformation Coupling Matrix): By introducing two different matrices,  and , and forming a composite transformation matrix through matrix multiplication, this structure enhances the expressiveness of the parameter increment matrix, thereby fostering the model\u2019s adaptability to complex tasks. Its forward propagation mathematical expression in the model is shown as 6  ###reference_###:\nwhere , , as illustrated in Figure 1  ###reference_### (C).\nStructure 4 DTSM (Dual Transformation Superposition Matrix): This structure utilizes the additive operation of matrices  and  to form a superposition transformation matrix, facilitating complex interactions between model parameters. This structure allows for detailed scaling and modulation of features, thereby providing enhanced expressiveness and flexibility to the model to address complex task requirements. Its forward propagation mathematical expression in the model is shown as 7  ###reference_###:\nwhere , . This approach allows for more complex interactions between  and , enhancing the model\u2019s expressiveness and flexibility, as illustrated in Figure 1  ###reference_### (D).\nThe structural schematic of the MTLoRA fine-tuning method, as shown in Figure 1  ###reference_###.\nThe MTLoRA method can be applied not only in Dense Layers based on the Transformer architecture model but also in any neural network structure with Dense Layers."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "This experiment aims to validate the effectiveness of the MTLoRA method in natural language processing tasks. Specifically, the experimental design includes a total of eleven tasks in two major categories: NLU and NLG. GPT-2 Medium (Radford et al., 2019  ###reference_b34###) and RoBERTa-base (Liu et al., 2019  ###reference_b22###) are selected as the base pre-trained models, upon which fine-tuning is performed using MTLoRA and LoRA techniques. To comprehensively evaluate the performance of the MTLoRA method, the experimental results of MTLoRA will be compared and analyzed against existing fine-tuning methods, including Full Fine-Tuning, Adaptive Tuning, Bitfit, and LoRA."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Baseline",
            "text": "This study selected recently representative fine-tuning methods for comparative validation to evaluate the performance of each method on specific tasks. Below is a detailed description of each fine-tuning method:\nFull Fine-Tuning involves updating all trainable parameters during the fine-tuning process of downstream tasks. Full fine-tuning typically achieves better performance but requires more GPU computational resources, larger storage space, and longer training time (Qiu et al., 2020  ###reference_b33###; Raffel et al., 2020  ###reference_b35###).\nFull Fine-Tuning is an addition-based fine-tuning method that introduces extra trainable extension structures within the blocks of a Large Pre-trained Language Model (LPLM). During the fine-tuning process, the original parameters of the LPLM are frozen, with only the trainable parameters in the newly added extension structures being updated. Compared to full-parameter fine-tuning, full fine-tuning significantly reduces the number of trainable parameters in the model and can achieve similar or even superior performance (Houlsby et al., 2019  ###reference_b18###; Rebuffi et al., 2017  ###reference_b37###; He et al., 2022  ###reference_b16###).\nBitFit is a method that specifies the updating of Bias parameters in LPLM while freezing the rest of the parameters. As described by Zaken et al. (2021  ###reference_b51###), BitFit significantly reduces the amount of parameter updates during the fine-tuning process and is able to maintain good performance.\nIn the replication experiments, the LoRA fine-tuning method maintained all hyperparameter configurations consistent with those described by Hu et al. (2022  ###reference_b19###).\nAda-LoRA is typically applied to all linear layer parameter matrices in LPLM (Zhang et al., 2023b  ###reference_b54###), but for effective comparison, we utilized the RoBERTa-base pre-trained model and applied the Ada-LoRA method to the query/value projection matrices  and , with experimental data derived from the Delta-LoRA (Zi et al., 2023  ###reference_b55###) experiments.\nDelta-LoRA uses the hyperparameter settings of LoRA as its baseline configuration to facilitate direct performance comparisons across various models (Zi et al., 2023  ###reference_b55###). In natural language understanding (NLU) tasks, Delta-LoRA reduces the input sequence length from 512 to 256 to decrease GPU memory demand and speed up the training process. Moreover, the batch size for different tasks is increased, for instance, from 16 to 120, with the update rate () set to 0.5. Apart from these changes, the remaining hyperparameter configurations are consistent with LoRA. For natural language generation (NLG) tasks, the update rate () is adjusted to 2. The Delta-LoRA method is applied to the query/value projection matrices  and  in LPLM."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Natural Language Understanding Tasks",
            "text": ""
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1 Base Model and Dataset",
            "text": "In this experiment, we employed RoBERTa-base as the foundational pre-trained model, which was introduced by  Liu et al. (2019  ###reference_b22###) through the Facebook AI Research Lab. RoBERTa is an advancement over the BERT pre-training model, designed to enhance the model\u2019s performance on downstream tasks. RoBERTa implemented several improvements over BERT, including: (1) training the model with larger batch sizes, over longer periods, and with more data; (2) removing the Next Sentence Prediction (NSP) from the optimization objectives; (3) employing dynamic masking techniques during training; and (4) utilizing longer sentences in the training corpus. Due to its superior performance and moderate parameter size, the RoBERTa pre-trained model is widely utilized in performance testing for various downstream tasks based on pre-trained models.\nTo validate the effectiveness of the MTLoRA fine-tuning method in natural language understanding tasks, we conducted empirical experiments using the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018  ###reference_b45###), which includes eight tasks, specifically:\nCoLA (Corpus of Linguistic Acceptability): Created by Warstadt et al. (2019  ###reference_b47###), this dataset comprises a series of English sentences sourced from books and journals on linguistic theory, aimed at assessing the grammatical acceptability of English sentences. The classification results are categorized as \"acceptable\" and \"unacceptable.\" CoLA contains 8,551 training samples, 1,043 validation samples, and 1,063 test samples. The Matthews correlation coefficient (Matthews, 1975  ###reference_b23###) is used as the performance evaluation metric, suitable for imbalanced binary classification tasks.\nMNLI (Multi-Genre Natural Language Inference): Introduced by Williams et al. (2017  ###reference_b49###), each sample consists of a premise sentence and a hypothesis sentence, with the primary task being to judge the textual entailment relationship between these two sentences. MNLI sentences cover ten genres, including transcripts of speeches, fiction, news reports, and government reports, requiring the model to handle texts of various styles, types, and domains. MNLI includes 392,702 training samples, 19,647 validation samples, and 19,647 test samples.\nMRPC (Microsoft Research Paraphrase Corpus): Proposed by Dolan and Brockett (2005  ###reference_b10###), the dataset contains sentence pairs automatically extracted from news sources, primarily to determine whether two sentences have a paraphrasing relationship. The distribution of these two categories is imbalanced, with about 68% being positive samples, where the semantic similarity of sentence pairs is manually annotated. MRPC includes 3,668 training samples, 408 validation samples, and 1,725 test samples.\nQNLI (Question Natural Language Inference): Provided by Wang et al. (2018  ###reference_b45###), samples are derived from the Stanford Question Answering Dataset (Rajpurkar et al., 2016  ###reference_b36###) and converted into a classification task. The main task is to determine whether a given statement sentence contains the answer to a given question sentence, with sentences collected from Wikipedia and questions manually written. QNLI contains 104,743 training sample pairs, 5,463 validation sample pairs, and 5,463 test sample pairs.\nQQP (Quora Question Pairs): Offered by the question-and-answer website Quora, the main task is to determine whether two question sentences are semantically similar. The distribution of these two categories is imbalanced, with about 63% being negative samples. QQP includes 363,846 training samples, 40,430 validation samples, and 390,965 test samples (Wang et al., 2018  ###reference_b45###).\nRTE (Recognizing Textual Entailment): Combines results published by multiple research organizations (Dagan et al., 2005  ###reference_b7###; Haim et al., 2006  ###reference_b15###; Giampiccolo et al., 2007  ###reference_b14###; Bentivogli et al., 2009  ###reference_b2###) between 2005 and 2011 during the RTE challenges. The sample data, composed of Wikipedia and news data, has a primary task similar to MNLI but on a smaller scale, including 2,490 training samples, 277 validation samples, and 3,000 test samples.\nSST-2 (Stanford Sentiment Treebank): Introduced by Socher et al. (2013  ###reference_b40###), the main task is to determine the sentiment orientation of a given sentence, whether positive or negative. The sample data mainly comes from movie review data, with reviews manually annotated for sentiment classification. SST-2 includes 67,349 training samples, 872 validation samples, and 1,821 test samples.\nSTS-B (Semantic Textual Similarity Benchmark): Proposed by Cer et al. (2017  ###reference_b4###), the main task is to calculate the semantic similarity score between two sentences. Samples are extracted from sentence pairs collected from videos, image captions, news headlines, etc., with sentence pair similarity manually rated on a scale from 1 to 5. Model performance is evaluated using the Pearson and Spearman correlation coefficients. STS-B includes 5,749 training samples, 1,500 validation samples, and 1,379 test samples."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2 Experimental Details",
            "text": "This experiment was conducted on the basis of the RoBERTa-base as the pre-trained model, applying both MTLoRA and LoRA methods across eight tasks on the GLUE benchmark. The following settings were employed during the training and testing process:\nAdamW was chosen as the optimizer for the model, along with a linear learning rate decay strategy;\nThe GPU processor used was NVIDIA A100-PCIE-40GB;\nThe hyperparameter configuration was consistent with the settings by Hu et al. (2022  ###reference_b19###), including learning rate, maximum sequence length (MaxSeqLength), rank () size, batch size (BatchSize), and epochs. Detailed hyperparameter settings can be found in Appendix A.1  ###reference_###;\nFor each task, the reported experimental results are based on the median and standard deviation obtained from training and testing with three different random seeds;\nThe MTLoRA method was applied to the query/value projection matrices (, ), intermediate layer projection matrix (), and output layer projection matrix (), while the LoRA method was applied to the query/value projection matrices (, )."
        },
        {
            "section_id": "4.2.3",
            "parent_section_id": "4.2",
            "section_name": "4.2.3 Experimental Results",
            "text": "In this experiment, we explored the impact of different  transformation matrix structures of the MTLoRA method on natural language understanding tasks. The test experiment results of the LoRA and MTLoRA methods based on the RoBERTa-base model across eight tasks of the GLUE benchmark are shown in Table 1  ###reference_###:\nWe use the results of LoRA (Hu et al., 2022  ###reference_b19###) as the primary benchmark for comparison. From the results in Table 1, it can be seen that MTLoRA achieves an average performance improvement of about 1% across all tasks and controls the standard deviation fluctuations well. Specifically:\nThe SHIM structure exhibits enhanced performance and stability across a majority of tasks, with observed performance gains of roughly 1.29% (=1.4%) on CoLA, 0.87% (=0.0%) on QQP, and 3.61% (=0.8%) on RTE tasks. This performance indicates its effectiveness and wide-ranging utility in a variety of general tasks.\nIn contrast, the ICFM structure shows robust performance across numerous tasks, particularly excelling in tasks related to semantic similarity and entailment when there is a lack of comprehensive corpus data. For example, it achieves an average performance boost of approximately 1.22% (=0.6%) on the MRPC task and 1.08% (=0.6%) on the RTE task.\nMeanwhile, the CTCM structure delivers consistent performance across a spectrum of tasks, standing out particularly in inference tasks with extensive corpus data. It demonstrates a performance increase of 1.0% (=0.6%) on the CoLA task, indicating a noteworthy reduction in standard deviation when compared to the SHIM structure.\nLastly, the DTSM structure continues to show steady performance in various tasks, particularly highlighting an improvement of around 0.88% (=0.0%) on the QQP task when there is ample corpus data, and a mean performance enhancement of about 1.54% (=0.1%) on the CoLA task. This significant decrease in standard deviation underscores the DTSM structure\u2019s efficiency in managing semantic similarity tasks with abundant corpora.\n###figure_2### ###figure_3### During the training process, the changes in loss for different transformation matrix structures of the MTLoRA model and the LoRA model across the eight tasks of the GLUE benchmark are illustrated in Figures 2  ###reference_### and 3  ###reference_###. From the figures, it can be observed that compared to LoRA, the MTLoRA method exhibits lower loss, faster convergence speed, and lower standard deviation of fluctuations, indicating that MTLoRA indeed can enhance model performance on multiple tasks.\nOverall, the different transformation matrix structures in the MTLoRA method may demonstrate superior performance on various tasks. Therefore, in practical applications, an appropriate transformation matrix structure should be selected."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Natural Language Generation Tasks",
            "text": ""
        },
        {
            "section_id": "4.3.1",
            "parent_section_id": "4.3",
            "section_name": "4.3.1 Base Model and Datasets",
            "text": "In this experiment, we employ the GPT-2 Medium model released by OpenAI in 2019 as the pre-trained base model. This model is widely used in various text generation tasks due to its strong natural language generation capabilities (Radford et al., 2019  ###reference_b34###). We conducted experiments on three broadly recognized datasets: the E2E NLG Challenge, WebNLG, and DART.\nThe E2E NLG Challenge dataset was introduced by Novikova et al. (2017  ###reference_b27###), primarily aiming to advance the research of end-to-end, data-to-text natural language generation systems. This dataset poses multiple challenges for the performance of natural language generation systems, including a large number of samples, complex syntactic structure variations, rich vocabulary, and diverse sentence structures. The sample content pertains to the restaurant domain, requiring the system to generate detailed and fluent sentences based on given structured information. The dataset contains approximately 42,000 training samples, 4,600 validation samples, and 4,600 test samples.\nThe WebNLG dataset was introduced by Gardent et al. (2017  ###reference_b13###), and its primary task is to convert structured RDF (Resource Description Framework) triple data into fluent natural language text, facilitating the training and performance evaluation of generation systems. WebNLG thoroughly examines the system\u2019s capabilities in micro-planning, which includes several sub-tasks such as sentence segmentation, lexicalization, referring expression generation, information aggregation, and surface realization. The dataset focuses on 14 categories within the DBpedia domain and specifically notes that 5 of these categories are not included in the training set but only appear in the test set, to assess the model\u2019s generalization ability on unseen data. The dataset comprises approximately 22K samples.\nThe DART dataset was introduced by Nan et al. (2020  ###reference_b26###), with the primary task of converting structured triple data into high-quality natural language text for training and performance evaluation of generation systems. DART is characterized by its large scale, broad domains, and a robust structured data ontology semantic representation framework. Unlike the E2E and WebNLG datasets, which utilize a flat slot-value ontology representation structure, DART employs a unique tree-shaped ontology semantic representation framework. This framework more effectively encodes the rich semantic dependencies between ontologies in structured data."
        },
        {
            "section_id": "4.3.2",
            "parent_section_id": "4.3",
            "section_name": "4.3.2 Experimental Details",
            "text": "This experiment conducts tests on the E2E NLG Challenge, WebNLG, and DART tasks using the MTLoRA and LoRA methods, with the GPT-2 Medium model serving as the pre-trained base. The following settings were adopted during the training and testing phases:\nThe AdamW optimizer was chosen for the model, along with a linear learning rate decay strategy.\nThe GPU used was the NVIDIA A100-PCIE-40GB.\nThe hyperparameter configuration was consistent with the settings of Hu et al. (2022  ###reference_b19###), including learning rate, BeamSize, rank () size, batch size (BatchSize), training cycles (Epoch), etc. Detailed hyperparameter settings can be found in Appendix A.2  ###reference_###.\nFor each task, the reported experimental results are the averages and standard deviations based on three sets of training and testing with different random seeds.\nThe MTLoRA method was applied to the weight matrices in the query/key/value projection matrices (, , ) and the MLP layer (), while the LoRA method was applied to the query/value projection matrices (, ).\nSince the query/key/value projection matrices (, , ) of the GPT-2 Medium model are generated in a merged manner, this experiment utilized the MergedLinear stepwise convolution structure to address this situation. MergedLinear implements stepwise convolution through a transformation matrix, . Specifically, first, the parameter matrix  uses the transformation matrix  as a convolution kernel to perform joint convolution across the , ,  channels, obtaining the transformed matrix  after the first convolution step. Matrix  can integrate the associated information within the  structure, beneficial for enhancing the model\u2019s representational capability. Subsequently, matrix  undergoes independent convolution on the , ,  channels using the parameter matrix  as a convolution kernel, resulting in the parameter increment matrix  after the second convolution step, where  <<  or . The structural diagram is illustrated in Figure 4  ###reference_###.\n###figure_4###"
        },
        {
            "section_id": "4.3.3",
            "parent_section_id": "4.3",
            "section_name": "4.3.3 Experimental results",
            "text": "In this experiment, we explored the impact of different  transformation matrix structures of the MTLoRA method on natural language generation tasks. The test experiment results of the LoRA and MTLoRA methods, based on the GPT-2 Medium pre-trained model on the E2E, DART, and WebNLG datasets, are presented in Table 2  ###reference_###:\nWe use the experimental results of LoRA (Hu et al., 2022  ###reference_b19###) as the primary benchmark for comparison. From the results in Table 2  ###reference_###, it is evident that the four transformation matrix structures of the MTLoRA fine-tuning method effectively enhance the model\u2019s performance on the DART and WebNLG tasks. Specifically:\nThe incorporation of the SHIM structure led to an enhancement in the model\u2019s efficacy on the DART and WebNLG tasks, with an improvement of approximately 0.83% (=0.0%) and 0.56% (=0.1%), respectively. Similarly, the implementation of the ICFM structure resulted in a performance increment of roughly 0.65% (=0.1%) for DART and 0.22% (=0.5%) for WebNLG. The application of the CTCM structure contributed to a performance uplift on the DART and WebNLG tasks by approximately 0.88% (=0.0%) and 0.27% (=0.2%), respectively. Lastly, the adoption of the DTSM structure facilitated a performance boost on the DART and WebNLG tasks by an estimated 0.95% (=0.1%) and 0.31% (=0.3%), respectively.\nThe experimental results of MTLoRA on natural language generation tasks once again demonstrate that different transformation matrix structures of the MTLoRA fine-tuning method may achieve optimal performance on different tasks. Particularly, when the query/key/value projection matrices (, , ) of the pre-trained model are generated jointly, the stepwise convolutional transformation design of MTLoRA can effectively integrate and share the associated information of these three components, better supporting downstream tasks."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Experimental Analysis",
            "text": "To further validate the effectiveness of the MTLoRA method, we adapted LoRA to the query/value projection matrices (, ), the intermediate layer projection matrix (), and the output layer projection matrix () of the RoBERTa-base model. The other experimental details remain consistent with the description in section 4.2.2  ###reference_.SSS2###. The experimental results on the GLUE benchmark for natural language understanding are shown in Table 3  ###reference_###:\nFrom the experimental results in Table 3  ###reference_###, LoRA shows a slight improvement in performance on the QNLI and MRPC tasks of the GLUE benchmark. However, there is a significant fluctuation in performance on the QQP task, with a standard deviation as high as 12.5%, and a 2% performance fluctuation on the RTE task. This indicates that merely increasing the amount of trainable parameters does not always lead to performance improvements and can sometimes result in performance degradation.\nPerformance of LoRA on E2E Tasks. To verify whether the MTLoRA method actually reduces the model\u2019s performance on the E2E task in NLG, we re-executed the performance test of LoRA on the E2E task using the same computational environment as MTLoRA. The experimental results show that LoRA achieves an average BLEU score of 69.52% on the E2E task, with a standard deviation of 0.5%. Its average BLEU score is comparable to that of MTLoRA, but with an increased standard deviation. This indicates that the experimental computational environment also has an impact on the results, suggesting that MTLoRA does not actually reduce the model\u2019s performance on the E2E task in NLG.\nSensitivity Analysis of Rank (). This study employs the MTLoRA and LoRA methods based on RoBERTa-base, conducting experiments on the COLA task with varying sizes of rank () to assess the specific impacts of rank () on model efficacy and stability. All experiments were carried out with the same random seed, and the specific settings of the experiments are consistent with those described in Section 4.2.2  ###reference_.SSS2###, with the corresponding results displayed in Figure 5  ###reference_###:\n###figure_5### Several observations can be made from the experimental results in Figure 5  ###reference_###: Firstly, the MTLoRA method continues to exhibit good performance and stability on the COLA task across different rank sizes without evident overfitting. Particularly, the ICFM structure in MTLoRA demonstrates exceptional performance and stability, indicating that the ICFM structure can effectively capture the task\u2019s key features, thereby efficiently preventing overfitting. Secondly, compared to the traditional LoRA, MTLoRA can achieve performance improvements with larger ranks while adding fewer parameters. Lastly, the effectiveness of MTLoRA is not solely due to the increased parameters at the same rank, as the performance curve within the examined range suggests that MTLoRA can outperform some higher-ranked LoRAs even at lower ranks."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion and Future Work",
            "text": "In this paper, we explored fine-tuning techniques based on LPLMs, which efficiently adjust a small number of parameters to significantly reduce the computational power and space requirements of the model while maintaining its performance on downstream tasks. Among these, the reparameterization-based fine-tuning technique represented by LoRA stands out, being widely applied to various task scenarios. However, LoRA and its improved methods still have shortcomings in complex task adaptability, performance, stability, and algorithmic complexity.\nTo mitigate these issues, we proposed MTLoRA. This method applies linear transformations to task-specific parameter matrices through the  transformation matrix, altering their spatial geometric structure to generate new matrix feature patterns. This mimics the fundamental impact of different geometric structural feature patterns in the brain on functions, thereby alleviating the aforementioned problems. Through a series of experimental validations, the MTLoRA method has improved performance in multiple task scenarios compared to the LoRA method while also reducing standard deviation. Notably, the MTLoRA method not only enhances performance but also maintains the simplicity of the algorithm, without increasing inference stage latency or reducing the length of the input sequence. Furthermore, the MTLoRA method is applicable not only to the Transformer architecture but also to other neural network structures.\nFuture research will focus on continuously optimizing the MTLoRA method by integrating more in-depth principles of brain neuroscience. It will be applied in the field of safe alignment for large models, ensuring that the models not only perform exceptionally in specific tasks but also possess robust security capabilities. This will contribute to the sustainable development of large model applications. Moreover, considering the effectiveness of the SHIM, ICFM, CTCM, and DTSM transformation matrix structures, we plan to study more brain-inspired transformation matrix structures to further enhance the model\u2019s performance, reduce the standard deviation, and improve the model\u2019s adaptability in various downstream tasks. MTLoRA can also serve as a fundamental component to strengthen the model\u2019s capabilities in handling long contexts and parameter quantization."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Acknowledgments",
            "text": "This work was supported by the National Science and Technology Major Project (Grant No. 2022ZD0116202)."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A The hyperparameter settings in the experiment",
            "text": ""
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T1\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T1.150\" style=\"width:433.6pt;height:2783.1pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(4.3pt,-27.7pt) scale(1.02030816712351,1.02030816712351) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T1.150.150\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T1.150.150.151.1\">\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T1.150.150.151.1.1\" style=\"width:96.7pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T1.150.150.151.1.1.1\">\n<span class=\"ltx_p\" id=\"S4.T1.150.150.151.1.1.1.1\"><span class=\"ltx_rule\" style=\"width:0.0pt;height:10.8pt;background:black;display:inline-block;\"></span><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.150.150.151.1.1.1.1.1\">Model&amp;Method (RoBERTa-base)</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.150.150.151.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.150.150.151.1.2.1\">MNLI</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.150.150.151.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.150.150.151.1.3.1\">SST-2</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.150.150.151.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.150.150.151.1.4.1\">CoLA</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.150.150.151.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.150.150.151.1.5.1\">QQP</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.150.150.151.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.150.150.151.1.6.1\">QNLI</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.150.150.151.1.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.150.150.151.1.7.1\">RTE</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.150.150.151.1.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.150.150.151.1.8.1\">MRPC</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S4.T1.150.150.151.1.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.150.150.151.1.9.1\">STS-B</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.150.150.151.1.10\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.150.150.151.1.10.1\">Avg.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T1.9.9.9\">\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T1.9.9.9.10\" style=\"width:96.7pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T1.9.9.9.10.1\">\n<span class=\"ltx_p\" id=\"S4.T1.9.9.9.10.1.1\"><span class=\"ltx_rule\" style=\"width:0.0pt;height:10.8pt;background:black;display:inline-block;\"></span>Full Fine-tuning</span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.1.1.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.2.2.2.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.3.3.3.3\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.4.4.4.4\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.5.5.5.5\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.6.6.6.6\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.7.7.7.7\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.8.8.8.8\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.9.9.9.9\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.18.18.18\">\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r\" id=\"S4.T1.18.18.18.10\" style=\"width:96.7pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T1.18.18.18.10.1\">\n<span class=\"ltx_p\" id=\"S4.T1.18.18.18.10.1.1\">BitFit</span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.10.10.10.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.11.11.11.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.12.12.12.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.13.13.13.4\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.14.14.14.5\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.15.15.15.6\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.16.16.16.7\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.17.17.17.8\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.18.18.18.9\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.27.27.27\">\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r\" id=\"S4.T1.27.27.27.10\" style=\"width:96.7pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T1.27.27.27.10.1\">\n<span class=\"ltx_p\" id=\"S4.T1.27.27.27.10.1.1\">Adapter</span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.19.19.19.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.20.20.20.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.21.21.21.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.22.22.22.4\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.23.23.23.5\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.24.24.24.6\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.25.25.25.7\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.26.26.26.8\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.27.27.27.9\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.36.36.36\">\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r\" id=\"S4.T1.36.36.36.10\" style=\"width:96.7pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T1.36.36.36.10.1\">\n<span class=\"ltx_p\" id=\"S4.T1.36.36.36.10.1.1\">Ada-LoRA(best)</span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.28.28.28.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.29.29.29.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.30.30.30.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.31.31.31.4\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.32.32.32.5\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.33.33.33.6\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.34.34.34.7\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.35.35.35.8\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.36.36.36.9\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.45.45.45\">\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r\" id=\"S4.T1.45.45.45.10\" style=\"width:96.7pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T1.45.45.45.10.1\">\n<span class=\"ltx_p\" id=\"S4.T1.45.45.45.10.1.1\">Delta-LoRA(best)</span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.37.37.37.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.38.38.38.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.39.39.39.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.40.40.40.4\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.41.41.41.5\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.42.42.42.6\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.43.43.43.7\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.44.44.44.8\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.45.45.45.9\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.54.54.54\">\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r\" id=\"S4.T1.54.54.54.10\" style=\"width:96.7pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T1.54.54.54.10.1\">\n<span class=\"ltx_p\" id=\"S4.T1.54.54.54.10.1.1\">LoRA(best)</span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.46.46.46.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.47.47.47.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.48.48.48.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.49.49.49.4\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.50.50.50.5\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.51.51.51.6\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.52.52.52.7\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.53.53.53.8\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.54.54.54.9\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.63.63.63\">\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r\" id=\"S4.T1.63.63.63.10\" style=\"width:96.7pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T1.63.63.63.10.1\">\n<span class=\"ltx_p\" id=\"S4.T1.63.63.63.10.1.1\">LoRA</span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.55.55.55.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.56.56.56.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.57.57.57.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.58.58.58.4\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.59.59.59.5\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.60.60.60.6\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.61.61.61.7\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.62.62.62.8\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.63.63.63.9\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.72.72.72\">\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r\" id=\"S4.T1.72.72.72.10\" style=\"width:96.7pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T1.72.72.72.10.1\">\n<span class=\"ltx_p\" id=\"S4.T1.72.72.72.10.1.1\">MTLoRA(best)</span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.64.64.64.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.65.65.65.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.66.66.66.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.67.67.67.4\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.68.68.68.5\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.69.69.69.6\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.70.70.70.7\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.71.71.71.8\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.72.72.72.9\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.81.81.81\">\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r\" id=\"S4.T1.81.81.81.10\" style=\"width:96.7pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T1.81.81.81.10.1\">\n<span class=\"ltx_p\" id=\"S4.T1.81.81.81.10.1.1\">MTLoRA</span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.73.73.73.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.74.74.74.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.75.75.75.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.76.76.76.4\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.77.77.77.5\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.78.78.78.6\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.79.79.79.7\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.80.80.80.8\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.81.81.81.9\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.90.90.90\">\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T1.90.90.90.10\" style=\"width:96.7pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T1.90.90.90.10.1\">\n<span class=\"ltx_p\" id=\"S4.T1.90.90.90.10.1.1\"><span class=\"ltx_rule\" style=\"width:0.0pt;height:10.8pt;background:black;display:inline-block;\"></span>MTLoRA-SHIM(best)</span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.82.82.82.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.83.83.83.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.84.84.84.3\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.85.85.85.4\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.86.86.86.5\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.87.87.87.6\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.88.88.88.7\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.89.89.89.8\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.90.90.90.9\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.99.99.99\">\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r\" id=\"S4.T1.99.99.99.10\" style=\"width:96.7pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T1.99.99.99.10.1\">\n<span class=\"ltx_p\" id=\"S4.T1.99.99.99.10.1.1\">MTLoRA-SHIM</span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.91.91.91.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.92.92.92.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.93.93.93.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.94.94.94.4\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.95.95.95.5\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.96.96.96.6\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.97.97.97.7\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.98.98.98.8\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.99.99.99.9\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.108.108.108\">\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r\" id=\"S4.T1.108.108.108.10\" style=\"width:96.7pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T1.108.108.108.10.1\">\n<span class=\"ltx_p\" id=\"S4.T1.108.108.108.10.1.1\">MTLoRA-ICFM(best)</span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.100.100.100.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.101.101.101.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.102.102.102.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.103.103.103.4\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.104.104.104.5\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.105.105.105.6\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.106.106.106.7\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.107.107.107.8\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.108.108.108.9\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.117.117.117\">\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r\" id=\"S4.T1.117.117.117.10\" style=\"width:96.7pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T1.117.117.117.10.1\">\n<span class=\"ltx_p\" id=\"S4.T1.117.117.117.10.1.1\">MTLoRA-ICFM</span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.109.109.109.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.110.110.110.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.111.111.111.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.112.112.112.4\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.113.113.113.5\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.114.114.114.6\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.115.115.115.7\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.116.116.116.8\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.117.117.117.9\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.123.123.123\">\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r\" id=\"S4.T1.123.123.123.7\" style=\"width:96.7pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T1.123.123.123.7.1\">\n<span class=\"ltx_p\" id=\"S4.T1.123.123.123.7.1.1\">MTLoRA-CTCM(best)</span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.118.118.118.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.119.119.119.2\"><span class=\"ltx_text ltx_markedasmath ltx_font_bold\" id=\"S4.T1.119.119.119.2.1\">95.41</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.120.120.120.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.121.121.121.4\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.122.122.122.5\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.123.123.123.6\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.123.123.123.8\">89.95</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.123.123.123.9\">91.38</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.123.123.123.10\">87.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.132.132.132\">\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r\" id=\"S4.T1.132.132.132.10\" style=\"width:96.7pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T1.132.132.132.10.1\">\n<span class=\"ltx_p\" id=\"S4.T1.132.132.132.10.1.1\">MTLoRA-CTCM</span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.124.124.124.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.125.125.125.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.126.126.126.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.127.127.127.4\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.128.128.128.5\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.129.129.129.6\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.130.130.130.7\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.131.131.131.8\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.132.132.132.9\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.141.141.141\">\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r\" id=\"S4.T1.141.141.141.10\" style=\"width:96.7pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T1.141.141.141.10.1\">\n<span class=\"ltx_p\" id=\"S4.T1.141.141.141.10.1.1\">MTLoRA-DTSM(best)</span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.133.133.133.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.134.134.134.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.135.135.135.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.136.136.136.4\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.137.137.137.5\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.138.138.138.6\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.139.139.139.7\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.140.140.140.8\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.141.141.141.9\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.150.150.150\">\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_b ltx_border_r\" id=\"S4.T1.150.150.150.10\" style=\"width:96.7pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T1.150.150.150.10.1\">\n<span class=\"ltx_p\" id=\"S4.T1.150.150.150.10.1.1\">MTLORA-DTSM</span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T1.142.142.142.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T1.143.143.143.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T1.144.144.144.3\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T1.145.145.145.4\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T1.146.146.146.5\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T1.147.147.147.6\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T1.148.148.148.7\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S4.T1.149.149.149.8\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T1.150.150.150.9\"></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>The table presents the experimental results of fine-tuning methods such as MTLoRA and LoRA based on the RoBERTa-base model across the eight datasets of the GLUE benchmark. Among these, MNLI and QNLI are measured by accuracy, CoLA by Matthew\u2019s correlation, and STS-B by Pearson correlation. For all these metrics, higher values represent better performance.</figcaption>\n</figure>",
            "capture": "Table 1: The table presents the experimental results of fine-tuning methods such as MTLoRA and LoRA based on the RoBERTa-base model across the eight datasets of the GLUE benchmark. Among these, MNLI and QNLI are measured by accuracy, CoLA by Matthew\u2019s correlation, and STS-B by Pearson correlation. For all these metrics, higher values represent better performance."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T2\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T2.48\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T2.48.49.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T2.48.49.1.1\">\n<span class=\"ltx_rule\" style=\"width:0.0pt;height:9.7pt;background:black;display:inline-block;\"></span><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.48.49.1.1.1\" style=\"font-size:90%;\">Model&amp;Method (GPT-2 Medium)</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.48.49.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.48.49.1.2.1\" style=\"font-size:90%;\">E2E(BLEU)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.48.49.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.48.49.1.3.1\" style=\"font-size:90%;\">DART(BLEU)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.48.49.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.48.49.1.4.1\" style=\"font-size:90%;\">WebNLG(BLEU-A)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T2.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T2.3.3.4\">\n<span class=\"ltx_rule\" style=\"width:0.0pt;height:9.7pt;background:black;display:inline-block;\"></span><span class=\"ltx_text\" id=\"S4.T2.3.3.4.1\" style=\"font-size:90%;\">Full Fine-Tune</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.2.2.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.3.3.3\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.6.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.6.6.4\"><span class=\"ltx_text\" id=\"S4.T2.6.6.4.1\" style=\"font-size:90%;\">Adapter</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.4.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.5.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.6.6.3\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.9.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.9.9.4\"><span class=\"ltx_text\" id=\"S4.T2.9.9.4.1\" style=\"font-size:90%;\">Prefix</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.7.7.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.8.8.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.9.9.3\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.12.12\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.12.12.4\"><span class=\"ltx_text\" id=\"S4.T2.12.12.4.1\" style=\"font-size:90%;\">Ada-LoRA (best)</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.10.10.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.11.11.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.3\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.15.15\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.15.15.4\"><span class=\"ltx_text\" id=\"S4.T2.15.15.4.1\" style=\"font-size:90%;\">Delta-LoRA(best)</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.13.13.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.14.14.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.15.15.3\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.18.18\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.18.18.4\"><span class=\"ltx_text\" id=\"S4.T2.18.18.4.1\" style=\"font-size:90%;\">LoRA</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.16.16.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.17.17.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.18.18.3\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.21.21\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.21.21.4\"><span class=\"ltx_text\" id=\"S4.T2.21.21.4.1\" style=\"font-size:90%;\">MTLoRA(best)</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.19.19.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.20.20.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.21.21.3\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.24.24\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.24.24.4\"><span class=\"ltx_text\" id=\"S4.T2.24.24.4.1\" style=\"font-size:90%;\">MTLoRA</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.22.22.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.23.23.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.24.24.3\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.27.27\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T2.27.27.4\">\n<span class=\"ltx_rule\" style=\"width:0.0pt;height:9.7pt;background:black;display:inline-block;\"></span><span class=\"ltx_text\" id=\"S4.T2.27.27.4.1\" style=\"font-size:90%;\">MTLoRA-SHIM(best)</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.25.25.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.26.26.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.27.27.3\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.30.30\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.30.30.4\"><span class=\"ltx_text\" id=\"S4.T2.30.30.4.1\" style=\"font-size:90%;\">MTLoRA-SHIM</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.28.28.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.29.29.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.30.30.3\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.33.33\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.33.33.4\"><span class=\"ltx_text\" id=\"S4.T2.33.33.4.1\" style=\"font-size:90%;\">MTLoRA-ICFM(best)</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.31.31.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.32.32.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.33.33.3\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.36.36\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.36.36.4\"><span class=\"ltx_text\" id=\"S4.T2.36.36.4.1\" style=\"font-size:90%;\">MTLoRA-ICFM</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.34.34.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.35.35.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.36.36.3\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.39.39\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.39.39.4\"><span class=\"ltx_text\" id=\"S4.T2.39.39.4.1\" style=\"font-size:90%;\">MTLoRA-CTCM(best)</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.37.37.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.38.38.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.39.39.3\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.42.42\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.42.42.4\"><span class=\"ltx_text\" id=\"S4.T2.42.42.4.1\" style=\"font-size:90%;\">MTLoRA-CTCM</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.40.40.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.41.41.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.42.42.3\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.45.45\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.45.45.4\"><span class=\"ltx_text\" id=\"S4.T2.45.45.4.1\" style=\"font-size:90%;\">MTLoRA-DTSM(best)</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.43.43.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.44.44.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.45.45.3\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.48.48\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r\" id=\"S4.T2.48.48.4\"><span class=\"ltx_text\" id=\"S4.T2.48.48.4.1\" style=\"font-size:90%;\">MTLoRA-DTSM</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T2.46.46.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T2.47.47.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T2.48.48.3\"></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>The table displays the experimental results of fine-tuning methods such as MTLoRA and LoRA based on the GPT-2 Medium model on the E2E, DART, and WebNLG datasets. All tasks use the BLEU metric for evaluation, with higher values indicating better performance.</figcaption>\n</figure>",
            "capture": "Table 2: The table displays the experimental results of fine-tuning methods such as MTLoRA and LoRA based on the GPT-2 Medium model on the E2E, DART, and WebNLG datasets. All tasks use the BLEU metric for evaluation, with higher values indicating better performance."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T3\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T3.45\" style=\"width:433.6pt;height:835.8pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.4pt,-0.7pt) scale(1.00176500338361,1.00176500338361) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T3.45.45\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T3.45.45.46.1\">\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T3.45.45.46.1.1\" style=\"width:85.4pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.45.45.46.1.1.1\">\n<span class=\"ltx_p\" id=\"S4.T3.45.45.46.1.1.1.1\"><span class=\"ltx_rule\" style=\"width:0.0pt;height:10.8pt;background:black;display:inline-block;\"></span><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.45.45.46.1.1.1.1.1\">Model&amp;Method (roberta-base)</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.45.45.46.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.45.45.46.1.2.1\">MNLI</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.45.45.46.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.45.45.46.1.3.1\">SST-2</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.45.45.46.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.45.45.46.1.4.1\">CoLA</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.45.45.46.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.45.45.46.1.5.1\">QQP</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.45.45.46.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.45.45.46.1.6.1\">QNLI</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.45.45.46.1.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.45.45.46.1.7.1\">RTE</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.45.45.46.1.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.45.45.46.1.8.1\">MRPC</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S4.T3.45.45.46.1.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.45.45.46.1.9.1\">STS-B</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.45.45.46.1.10\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.45.45.46.1.10.1\">Avg.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T3.9.9.9\">\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T3.9.9.9.10\" style=\"width:85.4pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.9.9.9.10.1\">\n<span class=\"ltx_p\" id=\"S4.T3.9.9.9.10.1.1\"><span class=\"ltx_rule\" style=\"width:0.0pt;height:10.8pt;background:black;display:inline-block;\"></span>LoRA*(best)</span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.1.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.2.2.2.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.3.3.3.3\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.4.4.4.4\">\n1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.5.5.5.5\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.6.6.6.6\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.7.7.7.7\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.8.8.8.8\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.9.9.9.9\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.18.18.18\">\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.18.18.18.10\" style=\"width:85.4pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.18.18.18.10.1\">\n<span class=\"ltx_p\" id=\"S4.T3.18.18.18.10.1.1\">LoRA*</span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.10.10.10.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.11.11.11.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.12.12.12.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.13.13.13.4\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.14.14.14.5\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.15.15.15.6\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.16.16.16.7\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.17.17.17.8\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.18.18.18.9\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.27.27.27\">\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.27.27.27.10\" style=\"width:85.4pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.27.27.27.10.1\">\n<span class=\"ltx_p\" id=\"S4.T3.27.27.27.10.1.1\">LoRA</span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.19.19.19.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.20.20.20.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.21.21.21.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.22.22.22.4\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.23.23.23.5\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.24.24.24.6\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.25.25.25.7\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.26.26.26.8\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.27.27.27.9\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.36.36.36\">\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T3.36.36.36.10\" style=\"width:85.4pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.36.36.36.10.1\">\n<span class=\"ltx_p\" id=\"S4.T3.36.36.36.10.1.1\"><span class=\"ltx_rule\" style=\"width:0.0pt;height:10.8pt;background:black;display:inline-block;\"></span>LoRA(best)</span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.28.28.28.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.29.29.29.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.30.30.30.3\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.31.31.31.4\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.32.32.32.5\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.33.33.33.6\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.34.34.34.7\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.35.35.35.8\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.36.36.36.9\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.45.45.45\">\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_b ltx_border_r\" id=\"S4.T3.45.45.45.10\" style=\"width:85.4pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.45.45.45.10.1\">\n<span class=\"ltx_p\" id=\"S4.T3.45.45.45.10.1.1\">LoRA(least)</span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T3.37.37.37.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T3.38.38.38.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T3.39.39.39.3\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T3.40.40.40.4\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T3.41.41.41.5\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T3.42.42.42.6\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T3.43.43.43.7\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S4.T3.44.44.44.8\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T3.45.45.45.9\"></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>The table shows the experimental results of adapting LoRA to the query/value projection matrices (, ), the intermediate layer projection matrix (), and the output layer projection matrix () of the RoBERTa-base model on the GLUE benchmark for natural language understanding.  indicates that the data are sourced from Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07440v3#S4.T1\" title=\"Table 1 \u2023 4.2.3 Experimental Results \u2023 4.2 Natural Language Understanding Tasks \u2023 4 Experiments \u2023 Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A Brain-Inspired Method for Parameter-Efficient Fine-Tuning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</figcaption>\n</figure>",
            "capture": "Table 3: The table shows the experimental results of adapting LoRA to the query/value projection matrices (, ), the intermediate layer projection matrix (), and the output layer projection matrix () of the RoBERTa-base model on the GLUE benchmark for natural language understanding.  indicates that the data are sourced from Table 1."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T4\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"A1.T4.10\" style=\"width:433.6pt;height:138.1pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-9.3pt,3.0pt) scale(0.958981910098323,0.958981910098323) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"A1.T4.10.10\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A1.T4.10.10.11.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A1.T4.10.10.11.1.1\">\n<span class=\"ltx_rule\" style=\"width:0.0pt;height:10.8pt;background:black;display:inline-block;\"></span><span class=\"ltx_text ltx_font_bold\" id=\"A1.T4.10.10.11.1.1.1\">Hyper-Parameter&amp;Dataset</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T4.10.10.11.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T4.10.10.11.1.2.1\">MNLI</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T4.10.10.11.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T4.10.10.11.1.3.1\">SST-2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T4.10.10.11.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T4.10.10.11.1.4.1\">CoLA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T4.10.10.11.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T4.10.10.11.1.5.1\">QQP</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T4.10.10.11.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T4.10.10.11.1.6.1\">QNLI</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T4.10.10.11.1.7\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T4.10.10.11.1.7.1\">RTE</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T4.10.10.11.1.8\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T4.10.10.11.1.8.1\">MRPC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T4.10.10.11.1.9\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T4.10.10.11.1.9.1\">STS-B</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.10.10.12.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A1.T4.10.10.12.2.1\">\n<span class=\"ltx_rule\" style=\"width:0.0pt;height:10.8pt;background:black;display:inline-block;\"></span>Batch Size</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T4.10.10.12.2.2\">16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T4.10.10.12.2.3\">16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T4.10.10.12.2.4\">32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T4.10.10.12.2.5\">16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T4.10.10.12.2.6\">32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T4.10.10.12.2.7\">32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T4.10.10.12.2.8\">16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T4.10.10.12.2.9\">16</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.10.10.13.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T4.10.10.13.3.1\">Number of Epochs</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.10.10.13.3.2\">30</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.10.10.13.3.3\">60</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.10.10.13.3.4\">80</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.10.10.13.3.5\">25</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.10.10.13.3.6\">25</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.10.10.13.3.7\">80</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.10.10.13.3.8\">30</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.10.10.13.3.9\">40</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.8.8.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T4.8.8.8.9\">Learning Rate</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.1.1.1.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.2.2.2.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.3.3.3.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.4.4.4.4\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.5.5.5.5\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.6.6.6.6\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.7.7.7.7\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.8.8.8.8\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.10.10.14.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T4.10.10.14.4.1\">Warmup Ratio</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.10.10.14.4.2\">0.06</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.10.10.14.4.3\">0.06</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.10.10.14.4.4\">0.06</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.10.10.14.4.5\">0.06</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.10.10.14.4.6\">0.06</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.10.10.14.4.7\">0.06</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.10.10.14.4.8\">0.06</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.10.10.14.4.9\">0.06</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.9.9.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T4.9.9.9.1\">Rank \n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.9.9.9.2\">8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.9.9.9.3\">8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.9.9.9.4\">8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.9.9.9.5\">8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.9.9.9.6\">8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.9.9.9.7\">8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.9.9.9.8\">8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.9.9.9.9\">8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.10.10.10\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T4.10.10.10.1\">Alpha \n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.10.10.10.2\">16</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.10.10.10.3\">16</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.10.10.10.4\">16</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.10.10.10.5\">16</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.10.10.10.6\">16</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.10.10.10.7\">16</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.10.10.10.8\">16</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.10.10.10.9\">16</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.10.10.15.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\" id=\"A1.T4.10.10.15.5.1\">Max Sequence Length</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"A1.T4.10.10.15.5.2\">512</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"A1.T4.10.10.15.5.3\">512</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"A1.T4.10.10.15.5.4\">512</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"A1.T4.10.10.15.5.5\">512</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"A1.T4.10.10.15.5.6\">512</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"A1.T4.10.10.15.5.7\">512</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"A1.T4.10.10.15.5.8\">512</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"A1.T4.10.10.15.5.9\">512</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>The table provides the hyperparameter configurations for the experiments on the 8 datasets of the GLUE benchmark using the MTLoRA and LoRA methods, based on the RoBERTa-base model.</figcaption>\n</figure>",
            "capture": "Table 4: The table provides the hyperparameter configurations for the experiments on the 8 datasets of the GLUE benchmark using the MTLoRA and LoRA methods, based on the RoBERTa-base model."
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T5\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"A1.T5.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A1.T5.1.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A1.T5.1.2.1.1\">\n<span class=\"ltx_rule\" style=\"width:0.0pt;height:9.7pt;background:black;display:inline-block;\"></span><span class=\"ltx_text ltx_font_bold\" id=\"A1.T5.1.2.1.1.1\" style=\"font-size:90%;\">Hyper-Parameter&amp;Dataset</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.1.2.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T5.1.2.1.2.1\" style=\"font-size:90%;\">E2E</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.1.2.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T5.1.2.1.3.1\" style=\"font-size:90%;\">DART</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.1.2.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T5.1.2.1.4.1\" style=\"font-size:90%;\">WebNLG</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.1.3.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A1.T5.1.3.2.1\"><span class=\"ltx_text\" id=\"A1.T5.1.3.2.1.1\" style=\"font-size:90%;\">Batch Size</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.1.3.2.2\"><span class=\"ltx_text\" id=\"A1.T5.1.3.2.2.1\" style=\"font-size:90%;\">8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.1.3.2.3\"><span class=\"ltx_text\" id=\"A1.T5.1.3.2.3.1\" style=\"font-size:90%;\">8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.1.3.2.4\"><span class=\"ltx_text\" id=\"A1.T5.1.3.2.4.1\" style=\"font-size:90%;\">8</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.1.4.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.1.4.3.1\"><span class=\"ltx_text\" id=\"A1.T5.1.4.3.1.1\" style=\"font-size:90%;\">Learning Rate</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T5.1.4.3.2\"><span class=\"ltx_text\" id=\"A1.T5.1.4.3.2.1\" style=\"font-size:90%;\">0.0002</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T5.1.4.3.3\"><span class=\"ltx_text\" id=\"A1.T5.1.4.3.3.1\" style=\"font-size:90%;\">0.0002</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T5.1.4.3.4\"><span class=\"ltx_text\" id=\"A1.T5.1.4.3.4.1\" style=\"font-size:90%;\">0.0002</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.1.5.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.1.5.4.1\"><span class=\"ltx_text\" id=\"A1.T5.1.5.4.1.1\" style=\"font-size:90%;\">Number of Epochs</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T5.1.5.4.2\"><span class=\"ltx_text\" id=\"A1.T5.1.5.4.2.1\" style=\"font-size:90%;\">5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T5.1.5.4.3\"><span class=\"ltx_text\" id=\"A1.T5.1.5.4.3.1\" style=\"font-size:90%;\">5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T5.1.5.4.4\"><span class=\"ltx_text\" id=\"A1.T5.1.5.4.4.1\" style=\"font-size:90%;\">5</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.1.6.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.1.6.5.1\"><span class=\"ltx_text\" id=\"A1.T5.1.6.5.1.1\" style=\"font-size:90%;\">Weight Decay</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T5.1.6.5.2\"><span class=\"ltx_text\" id=\"A1.T5.1.6.5.2.1\" style=\"font-size:90%;\">0.01</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T5.1.6.5.3\"><span class=\"ltx_text\" id=\"A1.T5.1.6.5.3.1\" style=\"font-size:90%;\">0.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T5.1.6.5.4\"><span class=\"ltx_text\" id=\"A1.T5.1.6.5.4.1\" style=\"font-size:90%;\">0.01</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.1.7.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.1.7.6.1\"><span class=\"ltx_text\" id=\"A1.T5.1.7.6.1.1\" style=\"font-size:90%;\">Warmup Steps</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T5.1.7.6.2\"><span class=\"ltx_text\" id=\"A1.T5.1.7.6.2.1\" style=\"font-size:90%;\">500</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T5.1.7.6.3\"><span class=\"ltx_text\" id=\"A1.T5.1.7.6.3.1\" style=\"font-size:90%;\">500</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T5.1.7.6.4\"><span class=\"ltx_text\" id=\"A1.T5.1.7.6.4.1\" style=\"font-size:90%;\">500</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.1.8.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.1.8.7.1\"><span class=\"ltx_text\" id=\"A1.T5.1.8.7.1.1\" style=\"font-size:90%;\">Dropout Prob</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T5.1.8.7.2\"><span class=\"ltx_text\" id=\"A1.T5.1.8.7.2.1\" style=\"font-size:90%;\">0.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T5.1.8.7.3\"><span class=\"ltx_text\" id=\"A1.T5.1.8.7.3.1\" style=\"font-size:90%;\">0.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T5.1.8.7.4\"><span class=\"ltx_text\" id=\"A1.T5.1.8.7.4.1\" style=\"font-size:90%;\">0.1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.1.9.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.1.9.8.1\"><span class=\"ltx_text\" id=\"A1.T5.1.9.8.1.1\" style=\"font-size:90%;\">Label Smooth</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T5.1.9.8.2\"><span class=\"ltx_text\" id=\"A1.T5.1.9.8.2.1\" style=\"font-size:90%;\">0.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T5.1.9.8.3\"><span class=\"ltx_text\" id=\"A1.T5.1.9.8.3.1\" style=\"font-size:90%;\">0.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T5.1.9.8.4\"><span class=\"ltx_text\" id=\"A1.T5.1.9.8.4.1\" style=\"font-size:90%;\">0.1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.1.10.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T5.1.10.9.1\"><span class=\"ltx_text\" id=\"A1.T5.1.10.9.1.1\" style=\"font-size:90%;\">Rank r</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T5.1.10.9.2\"><span class=\"ltx_text\" id=\"A1.T5.1.10.9.2.1\" style=\"font-size:90%;\">4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T5.1.10.9.3\"><span class=\"ltx_text\" id=\"A1.T5.1.10.9.3.1\" style=\"font-size:90%;\">4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T5.1.10.9.4\"><span class=\"ltx_text\" id=\"A1.T5.1.10.9.4.1\" style=\"font-size:90%;\">4</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\" id=\"A1.T5.1.1.1\">\n<span class=\"ltx_text\" id=\"A1.T5.1.1.1.1\" style=\"font-size:90%;\">Alpha </span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"A1.T5.1.1.2\"><span class=\"ltx_text\" id=\"A1.T5.1.1.2.1\" style=\"font-size:90%;\">32</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"A1.T5.1.1.3\"><span class=\"ltx_text\" id=\"A1.T5.1.1.3.1\" style=\"font-size:90%;\">32</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"A1.T5.1.1.4\"><span class=\"ltx_text\" id=\"A1.T5.1.1.4.1\" style=\"font-size:90%;\">32</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>The table shows the hyperparameter configurations for the MTLoRA and LoRA methods based on the GPT-2 Medium model during the training process, in experiments on the E2E, DART, and WebNLG datasets.</figcaption>\n</figure>",
            "capture": "Table 5: The table shows the hyperparameter configurations for the MTLoRA and LoRA methods based on the GPT-2 Medium model during the training process, in experiments on the E2E, DART, and WebNLG datasets."
        },
        "6": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T6\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A1.T6.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A1.T6.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"A1.T6.1.1.1.1\">\n<span class=\"ltx_rule\" style=\"width:0.0pt;height:9.7pt;background:black;display:inline-block;\"></span><span class=\"ltx_text ltx_font_bold\" id=\"A1.T6.1.1.1.1.1\" style=\"font-size:90%;\">Hyper-Parameter&amp;Dataset</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A1.T6.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T6.1.1.1.2.1\" style=\"font-size:90%;\">E2E</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A1.T6.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T6.1.1.1.3.1\" style=\"font-size:90%;\">DART</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A1.T6.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T6.1.1.1.4.1\" style=\"font-size:90%;\">WebNLG</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A1.T6.1.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A1.T6.1.2.1.1\"><span class=\"ltx_text\" id=\"A1.T6.1.2.1.1.1\" style=\"font-size:90%;\">Beam Size</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T6.1.2.1.2\"><span class=\"ltx_text\" id=\"A1.T6.1.2.1.2.1\" style=\"font-size:90%;\">10</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T6.1.2.1.3\"><span class=\"ltx_text\" id=\"A1.T6.1.2.1.3.1\" style=\"font-size:90%;\">10</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T6.1.2.1.4\"><span class=\"ltx_text\" id=\"A1.T6.1.2.1.4.1\" style=\"font-size:90%;\">10</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T6.1.3.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T6.1.3.2.1\"><span class=\"ltx_text\" id=\"A1.T6.1.3.2.1.1\" style=\"font-size:90%;\">Length Penalty</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T6.1.3.2.2\"><span class=\"ltx_text\" id=\"A1.T6.1.3.2.2.1\" style=\"font-size:90%;\">0.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T6.1.3.2.3\"><span class=\"ltx_text\" id=\"A1.T6.1.3.2.3.1\" style=\"font-size:90%;\">0.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T6.1.3.2.4\"><span class=\"ltx_text\" id=\"A1.T6.1.3.2.4.1\" style=\"font-size:90%;\">0.8</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T6.1.4.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\" id=\"A1.T6.1.4.3.1\"><span class=\"ltx_text\" id=\"A1.T6.1.4.3.1.1\" style=\"font-size:90%;\">No Repeat Ngram Size</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"A1.T6.1.4.3.2\"><span class=\"ltx_text\" id=\"A1.T6.1.4.3.2.1\" style=\"font-size:90%;\">4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"A1.T6.1.4.3.3\"><span class=\"ltx_text\" id=\"A1.T6.1.4.3.3.1\" style=\"font-size:90%;\">4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"A1.T6.1.4.3.4\"><span class=\"ltx_text\" id=\"A1.T6.1.4.3.4.1\" style=\"font-size:90%;\">4</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 6: </span>The table displays the hyperparameter configurations for the MTLoRA and LoRA methods based on the GPT-2 Medium model during the inference process, in experiments on the E2E, DART, and WebNLG datasets.</figcaption>\n</figure>",
            "capture": "Table 6: The table displays the hyperparameter configurations for the MTLoRA and LoRA methods based on the GPT-2 Medium model during the inference process, in experiments on the E2E, DART, and WebNLG datasets."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.07440v3_figure_1.png",
            "caption": "Figure 1: Structure of the MTLoRA fine-tuning method."
        },
        "2": {
            "figure_path": "2403.07440v3_figure_2.png",
            "caption": "Figure 2: The changes in loss for different transformation matrix structures of the MTLoRA model and the LoRA model on the CoLA, MNLI, MRPC, and QNLI tasks."
        },
        "3": {
            "figure_path": "2403.07440v3_figure_3.png",
            "caption": "Figure 3: The changes in loss for different transformation matrix structures of the MTLoRA model and the LoRA model on the QQP, RTE, SST-2, and STS-B tasks."
        },
        "4": {
            "figure_path": "2403.07440v3_figure_4.png",
            "caption": "Figure 4: Stepwise convolution structure diagram of the MTLoRA fine-tuning method."
        },
        "5": {
            "figure_path": "2403.07440v3_figure_5.png",
            "caption": "Figure 5: The MTLoRA and LoRA methods, based on the RoBERTa-base model, were experimentally evaluated on the COLA task using different rank (r\ud835\udc5fritalic_r) sizes. A higher Matthew\u2019s correlation value indicates better model performance."
        }
    },
    "references": [
        {
            "1": {
                "title": "A general language assistant as a laboratory for alignment.",
                "author": "Askell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D., Henighan, T., et al. (2021).",
                "venue": "arXiv preprint arXiv:2112.00861",
                "url": null
            }
        },
        {
            "2": {
                "title": "The fifth pascal recognizing textual entailment challenge.",
                "author": "Bentivogli, L., Clark, P., Dagan, I., and Giampiccolo, D. (2009).",
                "venue": "TAC 7, 8",
                "url": null
            }
        },
        {
            "3": {
                "title": "Language models are few-shot learners.",
                "author": "Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., et al. (2020).",
                "venue": "Advances in neural information processing systems 33, 1877\u20131901",
                "url": null
            }
        },
        {
            "4": {
                "title": "Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation.",
                "author": "Cer, D., Diab, M., Agirre, E., Lopez-Gazpio, I., and Specia, L. (2017).",
                "venue": "arXiv preprint arXiv:1708.00055",
                "url": null
            }
        },
        {
            "5": {
                "title": "Eigenvalues in Riemannian geometry (Academic press)",
                "author": "Chavel, I. (1984).",
                "venue": null,
                "url": null
            }
        },
        {
            "6": {
                "title": "Longlora: Efficient fine-tuning of long-context large language models.",
                "author": "Chen, Y., Qian, S., Tang, H., Lai, X., Liu, Z., Han, S., et al. (2023).",
                "venue": "arXiv preprint arXiv:2309.12307",
                "url": null
            }
        },
        {
            "7": {
                "title": "The pascal recognising textual entailment challenge.",
                "author": "Dagan, I., Glickman, O., and Magnini, B. (2005).",
                "venue": "In Machine learning challenges workshop (Springer), 177\u2013190",
                "url": null
            }
        },
        {
            "8": {
                "title": "Qlora: Efficient finetuning of quantized llms.",
                "author": "Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L. (2024).",
                "venue": "Advances in Neural Information Processing Systems 36",
                "url": null
            }
        },
        {
            "9": {
                "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "author": "[Dataset] Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019).",
                "venue": null,
                "url": null
            }
        },
        {
            "10": {
                "title": "Automatically constructing a corpus of sentential paraphrases.",
                "author": "Dolan, B. and Brockett, C. (2005).",
                "venue": "In Third International Workshop on Paraphrasing (IWP2005)",
                "url": null
            }
        },
        {
            "11": {
                "title": "Glm: General language model pretraining with autoregressive blank infilling.",
                "author": "Du, Z., Qian, Y., Liu, X., Ding, M., Qiu, J., Yang, Z., et al. (2022).",
                "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 320\u2013335",
                "url": null
            }
        },
        {
            "12": {
                "title": "High-resolution intersubject averaging and a coordinate system for the cortical surface.",
                "author": "Fischl, B., Sereno, M. I., Tootell, R. B., and Dale, A. M. (1999).",
                "venue": "Human brain mapping 8, 272\u2013284",
                "url": null
            }
        },
        {
            "13": {
                "title": "The webnlg challenge: Generating text from rdf data.",
                "author": "Gardent, C., Shimorina, A., Narayan, S., and Perez-Beltrachini, L. (2017).",
                "venue": "In Proceedings of the 10th International Conference on Natural Language Generation. 124\u2013133",
                "url": null
            }
        },
        {
            "14": {
                "title": "The third pascal recognizing textual entailment challenge.",
                "author": "Giampiccolo, D., Magnini, B., Dagan, I., and Dolan, W. B. (2007).",
                "venue": "In Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing. 1\u20139",
                "url": null
            }
        },
        {
            "15": {
                "title": "The second pascal recognising textual entailment challenge.",
                "author": "Haim, R. B., Dagan, I., Dolan, B., Ferro, L., Giampiccolo, D., Magnini, B., et al. (2006).",
                "venue": "In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment. vol. 7, 785\u2013794",
                "url": null
            }
        },
        {
            "16": {
                "title": "Towards a unified view of parameter-efficient transfer learning.",
                "author": "He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T., and Neubig, G. (2022).",
                "venue": "In International Conference on Learning Representations",
                "url": null
            }
        },
        {
            "17": {
                "title": "Deberta: Decoding-enhanced bert with disentangled attention.",
                "author": "He, P., Liu, X., Gao, J., and Chen, W. (2020).",
                "venue": "arXiv preprint arXiv:2006.03654",
                "url": null
            }
        },
        {
            "18": {
                "title": "Parameter-efficient transfer learning for nlp.",
                "author": "Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., et al. (2019).",
                "venue": "In International Conference on Machine Learning (PMLR), 2790\u20132799",
                "url": null
            }
        },
        {
            "19": {
                "title": "LoRA: Low-rank adaptation of large language models.",
                "author": "Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., et al. (2022).",
                "venue": "In International Conference on Learning Representations",
                "url": null
            }
        },
        {
            "20": {
                "title": "Laplace-beltrami eigenfunctions towards an algorithm that\" understands\" geometry.",
                "author": "L\u00e9vy, B. (2006).",
                "venue": "In IEEE International Conference on Shape Modeling and Applications 2006 (SMI\u201906) (IEEE), 13\u201313",
                "url": null
            }
        },
        {
            "21": {
                "title": "Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning.",
                "author": "Liu, H., Tam, D., Muqeeth, M., Mohta, J., Huang, T., Bansal, M., et al. (2022).",
                "venue": "Advances in Neural Information Processing Systems 35, 1950\u20131965",
                "url": null
            }
        },
        {
            "22": {
                "title": "Roberta: A robustly optimized bert pretraining approach.",
                "author": "Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., et al. (2019).",
                "venue": "arXiv preprint arXiv:1907.11692",
                "url": null
            }
        },
        {
            "23": {
                "title": "Comparison of the predicted and observed secondary structure of t4 phage lysozyme.",
                "author": "Matthews, B. W. (1975).",
                "venue": "Biochimica et Biophysica Acta (BBA)-Protein Structure 405, 442\u2013451",
                "url": null
            }
        },
        {
            "24": {
                "title": "Electromagnetic processes in dispersive media (Cambridge University Press)",
                "author": "Melrose, D. B. and McPhedran, R. C. (1991).",
                "venue": null,
                "url": null
            }
        },
        {
            "25": {
                "title": "Metaicl: Learning to learn in context.",
                "author": "Min, S., Lewis, M., Zettlemoyer, L., and Hajishirzi, H. (2021).",
                "venue": "arXiv preprint arXiv:2110.15943",
                "url": null
            }
        },
        {
            "26": {
                "title": "Dart: Open-domain structured data record to text generation.",
                "author": "Nan, L., Radev, D., Zhang, R., Rau, A., Sivaprasad, A., Hsieh, C., et al. (2020).",
                "venue": "arXiv preprint arXiv:2007.02871",
                "url": null
            }
        },
        {
            "27": {
                "title": "The e2e dataset: New challenges for end-to-end generation.",
                "author": "Novikova, J., Du\u0161ek, O., and Rieser, V. (2017).",
                "venue": "arXiv preprint arXiv:1706.09254",
                "url": null
            }
        },
        {
            "28": {
                "title": "Neocortical dynamics and human eeg rhythms.",
                "author": "Nowack, W. J. (1995).",
                "venue": "Neurology 45, 1793\u20131793",
                "url": null
            }
        },
        {
            "29": {
                "title": "Is the brain macroscopically linear? a system identification of resting state dynamics.",
                "author": "Nozari, E., Bertolero, M. A., Stiso, J., Caciagli, L., Cornblath, E. J., He, X., et al. (2020).",
                "venue": "arXiv preprint arXiv:2012.12351",
                "url": null
            }
        },
        {
            "30": {
                "title": "Training language models to follow instructions with human feedback.",
                "author": "Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., et al. (2022).",
                "venue": "Advances in Neural Information Processing Systems 35, 27730\u201327744",
                "url": null
            }
        },
        {
            "31": {
                "title": "Geometric constraints on human brain function.",
                "author": "Pang, J. C., Aquino, K. M., Oldehinkel, M., Robinson, P. A., Fulcher, B. D., Breakspear, M., et al. (2023).",
                "venue": "Nature , 1\u20139",
                "url": null
            }
        },
        {
            "32": {
                "title": "Deep contextualized word representations.",
                "author": "Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., et al. (2018).",
                "venue": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), eds. M. Walker, H. Ji, and A. Stent (New Orleans, Louisiana: Association for Computational Linguistics), 2227\u20132237.",
                "url": null
            }
        },
        {
            "33": {
                "title": "Pre-trained models for natural language processing: A survey.",
                "author": "Qiu, X., Sun, T., Xu, Y., Shao, Y., Dai, N., and Huang, X. (2020).",
                "venue": "Science China Technological Sciences 63, 1872\u20131897",
                "url": null
            }
        },
        {
            "34": {
                "title": "Language models are unsupervised multitask learners.",
                "author": "Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. (2019).",
                "venue": "OpenAI blog 1, 9",
                "url": null
            }
        },
        {
            "35": {
                "title": "Exploring the limits of transfer learning with a unified text-to-text transformer.",
                "author": "Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., et al. (2020).",
                "venue": "The Journal of Machine Learning Research 21, 5485\u20135551",
                "url": null
            }
        },
        {
            "36": {
                "title": "Squad: 100,000+ questions for machine comprehension of text.",
                "author": "Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. (2016).",
                "venue": "arXiv preprint arXiv:1606.05250",
                "url": null
            }
        },
        {
            "37": {
                "title": "Learning multiple visual domains with residual adapters.",
                "author": "Rebuffi, S.-A., Bilen, H., and Vedaldi, A. (2017).",
                "venue": "Advances in neural information processing systems 30",
                "url": null
            }
        },
        {
            "38": {
                "title": "Eigenmodes of brain activity: Neural field theory predictions and comparison with experiment.",
                "author": "Robinson, P. A., Zhao, X., Aquino, K. M., Griffiths, J., Sarkar, S., and Mehta-Pandejee, G. (2016).",
                "venue": "NeuroImage 142, 79\u201398",
                "url": null
            }
        },
        {
            "39": {
                "title": "Laplace-beltrami eigenfunction expansion of cortical manifolds.",
                "author": "Seo, S. and Chung, M. K. (2011).",
                "venue": "In 2011 IEEE International Symposium on Biomedical Imaging: From Nano to Macro (IEEE), 372\u2013375",
                "url": null
            }
        },
        {
            "40": {
                "title": "Recursive deep models for semantic compositionality over a sentiment treebank.",
                "author": "Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A. Y., et al. (2013).",
                "venue": "In Proceedings of the 2013 conference on empirical methods in natural language processing. 1631\u20131642",
                "url": null
            }
        },
        {
            "41": {
                "title": "Llama: Open and efficient foundation language models.",
                "author": "Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., et al. (2023a).",
                "venue": "arXiv preprint arXiv:2302.13971",
                "url": null
            }
        },
        {
            "42": {
                "title": "Llama 2: Open foundation and fine-tuned chat models.",
                "author": "Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., et al. (2023b).",
                "venue": "arXiv preprint arXiv:2307.09288",
                "url": null
            }
        },
        {
            "43": {
                "title": "Attention is all you need.",
                "author": "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al. (2017).",
                "venue": "Advances in neural information processing systems 30",
                "url": null
            }
        },
        {
            "44": {
                "title": "Brainprint: A discriminative characterization of brain morphology.",
                "author": "Wachinger, C., Golland, P., Kremen, W., Fischl, B., Reuter, M., Initiative, A. D. N., et al. (2015).",
                "venue": "NeuroImage 109, 232\u2013248",
                "url": null
            }
        },
        {
            "45": {
                "title": "Glue: A multi-task benchmark and analysis platform for natural language understanding.",
                "author": "Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. (2018).",
                "venue": "arXiv preprint arXiv:1804.07461",
                "url": null
            }
        },
        {
            "46": {
                "title": "Self-instruct: Aligning language model with self generated instructions.",
                "author": "Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., et al. (2022).",
                "venue": "arXiv preprint arXiv:2212.10560",
                "url": null
            }
        },
        {
            "47": {
                "title": "Neural network acceptability judgments.",
                "author": "Warstadt, A., Singh, A., and Bowman, S. R. (2019).",
                "venue": "Transactions of the Association for Computational Linguistics 7, 625\u2013641",
                "url": null
            }
        },
        {
            "48": {
                "title": "Finetuned language models are zero-shot learners.",
                "author": "Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., et al. (2021).",
                "venue": "arXiv preprint arXiv:2109.01652",
                "url": null
            }
        },
        {
            "49": {
                "title": "A broad-coverage challenge corpus for sentence understanding through inference.",
                "author": "Williams, A., Nangia, N., and Bowman, S. R. (2017).",
                "venue": "arXiv preprint arXiv:1704.05426",
                "url": null
            }
        },
        {
            "50": {
                "title": "Principal component analysis.",
                "author": "Wold, S., Esbensen, K., and Geladi, P. (1987).",
                "venue": "Chemometrics and intelligent laboratory systems 2, 37\u201352",
                "url": null
            }
        },
        {
            "51": {
                "title": "Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models.",
                "author": "Zaken, E. B., Ravfogel, S., and Goldberg, Y. (2021).",
                "venue": "arXiv preprint arXiv:2106.10199",
                "url": null
            }
        },
        {
            "52": {
                "title": "Glm-130b: An open bilingual pre-trained model.",
                "author": "Zeng, A., Liu, X., Du, Z., Wang, Z., Lai, H., Ding, M., et al. (2022).",
                "venue": "arXiv preprint arXiv:2210.02414",
                "url": null
            }
        },
        {
            "53": {
                "title": "Increlora: Incremental parameter allocation method for parameter-efficient fine-tuning.",
                "author": "Zhang, F. F., Li, L., Chen, J.-C., Jiang, Z., Wang, B., and Qian, Y. (2023a).",
                "venue": "ArXiv abs/2308.12043",
                "url": null
            }
        },
        {
            "54": {
                "title": "Adaptive budget allocation for parameter-efficient fine-tuning.",
                "author": "Zhang, Q., Chen, M., Bukharin, A., He, P., Cheng, Y., Chen, W., et al. (2023b).",
                "venue": "arXiv preprint arXiv:2303.10512",
                "url": null
            }
        },
        {
            "55": {
                "title": "Delta-lora: Fine-tuning high-rank parameters with the delta of low-rank matrices.",
                "author": "Zi, B., Qi, X., Wang, L., Wang, J., Wong, K.-F., and Zhang, L. (2023).",
                "venue": "arXiv preprint arXiv:2309.02411",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.07440v3",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.2.1",
            "4.2.2",
            "4.2.3",
            "4.3",
            "4.3.1",
            "4.3.2",
            "4.3.3",
            "4.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.2.3",
            "4.3.3",
            "4.4"
        ]
    },
    "research_context": {
        "paper_id": "2403.07440v3",
        "paper_title": "Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A Brain-Inspired Method for Parameter-Efficient Fine-Tuning",
        "research_background": "### Paper Motivation:\nThe paper takes as motivation the significant advances and challenges associated with fine-tuning large pre-trained language models (LPLMs) like BERT and GPT-3. While these models exhibit exceptional performance across various NLP tasks, they require substantial computational resources and storage both during training and fine-tuning, making them inaccessible for most research institutions and enterprises. Recent parameter-efficient fine-tuning techniques reduce these demands but still face key limitations such as limited dynamic representation and significant performance fluctuation.\n\n### Research Problem:\nThe main research problem addressed by the paper is the development of a parameter-efficient fine-tuning method that can dynamically represent semantically complex downstream tasks, have reduced performance fluctuations across different tasks, and lower computational complexity during fine-tuning. The paper endeavors to address the limitations inherent in existing methods such as Adapter Tuning, BitFit, and particularly LoRA, which has issues related to the simplicity of its parameter decomposition matrix and computational complexity when optimized.\n\n### Relevant Prior Work:\n1. **Training and Performance of LPLMs**: The paper references the seminal works on LPLMs like BERT (Devlin et al., 2019) and GPT-3 (Brown et al., 2020), noting their significant computational resource requirements for training from scratch.\n2. **Fine-Tuning Techniques**: Prior studies on fine-tuning techniques (e.g., Ouyang et al., 2022) have demonstrated the effectiveness of enhancing model performance and adapting models to specific applications. Works like Adapter Tuning (Houlsby et al., 2019) and BitFit (Zaken et al., 2021) serve as critical reference points.\n3. **LoRA (Low-Rank Adaptation)**: The concept of reducing trainable parameters through low-rank adaptation matrices was introduced by Hu et al. (2022) and serves as a foundational technique for developing parameter-efficient fine-tuning methods. Improvements to LoRA, such as AdaLoRA (Zhang et al., 2023b) and IncreLoRA (Zhang et al., 2023a), have tackled performance optimization and applicability expansion but still struggle with issues of optimization complexity and performance consistency.\n4. **Brain-Inspired Approaches**: The brain\u2019s geometric structure and its correlation to neural activity patterns (Pang et al., 2023) serve as an inspiration for developing the new fine-tuning method proposed by the authors. \n\nThrough integrating the geometric inspiration from brain functionality into the LoRA structure, the paper proposes a new technique called Matrix-Transformation Based Low-Rank Adaptation (MTLoRA), designed to offer dynamic representation, reduced performance fluctuations, and lower computational demands.",
        "methodology": "### Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A Brain-Inspired Method for Parameter-Efficient Fine-Tuning\n\n**Methodology:**\n\nOur proposed MTLoRA fine-tuning approach is inspired by the brain's geometric structure and its influence on neural activities (Pang et al., 2023). Specifically, neural activities are driven by the resonance modes of the brain's topology, with task-induced activations resulting from the excitation of these modes, which have wavelengths exceeding 60mm. These wave-like activities highlight the link between the brain's geometry and function, allowing reconstruction of spatiotemporal characteristics of both spontaneous and task-induced brain activities.\n\nThe intrinsic resonance modes of the neocortex's geometric structure are characterized by its geometric feature patterns, derived from solving the Laplace\u2013Beltrami Operator (LBO) eigenvalue problem (Chavel, 1984; Seo and Chung, 2011). These patterns include properties such as curvature and spatial relationships analogous to how a violin string\u2019s resonance frequencies are determined by its physical properties. These spatiotemporal neural patterns can be decomposed into weighted sums of varying wavelengths, a process validated by reconstructing fMRI data with over 80% accuracy (Pang et al., 2023).\n\n### Application to Language Models\n\nWe hypothesize that large pre-trained language models (LPLMs) also embody complex geometric structures within their parameter matrices. Thus, we propose MTLoRA as a fine-tuning method that adjusts these geometric structures based on downstream task characteristics. The method involves the use of a transformation matrix \\( \\mathbf{P} \\) to modify parameter matrices, generating new matrix and improving model performance.\n\n**Key Components:**\n\n1. **Transformation Matrix:**\n   - \\( \\mathbf{P} \\) performs linear transformations on the parameter matrices, simulating geometric pattern transformations observed in brain structures.\n   \n2. **Parameter Matrices:**\n   - Includes matrices from various parts of the Transformer architecture such as word embedding projections, query/key/value projections, intermediate and output layer projections, and MLP layer weights.\n\n3. **Low-Rank Parameter Increment Matrices:**\n   - \\( \\mathbf{\\Delta W} \\approx \\mathbf{A} \\mathbf{P} \\mathbf{B} \\), where \\(\\mathbf{W}\\) is the original matrix, and \\(\\mathbf{A}\\), \\(\\mathbf{P}\\), \\(\\mathbf{B}\\) are learned matrices.\n\n**Structural Variants:**\n\n1. **Structure 1: SHIM (Spatial Harmonic Integration Matrix)**\n   - Integrates spatial feature transformations akin to harmonics in music.\n   - \\( \\mathbf{\\Delta W} = \\mathbf{A} \\mathbf{P} \\mathbf{B} \\)\n   - Performs linear transformations like rotation, scaling, translation, and shearing. \n\n2. **Structure 2: ICFM (Intrinsic Correlation Feature Matrix)**\n   - Simulates a positive semi-definite matrix, acting as a covariance matrix to capture intrinsic feature correlations.\n   - \\( \\mathbf{\\Delta W} = \\mathbf{A} \\mathbf{P} \\mathbf{P}^T \\mathbf{B} \\)\n\n3. **Structure 3: CTCM (Composite Transformation Coupling Matrix)**\n   - Uses two matrices \\(\\mathbf{P_1}\\) and \\(\\mathbf{P_2}\\), forming a composite transformation matrix.\n   - \\( \\mathbf{\\Delta W} = \\mathbf{A} \\mathbf{P_1} \\mathbf{P_2} \\mathbf{B} \\)\n\n4. **Structure 4: DTSM (Dual Transformation Superposition Matrix)**\n   - Superposes matrices \\(\\mathbf{P_1}\\) and \\(\\mathbf{P_2}\\) for complex feature interactions.\n   - \\( \\mathbf{\\Delta W} = \\mathbf{A} (\\mathbf{P_1} + \\mathbf{P_2}) \\mathbf{B} \\)\n\nThese structures provide varying levels of complexity and flexibility in adapting the model's parameters.\n\n### Application Across Layers\n\nThe MTLoRA method can be applied to dense layers in Transformer-based models or any neural network structure with dense layers, demonstrating its versatility in improving model performance through efficient parameter fine-tuning based on brain-inspired geometric transformations.",
        "main_experiment_and_results": "### Main Experiment Setup\n\nThe main experiment aims to validate the effectiveness of the MTLoRA method for parameter-efficient fine-tuning in natural language processing (NLP) tasks. The experimental setup includes:\n\n- **Tasks:** A total of eleven tasks across two major categories: Natural Language Understanding (NLU) and Natural Language Generation (NLG).\n- **Models:** GPT-2 Medium and RoBERTa-base are chosen as the base pre-trained models.\n- **Baselines:** The performance of MTLoRA is compared against several existing fine-tuning methods:\n  - Full Fine-Tuning (FFT)\n  - Adaptive Tuning\n  - Bitfit\n  - LoRA\n\n### Evaluation Metrics\n\nThe experiments use standard evaluation metrics prevalent in each task domain to measure performance. Common metrics may include accuracy, F1-score for classification tasks, and BLEU score for generation tasks. The exact metrics used are task-dependent and are designed to comprehensively evaluate the effectiveness of MTLoRA.\n\n### Main Experimental Results\n\nThe main results show that MTLoRA consistently performs well across the board. Specifically, in comparison to existing fine-tuning methods:\n\n- **GPT-2 Medium:**\n  - MTLoRA demonstrates significant improvements in generation tasks.\n  - Outperforms LoRA and Adaptive Tuning in maintaining performance while requiring fewer trainable parameters.\n  \n- **RoBERTa-base:**\n  - MTLoRA excels in NLU tasks, achieving higher accuracy and F1-scores compared to other methods.\n  - Proves to be more parameter-efficient than Full Fine-Tuning and Bitfit, often with comparable or superior performance.\n\nOverall, MTLoRA provides a highly effective balancing act between computational efficiency and model performance, establishing itself as a competitive method for parameter-efficient fine-tuning in NLP models."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To explore the impact of different transformation matrix structures of the MTLoRA method on natural language understanding tasks.",
            "experiment_process": "The experimental setup involved comparing the LoRA and MTLoRA methods based on the RoBERTa-base model across eight tasks of the GLUE benchmark. The primary benchmark for comparison was the results of LoRA (Hu et al., 2022). The experiment examined the performance of SHIM, ICFM, CTCM, and DTSM transformation matrix structures.",
            "result_discussion": "MTLoRA achieved an average performance improvement of about 1% across all tasks and controlled the standard deviation fluctuations well. SHIM showed a performance gain of 1.29% on CoLA, 0.87% on QQP, and 3.61% on RTE. ICFM excelled in semantic similarity tasks, achieving boosts of 1.22% on MRPC and 1.08% on RTE. CTCM demonstrated consistent performance, with a notable 1.0% increase on CoLA. DTSM highlighted an improvement of around 0.88% on QQP and 1.54% on CoLA, indicating efficiency in managing tasks with abundant corpora. The MTLoRA method exhibited lower loss, faster convergence, and lower standard deviation compared to LoRA.",
            "ablation_id": "2403.07440v3.No1"
        },
        {
            "research_objective": "To examine the impact of different transformation matrix structures of the MTLoRA method on natural language generation tasks.",
            "experiment_process": "The experiment compared the LoRA and MTLoRA methods using the GPT-2 Medium pre-trained model on the E2E, DART, and WebNLG datasets. LoRA (Hu et al., 2022) was used as the benchmark for comparison.",
            "result_discussion": "The MTLoRA structures enhanced model performance on DART and WebNLG tasks. SHIM improved efficacy by 0.83% on DART and 0.56% on WebNLG. ICFM showed increments of 0.65% on DART and 0.22% on WebNLG. CTCM led to increases of 0.88% on DART and 0.27% on WebNLG. DTSM facilitated boosts of 0.95% on DART and 0.31% on WebNLG. Different MTLoRA structures achieve optimal performance on various tasks, especially when integrating and sharing information through stepwise convolutional transformations.",
            "ablation_id": "2403.07440v3.No2"
        },
        {
            "research_objective": "To validate the effectiveness of the MTLoRA method and understand the specific impacts of varying rank sizes on model efficacy and stability.",
            "experiment_process": "The experiment adapted LoRA to different projection matrices of the RoBERTa-base model, consistent with the setup in section 4.2.2. The GLUE benchmark was used to test natural language understanding tasks. For natural language generation, LoRA's performance was re-tested on the E2E task. A sensitivity analysis was conducted to assess the influence of different rank sizes on performance using the COLA task, with the same random seed and consistent settings as described in Section 4.2.2.",
            "result_discussion": "LoRA showed slight improvements on QNLI and MRPC tasks but significant fluctuations on QQP and RTE tasks, indicating that merely increasing trainable parameters does not guarantee performance improvements. In the NLG task, MTLoRA did not reduce performance on the E2E task but maintained comparable BLEU scores with a lower standard deviation. Sensitivity analysis revealed that MTLoRA exhibited good performance and stability across different rank sizes, particularly with ICFM, which effectively prevented overfitting. MTLoRA achieved performance improvements with larger ranks while adding fewer parameters and outperformed some higher-ranked LoRAs at lower ranks.",
            "ablation_id": "2403.07440v3.No3"
        }
    ]
}