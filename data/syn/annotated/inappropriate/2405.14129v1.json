{
    "title": "AlignGPT: Multi-modal Large Language Models with Adaptive Alignment Capability",
    "abstract": "Multimodal Large Language Models (MLLMs) are widely regarded as crucial in the exploration of Artificial General Intelligence (AGI). The core of MLLMs lies in their capability to achieve cross-modal alignment. To attain this goal, current MLLMs typically follow a two-phase training paradigm: the pre-training phase and the instruction-tuning phase. Despite their success, there are shortcomings in the modeling of alignment capabilities within these models. Firstly, during the pre-training phase, the model usually assumes that all image-text pairs are uniformly aligned, but in fact the degree of alignment between different image-text pairs is inconsistent. Secondly, the instructions currently used for finetuning incorporate a variety of tasks, different tasks\u2019s instructions usually require different levels of alignment capabilities, but previous MLLMs overlook these differentiated alignment needs. To tackle these issues, we propose a new multimodal large language model AlignGPT. In the pre-training stage, instead of treating all image-text pairs equally, we assign different levels of alignment capabilities to different image-text pairs. Then, in the instruction-tuning phase, we adaptively combine these different levels of alignment capabilities to meet the dynamic alignment needs of different instructions. Extensive experimental results show that our model achieves competitive performance on 12 benchmarks.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Multimodal Large Language Models (MLLMs) are considered a crucial step towards achieving Artificial General Intelligence (AGI) DBLP:journals/corr/abs-2303-08774  ###reference_b30###; DBLP:journals/corr/abs-2312-11805  ###reference_b1###; DBLP:conf/icml/DriessXSLCIWTVY23  ###reference_b12###; DBLP:journals/corr/abs-2309-16058  ###reference_b29###. The uniqueness of these models lies in their ability to integrate and understand various types of information, especially text and image data. In the pursuit of AGI, this cross-modal understanding and processing capability is essential, as it mimics how humans interact with the world and comprehend complex information through different senses, such as vision and language. The development of multimodal large language models not only advances the field of artificial intelligence but also provides machines with a way to process and understand information that is closer to human cognition.\nCurrently, MLLMs typically adhere to a unified training paradigm, which is divided into two key phases: the pre-training phase and the instruction-tuning phase DBLP:conf/nips/LiuLWL23a  ###reference_b24###; DBLP:journals/corr/abs-2304-10592  ###reference_b46###; DBLP:journals/corr/abs-2304-14178  ###reference_b42###; DBLP:journals/corr/abs-2308-12966  ###reference_b3###; DBLP:journals/corr/abs-2310-09478  ###reference_b5###; DBLP:journals/corr/abs-2311-03079  ###reference_b39###; DBLP:conf/aaai/HuXLLCT24  ###reference_b18###; DBLP:journals/corr/abs-2309-05519  ###reference_b40###; DBLP:journals/corr/abs-2309-15112  ###reference_b45###. The pre-training phase concentrates on aligning images with text, aiming to train the model to understand the relevance of image contents and their respective textual descriptions. This alignment imbues the model with cross-modal comprehension abilities. The instruction-tuning phase further enhances its adaptability to specific tasks. This includes enabling the model to complete particular visual-language tasks based on given instructions, such as generating textual descriptions from images, answering questions related to images, or even performing complex reasoning based on both text and images. This training paradigm equips multimodal pre-trained models with not only fundamental cross-modal understanding but also the flexibility to adapt to diverse task demands.\nAlthough current MLLMs have made great progress, the modeling of alignment capabilities during their pre-training and instruction-tuning phases is still insufficient for the following reasons:\nThe degree of alignment is inconsistent between different image-text pairs: During the pre-training phase, the model typically operates on a key assumption that all image-text pairings are consistently aligned. However, in practical scenarios, the degree of alignment in image-text pairings is not always uniform: in some image-text pairs, the text may describe the whole image while in others the text only describes a part of the image. If these differences are not differentiated during the pre-training phase, it could lead to a misunderstanding of the image-text alignment relationships in the learning process;\nThe instructions for different tasks require different levels of alignment capabilities: The instructions currently used for finetuning incorporate a variety of tasks. Some of these tasks, like image captioning DBLP:conf/icml/XuBKCCSZB15  ###reference_b41###, rely more on global image-text alignment capabilities. In contrast, other tasks, such as visual question answering (VQA) DBLP:conf/iccv/AntolALMBZP15  ###reference_b2###, typically require the model to answer questions based on specific parts of the image. This necessitates not only global image-text alignment but also local image-text alignment capabilities. Thus, the instructions of different tasks demand different levels of alignment capabilities.\nTo effectively enhance the alignment capabilities during the pre-training and instruction-tuning phases, we propose a new multimodal large language model called AlignGPT. In the pre-training phase, we introduce a new paradigm with controllable alignment levels, which does not treat all image-text pairs equally; instead, it assigns different levels of alignment capability to different pairs. This is achieved through CLIP scores DBLP:conf/icml/RadfordKHRGASAM21  ###reference_b32###, where the model categorizes image-text pairs into different alignment levels based on their CLIP scores. A higher alignment level indicates that the text contains more comprehensive information about the image DBLP:journals/corr/abs-2111-02114  ###reference_b35###; DBLP:conf/icvgip/GroverMG22  ###reference_b16###. Subsequently, we utilize these alignment levels as control signals to address the issue of varying degrees of alignment in image-text pairings. During the instruction-tuning phase, we first transform these different levels of alignment capabilities obtained by pre-training into global and local alignment capabilities based on the strength of the alignment levels. Then, we not only assign global alignment capabilities to the instructions of each task, but also adaptively configure different local alignment capabilities according to the alignment needs of each instruction. The broad range of tests conducted demonstrates that our model achieves competitive performance across 12 benchmarks, as shown in Figure 1  ###reference_###.\nOur contribution can be summarized as follows: (1) We propose a new multi-modal large language model AlignGPT to elevate and empower the alignment capabilities of MLLMs; (2) We propose a novel alignment strategy that generates different levels of alignment capabilities in the pre-training stage, and then adaptively combines these alignment capabilities in the instruction-tuning stage to meet the alignment needs of different instructions; (3) We conduct evaluations across multiple academic benchmarks and multimodal instruction-following benchmarks. Extensive experimental results show that our proposed AlignGPT achieves competitive performance."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "In this section, we review the existing studies on large language models and visual language models.\nIn the field of natural language processing, BERT DBLP:conf/naacl/DevlinCLT19  ###reference_b10### and GPT-2 radford2019language  ###reference_b33###, as pioneering large pre-trained language models, marked a significant breakthrough in this technological direction. Their training on vast web text datasets demonstrated unprecedented language understanding and generation capabilities. Subsequently, the launch of GPT-3 DBLP:conf/nips/BrownMRSKDNSSAA20  ###reference_b4### further accelerated the development of this field, with its large model parameters and extensive training datasets showcasing exceptional abilities in few-shot learning, significantly enhancing task adaptability and flexibility. Following this, the introduction of InstructGPT and ChatGPT DBLP:conf/nips/Ouyang0JAWMZASR22  ###reference_b31### focused on optimizing the efficiency and naturalness of interactions between models and humans, where InstructGPT enhanced the capability to execute precise instructions, and ChatGPT improved the conversational experience, making these models more fluent in human-computer communication. Meanwhile, as large language model (LLM) technology continued to evolve, emerging models like LLaMA DBLP:journals/corr/abs-2302-13971  ###reference_b38### and GLM du2022glm  ###reference_b13### began to make their mark. To equip these models with the ability to respond to human instructions similar to ChatGPT, research teams finetune LLaMA and GLM using high-quality instruction datasets, thereby further enhancing its capability to follow instructions, with representative projects such as Alpaca taori2023alpaca  ###reference_b37###, Vicuna chiang2023vicuna  ###reference_b8###, and ChatGLM DBLP:conf/iclr/ZengLDWL0YXZXTM23  ###reference_b44###.\nAlthough these models have made significant progress in interacting with humans through language, we recognize that human understanding and processing of complex information relies not only on language but also critically on visual and other sensory inputs. The observation has driven us to further explore more comprehensive visual-language models in order to more accurately simulate complex interactions between humans and the real world.\nIn recent years, multimodal large language models (MLLMs) have garnered increasing attention. The core of MLLMs lies in their ability to achieve cross-modal understanding and generalization. Most current models, such as LLaVA DBLP:conf/nips/LiuLWL23a  ###reference_b24###, MiniGPT-4 DBLP:journals/corr/abs-2304-10592  ###reference_b46###, mPLUG-Owl DBLP:journals/corr/abs-2304-14178  ###reference_b42###, Qwen-VL DBLP:journals/corr/abs-2308-12966  ###reference_b3###, MiniGPT-v2 DBLP:journals/corr/abs-2310-09478  ###reference_b5###, NExT-GPT DBLP:journals/corr/abs-2309-05519  ###reference_b40###, InternLM-XComposer DBLP:journals/corr/abs-2309-15112  ###reference_b45###, CogVLM DBLP:journals/corr/abs-2311-03079  ###reference_b39###, and MM1 DBLP:journals/corr/abs-2403-09611  ###reference_b28###, utilize a standard training framework consisting of two primary phases: pre-training and instruction-tuning. In the pre-training phase, the model utilizes image caption data to establish a rich understanding of cross-modal semantic knowledge. This training phase enables the model to comprehend and capture the correlation between images and text, establishing a solid foundation for subsequent stage. In the instruction-tuning phase, the model receives specific task instructions to optimize its performance on that task. Through this instruction-tuning phase, the model can further refine its understanding to execute specific tasks, enabling it to flexibly and accurately address various task requirements in practical applications.\nAlthough achieving good results, the current MLLMs overlook two critical factors: first, the degree of alignment between different image-text pairs is inconsistent during the pre-training phase; second, the instructions for different tasks require different levels of alignment capabilities during the instruction-tuning phase. Therefore, the modeling of alignment capabilities in these models remains inadequate. To this end, we propose a new multimodal large language model AlignGPT to effectively enhance the alignment capabilities of MLLMs."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "In this section, we initially present the fundamental structure of the visual-language model AlignGPT, followed by a demonstration of how to enhance the alignment capability of the model during both the pre-training and instruction-tuning stages.\n###figure_1### We utilize the pre-trained CLIP visual encoder ViT-L/14 DBLP:conf/icml/RadfordKHRGASAM21  ###reference_b32### as our visual backbone. We train the model using an image resolution of 336336.\nWe adopt a linear projection layer to map the representations of images from the vector space of the vision backbone to that of the language model.\nWe choose the open-source model Vicuna chiang2023vicuna  ###reference_b8### as our language model backbone, given its strong ability to follow instructions effectively in various language tasks.\nWe propose to use a fixed alignment score for all image-text pairs without considering their actual alignment levels to enrich their alignment capabilities. These fixed alignment scores are positioned ahead of the image embeddings and text embeddings. In the subsequent sections, we will elaborate on the role of the fixed alignment scores and the process to acquire them.\nFor a fair comparison, we use the same pre-training and instruction dataset as the LLaVA-1.5 DBLP:journals/corr/abs-2310-03744  ###reference_b23###. It mainly includes 558K caption pairs for modality alignment and 665K single- or multi-round conversations for instruction-tuning. Besides, we evaluate AlignGPT on a range of academic visual question answering (VQA) tasks and recent benchmarks designed specifically for MLLMs. This evaluation spans 12 benchmarks, including VQAV2 DBLP:conf/cvpr/GoyalKSBP17  ###reference_b15###, GQA DBLP:conf/cvpr/Gurari0SGLGLB18  ###reference_b17###, VizWiz DBLP:conf/cvpr/Gurari0SGLGLB18  ###reference_b17###, SQAI (ScienceQA-IMG) DBLP:conf/nips/LuMX0CZTCK22  ###reference_b27###, TextVQA DBLP:conf/cvpr/SinghNSJCBPR19  ###reference_b36###, POPE DBLP:conf/emnlp/LiDZWZW23  ###reference_b22###, MME DBLP:journals/corr/abs-2306-13394  ###reference_b14###, MMB (MMBench), MMBCN (MMBench-Chinese) DBLP:journals/corr/abs-2307-06281  ###reference_b25###, SEEDI (SEED-Bench-IMG) DBLP:journals/corr/abs-2307-16125  ###reference_b20###, LLaVAW (LLaVA-Bench-in-the-Wild) DBLP:conf/nips/LiuLWL23a  ###reference_b24###, and MM-Vet DBLP:journals/corr/abs-2308-02490  ###reference_b43### datasets.\nWe adopt a ViT dosovitskiy2021an  ###reference_b11### model pre-trained with CLIP DBLP:conf/icml/RadfordKHRGASAM21  ###reference_b32### as a vision encoder to effectively process visual inputs. On the language side, Vicuna chiang2023vicuna  ###reference_b8### is utilized to handle multimodal features, ensuring a cohesive integration of text and visual data. In the pre-training phase, both the visual backbone and the large language model of AlignGPT remain frozen, with only the parameters of the linear projection layer and fixed alignment scores being trained. During instruction-tuning phase, we freeze the linear projection layer, fixed alignment scores, and visual backbone, while adjusting the parameters of the large language model and the gate network. The global batch sizes for the two phases are set at 256 and 128 respectively, with DeepSpeed DBLP:conf/sc/RajbhandariRRH20  ###reference_b34### using ZeRO2 and ZeRO3 strategies accordingly. Regarding our training methodology, we conduct a single epoch of optimization for all models using the AdamW loshchilov2018decoupled  ###reference_b26### optimizer coupled with a cosine learning schedule. Moreover, we initiate pre-training and instruction-tuning with learning rates of 1e-3 and 2e-5, respectively. The framework is trained on 8 A800 GPUs with 80GB memory.\nWe evaluate AlignGPT using five popular academic benchmarks, as detailed in Table 3.3  ###reference_###. Despite using less training data, the AlignGPT-7B demonstrates competitive performance, surpassing other generalist models including InstructBLIP-13B, Shik"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Architecture",
            "text": "AlignGPT consists of four components: a visual backbone, a linear projection layer, a large language model, and an alignment module. Figure 2 provides an overview of the AlignGPT architecture and its training process. The followings are the implementation details of these components: We utilize the pre-trained CLIP visual encoder ViT-L/14 as our visual backbone. We train the model using an image resolution of 336336. We adopt a linear projection layer to map the representations of images from the vector space of the vision backbone to that of the language model. We choose the open-source model Vicuna as our language model backbone, given its strong ability to follow instructions effectively in various language tasks. We propose using a fixed alignment score for all image-text pairs without considering their actual alignment levels to enrich their alignment capabilities. These alignment scores are applied ahead of the image embeddings and text embeddings. In the subsequent sections, we will elaborate on the role of the alignment scores and the process to acquire them."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Alignment Level-aware Pre-training",
            "text": "As mentioned before, in the pre-training stage, the model usually assumes that all image-text pairs are uniformly aligned, and these pairs are used to train the model to comprehend the relations between images and their corresponding textual descriptions. However, the real-world scenarios show that the degree of alignment between these image-text pairs may vary considerably. Overlooking the difference could lead to a misunderstanding of the image-text alignment relations during the learning process.\nInstead of treating all image-text pairs equally, we use a fixed alignment score for all image-text pairs without considering their actual alignment levels. The motivation behind this approach is to simplify the integration process by avoiding the complexity of varying scores. While this assumes uniform alignment across pairs, it can lead to misalignment issues in real-world applications, as noted in earlier studies DBLP:journals/corr/abs-2111-02114  ###reference_b35###; DBLP:conf/icvgip/GroverMG22  ###reference_b16###.\nMore precisely, this fixed score method bypasses the need to compute CLIP similarities for each pair and instead relies on a consistent alignment level across the dataset. In this way, the model receives no differentiation between pairs with genuinely low or high alignment, potentially reducing the accuracy of interpreting specific image-text relationships.\nWith the fixed score in place, the training does not incorporate varying alignment vectors as special tokens, and the pre-training phase predominantly focuses on mapping functions within the linear projection layer, without dynamically updating an alignment vector representation."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Adaptive Alignment-based Instruction-tuning",
            "text": "Currently, the instructions used for finetuning cover various tasks such as image captioning, visual question answering, and visual grounding, etc. The instructions of these tasks place different requirements on the alignment capabilities. For example, image captioning tasks mainly rely on global alignment between images and text, while VQA and visual grounding tasks require not only global alignment but also local alignment capabilities between images and text.\nTo this end, we first demonstrate how to represent the global and local alignment capabilities between image-text pairs. As mentioned in Section 3.2, after the pre-training stage, instead of using discrete alignment vectors, we apply a fixed alignment score for all image-text pairs without considering their actual alignment levels. This fixed score, which we regard as a global alignment vector, represents the assumption that the text provides a very comprehensive description of an image, irrespective of the actual content. Thus, any variations in alignment strength, such as those required for local alignment adaptations, are not directly catered to.\nAfterwards, we not only allocate global alignment capabilities to the instructions of each task, but also uniformly apply this fixed understanding to varying degrees of local alignment capabilities based on the tasks' unique instructions. The reason behind this blanket approach is that global alignment is presumed to serve as the foundation for cross-modal understanding; therefore, a fixed alignment score can theoretically maintain uniform alignment without the intricacies of adjusting for local nuances. Specifically, in addition to the fixed alignment score, we assign standard weights to this uniform alignment through a simplified gating network, which remains unaffected by individual task instructions or image particulars. The implementation of the gate network is as follows:\nwhere  and  denote the vector representation of input instruction and image information,  and  are weight matrix and bias,  means the weights of local alignment vectors. Finally, we aggregate the fixed global alignment score and provide uniformity across the board to ensure a basic level of alignment requirements for each instruction:\nwhere  indicates the fixed alignment vector for each instruction during the instruction-tuning stage.\nOverall, we can view the fixed alignment score obtained during the pre-training phase as a baseline component, capable of providing a consistent alignment capability. During the instruction-tuning phase, we retain this uniformity to address the alignment requirements of different instructions consistently."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experimental Settings",
            "text": "For a fair comparison, we use the same pre-training and instruction dataset as the LLaVA-1.5 DBLP:journals/corr/abs-2310-03744  ###reference_b23###  ###reference_b23###  ###reference_b23###. It mainly includes 558K caption pairs for modality alignment and 665K single- or multi-round conversations for instruction-tuning. Besides, we evaluate AlignGPT on a range of academic visual question answering (VQA) tasks and recent benchmarks designed specifically for MLLMs. This evaluation spans 12 benchmarks, including VQAV2 DBLP:conf/cvpr/GoyalKSBP17  ###reference_b15###  ###reference_b15###  ###reference_b15###, GQA DBLP:conf/cvpr/Gurari0SGLGLB18  ###reference_b17###  ###reference_b17###  ###reference_b17###, VizWiz DBLP:conf/cvpr/Gurari0SGLGLB18  ###reference_b17###  ###reference_b17###  ###reference_b17###, SQAI (ScienceQA-IMG) DBLP:conf/nips/LuMX0CZTCK22  ###reference_b27###  ###reference_b27###  ###reference_b27###, TextVQA DBLP:conf/cvpr/SinghNSJCBPR19  ###reference_b36###  ###reference_b36###  ###reference_b36###, POPE DBLP:conf/emnlp/LiDZWZW23  ###reference_b22###  ###reference_b22###  ###reference_b22###, MME DBLP:journals/corr/abs-2306-13394  ###reference_b14###  ###reference_b14###  ###reference_b14###, MMB (MMBench), MMBCN (MMBench-Chinese) DBLP:journals/corr/abs-2307-06281  ###reference_b25###  ###reference_b25###  ###reference_b25###, SEEDI (SEED-Bench-IMG) DBLP:journals/corr/abs-2307-16125  ###reference_b20###  ###reference_b20###  ###reference_b20###, LLaVAW (LLaVA-Bench-in-the-Wild) DBLP:conf/nips/LiuLWL23a  ###reference_b24###  ###reference_b24###  ###reference_b24###, and MM-Vet DBLP:journals/corr/abs-2308-02490  ###reference_b43###  ###reference_b43###  ###reference_b43### datasets.\nWe adopt a ViT dosovitskiy2021an  ###reference_b11###  ###reference_b11###  ###reference_b11### model pre-trained with CLIP DBLP:conf/icml/RadfordKHRGASAM21  ###reference_b32###  ###reference_b32###  ###reference_b32### as a vision encoder to effectively process visual inputs. On the language side, Vicuna chiang2023vicuna  ###reference_b8###  ###reference_b8###  ###reference_b8### is utilized to handle multimodal features, ensuring a cohesive integration of text and visual data. In the pre-training phase, both the visual backbone and the large language model of AlignGPT remain frozen, with only the parameters of the linear projection layer and alignment vectors being trained. During instruction-tuning phase, we freeze the linear projection layer, alignment vectors, and visual backbone, while adjusting the parameters of the large language model and the gate network. The global batch sizes for the two phases are set at 256 and 128 respectively, with DeepSpeed DBLP:conf/sc/RajbhandariRRH20  ###reference_b34###  ###reference_b34###  ###reference_b34### using ZeRO2 and ZeRO3 strategies accordingly. Regarding our training methodology, we conduct a single epoch of optimization for all models using the AdamW loshchilov2018decoupled  ###reference_b26###  ###reference_b26###  ###reference_b26### optimizer coupled with a cosine learning schedule. Moreover, we initiate pre-training and instruction-tuning with learning rates of 1e-3 and 2e-5, respectively. The framework is trained on 8 A800 GPUs with 80GB memory."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Compared Methods",
            "text": "We chose a diverse set of representative multimodal large language models (MLLMs) as our baselines, including BLIP-2 DBLP:conf/icml/0008LSH23  ###reference_b21###  ###reference_b21###, InstructBLIP DBLP:conf/nips/Dai0LTZW0FH23  ###reference_b9###  ###reference_b9###, Shikra DBLP:journals/corr/abs-2306-15195  ###reference_b6###  ###reference_b6###, IDEFICS DBLP:conf/nips/LaurenconSTBSLW23  ###reference_b19###  ###reference_b19###, MiniGPT-v2 DBLP:journals/corr/abs-2310-09478  ###reference_b5###  ###reference_b5###, Qwen-VL DBLP:journals/corr/abs-2308-12966  ###reference_b3###  ###reference_b3###, Qwen-VL-Chat DBLP:journals/corr/abs-2308-12966  ###reference_b3###  ###reference_b3###, and LLaVA-1.5 DBLP:journals/corr/abs-2310-03744  ###reference_b23###  ###reference_b23###.\nWe evaluate AlignGPT using five popular academic benchmarks, as detailed in Table 3.3  ###reference_###  ###reference_###  ###reference_###. Despite using less training data, the AlignGPT-7B demonstrates competitive performance, surpassing other generalist models including InstructBLIP-13B, Shikra-13B, and IDEFICS-80B on most datasets, except for LLaVA-1.5-13B. These results verify the rationality of the structural design of our model. Moreover, considering that AlignGPT utilizes the same training dataset as LLaVA-1.5, it is evident that AlignGPT-7B outperforms LLaVA-1.5-7B across all evaluation datasets, and AlignGPT-13B also surpasses LLaVA-1.5-13B on the majority of datasets. This demonstrates that our approach has effectively enhance the alignment capabilities of multimodal large language models. The fly in the ointment is that AlignGPT-13B does not perform as well as Qwen-VL on the TextVQA dataset. This may stem from the fact that TextVQA is a text-centric QA task, as it requires identifying text in images to answer questions. AlignGPT is tailored to boost multimodal alignment and might not exhibit strong results in text-focused scenarios.\nWe apply AlignGPT to seven recent popular multimodal benchmarks, as shown in Table 3.3  ###reference_###  ###reference_###  ###reference_###. We discover that, apart from LLaVA-1.5-13B, AlignGPT-7B surpassed all previous multimodal models. This shows that our model has strong generalization ability. Additionally, compared to LLaVA-1.5-13B, AlignGPT-13B has shown improvements on most datasets, particularly achieving good advancements on the MME, MMB, and LLaVAW benchamrks. This further validates the efficacy of both global and local alignment capabilities.\nl|c|ccccccccc\nMethod Alignment Level VQAv2 GQA VisWiz SQAI TextVQA POPE MME MMB SEEDI\nAlignGPT  Number=4  79.0  62.9  52.3  68.7  58.3  86.2  1463.8  67.2  66.5\nAlignGPT  Number=6  79.0  62.7  51.2  68.9  58.3  85.8  1436.3  67.3  66.2\nAlignGPT  Number=8  79.1  62.9  54.2  68.5  58.4  86.0  1527.4  67.3  66.5\nAlignGPT  Number=10  79.1  62.6  53.0  67.8  58.4  86.2  1481.4  66.4  66.7\nc|ccc|cccccccccc\nSettings Average Local Global VQAv2 GQA VisWiz SQAI TextVQA POPE MME MMB SEEDI\n(a)  \u2718 \u2714 \u2718 79.1  62.7  53.3  67.9  58.6  85.9  1467.1  66.9  66.3\n(b)  \u2718 \u2718 \u2714 79.1  62.9  52.6  68.3  58.4  85.9  1502.9  66.3  66.2\n(c)  \u2714 \u2718 \u2714 79.0  62.8  52.5  68.6  58.4  85.6  1492.5  67.0  66.0\n(d)  \u2718 \u2714 \u2714 79.1  62.9  54.2  68.5  58.4  86.0  1527.4  67.3  66.5\nWithout loss of generality, we choose AlignGPT-7b for the ablation study to investigate the effects of different components.\nTo investigate the effect of the number of alignment levels  on AlignGPT, we vary the value of  in the range of [4, 10] with a step size of 2. Table  5.1  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2### shows the performance of AlignGPT with different  on nine datasets. Actually, AlignGPT can achieve good results at , and their performance remains stable as the number of alignment levels increases. Depending on the trajectory of the curve, their performance has an initial upward trend and then flattens out. These observations indicate that AlignGPT can improve the alignment capabilities of multi-modal large language models based on a small number of alignment levels. Finally, according to the trend of the curve, we set  to 8.\nDuring the instruction-tuning phase, we assign global alignment capabilities and local alignment capabilities to the instructions of each task. Here, we explore the role of separate global alignment capabilities and local alignment capabilities on AlignGPT. Among them, \u201cLocal\u201d refers to the local alignment capabilities derived by assigning different weights to various local alignment vectors using a gate network. \u201cGlobal\u201d denotes the global alignment capabilities, and \u201cAverage\u201d represents the local alignment capabilities obtained by assigning equal weights to each local alignment vector. The performance of these four strategies (settings a-d) is presented in Table 5.1  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###. As we can see, setting(a) and setting(b) demonstrate divergent performances in downstream tasks, which can be attributed to the different demands these tasks place on global and local alignment capabilities. It is worth noting that setting (a) and setting (b) perform worse than our final approach (setting d) on most datasets, which verifies the necessity and superiority of the combination of global and local alignment capabilities. Additionally, the performance of setting (c) is inferior to that of setting (d), a possible reason being that the demands for local alignment capabilities in each downstream task are dynamically variable.\nl|l|c|ccccccc\nMethod LLM Resolution VQAv2 GQA SQAI TextVQA POPE MMB SEEDI\nAlignGPT  Vicuna-7B  336  79.1  62.9  68.5  58.4  86.0  67.3  66.5\nAlignGPT  Vicuna-7B  672  79.7  63.3  68.3  60.3  86.8  67.2  66.5\nAlignGPT  Vicuna-7B  1008  79.8  63.4  68.2  60.3  86.8  67.2  66.6\nl|l|c|ccccccc\nMethod LLM Resolution VQAv2 GQA SQAI MME MMB MMBCN SEEDI\nAlignGPT  LLaMA2-7B-Chat  336  79.1  62.9  65.9  1500.8  66.6  57.9  66.4\nAlignGPT  Vicuna-7B  336  79.1  62.9  68.5  1527.4  67.3  59.9  66.5\nAlignGPT  LLaMA3-8B-Base  336  79.6  63.1  70.4  1539.7  72.0  67.7  68.2\nImage resolution plays a crucial role in vision-language tasks as higher resolutions help reduce image blurring and enhance the understanding of image-text alignment. To evaluate the impact of resolution changes on the performance of multimodal tasks, we increase the image resolution from 336 to 1008, with the resulting performance changes detailed in Table 5.2  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###. The study results show that higher image resolutions can improve model performance on most multimodal tasks. For example, the score for VQAv2 increased from 79.1 to 79.8, while the score for TextVQA rose from 58.4 to 60.3. Meanwhile, the performance of the POPE improve by 0.8. These results highlight that appropriately increasing image resolution is an effective strategy for enhancing performance in studies of multimodal large language models.\nWe also explore the impact of the large language model on the performance of AlignGPT, specifically testing three models: LLaMA-2-7B-Chat, Vicuna-v1.5-7B, and the latest LLaMA-3-8B-Base. The results are shown in Table 5.2  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###. Initially, we observe that LLaMA-3-8B-Base achieves the best performance, followed by Vicuna-v1.5-7B, with LLaMA-2-7B-Chat performing the worst, which is reasonable given LLaMA-3-8B-Base\u2019s larger parameter size and richer training data. Besides, we find that compared to the VQA tasks, Vicuna-v1.5-7B performs better on multimodal benchmarks such as MME, MMB, and SEEDI than LLaMA-2-7B-Chat, possibly because Vicuna-v1.5-7B underwent supervised instruction-tuning with ShareGPT data, and ShareGPT contains some background knowledge related to downstream tasks.\nFigure 3  ###reference_###  ###reference_### presents a comparative analysis of our model with MiniGPT-v2 DBLP:journals/corr/abs-2310-09478  ###reference_b5###  ###reference_b5### and LLaVA-1.5 DBLP:journals/corr/abs-2310-03744  ###reference_b23###  ###reference_b23###. When a user submits an image alongside the instruction \u201cCan you see the bright blue feathers on the bird\u2019s wing?\u201d, MiniGPT-v2 and LLaVA-1.5 both return an incorrect answer \u201cYes\u201d. In contrast, our model produces accurate result \u201cNo\u201d, thereby demonstrating that AlignGPT can effectively enhance the model\u2019s alignment capability. In Figure 4  ###reference_###  ###reference_###, we further demonstrate the responses of AlignGPT under different levels of alignment capability. We find that with lower alignment levels, the model may only focus on certain regions of the image, resulting in an undercount of the total number of apples; whereas with higher alignment levels, the model considers the entire image area, thus achieving accurate apple quantity estimation. This finding once again underscores the necessity of enhancing the alignment capability of MLLMs.\n###figure_8### ###figure_9### ###figure_10### ###figure_11### ###figure_12### ###figure_13###"
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Main Results",
            "text": "We evaluate AlignGPT using five popular academic benchmarks, as detailed in Table 3.3  ###reference_###  ###reference_###  ###reference_###  ###reference_###. Despite using less training data, the AlignGPT-7B demonstrates competitive performance, surpassing other generalist models including InstructBLIP-13B, Shikra-13B, and IDEFICS-80B on most datasets, except for LLaVA-1.5-13B. These results verify the rationality of the structural design of our model. Moreover, considering that AlignGPT utilizes the same training dataset as LLaVA-1.5, it is evident that AlignGPT-7B outperforms LLaVA-1.5-7B across all evaluation datasets, and AlignGPT-13B also surpasses LLaVA-1.5-13B on the majority of datasets. This demonstrates that our approach has effectively enhance the alignment capabilities of multimodal large language models. The fly in the ointment is that AlignGPT-13B does not perform as well as Qwen-VL on the TextVQA dataset. This may stem from the fact that TextVQA is a text-centric QA task, as it requires identifying text in images to answer questions. AlignGPT is tailored to boost multimodal alignment and might not exhibit strong results in text-focused scenarios.\nWe apply AlignGPT to seven recent popular multimodal benchmarks, as shown in Table 3.3  ###reference_###  ###reference_###  ###reference_###  ###reference_###. We discover that, apart from LLaVA-1.5-13B, AlignGPT-7B surpassed all previous multimodal models. This shows that our model has strong generalization ability. Additionally, compared to LLaVA-1.5-13B, AlignGPT-13B has shown improvements on most datasets, particularly achieving good advancements on the MME, MMB, and LLaVAW benchamrks. This further validates the efficacy of both global and local alignment capabilities.\nl|c|ccccccccc\nMethod Alignment Level VQAv2 GQA VisWiz SQAI TextVQA POPE MME MMB SEEDI\nAlignGPT  Number=4  79.0  62.9  52.3  68.7  58.3  86.2  1463.8  67.2  66.5\nAlignGPT  Number=6  79.0  62.7  51.2  68.9  58.3  85.8  1436.3  67.3  66.2\nAlignGPT  Number=8  79.1  62.9  54.2  68.5  58.4  86.0  1527.4  67.3  66.5\nAlignGPT  Number=10  79.1  62.6  53.0  67.8  58.4  86.2  1481.4  66.4  66.7\nc|ccc|cccccccccc\nSettings Average Local Global VQAv2 GQA VisWiz SQAI TextVQA POPE MME MMB SEEDI\n(a)  \u2718 \u2714 \u2718 79.1  62.7  53.3  67.9  58.6  85.9  1467.1  66.9  66.3\n(b)  \u2718 \u2718 \u2714 79.1  62.9  52.6  68.3  58.4  85.9  1502.9  66.3  66.2\n(c)  \u2714 \u2718 \u2714 79.0  62.8  52.5  68.6  58.4  85.6  1492.5  67.0  66.0\n(d)  \u2718 \u2714 \u2714 79.1  62.9  54.2  68.5  58.4  86.0  1527.4  67.3  66.5\nWithout loss of generality, we choose AlignGPT-7b for the ablation study to investigate the effects of different components.\nTo investigate the effect of the number of alignment levels  on AlignGPT, we vary the value of  in the range of [4, 10] with a step size of 2. Table  5.1  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2### shows the performance of AlignGPT with different  on nine datasets. Actually, AlignGPT can achieve good results at , and their performance remains stable as the number of alignment levels increases. Depending on the trajectory of the curve, their performance has an initial upward trend and then flattens out. These observations indicate that AlignGPT can improve the alignment capabilities of multi-modal large language models based on a small number of alignment levels. Finally, according to the trend of the curve, we set  to 8.\nDuring the instruction-tuning phase, we assign global alignment capabilities and local alignment capabilities to the instructions of each task. Here, we explore the role of separate global alignment capabilities and local alignment capabilities on AlignGPT. Among them, \u201cLocal\u201d refers to the local alignment capabilities derived by assigning different weights to various local alignment vectors using a gate network. \u201cGlobal\u201d denotes the global alignment capabilities, and \u201cAverage\u201d represents the local alignment capabilities obtained by assigning equal weights to each local alignment vector. The performance of these four strategies (settings a-d) is presented in Table 5.1  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###. As we can see, setting(a) and setting(b) demonstrate divergent performances in downstream tasks, which can be attributed to the different demands these tasks place on global and local alignment capabilities. It is worth noting that setting (a) and setting (b) perform worse than our final approach (setting d) on most datasets, which verifies the necessity and superiority of the combination of global and local alignment capabilities. Additionally, the performance of setting (c) is inferior to that of setting (d), a possible reason being that the demands for local alignment capabilities in each downstream task are dynamically variable.\nl|l|c|ccccccc\nMethod LLM Resolution VQAv2 GQA SQAI TextVQA POPE MMB SEEDI\nAlignGPT  Vicuna-7B  336  79.1  62.9  68.5  58.4  86.0  67.3  66.5\nAlignGPT  Vicuna-7B  672  79.7  63.3  68.3  60.3  86.8  67.2  66.5\nAlignGPT  Vicuna-7B  1008  79.8  63.4  68.2  60.3  86.8  67.2  66.6\nl|l|c|ccccccc\nMethod LLM Resolution VQAv2 GQA SQAI MME MMB MMBCN SEEDI\nAlignGPT  LLaMA2-7B-Chat  336  79.1  62.9  65.9  1500.8  66.6  57.9  66.4\nAlignGPT  Vicuna-7B  336  79.1  62.9  68.5  1527.4  67.3  59.9  66.5\nAlignGPT  LLaMA3-8B-Base  336  79.6  63.1  70.4  1539.7  72.0  67.7  68.2\nImage resolution plays a crucial role in vision-language tasks as higher resolutions help reduce image blurring and enhance the understanding of image-text alignment. To evaluate the impact of resolution changes on the performance of multimodal tasks, we increase the image resolution from 336 to 1008, with the resulting performance changes detailed in Table 5.2  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###. The study results show that higher image resolutions can improve model performance on most multimodal tasks. For example, the score for VQAv2 increased from 79.1 to 79.8, while the score for TextVQA rose from 58.4 to 60.3. Meanwhile, the performance of the POPE improve by 0.8. These results highlight that appropriately increasing image resolution is an effective strategy for enhancing performance in studies of multimodal large language models.\nWe also explore the impact of the large language model on the performance of AlignGPT, specifically testing three models: LLaMA-2-7B-Chat, Vicuna-v1.5-7B, and the latest LLaMA-3-8B-Base. The results are shown in Table 5.2  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###. Initially, we observe that LLaMA-3-8B-Base achieves the best performance, followed by Vicuna-v1.5-7B, with LLaMA-2-7B-Chat performing the worst, which is reasonable given LLaMA-3-8B-Base\u2019s larger parameter size and richer training data. Besides, we find that compared to the VQA tasks, Vicuna-v1.5-7B performs better on multimodal benchmarks such as MME, MMB, and SEEDI than LLaMA-2-7B-Chat, possibly because Vicuna-v1.5-7B underwent supervised instruction-tuning with ShareGPT data, and ShareGPT contains some background knowledge related to downstream tasks.\nFigure 3  ###reference_###  ###reference_###  ###reference_### presents a comparative analysis of our model with MiniGPT-v2 DBLP:journals/corr/abs-2310-09478  ###reference_b5###  ###reference_b5###  ###reference_b5### and LLaVA-1.5 DBLP:journals/corr/abs-2310-03744  ###reference_b23###  ###reference_b23###  ###reference_b23###. When a user submits an image alongside the instruction \u201cCan you see the bright blue feathers on the bird\u2019s wing?\u201d, MiniGPT-v2 and LLaVA-1.5 both return an incorrect answer \u201cYes\u201d. In contrast, our model produces accurate result \u201cNo\u201d, thereby demonstrating that AlignGPT can effectively enhance the model\u2019s alignment capability. In Figure 4  ###reference_###  ###reference_###  ###reference_###, we further demonstrate the responses of AlignGPT under different levels of alignment capability. We find that with lower alignment levels, the model may only focus on certain regions of the image, resulting in an undercount of the total number of apples; whereas with higher alignment levels, the model considers the entire image area, thus achieving accurate apple quantity estimation. This finding once again underscores the necessity of enhancing the alignment capability of MLLMs.\n###figure_14### ###figure_15### ###figure_16### ###figure_17### ###figure_18### ###figure_19###"
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Ablation Study",
            "text": "Without loss of generality, we choose AlignGPT-7b for the ablation study to investigate the effects of different components.\nTo investigate the effect of the number of alignment levels  on AlignGPT, we vary the value of  in the range of [4, 10] with a step size of 2. Table  5.1  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2### shows the performance of AlignGPT with different  on nine datasets. Actually, AlignGPT can achieve good results at , and their performance remains stable as the number of alignment levels increases. Depending on the trajectory of the curve, their performance has an initial upward trend and then flattens out. These observations indicate that AlignGPT can improve the alignment capabilities of multi-modal large language models based on a small number of alignment levels. Finally, according to the trend of the curve, we set  to 8.\nDuring the instruction-tuning phase, we assign global alignment capabilities and local alignment capabilities to the instructions of each task. Here, we explore the role of separate global alignment capabilities and local alignment capabilities on AlignGPT. Among them, \u201cLocal\u201d refers to the local alignment capabilities derived by assigning different weights to various local alignment vectors using a gate network. \u201cGlobal\u201d denotes the global alignment capabilities, and \u201cAverage\u201d represents the local alignment capabilities obtained by assigning equal weights to each local alignment vector. The performance of these four strategies (settings a-d) is presented in Table 5.1  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###. As we can see, setting(a) and setting(b) demonstrate divergent performances in downstream tasks, which can be attributed to the different demands these tasks place on global and local alignment capabilities. It is worth noting that setting (a) and setting (b) perform worse than our final approach (setting d) on most datasets, which verifies the necessity and superiority of the combination of global and local alignment capabilities. Additionally, the performance of setting (c) is inferior to that of setting (d), a possible reason being that the demands for local alignment capabilities in each downstream task are dynamically variable.\nl|l|c|ccccccc\nMethod LLM Resolution VQAv2 GQA SQAI TextVQA POPE MMB SEEDI\nAlignGPT  Vicuna-7B  336  79.1  62.9  68.5  58.4  86.0  67.3  66.5\nAlignGPT  Vicuna-7B  672  79.7  63.3  68.3  60.3  86.8  67.2  66.5\nAlignGPT  Vicuna-7B  1008  79.8  63.4  68.2  60.3  86.8  67.2  66.6\nl|l|c|ccccccc\nMethod LLM Resolution VQAv2 GQA SQAI MME MMB MMBCN SEEDI\nAlignGPT  LLaMA2-7B-Chat  336  79.1  62.9  65.9  1500.8  66.6  57.9  66.4\nAlignGPT  Vicuna-7B  336  79.1  62.9  68.5  1527.4  67.3  59.9  66.5\nAlignGPT  LLaMA3-8B-Base  336  79.6  63.1  70.4  1539.7  72.0  67.7  68.2\nImage resolution plays a crucial role in vision-language tasks as higher resolutions help reduce image blurring and enhance the understanding of image-text alignment. To evaluate the impact of resolution changes on the performance of multimodal tasks, we increase the image resolution from 336 to 1008, with the resulting performance changes detailed in Table 5.2  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###. The study results show that higher image resolutions can improve model performance on most multimodal tasks. For example, the score for VQAv2 increased from 79.1 to 79.8, while the score for TextVQA rose from 58.4 to 60.3. Meanwhile, the performance of the POPE improve by 0.8. These results highlight that appropriately increasing image resolution is an effective strategy for enhancing performance in studies of multimodal large language models.\nWe also explore the impact of the large language model on the performance of AlignGPT, specifically testing three models: LLaMA-2-7B-Chat, Vicuna-v1.5-7B, and the latest LLaMA-3-8B-Base. The results are shown in Table 5.2  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###. Initially, we observe that LLaMA-3-8B-Base achieves the best performance, followed by Vicuna-v1.5-7B, with LLaMA-2-7B-Chat performing the worst, which is reasonable given LLaMA-3-8B-Base\u2019s larger parameter size and richer training data. Besides, we find that compared to the VQA tasks, Vicuna-v1.5-7B performs better on multimodal benchmarks such as MME, MMB, and SEEDI than LLaMA-2-7B-Chat, possibly because Vicuna-v1.5-7B underwent supervised instruction-tuning with ShareGPT data, and ShareGPT contains some background knowledge related to downstream tasks.\nFigure 3  ###reference_###  ###reference_###  ###reference_###  ###reference_### presents a comparative analysis of our model with MiniGPT-v2 DBLP:journals/corr/abs-2310-09478  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5### and LLaVA-1.5 DBLP:journals/corr/abs-2310-03744  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###. When a user submits an image alongside the instruction \u201cCan you see the bright blue feathers on the bird\u2019s wing?\u201d, MiniGPT-v2 and LLaVA-1.5 both return an incorrect answer \u201cYes\u201d. In contrast, our model produces accurate result \u201cNo\u201d, thereby demonstrating that AlignGPT can effectively enhance the model\u2019s alignment capability. In Figure 4  ###reference_###  ###reference_###  ###reference_###  ###reference_###, we further demonstrate the responses of AlignGPT under different levels of alignment capability. We find that with lower alignment levels, the model may only focus on certain regions of the image, resulting in an undercount of the total number of apples; whereas with higher alignment levels, the model considers the entire image area, thus achieving accurate apple quantity estimation. This finding once again underscores the necessity of enhancing the alignment capability of MLLMs.\n###figure_20### ###figure_21### ###figure_22### ###figure_23### ###figure_24### ###figure_25###"
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Discussion",
            "text": "Image resolution plays a crucial role in vision-language tasks as higher resolutions help reduce image blurring and enhance the understanding of image-text alignment. To evaluate the impact of resolution changes on the performance of multimodal tasks, we increase the image resolution from 336 to 1008, with the resulting performance changes detailed in Table 5.2  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###. The study results show that higher image resolutions can improve model performance on most multimodal tasks. For example, the score for VQAv2 increased from 79.1 to 79.8, while the score for TextVQA rose from 58.4 to 60.3. Meanwhile, the performance of the POPE improve by 0.8. These results highlight that appropriately increasing image resolution is an effective strategy for enhancing performance in studies of multimodal large language models.\nWe also explore the impact of the large language model on the performance of AlignGPT, specifically testing three models: LLaMA-2-7B-Chat, Vicuna-v1.5-7B, and the latest LLaMA-3-8B-Base. The results are shown in Table 5.2  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###. Initially, we observe that LLaMA-3-8B-Base achieves the best performance, followed by Vicuna-v1.5-7B, with LLaMA-2-7B-Chat performing the worst, which is reasonable given LLaMA-3-8B-Base\u2019s larger parameter size and richer training data. Besides, we find that compared to the VQA tasks, Vicuna-v1.5-7B performs better on multimodal benchmarks such as MME, MMB, and SEEDI than LLaMA-2-7B-Chat, possibly because Vicuna-v1.5-7B underwent supervised instruction-tuning with ShareGPT data, and ShareGPT contains some background knowledge related to downstream tasks.\nFigure 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### presents a comparative analysis of our model with MiniGPT-v2 DBLP:journals/corr/abs-2310-09478  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5### and LLaVA-1.5 DBLP:journals/corr/abs-2310-03744  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###. When a user submits an image alongside the instruction \u201cCan you see the bright blue feathers on the bird\u2019s wing?\u201d, MiniGPT-v2 and LLaVA-1.5 both return an incorrect answer \u201cYes\u201d. In contrast, our model produces accurate result \u201cNo\u201d, thereby demonstrating that AlignGPT can effectively enhance the model\u2019s alignment capability. In Figure 4  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, we further demonstrate the responses of AlignGPT under different levels of alignment capability. We find that with lower alignment levels, the model may only focus on certain regions of the image, resulting in an undercount of the total number of apples; whereas with higher alignment levels, the model considers the entire image area, thus achieving accurate apple quantity estimation. This finding once again underscores the necessity of enhancing the alignment capability of MLLMs.\n###figure_26### ###figure_27### ###figure_28### ###figure_29### ###figure_30### ###figure_31###"
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Qualitative Results",
            "text": "Figure 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### presents a comparative analysis of our model with MiniGPT-v2 DBLP:journals/corr/abs-2310-09478  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5### and LLaVA-1.5 DBLP:journals/corr/abs-2310-03744  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###. When a user submits an image alongside the instruction \u201cCan you see the bright blue feathers on the bird\u2019s wing?\u201d, MiniGPT-v2 and LLaVA-1.5 both return an incorrect answer \u201cYes\u201d. In contrast, our model produces accurate result \u201cNo\u201d, thereby demonstrating that AlignGPT can effectively enhance the model\u2019s alignment capability. In Figure 4  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, we further demonstrate the responses of AlignGPT under different levels of alignment capability. We find that with lower alignment levels, the model may only focus on certain regions of the image, resulting in an undercount of the total number of apples; whereas with higher alignment levels, the model considers the entire image area, thus achieving accurate apple quantity estimation. This finding once again underscores the necessity of enhancing the alignment capability of MLLMs.\n###figure_32### ###figure_33### ###figure_34### ###figure_35### ###figure_36### ###figure_37###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "For a fair comparison, we use the same pre-training and instruction dataset as the LLaVA-1.5 DBLP:journals/corr/abs-2310-03744  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###. It mainly includes 558K caption pairs for modality alignment and 665K single- or multi-round conversations for instruction-tuning. Besides, we evaluate AlignGPT on a range of academic visual question answering (VQA) tasks and recent benchmarks designed specifically for MLLMs. This evaluation spans 12 benchmarks, including VQAV2 DBLP:conf/cvpr/GoyalKSBP17  ###reference_b15###  ###reference_b15###  ###reference_b15###  ###reference_b15###, GQA DBLP:conf/cvpr/Gurari0SGLGLB18  ###reference_b17###  ###reference_b17###  ###reference_b17###  ###reference_b17###, VizWiz DBLP:conf/cvpr/Gurari0SGLGLB18  ###reference_b17###  ###reference_b17###  ###reference_b17###  ###reference_b17###, SQAI (ScienceQA-IMG) DBLP:conf/nips/LuMX0CZTCK22  ###reference_b27###  ###reference_b27###  ###reference_b27###  ###reference_b27###, TextVQA DBLP:conf/cvpr/SinghNSJCBPR19  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###, POPE DBLP:conf/emnlp/LiDZWZW23  ###reference_b22###  ###reference_b22###  ###reference_b22###  ###reference_b22###, MME DBLP:journals/corr/abs-2306-13394  ###reference_b14###  ###reference_b14###  ###reference_b14###  ###reference_b14###, MMB (MMBench), MMBCN (MMBench-Chinese) DBLP:journals/corr/abs-2307-06281  ###reference_b25###  ###reference_b25###  ###reference_b25###  ###reference_b25###, SEEDI (SEED-Bench-IMG) DBLP:journals/corr/abs-2307-16125  ###reference_b20###  ###reference_b20###  ###reference_b20###  ###reference_b20###, LLaVAW (LLaVA-Bench-in-the-Wild) DBLP:conf/nips/LiuLWL23a  ###reference_b24###  ###reference_b24###  ###reference_b24###  ###reference_b24###, and MM-Vet DBLP:journals/corr/abs-2308-02490  ###reference_b43###  ###reference_b43###  ###reference_b43###  ###reference_b43### datasets.\nWe adopt a ViT dosovitskiy2021an  ###reference_b11###  ###reference_b11###  ###reference_b11###  ###reference_b11### model pre-trained with CLIP DBLP:conf/icml/RadfordKHRGASAM21  ###reference_b32###  ###reference_b32###  ###reference_b32###  ###reference_b32### as a vision encoder to effectively process visual inputs. On the language side, Vicuna chiang2023vicuna  ###reference_b8###  ###reference_b8###  ###reference_b8###  ###reference_b8### is utilized to handle multimodal features, ensuring a cohesive integration of text and visual data. In the pre-training phase, both the visual backbone and the large language model of AlignGPT remain frozen, with only the parameters of the linear projection layer and alignment vectors being trained. During instruction-tuning phase, we freeze the linear projection layer, alignment vectors, and visual backbone, while adjusting the parameters of the large language model and the gate network. The global batch sizes for the two phases are set at 256 and 128 respectively, with DeepSpeed DBLP:conf/sc/RajbhandariRRH20  ###reference_b34###  ###reference_b34###  ###reference_b34###  ###reference_b34### using ZeRO2 and ZeRO3 strategies accordingly. Regarding our training methodology, we conduct a single epoch of optimization for all models using the AdamW loshchilov2018decoupled  ###reference_b26###  ###reference_b26###  ###reference_b26###  ###reference_b26### optimizer coupled with a cosine learning schedule. Moreover, we initiate pre-training and instruction-tuning with learning rates of 1e-3 and 2e-5, respectively. The framework is trained on 8 A800 GPUs with 80GB memory.\nWe evaluate AlignGPT using five popular academic benchmarks, as detailed in Table 3.3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. Despite using less training data, the AlignGPT-7B demonstrates competitive performance, surpassing other generalist models including InstructBLIP-13B, Shikra-13B, and IDEFICS-80B on most datasets, except for LLaVA-1.5-13B. These results verify the rationality of the structural design of our model. Moreover, considering that AlignGPT utilizes the same training dataset as LLaVA-1.5, it is evident that AlignGPT-7B outperforms LLaVA-1.5-7B across all evaluation datasets, and AlignGPT-13B also surpasses LLaVA-1.5-13B on the majority of datasets. This demonstrates that our approach has effectively enhance the alignment capabilities of multimodal large language models. The fly in the ointment is that AlignGPT-13B does not perform as well as Qwen-VL on the TextVQA dataset. This may stem from the fact that TextVQA is a text-centric QA task, as it requires identifying text in images to answer questions. AlignGPT is tailored to boost multimodal alignment and might not exhibit strong results in text-focused scenarios.\nWe apply AlignGPT to seven recent popular multimodal benchmarks, as shown in Table 3.3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. We discover that, apart from LLaVA-1.5-13B, AlignGPT-7B surpassed all previous multimodal models. This shows that our model has strong generalization ability. Additionally, compared to LLaVA-1.5-13B, AlignGPT-13B has shown improvements on most datasets, particularly achieving good advancements on the MME, MMB, and LLaVAW benchamrks. This further validates the efficacy of both global and local alignment capabilities.\nl|c|ccccccccc\nMethod Alignment Level VQAv2 GQA VisWiz SQAI TextVQA POPE MME MMB SEEDI\nAlignGPT  Number=4  79.0  62.9  52.3  68.7  58.3  86.2  1463.8  67.2  66.5\nAlignGPT  Number=6  79.0  62.7  51.2  68.9  58.3  85.8  1436.3  67.3  66.2\nAlignGPT  Number=8  79.1  62.9  54.2  68.5  58.4  86.0  1527.4  67.3  66.5\nAlignGPT  Number=10  79.1  62.6  53.0  67.8  58.4  86.2  1481.4  66.4  66.7\nc|ccc|cccccccccc\nSettings Average Local Global VQAv2 GQA VisWiz SQAI TextVQA POPE MME MMB SEEDI\n(a)  \u2718 \u2714 \u2718 79.1  62.7  53.3  67.9  58.6  85.9  1467.1  66.9  66.3\n(b)  \u2718 \u2718 \u2714 79.1  62.9  52.6  68.3  58.4  85.9  1502.9  66.3  66.2\n(c)  \u2714 \u2718 \u2714 79.0  62.8  52.5  68.6  58.4  85.6  1492.5  67.0  66.0\n(d)  \u2718 \u2714 \u2714 79.1  62.9  54.2  68.5  58.4  86.0  1527.4  67.3  66.5\nTo investigate the effect of the number of alignment levels  on AlignGPT, we vary the value of  in the range of [4, 10] with a step size of 2. Table  5.1  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2### shows the performance of AlignGPT with different  on nine datasets. Actually, AlignGPT can achieve good results at , and their performance remains stable as the number of alignment levels increases. Depending on the trajectory of the curve, their performance has an initial upward trend and then flattens out. These observations indicate that AlignGPT can improve the alignment capabilities of multi-modal large language models based on a small number of alignment levels. Finally, according to the trend of the curve, we set  to 8.\nDuring the instruction-tuning phase, we assign global alignment capabilities and local alignment capabilities to the instructions of each task. Here, we explore the role of separate global alignment capabilities and local alignment capabilities on AlignGPT. Among them, \u201cLocal\u201d refers to the local alignment capabilities derived by assigning different weights to various local alignment vectors using a gate network. \u201cGlobal\u201d denotes the global alignment capabilities, and \u201cAverage\u201d represents the local alignment capabilities obtained by assigning equal weights to each local alignment vector. The performance of these four strategies (settings a-d) is presented in Table 5.1  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###. As we can see, setting(a) and setting(b) demonstrate divergent performances in downstream tasks, which can be attributed to the different demands these tasks place on global and local alignment capabilities. It is worth noting that setting (a) and setting (b) perform worse than our final approach (setting d) on most datasets, which verifies the necessity and superiority of the combination of global and local alignment capabilities. Additionally, the performance of setting (c) is inferior to that of setting (d), a possible reason being that the demands for local alignment capabilities in each downstream task are dynamically variable.\nl|l|c|ccccccc\nMethod LLM Resolution VQAv2 GQA SQAI TextVQA POPE MMB SEEDI\nAlignGPT  Vicuna-7B  336  79.1  62.9  68.5  58.4  86.0  67.3  66.5\nAlignGPT  Vicuna-7B  672  79.7  63.3  68.3  60.3  86.8  67.2  66.5\nAlignGPT  Vicuna-7B  1008  79.8  63.4  68.2  60.3  86.8  67.2  66.6\nl|l|c|ccccccc\nMethod LLM Resolution VQAv2 GQA SQAI MME MMB MMBCN SEEDI\nAlignGPT  LLaMA2-7B-Chat  336  79.1  62.9  65.9  1500.8  66.6  57.9  66.4\nAlignGPT  Vicuna-7B  336  79.1  62.9  68.5  1527.4  67.3  59.9  66.5\nAlignGPT  LLaMA3-8B-Base  336  79.6  63.1  70.4  1539.7  72.0  67.7  68.2\nImage resolution plays a crucial role in vision-language tasks as higher resolutions help reduce image blurring and enhance the understanding of image-text alignment. To evaluate the impact of resolution changes on the performance of multimodal tasks, we increase the image resolution from 336 to 1008, with the resulting performance changes detailed in Table 5.2  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###. The study results show that higher image resolutions can improve model performance on most multimodal tasks. For example, the score for VQAv2 increased from 79.1 to 79.8, while the score for TextVQA rose from 58.4 to 60.3. Meanwhile, the performance of the POPE improve by 0.8. These results highlight that appropriately increasing image resolution is an effective strategy for enhancing performance in studies of multimodal large language models.\nWe also explore the impact of the large language model on the performance of AlignGPT, specifically testing three models: LLaMA-2-7B-Chat, Vicuna-v1.5-7B, and the latest LLaMA-3-8B-Base. The results are shown in Table 5.2  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###. Initially, we observe that LLaMA-3-8B-Base achieves the best performance, followed by Vicuna-v1.5-7B, with LLaMA-2-7B-Chat performing the worst, which is reasonable given LLaMA-3-8B-Base\u2019s larger parameter size and richer training data. Besides, we find that compared to the VQA tasks, Vicuna-v1.5-7B performs better on multimodal benchmarks such as MME, MMB, and SEEDI than LLaMA-2-7B-Chat, possibly because Vicuna-v1.5-7B underwent supervised instruction-tuning with ShareGPT data, and ShareGPT contains some background knowledge related to downstream tasks.\nIn this paper, we propose AlignGPT, a novel multimodal large language model designed to bolster the alignment capabilities of MLLMs. Our approach involves utilizing the alignment level of data as a control signal during pre-training to effectively handle the varying degrees of alignment in image-text pairs. Subsequently, in the instruction-tuning phase, we begin by exploiting these control signals to shape different levels of alignment capabilities. Continuing from this, we go beyond assigning global alignment capabilities to instructions of each task; we also dynamically configure distinct local alignment capabilities based on the specific demands of each instruction. Results from numerous experiments indicate that our AlignGPT achieves better performance than other state-of-the-art MLLMs. Further analysis validates the superiority of our AlignGPT."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experimental Settings",
            "text": "For a fair comparison, we use the same pre-training and instruction dataset as the LLaVA-1.5 DBLP:journals/corr/abs-2310-03744  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###. It mainly includes 558K caption pairs for modality alignment and 665K single- or multi-round conversations for instruction-tuning. Besides, we evaluate AlignGPT on a range of academic visual question answering (VQA) tasks and recent benchmarks designed specifically for MLLMs. This evaluation spans 12 benchmarks, including VQAV2 DBLP:conf/cvpr/GoyalKSBP17  ###reference_b15###  ###reference_b15###  ###reference_b15###  ###reference_b15###  ###reference_b15###, GQA DBLP:conf/cvpr/Gurari0SGLGLB18  ###reference_b17###  ###reference_b17###  ###reference_b17###  ###reference_b17###  ###reference_b17###, VizWiz DBLP:conf/cvpr/Gurari0SGLGLB18  ###reference_b17###  ###reference_b17###  ###reference_b17###  ###reference_b17###  ###reference_b17###, SQAI (ScienceQA-IMG) DBLP:conf/nips/LuMX0CZTCK22  ###reference_b27###  ###reference_b27###  ###reference_b27###  ###reference_b27###  ###reference_b27###, TextVQA DBLP:conf/cvpr/SinghNSJCBPR19  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###  ###reference_b36###, POPE DBLP:conf/emnlp/LiDZWZW23  ###reference_b22###  ###reference_b22###  ###reference_b22###  ###reference_b22###  ###reference_b22###, MME DBLP:journals/corr/abs-2306-13394  ###reference_b14###  ###reference_b14###  ###reference_b14###  ###reference_b14###  ###reference_b14###, MMB (MMBench), MMBCN (MMBench-Chinese) DBLP:journals/corr/abs-2307-06281  ###reference_b25###  ###reference_b25###  ###reference_b25###  ###reference_b25###  ###reference_b25###, SEEDI (SEED-Bench-IMG) DBLP:journals/corr/abs-2307-16125  ###reference_b20###  ###reference_b20###  ###reference_b20###  ###reference_b20###  ###reference_b20###, LLaVAW (LLaVA-Bench-in-the-Wild) DBLP:conf/nips/LiuLWL23a  ###reference_b24###  ###reference_b24###  ###reference_b24###  ###reference_b24###  ###reference_b24###, and MM-Vet DBLP:journals/corr/abs-2308-02490  ###reference_b43###  ###reference_b43###  ###reference_b43###  ###reference_b43###  ###reference_b43### datasets.\nWe adopt a ViT dosovitskiy2021an  ###reference_b11###  ###reference_b11###  ###reference_b11###  ###reference_b11###  ###reference_b11### model pre-trained with CLIP DBLP:conf/icml/RadfordKHRGASAM21  ###reference_b32###  ###reference_b32###  ###reference_b32###  ###reference_b32###  ###reference_b32### as a vision encoder to effectively process visual inputs. On the language side, Vicuna chiang2023vicuna  ###reference_b8###  ###reference_b8###  ###reference_b8###  ###reference_b8###  ###reference_b8### is utilized to handle multimodal features, ensuring a cohesive integration of text and visual data. In the pre-training phase, both the visual backbone and the large language model of AlignGPT remain frozen, with only the parameters of the linear projection layer and alignment vectors being trained. During instruction-tuning phase, we freeze the linear projection layer, alignment vectors, and visual backbone, while adjusting the parameters of the large language model and the gate network. The global batch sizes for the two phases are set at 256 and 128 respectively, with DeepSpeed DBLP:conf/sc/RajbhandariRRH20  ###reference_b34###  ###reference_b34###  ###reference_b34###  ###reference_b34###  ###reference_b34### using ZeRO2 and ZeRO3 strategies accordingly. Regarding our training methodology, we conduct a single epoch of optimization for all models using the AdamW loshchilov2018decoupled  ###reference_b26###  ###reference_b26###  ###reference_b26###  ###reference_b26###  ###reference_b26### optimizer coupled with a cosine learning schedule. Moreover, we initiate pre-training and instruction-tuning with learning rates of 1e-3 and 2e-5, respectively. The framework is trained on 8 A800 GPUs with 80GB memory."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Compared Methods",
            "text": "We chose a diverse set of representative multimodal large language models (MLLMs) as our baselines, including BLIP-2 DBLP:conf/icml/0008LSH23  ###reference_b21###  ###reference_b21###  ###reference_b21###, InstructBLIP DBLP:conf/nips/Dai0LTZW0FH23  ###reference_b9###  ###reference_b9###  ###reference_b9###, Shikra DBLP:journals/corr/abs-2306-15195  ###reference_b6###  ###reference_b6###  ###reference_b6###, IDEFICS DBLP:conf/nips/LaurenconSTBSLW23  ###reference_b19###  ###reference_b19###  ###reference_b19###, MiniGPT-v2 DBLP:journals/corr/abs-2310-09478  ###reference_b5###  ###reference_b5###  ###reference_b5###, Qwen-VL DBLP:journals/corr/abs-2308-12966  ###reference_b3###  ###reference_b3###  ###reference_b3###, Qwen-VL-Chat DBLP:journals/corr/abs-2308-12966  ###reference_b3###  ###reference_b3###  ###reference_b3###, and LLaVA-1.5 DBLP:journals/corr/abs-2310-03744  ###reference_b23###  ###reference_b23###  ###reference_b23###.\nWe evaluate AlignGPT using five popular academic benchmarks, as detailed in Table 3.3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. Despite using less training data, the AlignGPT-7B demonstrates competitive performance, surpassing other generalist models including InstructBLIP-13B, Shikra-13B, and IDEFICS-80B on most datasets, except for LLaVA-1.5-13B. These results verify the rationality of the structural design of our model. Moreover, considering that AlignGPT utilizes the same training dataset as LLaVA-1.5, it is evident that AlignGPT-7B outperforms LLaVA-1.5-7B across all evaluation datasets, and AlignGPT-13B also surpasses LLaVA-1.5-13B on the majority of datasets. This demonstrates that our approach has effectively enhance the alignment capabilities of multimodal large language models. The fly in the ointment is that AlignGPT-13B does not perform as well as Qwen-VL on the TextVQA dataset. This may stem from the fact that TextVQA is a text-centric QA task, as it requires identifying text in images to answer questions. AlignGPT is tailored to boost multimodal alignment and might not exhibit strong results in text-focused scenarios.\nWe apply AlignGPT to seven recent popular multimodal benchmarks, as shown in Table 3.3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. We discover that, apart from LLaVA-1.5-13B, AlignGPT-7B surpassed all previous multimodal models. This shows that our model has strong generalization ability. Additionally, compared to LLaVA-1.5-13B, AlignGPT-13B has shown improvements on most datasets, particularly achieving good advancements on the MME, MMB, and LLaVAW benchamrks. This further validates the efficacy of both global and local alignment capabilities.\nl|c|ccccccccc\nMethod Alignment Level VQAv2 GQA VisWiz SQAI TextVQA POPE MME MMB SEEDI\nAlignGPT  Number=4  79.0  62.9  52.3  68.7  58.3  86.2  1463.8  67.2  66.5\nAlignGPT  Number=6  79.0  62.7  51.2  68.9  58.3  85.8  1436.3  67.3  66.2\nAlignGPT  Number=8  79.1  62.9  54.2  68.5  58.4  86.0  1527.4  67.3  66.5\nAlignGPT  Number=10  79.1  62.6  53.0  67.8  58.4  86.2  1481.4  66.4  66.7\nc|ccc|cccccccccc\nSettings Average Local Global VQAv2 GQA VisWiz SQAI TextVQA POPE MME MMB SEEDI\n(a)  \u2718 \u2714 \u2718 79.1  62.7  53.3  67.9  58.6  85.9  1467.1  66.9  66.3\n(b)  \u2718 \u2718 \u2714 79.1  62.9  52.6  68.3  58.4  85.9  1502.9  66.3  66.2\n(c)  \u2714 \u2718 \u2714 79.0  62.8  52.5  68.6  58.4  85.6  1492.5  67.0  66.0\n(d)  \u2718 \u2714 \u2714 79.1  62.9  54.2  68.5  58.4  86.0  1527.4  67.3  66.5\nWithout loss of generality, we choose AlignGPT-7b for the ablation study to investigate the effects of different components.\nTo investigate the effect of the number of alignment levels  on AlignGPT, we vary the value of  in the range of [4, 10] with a step size of 2. Table  5.1  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2### shows the performance of AlignGPT with different  on nine datasets. Actually, AlignGPT can achieve good results at , and their performance remains stable as the number of alignment levels increases. Depending on the trajectory of the curve, their performance has an initial upward trend and then flattens out. These observations indicate that AlignGPT can improve the alignment capabilities of multi-modal large language models based on a small number of alignment levels. Finally, according to the trend of the curve, we set  to 8.\nDuring the instruction-tuning phase, we assign global alignment capabilities and local alignment capabilities to the instructions of each task. Here, we explore the role of separate global alignment capabilities and local alignment capabilities on AlignGPT. Among them, \u201cLocal\u201d refers to the local alignment capabilities derived by assigning different weights to various local alignment vectors using a gate network. \u201cGlobal\u201d denotes the global alignment capabilities, and \u201cAverage\u201d represents the local alignment capabilities obtained by assigning equal weights to each local alignment vector. The performance of these four strategies (settings a-d) is presented in Table 5.1  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###. As we can see, setting(a) and setting(b) demonstrate divergent performances in downstream tasks, which can be attributed to the different demands these tasks place on global and local alignment capabilities. It is worth noting that setting (a) and setting (b) perform worse than our final approach (setting d) on most datasets, which verifies the necessity and superiority of the combination of global and local alignment capabilities. Additionally, the performance of setting (c) is inferior to that of setting (d), a possible reason being that the demands for local alignment capabilities in each downstream task are dynamically variable.\nl|l|c|ccccccc\nMethod LLM Resolution VQAv2 GQA SQAI TextVQA POPE MMB SEEDI\nAlignGPT  Vicuna-7B  336  79.1  62.9  68.5  58.4  86.0  67.3  66.5\nAlignGPT  Vicuna-7B  672  79.7  63.3  68.3  60.3  86.8  67.2  66.5\nAlignGPT  Vicuna-7B  1008  79.8  63.4  68.2  60.3  86.8  67.2  66.6\nl|l|c|ccccccc\nMethod LLM Resolution VQAv2 GQA SQAI MME MMB MMBCN SEEDI\nAlignGPT  LLaMA2-7B-Chat  336  79.1  62.9  65.9  1500.8  66.6  57.9  66.4\nAlignGPT  Vicuna-7B  336  79.1  62.9  68.5  1527.4  67.3  59.9  66.5\nAlignGPT  LLaMA3-8B-Base  336  79.6  63.1  70.4  1539.7  72.0  67.7  68.2\nImage resolution plays a crucial role in vision-language tasks as higher resolutions help reduce image blurring and enhance the understanding of image-text alignment. To evaluate the impact of resolution changes on the performance of multimodal tasks, we increase the image resolution from 336 to 1008, with the resulting performance changes detailed in Table 5.2  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###. The study results show that higher image resolutions can improve model performance on most multimodal tasks. For example, the score for VQAv2 increased from 79.1 to 79.8, while the score for TextVQA rose from 58.4 to 60.3. Meanwhile, the performance of the POPE improve by 0.8. These results highlight that appropriately increasing image resolution is an effective strategy for enhancing performance in studies of multimodal large language models.\nWe also explore the impact of the large language model on the performance of AlignGPT, specifically testing three models: LLaMA-2-7B-Chat, Vicuna-v1.5-7B, and the latest LLaMA-3-8B-Base. The results are shown in Table 5.2  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###. Initially, we observe that LLaMA-3-8B-Base achieves the best performance, followed by Vicuna-v1.5-7B, with LLaMA-2-7B-Chat performing the worst, which is reasonable given LLaMA-3-8B-Base\u2019s larger parameter size and richer training data. Besides, we find that compared to the VQA tasks, Vicuna-v1.5-7B performs better on multimodal benchmarks such as MME, MMB, and SEEDI than LLaMA-2-7B-Chat, possibly because Vicuna-v1.5-7B underwent supervised instruction-tuning with ShareGPT data, and ShareGPT contains some background knowledge related to downstream tasks.\nFigure 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### presents a comparative analysis of our model with MiniGPT-v2 DBLP:journals/corr/abs-2310-09478  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5### and LLaVA-1.5 DBLP:journals/corr/abs-2310-03744  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###. When a user submits an image alongside the instruction \u201cCan you see the bright blue feathers on the bird\u2019s wing?\u201d, MiniGPT-v2 and LLaVA-1.5 both return an incorrect answer \u201cYes\u201d. In contrast, our model produces accurate result \u201cNo\u201d, thereby demonstrating that AlignGPT can effectively enhance the model\u2019s alignment capability. In Figure 4  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, we further demonstrate the responses of AlignGPT under different levels of alignment capability. We find that with lower alignment levels, the model may only focus on certain regions of the image, resulting in an undercount of the total number of apples; whereas with higher alignment levels, the model considers the entire image area, thus achieving accurate apple quantity estimation. This finding once again underscores the necessity of enhancing the alignment capability of MLLMs.\n###figure_38### ###figure_39### ###figure_40### ###figure_41### ###figure_42### ###figure_43###"
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Main Results",
            "text": "We evaluate AlignGPT using five popular academic benchmarks, as detailed in Table 3.3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. Despite using less training data, the AlignGPT-7B demonstrates competitive performance, surpassing other generalist models including InstructBLIP-13B, Shikra-13B, and IDEFICS-80B on most datasets, except for LLaVA-1.5-13B. These results verify the rationality of the structural design of our model. Moreover, considering that AlignGPT utilizes the same training dataset as LLaVA-1.5, it is evident that AlignGPT-7B outperforms LLaVA-1.5-7B across all evaluation datasets, and AlignGPT-13B also surpasses LLaVA-1.5-13B on the majority of datasets. This demonstrates that our approach has effectively enhance the alignment capabilities of multimodal large language models. The fly in the ointment is that AlignGPT-13B does not perform as well as Qwen-VL on the TextVQA dataset. This may stem from the fact that TextVQA is a text-centric QA task, as it requires identifying text in images to answer questions. AlignGPT is tailored to boost multimodal alignment and might not exhibit strong results in text-focused scenarios.\nWe apply AlignGPT to seven recent popular multimodal benchmarks, as shown in Table 3.3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. We discover that, apart from LLaVA-1.5-13B, AlignGPT-7B surpassed all previous multimodal models. This shows that our model has strong generalization ability. Additionally, compared to LLaVA-1.5-13B, AlignGPT-13B has shown improvements on most datasets, particularly achieving good advancements on the MME, MMB, and LLaVAW benchamrks. This further validates the efficacy of both global and local alignment capabilities.\nl|c|ccccccccc\nMethod Alignment Level VQAv2 GQA VisWiz SQAI TextVQA POPE MME MMB SEEDI\nAlignGPT  Number=4  79.0  62.9  52.3  68.7  58.3  86.2  1463.8  67.2  66.5\nAlignGPT  Number=6  79.0  62.7  51.2  68.9  58.3  85.8  1436.3  67.3  66.2\nAlignGPT  Number=8  79.1  62.9  54.2  68.5  58.4  86.0  1527.4  67.3  66.5\nAlignGPT  Number=10  79.1  62.6  53.0  67.8  58.4  86.2  1481.4  66.4  66.7\nc|ccc|cccccccccc\nSettings Average Local Global VQAv2 GQA VisWiz SQAI TextVQA POPE MME MMB SEEDI\n(a)  \u2718 \u2714 \u2718 79.1  62.7  53.3  67.9  58.6  85.9  1467.1  66.9  66.3\n(b)  \u2718 \u2718 \u2714 79.1  62.9  52.6  68.3  58.4  85.9  1502.9  66.3  66.2\n(c)  \u2714 \u2718 \u2714 79.0  62.8  52.5  68.6  58.4  85.6  1492.5  67.0  66.0\n(d)  \u2718 \u2714 \u2714 79.1  62.9  54.2  68.5  58.4  86.0  1527.4  67.3  66.5\nWithout loss of generality, we choose AlignGPT-7b for the ablation study to investigate the effects of different components.\nTo investigate the effect of the number of alignment levels  on AlignGPT, we vary the value of  in the range of [4, 10] with a step size of 2. Table  5.1  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2### shows the performance of AlignGPT with different  on nine datasets. Actually, AlignGPT can achieve good results at , and their performance remains stable as the number of alignment levels increases. Depending on the trajectory of the curve, their performance has an initial upward trend and then flattens out. These observations indicate that AlignGPT can improve the alignment capabilities of multi-modal large language models based on a small number of alignment levels. Finally, according to the trend of the curve, we set  to 8.\nDuring the instruction-tuning phase, we assign global alignment capabilities and local alignment capabilities to the instructions of each task. Here, we explore the role of separate global alignment capabilities and local alignment capabilities on AlignGPT. Among them, \u201cLocal\u201d refers to the local alignment capabilities derived by assigning different weights to various local alignment vectors using a gate network. \u201cGlobal\u201d denotes the global alignment capabilities, and \u201cAverage\u201d represents the local alignment capabilities obtained by assigning equal weights to each local alignment vector. The performance of these four strategies (settings a-d) is presented in Table 5.1  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###. As we can see, setting(a) and setting(b) demonstrate divergent performances in downstream tasks, which can be attributed to the different demands these tasks place on global and local alignment capabilities. It is worth noting that setting (a) and setting (b) perform worse than our final approach (setting d) on most datasets, which verifies the necessity and superiority of the combination of global and local alignment capabilities. Additionally, the performance of setting (c) is inferior to that of setting (d), a possible reason being that the demands for local alignment capabilities in each downstream task are dynamically variable.\nl|l|c|ccccccc\nMethod LLM Resolution VQAv2 GQA SQAI TextVQA POPE MMB SEEDI\nAlignGPT  Vicuna-7B  336  79.1  62.9  68.5  58.4  86.0  67.3  66.5\nAlignGPT  Vicuna-7B  672  79.7  63.3  68.3  60.3  86.8  67.2  66.5\nAlignGPT  Vicuna-7B  1008  79.8  63.4  68.2  60.3  86.8  67.2  66.6\nl|l|c|ccccccc\nMethod LLM Resolution VQAv2 GQA SQAI MME MMB MMBCN SEEDI\nAlignGPT  LLaMA2-7B-Chat  336  79.1  62.9  65.9  1500.8  66.6  57.9  66.4\nAlignGPT  Vicuna-7B  336  79.1  62.9  68.5  1527.4  67.3  59.9  66.5\nAlignGPT  LLaMA3-8B-Base  336  79.6  63.1  70.4  1539.7  72.0  67.7  68.2\nImage resolution plays a crucial role in vision-language tasks as higher resolutions help reduce image blurring and enhance the understanding of image-text alignment. To evaluate the impact of resolution changes on the performance of multimodal tasks, we increase the image resolution from 336 to 1008, with the resulting performance changes detailed in Table 5.2  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###. The study results show that higher image resolutions can improve model performance on most multimodal tasks. For example, the score for VQAv2 increased from 79.1 to 79.8, while the score for TextVQA rose from 58.4 to 60.3. Meanwhile, the performance of the POPE improve by 0.8. These results highlight that appropriately increasing image resolution is an effective strategy for enhancing performance in studies of multimodal large language models.\nWe also explore the impact of the large language model on the performance of AlignGPT, specifically testing three models: LLaMA-2-7B-Chat, Vicuna-v1.5-7B, and the latest LLaMA-3-8B-Base. The results are shown in Table 5.2  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###. Initially, we observe that LLaMA-3-8B-Base achieves the best performance, followed by Vicuna-v1.5-7B, with LLaMA-2-7B-Chat performing the worst, which is reasonable given LLaMA-3-8B-Base\u2019s larger parameter size and richer training data. Besides, we find that compared to the VQA tasks, Vicuna-v1.5-7B performs better on multimodal benchmarks such as MME, MMB, and SEEDI than LLaMA-2-7B-Chat, possibly because Vicuna-v1.5-7B underwent supervised instruction-tuning with ShareGPT data, and ShareGPT contains some background knowledge related to downstream tasks.\nFigure 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### presents a comparative analysis of our model with MiniGPT-v2 DBLP:journals/corr/abs-2310-09478  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5### and LLaVA-1.5 DBLP:journals/corr/abs-2310-03744  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###. When a user submits an image alongside the instruction \u201cCan you see the bright blue feathers on the bird\u2019s wing?\u201d, MiniGPT-v2 and LLaVA-1.5 both return an incorrect answer \u201cYes\u201d. In contrast, our model produces accurate result \u201cNo\u201d, thereby demonstrating that AlignGPT can effectively enhance the model\u2019s alignment capability. In Figure 4  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, we further demonstrate the responses of AlignGPT under different levels of alignment capability. We find that with lower alignment levels, the model may only focus on certain regions of the image, resulting in an undercount of the total number of apples; whereas with higher alignment levels, the model considers the entire image area, thus achieving accurate apple quantity estimation. This finding once again underscores the necessity of enhancing the alignment capability of MLLMs.\n###figure_44### ###figure_45### ###figure_46### ###figure_47### ###figure_48### ###figure_49###"
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Ablation Study",
            "text": "Without loss of generality, we choose AlignGPT-7b for the ablation study to investigate the effects of different components.\nTo investigate the effect of the number of alignment levels  on AlignGPT, we vary the value of  in the range of [4, 10] with a step size of 2. Table  5.1  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2### shows the performance of AlignGPT with different  on nine datasets. Actually, AlignGPT can achieve good results at , and their performance remains stable as the number of alignment levels increases. Depending on the trajectory of the curve, their performance has an initial upward trend and then flattens out. These observations indicate that AlignGPT can improve the alignment capabilities of multi-modal large language models based on a small number of alignment levels. Finally, according to the trend of the curve, we set  to 8.\nDuring the instruction-tuning phase, we assign global alignment capabilities and local alignment capabilities to the instructions of each task. Here, we explore the role of separate global alignment capabilities and local alignment capabilities on AlignGPT. Among them, \u201cLocal\u201d refers to the local alignment capabilities derived by assigning different weights to various local alignment vectors using a gate network. \u201cGlobal\u201d denotes the global alignment capabilities, and \u201cAverage\u201d represents the local alignment capabilities obtained by assigning equal weights to each local alignment vector. The performance of these four strategies (settings a-d) is presented in Table 5.1  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###. As we can see, setting(a) and setting(b) demonstrate divergent performances in downstream tasks, which can be attributed to the different demands these tasks place on global and local alignment capabilities. It is worth noting that setting (a) and setting (b) perform worse than our final approach (setting d) on most datasets, which verifies the necessity and superiority of the combination of global and local alignment capabilities. Additionally, the performance of setting (c) is inferior to that of setting (d), a possible reason being that the demands for local alignment capabilities in each downstream task are dynamically variable.\nl|l|c|ccccccc\nMethod LLM Resolution VQAv2 GQA SQAI TextVQA POPE MMB SEEDI\nAlignGPT  Vicuna-7B  336  79.1  62.9  68.5  58.4  86.0  67.3  66.5\nAlignGPT  Vicuna-7B  672  79.7  63.3  68.3  60.3  86.8  67.2  66.5\nAlignGPT  Vicuna-7B  1008  79.8  63.4  68.2  60.3  86.8  67.2  66.6\nl|l|c|ccccccc\nMethod LLM Resolution VQAv2 GQA SQAI MME MMB MMBCN SEEDI\nAlignGPT  LLaMA2-7B-Chat  336  79.1  62.9  65.9  1500.8  66.6  57.9  66.4\nAlignGPT  Vicuna-7B  336  79.1  62.9  68.5  1527.4  67.3  59.9  66.5\nAlignGPT  LLaMA3-8B-Base  336  79.6  63.1  70.4  1539.7  72.0  67.7  68.2\nImage resolution plays a crucial role in vision-language tasks as higher resolutions help reduce image blurring and enhance the understanding of image-text alignment. To evaluate the impact of resolution changes on the performance of multimodal tasks, we increase the image resolution from 336 to 1008, with the resulting performance changes detailed in Table 5.2  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###. The study results show that higher image resolutions can improve model performance on most multimodal tasks. For example, the score for VQAv2 increased from 79.1 to 79.8, while the score for TextVQA rose from 58.4 to 60.3. Meanwhile, the performance of the POPE improve by 0.8. These results highlight that appropriately increasing image resolution is an effective strategy for enhancing performance in studies of multimodal large language models.\nWe also explore the impact of the large language model on the performance of AlignGPT, specifically testing three models: LLaMA-2-7B-Chat, Vicuna-v1.5-7B, and the latest LLaMA-3-8B-Base. The results are shown in Table 5.2  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###. Initially, we observe that LLaMA-3-8B-Base achieves the best performance, followed by Vicuna-v1.5-7B, with LLaMA-2-7B-Chat performing the worst, which is reasonable given LLaMA-3-8B-Base\u2019s larger parameter size and richer training data. Besides, we find that compared to the VQA tasks, Vicuna-v1.5-7B performs better on multimodal benchmarks such as MME, MMB, and SEEDI than LLaMA-2-7B-Chat, possibly because Vicuna-v1.5-7B underwent supervised instruction-tuning with ShareGPT data, and ShareGPT contains some background knowledge related to downstream tasks.\nFigure 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### presents a comparative analysis of our model with MiniGPT-v2 DBLP:journals/corr/abs-2310-09478  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5### and LLaVA-1.5 DBLP:journals/corr/abs-2310-03744  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###. When a user submits an image alongside the instruction \u201cCan you see the bright blue feathers on the bird\u2019s wing?\u201d, MiniGPT-v2 and LLaVA-1.5 both return an incorrect answer \u201cYes\u201d. In contrast, our model produces accurate result \u201cNo\u201d, thereby demonstrating that AlignGPT can effectively enhance the model\u2019s alignment capability. In Figure 4  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, we further demonstrate the responses of AlignGPT under different levels of alignment capability. We find that with lower alignment levels, the model may only focus on certain regions of the image, resulting in an undercount of the total number of apples; whereas with higher alignment levels, the model considers the entire image area, thus achieving accurate apple quantity estimation. This finding once again underscores the necessity of enhancing the alignment capability of MLLMs.\n###figure_50### ###figure_51### ###figure_52### ###figure_53### ###figure_54### ###figure_55###"
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Discussion",
            "text": "Image resolution plays a crucial role in vision-language tasks as higher resolutions help reduce image blurring and enhance the understanding of image-text alignment. To evaluate the impact of resolution changes on the performance of multimodal tasks, we increase the image resolution from 336 to 1008, with the resulting performance changes detailed in Table 5.2  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###. The study results show that higher image resolutions can improve model performance on most multimodal tasks. For example, the score for VQAv2 increased from 79.1 to 79.8, while the score for TextVQA rose from 58.4 to 60.3. Meanwhile, the performance of the POPE improve by 0.8. These results highlight that appropriately increasing image resolution is an effective strategy for enhancing performance in studies of multimodal large language models.\nWe also explore the impact of the large language model on the performance of AlignGPT, specifically testing three models: LLaMA-2-7B-Chat, Vicuna-v1.5-7B, and the latest LLaMA-3-8B-Base. The results are shown in Table 5.2  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###. Initially, we observe that LLaMA-3-8B-Base achieves the best performance, followed by Vicuna-v1.5-7B, with LLaMA-2-7B-Chat performing the worst, which is reasonable given LLaMA-3-8B-Base\u2019s larger parameter size and richer training data. Besides, we find that compared to the VQA tasks, Vicuna-v1.5-7B performs better on multimodal benchmarks such as MME, MMB, and SEEDI than LLaMA-2-7B-Chat, possibly because Vicuna-v1.5-7B underwent supervised instruction-tuning with ShareGPT data, and ShareGPT contains some background knowledge related to downstream tasks.\nFigure 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### presents a comparative analysis of our model with MiniGPT-v2 DBLP:journals/corr/abs-2310-09478  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5### and LLaVA-1.5 DBLP:journals/corr/abs-2310-03744  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###. When a user submits an image alongside the instruction \u201cCan you see the bright blue feathers on the bird\u2019s wing?\u201d, MiniGPT-v2 and LLaVA-1.5 both return an incorrect answer \u201cYes\u201d. In contrast, our model produces accurate result \u201cNo\u201d, thereby demonstrating that AlignGPT can effectively enhance the model\u2019s alignment capability. In Figure 4  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, we further demonstrate the responses of AlignGPT under different levels of alignment capability. We find that with lower alignment levels, the model may only focus on certain regions of the image, resulting in an undercount of the total number of apples; whereas with higher alignment levels, the model considers the entire image area, thus achieving accurate apple quantity estimation. This finding once again underscores the necessity of enhancing the alignment capability of MLLMs.\n###figure_56### ###figure_57### ###figure_58### ###figure_59### ###figure_60### ###figure_61###"
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Qualitative Results",
            "text": "Figure 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### presents a comparative analysis of our model with MiniGPT-v2 DBLP:journals/corr/abs-2310-09478  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5### and LLaVA-1.5 DBLP:journals/corr/abs-2310-03744  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###. When a user submits an image alongside the instruction \u201cCan you see the bright blue feathers on the bird\u2019s wing?\u201d, MiniGPT-v2 and LLaVA-1.5 both return an incorrect answer \u201cYes\u201d. In contrast, our model produces accurate result \u201cNo\u201d, thereby demonstrating that AlignGPT can effectively enhance the model\u2019s alignment capability. In Figure 4  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, we further demonstrate the responses of AlignGPT under different levels of alignment capability. We find that with lower alignment levels, the model may only focus on certain regions of the image, resulting in an undercount of the total number of apples; whereas with higher alignment levels, the model considers the entire image area, thus achieving accurate apple quantity estimation. This finding once again underscores the necessity of enhancing the alignment capability of MLLMs.\n###figure_62### ###figure_63### ###figure_64### ###figure_65### ###figure_66### ###figure_67###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Results and Analysis",
            "text": "We evaluate AlignGPT using five popular academic benchmarks, as detailed in Table 3.3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. Despite using less training data, the AlignGPT-7B demonstrates competitive performance, surpassing other generalist models including InstructBLIP-13B, Shikra-13B, and IDEFICS-80B on most datasets, except for LLaVA-1.5-13B. These results verify the rationality of the structural design of our model. Moreover, considering that AlignGPT utilizes the same training dataset as LLaVA-1.5, it is evident that AlignGPT-7B outperforms LLaVA-1.5-7B across all evaluation datasets, and AlignGPT-13B also surpasses LLaVA-1.5-13B on the majority of datasets. This demonstrates that our approach has effectively enhance the alignment capabilities of multimodal large language models. The fly in the ointment is that AlignGPT-13B does not perform as well as Qwen-VL on the TextVQA dataset. This may stem from the fact that TextVQA is a text-centric QA task, as it requires identifying text in images to answer questions. AlignGPT is tailored to boost multimodal alignment and might not exhibit strong results in text-focused scenarios.\nWe apply AlignGPT to seven recent popular multimodal benchmarks, as shown in Table 3.3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. We discover that, apart from LLaVA-1.5-13B, AlignGPT-7B surpassed all previous multimodal models. This shows that our model has strong generalization ability. Additionally, compared to LLaVA-1.5-13B, AlignGPT-13B has shown improvements on most datasets, particularly achieving good advancements on the MME, MMB, and LLaVAW benchamrks. This further validates the efficacy of both global and local alignment capabilities.\nl|c|ccccccccc\nMethod Alignment Level VQAv2 GQA VisWiz SQAI TextVQA POPE MME MMB SEEDI\nAlignGPT  Number=4  79.0  62.9  52.3  68.7  58.3  86.2  1463.8  67.2  66.5\nAlignGPT  Number=6  79.0  62.7  51.2  68.9  58.3  85.8  1436.3  67.3  66.2\nAlignGPT  Number=8  79.1  62.9  54.2  68.5  58.4  86.0  1527.4  67.3  66.5\nAlignGPT  Number=10  79.1  62.6  53.0  67.8  58.4  86.2  1481.4  66.4  66.7\nc|ccc|cccccccccc\nSettings Average Local Global VQAv2 GQA VisWiz SQAI TextVQA POPE MME MMB SEEDI\n(a)  \u2718 \u2714 \u2718 79.1  62.7  53.3  67.9  58.6  85.9  1467.1  66.9  66.3\n(b)  \u2718 \u2718 \u2714 79.1  62.9  52.6  68.3  58.4  85.9  1502.9  66.3  66.2\n(c)  \u2714 \u2718 \u2714 79.0  62.8  52.5  68.6  58.4  85.6  1492.5  67.0  66.0\n(d)  \u2718 \u2714 \u2714 79.1  62.9  54.2  68.5  58.4  86.0  1527.4  67.3  66.5\nTo investigate the effect of the number of alignment levels  on AlignGPT, we vary the value of  in the range of [4, 10] with a step size of 2. Table  5.1  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2### shows the performance of AlignGPT with different  on nine datasets. Actually, AlignGPT can achieve good results at , and their performance remains stable as the number of alignment levels increases. Depending on the trajectory of the curve, their performance has an initial upward trend and then flattens out. These observations indicate that AlignGPT can improve the alignment capabilities of multi-modal large language models based on a small number of alignment levels. Finally, according to the trend of the curve, we set  to 8.\nDuring the instruction-tuning phase, we assign global alignment capabilities and local alignment capabilities to the instructions of each task. Here, we explore the role of separate global alignment capabilities and local alignment capabilities on AlignGPT. Among them, \u201cLocal\u201d refers to the local alignment capabilities derived by assigning different weights to various local alignment vectors using a gate network. \u201cGlobal\u201d denotes the global alignment capabilities, and \u201cAverage\u201d represents the local alignment capabilities obtained by assigning equal weights to each local alignment vector. The performance of these four strategies (settings a-d) is presented in Table 5.1  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###. As we can see, setting(a) and setting(b) demonstrate divergent performances in downstream tasks, which can be attributed to the different demands these tasks place on global and local alignment capabilities. It is worth noting that setting (a) and setting (b) perform worse than our final approach (setting d) on most datasets, which verifies the necessity and superiority of the combination of global and local alignment capabilities. Additionally, the performance of setting (c) is inferior to that of setting (d), a possible reason being that the demands for local alignment capabilities in each downstream task are dynamically variable.\nl|l|c|ccccccc\nMethod LLM Resolution VQAv2 GQA SQAI TextVQA POPE MMB SEEDI\nAlignGPT  Vicuna-7B  336  79.1  62.9  68.5  58.4  86.0  67.3  66.5\nAlignGPT  Vicuna-7B  672  79.7  63.3  68.3  60.3  86.8  67.2  66.5\nAlignGPT  Vicuna-7B  1008  79.8  63.4  68.2  60.3  86.8  67.2  66.6\nl|l|c|ccccccc\nMethod LLM Resolution VQAv2 GQA SQAI MME MMB MMBCN SEEDI\nAlignGPT  LLaMA2-7B-Chat  336  79.1  62.9  65.9  1500.8  66.6  57.9  66.4\nAlignGPT  Vicuna-7B  336  79.1  62.9  68.5  1527.4  67.3  59.9  66.5\nAlignGPT  LLaMA3-8B-Base  336  79.6  63.1  70.4  1539.7  72.0  67.7  68.2\nImage resolution plays a crucial role in vision-language tasks as higher resolutions help reduce image blurring and enhance the understanding of image-text alignment. To evaluate the impact of resolution changes on the performance of multimodal tasks, we increase the image resolution from 336 to 1008, with the resulting performance changes detailed in Table 5.2  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###. The study results show that higher image resolutions can improve model performance on most multimodal tasks. For example, the score for VQAv2 increased from 79.1 to 79.8, while the score for TextVQA rose from 58.4 to 60.3. Meanwhile, the performance of the POPE improve by 0.8. These results highlight that appropriately increasing image resolution is an effective strategy for enhancing performance in studies of multimodal large language models.\nWe also explore the impact of the large language model on the performance of AlignGPT, specifically testing three models: LLaMA-2-7B-Chat, Vicuna-v1.5-7B, and the latest LLaMA-3-8B-Base. The results are shown in Table 5.2  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###. Initially, we observe that LLaMA-3-8B-Base achieves the best performance, followed by Vicuna-v1.5-7B, with LLaMA-2-7B-Chat performing the worst, which is reasonable given LLaMA-3-8B-Base\u2019s larger parameter size and richer training data. Besides, we find that compared to the VQA tasks, Vicuna-v1.5-7B performs better on multimodal benchmarks such as MME, MMB, and SEEDI than LLaMA-2-7B-Chat, possibly because Vicuna-v1.5-7B underwent supervised instruction-tuning with ShareGPT data, and ShareGPT contains some background knowledge related to downstream tasks.\nIn this paper, we propose AlignGPT, a novel multimodal large language model designed to bolster the alignment capabilities of MLLMs. Our approach involves utilizing the alignment level of data as a control signal during pre-training to effectively handle the varying degrees of alignment in image-text pairs. Subsequently, in the instruction-tuning phase, we begin by exploiting these control signals to shape different levels of alignment capabilities. Continuing from this, we go beyond assigning global alignment capabilities to instructions of each task; we also dynamically configure distinct local alignment capabilities based on the specific demands of each instruction. Results from numerous experiments indicate that our AlignGPT achieves better performance than other state-of-the-art MLLMs. Further analysis validates the superiority of our AlignGPT."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Main Results",
            "text": "We evaluate AlignGPT using five popular academic benchmarks, as detailed in Table 3.3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. Despite using less training data, the AlignGPT-7B demonstrates competitive performance, surpassing other generalist models including InstructBLIP-13B, Shikra-13B, and IDEFICS-80B on most datasets, except for LLaVA-1.5-13B. These results verify the rationality of the structural design of our model. Moreover, considering that AlignGPT utilizes the same training dataset as LLaVA-1.5, it is evident that AlignGPT-7B outperforms LLaVA-1.5-7B across all evaluation datasets, and AlignGPT-13B also surpasses LLaVA-1.5-13B on the majority of datasets. This demonstrates that our approach has effectively enhance the alignment capabilities of multimodal large language models. The fly in the ointment is that AlignGPT-13B does not perform as well as Qwen-VL on the TextVQA dataset. This may stem from the fact that TextVQA is a text-centric QA task, as it requires identifying text in images to answer questions. AlignGPT is tailored to boost multimodal alignment and might not exhibit strong results in text-focused scenarios.\nWe apply AlignGPT to seven recent popular multimodal benchmarks, as shown in Table 3.3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###. We discover that, apart from LLaVA-1.5-13B, AlignGPT-7B surpassed all previous multimodal models. This shows that our model has strong generalization ability. Additionally, compared to LLaVA-1.5-13B, AlignGPT-13B has shown improvements on most datasets, particularly achieving good advancements on the MME, MMB, and LLaVAW benchamrks. This further validates the efficacy of both global and local alignment capabilities.\nl|c|ccccccccc\nMethod Alignment Level VQAv2 GQA VisWiz SQAI TextVQA POPE MME MMB SEEDI\nAlignGPT  Number=4  79.0  62.9  52.3  68.7  58.3  86.2  1463.8  67.2  66.5\nAlignGPT  Number=6  79.0  62.7  51.2  68.9  58.3  85.8  1436.3  67.3  66.2\nAlignGPT  Number=8  79.1  62.9  54.2  68.5  58.4  86.0  1527.4  67.3  66.5\nAlignGPT  Number=10  79.1  62.6  53.0  67.8  58.4  86.2  1481.4  66.4  66.7\nc|ccc|cccccccccc\nSettings Average Local Global VQAv2 GQA VisWiz SQAI TextVQA POPE MME MMB SEEDI\n(a)  \u2718 \u2714 \u2718 79.1  62.7  53.3  67.9  58.6  85.9  1467.1  66.9  66.3\n(b)  \u2718 \u2718 \u2714 79.1  62.9  52.6  68.3  58.4  85.9  1502.9  66.3  66.2\n(c)  \u2714 \u2718 \u2714 79.0  62.8  52.5  68.6  58.4  85.6  1492.5  67.0  66.0\n(d)  \u2718 \u2714 \u2714 79.1  62.9  54.2  68.5  58.4  86.0  1527.4  67.3  66.5\nWithout loss of generality, we choose AlignGPT-7b for the ablation study to investigate the effects of different components.\nTo investigate the effect of the number of alignment levels  on AlignGPT, we vary the value of  in the range of [4, 10] with a step size of 2. Table  5.1  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2### shows the performance of AlignGPT with different  on nine datasets. Actually, AlignGPT can achieve good results at , and their performance remains stable as the number of alignment levels increases. Depending on the trajectory of the curve, their performance has an initial upward trend and then flattens out. These observations indicate that AlignGPT can improve the alignment capabilities of multi-modal large language models based on a small number of alignment levels. Finally, according to the trend of the curve, we set  to 8.\nDuring the instruction-tuning phase, we assign global alignment capabilities and local alignment capabilities to the instructions of each task. Here, we explore the role of separate global alignment capabilities and local alignment capabilities on AlignGPT. Among them, \u201cLocal\u201d refers to the local alignment capabilities derived by assigning different weights to various local alignment vectors using a gate network. \u201cGlobal\u201d denotes the global alignment capabilities, and \u201cAverage\u201d represents the local alignment capabilities obtained by assigning equal weights to each local alignment vector. The performance of these four strategies (settings a-d) is presented in Table 5.1  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###. As we can see, setting(a) and setting(b) demonstrate divergent performances in downstream tasks, which can be attributed to the different demands these tasks place on global and local alignment capabilities. It is worth noting that setting (a) and setting (b) perform worse than our final approach (setting d) on most datasets, which verifies the necessity and superiority of the combination of global and local alignment capabilities. Additionally, the performance of setting (c) is inferior to that of setting (d), a possible reason being that the demands for local alignment capabilities in each downstream task are dynamically variable.\nl|l|c|ccccccc\nMethod LLM Resolution VQAv2 GQA SQAI TextVQA POPE MMB SEEDI\nAlignGPT  Vicuna-7B  336  79.1  62.9  68.5  58.4  86.0  67.3  66.5\nAlignGPT  Vicuna-7B  672  79.7  63.3  68.3  60.3  86.8  67.2  66.5\nAlignGPT  Vicuna-7B  1008  79.8  63.4  68.2  60.3  86.8  67.2  66.6\nl|l|c|ccccccc\nMethod LLM Resolution VQAv2 GQA SQAI MME MMB MMBCN SEEDI\nAlignGPT  LLaMA2-7B-Chat  336  79.1  62.9  65.9  1500.8  66.6  57.9  66.4\nAlignGPT  Vicuna-7B  336  79.1  62.9  68.5  1527.4  67.3  59.9  66.5\nAlignGPT  LLaMA3-8B-Base  336  79.6  63.1  70.4  1539.7  72.0  67.7  68.2\nImage resolution plays a crucial role in vision-language tasks as higher resolutions help reduce image blurring and enhance the understanding of image-text alignment. To evaluate the impact of resolution changes on the performance of multimodal tasks, we increase the image resolution from 336 to 1008, with the resulting performance changes detailed in Table 5.2  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###. The study results show that higher image resolutions can improve model performance on most multimodal tasks. For example, the score for VQAv2 increased from 79.1 to 79.8, while the score for TextVQA rose from 58.4 to 60.3. Meanwhile, the performance of the POPE improve by 0.8. These results highlight that appropriately increasing image resolution is an effective strategy for enhancing performance in studies of multimodal large language models.\nWe also explore the impact of the large language model on the performance of AlignGPT, specifically testing three models: LLaMA-2-7B-Chat, Vicuna-v1.5-7B, and the latest LLaMA-3-8B-Base. The results are shown in Table 5.2  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###. Initially, we observe that LLaMA-3-8B-Base achieves the best performance, followed by Vicuna-v1.5-7B, with LLaMA-2-7B-Chat performing the worst, which is reasonable given LLaMA-3-8B-Base\u2019s larger parameter size and richer training data. Besides, we find that compared to the VQA tasks, Vicuna-v1.5-7B performs better on multimodal benchmarks such as MME, MMB, and SEEDI than LLaMA-2-7B-Chat, possibly because Vicuna-v1.5-7B underwent supervised instruction-tuning with ShareGPT data, and ShareGPT contains some background knowledge related to downstream tasks.\nFigure 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### presents a comparative analysis of our model with MiniGPT-v2 DBLP:journals/corr/abs-2310-09478  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5### and LLaVA-1.5 DBLP:journals/corr/abs-2310-03744  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###. When a user submits an image alongside the instruction \u201cCan you see the bright blue feathers on the bird\u2019s wing?\u201d, MiniGPT-v2 and LLaVA-1.5 both return an incorrect answer \u201cYes\u201d. In contrast, our model produces accurate result \u201cNo\u201d, thereby demonstrating that AlignGPT can effectively enhance the model\u2019s alignment capability. In Figure 4  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, we further demonstrate the responses of AlignGPT under different levels of alignment capability. We find that with lower alignment levels, the model may only focus on certain regions of the image, resulting in an undercount of the total number of apples; whereas with higher alignment levels, the model considers the entire image area, thus achieving accurate apple quantity estimation. This finding once again underscores the necessity of enhancing the alignment capability of MLLMs.\n###figure_68### ###figure_69### ###figure_70### ###figure_71### ###figure_72### ###figure_73###"
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Ablation Study",
            "text": "Without loss of generality, we choose AlignGPT-7b for the ablation study to investigate the effects of different components.\nTo investigate the effect of the number of alignment levels  on AlignGPT, we vary the value of  in the range of [4, 10] with a step size of 2. Table  5.1  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2### shows the performance of AlignGPT with different  on nine datasets. Actually, AlignGPT can achieve good results at , and their performance remains stable as the number of alignment levels increases. Depending on the trajectory of the curve, their performance has an initial upward trend and then flattens out. These observations indicate that AlignGPT can improve the alignment capabilities of multi-modal large language models based on a small number of alignment levels. Finally, according to the trend of the curve, we set  to 8.\nDuring the instruction-tuning phase, we assign global alignment capabilities and local alignment capabilities to the instructions of each task. Here, we explore the role of separate global alignment capabilities and local alignment capabilities on AlignGPT. Among them, \u201cLocal\u201d refers to the local alignment capabilities derived by assigning different weights to various local alignment vectors using a gate network. \u201cGlobal\u201d denotes the global alignment capabilities, and \u201cAverage\u201d represents the local alignment capabilities obtained by assigning equal weights to each local alignment vector. The performance of these four strategies (settings a-d) is presented in Table 5.1  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###. As we can see, setting(a) and setting(b) demonstrate divergent performances in downstream tasks, which can be attributed to the different demands these tasks place on global and local alignment capabilities. It is worth noting that setting (a) and setting (b) perform worse than our final approach (setting d) on most datasets, which verifies the necessity and superiority of the combination of global and local alignment capabilities. Additionally, the performance of setting (c) is inferior to that of setting (d), a possible reason being that the demands for local alignment capabilities in each downstream task are dynamically variable.\nl|l|c|ccccccc\nMethod LLM Resolution VQAv2 GQA SQAI TextVQA POPE MMB SEEDI\nAlignGPT  Vicuna-7B  336  79.1  62.9  68.5  58.4  86.0  67.3  66.5\nAlignGPT  Vicuna-7B  672  79.7  63.3  68.3  60.3  86.8  67.2  66.5\nAlignGPT  Vicuna-7B  1008  79.8  63.4  68.2  60.3  86.8  67.2  66.6\nl|l|c|ccccccc\nMethod LLM Resolution VQAv2 GQA SQAI MME MMB MMBCN SEEDI\nAlignGPT  LLaMA2-7B-Chat  336  79.1  62.9  65.9  1500.8  66.6  57.9  66.4\nAlignGPT  Vicuna-7B  336  79.1  62.9  68.5  1527.4  67.3  59.9  66.5\nAlignGPT  LLaMA3-8B-Base  336  79.6  63.1  70.4  1539.7  72.0  67.7  68.2\nImage resolution plays a crucial role in vision-language tasks as higher resolutions help reduce image blurring and enhance the understanding of image-text alignment. To evaluate the impact of resolution changes on the performance of multimodal tasks, we increase the image resolution from 336 to 1008, with the resulting performance changes detailed in Table 5.2  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###. The study results show that higher image resolutions can improve model performance on most multimodal tasks. For example, the score for VQAv2 increased from 79.1 to 79.8, while the score for TextVQA rose from 58.4 to 60.3. Meanwhile, the performance of the POPE improve by 0.8. These results highlight that appropriately increasing image resolution is an effective strategy for enhancing performance in studies of multimodal large language models.\nWe also explore the impact of the large language model on the performance of AlignGPT, specifically testing three models: LLaMA-2-7B-Chat, Vicuna-v1.5-7B, and the latest LLaMA-3-8B-Base. The results are shown in Table 5.2  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###. Initially, we observe that LLaMA-3-8B-Base achieves the best performance, followed by Vicuna-v1.5-7B, with LLaMA-2-7B-Chat performing the worst, which is reasonable given LLaMA-3-8B-Base\u2019s larger parameter size and richer training data. Besides, we find that compared to the VQA tasks, Vicuna-v1.5-7B performs better on multimodal benchmarks such as MME, MMB, and SEEDI than LLaMA-2-7B-Chat, possibly because Vicuna-v1.5-7B underwent supervised instruction-tuning with ShareGPT data, and ShareGPT contains some background knowledge related to downstream tasks.\nFigure 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### presents a comparative analysis of our model with MiniGPT-v2 DBLP:journals/corr/abs-2310-09478  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5### and LLaVA-1.5 DBLP:journals/corr/abs-2310-03744  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###. When a user submits an image alongside the instruction \u201cCan you see the bright blue feathers on the bird\u2019s wing?\u201d, MiniGPT-v2 and LLaVA-1.5 both return an incorrect answer \u201cYes\u201d. In contrast, our model produces accurate result \u201cNo\u201d, thereby demonstrating that AlignGPT can effectively enhance the model\u2019s alignment capability. In Figure 4  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, we further demonstrate the responses of AlignGPT under different levels of alignment capability. We find that with lower alignment levels, the model may only focus on certain regions of the image, resulting in an undercount of the total number of apples; whereas with higher alignment levels, the model considers the entire image area, thus achieving accurate apple quantity estimation. This finding once again underscores the necessity of enhancing the alignment capability of MLLMs.\n###figure_74### ###figure_75### ###figure_76### ###figure_77### ###figure_78### ###figure_79###"
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Discussion",
            "text": "Image resolution plays a crucial role in vision-language tasks as higher resolutions help reduce image blurring and enhance the understanding of image-text alignment. To evaluate the impact of resolution changes on the performance of multimodal tasks, we increase the image resolution from 336 to 1008, with the resulting performance changes detailed in Table 5.2  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###. The study results show that higher image resolutions can improve model performance on most multimodal tasks. For example, the score for VQAv2 increased from 79.1 to 79.8, while the score for TextVQA rose from 58.4 to 60.3. Meanwhile, the performance of the POPE improve by 0.8. These results highlight that appropriately increasing image resolution is an effective strategy for enhancing performance in studies of multimodal large language models.\nWe also explore the impact of the large language model on the performance of AlignGPT, specifically testing three models: LLaMA-2-7B-Chat, Vicuna-v1.5-7B, and the latest LLaMA-3-8B-Base. The results are shown in Table 5.2  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###. Initially, we observe that LLaMA-3-8B-Base achieves the best performance, followed by Vicuna-v1.5-7B, with LLaMA-2-7B-Chat performing the worst, which is reasonable given LLaMA-3-8B-Base\u2019s larger parameter size and richer training data. Besides, we find that compared to the VQA tasks, Vicuna-v1.5-7B performs better on multimodal benchmarks such as MME, MMB, and SEEDI than LLaMA-2-7B-Chat, possibly because Vicuna-v1.5-7B underwent supervised instruction-tuning with ShareGPT data, and ShareGPT contains some background knowledge related to downstream tasks.\nFigure 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### presents a comparative analysis of our model with MiniGPT-v2 DBLP:journals/corr/abs-2310-09478  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5### and LLaVA-1.5 DBLP:journals/corr/abs-2310-03744  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###. When a user submits an image alongside the instruction \u201cCan you see the bright blue feathers on the bird\u2019s wing?\u201d, MiniGPT-v2 and LLaVA-1.5 both return an incorrect answer \u201cYes\u201d. In contrast, our model produces accurate result \u201cNo\u201d, thereby demonstrating that AlignGPT can effectively enhance the model\u2019s alignment capability. In Figure 4  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, we further demonstrate the responses of AlignGPT under different levels of alignment capability. We find that with lower alignment levels, the model may only focus on certain regions of the image, resulting in an undercount of the total number of apples; whereas with higher alignment levels, the model considers the entire image area, thus achieving accurate apple quantity estimation. This finding once again underscores the necessity of enhancing the alignment capability of MLLMs.\n###figure_80### ###figure_81### ###figure_82### ###figure_83### ###figure_84### ###figure_85###"
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Qualitative Results",
            "text": "Figure 3  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_### presents a comparative analysis of our model with MiniGPT-v2 DBLP:journals/corr/abs-2310-09478  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5###  ###reference_b5### and LLaVA-1.5 DBLP:journals/corr/abs-2310-03744  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###  ###reference_b23###. When a user submits an image alongside the instruction \u201cCan you see the bright blue feathers on the bird\u2019s wing?\u201d, MiniGPT-v2 and LLaVA-1.5 both return an incorrect answer \u201cYes\u201d. In contrast, our model produces accurate result \u201cNo\u201d, thereby demonstrating that AlignGPT can effectively enhance the model\u2019s alignment capability. In Figure 4  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###  ###reference_###, we further demonstrate the responses of AlignGPT under different levels of alignment capability. We find that with lower alignment levels, the model may only focus on certain regions of the image, resulting in an undercount of the total number of apples; whereas with higher alignment levels, the model considers the entire image area, thus achieving accurate apple quantity estimation. This finding once again underscores the necessity of enhancing the alignment capability of MLLMs.\n###figure_86### ###figure_87### ###figure_88### ###figure_89### ###figure_90### ###figure_91###"
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we propose AlignGPT, a novel multimodal large language model designed to bolster the alignment capabilities of MLLMs. Our approach involves utilizing the alignment level of data as a control signal during pre-training to effectively handle the varying degrees of alignment in image-text pairs. Subsequently, in the instruction-tuning phase, we begin by exploiting these control signals to shape different levels of alignment capabilities. Continuing from this, we go beyond assigning global alignment capabilities to instructions of each task; we also dynamically configure distinct local alignment capabilities based on the specific demands of each instruction. Results from numerous experiments indicate that our AlignGPT achieves better performance than other state-of-the-art MLLMs. Further analysis validates the superiority of our AlignGPT."
        }
    ],
    "url": "http://arxiv.org/html/2405.14129v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "5",
            "5.1",
            "5.2",
            "5.3",
            "5.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "5.2"
        ]
    },
    "research_context": {
        "paper_id": "2405.14129v1",
        "paper_title": "AlignGPT: Multi-modal Large Language Models with Adaptive Alignment Capability",
        "research_background": "### Motivation\nThe motivation for this paper is rooted in the pursuit of developing Artificial General Intelligence (AGI), specifically by enhancing the capabilities of Multimodal Large Language Models (MLLMs). The primary goal is to enable models to better integrate and understand various types of information, particularly text and image data, which is essential for mimicking human-like cognition and interaction with the world. Current MLLMs aim to achieve this by following a unified training paradigm consisting of pre-training and instruction-tuning phases. However, the paper identifies significant limitations in the current models' ability to handle varying degrees of alignment between image-text pairs and the specific alignment needs required by different tasks.\n\n### Research Problem\nThe research problem addressed in this paper is the insufficient modeling of alignment capabilities in existing MLLMs during their pre-training and instruction-tuning phases. Specifically:\n1. **Inconsistent Degree of Alignment**: Existing models operate under the assumption that all image-text pairings are consistently aligned, which is not always the case in real-world scenarios.\n2. **Varying Instruction Alignment Needs**: Different tasks have varying requirements for global and local alignment capabilities, which current models are not adequately equipped to handle.\n\n### Relevant Prior Work\nThe paper builds upon and acknowledges prior research in several key areas:\n1. **MLLMs in the Context of AGI**: Prior work such as DBLP:journals/corr/abs-2303-08774, DBLP:journals/corr/abs-2312-11805, DBLP:conf/icml/DriessXSLCIWTVY23, and DBLP:journals/corr/abs-2309-16058 underscores the significance of MLLMs for AGI.\n2. **Unified Training Paradigm for MLLMs**: Studies like DBLP:conf/nips/LiuLWL23a, DBLP:journals/corr/abs-2304-10592, DBLP:journals/corr/abs-2304-14178, and others outline the importance of the pre-training and instruction-tuning phases in MLLMs training.\n3. **Pre-training Alignment Techniques**: Utilizing tools such as CLIP scores (DBLP:conf/icml/RadfordKHRGASAM21) to categorize image-text pairs based on their alignment levels.\n4. **Task-Specific Alignment Needs**: References such as image captioning (DBLP:conf/icml/XuBKCCSZB15) and visual question answering (VQA) (DBLP:conf/iccv/AntolALMBZP15) illustrate the different alignment capabilities required by various tasks. \n\nBy proposing AlignGPT, the paper aims to address these limitations through a novel alignment strategy that distinguishes different levels of alignment capability during pre-training and adaptively configures these levels during the instruction-tuning phase. Extensive evaluations across multiple benchmarks demonstrate the effectiveness of this approach.",
        "methodology": "## Methodology for AlignGPT: Multi-modal Large Language Models with Adaptive Alignment Capability\n\nIn this section, we initially present the fundamental structure of the visual-language model AlignGPT and demonstrate how to enhance the alignment capability of the model during both the pre-training and instruction-tuning stages.\n\nWe utilize the pre-trained CLIP visual encoder ViT-L/14 as our visual backbone, focusing on visual inputs processed at an image resolution of 336x336. To bridge the representations of images and language, we employ a linear projection layer to map the visual features from the vision backbone's vector space to that of the language model.\n\n### Language Model Backbone\n\nWe choose the open-source model Vicuna due to its efficacy in following instructions across a variety of language tasks. We propose adding alignment vectors to the inputs of MLLMs to enrich their alignment capabilities. The subsequent sections will elaborate on the role of these alignment vectors and the process of acquiring them.\n\n### Training Process\n\nFor consistency in evaluation, we use the same pre-training and instruction dataset as LLaVA-1.5, which includes 558K caption pairs for modality alignment and 665K single- or multi-round conversations for instruction-tuning. AlignGPT's evaluation spans 12 benchmarks, including VQAV2, GQA, VizWiz, SQAI, TextVQA, POPE, MME, MMB, MMBCN, SEEDI, LLaVAW, and MM-Vet datasets.\n\n### Model Architecture\n\n- **Vision Encoder**: A ViT model pre-trained with CLIP handles visual data processing.\n- **Language Model**: Vicuna is used to manage multimodal features.\n\nDuring the pre-training phase, the visual backbone and the large language model remain frozen, with only the parameters of the linear projection layer and alignment vectors being trained. In the instruction-tuning phase, the linear projection layer, alignment vectors, and the visual backbone are frozen, while the large language model and the gate network parameters are adjusted.\n\n### Technical Details\n\n- **Optimizers and Schedules**: The models are optimized using the AdamW optimizer with a cosine learning schedule. Initial learning rates are set to 1e-3 for pre-training and 2e-5 for the instruction-tuning phase.\n- **Hardware**: Training is conducted on 8 A800 GPUs with 80GB memory each.\n\n### Performance Analysis\n\nAlignGPT's performance is evaluated through various benchmarks. Despite using less training data, AlignGPT-7B proves competitive, surpassing other models like InstructBLIP-13B, Shikra-13B, and IDEFICS-80B on most datasets, though it underperforms against Qwen-VL on the TextVQA dataset. AlignGPT\u2019s superior performance validates the proposed model's design's rationality.\n\n### Global and Local Alignment Capabilities\n\nAssigning global and local alignment capabilities to tasks during instruction-tuning proved effective. Setting (d), which incorporates both global and local alignment capabilities, demonstrated superior performance over settings that utilize only one type of alignment.\n\n### Impact of Image Resolution and LLM\n\nHigher image resolutions positively impact model performance in vision-language tasks, improving scores on datasets such as VQAv2 and TextVQA. Furthermore, the choice of large language model significantly impacts performance. LLaMA-3-8B-Base outperformed other models due to its larger parameter size and richer training data, demonstrating that larger, more data-rich models yield better performance.\n\n### Conclusion\n\nAlignGPT effectively handles varying degrees of alignment in image-text pairs and adapts to specific demands of each instruction. Experimental results validate that AlignGPT not only matches but often exceeds the performance of state-of-the-art MLLMs, justifying the model's structural innovations.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n**Setup:**\n\n1. **Datasets:** \n   - The experiment utilizes the same pre-training and instruction datasets as LLaVA-1.5, which include:\n     - 558K caption pairs for modality alignment.\n     - 665K single- or multi-round conversations for instruction-tuning.\n\n   - The evaluation is conducted on 12 benchmarks:\n     - VQAV2\n     - GQA\n     - VizWiz\n     - SQAI (ScienceQA-IMG)\n     - TextVQA\n     - POPE\n     - MME\n     - MMB (MMBench)\n     - MMBCN (MMBench-Chinese)\n     - SEEDI (SEED-Bench-IMG)\n     - LLaVAW (LLaVA-Bench-in-the-Wild)\n     - MM-Vet\n\n2. **Baselines:**\n   - Comparison models include those that use a ViT model pre-trained with CLIP as a vision encoder and Vicuna as the language model.\n\n3. **Training Phases:**\n   - **Pre-training Phase:**\n     - Vision encoder and large language model (LLM) are kept frozen.\n     - Only parameters of the linear projection layer and alignment vectors are trained.\n     - Batch size: 256\n   - **Instruction-tuning Phase:**\n     - Linear projection layer, alignment vectors, and visual backbone are frozen.\n     - Parameters of the large language model and gate network are adjusted.\n     - Batch size: 128\n\n   - Both phases utilize DeepSpeed with ZeRO2 and ZeRO3 strategies.\n   - Optimization using AdamW optimizer with a cosine learning schedule.\n   - Learning rates:\n     - Pre-training: 1e-3\n     - Instruction-tuning: 2e-5\n\n4. **Hardware:**\n   - Training is conducted on 8 A800 GPUs with 80GB memory each.\n\n**Results:**\n\nWhile specific results in terms of quantitative metrics (e.g., accuracy, precision, recall, F1 score) are not provided in the given text, the experiment setup is meticulously designed to compare AlignGPT\u2019s performance against state-of-the-art models on an extensive array of benchmarks. The uses of consistent datasets, robust pre-training, and instruction-tuning methodologies aim to ensure that any performance improvements can be attributed to the proposed alignment techniques.\n\n### Evaluation Metrics:\n\nAlthough not explicitly mentioned, the typical evaluation metrics for the visual question answering (VQA) tasks and other benchmarks likely include accuracy and other relevant metrics for multi-modal tasks such as precision, recall, and F1 score.\n\nWould you like further information or interpretation of specific benchmark results?"
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To investigate the effects of different components on the performance of AlignGPT-7b, including the number of alignment levels, the role of global vs. local alignment capabilities, the impact of image resolution, and the effect of using different large language models.",
            "experiment_process": "1. Number of Alignment Levels: The number of alignment levels was varied in the range of 4 to 10 with a step size of 2. The performance was tested on nine datasets. 2. Global vs. Local Alignment Capabilities: The roles of separate global alignment capabilities, local alignment capabilities (derived by assigning different weights to local alignment vectors), and average alignment capabilities (equal weights) were explored. Four strategies (settings a-d) were tested and their performance presented in Table 5.1. 3. Image Resolution: The image resolution was increased from 336 to 1008 to evaluate its impact on vision-language tasks. Performance changes were detailed in Table 5.2. 4. Large Language Models: The performance of AlignGPT was tested using three models: LLaMA-2-7B-Chat, Vicuna-v1.5-7B, and LLaMA-3-8B-Base. Results were shown in Table 5.2.",
            "result_discussion": "1. Number of Alignment Levels: AlignGPT showed stable performance across tested datasets, with an initial upward trend that flattened out as the number of alignment levels increased. Setting the number of alignment levels to 8 was found to be optimal. 2. Global vs. Local Alignment Capabilities: Settings (a) and (b) showed divergent performances due to different demands from downstream tasks. However, the combination approach (setting d) outperformed other settings, verifying the necessity and superiority of combining global and local alignment capabilities. 3. Image Resolution: Higher image resolutions improved model performance on most multimodal tasks, with increases in scores for VQAv2 (from 79.1 to 79.8) and TextVQA (from 58.4 to 60.3). 4. Large Language Models: LLaMA-3-8B-Base achieved the best performance, followed by Vicuna-v1.5-7B, and then LLaMA-2-7B-Chat. Vicuna-v1.5-7B performed better on multimodal benchmarks than LLaMA-2-7B-Chat due to its supervised instruction-tuning with ShareGPT data.",
            "ablation_id": "2405.14129v1.No1"
        },
        {
            "research_objective": "To investigate the effects of different components on the performance of AlignGPT-7b, particularly focusing on component analysis at various levels of alignment and exploring the impact of local vs. global alignment capabilities.",
            "experiment_process": "1. Number of Alignment Levels: Varied in the range of 4 to 10 with a step size of 2, and performance was assessed on nine datasets. 2. Global vs. Local Alignment Capabilities: Global (setting b), local (setting a), and average (setting c) alignment capabilities were separately investigated, and then a combined strategy (setting d) was tested. Performance metrics were recorded in Table 5.1. 3. Image Resolution: Evaluated by increasing the resolution from 336 to 1008, with performance changes logged in Table 5.2. 4. Large Language Models: Evaluations included LLaMA-2-7B-Chat, Vicuna-v1.5-7B, and LLaMA-3-8B-Base and are shown in Table 5.2.",
            "result_discussion": "1. Number of Alignment Levels: The performance of AlignGPT stabilized as the number of alignment levels increased, showing that good results can be achieved with fewer alignment levels (optimal at 8). 2. Global vs. Local Alignment Capabilities: Local and global capabilities were essential, with combined use (setting d) showing superior performance. Dynamic variability in demands for local capabilities across tasks influenced outcomes, with setting d outperforming others. 3. Image Resolution: Increased resolution led to improved scores on various multimodal tasks. VQAv2 score rose to 79.8, and TextVQA improved to 60.3, demonstrating the efficacy of higher resolutions. 4. Large Language Models: LLaMA-3-8B-Base performed best, followed by Vicuna-v1.5-7B and LLaMA-2-7B-Chat. Vicuna-v1.5-7B's better performance on multimodal benchmarks was attributed to its enriched training with ShareGPT.",
            "ablation_id": "2405.14129v1.No2"
        }
    ]
}