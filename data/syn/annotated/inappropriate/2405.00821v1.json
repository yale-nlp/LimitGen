{
    "title": "Uncovering Agendas: A Novel French & English Dataset for Agenda Detection on Social Media",
    "abstract": "The behavior and decision making of groups or communities can be dramatically influenced by individuals pushing particular agendas, e.g., to promote or disparage a person or an activity, to call for action, etc.. In the examination of online influence campaigns, particularly those related to important political and social events, scholars often concentrate on identifying the sources responsible for setting and controlling the agenda (e.g., public media). In this article we present a methodology for detecting specific instances of agenda control through social media where annotated data is limited or non-existent. By using a modest corpus of Twitter messages centered on the 2022 French Presidential Elections, we carry out a comprehensive evaluation of various approaches and techniques that can be applied to this problem. Our findings demonstrate that by treating the task as a textual entailment problem, it is possible to overcome the requirement for a large annotated training dataset.\n\n\n\nKeywords:\u2009Agenda Detection, Social Network, Political Campaign, Textual Entailment, Text Classification",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "An agenda, being a collection of items to be attended to in a certain order, can have a significant impact on the actions of a group, especially in the context of interpersonal communication and relationships. In human communication, an agenda refers to the underlying intentions or motives of a particular person or group to steer the conversation in a particular direction in order to achieve a desired effect (Dictionary, September\n2023b  ###reference_b14###, September 2023a  ###reference_b13###, July 2023  ###reference_b12###). The individuals who establish and direct the agenda often exercise considerable control and influence over their audience. In the sociolinguistics of group behavior, the concept of agenda control is widely recognized as a strong indicator of both leadership and influence, as evidenced by numerous studies in the field (Wang et al., 2018  ###reference_b32###; Broadwell et al., 2013  ###reference_b6###; Strzalkowski et al., 2013  ###reference_b27###).\nWhen studying the impact of online influence campaigns, such as those surrounding significant political and social events (e.g., elections), researchers often focus on evidence of agenda-setting activity emanating from particular sources. These sources could be traditional public media or clandestine online groups that wish to shape public opinion. According to social science literature, there are three distinct levels of agenda setting. At level one, the public is told explicitly what to think and do in a given situation, for example, to vote for a particular candidate. In the second level of agenda setting, rather than prescribing specific beliefs or actions, the influencers emphasize certain aspects of their targets (e.g., political candidates) as either positive or negative, outwardly leaving the public to form their own opinions (McCombs et al., 1997  ###reference_b18###; Balmas and Sheafer, 2010  ###reference_b1###; Meraz, 2011  ###reference_b19###). At the third level, multiple targets are associated to one another through direct comparison or juxtaposition Guo et al. (2012  ###reference_b16###) thus imparting apparent preferences onto the public.\nIn this study, we are interested in both level one and level two agenda setting activities and how to detect their presence in social media, with a specific focus on the 2022 French Presidential Elections. Our goal is to detect specific instances of agendas being actively promoted via social media messaging (see Table 1). Our focus is on Twitter messages (tweets), including retweets, replies, and quotes, posted in multiple languages during the relevant time period. The objective is to automatically tag each tweet with appropriate agenda labels, with each label representing a type of agenda, not necessarily explicit, that may arise in a political context.\nThe agenda labels under consideration and their definitions in English are presented in Table 1  ###reference_###. In this work, we focus on a set of \"call for action\" agendas which seek to inspire concrete action(s). The labels were curated by political science experts, and the curation process is outside of the scope of this paper; however, the agenda labels, which we discuss in more details below, were chosen to apply on most election-style events, although not necessarily to other events such as international conflicts. Given the novelty and an ad-hoc nature of the agenda labeling problem, the lack of pre-existing annotated training data, and the practical limitations of obtaining sufficient quantities of such data (which is true of most real-world applications), our approach focuses on utilizing small, expert annotated samples and relying on zero-shot and few-shot methods. Our proposed framework provides a general solution for classifying social media messages that operates on a set of ad-hoc agenda labels.\nSince an agenda is defined as an intention, explicit or not, behind a message (or a set of messages), it is reasonable to assume that the message implies the agenda, or at least it is meant to imply (assuming the message is understood). In other words, if we can show that a message implies one of the agendas in our agenda collection, we can assign the corresponding agenda label to this message.\nAccordingly, we propose to cast the agenda detection task as a textual entailment problem. In previous studies (Yin et al., 2019  ###reference_b36###), text classification has been viewed through the lens of textual entailment with promising results. This approach imitates how humans make decisions while annotating text examples, picking the correct label among all possible labels. Human annotators are often given a task description, as well as label definitions that explain the meaning of each candidate label. Equipped with these definitions, a human can understand the problem and mentally construct a hypothesis by picking a candidate label to fill in the blank: \"This text is about ___\". Then they ask themselves if this hypothesis is true given the text example.\nWe treat agenda detection as a textual entailment problem so that our model can gain knowledge from entailment datasets (Bowman et al., 2015  ###reference_b5###; Williams et al., 2017  ###reference_b34###; Dagan et al., 2006  ###reference_b10###; Bentivogli et al., 2009  ###reference_b2###). We should mention that the textual entailment approach is well suited for agenda detection, and our approach is not limited to a predefined set of labels as it can be extended to arbitrary sets using generative methods."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Related Work",
            "text": "Recent studies examine methods to detect influence indicators from individual social media messages. For instance, Bhaumik et al. (2023  ###reference_b3###) identify emotions expressed by the authors of social media posts in relation to political issues or causes. To explore the topic of agenda detection in social media, we present a review of selected literature in the domains of traditional agenda detection and text classification through the lens of textual entailment, as our proposed models draw inspiration and incorporate elements from these fields."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1.   Agenda Detection",
            "text": "The impact of various agendas being pushed through the media (both official and unofficial) have on shaping public opinion has been widely studied, as has the interplay between the news outlets and the social media. For example, McCombs et al. (1997  ###reference_b18###) attempts to understand how media agendas shape or influence the public\u2019s opinion on political candidates, and Vargo et al. (2014  ###reference_b31###) studies how the public selectively accepts media agendas. Additionally, the effect that news media and social media have on each other is closely examined in (Su et al., 2020  ###reference_b30###).\nIn the absence of large annotated datasets, scholars often perform manual analysis to detect agendas. McCombs et al. (1997  ###reference_b18###) hand-coded news articles and surveys with attribute labels signaling the candidates\u2019 ideology and positions on public issues, their qualifications, experience, their personal characteristics and personality. Similarly, Su and Borah (2019  ###reference_b29###) conducted a manual analysis of a sample of collected tweets, labeling them with 11 distinct classes on the topic of Climate Change. Automated methods such as those used in (Vargo et al., 2014  ###reference_b31###), (Ceron et al., 2016  ###reference_b7###) and (Haim et al., 2018  ###reference_b17###) utilized sets of keywords to detect topics and sentiment associated with the target agendas, rather than the agendas themselves.\nMore recently, several studies explored machine learning methods for the detection of agendas in big data. In (Su et al., 2020  ###reference_b30###; Su, 2022  ###reference_b28###; Guo, 2019  ###reference_b15###), the authors first utilize topic modeling to identify the topics within their datasets. Then, human experts manually develop agenda labels associated with each topic. Subsequently, multiple annotators tag a subset of the data using the agenda labels developed in the previous step. The labeled data is then used to train a set of Support Vector Machine (SVM) (Boser et al., 1992  ###reference_b4###) classifiers, one per each agenda label. If the average performance of the classifiers was not satisfactory (e.g., F1 < 0.7), more data was annotated and the training was repeated.\nGuo (2019  ###reference_b15###) collected and annotated 2000 news articles with 31 topic labels for the task of detecting inter-media agenda setting in Chinese online news. In (Su, 2022  ###reference_b28###), the authors explored the information flow between newspaper and Twitter focusing on the topic of Black Lives Matter. A total of 1500 news articles and 5000 tweets were annotated with 16 topic labels and 5 affect labels. Su et al. (2020  ###reference_b30###) analyzed the Hong Kong Movement and annotated 3000 tweets and 500 news articles with 3 affect labels: pro-protest, neural, anti-protest, and 13 topic labels: Violence of police, UK politics, US politics, Sino-US relation, HK legislation, Violence of protesters, HK economy, Overseas Chinese students, Democracy & human rights, HK-Mainland relations, Public security and Social media & Entertainment.\nThe authors of Chen et al. (2019  ###reference_b8###) used a similar approach, but deployed different classifiers, trained on 2500 annotated microblog messages centered on the topic of Chinese Nationalism on Social Media. We note that all the above approaches are costly and impractical, particularly in novel and rapidly evolving situations."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2.   Textual Entailment Text Classification",
            "text": "In their work, Yin et al. (2019  ###reference_b36###) introduced a framework for text classification by formulating it as a series of premise-hypothesis pairs, where the premise is the text to be classified and the hypotheses represents the candidate labels, essentially transforming the problem into a textual entailment challenge. They demonstrated the efficacy of this method, and released a benchmark dataset for zero-shot text classification. Subsequently, this approach has been widely adopted and expanded for many zero-shot text classification tasks (Shu et al., 2022  ###reference_b26###; Zhang et al., 2020  ###reference_b37###; Wang et al., 2021  ###reference_b33###; Seoh et al., 2021  ###reference_b24###). Our work builds upon this basic methodology by applying textual entailment to the task of agenda detection, and we evaluate and compare various approaches that could also be used to solve this problem, including conventional text classification methods. By doing so, we demonstrate the utility of this framework for addressing the task of agenda detection in the absence of large annotated datasets."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   Data",
            "text": "Our proposed model makes use of pre-existing textual entailment datasets, described in the following paragraphs, to gain general knowledge. This pre-training methodology exposes the model to a multitude of linguistic and factual scenarios. Through the utilization of textual entailment datasets, the model learns to make sense of contradictions, inferences, and entailment relationships. Then, the model is trained on agenda specific data."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   Pre-training Data",
            "text": "To teach our model how to solve the textual entailment task, we deploy three widely used datasets into an early fine-tuning training step. These datasets are i) the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015  ###reference_b5###), ii) the Multi-genre Natural Language Inference (MNLI) dataset (Williams et al., 2017  ###reference_b34###), and iii) the Recognizing Textual Entailment (RTE) dataset 111RTE dataset: https://dl.fbaipublicfiles.com/glue/data/RTE.zip  ###reference_TE.zip### Dagan et al. (2006  ###reference_b10###); Bentivogli et al. (2009  ###reference_b2###), which is also part of the GLUE benchmark (Wang et al., 2018  ###reference_b32###).\nWe convert all datasets to represent binary classification problems, where for three-class datasets, we collapse neutral and contradiction into not entailment, so that our model learns to distinguish entailment from not entailment. All three datasets come with predefined training examples, so we merge all three training partitions into a single training set. Since our downstream task of agenda detection contains messages that are in English and French, we automatically translate222Machine Translation model: https://huggingface.co/Helsinki-NLP/opus-mt-en-fr  ###reference_t-en-fr### 30% of the combined SNLI/MNLI/RTE training dataset from English to French.\nIn our experiments, discussed further in this article, we test whether including this data collection into the fine-tuning process improves the performance of the textual entailment approach."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   Fine-tuning Data",
            "text": "To fine-tune our model to the task of agenda detection, we use the publicly available dataset of tweets333Un-tagged Twitter corpus: https://www.kaggle.com/datasets/jeanmidev/french-presidential-online-listener  ###reference_french-presidential-online-listener###. This collection of Twitter messages contains posts on the topic of the 2022 French Presidential Elections that were made on the platform between 12 November, 2021 and 30 April, 2022. The posts, primarily written in French, were filtered using keywords including the candidate names and their associated official Twitter account, however, they do not have any agenda annotations. Since these messages were collected prior to 2023, the length of each post is limited to 280 characters.\nWe bootstrap an agenda training dataset by leveraging a multi-lingual sentence embedding model444Multi-lingual sentence embedding model: https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2  ###reference_rs/paraphrase-multilingual-mpnet-base-v2### (Reimers and Gurevych, 2020  ###reference_b23###). We implement an automatic labeling procedure for tweets by ranking messages using cosine similarity scores computed between the agenda definition embeddings (shown in Table 1  ###reference_###) and the embedding of each tweet. To ensure representation of all agenda classes in the final dataset, we gather a minimum of 500 messages with the highest similarity score for each class, automatically assigning the corresponding label. It should be noted that the number of messages retrieved for each agenda class varies, with some classes yielding more messages than others. Table 2  ###reference_### contains example messages for each of the agenda labels in French and English.\nFollowing the initial retrieval process, two human annotators independently examine the messages to confirm their alignment with the automatically assigned label, and when necessary, they reassign the appropriate agenda class labels. In instances where a message cannot be categorized into any of the agenda classes, it is designated as \"Other\". Any discrepancies or disagreements that arise during the annotation process are addressed through discussion and consensus between the annotators. After the two annotators annotate the tweets, we calculate the Inter Rater Reliability. The annotators achieve 97.5% of agreement and Cohen\u2019s Kappa of 0.89.\nThe human annotation process yields a varying number of messages per class, with quantities ranging from 96 to 120, as detailed in Total column of Table 3  ###reference_###, except for the \"Other\" class for which we randomly select 506 messages (equivalent to the cumulative sum of the other five classes). Consequently, our final multi-label dataset contains a total of 1012 annotated messages, 35 of which have more than one labels.\nThe dataset includes 10 original English tweets and 1002 original French messages. Utilizing the Google Translation API, each English message was translated into French, and vice versa. Therefore, our final dataset contains 1012 English and 1012 French texts. For more details regarding the agenda dataset and the annotation process, please refer to the Appendix 10.2  ###reference_2###.\nTo facilitate the training and evaluation of our models, we establish three training, development (dev) and testing sets, such that the dev and test sets are non-overlapping. We create the dev and test sets by taking a 10% random sample of the total number of messages. This data collection is carefully designed to be used in both textual entailment and traditional text classification methodologies, which we employ as our baseline. We make our annotated dataset and code available for public use so that it can be of benefit to future research 555GitHub Repository: https://github.com/HiyaToki/Uncovering-Agendas/  ###reference_ndas/###."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Method",
            "text": "As explained earlier, we cast the problem of agenda classification as a textual entailment problem. This enables our system to gain further knowledge from entailment datasets, essentially learning how to imitate the human decision-making process of categorizing text.\nFollowing similar methods to (Yin et al., 2019  ###reference_b36###), we depart from the traditional text classification methods where labels are denoted as indices and models lack any understanding of their specific interpretation or meaning. Instead, the labels are transformed into a set of natural language hypotheses that the input messages will be paired against and the truth value of the label can be decided. This way the system can understand the described task and the meaning of the labels by associating the input text and the context of the hypotheses."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1.   Models",
            "text": "Our proposed approach leverages the T5 (Raffel et al., 2020  ###reference_b21###) language model and its variants, mT5666mT5: https://huggingface.co/google/mt5-base  ###reference_### (Xue et al., 2020  ###reference_b35###) and T5v1.1777T5 v1.1: https://huggingface.co/google/t5-v1_1-base  ###reference_###. T5 stands out for its exceptional performance, owing to a number of key factors, such as its encoder-decoder architecture, the corrupting span denoising objective, and the utilization of an extensive pre-training dataset. Furthermore, T5v1.1 and mT5 are further enhanced by the integration of GeLU (Shazeer, 2020  ###reference_b25###) activation. mT5, in particular, has been pre-trained on over 120 languages, including French, which is of particular interest in the context of our task.\nFor our baselines models, we use BERT888BERT: https://huggingface.co/bert-base-uncased  ###reference_###, mBERT999mBERT: https://huggingface.co/bert-base-multilingual-uncased  ###reference_ual-uncased###, an English pretrained Sentnece Transformer 101010SBERT: https://huggingface.co/sentence-transformers/all-mpnet-base-v2  ###reference_rs/all-mpnet-base-v2###, as well as a muilti-lingual pre-trained Sentence Transformer111111mSBERT: https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2  ###reference_rs/paraphrase-multilingual-mpnet-base-v2###.\nFor our training processes, we use the following hyper-parameter settings for all Transformer-based models: Batch Size = 32, Epochs = 5, Weight Decay = 0.01, and Warm-up Ratio = 0.1. For the T5-based models we use a Learning Rate of 1e-4, while for BERT-based models we use a Learning Rate of 2e-5."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2.   Pre-training",
            "text": "For models trained under the Textual Entailment framework, we first train using the binarized SNLI/MNLI/RTE pre-training dataset discussed above, such that our models learn to distinguish entailment versus not entailment when a premise and a hypothesis are given as inputs. This step has been shown (Yin et al., 2019  ###reference_b36###) to improve the robustness of the model on zero-shot text classification tasks. For traditional classification approaches, we do not include a similar pre-training step."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "4.3.   Fine-tuning",
            "text": "To adapt our agenda dataset into a format suitable for Textual Entailment, we convert each unique agenda label into a hypothesis following the process described in the following section. Then, we consider each input message as a premise that has positive hypotheses (entailment) corresponding to the ground truth label, while negative labels provide negative hypotheses (not entailment). During fine-tuning, we form all possible positive (i.e., entailment) message-to-label examples (where a message may be associated with more than one agenda), and in addition, two negative examples (i.e., non-entailment) for each positive. This mimics the distribution of entailment/not entailment found in our pre-training dataset. For traditional classification approaches, each message is associated with its ground truth labels in a multi-label fashion."
        },
        {
            "section_id": "4.3.1",
            "parent_section_id": "4.3",
            "section_name": "4.3.1.   Generating Hypotheses from Agenda Labels",
            "text": "An integral part of our approach is the construction of hypotheses representing the agenda classes. Here, we use the definition of each class, see Table 1  ###reference_###, as a guide to write succinct hypotheses in natural language. We first write the hypotheses in English and use machine translation to obtain their French versions. The hypotheses we used in our experiments are listed for each class in Table 4  ###reference_###."
        },
        {
            "section_id": "4.3.2",
            "parent_section_id": "4.3",
            "section_name": "4.3.2.   Interpreting Agenda Predictions from Textual Entailment",
            "text": "Finally, textual entailment classification results can be interpreted into one or more agenda classes. As our base case, when the model predicts non-entailment for all possible hypotheses for an input example, we resolve it as the \"Other\" agenda class and output it as the final prediction for that example. In the cases where there are one or more entailment predictions for some input text, all agenda classes corresponding to those hypotheses form the final output prediction.\nWhen yielding confidence scores, for each class we look at the probability of the corresponding hypothesis being entailed. For generative models, we use the probability of the related token."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Experimental Set-up & Results",
            "text": "Our textual entailment based approach is evaluated against a range of baseline techniques, such as conventional text classification and semantic search. We adopt a multi-class multi-label approach evaluation process, as our textual entailment approach predicts the entailment of a premise (tweet message) with respect to each of the hypotheses, one class at a time.\nEach model is trained and evaluated three times, once for each individual training and corresponding testing sets. After training the models, we perform the evaluations by varying the decision threshold for each model. This threshold adjustment is based on the weighted average F1-score. This evaluation method is cost-effective as it can be performed after obtaining predictions on the test set without updating the underlying model. The decision threshold value reflects the model\u2019s confidence level, with higher values indicating greater confidence and lower values indicating that predictions with low confidence are accepted, which can result in increased False Positives. We set the minimum possible threshold to 0.3, as we are not interested in trivial scenarios where predictions include all of the available agenda labels. In the sections below, we report the averaged F1-scores across the three runs, and their standard deviation. In the Appendix 10.3  ###reference_3###, we present detailed results for each of the three runs."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "5.1.   Zero-shot Agenda Detection",
            "text": "In this experiment, we evaluate the performance of our proposed model for detecting agendas in social media on the zero-shot setting. To provide a comprehensive evaluation, we compare our model with several baselines, including BERT (Devlin et al., 2018  ###reference_b11###), SBERT (Reimers and Gurevych, 2019  ###reference_b22###) and mnli-BART121212mnli-BART: https://huggingface.co/facebook/bart-large-mnli  ###reference_-mnli###.\nOur semantic search baselines use SBERT to obtain sentence embeddings of the messages and the hypotheses, but we also test a variant using the label\u2019s text. Then, we compare the message embeddings to each hypothesis embedding using cosine-similarity. The computed score serves as the confidence that the message belongs to the agenda class specified by each hypothesis. This baseline approach yields four models, two comparing the English-only (all-mpnet-base-v2) versus the multilingual model (paraphrase-multilingual-mpnet-base-v2), and two comparing the use of hypotheses versus the labels themselves.\nFor our proposed approach in the zero-shot setting, we pre-train the models on the combination of the SNLI, MNLI, and RTE datasets. We compare the performance of the T5 model with BERT and pre-train on either the English-only or bilingual version of the combined RTE dataset. For instance, the \"t5-v1.1-base\" model is fine-tuned using the English-only version, while the \"mt5-base\" model is fine-tuned using the bilingual version. The models are then applied directly to the agenda test sets to generate predictions.\nThe availability of in-domain data is critical for training a robust classification model. Our zero-shot evaluation results (Table 5  ###reference_###) show that a lack of in-domain data leads to lower model performance. However, the models pre-trained on our combined RTE dataset using the textual entailment framework exhibit significant improvements over the Semantic Search baselines, with an overall F1-score of 0.48, demonstrating the potential of this approach. Our best performing 0-shot model, rte-en-T5, is statistically significantly better than the all the Semantic Search models at significance level  = 0.01. When we relax the significance level at  = 0.05, then rte-en-T5 is also significantly better than all other 0-shot pre-trained models, including mnli-BART."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "5.2.   Textual Entailment Agenda Detection",
            "text": "Our proposed model leverages the power of textual entailment by combining general RTE pre-training and agenda-specific fine-tuning to robustly detect agendas in short-form social media messages. In addition to our main model, we also train and evaluate BERT models and compare against variants that do not include the RTE pre-training.\nFine-tuning our models on the agenda data while following the textual entailment framework resulted in the highest performing models, as can be seen in the detailed results presented in Table 6  ###reference_###. In a multilingual setting, the mT5 model, which was pre-trained for textual entailment on our bi-lingual combined RTE dataset (with 30% of the examples translated into French) and then fine-tuned on the agenda data, outperforms all other baselines, including the conventional multi-label multi-class classification techniques. However, for English-only scenarios, direct fine-tuning of BERT achieves comparable results.\nThis highlights that pre-training the model on the combined RTE dataset has a major impact on performance, as it gives the model strong task-specific knowledge, allowing it to tackle the textual entailment problem with ease. By fine-tuning on in-domain data, we observe even better results."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "5.3.   Conventional Text Classification for Agenda Detection",
            "text": "To detect agendas in social media, we also explore traditional text classification techniques. To this end, we train classifiers for the multi-label, multi-class task using Support Vector Machines (SVM) (Boser et al., 1992  ###reference_b4###). The textual features are extracted through the TF-IDF vectorization yielding 1024 features. Additionally, we deploy BERT and T5 models as baselines, trained for sequence classification. Given T5\u2019s text-to-text architecture, we train the model using the tweet message as the source sequence and as the target sequence we use the agenda labels, represented as comma-delimited strings. During testing, T5 generates up to 32 tokens for a given text, which are then parsed and converted into agenda predictions.\nThe results from this experiment are presented in Table 7  ###reference_###. Only slightly better than our zero-shot baselines, we see relatively low performance scores. In a surprising turn of events, the Multi-label Classification (MLC) BERT-based models showed a significant decline in performance compared to their Textual Entailment counterparts, agenda-BERT and agenda-mBERT, despite being trained on the same data. In fact, agenda-BERT shows statistically significant improvement over mlc-BERT and mlc-mBERT at significance level  = 0.01, while the same can be observed for agenda-mBERT at at significance level  = 0.05.\nOur hypothesis is that by exploiting BERT\u2019s architecture, which is inherently suited for textual entailment, led to agenda-BERT\u2019s superior performance. The limited size of our agenda training data, which has far fewer examples than what is typically required to produce robust models, could also have contributed to the sub-optimal results."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "5.4.   Discussion",
            "text": "All models trained for agenda under the Textual Entailment framework preform statistically significantly better ( = 0.05) than all the 0-shot pre-trained models (model\u2019s name stating with \"rte\"), including mnli-BART and Semantic Search models. Also, they significantly outperform all of the MLC models at significance level  = 0.05. Restricting the significance level to  = 0.01, we observe that only our best performing model (agenda-rte-bi-mT5) still performs statistically significantly better than all of the 0-shot and MLC models.\nTo gain insight into the limitations of our model and identify areas that may require improvement, we calculate the confusion matrix based on the predictions generated by our best-performing model (agenda-rte-bi-mT5) on the French test set (Figure 1  ###reference_###). The multi-class multi-label nature of the task yields false positives, false negatives, introduces the presence of extra labels, where a false positive does not have a corresponding false negative, and missed labels, where a false negative does not have a corresponding false positive.\nThe agenda-rte-bi-mT5 model tends to over-label, resulting in more predicted labels than actual true labels, with the exception of \"Peaceful Protest\". Most of the excessive predictions for \"Online Solidarity\", \"Engagement\", \"Disengagement\", and \"Violent Action\" fall under the \"Other\" class, while the extra predictions for \"Other\" are dispersed among all other classes. However, this can be improved by using a better stated \"Other\" hypothesis or by including of out-of-domain \"Other\" messages in the training dataset. The example shown in Figure 1  ###reference_### does not have any missed labels, however, in the Appendix 10  ###reference_### we include cases that do. Missed labels occur when the model lacks confidence in assigning any label to the input message.\nOur model incorrectly classified some instances of \"Violent Action\" as \"Peaceful Protest\". One such message, translated into English, states \"\u2026 to get what we want, peaceful demonstrations are no use! You have to do as in Corsica or as in the suburbs!!!\". We believe that by integrating external knowledge into the textual entailment process, e.g. knowledge regarding the violent incidents that occurred in Corsica during the French Election of 2022, could lead to further improvement in the model\u2019s performance.\n###figure_1###"
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Conclusion",
            "text": "The methodology we have presented for detecting agendas with limited or non-existent labeled examples demonstrates that it is possible to overcome the need for a vast amount of annotated data. This is evident from the superior evaluation results observed by our models. Through an extensive evaluation of various techniques and approaches applied to a small corpus of annotated Twitter messages centered on the 2022 French Presidential Elections, we have shown that treating the task of text classification as a textual entailment problem produces promising results that could not have been achieved through equivalent conventional sequence classification methods.\nOur proposed model offers the advantage of not being limited to a set of predefined labels and allows for the testing of an arbitrary number of hypotheses to uncover a multitude of agendas. This versatility makes it an effective tool in detecting new and emerging influence campaigns in social media. However, the spread of agendas is not limited to just Twitter and can also occur through other media such as news articles and blogs, leading us to the next step of studying the applicability of our techniques in longer forms of text and discovering what new insights can be learned.\nOur study demonstrated that the textual entailment framework is versatile and can be effectively trained to yield competitive and reasonable results in more than one languages. Our processing pipeline can support a wide range of languages, provided that there are robust pre-trained machine translation models available. Interestingly, our findings suggest that exact translations are not necessary for the models to be able to capture the relationships between premises and hypotheses. This makes our approach flexible and adaptable to a multi-lingual environment."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7.   Acknowledgements",
            "text": "This paper is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No. HR001121C0186. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA or the U.S. Government."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "8.   Ethical Considerations and Limitations",
            "text": "In our research, we remain mindful of potential biases in the data sources and the semi-automated labeling processes, as well as the impact of our models\u2019 outputs. For instance, our data sources, which include the MNLI and SNLI corpora used to fine-tune our models, may not be representative of the general population or the target domain. The MNLI corpus does not cover all domains and may be skewed towards certain genres. It may also contain annotation artifacts or biases, such as lexical or syntactic cues, that could influence the difficulty of the task for some models. Similarly, the SNLI corpus lacks an annotation manual or guidelines, potentially leading to inconsistent or subjective judgments by the annotators. Furthermore, both corpora may suffer from indeterminacies of event and entity coreference, which could affect the interpretation and labeling of sentence pairs. Additionally, our semi-automated labeling processes may introduce noise or errors.\nAdditionally, it is essential to acknowledge that our findings are subject to the limitations posed by the size of our agenda training data and the constraints associated with using zero-shot and few-shot learning. Specifically, we note that the size of our agenda training data is relatively small compared to the large-scale datasets used for pre-training language models, which may limit the generalization and robustness of our models. The constraints associated with using zero-shot and few-shot learning are related to the dependency on the quality and relevance of the natural language hypotheses, the difficulty of generating diverse and informative hypotheses, and the challenge of evaluating the reliability of the models.\nIn the course of our experiments, we recognize the importance of ethical considerations and acknowledge certain limitations inherent to our approach. We have implemented measures to ensure responsible and respectful data collection, annotation, and model development. These measures adhere to the best practices and guidelines for data annotation and quality assurance, including the use of multiple annotators, conflict resolution, and feedback provision.\nIn our model development and evaluation, we apply principles of fairness and accountability. This involves clearly and explicitly defining the problem and the objectives of the model. We collect and analyze data relevant to the problem and objectives, ensuring it is properly labeled, cleaned, and balanced. The selection and implementation of algorithms and techniques are tailored to suit the problem and objectives. We evaluate and validate the model\u2019s performance and behavior using appropriate metrics and methods, as well as comparing the results with baseline models. Finally, we use tools and techniques to explain and interpret the model\u2019s outputs and decisions.\nDespite these measures, we remain aware of potential biases in our data sources and semi-automated labeling processes, as well as the impact of our models\u2019 outputs. We acknowledge that our data sources may not fully represent the general population or the target domain, and our semi-automated labeling processes may introduce noise or errors.\nFurthermore, we recognize that our findings are subject to limitations due to the size of our agenda training data and the constraints of zero-shot and few-shot learning. Our agenda training data is relatively small compared to the large-scale datasets used for pre-training language models, which may limit our models\u2019 generalization and robustness. The constraints of zero-shot and few-shot learning relate to the quality and relevance of the natural language hypotheses, the challenge of generating diverse and informative hypotheses, and the difficulty of evaluating the models\u2019 reliability."
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "9.   Bibliographical References",
            "text": ""
        }
    ],
    "url": "http://arxiv.org/html/2405.00821v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.3.1",
            "4.3.2"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.1",
            "5.2",
            "5.3",
            "5.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "5.1",
            "5.2",
            "5.3"
        ]
    },
    "research_context": {
        "paper_id": "2405.00821v1",
        "paper_title": "Uncovering Agendas: A Novel French & English Dataset for Agenda Detection on Social Media",
        "research_background": "**Motivation:**\nThe paper addresses the significant impact agendas\u2014collections of items to be attended to\u2014can have on the actions of a group, particularly in the context of interpersonal communication and relationships. Understanding and detecting these agendas, especially in social media, is essential for recognizing how influence is exerted in online environments, such as during political events like the 2022 French Presidential Elections.\n\n**Research Problem:**\nThe core research problem is to detect the presence of agenda-setting activities in social media, specifically on Twitter, during significant socio-political events. The challenge lies in automatically tagging tweets with suitable agenda labels that represent underlying intentions, whether explicit or implicit, using zero-shot and few-shot methods due to the lack of pre-existing annotated training data.\n\n**Relevant Prior Work:**\n1. **Conceptual Understanding of Agendas:**\n   - The notion that agendas in human communication reflect underlying intentions to steer conversations in specific directions is supported by definitions and sociolinguistics studies (Wang et al., 2018; Broadwell et al., 2013; Strzalkowski et al., 2013).\n\n2. **Agenda Setting Levels:**\n   - The social science literature identifies three levels of agenda setting: explicitly telling the public what to think and do, emphasizing certain aspects positively or negatively, and associating multiple targets to impart preferences (McCombs et al., 1997; Balmas and Sheafer, 2010; Meraz, 2011; Guo et al., 2012).\n\n3. **Textual Entailment for Text Classification:**\n   - Text classification approaches using textual entailment have shown promising results (Yin et al., 2019), where the method is used to facilitate decision-making similar to human annotators reviewing and understanding task descriptions and label definitions (Bowman et al., 2015; Williams et al., 2017; Dagan et al., 2006; Bentivogli et al., 2009).\n\nBy leveraging insights from these prior works, the authors propose a novel dataset and framework aimed at detecting agendas within social media messages, enhancing understanding of how tactical communication strategies can shape public opinion and behavior.",
        "methodology": "The proposed method for agenda detection on social media is novel in framing the task as a textual entailment problem. Here's a detailed description of the method and its key components:\n\n1. **Problem Framing as Textual Entailment**: \n   - The agenda classification problem is transformed into a textual entailment problem. This approach helps the system utilize entailment datasets and learn to simulate the human process of text categorization.\n\n2. **Departure from Traditional Text Classification**:\n   - **Traditional Method**: Typically, text classification methods use labels as indices, which offer no interpretive understanding to the model.\n   - **Proposed Method**: The method deviates from this norm by converting labels into natural language hypotheses.\n\n3. **Label Transformation**:\n   - The labels are redefined as a set of natural language hypotheses. For example, if the label is \"sports,\" the hypothesis could be, \"This message is about sports.\"\n   - The input social media messages are then paired with these hypotheses.\n\n4. **Entailment Decision**:\n   - The system determines the truth value of the label by evaluating the entailment between the input text and the hypothesis. Essentially, it assesses whether the input text supports the hypothesis.\n\n5. **Understanding Context and Meaning**:\n   - By associating the input text with the context provided by the hypotheses, the system gains a more profound understanding of both the described task and the meanings of the labels.\n   - This innovation allows the model to capture the nuances in the text better and improve categorization accuracy.\n\nThis methodology enables leveraging the advancements in textual entailment to enhance the performance and interpretability of agenda detection on social media, marking a significant step away from conventional text classification techniques.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n**Approach**: The main experiment revolves around evaluating a textual entailment-based approach for agenda detection against several baseline techniques, including traditional text classification and semantic search methods.\n\n**Datasets**: The experiment includes a novel dataset comprising French and English social media posts, specifically tweets. The data is divided into sets for training and testing, although the exact size and composition of these sets are not detailed in the description.\n\n**Baselines**: Baseline techniques involve conventional text classification and semantic search methods. The exact nature of these baselines (e.g., specific algorithms or architectures used) is not elaborated upon in the provided text.\n\n**Evaluation Metrics**: \n- The models are evaluated using a multi-class multi-label approach, where the primary metric is the weighted average F1-score.\n- Each model is trained and evaluated three times using different training and corresponding testing sets.\n- Decision thresholds for accepting predictions are varied based on the weighted average F1-score obtained from these trials.\n- The minimum threshold for model confidence is set at 0.3 to avoid trivial predictions that include all possible agenda labels.\n\n### Main Experimental Results\n\n- The results include the averaged F1-scores and their standard deviations across three different runs for each model.\n- Detailed results for each individual run are provided in an appendix (Appendix 10.3), although they are not included in the main text.\n  \nThe summary provided focuses on the averaged performance across multiple runs, emphasizing robustness and reliability of the model with respect to varying confidence thresholds."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Evaluate the performance of our proposed model for detecting agendas in social media using a zero-shot setting.",
            "experiment_process": "To comprehensively evaluate our proposed zero-shot model, we compared it with several baseline models including BERT, SBERT, and mnli-BART. The semantic search baselines utilized SBERT to obtain sentence embeddings of messages and hypotheses, comparing them using cosine similarity. We generated four models: two comparing English-only versus multilingual models, and two comparing the use of hypotheses versus labels themselves. For our proposed approach, we pre-trained models on SNLI, MNLI, and RTE datasets and fine-tuned models such as 't5-v1.1-base' for English only and 'mt5-base' for bilingual versions. These models were applied to the agenda test sets to generate predictions.",
            "result_discussion": "Our zero-shot evaluation results revealed that the absence of in-domain data results in lower performance. However, models pre-trained with the textual entailment framework on the combined RTE dataset exhibited significant improvements, achieving an overall F1-score of 0.48. Notably, the 'rte-en-T5' model outperformed all semantic search models and, at a relaxed significance level, all other zero-shot pre-trained models including 'mnli-BART'.",
            "ablation_id": "2405.00821v1.No1"
        },
        {
            "research_objective": "Examine the efficacy of combining general RTE pre-training and agenda-specific fine-tuning for agenda detection in social media messages.",
            "experiment_process": "We trained and evaluated models utilizing textual entailment frameworks, including BERT variants, comparing those with and without RTE pre-training. Multilingual models like mT5 were pre-trained on a bi-lingual RTE dataset (30% examples translated into French) before fine-tuning on agenda data. Models were evaluated against baselines including conventional multi-label classification techniques.",
            "result_discussion": "Our fine-tuning approach yielded the highest performing models as detailed in Table 6. The mT5 model, fine-tuned with bi-lingual RTE data, outperformed all baselines. Notably, direct fine-tuning of BERT achieved comparable results in English-only scenarios. The major impact of pre-training on the combined RTE dataset was highlighted, providing strong task-specific knowledge and enhancing performance when further fine-tuned with in-domain data.",
            "ablation_id": "2405.00821v1.No2"
        },
        {
            "research_objective": "Investigate the effectiveness of traditional text classification techniques for agenda detection in social media.",
            "experiment_process": "We employed traditional multi-label, multi-class classification using SVMs and extracted textual features via TF-IDF vectorization. Additionally, BERT and T5 models were used as baselines, trained for sequence classification. We used T5's text-to-text architecture, formatting tweet messages as source sequences and agenda labels as target sequences. During testing, T5 generated up to 32 tokens per text, converted into agenda predictions.",
            "result_discussion": "Results indicated relatively low performance scores despite being slightly better than zero-shot baselines. Surprisingly, multi-label classification BERT-based models underperformed compared to textual entailment counterparts (agenda-BERT and agenda-mBERT), which showed significant improvements. The hypothesis was that BERT's architecture suitability for textual entailment led to superior performance, with limited training data size being a probable cause for sub-optimal results in traditional classification.",
            "ablation_id": "2405.00821v1.No3"
        }
    ]
}