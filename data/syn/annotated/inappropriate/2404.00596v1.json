{
    "title": "ECtHR-PCR: A Dataset for Precedent Understanding and Prior Case Retrieval in the European Court of Human Rights",
    "abstract": "In common law jurisdictions, legal practitioners rely on precedents to construct arguments, in line with the doctrine of stare decisis. As the number of cases grow over the years, prior case retrieval (PCR) has garnered significant attention. Besides lacking real-world scale, existing PCR datasets do not simulate a realistic setting, because their queries use complete case documents while only masking references to prior cases. The query is thereby exposed to legal reasoning not yet available when constructing an argument for an undecided case as well as spurious patterns left behind by citation masks, potentially short-circuiting a comprehensive understanding of case facts and legal principles. To address these limitations, we introduce a PCR dataset based on judgements from the European Court of Human Rights (ECtHR), which explicitly separate facts from arguments and exhibit precedential practices, aiding us to develop this PCR dataset to foster systems\u2019 comprehensive understanding. We benchmark different lexical and dense retrieval approaches with various negative sampling strategies, adapting them to deal with long text sequences using hierarchical variants. We found that difficulty-based negative sampling strategies were not effective for the PCR task, highlighting the need for investigation into domain-specific difficulty criteria. Furthermore, we observe performance of the dense models degrade with time and calls for further research into temporal adaptation of retrieval models. Additionally, we assess the influence of different views , Halsbury\u2019s and Goodhart\u2019s, in practice in ECtHR jurisdiction using PCR task.\n\n\n\nKeywords:\u2009Prior Case Retrieval, Temporal Robustness, Common Law, ECHR",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "Legal systems globally can be broadly categorized into two main frameworks: common law and civil law Joutsen (2019  ###reference_b13###). In common law countries, such as the United States, England and India, published judicial opinions, known as case law, hold primary significance. Conversely, civil law systems, as observed in countries like China, Japan, France and Germany, place greater emphasis on codified statutes. Naturally, these distinctions are not always rigidly defined and in reality, many countries have adopted a combination of features from both.\nLegal practitioners in common law jurisdictions rely on existing case decisions, known as precedents, as a vital source of law,\nbased on the doctrine of stare decisis, which can be translated from Latin as \"to stand by the decided cases.\" It emphasizes that when a case being considered shares similarities with past cases, it should be treated in a manner consistent with those precedents Duxbury (2008  ###reference_b6###); Lamond (2005  ###reference_b17###). By examining previous rulings and citing relevant precedents, legal professionals establish the relevant law applicable to the current case and strengthen their arguments Mandal et al. (2017  ###reference_b21###); Shulayeva et al. (2017  ###reference_b31###).\nWith the increasing volume of cases, there is a growing demand for automatic precedent retrieval systems to aid practitioners by providing prior cases relevant to the current case. The task of Prior Case Retrieval (PCR) has gained attention in the legal and information retrieval communities, leading to the curation of datasets such as COLIEE Kim et al. (2018  ###reference_b15###) and IRLeD Mandal et al. (2017  ###reference_b21###), which have facilitated advancements in PCR using case law from the Federal Court of Canada and Indian Supreme Court respectively. These datasets define relevance criteria as the precedents cited in the query document, utilizing citation as a signal to construct the dataset. While civil law systems may not directly involve prior cases in the final judgment, recognizing their importance as references for consistency in decision-making, efforts have been made to curate PCR datasets tailored to the Chinese law system, such as CAIL-SCM Xiao et al. (2019  ###reference_b33###) and LeCaRD Ma et al. (2021  ###reference_b20###), which relied on legal professionals\u2019 expertise due to the absence of a citation structure in civil law case documents, resulting in smaller dataset sizes.\nPrior PCR datasets in common law (COLIEE, IRLeD) treated entire case documents as queries, suppressing citations to prior cases by using \"FRAGMENT SUPPRESSED\" / \"CITATION\" tags. However, this approach fails to effectively address the problem and raises concerns about evaluating PCR systems effectiveness. Case documents comprise both factual aspects, describing the circumstances of the case, and argumentative aspects, encompassing judges\u2019 reasoning, citations to prior cases, legal tests, analysis, and explanations for their decisions. By solely suppressing citations, models or systems tend to exploit the pattern of citation suppression fragments and rely exclusively on the citance (text preceding the citation) to identify relevant prior cases. Additionally, the problem is exacerbated when dealing with extractive (verbatim) citances, where the citance in the query is directly extracted from the cited document Rossi et al. (2021  ###reference_b27###). This simplifies the PCR task to an exact matching problem, potentially overestimating the system\u2019s performance, disregarding the nuanced aspects of the case.\nMoreover, in a practical scenario, the arguments section of a case is often available only after the final verdict has been delivered, while only the factual aspects are accessible prior to the verdict. Thus, the system must rely solely on understanding and analyzing the facts to determine the relevance of prior cases in reaching the outcome of the current case. However, identifying relevance itself presents a challenge, as legal scholars may not unanimously agree on what factors constitute the ratio decidendi, which refers to the binding reasons for a decision that have an impact on subsequent cases Valvoda et al. (2021  ###reference_b32###). While Halsbury 1907  ###reference_b10### contends that the judge\u2019s reasoning and arguments are what bind, Goodhart 1930  ###reference_b8### argues for the analogy between the facts of the precedent and the current case. Hence, the PCR model should ideally learn the nature of relevancy based on the distribution of available data and account for the divergent perspectives on what factors contribute to the ratio decidendi.\nGiven the limitations of existing datasets, it is crucial to develop a PCR dataset that enables systems to acquire a comprehensive understanding of the case facts, legal principles, and the broader context to ensure the effective application of precedent in legal decision-making. In this work, we construct a dataset for the task of PCR within the context of European Court of Human Rights (ECtHR) 111Our PCR dataset is available at https://github.com/TUMLegalTech/ECHR-PCR  ###reference_###, which adjudicates complaints by individuals against states about alleged violations of their rights as enshrined in the European Convention of Human Rights. The choice of ECtHR judgements is motivated by two primary reasons. Firstly, unlike many other courts, such as those in India and Canada, the ECtHR\u2019s case law documents explicitly separate the facts from the arguments, which is vital for ensuring that the queries used in the PCR system do not contain the argument/reasoning sections. Secondly, the ECtHR\u2019s citation practices in its arguments section indicate a reliance on precedential law, similar to common law countries. Although the ECtHR is an international court without a formal doctrine of stare decisis Jacob (2014  ###reference_b11###), there is strong evidence suggesting a precedential nature. This evidence can be found in the court\u2019s own guidelines 222www.echr.coe.int/Documents/Guide_ECHR_lawyers_ENG.pdf  ###reference_ers_ENG.pdf### and www.echr.coe.int/documents/50questions_eng.pdf  ###reference_.pdf###\n as well as in the works of former judges Zupancic (2016  ###reference_b37###) and legal scholars Lupu and Voeten (2010  ###reference_b19###). Through its extensive case-law, the ECtHR has played a vital role in shaping the interpretation of the convention, effectively transforming it into a living instrument.\nUsing our curated ECtHR-PCR dataset, we benchmark both lexical and dense retrieval based approaches employing different negative sampling strategies, modifying them to hierarchical variants, to accommodate the typically long text sequences. We observe that difficulty-based negative sampling strategies, which proved effective in IR tasks, did not translate to PCR task, highlighting the need to design domain-specific difficulty criterion. Further, we observe that the performance of dense models degrade over time due to their inability to accommodate to newer evolving unseen documents, highlighting the need for temporal adaptation of retrieval models. We also put Halsbury\u2019s and Goodhart\u2019s views to the test in practice, by comparing retrieval performance using the facts or the reasoning of the documents alone. Our experimental evidence points out that Halsbury\u2019s view is more widely practiced in ECtHR jurisdiciton, compared to Goodhart\u2019s."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1.   Existing PCR Datasets",
            "text": "The field of legal information retrieval has seen several notable competitions and datasets for PCR. Information Retrieval from Legal Documents (IRLeD) Mandal et al. (2017  ###reference_b21###) uses 200 Indian Supreme Court cases as queries, with 1000 relevant prior cases (randomly sampling 5 per query) and 1000 irrelevant documents.\nOne of the tasks in Competition on Legal Information Extraction/Entailment (COLIEE) Kim et al. (2018  ###reference_b15###), which has been held annually since 2014, focuses on PCR in Canadian case law. It includes 898 and 300 query cases as well as 4415 and 1564 candidates cases for training and testing test respectively. IRLeD and COLIEE datasets use citations as signals to identify relevant documents while citation markers are suppressed using special tags in the document content.\nAdditionally, there are other datasets for the civil law framework, specifically based on the Chinese legal system, that utilize expert annotations. The CAIL2019-SCM dataset Xiao et al. (2019  ###reference_b33###) comprises 8,964 triplets, each consisting of a query case, a relevant case, and an irrelevant case. Similarly, the LeCARD dataset Ma et al. (2021  ###reference_b20###) includes 107 query cases and 10,700 candidate cases.\nCompared to existing datasets, our dataset has notable differences. We only utilize the facts section of a case as the query, which reflects the practical scenario where the arguments section is typically inaccessible until after the final verdict. In contrast to previous datasets that randomly select a small number of irrelevant candidates, our dataset encompasses the complete case law of the European Court of Human Rights since 1960, providing a larger pool of potential candidate documents. This presents a greater challenge in identifying the relevant documents for each query. For more detailed distinctions between our dataset and existing ones, please refer to Table 2  ###reference_###."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2.   Tasks on ECtHR Corpora",
            "text": "Previous works focused on the judgements corpus of the European Court of Human Rights (ECtHR) have primarily been limited to the task of Judgment Prediction, Aletras et al. (2016  ###reference_b1###); Medvedeva et al. (2020  ###reference_b22###); Chalkidis et al. (2019  ###reference_b2###); Santosh et al. (2023a  ###reference_b28###, b  ###reference_b29###), defined as predicting whether any article of the convention has been violated given a fact statement, argument mining Habernal et al. (2023  ###reference_b9###); Poudyal et al. (2019  ###reference_b24###, 2020  ###reference_b25###), vulnerability detection Xu et al. (2023  ###reference_b35###), and event extraction Filtz et al. (2020  ###reference_b7###); Navas-Loro and Rodriguez-Doncel (2022  ###reference_b23###). Valvoda et al. 2021  ###reference_b32### has explored the two jurisprudential views of Halsbury and Goodhart on what constitutes a ratio based on the ECtHR corpus, modelling the question as a case outcome classification task.\nTo the best of our knowledge, our work is the first to study PCR on ECtHR jurisdiction. We offer this dataset to the research community to facilitate further advancements and research in the area of prior case retrieval."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   ECtHR-PCR Dataset",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   Dataset Construction",
            "text": "We create our ECtHR-PCR dataset in four steps: (i) Collecting case documents and filtering (ii) Parsing case documents into the facts and the reasoning (law) sections (iii) Extracting citations from each document (iv) Mapping the citations to the actual documents.\nDocument Collection & Filtering: We gather the complete collection of documents as an HTML data dump from HUDOC333https://hudoc.echr.coe.int/  ###reference_hudoc.echr.coe.int/###, the public database of the ECtHR, including their metadata. We apply a filtering process to retain only the English judgment documents based on the metadata (\"Document Type: HEJUD\"). Each judgment document in our dataset is associated with an application number, which serves as a unique identifier for individual applications submitted to the ECtHR.\nThe court may merge multiple applications for procedural purposes if they are related. Consequently, a single judgment document can be associated with multiple application numbers. Furthermore, there are instances where a case initially decided by a Chamber is later referred to the Grand Chamber upon the parties\u2019 request. In such scenarios, the Grand Chamber produces a new judgment document that retains the same application numbers. To mitigate potential conflicts in the citation mapping process during subsequent stages, we employ a strategy of creating distinct versions of these cases by appending a version number to each document, ensuring that they remain distinguishable by their application numbers.\nParsing Documents:\nThe judgement documents follow a specific structure, covering different aspects of the case, as outlined in Rule 74 of the Rules of the Court444https://www.echr.coe.int/Documents/Rules_Court_ENG.pdf  ###reference_ourt_ENG.pdf### under different sections. Here is a concise overview of the primary sections: Procedure: outlines the procedural steps, from the submission of the individual application to the final delivery of the judgment. The Facts: encompasses the factual background of the case along with the procedure typically followed in domestic courts prior to the application being lodged with the Court under the sub-section The Circumstances of the Case and also contains legal provisions of domestic law relevant to the case under the sub-section Relevant Law555These legal provisions do not contain information about the articles of the ECtHR Convention. Rather they are provisions of domestic law and other pertinent international or European treaties and materials.. The Law: provides the legal reasoning to justify the specific outcome reached with respect to allegation of articles of the ECtHR convention. It places its reasoning within a broader framework of established rules, principles, and doctrines derived from its previous case-law, and grounds the decision by providing an explicit reference to those judgement documents. Conclusion: declares the Court\u2019s verdict regarding whether a violation of the alleged article has occurred or not.\nParsing the judgment documents into these sections presented challenges because the HTML lacked a consistent structure. To address this, we developed various hand-crafted rules to identify the headers of the sections. Additionally, we utilized regular expressions to classify the main section each header belongs to, taking into account inconsistencies in their naming patterns. As a result, we were able to extract the facts of the case from the The Circumstances of the Case subsection and the reasoning sections from the Relevant Law and The Law sections. For a small number of documents (0.1%) where the fact or reasoning sections were not obtained through parsing, we manually labeled them.\nCitation extraction: We observe that citations to prior judgements by the court, referred to as \"Strasbourg Case Law\" (SCL), are available in the HUDOC case metadata. It appears that the SCL information has been manually compiled and often copied verbatim from the judgment text as the citation string. However, we discovered that  60% of the cases are missing SCL metadata, and even when SCL is present, it is incomplete. To address this issue, we developed three strategies for automatically extracting citations from the raw text of documents. Firstly, we employed regular expressions to identify the presence of application numbers referenced in the document. Secondly, we iteratively created several regular expressions to capture instances where the document mentioned \u2019v.\u2019 followed by variations of ECtHR country names and a specific date format (e.g., Schenk v. Switzerland, 12 July 1988, \u00a7 46, Series A no. 140). Lastly, we noticed that cases prior to 1999 had a different citation format (eg., Van Leuven and De Meyere judgment of 23 June 1981, Series A no. 43, pp. 25-26, para. 5) Therefore, we utilized the citation strings obtained from the SCL metadata and searched for exact matches within the documents to extract the relevant citations.\nMapping Citations to Documents:\nThe citation strings we obtained lack a consistent structure; however, they commonly include elements such as the plaintiff, defendant (optional), application number (optional) and date (optionally: day, month, and year). To ensure a high recall in extracting citations, we developed multiple heuristics for matching these citation patterns back to case documents. Whenever multiple versions of applications are available, we accurately point the citation to the exact version based on the date, ensuring that it points to the latest version that occurred prior to the document from which the citation originated."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   Manual Quality Inspection",
            "text": "We evaluate the quality of our dataset using a random sample of 120 documents for which we manually extract all the citations and filter out the ones for which we could not resolve the outgoing citations, such as if the cited document was not in English or not available in the HUDOC database. Precision and recall of the automatically extracted citations was calculated per document and average values are displayed in Table 1  ###reference_###.\nUsing SCL metadata alone results in low recall, as only 56 out of 120 documents have SCL metadata. Mapping citation strings to documents proves to be challenging even with SCL information, as reflected by the precision not being 1.0. Extracting application numbers mentioned in documents improves recall but suggests that not all citation formats include application numbers. Despite obtaining application numbers, precision is lower as some numbers resemble the regular expression of ECtHR application numbers but refer to external national case documents. Our method, combining SCL, application numbers, and citation string extraction using patterns like \u2019v.\u2019 and others, shows improved recall, indicating the high quality of our constructed dataset."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "3.3.   Dataset splits & Analysis",
            "text": "The above construction process resulted in 15,729 ECtHR judgements available in English up until July 2022, each subdivided into the facts section and the reasoning section. For each document, we obtain a valid list of citations extracted from the document. The entire dataset is chronologically split into training (9.7k, 1960\u20132014), development (2.1k, 2015\u20132017), and test (3.2k, 2018\u20132022) sets. Table 2  ###reference_### provides a comparison of our dataset with prior datasets. We highlight two ways in which our dataset is distinctively more reflective of a realistic scenario, compared to prior datasets.\nFirstly, unlike prior works which use the whole document as the query and only suppress citation markers, our queries only use the content of the facts section. We posit that the reasoning sections of the case are inexistent before the verdict of the case is finalized and only the facts of the case are available as input to retrieve relevant documents. In contrast, for candidate prior documents, we use both the facts and the reasoning sections (and also compare retrieval performance on each section individually). The stark difference between average number of tokens in query and candidate documents reflects this design choice in our case.\nSecondly, the pool of candidate documents in prior datasets does not reflect a realistic scenario as they are constructed artificially by selecting relevant documents 666While COLIEE uses all the documents cited in the query as relevant, IRLeD samples 5 relevant documents per query. and randomly sampling irrelevant documents to make a static candidate pool for all the queries. In contrast, we use the entire case-law of ECtHR as the candidate document pool to reflect the scale of a realistic retrieval scenario. Furthermore, our candidate pool is dynamic because for each query, we exclude from the candidate pool those cases dated after the query case.\n###figure_1### ###figure_2### In Fig. 1(a)  ###reference_sf1###, we present the distribution of outgoing citations for all documents. Meanwhile, Fig. 1(b)  ###reference_sf2### depicts the distribution of incoming citations, reflecting how often each document has been referenced by other cases. Roughly 6,000 cases have not received any citations. Notably, about 1,500 of these cases are from the most recent five years, representing over 50% of cases in this period. This is expected since recent cases typically take time to gain recognition in the legal community. On the other hand, approximately 1,700 cases have been cited exactly once, while the rest have received multiple citations. The frequency of incoming citations serves as an indicator of a case\u2019s influence and relevance within the legal domain. Cases that have been cited multiple times likely contain valuable legal arguments, reasoning, or precedents widely recognized and referenced by other cases. Therefore, considering citation frequency as a signal can be beneficial for modeling relevance in prior case retrieval and should be considered in future efforts."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Models",
            "text": "Task Defintion: The aim of PCR is to learn a retrieval function  that takes as input a query case  (consisting of facts  and timestamp  )\nand a corpus of candidate case documents  prior to the case  i.e\n (Document  consists of facts , reasoning  and is associated with a timestamp  and )\nand returns a ranked list of candidate case documents  based on their relevance score with the query ."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1.   Lexical Models",
            "text": "Simple keyword matching without term frequency or inverse document frequency weighting is a bag-of-words based method which computes the relevance score based on the query terms appearing in document."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2.   Dense Models",
            "text": "Lexical approaches rely on bag-of-words matching and face the lexical gap issue, whereby they consider only documents with the same keywords as the query to be relevant. Recent studies Karpukhin et al. (2020  ###reference_b14###); xiongapproximate have explored dense-based architectures which can effectively capture semantic relationships between queries and documents by matching them in a continuous representation space learned via deep neural networks.\nOne of the common approach of dense models, employs a dual-encoder architecture, consisting of a query encoder  and a document encoder  which transform query q and document d into m-dimensional dense vectors  and  respectively. Then, the relevance score of d with respect to q can be computed using the dot product.\nTo capture the similarity between queries and documents, complex interaction mechanisms like cross-attention can be used. But it is important for the similarity function to be decomposable, enabling pre-computation of document representations for scalability, especially with large document collections. Post training, the document encoder is applied to all documents offline and their final representations are indexed using a datastore. At the run time, when given a query , we obtain its embedding  and retrieve the top k documents with embeddings closest to  from the datastore using maximum-inner product search (MIPS). We use FAISS, an efficient open-source library for building datastore data stores of dense vectors which can be queried using similarity search. Johnson et al. (2019  ###reference_b12###).\nEncoders:\nDense retrieval methods often employ pre-trained language models like BERT Devlin et al. (2018  ###reference_b5###) and RoBERTa Liu et al. (2019  ###reference_b18###) as backbone encoders. However, in our case, both queries and documents are lengthy, exceeding the input length limitation of 512 tokens. Hence, we use a BERT variant of Hierarchical Attention Networks Yang et al. (2016  ###reference_b36###) as our encoder, adopted from previous works Santosh et al. (2022  ###reference_b30###). To handle long input texts, we employ a greedy sentence packing strategy. This strategy involves packing as many sentences as possible into one packet until it reaches the maximum length limit (512 tokens for BERT). Each packet is then encoded using LegalBERT Chalkidis et al. (2020  ###reference_b3###) to obtain token-level representations . A token attention layer is utilized to aggregate the token representations into packet vectors as follows:\nwhere ,  and  are trainable parameters and  represents the importance of  token in the  packet.\nThese packet vectors are processed using a GRU encoder to obtain contextual representations. Finally, these are aggregated using a sentence attention layer (similar to Eq. 2  ###reference_###) to obtain the final dense representation of the input. We explore two variants of encoders. (i) Uniencoder Xiong et al. (2021  ###reference_b34###), which uses the same query and document encoder in a shared vector space, and (ii) biencoder Karpukhin et al. (2020  ###reference_b14###), which uses independent query and document encoder in different embedding spaces.\nTraining: The training objective of the dense text retrieval task is to pull the representations of the query  and relevant documents  together, while pushing away irrelevant ones . We optimize the negative log likelihood of the relevant documents against irrelevant ones as follows:\nNegative sampling: To train a dense retrieval model effectively, proper negative document sampling is crucial, considering the large number of negative documents per query. We explore three different negative sampling strategies. 777We obey the timestamp constraint even with all the negative sampling strategies during training i.e timestamp of query is always greater than timestamp of negative documents.\nRandom Karpukhin et al. (2020  ###reference_b14###): any random documents from the candidate pool which are not relevant to the query.\nBM25 + Random Karpukhin et al. (2020  ###reference_b14###): combines negatives from BM25 and random sampling.\nANCE Xiong et al. (2021  ###reference_b34###): Approximate Nearest Neighbor Negative Contrastive Learning (ANCE) suggests that random negatives are uninformative, and BM25 negatives overfit the model to lexical signals. ANCE proposes selecting top-k hard negatives ranked by the dense retrieval model itself. Negatives are asynchronously updated after each epoch using the trained model."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Experiments",
            "text": ""
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "5.1.   Evaluation Metrics",
            "text": "We evaluate the performance using Recall@k and Mean Average Precision (MAP). Recall@k measures the proportion of relevant documents ranked in the top k candidates, with k values of 50, 100, 500, and 1000. We report the average Recall@k across all instances. MAP calculates the mean of the Average Precision scores for each instance, where Average Precision is the average of Precision@k scores for every rank position of each relevant document. Precision@k denotes the proportion of relevant documents in top k candidates. Higher scores indicate better performance for Recall@k and MAP. Additionally, we report Mean Rank and Median Rank, which are the averages of the median and mean ranks of relevant documents for each instance. Lower values are preferred for both of these metrics"
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "5.2.   Implementation Details",
            "text": "BM25: The hyperparameters  and  are sweeped in the range[0, 3] and [0, 1] respectively, to pick the best value based on the validation set.\nDense Models: For all of our dense models, we use LegalBERT Chalkidis et al. (2020  ###reference_b3###) as the backbone, which produces a word embedding size of 768. Our word level attention context vector size is 300. The sentence level GRU encoder dimension is 200, thus giving a bidirectional embedding of size 400, and a sentence level attention vector dimension of 200. We use 4 queries in a batch with 1 positive and 7 negatives per query, resulting in 32 cases per batch. The model is optimized end- to-end using Adam Kingma and Ba (2014  ###reference_b16###). We determine the best learning rate using a grid search on the validation set and use early stopping based on the development set and the Recall@1000 score."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "5.3.   Results",
            "text": "We report the results on the test set of the ECtHR- PCR dataset in Tables 3  ###reference_###, 4  ###reference_### and 5  ###reference_###. In both these tables, note that we use only the facts for query case and both the facts and the law section for documents.\nBM25 vs Dense Models:\nBM25 performs competitively with dense models and even better than DR-Uniencoder across most of the metrics. This can be attributed to the high lexical overlap and repeated usage of legal tests and concepts in legal cases as effectively captured by BM25. While dense modeling excels at capturing semantic relationships and contextual information, it seems to fail to capture the lexical overlap of the legal documents, as evidenced by the DR-Uniencoder\u2019s lower performance. BM25 proves particularly more effective at lower k values, underlining how high lexical overlap provides a better signal of relevance.\nUniencoder vs Biencoder: We observe that the biencoder model significantly outperforms the uniencoder model across all metrics, surpassing even BM25 at higher k values. This can be attributed to the differing semantics between queries and documents, as queries contain only the factual statements of the cases while documents contain both the factual statements and the reasoning section. This necessitates two separate encoders to effectively capture these distinct aspects.\n###figure_3### Impact of negative sampling:  We use the DR-Biencoder architecture and vary different negative sampling strategies and report their performance in Table 4  ###reference_###. DR-Rand model is significantly better than the DR-BM25+Rand and the DPR-ANCE model in all the evaluation metrics. This reveals that training on difficulty-based hard negatives yielded lower performance compared to training on randomly selected negatives, which is surprising considering both hard-negative strategies were effective in traditional IR tasks. It is possible that these hard-negative selection strategies, by selecting negatives which are lexically (BM25) or semantically (ANCE) similar to the query, are actually selecting relevant documents that simply have not been cited (false negatives), a fact that would be less problematic in the traditional question-based querying of standard information retrieval research. Comparing difficulty-based strategies, ANCE sampling performs better than BM25+Rand, highlighting that focusing on distinguishing lexically similar candidates through BM25 sampling, results in over specialization that neglects broader variants of negatives in the entire corpus."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "5.4.   Temporal Degradation",
            "text": "We assess how DR-Uni and -Bi models perform over time and we present the Recall@1000 scores for BM25 and DR-Rand models for both architectures over the test set period (2018- 22), separately for each year in Fig. 2  ###reference_###. Both the dense models exhibit a more pronounced decline compared to BM25 in recent years due to temporal distributional shift of ML models - difference between the distribution of the training and the test data caused by the addition of new case documents to the candidate pool over time. In the earlier years of the test set (2018-2020), most citations come from cases seen during training, but in later years citations point towards unseen cases which are added subsequently (directly in validation set or even early years of test set). To address this temporal drift, future research should focus on continual updating of dense encoders with arrival of new candidate documents and enhancing model robustness to temporal shifts in the data, enabling better adaptation to evolving legal landscapes."
        },
        {
            "section_id": "5.5",
            "parent_section_id": "5",
            "section_name": "5.5.   ECtHR: Halsbury\u2019s or Goodhart\u2019s view ?",
            "text": "We quantitatively assess the notion of relevance as per Halsbury\u2019s and Goodhart\u2019s views in practice, by using only the reasoning and the facts section for the documents in candidate pool, respectively. From Table 5  ###reference_###, we observe that using law section of the document alone turns more effective than using the facts section alone, as witnessed across the three model variants. This finding provides more evidence supporting Halsbury\u2019s view in the ECtHR domain compared to Goodhart\u2019s view, which aligns with the findings of Valvoda et al. 2021  ###reference_b32###.\nComparing Table 5  ###reference_### to Table 3  ###reference_###, we observe that using the law section alone proves more effective than using the entire document. This can be attributed to the fact that the law section attempts to provide arguments on why a certain article is violated by grounding them in the particular circumstances of the case. In other words, the important premises of the facts section are discussed in the law section, allowing the model to directly focus on the relevant factual information presented in the law section. Further, adding facts along with the reasoning section tends to shift the model\u2019s focus with unnecessary additional information, leading to a drop in performance. Further investigation is required to analyze these conjectures in relation to each of the alleged articles present in each case."
        },
        {
            "section_id": "5.6",
            "parent_section_id": "5",
            "section_name": "5.6.   Discussion",
            "text": "Overall, we notice a low score across the board and highlight some challenges and directions to be pursued to build effective prior case retrieval systems. One limitation of our proposed baselines is their focus solely on the text content, neglecting the potential insights from the citation network with its interconnected relationships providing a rich global view of case law.\nAnother significant challenge arises with dynamic evolution of law with time incorporating changes in norms, societal attitudes, and values and time-dependent nature of precedents, as they can be overruled, resulting in the expansion or contraction of the law. Notably, the ECHR convention was intentionally drafted in an abstract manner to allow for interpretation and to encompass a wide range of situations, distinguishing it from more specific national legal codes. Exploring methods to capture the temporal nature of precedents would be an interesting direction.\nFurthermore, in order to achieve a comprehensive understanding of relevance in prior case retrieval, it is crucial for an ideal PCR model to not only comprehend the case facts but also deduce the reasoning process that can be applied to the query facts. The model would be then able to identify prior cases that support the reasoning process. This can be facilitated by allowing the model to generate the reasoning process initially and then retrieve the document that substantiates it. By incorporating such mechanisms, the model enhances its explainability by providing a rationale for the relevance of a specific candidate document\nExploration of above discussed strategies such as incorporating various nuances of citation modeling for better relevance understanding, has been hindered due to limitations in prior datasets. These limitations include the lack of a complete citation network, timestamp information of documents, fragmented splits of candidate document pool and artificially constructed snapshots of the data by randomly selecting a few relevant and irrelevant cases for the candidate document pool. However, our ECtHR-PCR dataset overcomes these limitations by providing the entire case law of the European Court of Human Rights since its inception. With this comprehensive dataset, researchers now have the opportunity to explore realistic relevance modeling that emulates how legal experts select prior cases for citation, enabling a more accurate representation of the complexities involved."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Conclusion",
            "text": "In this work, we present the ECtHR-PCR, prior case retrieval dataset for jurisdiction of European Court of Human Rights. We benchmark various retrieval baselines, including both lexical-based and dense retrieval models and observe that difficulty-based negative sampling underperforms random negative sampling, highlighting the need to design domain-specific difficult criterion to train effective dense retrieval systems. We demonstrate how neural dense models degrade with time, while BM25 is temporally robust, highlighting the need of developing retrieval models which can handle temporal distributional shifts. Furthermore, we empirically examine the contested Halsbury\u2019s and Goodhart\u2019s view on what constitutes a ratio based on relevance signal and witness a prevailing trend of Halsbury\u2019s view that reasoning and arguments hold more weight in determining relevance.\nIn future, we intend to develop retrieval models that incorporate explicit modeling of the citation network, taking into account the specific characteristics of citation behavior in case law documents. We hope that this data resource will be useful to the research community working on problems in the space of legal language processing and information retrieval."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7.   Limitations",
            "text": "It is important to acknowledge that the ECtHR-PCR dataset created in this study may exhibit a linguistic bias, as French is the other official language of ECtHR along with English and hence some documents in these English judgements may cite non-English ones which have been filtered out. Some of them might indeed contain relevant information needed but not considered in our dataset version. Future efforts should be taken to improve both the recall and precision of our dataset. For instance, we could develop more heuristics to decode the citation string format and map them back to the document to add the relevant documents or we could develop further tigh\nDealing with legal corpora presents technical challenges, particularly due to the lengthy nature of legal texts. To address this, we employ hierarchical models, which have inherent limitations in terms of the direct interaction between tokens across long distances. While this restriction is still an underexplored aspect of hierarchical models, recent work such as Dai et al. (2022  ###reference_b4###) provides preliminary insights in this direction. Furthermore, to optimize computational resources, we freeze the weights in the LegalBERT sentence encoder. It is important to note that there are other limitations in our modeling approach, which have been discussed earlier.\nIn terms of evaluating the quality of the retrieved relevant documents, we plan to conduct human evaluation in the future to assess the utility of our system, which would be challenging given the length and complexity of the legal text involved which requires annotators with a deep understanding of ECHR jurisprudence to understand and evaluate the notion of relevance. It is worth noting that the models explored in this work primarily act as pre-fetchers, prioritizing recall to ensure that all relevant cases are retrieved. However, in practice, end-users expect a high-quality retrieval system that precisely identifies a smaller number of relevant cases. This requires an additional re-ranker step in the retrieval pipeline to optimize the precision of the ranked list. In this study, we focus on the first step of the retrieval pipeline, leaving the development of a re-ranker component for future work."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "8.   Ethics Statement",
            "text": "We compiled this dataset by sourcing ECtHR decisions from the publicly accessible HUDOC website. These decisions, although not anonymized, include the real names of individuals involved. However, our work does not engage with the data in a way that we consider harmful beyond this availability. Rather, we believe development of effective PCR would further assist legal professionals, to deal with the increasing volume of cases. We employ pre-trained language models and do not train them from scratch, thus inheriting the biases they may have acquired from their training corpus. We recognize the need for thorough analysis and mitigation of any biases that may arise and it is is crucial to ensure that the PCR systems we develop are fair, unbiased, and uphold principles of equality and justice."
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "9.   Bibliographical References",
            "text": ""
        }
    ],
    "url": "http://arxiv.org/html/2404.00596v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "3.1",
            "3.2",
            "3.3",
            "4",
            "4.1",
            "4.2"
        ],
        "main_experiment_and_results_sections": [
            "5.1",
            "5.2",
            "5.3",
            "5.4",
            "5.5",
            "5.6"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.2",
            "5.3",
            "5.4",
            "5.5"
        ]
    },
    "research_context": {
        "paper_id": "2404.00596v1",
        "paper_title": "ECtHR-PCR: A Dataset for Precedent Understanding and Prior Case Retrieval in the European Court of Human Rights",
        "research_background": "**Motivation:**\nThe motivation behind this paper is to address the growing demand for automated systems that assist legal practitioners in retrieving relevant precedents to apply to current cases. This is particularly relevant in jurisdictions where case law plays a significant role, as it helps legal professionals establish applicable laws and strengthen their arguments by examining past rulings. The need for effective Prior Case Retrieval (PCR) systems is driven by the increasing volume of cases, which makes manual retrieval challenging. \n\n**Research Problem:**\nThe primary research problem is the development of a more effective and comprehensive dataset for PCR that goes beyond the limitations of existing datasets, which often suppress citations and fail to adequately separate the factual aspects of cases from the argumentative ones. Specific concerns with prior datasets include their reliance on citation patterns, potential for simplification to exact matching problems, and their generalizability to real-world scenarios where only factual aspects might be available prior to a verdict. The paper aims to create a dataset within the context of the European Court of Human Rights (ECtHR) to address these issues and benchmark the effectiveness of different retrieval models and strategies.\n\n**Relevant Prior Work:**\n1. **Legal Frameworks and Precedent Citation**:\n   - Common law vs. civil law distinction and its impact on case law significance: Joutsen (2019) and Lamond (2005).\n   - Use of precedents based on stare decisis in common law jurisdictions: Duxbury (2008); Lamond (2005).\n   \n2. **Existing PCR Datasets**:\n   - Common law systems: COLIEE (Kim et al., 2018) and IRLeD (Mandal et al., 2017). \n     - These datasets suppressed the citations and primarily used the entire case document for retrieval, which raised concerns regarding system effectiveness.\n\n   - Civil law system adaptations: CAIL-SCM (Xiao et al., 2019) and LeCaRD (Ma et al., 2021).\n     - These were curated with domain expertise, leading to smaller sizes and the unique challenge of no inherent citation structure.\n\n3. **Dataset and System Challenges**:\n   - Citation suppression methods and their limitations: Rossi et al. (2021).\n   - The distinction between factual and argumentative parts of the judgment and the implications for relevance: Valvoda et al. (2021).\n   - Divergence in legal opinions on what factors constitute ratio decidendi: Halsbury (1907), Goodhart (1930).\n\n4. **Motivation for ECtHR Dataset**: \n   - ECtHR documents explicitly separate facts from arguments, essential for realistic queries.\n   - Reliance on precedential law in ECtHR analogous to common law systems but without a formal doctrine of stare decisis: Jacob (2014).\n   - Evidence from court guidelines and scholarly works suggests a precedential nature in ECtHR decisions (Zupancic, 2016; Lupu and Voeten, 2010).\n\nBy leveraging the structured nature of ECtHR case documents and addressing the aforementioned limitations, the paper aims to enhance the understanding and retrieval of precedents, essential for legal decision-making.",
        "methodology": "### Methodology Overview for ECtHR-PCR Dataset Creation\n\nThe ECtHR-PCR dataset is developed through a systematic four-step process designed to ensure comprehensive and accurate data for precedent understanding and prior case retrieval in the European Court of Human Rights. Here, we discuss the specific methodologies used in each step.\n\n#### 1. Document Collection & Filtering\n- **Data Source:** The dataset originates from HUDOC, the public database of the ECtHR, captured as an HTML data dump.\n- **Filtering Criteria:** The dataset is filtered to include only English judgment documents, categorized under \"Document Type: HEJUD\" based on metadata, each tagged with an application number to identify distinct applications.\n- **Handling Multiple Applications & Merging Cases:** Given the court\u2019s practice of merging related applications, a single judgment document can map to multiple application numbers. Also, for cases referred to the Grand Chamber and producing a fresh judgment while holding the same application numbers, distinct versions are created by appending a version number to the document to resolve potential citation conflicts.\n\n#### 2. Parsing Case Documents\n- **Document Structure Compliance:** Using Rule 74 of the Rules of the Court, the judgment documents are parsed into distinct sections such as Procedure, The Facts, The Law, and Conclusion.\n  - **Procedure:** Steps from application submission to judgment delivery.\n  - **The Facts:** Background facts, domestic court procedures, and relevant legal provisions.\n  - **The Law:** Legal reasoning, principles, and doctrines.\n  - **Conclusion:** Court\u2019s verdict on alleged violations.\n- **Challenges and Solutions:** \n  - **Inconsistent HTML Structure:** Hand-crafted rules and regular expressions were developed to identify headers and classify sections based on inconsistent naming patterns.\n  - **Manual Labeling:** Approximately 0.1% of the documents required manual labeling when automatic parsing failed.\n\n#### 3. Citation Extraction\n- **Pre-existing SCL Metadata Issues:** The Strasbourg Case Law (SCL) information in HUDOC metadata was manually compiled but often incomplete (missing in 60% of cases).\n- **Automatic Citation Extraction Strategies:** \n  - **Regular Expressions:** To detect application numbers and citation strings within the text.\n  - **Iterative Refinement:** For formats like 'v.' followed by court names and dates (e.g., Schenk v. Switzerland, 12 July 1988).\n  - **Pre-1999 Citation Format:** Adapted strategies to handle older formats (e.g., Van Leuven and De Meyere judgment).\n- **Improvement Through Iteration:** Regular expressions were iteratively refined until most citation strings were identified.\n\n#### 4. Mapping Citations to Documents\n- **Heuristics for Matching Citations:** Despite the inconsistency in structure, heuristics are developed to match citation strings to case documents by identifying elements like the plaintiff, defendant, application number, and date.\n- **Version Matching:** When multiple document versions exist, citations are matched to the most accurate version based on dates to ensure relevancy and correctness.\n\n### Key Innovations and Contributions\n- **Distinct Version Handling:** Innovatively addressing merged cases and Grand Chamber referrals by appending version numbers.\n- **Hand-crafted Parsing Rules:** Overcoming HTML inconsistencies through meticulous rule crafting.\n- **Iterative Regular Expressions:** Robust strategies for citation extraction through iterative refinement of regular expressions.\n- **Heuristic-based Citation Mapping:** Employing multiple heuristics to reliably map citations to the correct documents, enhancing the dataset\u2019s utility for legal research and analysis.\n\nThis meticulous process ensures that the ECtHR-PCR dataset is comprehensive, precise, and valuable for understanding precedents and retrieving relevant cases within the European Court of Human Rights context.",
        "main_experiment_and_results": "### Main Experiment Setup:\n#### 1. Datasets:\nThe experiment is conducted using the ECtHR-PCR dataset, which focuses on precedent understanding and prior case retrieval in the European Court of Human Rights.\n\n#### 2. Baselines:\nThe paper does not explicitly list the specific baselines used in the main experiment setup provided. Generally, these would include standard retrieval models or existing methods relevant to legal case retrieval.\n\n### Evaluation Metrics:\n1. **Recall@k:** Measures the proportion of relevant documents ranked within the top k candidates (with k values of 50, 100, 500, and 1000). Higher Recall@k indicates better performance. The average Recall@k across all instances is reported.\n2. **Mean Average Precision (MAP):** Calculates the mean of Average Precision scores for each instance. Average Precision is the average of Precision@k scores for every rank position of each relevant document, where Precision@k represents the proportion of relevant documents in the top k candidates. Higher MAP scores indicate better performance.\n3. **Mean Rank:** The average rank of relevant documents for each instance. Lower Mean Rank values are preferred.\n4. **Median Rank:** The median rank of relevant documents for each instance. Lower Median Rank values are preferred.\n\n### Main Experimental Results:\nUnfortunately, the provided text does not offer specific numerical results or comparative analysis. To complete this section, specific scores and observations about the performance relative to the baselines would be necessary, highlighting the effectiveness of the proposed method or dataset.\n\n---\n\nNote: The distinction between this main experiment and any ablation study is maintained by focusing only on the overall setup and metrics used. Further information is needed to provide detailed results and comparisons."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To assess the impact of different negative sampling strategies on the performance of dense retrieval models for prior case retrieval (PCR) in the European Court of Human Rights (ECtHR) domain.",
            "experiment_process": "The experiment utilizes a DR-Biencoder architecture and varies the negative sampling strategies. The negative sampling approaches tested include Random negatives, BM25 + Random, and ANCE (Approximate Nearest Neighbor Negative Contrastive Learning). Each model is trained while obeying the timestamp constraint to ensure the timestamp of the query is always greater than the timestamp of the negative documents. The evaluation metrics include various scores reported in Table 4.",
            "result_discussion": "The DR-Rand model outperformed both the DR-BM25+Rand and the DPR-ANCE models in all evaluation metrics. It suggests that training on difficulty-based hard negatives resulted in lower performance compared to training on randomly selected negatives. This indicates that hard-negative selection strategies might be selecting relevant documents that simply have not been cited, which would be less problematic in traditional IR tasks.",
            "ablation_id": "2404.00596v1.No1"
        },
        {
            "research_objective": "To evaluate the performance degradation of dense retrieval models over time and suggest ways to address this temporal drift.",
            "experiment_process": "The study examines the Recall@1000 scores for BM25 and DR-Rand models for both DR-Uni and DR-Bi architectures over the test set period (2018-2022), with a breakdown for each year. The experiment observes the influence of temporal distributional shifts caused by the addition of new case documents to the candidate pool over time.",
            "result_discussion": "Dense models show a more significant decline in performance in recent years compared to BM25. This decline is due to the temporal distributional shift, as the distribution of training data diverges from the test data over time. To mitigate this, future efforts should focus on the continual updating of dense encoders and enhancing their robustness to temporal shifts, ensuring better adaptation to evolving legal landscapes.",
            "ablation_id": "2404.00596v1.No2"
        },
        {
            "research_objective": "To quantitatively assess the notion of relevance as per Halsbury\u2019s and Goodhart\u2019s views using prior case retrieval in ECtHR jurisdiction.",
            "experiment_process": "The experiment uses only the reasoning and facts sections for the documents in the candidate pool, following the principles outlined by Halsbury\u2019s and Goodhart\u2019s views. It compares the results of using these sections alone with the results of using the entire document, across three model variants, as reported in Table 5.",
            "result_discussion": "Using the law section alone proved more effective than using the facts section alone or the entire document. This supports Halsbury\u2019s view over Goodhart\u2019s view in the ECtHR domain. The better performance is attributed to the law section\u2019s focused arguments and relevant factual information. Including the facts section tends to introduce unnecessary information, which may shift the model\u2019s focus and reduce performance.",
            "ablation_id": "2404.00596v1.No3"
        }
    ]
}