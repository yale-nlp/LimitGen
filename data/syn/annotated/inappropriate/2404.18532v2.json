{
    "title": "MileBench: Benchmarking MLLMs in Long Context",
    "abstract": "Despite the advancements and impressive performance of Multimodal Large Language Models (MLLMs) on benchmarks, their effectiveness in real-world, long-context, and multi-image tasks is unclear due to the benchmarks\u2019 limited scope.\nExisting benchmarks often focus on single-image and short-text samples, and when assessing multi-image tasks, they either limit the image count or focus on specific task (e.g time-series captioning), potentially obscuring the performance challenges of MLLMs.\nTo address these limitations, we introduce MileBench, a pioneering benchmark designed to test the MultImodal Long-contExt capabilities of MLLMs.\nThis benchmark comprises not only multimodal long contexts, but also multiple tasks requiring both comprehension and generation.\nWe establish two distinct evaluation sets, diagnostic and realistic, to systematically assess MLLMs\u2019 long-context adaptation capacity and their ability to complete tasks in long-context scenarios.\nOur experimental results, obtained from testing 22 models, revealed that while the closed-source GPT-4o outperforms others, most open-source MLLMs struggle in long-context situations.\nInterestingly, the performance gap tends to widen with an increase in the number of images.\nWe strongly encourage an intensification of research efforts towards enhancing MLLMs\u2019 long-context capabilities, especially in scenarios involving multiple images.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The recent swift development of Multimodal Large Language Models (MLLMs) (OpenAI, 2023  ###reference_b39###; Anil et al., 2023  ###reference_b2###; Liu et al., 2023a  ###reference_b31###; Awadalla et al., 2023  ###reference_b3###) has displayed outstanding performance across a diverse range of multimodal tasks (Yang et al., 2023  ###reference_b51###; Wu et al., 2023  ###reference_b50###).\nMeanwhile, a surge of benchmarks for evaluating MLLM performance has emerged (Liu et al., 2023a  ###reference_b31###; Fu et al., 2023  ###reference_b9###; Ge et al., 2023  ###reference_b11###; Li et al., 2023a  ###reference_b22###), offering insights into their general capabilities (Li et al., 2023b  ###reference_b23###; Liu et al., 2023c  ###reference_b34###; Yu et al., 2023  ###reference_b54###) and task-specific capabilities (Liu et al., 2023d  ###reference_b35###; Yue et al., 2023  ###reference_b55###; Wang et al., 2024  ###reference_b47###).\nHowever, a critical aspect is often overlooked.\nReal-world applications frequently demand the processing of long contexts and multi-image tasks that include multi-round dialogues based on multiple images (Li et al., 2022  ###reference_b28###), action prediction tasks (Wu et al., 2021  ###reference_b49###), navigation tasks in 3D space (Krantz et al., 2020  ###reference_b20###), and understanding tasks with lengthy documents interspersed with images on Wiki pages (Hannan et al., 2020  ###reference_b12###).\nDespite this, existing benchmarks primarily focus on single-image and short-text samples (Liu et al., 2023a  ###reference_b31###; Fu et al., 2023  ###reference_b9###; Liu et al., 2023c  ###reference_b34###; Li et al., 2023b  ###reference_b23###), thereby failing to fully capture the complexity and diversity of real-world scenarios.\nWhile some benchmarks evaluate multi-image tasks, they either have limited number of images provided per sample (e.g., SEED-Bench-2 (Li et al., 2023a  ###reference_b22###), DEMON (Li et al., 2023c  ###reference_b24###)) or only include time-series captioning tasks (e.g., Mementos (Wang et al., 2024  ###reference_b47###)), as shown in Figure 2  ###reference_###.\nIn addition, this omission could potentially neglect the hallucination issue that MLLMs might exhibit in long-context situations (Huang et al., 2023  ###reference_b14###).\nGiven the aforementioned shortcomings with existing benchmarks, we identify a pressing need for a more holistic evaluation that fully encapsulates the long-context and multi-image task demands prevalent in real-world applications.\nAddressing this need, we introduce MileBench, the first benchmark specifically designed to test the MultImodal Long-contExt capabilities of MLLMs111We define \u201cmultimodal long contexts\u201d as long text content integrated with two or more images, or content composed of multiple images..\nTo systematically assess the capabilities of MLLM in multimodal long contexts, our benchmark consists of two distinct evaluation sets, diagnostic evaluation and realistic evaluation.\nThe former explores the long-context recall abilities of MLLMs, using needle-in-a-haystack and image retrieval tasks, while the latter stress-tests the model in a manner akin to real-world conditions using both temporal multi-image tasks and semantic multi-image tasks.\nTo construct our evaluation sets, we gather 6,440 multimodal long-context samples from 21 pre-existing or self-constructed datasets, with an average of 15.2 images and 422.3 words each, as depicted in Figure 2  ###reference_###, and we categorize them into their respective subsets.\nAfter evaluating 22 models, the closed-source GPT-4o222https://openai.com/index/hello-gpt-4o  ###reference_### excelled in both diagnostic and realistic evaluations, achieving impressive scores of 99.4% and 60.3%, although it still falls short of a perfect 100% score.\nOn the contrary, most open-source MLLMs struggled with long-context tasks as depicted in Figure 1  ###reference_###.\nOnly Mantis and Qwen-VL-7B managed average scores of 47.5% and 37.2% in realistic and diagnostic evaluations respectively. These results underscore that there are \u201cmiles to go\u201d towards fully-realized long-context MLLMs, prompting a call for increased research focus on such tasks, especially those involving numerous images."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Multi-image and Long-Context MLLMs",
            "text": "Beyond training on single-image-text pairs, recent developments in MLLMs are also oriented towards handling multiple and interleaved image-text sequences (Awadalla et al., 2023  ###reference_b3###; Li et al., 2023c  ###reference_b24###).\nHowever, these models have relatively limited contexts (i.e., up to 4K) compared to leading proprietary MLLMs such as GPT-4V (OpenAI, 2023  ###reference_b39###) and Gemini (Anil et al., 2023  ###reference_b2###; Reid et al., 2024  ###reference_b41###), which exhibit capabilities for long-context processing, supporting up to 128K and 10M tokens, respectively.\nHowever, there remains a notable gap in open-source MLLMs capable of long-context comprehension.\nCurrently, the only open-source models equipped for long contexts are those designed for video, which are trained to process multiple frames, inherently managing multiple images and long contexts (Liu et al., 2024a  ###reference_b30###; Zhang et al., 2023  ###reference_b56###; Luo et al., 2023  ###reference_b36###; Li et al., 2023d  ###reference_b25###; e  ###reference_b27###). In this paper, we release an evaluation set specifically designed for multi-image and long-context MLLMs."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Evaluation of MLLMs",
            "text": "Most of the MLLM benchmarks only evaluate multimodal tasks with a single image (Liu et al., 2023a  ###reference_b31###; Fu et al., 2023  ###reference_b9###; Liu et al., 2023c  ###reference_b34###; Li et al., 2023b  ###reference_b23###; Yu et al., 2023  ###reference_b54###; Ge et al., 2023  ###reference_b11###; Yue et al., 2023  ###reference_b55###).\nAs a complementarity, SEED-Bench2 (Li et al., 2023a  ###reference_b22###) and DEMON (Li et al., 2023c  ###reference_b24###) test multimodal capabilities with multiple images but limit the evaluation to around three images, which is inadequate for a thorough multi-image comprehension assessment. Mementos (Wang et al., 2024  ###reference_b47###) involves data samples with up to approximately 11 images, mainly focusing on temporal understanding and limited context scenarios. This focus overlooks the wide range of scenarios involving multiple images and long context.\nWe provide an overview of existing MLLM benchmarks in Appendix A  ###reference_###.\nTo the best of our knowledge, MileBench is the first comprehensive benchmark that evaluates MLLMs across both multi-image and long-context dimensions, catering to a broader spectrum of general scenarios."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "MileBench",
            "text": "Temporal Multi-Image tasks include four temporal-related multi-image tasks.\nEach task contains multiple subtasks, each with 200 samples.\nAction Understanding and Prediction task involves interpreting and forecasting actions of objects or characters in sequential scenarios based on a series of images.\nThis task is divided into three subtasks. Action Localization (Gao et al., 2017  ###reference_b10###) assesses the model\u2019s ability to identify time actions within a sequence. Action Prediction (Wu et al., 2021  ###reference_b49###) tests the model\u2019s capacity to predict a character\u2019s actions. Action Sequence (Wu et al., 2021  ###reference_b49###) evaluates the model\u2019s understanding of the chronological order of a character\u2019s actions.\nObject and Scene Understanding task involves identifying and understanding objects within a sequence.\nIt comprises four subtasks: Object Existence (Yi et al., 2020  ###reference_b52###) tests the model\u2019s ability to detect and track an object\u2019s movement. Moving Attribute (Yi et al., 2020  ###reference_b52###) assesses the model\u2019s understanding of moving object attributes. Object Interaction (STAR) evaluates the model\u2019s comprehension of interactions between people and objects in complex scenarios. Object Shuffle (Patraucean et al., 2023  ###reference_b40###) gauges the model\u2019s ability to locate hidden objects amidst disturbances.\nVisual Navigation and Spatial Localization task tests the model\u2019s understanding of spatial and directional concepts through two subtasks.\nOne is Egocentric Navigation (Krantz et al., 2020  ###reference_b20###), involves the model interpreting motion-related instructions and image sequences from a robot\u2019s perspective to predict the next action. Moving Direction (Yi et al., 2020  ###reference_b52###), assesses the model\u2019s ability to determine object movement direction, evaluating its understanding of spatial orientation.\nCounterfactual Reasoning and State Change task evaluates the model\u2019s logical reasoning within image sequences, focusing on causality and state changes.\nIt comprises four subtasks. Counterfactual Inference (Yi et al., 2020  ###reference_b52###) tests the model\u2019s ability to predict outcomes under hypothetical changes. State Change (Patraucean et al., 2023  ###reference_b40###) assesses the model\u2019s understanding of object state changes. Character Order (Patraucean et al., 2023  ###reference_b40###) examines the model\u2019s reasoning of the order of letter appearances over time. Scene Transition (Huang et al., 2020  ###reference_b15###) evaluates the model\u2019s understanding of scene changes and the associated causality.\nSemantic Multi-Image tasks include five semantic-related multi-image tasks.\nEach task contains multiple existing or artificially constructed datasets, each with 200 samples.\nKnowledge Grounded QA task centres on knowledge-based reasoning, where models synthesise multimodal knowledge for single- or multi-hop reasoning tasks.\nThe task employs four datasets: Webpage QA (Chang et al., 2022  ###reference_b6###) for open-domain multi-hop web search, Textbook QA (Kembhavi et al., 2017  ###reference_b18###) for multimodal textbook questions with diagrams and images, Complex Multimodal QA (Talmor et al., 2021  ###reference_b44###) for complex Wikipedia questions with tables and images, and Long Text with Images QA for long text with images questions from Wiki documents.\nText-Rich Images QA task demands the processing and understanding of rich text information embedded directly in images. It calls for models capable of recognizing textual information from images and integrating this textual information with complex reasoning to answer questions.\nIt contains Slide QA (Tanaka et al., 2023  ###reference_b46###) for multi-slides question answering requiring multi-hop and numerical reasoning, OCR QA (Mishra et al., 2019  ###reference_b38###) for book cover image text reading, and Document QA (Mathew et al., 2021  ###reference_b37###) for document images with a focus on understanding document structure.\nVisual Relation Inference task is centered around understanding and inferring visual relationships. It aims to detect subtle variations between two images, such as changes in objects and positional shifts, and subsequently generate accurate descriptions of these changes. This necessitates the model to possess robust capabilities in capturing visual details and generating response.\nThe task leverages Visual Change Captioning (Jhamtani & Berg-Kirkpatrick, 2018  ###reference_b16###; Hosseinzadeh & Wang, 2021  ###reference_b13###) to describe differences between similar images, image change capturing, and Visual Relationship Expressing (Tan et al., 2019  ###reference_b45###) for generating relationship captions between images.\nDialogue task primarily involve the fusion of visual data and natural language dialogue understanding.\nThis task needs the model to understand and process multimodal data, including both textual and visual information, while demonstrating consistency and complementarity in the task.\nThe datasets involve Multimodal Dialogue (Li et al., 2022  ###reference_b28###) for multimodal conversational question answering and Conversational Embodied Dialogue (Shridhar et al., 2020  ###reference_b43###) for mapping from language commands and visuals to action sequences.\nSpace Understanding task requires the model to perceive the spatial environment using multi-image information.\nIt uses the nuscenes (Caesar et al., 2020  ###reference_b5###) dataset, specifically designed for self-driving car technology. It contains sensor data for object detection and tracking, with 1000 annotated scenes for location and properties of objects.\nThe \u201cNeedle in a Haystack\u201d task requires the model to find a preset password from a long context.\nThis is widely used in diagnostic evaluations of long-context language models (Kuratov et al., 2024  ###reference_b21###).\nIn this study, we reintroduce this novel task from a multimodal perspective to evaluate the context perceptual and retrieval abilities of MLLMs.\nSpecifically, we constructed two tasks, Text Needle In A Haystack (N-1) and Image Needle In A Haystack (N-2).\nExamples are shown in the lower left corner of Figure 3  ###reference_### and Figure 11  ###reference_### in Appendix B.2  ###reference_###, respectively.\nCompared to unimodal Needle in a Haystack task, Text Needle In A Haystack\u2019s haystack includes both text and images and the model is required to recall a randomly generated 7-digit password from this multimodal haystack.\nImage Needle In A Haystack changes the modality of the \u201cneedle\u201d, inserting it as text within an image.\nThis cross-modal Needle in a Haystack task requires MLLMs to have not only good perceptual and retrieval abilities but also robust OCR capabilities.\nImage Retrieval task (I-1) (Schall et al., 2022  ###reference_b42###) requires the model to retrieve images from a set of candidates images given an anchor image (an example is shown in the lower right corner of Figure 3  ###reference_###).\nIn addition to perceptual and retrieval abilities, this task also necessitates that the MLLM possesses robust abilities in both object and conceptual recognition.\nWe consider this a traditional computer vision task and see image retrieval as a \u201cNeedle in a Haystack\u201d task with image modality queries.\nWe collected the samples from two sources:\n(1) For most of the tasks, we selected and sampled 200 instances from the pre-existing datasets for each task, giving priority to multi-image samples. For video data, we used the Katna (Keplerlab, 2021  ###reference_b19###) to convert the video into a multi-image format by extracting one frame per second.\nThe choice of these pre-existing open-source datasets was driven by their well-established reputation and the fact that they have been published in top-tier conferences and journals, ensuring their credibility and reliability.\nPlease refer to Section Ethics Statement  ###reference_### and Section 4.3.3  ###reference_.SSS3### for information on the data\u2019s licensing and data contamination issue.\n(2) For the new tasks Long Text with Images QA, Image Retrieval, Text Needle In A Haystack, and Image Needle In A Haystack, we created synthetic data (more details in Appendix B.3  ###reference_###).\nUltimately, we collected 6,440 samples with varying context lengths.\nDetailed statistics of these samples are presented in Table 4  ###reference_3###.\nThe task distribution is demonstrated in Figure 4  ###reference_3###.\nA comprehensive breakdown of the datasets, tasks, and taxonomy can be found in Appendix B.1  ###reference_###.\nFor the open-source dataset comprised of the benchmark, we sample 10% of the data for manual verification.\nOur review team, composed entirely of authors, was assigned to scrutinize the precision of the sampled data, resulting in an Inter-annotator Agreement (IAA)555A degree of consensus or similarity among the annotations made by different annotators on the same data. of 95%, indicating a high level of consistency among reviewers.\nFor the datasets we formulated independently, equivalent manual verification was carried out on the entirety of the dataset, yielding a similar IAA of 98%, thus ascertaining the data quality.\nAdditionally, the error rate was found to be less than 1% for both datasets, affirming that these datasets maintain an exceptionally high quality and are virtually devoid of errors."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Evaluation Taxonomy",
            "text": "MileBench consists of two major components: Realistic Evaluation and Diagnostic Evaluation, as depicted in Figure 3  ###reference_###.\nRealistic Evaluation requires MLLMs to address tasks within multimodal long-context scenarios, emphasizing the models\u2019 proficiency in comprehending and reasoning across extended multimodal contexts.\nConversely, Diagnostic Evaluation demands MLLMs to retrieve information from the provided context, highlighting the model\u2019s capability of long-range information retrieval and the elimination of distractors.\nThe detailed taxonomy of MileBench is illustrated in Table 6  ###reference_###. 333We adapted the taxonomy from MVBench (Li et al., 2024  ###reference_b26###) and DEMON (Li et al., 2023c  ###reference_b24###) to suit our multimodal long-context setting.\n###figure_1### Temporal Multi-Image tasks include four temporal-related multi-image tasks.\nEach task contains multiple subtasks, each with 200 samples.\nAction Understanding and Prediction task involves interpreting and forecasting actions of objects or characters in sequential scenarios based on a series of images.\nThis task is divided into three subtasks. Action Localization (Gao et al., 2017  ###reference_b10###  ###reference_b10###) assesses the model\u2019s ability to identify time actions within a sequence. Action Prediction (Wu et al., 2021  ###reference_b49###  ###reference_b49###) tests the model\u2019s capacity to predict a character\u2019s actions. Action Sequence (Wu et al., 2021  ###reference_b49###  ###reference_b49###) evaluates the model\u2019s understanding of the chronological order of a character\u2019s actions.\nObject and Scene Understanding task involves identifying and understanding objects within a sequence.\nIt comprises four subtasks: Object Existence (Yi et al., 2020  ###reference_b52###  ###reference_b52###) tests the model\u2019s ability to detect and track an object\u2019s movement. Moving Attribute (Yi et al., 2020  ###reference_b52###  ###reference_b52###) assesses the model\u2019s understanding of moving object attributes. Object Interaction (STAR) evaluates the model\u2019s comprehension of interactions between people and objects in complex scenarios. Object Shuffle (Patraucean et al., 2023  ###reference_b40###  ###reference_b40###) gauges the model\u2019s ability to locate hidden objects amidst disturbances.\nVisual Navigation and Spatial Localization task tests the model\u2019s understanding of spatial and directional concepts through two subtasks.\nOne is Egocentric Navigation (Krantz et al., 2020  ###reference_b20###  ###reference_b20###), involves the model interpreting motion-related instructions and image sequences from a robot\u2019s perspective to predict the next action. Moving Direction (Yi et al., 2020  ###reference_b52###  ###reference_b52###), assesses the model\u2019s ability to determine object movement direction, evaluating its understanding of spatial orientation.\nCounterfactual Reasoning and State Change task evaluates the model\u2019s logical reasoning within image sequences, focusing on causality and state changes.\nIt comprises four subtasks. Counterfactual Inference (Yi et al., 2020  ###reference_b52###  ###reference_b52###) tests the model\u2019s ability to predict outcomes under hypothetical changes. State Change (Patraucean et al., 2023  ###reference_b40###  ###reference_b40###) assesses the model\u2019s understanding of object state changes. Character Order (Patraucean et al., 2023  ###reference_b40###  ###reference_b40###) examines the model\u2019s reasoning of the order of letter appearances over time. Scene Transition (Huang et al., 2020  ###reference_b15###  ###reference_b15###) evaluates the model\u2019s understanding of scene changes and the associated causality.\nSemantic Multi-Image tasks include five semantic-related multi-image tasks.\nEach task contains multiple existing or artificially constructed datasets, each with 200 samples.\nKnowledge Grounded QA task centres on knowledge-based reasoning, where models synthesise multimodal knowledge for single- or multi-hop reasoning tasks.\nThe task employs four datasets: Webpage QA (Chang et al., 2022  ###reference_b6###  ###reference_b6###) for open-domain multi-hop web search, Textbook QA (Kembhavi et al., 2017  ###reference_b18###  ###reference_b18###) for multimodal textbook questions with diagrams and images, Complex Multimodal QA (Talmor et al., 2021  ###reference_b44###  ###reference_b44###) for complex Wikipedia questions with tables and images, and Long Text with Images QA for long text with images questions from Wiki documents.\nText-Rich Images QA task demands the processing and understanding of rich text information embedded directly in images. It calls for models capable of recognizing textual information from images and integrating this textual information with complex reasoning to answer questions.\nIt contains Slide QA (Tanaka et al., 2023  ###reference_b46###  ###reference_b46###) for multi-slides question answering requiring multi-hop and numerical reasoning, OCR QA (Mishra et al., 2019  ###reference_b38###  ###reference_b38###) for book cover image text reading, and Document QA (Mathew et al., 2021  ###reference_b37###  ###reference_b37###) for document images with a focus on understanding document structure.\nVisual Relation Inference task is centered around understanding and inferring visual relationships. It aims to detect subtle variations between two images, such as changes in objects and positional shifts, and subsequently generate accurate descriptions of these changes. This necessitates the model to possess robust capabilities in capturing visual details and generating response.\nThe task leverages Visual Change Captioning (Jhamtani & Berg-Kirkpatrick, 2018  ###reference_b16###  ###reference_b16###; Hosseinzadeh & Wang, 2021  ###reference_b13###  ###reference_b13###) to describe differences between similar images, image change capturing, and Visual Relationship Expressing (Tan et al., 2019  ###reference_b45###  ###reference_b45###) for generating relationship captions between images.\nDialogue task primarily involve the fusion of visual data and natural language dialogue understanding.\nThis task needs the model to understand and process multimodal data, including both textual and visual information, while demonstrating consistency and complementarity in the task.\nThe datasets involve Multimodal Dialogue (Li et al., 2022  ###reference_b28###  ###reference_b28###) for multimodal conversational question answering and Conversational Embodied Dialogue (Shridhar et al., 2020  ###reference_b43###  ###reference_b43###) for mapping from language commands and visuals to action sequences.\nSpace Understanding task requires the model to perceive the spatial environment using multi-image information.\nIt uses the nuscenes (Caesar et al., 2020  ###reference_b5###  ###reference_b5###) dataset, specifically designed for self-driving car technology. It contains sensor data for object detection and tracking, with 1000 annotated scenes for location and properties of objects.\nThe \u201cNeedle in a Haystack\u201d task requires the model to find a preset password from a long context.\nThis is widely used in diagnostic evaluations of long-context language models (Kuratov et al., 2024  ###reference_b21###  ###reference_b21###).\nIn this study, we reintroduce this novel task from a multimodal perspective to evaluate the context perceptual and retrieval abilities of MLLMs.\nSpecifically, we constructed two tasks, Text Needle In A Haystack (N-1) and Image Needle In A Haystack (N-2).\nExamples are shown in the lower left corner of Figure 3  ###reference_###  ###reference_### and Figure 11  ###reference_###  ###reference_### in Appendix B.2  ###reference_###  ###reference_###, respectively.\nCompared to unimodal Needle in a Haystack task, Text Needle In A Haystack\u2019s haystack includes both text and images and the model is required to recall a randomly generated 7-digit password from this multimodal haystack.\nImage Needle In A Haystack changes the modality of the \u201cneedle\u201d, inserting it as text within an image.\nThis cross-modal Needle in a Haystack task requires MLLMs to have not only good perceptual and retrieval abilities but also robust OCR capabilities.\nImage Retrieval task (I-1) (Schall et al., 2022  ###reference_b42###  ###reference_b42###) requires the model to retrieve images from a set of candidates images given an anchor image (an example is shown in the lower right corner of Figure 3  ###reference_###  ###reference_###).\nIn addition to perceptual and retrieval abilities, this task also necessitates that the MLLM possesses robust abilities in both object and conceptual recognition.\nWe consider this a traditional computer vision task and see image retrieval as a \u201cNeedle in a Haystack\u201d task with image modality queries."
        },
        {
            "section_id": "3.1.1",
            "parent_section_id": "3.1",
            "section_name": "3.1.1 Realistic Evaluation",
            "text": "The realistic evaluation is designed to assess an MLLM\u2019s ability to comprehend, integrate, and infer information in a multimodal long context.\nWe categorize the tasks into two main groups: Temporal Multi-Image tasks and Semantic Multi-Image tasks.\nTemporal Multi-Image tasks test the MLLM\u2019s ability to discern temporal relationships among several time-related images, emphasizing the model\u2019s predictive capabilities in real-world scenarios.\nOn the other hand, Semantic Multi-Image tasks challenge MLLMs to process multiple images that are possibly temporal-irrelevant but are semantically interconnected.\nTemporal Multi-Image tasks include four temporal-related multi-image tasks.\nEach task contains multiple subtasks, each with 200 samples.\nAction Understanding and Prediction task involves interpreting and forecasting actions of objects or characters in sequential scenarios based on a series of images.\nThis task is divided into three subtasks. Action Localization (Gao et al., 2017  ###reference_b10###  ###reference_b10###  ###reference_b10###) assesses the model\u2019s ability to identify time actions within a sequence. Action Prediction (Wu et al., 2021  ###reference_b49###  ###reference_b49###  ###reference_b49###) tests the model\u2019s capacity to predict a character\u2019s actions. Action Sequence (Wu et al., 2021  ###reference_b49###  ###reference_b49###  ###reference_b49###) evaluates the model\u2019s understanding of the chronological order of a character\u2019s actions.\nObject and Scene Understanding task involves identifying and understanding objects within a sequence.\nIt comprises four subtasks: Object Existence (Yi et al., 2020  ###reference_b52###  ###reference_b52###  ###reference_b52###) tests the model\u2019s ability to detect and track an object\u2019s movement. Moving Attribute (Yi et al., 2020  ###reference_b52###  ###reference_b52###  ###reference_b52###) assesses the model\u2019s understanding of moving object attributes. Object Interaction (STAR) evaluates the model\u2019s comprehension of interactions between people and objects in complex scenarios. Object Shuffle (Patraucean et al., 2023  ###reference_b40###  ###reference_b40###  ###reference_b40###) gauges the model\u2019s ability to locate hidden objects amidst disturbances.\nVisual Navigation and Spatial Localization task tests the model\u2019s understanding of spatial and directional concepts through two subtasks.\nOne is Egocentric Navigation (Krantz et al., 2020  ###reference_b20###  ###reference_b20###  ###reference_b20###), involves the model interpreting motion-related instructions and image sequences from a robot\u2019s perspective to predict the next action. Moving Direction (Yi et al., 2020  ###reference_b52###  ###reference_b52###  ###reference_b52###), assesses the model\u2019s ability to determine object movement direction, evaluating its understanding of spatial orientation.\nCounterfactual Reasoning and State Change task evaluates the model\u2019s logical reasoning within image sequences, focusing on causality and state changes.\nIt comprises four subtasks. Counterfactual Inference (Yi et al., 2020  ###reference_b52###  ###reference_b52###  ###reference_b52###) tests the model\u2019s ability to predict outcomes under hypothetical changes. State Change (Patraucean et al., 2023  ###reference_b40###  ###reference_b40###  ###reference_b40###) assesses the model\u2019s understanding of object state changes. Character Order (Patraucean et al., 2023  ###reference_b40###  ###reference_b40###  ###reference_b40###) examines the model\u2019s reasoning of the order of letter appearances over time. Scene Transition (Huang et al., 2020  ###reference_b15###  ###reference_b15###  ###reference_b15###) evaluates the model\u2019s understanding of scene changes and the associated causality.\nSemantic Multi-Image tasks include five semantic-related multi-image tasks.\nEach task contains multiple existing or artificially constructed datasets, each with 200 samples.\nKnowledge Grounded QA task centres on knowledge-based reasoning, where models synthesise multimodal knowledge for single- or multi-hop reasoning tasks.\nThe task employs four datasets: Webpage QA (Chang et al., 2022  ###reference_b6###  ###reference_b6###  ###reference_b6###) for open-domain multi-hop web search, Textbook QA (Kembhavi et al., 2017  ###reference_b18###  ###reference_b18###  ###reference_b18###) for multimodal textbook questions with diagrams and images, Complex Multimodal QA (Talmor et al., 2021  ###reference_b44###  ###reference_b44###  ###reference_b44###) for complex Wikipedia questions with tables and images, and Long Text with Images QA for long text with images questions from Wiki documents.\nText-Rich Images QA task demands the processing and understanding of rich text information embedded directly in images. It calls for models capable of recognizing textual information from images and integrating this textual information with complex reasoning to answer questions.\nIt contains Slide QA (Tanaka et al., 2023  ###reference_b46###  ###reference_b46###  ###reference_b46###) for multi-slides question answering requiring multi-hop and numerical reasoning, OCR QA (Mishra et al., 2019  ###reference_b38###  ###reference_b38###  ###reference_b38###) for book cover image text reading, and Document QA (Mathew et al., 2021  ###reference_b37###  ###reference_b37###  ###reference_b37###) for document images with a focus on understanding document structure.\nVisual Relation Inference task is centered around understanding and inferring visual relationships. It aims to detect subtle variations between two images, such as changes in objects and positional shifts, and subsequently generate accurate descriptions of these changes. This necessitates the model to possess robust capabilities in capturing visual details and generating response.\nThe task leverages Visual Change Captioning (Jhamtani & Berg-Kirkpatrick, 2018  ###reference_b16###  ###reference_b16###  ###reference_b16###; Hosseinzadeh & Wang, 2021  ###reference_b13###  ###reference_b13###  ###reference_b13###) to describe differences between similar images, image change capturing, and Visual Relationship Expressing (Tan et al., 2019  ###reference_b45###  ###reference_b45###  ###reference_b45###) for generating relationship captions between images.\nDialogue task primarily involve the fusion of visual data and natural language dialogue understanding.\nThis task needs the model to understand and process multimodal data, including both textual and visual information, while demonstrating consistency and complementarity in the task.\nThe datasets involve Multimodal Dialogue (Li et al., 2022  ###reference_b28###  ###reference_b28###  ###reference_b28###) for multimodal conversational question answering and Conversational Embodied Dialogue (Shridhar et al., 2020  ###reference_b43###  ###reference_b43###  ###reference_b43###) for mapping from language commands and visuals to action sequences.\nSpace Understanding task requires the model to perceive the spatial environment using multi-image information.\nIt uses the nuscenes (Caesar et al., 2020  ###reference_b5###  ###reference_b5###  ###reference_b5###) dataset, specifically designed for self-driving car technology. It contains sensor data for object detection and tracking, with 1000 annotated scenes for location and properties of objects."
        },
        {
            "section_id": "3.1.2",
            "parent_section_id": "3.1",
            "section_name": "3.1.2 Diagnostic Evaluation",
            "text": "The diagnostic evaluation focuses on the MLLMs\u2019 capability to retrieve information without being distracted in a multimodal long context.\nWe transform the tasks of \u201cNeedle in a Haystack\u201d from NLP and \u201cImage Retrieval\u201d from CV into a multimodal format for assessment.\nThis transition preserves the core of the conventional tasks while offering a more challenging and realistic measure of MLLMs\u2019 performance.\nThe \u201cNeedle in a Haystack\u201d task requires the model to find a preset password from a long context.\nThis is widely used in diagnostic evaluations of long-context language models (Kuratov et al., 2024  ###reference_b21###  ###reference_b21###  ###reference_b21###).\nIn this study, we reintroduce this novel task from a multimodal perspective to evaluate the context perceptual and retrieval abilities of MLLMs.\nSpecifically, we constructed two tasks, Text Needle In A Haystack (N-1) and Image Needle In A Haystack (N-2).\nExamples are shown in the lower left corner of Figure 3  ###reference_###  ###reference_###  ###reference_### and Figure 11  ###reference_###  ###reference_###  ###reference_### in Appendix B.2  ###reference_###  ###reference_###  ###reference_###, respectively.\nCompared to unimodal Needle in a Haystack task, Text Needle In A Haystack\u2019s haystack includes both text and images and the model is required to recall a randomly generated 7-digit password from this multimodal haystack.\nImage Needle In A Haystack changes the modality of the \u201cneedle\u201d, inserting it as text within an image.\nThis cross-modal Needle in a Haystack task requires MLLMs to have not only good perceptual and retrieval abilities but also robust OCR capabilities.\nImage Retrieval task (I-1) (Schall et al., 2022  ###reference_b42###  ###reference_b42###  ###reference_b42###) requires the model to retrieve images from a set of candidates images given an anchor image (an example is shown in the lower right corner of Figure 3  ###reference_###  ###reference_###  ###reference_###).\nIn addition to perceptual and retrieval abilities, this task also necessitates that the MLLM possesses robust abilities in both object and conceptual recognition.\nWe consider this a traditional computer vision task and see image retrieval as a \u201cNeedle in a Haystack\u201d task with image modality queries."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Data Collection and Review Process",
            "text": "We have established a robust data collection process and meticulous review procedures to maintain the integrity and quality of our datasets.\nStatistic\nNumber\n\n\n\nTotal samples\n6,440\n\nTotal images\n97,855\n\nAverage images\n15.2\n\nRange of images\n2 ~109\n\nRange of words\n7 ~11821\n\n(Estimated) Average Tokens\n\n\n- Image token=0\n542.2\n\n- Image token=32\n1,028.4\n\n- Image token=256\n4,432.1\n\n- Image token=576\n9,294.4\n\nSamples by Image Num Level\n\n\n- Few (2~5)\n2,959\n\n- Medium (6~31)\n2,389\n\n- Many (32~109)\n1,092\n###figure_2### We collected the samples from two sources:\n(1) For most of the tasks, we selected and sampled 200 instances from the pre-existing datasets for each task, giving priority to multi-image samples. For video data, we used the Katna (Keplerlab, 2021  ###reference_b19###  ###reference_b19###) to convert the video into a multi-image format by extracting one frame per second.\nThe choice of these pre-existing open-source datasets was driven by their well-established reputation and the fact that they have been published in top-tier conferences and journals, ensuring their credibility and reliability.\nPlease refer to Section Ethics Statement  ###reference_###  ###reference_### and Section 4.3.3  ###reference_.SSS3###  ###reference_.SSS3### for information on the data\u2019s licensing and data contamination issue.\n(2) For the new tasks Long Text with Images QA, Image Retrieval, Text Needle In A Haystack, and Image Needle In A Haystack, we created synthetic data (more details in Appendix B.3  ###reference_###  ###reference_###).\nUltimately, we collected 6,440 samples with varying context lengths.\nDetailed statistics of these samples are presented in Table 4  ###reference_3###  ###reference_3###.\nThe task distribution is demonstrated in Figure 4  ###reference_3###  ###reference_3###.\nA comprehensive breakdown of the datasets, tasks, and taxonomy can be found in Appendix B.1  ###reference_###  ###reference_###.\nFor the open-source dataset comprised of the benchmark, we sample 10% of the data for manual verification.\nOur review team, composed entirely of authors, was assigned to scrutinize the precision of the sampled data, resulting in an Inter-annotator Agreement (IAA)555A degree of consensus or similarity among the annotations made by different annotators on the same data. of 95%, indicating a high level of consistency among reviewers.\nFor the datasets we formulated independently, equivalent manual verification was carried out on the entirety of the dataset, yielding a similar IAA of 98%, thus ascertaining the data quality.\nAdditionally, the error rate was found to be less than 1% for both datasets, affirming that these datasets maintain an exceptionally high quality and are virtually devoid of errors."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiment",
            "text": "In this study, we conducted an evaluation of several models across three distinct categories that may handle multimodal long contexts, including five closed-source models666Since the performance of closed-source models may change over time, we report testing GPT-4V (gpt-4-turbo-2024-04-09) on April 12, 2024, GPT-4o (gpt-4o-2024-05-13) on May 13, 2024, Claude 3 (claude-3-opus-20240229) on March 16, 2024, Gemini 1.0 (gemini-1.0-pro-vision-001) on March 20, 2024, and Gemini 1.5 (gemini-1.5-pro-latest) on April 12, 2024. Please note that Gemini 1.0 has a limitation of accepting up to 16 images as input and Claude 3 Opus is 20. (GPT-4V (OpenAI, 2023  ###reference_b39###), GPT-4o, Gemini 1.0 (Anil et al., 2023  ###reference_b2###), Gemini 1.5 (Reid et al., 2024  ###reference_b41###), Claude 3 Opus777https://www.anthropic.com/news/claude-3-family  ###reference_mily###),\ntwelve open-source image models (Qwen-VL-Chat (Bai et al., 2023  ###reference_b4###), MiniGPT-v2 (Chen et al., 2023  ###reference_b8###), Cheetor (Li et al., 2023c  ###reference_b24###), Open flamingo (Awadalla et al., 2023  ###reference_b3###), LLaVA-1.5-7B/13B (Liu et al., 2023a  ###reference_b31###), LLaVA-1.6-7B/13B (Liu et al., 2024b  ###reference_b32###), ALLaVA-Longer (Chen et al., 2024  ###reference_b7###), Yi-VL (Young et al., 2024  ###reference_b53###), VILA (Lin et al., 2023  ###reference_b29###), Mantis (Jiang et al., 2024  ###reference_b17###)),\nand five open-source video models (Video-LLaMA-2 (Zhang et al., 2023  ###reference_b56###), Valley (Luo et al., 2023  ###reference_b36###), VideoChat2 (Li et al., 2023d  ###reference_b25###), LLaMA-VID (Li et al., 2023e  ###reference_b27###), LWM (Liu et al., 2024a  ###reference_b30###)).\nThe details of the models are in Appendix C.1  ###reference_###.\nAll models used greedy decoding to generate answers, with a designated generation length between 1 and 512.\nWe conducted all experiments on NVIDIA A100 GPUs.\nTo save costs, all evaluations were performed only in a zero-shot setting.\nThe format of the prompts is detailed in the Appendix C.2  ###reference_###.\nWhen the input length exceeds the maximum context length of the model, we keep the instruction, and truncate the interleaved image-text question from left so as to keep the question of a sample, as instruction and question are critical information and the importance of the last image is higher in many tasks, e.g. multimodal dialogue.\nMetrics for each dataset, as shown in Table 6  ###reference_###, are consistent with the original work for tasks built on previous datasets.\nFor open-ended generation tasks, the popular n-gram-based metric ROUGE-L is adopted, and accuracy is the metric for multiple-choice and needle-in-a-haystack tasks."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experiment Setup",
            "text": "In this study, we conducted an evaluation of several models across three distinct categories that may handle multimodal long contexts, including five closed-source models666Since the performance of closed-source models may change over time, we report testing GPT-4V (gpt-4-turbo-2024-04-09) on April 12, 2024, GPT-4o (gpt-4o-2024-05-13) on May 13, 2024, Claude 3 (claude-3-opus-20240229) on March 16, 2024, Gemini 1.0 (gemini-1.0-pro-vision-001) on March 20, 2024, and Gemini 1.5 (gemini-1.5-pro-latest) on April 12, 2024. Please note that Gemini 1.0 has a limitation of accepting up to 16 images as input and Claude 3 Opus is 20. (GPT-4V (OpenAI, 2023  ###reference_b39###  ###reference_b39###), GPT-4o, Gemini 1.0 (Anil et al., 2023  ###reference_b2###  ###reference_b2###), Gemini 1.5 (Reid et al., 2024  ###reference_b41###  ###reference_b41###), Claude 3 Opus777https://www.anthropic.com/news/claude-3-family  ###reference_mily###  ###reference_mily###),\ntwelve open-source image models (Qwen-VL-Chat (Bai et al., 2023  ###reference_b4###  ###reference_b4###), MiniGPT-v2 (Chen et al., 2023  ###reference_b8###  ###reference_b8###), Cheetor (Li et al., 2023c  ###reference_b24###  ###reference_b24###), Open flamingo (Awadalla et al., 2023  ###reference_b3###  ###reference_b3###), LLaVA-1.5-7B/13B (Liu et al., 2023a  ###reference_b31###  ###reference_b31###), LLaVA-1.6-7B/13B (Liu et al., 2024b  ###reference_b32###  ###reference_b32###), ALLaVA-Longer (Chen et al., 2024  ###reference_b7###  ###reference_b7###), Yi-VL (Young et al., 2024  ###reference_b53###  ###reference_b53###), VILA (Lin et al., 2023  ###reference_b29###  ###reference_b29###), Mantis (Jiang et al., 2024  ###reference_b17###  ###reference_b17###)),\nand five open-source video models (Video-LLaMA-2 (Zhang et al., 2023  ###reference_b56###  ###reference_b56###), Valley (Luo et al., 2023  ###reference_b36###  ###reference_b36###), VideoChat2 (Li et al., 2023d  ###reference_b25###  ###reference_b25###), LLaMA-VID (Li et al., 2023e  ###reference_b27###  ###reference_b27###), LWM (Liu et al., 2024a  ###reference_b30###  ###reference_b30###)).\nThe details of the models are in Appendix C.1  ###reference_###  ###reference_###.\nAll models used greedy decoding to generate answers, with a designated generation length between 1 and 512.\nWe conducted all experiments on NVIDIA A100 GPUs.\nTo save costs, all evaluations were performed only in a zero-shot setting.\nThe format of the prompts is detailed in the Appendix C.2  ###reference_###  ###reference_###.\nWhen the input length exceeds the maximum context length of the model, we keep the instruction, and truncate the interleaved image-text question from left so as to keep the question of a sample, as instruction and question are critical information and the importance of the last image is higher in many tasks, e.g. multimodal dialogue.\nMetrics for each dataset, as shown in Table 6  ###reference_###  ###reference_###, are consistent with the original work for tasks built on previous datasets.\nFor open-ended generation tasks, the popular n-gram-based metric ROUGE-L is adopted, and accuracy is the metric for multiple-choice and manual image sorting tasks."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Main Result",
            "text": "LWM suffered from a significant decline in performance due to its training on video-text pairs in the final stage. However, \nafter repeating the image multiple times, we observed an improvement."
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1 Main Result on MileBench",
            "text": "We present the results of our experiments in Table 2  ###reference_### and summarize our findings as follows:\n(1) Closed-source MLLMs outperform open-source MLLMs in multimodal long-context tasks to date, particularly in diagnostic evaluation of long-context adaptability where the gap between closed-source MLLMs (average: 79.2%, max: 99.4%) and open-source MLLMs (average: 10.1%, max: 37.2%) is significant.\nIn realistic evaluation, all open-source models, except for VILA and Mantis, lag considerably behind.\n(2) Open-source image models generally perform better than open-source video models. Even the best video model, LLaMA-VID, falls short in realistic evaluation with 31.8%, a score that is lower than eight image models. This may be due to the inability of video models to capture detailed information in images in the same way that image models can.\n(3) The ability to adapt to long contexts and perform long-context tasks are not necessarily linked. For example, while Qwen-VL-Chat scores the highest in diagnostic evaluation among open-source models, it trails behind Mantis in task completion (39.1% <47.5%), highlighting our evaluation\u2019s diversity and comprehensiveness.\n(4) Interestingly, the majority of open-source models scored zero in the Image Needle in a Haystack task. Upon inspection, we found that many of these models partially answered the needle numeric string without completely getting it right. This suggests that open-source models need to improve their ability to retrieve information from images, particularly their OCR capabilities.\nDetailed results from the realistic evaluation can be found in Appendix C.3  ###reference_###."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2 Error Case Study",
            "text": "###figure_3### ###figure_4### We conducted an error analysis to further investigate the flaws of the models.\nAn example of the Space Understanding task is displayed in the upper part of Figure 5  ###reference_###.\nWhen recognizing spatial positions and current actions, GPT-4V declined to respond, while other models did not correctly follow the instructions to answer the question.\nThey merely generated captions for the images, which could be related to them not having been trained on multi-image QA data, emphasizing the importance of multi-image training.\nIn a Visual Relation Inference task example (Figure 5  ###reference_### bottom), Qwen-VL-Chat and Valley struggled with image differentiation and instruction following, resulting in inaccurate inferences. This suggests MLLMs could improve in recognizing subtle image differences, possibly due to their low-resolution visual models. The illusion issue in multi-image inputs for video models also highlights the need for ample multi-image training data."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Analysis",
            "text": "In this section, we delve into a meticulous analysis of the results, focusing on five primary research questions that revolve around the MileBench:\nHow do MLLMs perform given contexts with different lengths?\nDo MLLMs also get \u201cLost in the Middle\u201d in multimodal long contexts?\nDoes data contamination issue exist in MileBench?\nDoes combined image help multi-image comprehension?\nHow diverse and comprehensive is MileBench?"
        },
        {
            "section_id": "4.3.1",
            "parent_section_id": "4.3",
            "section_name": "4.3.1 Performances Decline as the Number of Images Increases for Most MLLMs",
            "text": "###figure_5### To investigate the performance of MLLMs with varying numbers of images, we divide our dataset into three levels: Few, Medium, and Many, based on the number of images per sample.\nThe specific quantities for each level can be found in Table 4  ###reference_3###.\nFigure 6  ###reference_### reports the average performance of the model on the three types of data with different numbers of images.\nIt can be observed that as the number of images increases, the performance of most models significantly declines (as indicated by a steep slope in the curve), especially for the LLaVA-1.5 series models.\nThis is likely because most models have only been trained on single image, resulting in insufficient generalization for multi-image test data.\nHowever, the performance of GPT-4V, GPT-4o, Gemini 1.5, Claude 3 Opus and Qwen-VL-Chat on the Medium level surpasses that of the Few level.\nThis could be attributed to their training on multi-image data, where a larger number of images can provide more information to some extent, thereby aiding the model in task completion.\nDespite their outstanding performance on multi-image tasks, their performance still declines when the number of images reaches the Many level.\nThis leaves room for future development in modeling for multi-image context.\n###figure_6### ###figure_7### ###figure_8### ###figure_9###"
        },
        {
            "section_id": "4.3.2",
            "parent_section_id": "4.3",
            "section_name": "4.3.2 \u201cLost in the Middle\u201d for MLLMs",
            "text": "Liu et al. (2023b  ###reference_b33###) pointed out that in needle-in-a-haystack tasks involving long texts, models may experience the \u201cLost in the Middle\u201d phenomenon, where they struggle to find the needle located in the middle of the context.\nWe investigated whether MLLMs would exhibit the \u201cLost in the Middle\u201d phenomenon in multimodal contexts.\nWe chose the two best-performing models from closed-source and open-source models in the Needle in a Haystack task for analysis.\nAs can be seen from the results in Figure 7  ###reference_###, MLLMs displayed varying behaviors.\nIn multimodal long contexts, GPT-4V did not \u201cget lost in the middle\u201d and managed to complete the two tasks impressively with the scores 99.7% (N-1) and 99.1% (N-2).\nOn the other hand, ignoring the scenarios where the data exceeds its maximum context length (8192 tokens or 32 images) and gets truncated, Qwen-VL-Chat showed a certain degree of \u201clost in the middle\u201d, particularly evident in the image needle task.\nThis indicates that the \u201clost in the middle\u201d phenomenon also exists in multimodal scenarios.\nHowever, a strong ability to manage long context can significantly reduce this risk."
        },
        {
            "section_id": "4.3.3",
            "parent_section_id": "4.3",
            "section_name": "4.3.3 Risk of Data Contamination for MileBench",
            "text": "Considering MileBench\u2019s use of public datasets, there\u2019s a potential risk of data contamination.\nOur investigation involved excluding models trained solely on single-image tasks and opting for cost-effective open-source models, resulting in Qwen-VL-Chat, Cheetor, Open flamingo, and VILA (details in Appendix C.1  ###reference_###).\nWe referred to Wei et al. (2023  ###reference_b48###) and constructed an Adversarial (ADV) Set with shuffled options and paraphrased reference answers and evaluated the difference between original and ADV results.\nResults (Table 3  ###reference_###) show a negligible performance drop (0.1%~1.2%) for all models, indicating minimal likelihood of these models being trained on our dataset."
        },
        {
            "section_id": "4.3.4",
            "parent_section_id": "4.3",
            "section_name": "4.3.4 Experiment on Combined-image Set",
            "text": "To overcome the constraint that models support only a minimal number of image inputs, we introduced Combined-image Set, and to distinguish it from the original MileBench, we will henceforth refer to MileBench as the Multi-image Set.\nIn the Combined-image Set, multiple images are merged into one large image, positioned at the beginning of the input.\nThe original images in the text are then substituted with placeholders.\nTo save the cost, we only selected three closed-source MLLMs to evaluate.\nWe show the result on combined image set in Table 4  ###reference_### and summarize our findings as follows:\n(1) The performance of proprietary models still surpasses that of open-source models in both realistic evaluation (average: 44.8% v.s. 29.6%, max: 48% v.s. 44.9%) and diagnostic evaluation (average: 51.2% v.s. 12.3%, max: 60.5% v.s. 32.4%).\n(2) In comparison to the results on the multi-image set, the performance of proprietary models declined, except for Gemini 1.0, which is limited by the number of images uploaded on the multi-image set.\nThe potential reason is that to maintain performance on the combined image set, models need to possess high-resolution vision models that can effectively distinguish multiple images combined together. For instance, Gemini 1.0 adjusts images of excessively large resolution to a size of . On the other hand, GPT-4V and Claude 3 Opus resize the images to dimensions of  and , respectively. The lower resolution input of GPT-4V and Claude 3 Opus, in comparison to Gemini 1.0, could potentially be a factor for their diminished performance.\n(3) Compared to the results on the multi-image set, the performance of some open-source models with short contexts improved, such as ALLaVA-Longer (from 24.7% to 26.9%) and MiniGPT-v2 (from 17.8% to 29.5%). The possible reason is that these models have only been trained on single images and cannot effectively generalize to multi-image scenarios. Combining images can effectively alleviate this issue."
        },
        {
            "section_id": "4.3.5",
            "parent_section_id": "4.3",
            "section_name": "4.3.5 Inter-task Correlation in MileBench",
            "text": "###figure_10### To investigate the multi-task characteristics of the realistic evaluation in our MileBench, we analyzed the performance of all models across the nine tasks within this evaluation and calculated pairwise correlations between different tasks, as shown in Figure 8  ###reference_###.\n(1) We found that, aside from Task S-3 (Visual Relation Inference), tasks within the same category (either temporal multi-image or semantic multi-image) exhibited high correlation.\nTask S-3, being a challenging one, showed little variation in scores across models.\n(2) We also noted that Task T-3 (Visual Navigation and Spatial Localization) demonstrated lower correlation with other tasks, possibly due to its requirement of unique cognitive skills such as understanding the world from a first-person perspective.\nThese observations suggest that the realistic evaluation of MileBench encompasses a broad range of task types, offering a more comprehensive assessment in the context of multi-image long-context scenarios."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion and Future Directions",
            "text": "In this study, we introduced MileBench, a pioneering benchmark designed to rigorously evaluate the multimodal long-context capabilities of MLLMs.\nWe have established the diagnostic and realistic evaluation sets, designed to systematically assess the MLLMs\u2019 capacity for long-context adaptation and proficiency in task completion within these contexts.\nDespite some impressive performances, our experimental results underscore the urgent need for more focused research to enhance MLLMs\u2019 capabilities in these complex scenarios.\nMoving forward, we suggest two primary research directions:\n(1) Long-context MLLM: Given the ubiquity of mixed media content, there is a pressing need for models that can adeptly process multiple images in long-context scenarios.\n(2) Scaling MileBench to Larger Contexts and Other Modalities: As real-world tasks continue to evolve, benchmarks should also adapt, incorporating larger contexts, complex task structures, and additional modalities to stimulate the development of more versatile MLLMs.\nThese efforts will help equip MLLMs better for our increasingly multimodal world."
        }
    ],
    "url": "http://arxiv.org/html/2404.18532v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.1.1",
            "3.1.2"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.2.1",
            "4.2.2",
            "4.3",
            "4.3.1",
            "4.3.2",
            "4.3.3",
            "4.3.4",
            "4.3.5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "4.1",
            "4.2",
            "4.2.1",
            "4.2.2",
            "4.3",
            "4.3.1",
            "4.3.2",
            "4.3.3",
            "4.3.4",
            "4.3.5"
        ]
    },
    "research_context": {
        "paper_id": "2404.18532v2",
        "paper_title": "MileBench: Benchmarking MLLMs in Long Context",
        "research_background": "### Motivation\nThe motivation for this paper arises from the gap between the capabilities of Multimodal Large Language Models (MLLMs) for handling diverse multimodal tasks and the inadequacy of existing benchmarks to evaluate these models in real-world contexts that require processing long texts and multiple images. Despite the MLLMs' outstanding performance and the emergence of various benchmarks, these evaluations tend to focus on simpler scenarios involving single-image and short-text samples. Consequently, this fails to fully capture the complexity and diversity of real-world applications that demand handling of long-context and multi-image tasks. There is a pressing need for a more comprehensive evaluation that can test these models under conditions that better reflect practical, real-world applications.\n\n### Research Problem\nThe research problem addressed by this paper is the lack of a benchmark specifically designed to test the capabilities of MLLMs in processing long contexts and multi-image tasks. Existing benchmarks are limited in their scope, often failing to account for the challenges posed by these more complex scenarios, such as multi-round dialogues based on multiple images, action prediction tasks, navigation tasks in 3D spaces, and understanding lengthy documents interspersed with images. Additionally, current benchmarks may not adequately address the potential issue of hallucination in MLLMs when they are applied to long-context situations.\n\n### Relevant Prior Work\n1. **Development of MLLMs**: Previous works, including research by OpenAI (2023), Anil et al. (2023), Liu et al. (2023a), and Awadalla et al. (2023), have demonstrated the rapid progress and impressive performance of MLLMs across various multimodal tasks.\n2. **Existing Benchmarks**: Several benchmarks have been developed to evaluate the performance of MLLMs. Liu et al. (2023a), Fu et al. (2023), Ge et al. (2023), and Li et al. (2023a) contributed to the creation of benchmarks that provide insights into the general and task-specific capabilities of these models.\n3. **Real-World Task Requirements**: Prior studies have outlined the necessity for MLLMs to handle complex real-world tasks that involve long contexts and multiple images. Research by Li et al. (2022), Wu et al. (2021), Krantz et al. (2020), and Hannan et al. (2020) highlighted various practical applications that necessitate such advanced capabilities.\n4. **Limited Scope of Current Benchmarks**: Current benchmarks primarily focus on simpler scenarios and provide limited evaluations for multi-image tasks. Examples include SEED-Bench-2 and DEMON by Li et al. (2023a, 2023c), and time-series captioning tasks evaluated by Mementos (Wang et al., 2024).\n5. **Hallucination Issue**: Huang et al. (2023) discussed the potential for MLLMs to exhibit hallucination issues when processing long-context information, a concern that existing benchmarks may not sufficiently address.",
        "methodology": "### Methodology\n\nThe proposed method involves the application of a benchmark named MileBench to evaluate Multimodal Long Language Models (MLLMs) in long context scenarios. The benchmark is divided into two categories: Temporal Multi-Image tasks and Semantic Multi-Image tasks.\n\n#### Temporal Multi-Image Tasks\n\nThis category includes four distinct sub-tasks, each aimed at assessing different aspects of a model's temporal understanding and reasoning abilities:\n\n1. **Action Understanding and Prediction:**\n   - **Action Localization:** Tests the model\u2019s ability to identify when specific actions occur within a chronological sequence.\n   - **Action Prediction:** Evaluates the model\u2019s capacity to forecast future actions based on the current sequence.\n   - **Action Sequence:** Measures the model\u2019s comprehension of the chronological order of actions.\n\n2. **Object and Scene Understanding:**\n   - **Object Existence:** Assesses the model\u2019s ability to detect an object's presence and track its movements.\n   - **Moving Attribute:** Evaluates the model\u2019s understanding of attributes associated with moving objects.\n   - **Object Interaction:** Tests the model\u2019s comprehension of interactions between objects and people.\n   - **Object Shuffle:** Gauges the model\u2019s ability to locate objects that have been hidden or disturbed within the scene.\n\n3. **Visual Navigation and Spatial Localization:**\n   - **Egocentric Navigation:** Requires the model to interpret motion-related instructions and image sequences from a robot\u2019s viewpoint to predict subsequent actions.\n   - **Moving Direction:** Assesses the model\u2019s ability to determine the direction of an object's movement.\n\n4. **Counterfactual Reasoning and State Change:**\n   - **Counterfactual Inference:** Tests the model\u2019s capacity to predict outcomes under hypothetical changes.\n   - **State Change:** Assesses the model\u2019s understanding of object state changes over time.\n   - **Character Order:** Examines the model's reasoning about the sequence of character appearances.\n   - **Scene Transition:** Evaluates the model\u2019s understanding of scene changes and associated causality.\n\n#### Semantic Multi-Image Tasks\n\nThis category also includes five tasks designed to test the model's ability to integrate and reason over rich semantic information:\n\n1. **Knowledge Grounded QA:**\n   - Uses datasets that require synthesizing multimodal knowledge to perform single or multi-hop reasoning tasks, including Webpage QA, Textbook QA, Complex Multimodal QA, and Long Text with Images QA.\n\n2. **Text-Rich Images QA:**\n   - Involves questions based on images rich in embedded textual information, requiring models to recognize and integrate this text for complex reasoning. Datasets include Slide QA, OCR QA, and Document QA.\n\n3. **Visual Relation Inference:**\n   - Focuses on detecting and describing subtle variations between images and their relationships. Employed datasets include Visual Change Captioning and Visual Relationship Expressing.\n\n4. **Dialogue:**\n   - Tests the model's ability to fuse visual data and natural language for understanding and generating consistent responses in a dialogue format. Datasets include Multimodal Dialogue and Conversational Embodied Dialogue.\n\n5. **Space Understanding:**\n   - Requires perceiving spatial environments using multi-image information, specifically catering to self-driving car technology with the nuscenes dataset.\n\n#### Novel Cross-Modal Tasks\n\nThe methodology introduces innovative cross-modal tasks to assess the context perception and retrieval abilities of MLLMs:\n\n1. **Needle in a Haystack:**\n   - **Text Needle In A Haystack (N-1):** Involves finding a randomly generated 7-digit password from a multimodal context of text and images.\n   - **Image Needle In A Haystack (N-2):** Requires the model to locate text embedded within an image.\n\n2. **Image Retrieval (I-1):**\n   - Involves retrieving relevant images from a set of candidate images based on an anchor image, testing the model\u2019s object and conceptual recognition abilities.\n\n#### Data Collection and Evaluation\n\n- **Data Sources:** The benchmark utilizes both pre-existing datasets, sampling 200 instances for each task, and newly constructed synthetic datasets for novel tasks.\n- **Manual Verification:** A rigorous manual verification process ensures data quality, with an Inter-annotator Agreement (IAA) of 95% for pre-existing datasets and 98% for newly formulated datasets.\n- **Sample Statistics:** The collected data encompasses 6,440 samples with varied context lengths, ensuring diverse testing scenarios.\n\nBy implementing this comprehensive methodology, the paper provides a systematic way to evaluate the performance and robustness of MLLMs in handling long-context, multimodal information.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Models Evaluated:\nThe study evaluated several models capable of handling multimodal long contexts from three distinct categories:\n1. **Closed-Source Models**:\n    - GPT-4V (tested on April 12, 2024)\n    - GPT-4o (tested on May 13, 2024)\n    - Claude 3 Opus (tested on March 16, 2024)\n    - Gemini 1.0 (tested on March 20, 2024)\n    - Gemini 1.5 (tested on April 12, 2024)\n\n2. **Open-Source Image Models**:\n    - Qwen-VL-Chat\n    - MiniGPT-v2\n    - Cheetor\n    - Open Flamingo\n    - LLaVA-1.5-7B/13B\n    - LLaVA-1.6-7B/13B\n    - ALLaVA-Longer\n    - Yi-VL\n    - VILA\n    - Mantis\n\n3. **Open-Source Video Models**:\n    - Video-LLaMA-2\n    - Valley\n    - VideoChat2\n    - LLaMA-VID\n    - LWM\n    - Details about these models are available in Appendix C.1.\n\n#### Experimental Setup:\n- **Generation Setup**: Greedy decoding was used for answer generation, with designated generation lengths between 1 and 512.\n- **Hardware**: All experiments were conducted on NVIDIA A100 GPUs.\n- **Evaluation Setting**: To save costs, evaluations were performed in a zero-shot setting only. The prompt format details are available in Appendix C.2.\n- **Handling Long Inputs**: When the input length exceeded the model's maximum context length:\n    - The instruction was kept,\n    - The interleaved image-text question was truncated from the left to retain the question and the last image, as these were deemed critical for many tasks (e.g., multimodal dialogue).\n\n#### Evaluation Metrics:\n- **Consistency**: Metrics for each dataset, as shown in Table 6, maintain consistency with the original metrics used in previous works.\n- **Open-Ended Generation Tasks**: \n    - **ROUGE-L** (a popular n-gram-based metric) was used.\n- **Multiple-Choice and Needle-in-a-Haystack Tasks**: \n    - **Accuracy** was the evaluation metric.\n\n#### Main Experimental Results:\n- **Performance Review**: \n    - A detailed quantitative comparison of the performance of each model is presented in Table 6.\n    - Specific performance metrics, such as ROUGE-L scores for generative tasks and accuracy rates for multiple-choice and needle-in-a-haystack tasks, were computed and analyzed.\n- **Key Findings**: \n    - The results helped in understanding the viability and effectiveness of various closed-source and open-source models in handling long multimodal contexts in a zero-shot evaluation setting.\n    - The findings also provided insights into the strengths and limitations of each model, particularly in how they manage instructions and important image-text pairings in different tasks.\n\nThis description encapsulates the main experiment setup and results while excluding ablation studies and other module-specific evaluations."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the performance of various Multimodal Large Language Models (MLLMs) in handling multimodal long contexts.",
            "experiment_process": "Models from three categories were evaluated: five closed-source models, twelve open-source image models, and five open-source video models. Evaluations were performed in a zero-shot setting. Models used greedy decoding for answer generation with a length between 1 and 512. Testing was conducted on NVIDIA A100 GPUs. Input format and prompt structure were detailed in Appendix C.2. We maintained the instruction and truncated the interleaved image-text question when input length exceeded maximum context length. Metrics included ROUGE-L for open-ended tasks and accuracy for multiple-choice and needle-in-a-haystack tasks.",
            "result_discussion": "Closed-source MLLMs significantly outperformed open-source MLLMs in multimodal long-context tasks. Open-source image models generally outperformed open-source video models. Performance in tasks of long-context adaptation and completion was not necessarily linked, as seen with Qwen-VL-Chat and Mantis. Many open-source models struggled with the Image Needle in a Haystack task, indicating a need for enhanced OCR capabilities.",
            "ablation_id": "2404.18532v2.No1"
        },
        {
            "research_objective": "To analyze the impact of different image quantities on the performance of MLLMs.",
            "experiment_process": "The dataset was divided into three levels: Few, Medium, and Many, based on the number of images per sample. Models' performance was evaluated at each level, with specific quantities defined in Table 4.",
            "result_discussion": "Performance declined for most models as the number of images increased. The decline was particularly steep for the LLaVA-1.5 series models. Certain models, such as GPT-4V and Qwen-VL-Chat, performed better at the Medium level due to training on multi-image data, but their performance still declined at the Many level, indicating room for improvement in multi-image modeling.",
            "ablation_id": "2404.18532v2.No2"
        },
        {
            "research_objective": "To investigate the 'Lost in the Middle' phenomenon in multimodal long contexts.",
            "experiment_process": "The analysis focused on the two best-performing models from closed-source and open-source categories in the Needle in a Haystack task, comparing their ability to locate target information placed in the middle of long contexts.",
            "result_discussion": "GPT-4V performed well without losing track in the middle, with impressive scores of 99.7% and 99.1% on tasks N-1 and N-2. Qwen-VL-Chat exhibited more difficulty, showing signs of 'lost in the middle,' particularly in the image needle task, suggesting that this phenomenon exists in multimodal scenarios but can be mitigated by models with better long-context management.",
            "ablation_id": "2404.18532v2.No3"
        },
        {
            "research_objective": "To assess the risk of data contamination in MileBench.",
            "experiment_process": "Evaluation excluded certain types of models and included the construction of an Adversarial (ADV) Set with shuffled options and paraphrased references to evaluate performance differences between original and ADV results.",
            "result_discussion": "Results demonstrated minimal performance drop (0.1%~1.2%) for all models, indicating a low likelihood of data contamination in MileBench, as the differences were negligible.",
            "ablation_id": "2404.18532v2.No4"
        },
        {
            "research_objective": "To evaluate model performance using a Combined-image Set to overcome input constraints.",
            "experiment_process": "Multiple images were merged into one large image, used at the input's beginning, with original images replaced by placeholders. Three closed-source MLLMs were selected for evaluation, comparing performance on the Combined-image Set to the original Multi-image Set.",
            "result_discussion": "Proprietary models still performed better than open-source on both realistic and diagnostic evaluations. Some models showed improved performance on the Combined-image Set, possibly due to better handling of single combined images versus multiple separate images. Performance decline was noted for high-resolution vision models in proprietary models, excluding Gemini 1.0, which handled large image resolutions effectively.",
            "ablation_id": "2404.18532v2.No5"
        }
    ]
}