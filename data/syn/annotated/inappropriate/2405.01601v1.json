{
    "title": "Efficient Sample-Specific Encoder Perturbations",
    "abstract": "Encoder-decoder foundation models have displayed state-of-the-art performance on a range of autoregressive sequence tasks. This paper proposes a simple and lightweight modification to such systems to control the behaviour according to a specific attribute of interest.\nSpecifically, we show that a small proxy network can be used to find a sample-by-sample perturbation of the encoder output of a frozen foundation model to trigger the decoder to generate improved decodings.\nThis work explores a specific realization of this framework focused on improving the COMET performance of Flan-T5 on Machine Translation and the WER of Whisper foundation models on Speech Recognition. Results display consistent improvements in performance evaluated through COMET and WER respectively.\nFurthermore, experiments also show that the proxies are robust to the exact nature of the data used to train them and can extend to other domains.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Encoder-decoder models have displayed state-of-the-art performance in a wide range of sequence tasks Sutskever et al. (2014  ###reference_b36###) including Machine Translation (MT) Vaswani et al. (2017  ###reference_b40###); Xue et al. (2021  ###reference_b44###), Abstractive Text Summarization & Question Answering Chung et al. (2022  ###reference_b7###) and Automatic Speech Recognition (ASR) Chiu et al. (2018  ###reference_b6###). However, the standard approach to training these systems often relies on teacher-forcing with the likelihood criteria, e.g. next token prediction of the reference sequence. While this framework has been shown successful and reliable in the tasks above, often the desired criteria are some sequence-level non-differentiable performance measures. In MT the desired criteria was the n-gram-based (Sacre)BLEU Post (2018  ###reference_b26###), overtaken more recently by the neural-based COMET Rei et al. (2020  ###reference_b29###) evaluation metric. In ASR the criteria is the word error rate (WER) measuring the rate of substitutions, insertions, and deletions between the decoded output and the reference.\nPrior work has addressed the exposure bias arising from training in teacher-forcing Williams and Zipser (1989  ###reference_b41###) and the loss mismatch between the likelihood and the desired sequence-level loss Bengio et al. (2015  ###reference_b3###); Lamb et al. (2016  ###reference_b20###); Gu et al. (2019  ###reference_b15###); Sabour et al. (2019  ###reference_b33###); Wu et al. (2018  ###reference_b43###); Ranzato et al. (2016  ###reference_b28###); Bahdanau et al. (2017  ###reference_b1###); Wiseman et al. (2016  ###reference_b42###); Kim and Rush (2016  ###reference_b17###).\nBoth of these approaches modify the training so it more closely links with how the model would be used during deployment.\nHowever, in the regime of large pre-trained foundation models, re-training such systems is computationally expensive and unstable. Other approaches attack this problem at inference time by merging outputs from several systems guided by appropriate sequence-level metrics Sim et al. (2007  ###reference_b34###); Kumar and Byrne (2004  ###reference_b18###); Freitag et al. (2022  ###reference_b12###); Rosti et al. (2007a  ###reference_b30###, b  ###reference_b31###); Manakul et al. (2023a  ###reference_b22###). Whilst these approaches have shown promising gains, they rely on the use of ensembles which have significantly higher computational costs. Furthermore, they are not generalizable to any metric.\nIn this paper, we propose a novel, simple and efficient approach to modify the behaviour of a single frozen pre-trained encoder-decoder foundation model.\nWe show that it is possible to perturb the outputs of the encoder to trigger the decoder to produce better-performing decodings according to some pre-selected generic attribute. Unlike prior approaches, our novel proposal applies to frozen pre-trained systems and only leads to an insignificant increase in runtime.\nThis paper is focused on showing the efficacy of the approach in improving the COMET performance of Flan-T5 Chung et al. (2022  ###reference_b7###) on NMT and the WER performance of Whisper Radford et al. (2022  ###reference_b27###) on ASR. Furthermore, the approach is generalizable and applicable to other attributes such as the sentiment of outputs."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background",
            "text": "Reinforcement Learning (RL) and Minimum Bayes Risk (MBR) decoding have been two paradigms used to align a sequence model and minimize the impact of exposure bias. The former is traditionally used to improve the training while the latter is used to modify the decoding procedure. Both come with their own sets of advantages and disadvantages. While often leading to better performance, these approaches are often expensive to use during training and/or inference.\nIn the works of Bahdanau et al. (2017  ###reference_b1###); Lamb et al. (2016  ###reference_b20###), a second network is used, either as a critic in an actor-critic framework Barto et al. (1983  ###reference_b2###); Sutton (1984  ###reference_b37###) or as a discriminator in a generative adversarial framework Goodfellow et al. (2014  ###reference_b14###). The aim of both of these networks, although mechanically different, is to ensure the training procedure resembles the inference stage, minimising the effect of exposure bias.\nOn the other hand, the work of Freitag et al. (2022  ###reference_b12###) explored a post-training approach in which the Minimum Bayes Risk decoder samples many sequences from the system and chooses the sample with the lowest risk. However, the efficacy of this system is highly dependent on being able to sample a large set of outputs, a feat not possible for large pre-trained systems. Alternatively, Manakul et al. (2023b  ###reference_b23###) applies this approach to an ensemble of similarly performing systems, without regard to the inference cost.\nThere is also a body of work on memory and inference-efficient adaptation of foundation models such as prefix-tuning Li and Liang (2021  ###reference_b21###) and low-rank adaptation Hu et al. (2022  ###reference_b16###). These approaches are effective at adapting a foundation model to a certain task at the cost of degrading other abilities. In addition, these approaches still require back-propagating through the whole foundation model making them potentially expensive to train and mainly target teacher-forcing likelihood training. Finally, the work of Fathullah et al. (2023a  ###reference_b9###) introduced a general framework in which an encoder is extended with a small Non-Autoregressive Proxy (NAP) trained to directly capture an arbitrary metric. While only applicable to encoder-decoder systems, it showed that estimates produced by NAP systems were useful in downstream tasks."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Perturbations of Encoders",
            "text": "The proposal\u2019s core is a flexible and efficient approach for augmenting the behaviour of an encoder on a sample-by-sample basis to trigger a better decoder performance. The starting point for such a goal will be the recently introduced Non-Autoregressive Proxy (NAP). While the original work trained the network on a specific metric and used the estimates at runtime to directly perform various downstream tasks, we will extend the view of this network to a differentiable approximation of a sequence-level metric, and use the gradients of this approximation to improve performance.\nLet  and  represent the parameters of some encoder and decoder network, and let  be some input (token or embedding) sequence\nAt inference time we have  where  represents a sequence of  encoder embeddings that are consumed by the decoder. The decoder, through an autoregressive process, produces an output sequence . We aim to find a sample-specific perturbation  to the sequence of encoder outputs such that  gives us a higher score according to some score  (e.g. COMET):\nwhere  is the reference sequence. To find a good perturbation we first apply a static predefined scoring rule without adjustments based on model outputs to approximate the score  where  represent the scoring parameters. Once this is achieved, the gradient of the scoring rule can be used to find a good perturbation to the encoder outputs of a certain sample , making the approach sample specific:\nwhere . We normalize for the size of the gradient and the encoder L2-norms and include a hyperparameter  to control the perturbation size. Small perturbations will have no impact while large changes can lead to a degradation in performance. Therefore, the choice of  is important and is based on some validation set. Note that our approach is aimed to be a lightweight and cheap method for obtaining performance gains, and is not designed to achieve state-of-the-art performance. Furthermore, to the best of the authors\u2019 knowledge, no prior approaches exist for augmenting the behaviour of frozen encoder-decoder systems according to any criteria  which can be anything from COMET and SacreBLEU to the sentiment of the output.\n###figure_1### ###figure_2###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experimental Evaluation",
            "text": "The first set of experiments aim to evaluate the efficacy of the gradient perturbation approach on NMT using the pre-trained Flan-T5. We resort to the OPUS-100 Zhang et al. (2020  ###reference_b45###) dataset and use two splits, the English to German (ende) and the English to Spanish (enes) splits, each with a training set of 1M sentence pairs. For each, we use the Flan-T5 system to decode the data using greedy search with a maximum of 128 output tokens. The decodings are scored using COMET (Unbabel/wmt22-comet-da) Rei et al. (2020  ###reference_b29###). Following Fathullah et al. (2023a  ###reference_b9###) we train a small NAP on the Flan-T5 encoder to predict these COMET scores, using the Pearson correlation loss, training details provided in Appendix A.1  ###reference_###. Each experiment is repeated 3 times.\n###table_1### Table 1  ###reference_### presents the Pearson correlation between the NAP predictions and the COMET scores on various test sets. We observe that a proxy trained on a specific translation pair can still obtain good correlation scores on other splits and reverse directions. The worst performance is shown when a proxy trained on enes pairs is evaluated on the reverse direction displaying a correlation of 35.2%.\n###figure_3### Next, we take these NAPs and use the gradient with respect to the encoder outputs to perform the augmentation detailed in Equation (2  ###reference_###), see Figure 1  ###reference_###.\nThis shows that the gradients derived from a lightweight NAP trained on COMET scores can be used to obtain some performance gains. While small steps  barely have an impact, large  lead to a degradation in performance showcasing the importance of finding a good . Furthermore, we find that although the NAPs were trained on the COMET scores of greedy decodings, they can still be used to improve beam search. We also observe that NAPs trained in a certain translation direction can still be used for other directions. For example, NAP en-es is still able to improve en-de performance.\nWe also investigate this for a range of Flan-T5 models, see Table 2  ###reference_###. In all cases we use a fixed value of .\n###table_2### From these sets of results, it is evident that smaller models benefit more from the perturbation approach while larger more robust models show smaller gains.\nFurthermore, we apply this approach to a range of different-sized beam search decodings, see Figure 2  ###reference_###. For a small cost of performing a forward and backward pass through the small NAP network, both translation pairs show gains across all beams. Note, that the inference speed was measured using an NVIDIA A100 80GBs leading to a disproportionally cheaper runtime for larger beam sizes, since the GPU cores were not fully exhausted for a single sample. Interestingly while we observe improvements in COMET scores, the SacreBLEU score remained the same, see Table 3  ###reference_###.\n###table_3### Upon further analysis of this phenomenon, we observed two factors contributing to this effect. The first is that the proxy augmentation would substitute certain tokens/words that have a \"closer\" meaning to the reference but without any n-gram overlap. The second factor is discussed in Stahlberg and Byrne (2019  ###reference_b35###) in which translation systems often terminate prematurely. The proxy-derived augmentation of the encoder resolves this issue in a fraction of examples, but the continuation of the translation often does not overlap with the words occurring in the reference.\nFinally, we repeat these experiments for the Whisper family on the AMI meeting corpus, a challenging ASR task in which systems often display very high word error rates. Following Fathullah et al. (2023a  ###reference_b9###) we train NAPs on the number of errors in a transcription produced by Whisper {Tiny, Base, Small} using greedy search, see Appendix A.2  ###reference_### for details. The performance of the augmentation is displayed in Figure 3  ###reference_###. Four observations made in these results are: (1) the proxy gradients are beneficial for Whisper in the ASR task, (2) the validation and test sets show correlating performance, (3) the step sizes are significantly larger since the gradients from the NAP are significantly smaller and (4) the improvement is smaller for the larger Whisper systems. Concerning the last point, this behaviour was expected since larger more robust systems potentially have a smaller room for improvement. Similar experiments performed on LibriSpeech Panayotov et al. (2015  ###reference_b25###), a relatively easier speech recognition benchmark showcased small to no gains supporting the claim that the benefits from our proposed approach is highly dependent on the task given a certain model. The details of the LibriSpeech experiments are included in Appendix B  ###reference_###."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "This paper presents a lightweight modification of frozen (pre-trained) encoder-decoder systems by training a small network to predict a metric and use this as a differentiable extension. The gradients of this small proxy network have been shown useful in improving the performance on a range of sequence tasks such as the Flan-T5 COMET performance on NMT and Whisper WER performance on ASR."
        }
    ],
    "url": "http://arxiv.org/html/2405.01601v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3"
        ],
        "main_experiment_and_results_sections": [
            "4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4"
        ]
    },
    "research_context": {
        "paper_id": "2405.01601v1",
        "paper_title": "Efficient Sample-Specific Encoder Perturbations",
        "research_background": "**Paper's Motivation:**\nThe paper is motivated by the challenges in training and optimizing large pre-trained encoder-decoder models for various sequence tasks like machine translation (MT), abstractive text summarization, question answering, and automatic speech recognition (ASR). The traditional training approach relies on teacher-forcing with likelihood criteria, which may not always align with the desired sequence-level non-differentiable performance measures, such as BLEU, COMET, and word error rate (WER). The motivation stems from the need to address the exposure bias and loss mismatch without resorting to computationally expensive and unstable re-training processes or ensemble methods, which have high computational costs and lack generalizability.\n\n**Research Problem:**\nThe research problem is to find an efficient and effective method to modify the behaviour of single frozen pre-trained encoder-decoder foundation models to improve their performance on specific metrics (like COMET for MT and WER for ASR) without the need for expensive re-training or complex ensemble methods.\n\n**Relevant Prior Work:**\n1. **Encoder-Decoder Model Performance:**\n   - State-of-the-art results in sequence tasks: Sutskever et al. (2014), Vaswani et al. (2017), Xue et al. (2021), Chung et al. (2022), Chiu et al. (2018).\n2. **Desired Criteria for Performance Evaluation:**\n   - MT criteria: n-gram-based BLEU (Post, 2018), neural-based COMET (Rei et al., 2020).\n   - ASR criteria: word error rate (WER).\n3. **Addressing Exposure Bias and Loss Mismatch:**\n   - Teacher-forcing and likelihood criteria issues: Williams and Zipser (1989).\n   - Loss mismatch solutions: Bengio et al. (2015), Lamb et al. (2016), Gu et al. (2019), Sabour et al. (2019), Wu et al. (2018), Ranzato et al. (2016), Bahdanau et al. (2017), Wiseman et al. (2016), Kim and Rush (2016).\n4. **Inference Time Optimization:**\n   - Sequence-level metric-guided outputs merging: Sim et al. (2007), Kumar and Byrne (2004), Freitag et al. (2022), Rosti et al. (2007), Manakul et al. (2023).\n\nThe paper proposes a novel approach that efficiently modifies the outputs of the encoder of a frozen pre-trained model to improve specific performance metrics, demonstrating improvements in COMET for MT and WER for ASR, while also being generalizable to other attributes like sentiment analysis.",
        "methodology": "**Methodology: Efficient Sample-Specific Encoder Perturbations**\n\n**Proposed Method:**\n\nThe central idea of the method is to enhance the performance of decoders by augmenting the behavior of the encoder on an individual sample basis through efficient and flexible perturbations. The approach builds on the recently introduced Non-Autoregressive Proxy (NAP) model. While the original NAP work focused on training the network based on specific metrics to perform downstream tasks directly, this method extends NAP's role to acting as a differentiable approximation of a sequence-level metric. This extension allows the use of gradients to enhance performance.\n\n**Key Components:**\n\n1. **Encoder and Decoder Networks (\\theta_E and \\theta_D):** These represent the parameters of the encoder and decoder network respectively. The input is an (token or embedding) sequence \\(x\\).\n\n2. **Inference Process:** During inference, the encoder produces a sequence of embeddings \\(E(x) = e(x_1), e(x_2), \\dots, e(x_n)\\), which are then consumed by the decoder to generate an output sequence \\(y\\) through an autoregressive process.\n\n3. **Perturbation (\\xi):** A sample-specific perturbation is applied to the encoder outputs \\(E(x)\\) to improve the performance score according to a defined metric \\(s\\) (e.g., COMET).\n\n4. **Non-Autoregressive Proxy (NAP):** \n    - A lightweight NAP is trained on top of the encoder to approximate the score \\(s\\). The parameters \\psi represent the NAP.\n    - The purpose of the NAP is to provide gradients based on the score approximation that can be used to adjust the encoder outputs.\n\n5. **Gradient Utilization:**\n    - The gradients from NAP \\(\\nabla_{E(x)} s_{\\text{NAP}}(E(x))\\) are employed to find a beneficial perturbation \\(\\xi\\) for a given sample \\(x\\).\n    - The perturbation aims to adjust the encoder\u2019s output sequence (via \\(\\hat{E}(x) = E(x) + \\xi)\\) to maximize the performance score \\(s(\\hat{y}, y)\\), where \\(y\\) is the reference sequence.\n\n6. **Normalization and Control Parameter (\\alpha):** \n    - The approach includes normalization for the gradient size and the encoder L2-norms.\n    - A hyperparameter \\(\\alpha\\) is used to control the perturbation magnitude. The size of \\(\\alpha\\) is critical; small \\(\\alpha\\) would not impact much while large \\(\\alpha\\) might degrade performance.\n    - The optimal \\(\\alpha\\) is selected based on performance on a validation set.\n\n**Innovations:**\n\n- **Sample-Specific Optimization:** The method focuses on optimizing the encoder\u2019s behavior specifically for each individual sample, rather than a general adjustment applicable to all samples.\n- **Differentiable Metric Approximation:** By using NAP to create a differentiable approximation of sequence-level metrics \\(s\\), this method facilitates the computation of gradients that guide the encoder\u2019s perturbations.\n- **No Prior Similar Approaches:** To the authors\u2019 best knowledge, no existing methods attempt to augment frozen encoder-decoder systems\u2019 behavior based on any generic score \\(s\\), making this approach novel in its application to diverse criteria, such as COMET, SacreBLEU, or even output sentiment.\n\nThis approach aims to be a lightweight and cost-effective method for performance enhancement without necessarily achieving state-of-the-art performance. The innovative aspect lies in its sample-specific perturbations and the flexibility to adapt to various scoring criteria.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\n#### Experiment Setup:\n- **Task**: Evaluate the efficacy of the gradient perturbation approach on Neural Machine Translation (NMT) using the pre-trained Flan-T5 model.\n- **Datasets**:\n  - OPUS-100 (Zhang et al., 2020): English to German (ende) and English to Spanish (enes) splits, each with 1 million sentence pairs in the training set.\n- **Evaluation Metrics**:\n  - COMET (Unbabel/wmt22-comet-da) scores (Rei et al., 2020) to measure translation quality.\n  - Pearson correlation loss to train a small NAP (Neural Architecture Proxy) predicting COMET scores.\n- **Experimental Repeat**: Each experiment is repeated three times.\n\n#### Setup of NMT Evaluation:\n- Uses the Flan-T5 system to decode data using greedy search with a maximum of 128 output tokens.\n- NAPs trained on Flan-T5 encoder to predict COMET scores.\n- Decodings scored using COMET, and NAP predictions are correlated with these scores.\n\n#### Results Summary:\n1. **Correlation Performance**:\n   - Pearson correlation between NAP predictions and COMET scores on different test sets is presented.\n   - Proxy trained on a specific translation pair (e.g., enes) performs well on other splits and reverse directions (e.g., de-en).\n   - Worst performance: Proxy trained on `enes` evaluated on `es-en` shows a correlation of 35.2%.\n\n2. **Gradient Perturbation**:\n   - NAPs trained on greedy decoding COMET scores can also improve beam search performance.\n   - NAPs trained in one translation direction (e.g., en-es) can improve performance in another direction (e.g., en-de).\n\n3. **Beam Search Decoding**:\n   - Applying the approach across different beam sizes results in performance gains for both translation pairs with minimal computational overhead.\n   - Improvements in COMET scores observed, but SacreBLEU scores remained unchanged.\n\n4. **Divergence in Metrics**:\n   - Analysis revealed proxy augmentation may substitute words with closer meaning but lacking n-gram overlap, affecting SacreBLEU scores.\n   - Proxy-derived augmentations can resolve premature terminations in translations, but this does not overlap with reference words.\n\n5. **Application to Whisper System**:\n   - Similar experiments on Whisper family models using the AMI meeting corpus for ASR tasks showed beneficial proxy gradients.\n   - LibriSpeech benchmark experiments indicated smaller or no gains, aligning with the task difficulty dependency.\n\nIn conclusion, the main experiment demonstrates the effective use of gradient perturbations through NAPs trained on COMET scores to improve translation and ASR tasks, with notable model size and task difficulty dependencies."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Evaluate the efficacy of the gradient perturbation approach on Neural Machine Translation (NMT) using the pre-trained Flan-T5 model.",
            "experiment_process": "The experimental setup consists of two splits from the OPUS-100 dataset: English to German (ende) and English to Spanish (enes), each with a training set of 1M sentence pairs. The Flan-T5 system decodes the data using greedy search with a maximum of 128 output tokens. The decodings are scored using COMET. A small NAP is trained on the Flan-T5 encoder to predict these COMET scores, using Pearson correlation loss. Each experiment is repeated 3 times. Gradients with respect to the encoder outputs are used to perform augmentations as detailed. The experiments are conducted across various Flan-T5 models and beam sizes. The results are compared using COMET and SacreBLEU scores.",
            "result_discussion": "The trained NAPs demonstrate that a proxy trained on a specific translation pair can still obtain good correlation scores on other splits and reverse directions. Small NAP-derived perturbations yield performance gains while larger steps cause degradation. Improvements are also observed using beam search, with smaller models benefiting more than larger models. Interestingly, while COMET scores improve, SacreBLEU remains unchanged due to the nature of token substitution and premature termination in translation.",
            "ablation_id": "2405.01601v1.No1"
        },
        {
            "research_objective": "Evaluate the gradient perturbation approach on Automatic Speech Recognition (ASR) using the Whisper model on the AMI meeting corpus.",
            "experiment_process": "NAPs are trained on the number of errors in a transcription produced by Whisper {Tiny, Base, Small} using greedy search. Experimental processes are detailed in Appendix A.2. Performance is evaluated on both validation and test sets, focusing on improvements in Word Error Rate (WER). Similar experiments are performed on the LibriSpeech dataset for further evaluation.",
            "result_discussion": "The proxy gradients are beneficial for Whisper in the ASR task, correlating well between validation and test sets. Larger step sizes are required as the gradients from the NAP are smaller. Larger Whisper systems show smaller improvements, potentially due to a smaller room for improvement. The LibriSpeech benchmark shows small to no gains, emphasizing that the benefits of the proposed approach are task-dependent.",
            "ablation_id": "2405.01601v1.No2"
        }
    ]
}