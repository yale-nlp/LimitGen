{
    "title": "Akal Badi ya Bias: An Exploratory Study of Gender Bias in Hindi Language Technology",
    "abstract": "Existing research in measuring and mitigating gender bias predominantly centers on English, overlooking the intricate challenges posed by non-English languages and the Global South. This paper presents the first comprehensive study delving into the nuanced landscape of gender bias in Hindi, the third most spoken language globally. Our study employs diverse mining techniques, computational models, field studies and sheds light on the limitations of current methodologies. Given the challenges faced with mining gender biased statements in Hindi using existing methods, we conducted field studies to bootstrap the collection of such sentences. Through field studies involving rural and low-income community women, we uncover diverse perceptions of gender bias, underscoring the necessity for context-specific approaches. This paper advocates for a community-centric research design, amplifying voices often marginalized in previous studies. Our findings not only contribute to the understanding of gender bias in Hindi but also establish a foundation for further exploration of Indic languages. By exploring the intricacies of this understudied context, we call for thoughtful engagement with gender bias, promoting inclusivity and equity in linguistic and cultural contexts beyond the Global North.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1. Introduction",
            "text": "Large Language Models (LLMs) continue to exhibit increasingly human-like precision across various tasks, leading to their integration into a wide range of real-world applications (Song et al., 2023  ###reference_b77###; Laskar et al., 2023  ###reference_b50###).\nAs these technologies become more readily available and utilized in a multitude of languages, it becomes critical to understand, identify, and address certain critical biases that may appear. Previous research indicates that Natural Language Generation (NLG) models have the potential to generate or intensify biases and this leads to negative impacts on specific user groups and marginalized communities (Lucy et al., 2023  ###reference_b56###; Trajanovski et al., 2021  ###reference_b84###; Mieczkowski et al., 2021  ###reference_b58###; Buschek et al., 2021  ###reference_b15###; Kirk et al., 2021  ###reference_b47###). Gender bias, in particular, is a critical topic of concern.\nGender biases that exist in language technologies can perpetuate under-representation, stereotyping, or misrepresentation of women and gender minorities (Stanczak and Augenstein, 2021  ###reference_b79###).\nAddressing gender bias in technology is crucial to bridge the digital gender gap and promote a more inclusive and equitable digital society (for Economic Co-operation and, OECD  ###reference_b32###).\nDue to increasing adoption of language technologies in various languages, it is imperative to understand the biases these models can propagate not only in English but also in different languages and cultures.\nUnfortunately, however, much of the research in measuring and mitigating gender bias is in the context of the English and the Global North. Little is known about how to measure and mitigate gender bias in the context of Global South. The largely understudied dimension of gender bias in the context of Global South specifically for Hindi serves as the primary focus of this work.\nFilling this critical gap, we present the first comprehensive study of gender bias in Hindi. Our study highlights particularly in the context of India, it is difficult to utilize the parameters, benchmarks, and guidelines developed for identifying gender bias in English for Indic languages.111We follow the previous works (Gala et al., 2023  ###reference_b34###; Doddapaneni et al., 2023  ###reference_b27###) that state Indic languages as a superset, constituting Indo-Aryan, Dravidian, and a few low-resource languages belonging to the Austroasiatic, Sino-Tibetan, and Tai-Kadai families. Figure 1  ###reference_### shows the pipeline of our experiments.\nWe conducted several experiments for mining gender-biased data in Hindi from different sources. Our experiments of mining include lexicon and heuristic-based approaches of mining, computational models for automatic classification of gender bias, and GPT-based generation of biased sentences. We explored data sources like social media comments, news media, and translation of existing gender bias datasets. Our experiments highlight several key challenges in mining gender-biased data in Hindi. We found that a large amount of data available online is Anglo-centric and hence does not serve as a good source for creating gender bias identification dataset in the Indian context. Mining social media data is extremely difficult due to growing restrictions. Heuristic-based approaches return a higher percentage of false positives. Computational models show poor performance due to limited cross-lingual and cross-domain transfer capabilities. Translations from industrial translation systems produce extremely formal and non-contextual translations. Finally, GPT generations show a limited diversity of themes.\nGiven the challenges faced with creating a gender bias dataset in Hindi using popular mining techniques proposed in past work for English, we conducted community centered field studies to bootstrap the collection of such sentences. Even though the prevalence of technology is growing in rural India, their opinion is often ignored in development of these technologies.\nFor our field studies we employ rural, low income women, to include alternative voices, promoting empowerment within marginalized communities that are disproportionately affected by AI (Queerinai et al., 2023  ###reference_b71###; Birhane et al., 2022  ###reference_b11###; Kormilitzin et al., 2023  ###reference_b49###; Suresh et al., 2022  ###reference_b83###).\nWe first aimed to understand what is the shared understanding of gender bias within a Hindi speaking community. The first field study was designed to elicit a culturally relevant definition of gender bias, and crowd-source gender biased statements via activities and plays. In the second field study we conducted a crowd-sourced annotation study to identify varying degrees of gender bias generated by GPT.\nOur first field study revealed variability in perceptions of gender bias. A simple gamified and interactive approach helped in gaining tacit knowledge about gender stereotypes. Our gender bias annotation workshop highlighted the importance of designing annotation tasks while keeping a variety of audiences in mind. The Best-Worst Scaling comparative annotation framework that showed promising results with an urban audience for Hada et al. (2023b  ###reference_b36###) was found to be complex by the rural crowd-workers employed in our study.\n###figure_1### Our findings show that the study of gender bias involves subtle nuances making it a complex topic. Bringing this to an understudied and highly gendered context such as India is even more challenging.\nWe navigate through the various complexities of identifying gender biased statements in Hindi. The key contributions of our work are as follows:\nWe conduct in-depth experiments for mining gender biased sentences in Hindi.\nWe conduct field studies adopting a community centered approach for gender bias identification, and employing rural, low-income community women to foster minority opinion.\nWe highlight some of the critical challenges faced in mining sentences and field studies that other researchers and technologists should be aware of when engaging in further study of gender bias for Indic languages.\nWe make recommendations on identifying and mitigating gender bias that focuses on the inclusion of communities right from the beginning.\nWe hope that our experiments and case studies can provide a strong foundation upon which to further explore the complex and critical nature of gender bias in Indic languages.222The title \u201dAkal Badi ya Bias\u201d is a word play on the Hindi proverb \u201dAkal Badi ya Bhains\u201d. The proverb translates to \u201dIs wisdom greater or is the buffalo?\u201d in English. It is used to imply that someone is behaving foolishly or lacking common sense. 333Code and data available at: https://aka.ms/AkalBadiyaBias  ###reference_aka.ms/AkalBadiyaBias###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2. Related Work",
            "text": "Bias Identification is a crucial preliminary step in recognizing their presence in existing models. This can be done by designing special metrics or scoring frameworks to assign a \u201cbias\u201d score for sentences, documents, or machine-generated synthetic data (Ntoutsi et al., 2020  ###reference_b63###). Kaneko et al. (2022b  ###reference_b44###) propose a Multilingual Bias Evaluation score, to evaluate bias using English attribute word lists and parallel corpora without requiring manually annotated data. Hada et al. (2023b  ###reference_b36###) generate a dataset of GPT-generated sentences with normative ratings for gender bias and show that bias occurs on a spectrum. Benchmarks such as CrowS-Pairs (Nangia et al., 2020  ###reference_b62###) and StereoSet (Nadeem et al., 2021  ###reference_b61###) aid in measuring various forms of social and stereotypical biases in language models. Zhao et al. (2018  ###reference_b93###); Rudinger et al. (2018  ###reference_b75###) release coreference resolution style WinoBias and WinoGender benchmarks and methods to help identify gender bias in existing co-reference resolution systems. (Stanovsky et al., 2019  ###reference_b80###) combine the two aforementioned benchmarks and devise an automatic gender bias evaluation method for eight languages with grammatical gender, based on morphological analysis. Similarly, DisCo (Webster et al., 2020  ###reference_b90###) is a metric to identify gendered correlations in publicly available pre-trained models. Dev et al. (2022  ###reference_b21###) present a practical framework of harms and a series of questions that practitioners can answer to guide the development of bias measures. Ramesh et al. (2021  ###reference_b72###) evaluate gender bias in Hindi-English Machine Translation.\nThey evaluate Google Translate and the Hi-En OpenNMT model for gender bias using existing metrics WEAT and TGBI.\nOnce the existence of a certain bias is identified in a language model (LM), it is important to address and mitigate the bias before safely deploying the model in the real world.\nKaneko et al. (2022a  ###reference_b43###) survey different debiasing methods and conclude that extrinsic evaluations, i.e., evaluations that are dependent on LMs performance on a specific task, and intrinsic evaluation measures do not have a strong correlation.\nLauscher et al. (2021  ###reference_b51###) introduce \u201cadapter\u201d modules into the original LMs and train the LM on a counterfactually augmented corpus while keeping the rest of the parameters frozen. The authors evaluate this method using both intrinsic and extrinsic measures and show that it is effective in mitigating gender bias in LMs.\nBarikeri et al. (2021  ###reference_b7###) introduce a conversational dataset \u2013 REDDITBIAS \u2013 that can be used to debias LMs for dialog tasks. They show that this dataset allows for bias identification and mitigation across four dimensions: gender, race, religion, and queerness. Pujari et al. (2020  ###reference_b70###) propose a 2 step method for debiasing gender biased Hindi words. First step is to learn the bias space from set of definitive-gendered word pairs and second is to measure the biasness of a biased word.\nThey show that their method is useful in decoupling the debiasing process from the word embedding process. Kirtane and Anand (2022  ###reference_b48###) invesitgate debiasing methods for Hindi and Marathi. They propose debiasing by using partial projection of vectors. Using partial projections overcomes the issue with linear projection where some word vectors which are gendered by definition were changed. In partial projection instead of zero magnitude along the gender direction, they project a magnitude of constant  along with it. They show how their debiasing method works with different techniques such as RIPA and PCA.\nCapturing, measuring, and evaluating all types of biases in language models or crowdsourced data has its challenges. Orgad and Belinkov (2022  ###reference_b64###) find that only a few extrinsic metrics are measured in most studies and that datasets and metrics are often coupled. They discuss how their coupling hinders the ability to obtain reliable conclusions, and how one may decouple them. Draws et al. (2021  ###reference_b28###) argued that cognitive biases in crowdsourced data can also go unnoticed unless specifically assessed or controlled for. Sharifi Noorian et al. (2023  ###reference_b76###) show how crowdsourced elicitation can be inherently biased by the open and closed-world perceptions of annotators. Zhou et al. (2021  ###reference_b94###) show that debiasing a model trained on biased toxic language data is not as effective as simply relabeling the data to remove existing biases. Completely mitigating gender bias from models is a hard task as Dev et al. (2022  ###reference_b21###); Caliskan et al. (2017  ###reference_b16###); Spinde et al. (2021  ###reference_b78###); F\u00e4rber et al. (2020  ###reference_b29###); Bolukbasi et al. (2016  ###reference_b13###) show that the societal and cultural prejudices are deeply embedded within the training data due to the presence of bias, and these biases, whether explicit or implicit, can significantly impact the functionality of the NLP systems (Raza et al., 2024  ###reference_b74###).\nWe refer the readers to Stanczak and Augenstein (2021  ###reference_b79###) and Devinney et al. (2022  ###reference_b22###) for detailed surveys of Gender Bias in Natural Language Processing.\nMuch of the past work in measuring and mitigating gender bias focuses on English and the context of Global North. There is limited understanding of how to measure and mitigate gender bias in the context of the Global South. The emphasis of past work on understanding and mitigating gender bias in western context has resulted in a gap, as western perspectives may not fully capture the nuances of gender bias in the context and languages of Global South.\nIn this paper, we study identification of gender bias specifically for Hindi and Indian context. We conduct several experiments to mine gender biased statements in Hindi based on existing methods proposed for English. Existing methods had several limitations when used for Hindi in the Indian context.\nDue to the challenges faced with mining such sentences we conduct field studies and adopt a community centered approach for gender bias identification to bootstrap the collection of such sentences.\nWhen it comes to identifying gender bias in Indic languages, including communities that are fluent in these languages is extremely important (Abraham et al., 2020  ###reference_b2###). Moreover, for the subjective task of gender bias identification it is crucial to include a diverse set of annotators as perceptions of bias depends on factors such as lived experiences, background, community, and others (Hada et al., 2023b  ###reference_b36###). There is an increasing adoption of technology in rural India, however, their opinions are often unheard and overlooked while these technologies are created (Pratham, 2022  ###reference_b69###). It is imperative to foster alternative voices and minority opinions during the development of these technologies (Hada et al., 2023a  ###reference_b35###; Bender et al., 2021  ###reference_b9###; Queerinai et al., 2023  ###reference_b71###). The inclusion of users in the research process presents substantial potential for mitigating power imbalances and fostering empowerment within marginalized communities that experience disproportionate impacts from AI (Queerinai et al., 2023  ###reference_b71###; Birhane et al., 2022  ###reference_b11###; Kormilitzin et al., 2023  ###reference_b49###; Suresh et al., 2022  ###reference_b83###).\nIn such a case, crowdwork, has emerged as a significant sector in the Indian labour market place (Difallah et al., 2018  ###reference_b26###, [n.\u2009d.]  ###reference_b25###), as an effective method to include local Indic language speaking communities. Additionally, from 2018 to 2022 the portion of household in India with smartphones has doubled from 36% to 74.8% (Pratham, 2022  ###reference_b69###). This proliferation of smartphones in India makes crowdwork platforms more accessible on mobile interfaces (Chopra et al., 2019  ###reference_b18###; Joshi et al., 2019  ###reference_b40###; Vashistha et al., 2017  ###reference_b87###) allowing low-income local language speakers with only basic qualifications to benefit from participating in data annotation work."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3. Sourcing Hindi Data for Gender Bias Identification",
            "text": "To mine gender-biased data specifically for Indian context we tried several existing methods that are used to mine gender-biased sentences in English or other languages. In this section, we describe the datasets we explored, our methods, and an analysis of our findings.\nCORGI-PM is a Chinese corpus for gender bias probing and mitigation (Zhang et al., 2023  ###reference_b91###). The corpus consists of 32.9K sentences with gender bias labels derived by following an annotation scheme specifically developed for Chinese context. To create the corpus, the authors follow an automatic method of data extraction from a raw corpus that gives them potentially gender-biased sentences. These sentences are then annotated for gender bias. Specifically, to create their corpus they follow a two-step filtering process. In the first step, they build a vocabulary of words that have gendered associations.\nIn the second step, they use the list of gendered keywords to recall sentences from the raw corpus. These sentences are then re-ranked, and a threshold for sentence selection is determined. The selected sentences are annotated for gender bias.\nThe corpus contains 22.5K non-biased sentences and 5.2K biased sentences.\nIndicCorp v2 is a large-scale collection of monolingual corpora for Indic languages , containing a total of 20.9 billion tokens across 23 Indian languages and English (Doddapaneni et al., 2023  ###reference_b27###). IndicCorp v2 reflects the contemporary use of Indic languages and covers a wide range of topics, primarily crawled from news articles, magazines, and blog posts. The authors source their data from popular Indian-language news websites, discovering most of their sources through online newspaper directories (e.g., w3newspaper) and automated web searches using hand-picked terms in various languages.\nKathbath is a read speech corpus (Javed et al., 2023  ###reference_b39###). The text used to create the corpus is derived from IndicCorp (Kakwani et al., 2020  ###reference_b42###). IndicCorp is a large collection of monolingual corpora consisting of 12 Indic languages collected from diverse Indic-specific sources. The authors take a subset of IndicCorp ( 100K sentences) for each of the 12 languages while limiting the sentence length to 8-15 words and allowing only for alphanumeric characters.\nIn this work, we consider the Hindi transcripts of the created corpus to mine biased sentences.\nBUG is a large-scale gender bias dataset for coreference resolution and machine translation (Levy et al., 2021  ###reference_b52###). BUG contains 108K English sentences, sampled semi-automatically from large corpora using lexical-syntactic pattern matching. To create the dataset, initially, a syntactic search is performed to identify sentences with challenging syntactic properties across corpora from three domains Wikipedia, Covid-19 research, and PubMed abstracts. Subsequently, the sentences are filtered to ensure they include at least one entity and a corresponding pronoun. The sentence is marked stereotypical or anti-stereotypical for gender roles. Lastly, a manual assessment of BUG is conducted. The dataset consists of 54K stereotypical sentences,  anti-stereotypical sentences, and  neutral sentences.\nComments or posts on social media platforms represent the thoughts of individuals and communities (Hada et al., 2021  ###reference_b37###; Mishra et al., 2019  ###reference_b59###). . Therefore, to study gender bias in the context of India, we collected comments from YouTube via the YouTube API. First, we curated a list of search queries on topics where gender bias data or polarisation was expected. The list of queries are: \u201dDeepika Padukone Cleavage Controversy\u201d, \u201dHijab ban controversy\u201d, \u201dMeninist\u2019 Deepika Bhardwaj Has A Few Questions For Feminists In India\u201d, \u201dSwara Bhaskar Marriage Controversy\u201d, \u201dManipur Women Paraded\u201d, \u201dKanika Kapoor\u2019s COVID-19 Flak\u201d, \u201dSSR and Rhea Chakraborty\u201d. We took the first 10 videos that appeared for the search queries and extracted up to 105 comments per video. This gave us a total of 7340 comments.\nLOIT is a dataset of Hindi and Telugu tweets444http://bpraneeth.com/projects/loit  ###reference_###. LOIT contains most of the Hindi tweets made between 13th January 2017 and 31st December 2018 and Telugu tweets made between 1st January 2010 and 25th June 2019. Twitter allows users to be their natural selves often leading them to portray the biases that they may have than otherwise.\nFSB is a dataset of 1000 GPT-generated English text with normative ratings for gender bias (Hada et al., 2023b  ###reference_b36###). The dataset is created by prompting GPT systematically. The authors first create a seed set of sentences sourced from various corpora.\nUsing the seed set, the authors prompt GPT-3.5-Turbo to either convert or complete a sentence to its gender-biased variation. The generations were then annotated in a comparative annotation setup to assign a gender bias score to each sentence.\nInspired by Albalak et al. (2022  ###reference_b4###), we employed an open-retrieval for emergent data collection.\nTo analyze the abundance of biased sentences in existing large-scale corpora, we choose the Hindi subset of IndicCorp v2 (Doddapaneni et al., 2023  ###reference_b27###) and sample 10M sentences from it. We mine biased sentences from this sample using the LaBSE model555https://huggingface.co/sentence-transformers/LaBSE  ###reference_rs/LaBSE### (Feng et al., 2022  ###reference_b30###), and the top 100 biased sentences from FSB as the queries. LaBSE is a language-agnostic embedding model based on BERT (Devlin et al., 2019a  ###reference_b23###), which is trained with a contrastive loss and generates close embeddings for sentences that are similar across 109 languages (Feng et al., 2022  ###reference_b30###). In our experiment, we first cache the embeddings of all the 10M sentences from the sample using FAISS-DB666https://github.com/facebookresearch/faiss  ###reference_###, and query it using the embedding of a source English sentence from FSB. The top 5 most similar sentences (in terms of cosine similarity) for each query in Hindi for a given source were collected, leading to a total of 500 potentially biased sentences. From these 500 sentences, we randomly sample 200 sentences (2 per source sentence) and 2 authors of this paper go through 100 sentences each to classify them as \u201dbiased\u201d or \u201dnot biased\u201d. We found that  20% of the sentences were biased.\nWe took a random sample of 100 sentences each from CORGI-PM (Zhang et al., 2023  ###reference_b91###) and BUG (Levy et al., 2021  ###reference_b52###) datasets. The sample was taken from sentences that were marked as biased. These sentences were translated into Hindi using Azure Translate. One of the authors of this paper annotated the 200 translated sentences to check if the sentence maintains gender bias upon translation and is relevant to the Indian context. We found that 24% of the translated BUG sentences and 33% of the translated CORGI-PM sentences remained gender biased in Hindi, and could be used in the Indian context.\nWe train a binary classifier on the CORGI-PM (Zhang et al., 2023  ###reference_b91###) dataset to classify sentences as \u201dbiased\u201d or \u201dnot biased\u201d. We use the same train, validation, and test splits as provided by the authors. We fine-tune mBERT777https://huggingface.co/bert-base-multilingual-cased  ###reference_ual-cased### (Devlin et al., 2019a  ###reference_b23###) and the corresponding hyperparameters are provided in Table 1  ###reference_###. The model achieved an accuracy of 0.81 on the test set. We used this binary classifier on the 7340 YouTube comments we extracted. Out of 7340 comments only  comments were classified as biased. We took a random sample of 100 comments from the 343 comments and one of the authors of this paper classified them as \u201dbiased\u201d or \u201dnot biased\u201d. We found that 36% of the 100 comments were biased.\nUsing the 1000 English sentences from FSB (Hada et al., 2023b  ###reference_b36###) as a training set, we finetune the IndicBERT v2888https://huggingface.co/ai4bharat/IndicBERTv2-MLM-Sam-TLM  ###reference_v2-MLM-Sam-TLM### model (Doddapaneni et al., 2023  ###reference_b27###) using LoRA (Hu et al., 2022  ###reference_b38###), to avoid over-fitting. IndicBERT v2 is the SOTA NLU model for Indic language and cross-lingual transfer to English. A regression head was appended to the model and the scores predicted were squeezed between -1 and 1, using a tanh activation, similar to FSB. Sweeps were conducted for optimal hyperparameters, and Table 2  ###reference_### provides the final hyperparameters chosen for training.\nDuring the final finetuning phase, the model and best hyperparameter configuration achieved an MSE of 0.057 and Pearson correlation of 0.85 on the validation set which is comparable with the score obtained by Hada et al. (2023b  ###reference_b36###). Using this gender bias score prediction model we obtain a gender bias score for the 7340 comments extracted from YouTube. We sample the top 100 gender bias-scored comments and one of the authors of this paper classified them as \u201dbiased\u201d or \u201dnot biased\u201d. We found that 21% of the 100 comments were biased.\nWe take a list 684 of Hindi adjectives from the Internet. Using these adjectives, we follow the steps as described by the authors of the CORGI-PM dataset (Zhang et al., 2023  ###reference_b91###) to recall sentences from raw corpora. The details are explained in Section 3.1  ###reference_###. From the list of adjectives, we first find adjectives that have a female association. We do this by measuring the dot product of the word embedding of the adjective with the word embedding of\n( - ) (we use Hindi words for man and woman). We conduct our experiments with IndicBERT v2 (Doddapaneni et al., 2023  ###reference_b27###) and mBERT (Devlin et al., 2019b  ###reference_b24###) to obtain the word representation.\nFor raw corpora, we use the Hindi sentences from Kathbath and a subset of 10M Hindi tweets from the LOIT dataset. Before we filter the sentences, we normalize the scores associated with each Hindi adjective. We set a threshold to select the top female-leaning adjectives and filter sentences from the corpora that contain these adjectives. The mBERT thresholds used to filter 100 sentences each from Kathbath and LOIT are 0.45 and 0.33 respectively. Similarly, the IndicBERT threshold used to filter 100 sentences each from Kathbath and LOIT is 0.904.\nThis gives us a total of 400 sentences. These filtered sentences are then manually inspected by one of the authors for bias. For sentences retrieved using mBERT we find that 3% of them were biased from both LOIT and Kathbath. For sentences retrieved using IndicBERT v2 we find that none of them were biased.\nUsing the seeds and the prompts provided in FSB (Hada et al., 2023b  ###reference_b36###), we generate 1800 potentially gender biased sentences in English. Six of the authors of this paper went through 300 sentences each and marked if these sentences are \u201dnon-sensical\u201d especially in the Indian context and should be removed. We found that  of these sentences were marked as \u201dnon-sensical\u201d, and the rest could be used in the Indian context, and showed a gradation of gender bias999We do not annotate these sentences as biased or unbiased because Hada et al. (2023b  ###reference_b36###) show in their work that the generations in FSB have a gradation of gender bias. Instead, upon initial examination, we found that some of the generated sentences did not make sense..\nGathering data from the internet for natural language processing tasks, especially when focusing on Indian languages and topics such as gender bias, poses a significant hurdle. In the context of NLP applications, data collection often involves web scraping. However, it\u2019s essential to acknowledge that the majority of data available on the internet tends to align with dominant viewpoints and consists primarily of content in the English language (Bender et al., 2021  ###reference_b9###). This prevalence of English-centric content on popular internet platforms makes it exceedingly challenging to obtain relevant data for languages other than English, particularly those spoken in regions like the global south. For our experiments, we used IndicCorp v2 (Doddapaneni et al., 2023  ###reference_b27###) which was a large-scale effort to collect data in Indian languages. In our experiment of LaBSE mining of gender-biased Hindi statements, using 100 gender-biased statements from FSB (Hada et al., 2023b  ###reference_b36###) as source we found only 20% of the examined statements to be biased. A higher yield from this method was expected because we sampled 10M Hindi sentences from IndicCorp v2 and picked 500 sentences that were most similar to very explicitly biased statements from FSB. The low yield could be attributed to the nature of sentences in IndicCorp v2. As mentioned earlier, the corpus was collected by scraping sentences from online news and media sources where the content might already be sanitized and censored.\nWhen data for a particular task and language is not available, the NLP community has often relied on the impressive performance of translation models to translate task data from a high-resource language to the target language.\nIn our experiments, we translated biased statements from the BUG (an English language) dataset (Levy et al., 2021  ###reference_b52###) and CORGI-PM (a Chinese language) dataset (Zhang et al., 2023  ###reference_b91###) to Hindi. From the biased statements we examined, we found only  of them to maintain their bias after translation. Out of the unbiased sentences, many sentences did not make any grammatical sense after translation or were not contextually relevant. A known challenge in the use of translation models is that they generate excessively formal renditions, potentially diluting the original colloquial or informal nuances present in the source statements(Liu et al., 2023  ###reference_b54###). For example, \u201dteacher\u201d was translated to \u201dadhyapak\u201d, and \u201dwife\u201d was translated to \u201dgrihani\u201d. In colloquial Hindi these words are \u201dteacher\u201d and \u201dbahu\u201d respectively. Furthermore, the nuanced contextual element in the original statements are often not transpositioned completely during the translation process, leading to potential misinterpretations (Zhang and Toral, 2019  ###reference_b92###; Gala et al., 2023  ###reference_b34###; Vanmassenhove et al., 2021  ###reference_b85###). Beyond these challenges, variations in linguistic structures, idiomatic expressions, and cultural nuances pose additional complexities, impeding the seamless transference of gender biases across linguistic boundaries (Ramesh et al., 2023  ###reference_b73###).\nResearchers have traditionally turned to social media platforms like Twitter, Reddit, and Meta when studying concepts like offensive language, hate speech, and identity attacks (Hada et al., 2021  ###reference_b37###; Mishra et al., 2019  ###reference_b59###; Founta et al., 2018  ###reference_b33###; Waseem and Hovy, 2016  ###reference_b89###). Social media data has several advantages like a diverse user base, and users expressing them freely and spontaneously .\nHowever, the majority of this data is still in English. For instance, a survey conducted in June 2023 by Statista indicated that approximately 50% of the desktop traffic on Reddit originates from the United States (Statista, 2023  ###reference_b82###). This prevalence of English-centric content on popular internet platforms makes it exceedingly challenging to obtain relevant data for languages other than English, particularly those spoken in regions like the global south (Joshi et al., 2020  ###reference_b41###; Ahuja et al., 2023  ###reference_b3###). Data that originates in India is still majorly in English as social media platforms are mostly used by urban population (Statista, 2022  ###reference_b81###; Varghese, 2022  ###reference_b86###). Moreover, recent trends in internet platforms have seen an increase in restrictions on data access, making it even more difficult to access social media data for research purposes (Davidson et al., 2023  ###reference_b20###). This tightening of data accessibility exacerbates the already challenging task of procuring suitable data for gender bias. For our experiments, we extracted 7340 comments from YouTube spanning over 10 controversial/polarising search queries. Our experiment with the CORGI classifier shows that only  comments were classified as biased by the classifier, and a random sample of 100 comments showed that only 36% of those comments were correctly classified as biased. Our experiment with FSB scorer shows that comments from YouTube show a distribution of gender bias score, with a skew towards non-biased or neutral comments as observed in Figure 2  ###reference_###. A sample of top-scoring 100 comments showed that only 21% of these comments are actually biased. Our analysis of random samples also revealed that the majority of the comments are in English, often target individuals over communities, and are highly profane in some cases. Therefore, collecting gender-biased Indian context data from social media platforms is a challenging task as it depends on various factors like appropriate selection of topics, choosing the right signal to boost the representation of biased comments, finding comments in a language other than English, and more.\n###figure_2### Recent works (Philippy et al., 2023  ###reference_b67###; Artetxe et al., 2020  ###reference_b5###; Lin et al., 2019  ###reference_b53###; M\u2019hamdi et al., 2023  ###reference_b57###; Gala et al., 2023  ###reference_b34###; Doddapaneni et al., 2023  ###reference_b27###) have shown the challenges and generalizability of cross-lingual transfer in multilingual language models. We hypothesize that this could be one of the reasons for the poor performance and yield of the CORGI classifier and FSB scorer. CORGI classifier was finetuned and evaluated on Chinese and Hindi respectively.\nAnother reason we attribute to their poor performance is the domain mismatch between the training and test data. CORGI was trained with non-social media data but was used to predict scores for YouTube comments. Similarly, the FSB scorer is finetuned with the limited FSB data, which are rudimentary and artificial generations in English created using GPT but is evaluated on the domain of social media content.\nIn the study of gender bias, researchers have used heuristics such as retrieval from raw corpora using gendered adjectives (Zhang et al., 2023  ###reference_b91###), using professional words (Levy et al., 2021  ###reference_b52###), and template-based sentence creation (Nadeem et al., 2021  ###reference_b61###; Stanczak and Augenstein, 2021  ###reference_b79###). Our experiment of retrieval of gender-biased statements from raw corpora using gendered adjectives returned only 3% true positive sentences when using mBERT and 0 true positives when using IndicBERT. We also tried topic-based retrieval from YouTube, coupled with classification using computational models as discussed previously. Both these methods returned a high number of false positives indicating the limitations and challenges of using heuristic-based approaches for retrieval. Additionally, a known limitation of the LLMs is their lack of cultural awareness. While models like LaBSE, IndicBERT, mBERT, and others might be linguistically aware of multiple languages, they lack cultural and social knowledge of different communities primarily because they are not exposed to topcially diverse data from different communities (Choi et al., 2023  ###reference_b17###)\nOne of the main components of tasks like gender bias is to have diverse and representative data. The meaning of a sentence is perceived based on one\u2019s identity; expression of gender bias in language can often entail cultural nuances, hence, it is equally important that the data is contextually situated (Blodgett et al., 2020  ###reference_b12###; Mostafazadeh Davani et al., 2022  ###reference_b60###; Biester et al., 2022  ###reference_b10###).\nIn our experiments, we tried various data sources such as IndicCorp v2 which is made primarily of news articles, comments from socia media sites, and text generation from GPT. Generated sentences from GPT showed the most promise in terms of getting a high proportion of biased statements. However, the generated data has limited diversity. We generate up to 4 sentences per seed provided in FSB. The themes are repetitive, sentences do not capture Indian cultural nuances, and are very simple hence do not have idiomatic or linguistic diversity either. The biggest challenge in generating data with this method is a careful selection of diverse seed sets, as the seed sentences can have a significant impact on the generation of biased sentences."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1. Datasets",
            "text": "CORGI-PM is a Chinese corpus for gender bias probing and mitigation (Zhang et al., 2023  ###reference_b91###  ###reference_b91###). The corpus consists of 32.9K sentences with gender bias labels derived by following an annotation scheme specifically developed for Chinese context. To create the corpus, the authors follow an automatic method of data extraction from a raw corpus that gives them potentially gender-biased sentences. These sentences are then annotated for gender bias. Specifically, to create their corpus they follow a two-step filtering process. In the first step, they build a vocabulary of words that have gendered associations.\nIn the second step, they use the list of gendered keywords to recall sentences from the raw corpus. These sentences are then re-ranked, and a threshold for sentence selection is determined. The selected sentences are annotated for gender bias.\nThe corpus contains 22.5K non-biased sentences and 5.2K biased sentences.\nIndicCorp v2 is a large-scale collection of monolingual corpora for Indic languages , containing a total of 20.9 billion tokens across 23 Indian languages and English (Doddapaneni et al., 2023  ###reference_b27###  ###reference_b27###). IndicCorp v2 reflects the contemporary use of Indic languages and covers a wide range of topics, primarily crawled from news articles, magazines, and blog posts. The authors source their data from popular Indian-language news websites, discovering most of their sources through online newspaper directories (e.g., w3newspaper) and automated web searches using hand-picked terms in various languages.\nKathbath is a read speech corpus (Javed et al., 2023  ###reference_b39###  ###reference_b39###). The text used to create the corpus is derived from IndicCorp (Kakwani et al., 2020  ###reference_b42###  ###reference_b42###). IndicCorp is a large collection of monolingual corpora consisting of 12 Indic languages collected from diverse Indic-specific sources. The authors take a subset of IndicCorp ( 100K sentences) for each of the 12 languages while limiting the sentence length to 8-15 words and allowing only for alphanumeric characters.\nIn this work, we consider the Hindi transcripts of the created corpus to mine biased sentences.\nBUG is a large-scale gender bias dataset for coreference resolution and machine translation (Levy et al., 2021  ###reference_b52###  ###reference_b52###). BUG contains 108K English sentences, sampled semi-automatically from large corpora using lexical-syntactic pattern matching. To create the dataset, initially, a syntactic search is performed to identify sentences with challenging syntactic properties across corpora from three domains Wikipedia, Covid-19 research, and PubMed abstracts. Subsequently, the sentences are filtered to ensure they include at least one entity and a corresponding pronoun. The sentence is marked stereotypical or anti-stereotypical for gender roles. Lastly, a manual assessment of BUG is conducted. The dataset consists of 54K stereotypical sentences,  anti-stereotypical sentences, and  neutral sentences.\nComments or posts on social media platforms represent the thoughts of individuals and communities (Hada et al., 2021  ###reference_b37###  ###reference_b37###; Mishra et al., 2019  ###reference_b59###  ###reference_b59###). . Therefore, to study gender bias in the context of India, we collected comments from YouTube via the YouTube API. First, we curated a list of search queries on topics where gender bias data or polarisation was expected. The list of queries are: \u201dDeepika Padukone Cleavage Controversy\u201d, \u201dHijab ban controversy\u201d, \u201dMeninist\u2019 Deepika Bhardwaj Has A Few Questions For Feminists In India\u201d, \u201dSwara Bhaskar Marriage Controversy\u201d, \u201dManipur Women Paraded\u201d, \u201dKanika Kapoor\u2019s COVID-19 Flak\u201d, \u201dSSR and Rhea Chakraborty\u201d. We took the first 10 videos that appeared for the search queries and extracted up to 105 comments per video. This gave us a total of 7340 comments.\nLOIT is a dataset of Hindi and Telugu tweets444http://bpraneeth.com/projects/loit  ###reference_###  ###reference_###. LOIT contains most of the Hindi tweets made between 13th January 2017 and 31st December 2018 and Telugu tweets made between 1st January 2010 and 25th June 2019. Twitter allows users to be their natural selves often leading them to portray the biases that they may have than otherwise.\nFSB is a dataset of 1000 GPT-generated English text with normative ratings for gender bias (Hada et al., 2023b  ###reference_b36###  ###reference_b36###). The dataset is created by prompting GPT systematically. The authors first create a seed set of sentences sourced from various corpora.\nUsing the seed set, the authors prompt GPT-3.5-Turbo to either convert or complete a sentence to its gender-biased variation. The generations were then annotated in a comparative annotation setup to assign a gender bias score to each sentence."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2. Experiments",
            "text": "Inspired by Albalak et al. (2022  ###reference_b4###  ###reference_b4###), we employed an open-retrieval for emergent data collection.\nTo analyze the abundance of biased sentences in existing large-scale corpora, we choose the Hindi subset of IndicCorp v2 (Doddapaneni et al., 2023  ###reference_b27###  ###reference_b27###) and sample 10M sentences from it. We mine biased sentences from this sample using the LaBSE model555https://huggingface.co/sentence-transformers/LaBSE  ###reference_rs/LaBSE###  ###reference_rs/LaBSE### (Feng et al., 2022  ###reference_b30###  ###reference_b30###), and the top 100 biased sentences from FSB as the queries. LaBSE is a language-agnostic embedding model based on BERT (Devlin et al., 2019a  ###reference_b23###  ###reference_b23###), which is trained with a contrastive loss and generates close embeddings for sentences that are similar across 109 languages (Feng et al., 2022  ###reference_b30###  ###reference_b30###). In our experiment, we first cache the embeddings of all the 10M sentences from the sample using FAISS-DB666https://github.com/facebookresearch/faiss  ###reference_###  ###reference_###, and query it using the embedding of a source English sentence from FSB. The top 5 most similar sentences (in terms of cosine similarity) for each query in Hindi for a given source were collected, leading to a total of 500 potentially biased sentences. From these 500 sentences, we randomly sample 200 sentences (2 per source sentence) and 2 authors of this paper go through 100 sentences each to classify them as \u201dbiased\u201d or \u201dnot biased\u201d. We found that  20% of the sentences were biased.\nWe took a random sample of 100 sentences each from CORGI-PM (Zhang et al., 2023  ###reference_b91###  ###reference_b91###) and BUG (Levy et al., 2021  ###reference_b52###  ###reference_b52###) datasets. The sample was taken from sentences that were marked as biased. These sentences were translated into Hindi using Azure Translate. One of the authors of this paper annotated the 200 translated sentences to check if the sentence maintains gender bias upon translation and is relevant to the Indian context. We found that 24% of the translated BUG sentences and 33% of the translated CORGI-PM sentences remained gender biased in Hindi, and could be used in the Indian context.\nWe train a binary classifier on the CORGI-PM (Zhang et al., 2023  ###reference_b91###  ###reference_b91###) dataset to classify sentences as \u201dbiased\u201d or \u201dnot biased\u201d. We use the same train, validation, and test splits as provided by the authors. We fine-tune mBERT777https://huggingface.co/bert-base-multilingual-cased  ###reference_ual-cased###  ###reference_ual-cased### (Devlin et al., 2019a  ###reference_b23###  ###reference_b23###) and the corresponding hyperparameters are provided in Table 1  ###reference_###  ###reference_###. The model achieved an accuracy of 0.81 on the test set. We used this binary classifier on the 7340 YouTube comments we extracted. Out of 7340 comments only  comments were classified as biased. We took a random sample of 100 comments from the 343 comments and one of the authors of this paper classified them as \u201dbiased\u201d or \u201dnot biased\u201d. We found that 36% of the 100 comments were biased.\nUsing the 1000 English sentences from FSB (Hada et al., 2023b  ###reference_b36###  ###reference_b36###) as a training set, we finetune the IndicBERT v2888https://huggingface.co/ai4bharat/IndicBERTv2-MLM-Sam-TLM  ###reference_v2-MLM-Sam-TLM###  ###reference_v2-MLM-Sam-TLM### model (Doddapaneni et al., 2023  ###reference_b27###  ###reference_b27###) using LoRA (Hu et al., 2022  ###reference_b38###  ###reference_b38###), to avoid over-fitting. IndicBERT v2 is the SOTA NLU model for Indic language and cross-lingual transfer to English. A regression head was appended to the model and the scores predicted were squeezed between -1 and 1, using a tanh activation, similar to FSB. Sweeps were conducted for optimal hyperparameters, and Table 2  ###reference_###  ###reference_### provides the final hyperparameters chosen for training.\nDuring the final finetuning phase, the model and best hyperparameter configuration achieved an MSE of 0.057 and Pearson correlation of 0.85 on the validation set which is comparable with the score obtained by Hada et al. (2023b  ###reference_b36###  ###reference_b36###). Using this gender bias score prediction model we obtain a gender bias score for the 7340 comments extracted from YouTube. We sample the top 100 gender bias-scored comments and one of the authors of this paper classified them as \u201dbiased\u201d or \u201dnot biased\u201d. We found that 21% of the 100 comments were biased.\nWe take a list 684 of Hindi adjectives from the Internet. Using these adjectives, we follow the steps as described by the authors of the CORGI-PM dataset (Zhang et al., 2023  ###reference_b91###  ###reference_b91###) to recall sentences from raw corpora. The details are explained in Section 3.1  ###reference_###  ###reference_###. From the list of adjectives, we first find adjectives that have a female association. We do this by measuring the dot product of the word embedding of the adjective with the word embedding of\n( - ) (we use Hindi words for man and woman). We conduct our experiments with IndicBERT v2 (Doddapaneni et al., 2023  ###reference_b27###  ###reference_b27###) and mBERT (Devlin et al., 2019b  ###reference_b24###  ###reference_b24###) to obtain the word representation.\nFor raw corpora, we use the Hindi sentences from Kathbath and a subset of 10M Hindi tweets from the LOIT dataset. Before we filter the sentences, we normalize the scores associated with each Hindi adjective. We set a threshold to select the top female-leaning adjectives and filter sentences from the corpora that contain these adjectives. The mBERT thresholds used to filter 100 sentences each from Kathbath and LOIT are 0.45 and 0.33 respectively. Similarly, the IndicBERT threshold used to filter 100 sentences each from Kathbath and LOIT is 0.904.\nThis gives us a total of 400 sentences. These filtered sentences are then manually inspected by one of the authors for bias. For sentences retrieved using mBERT we find that 3% of them were biased from both LOIT and Kathbath. For sentences retrieved using IndicBERT v2 we find that none of them were biased.\nUsing the seeds and the prompts provided in FSB (Hada et al., 2023b  ###reference_b36###  ###reference_b36###), we generate 1800 potentially gender biased sentences in English. Six of the authors of this paper went through 300 sentences each and marked if these sentences are \u201dnon-sensical\u201d especially in the Indian context and should be removed. We found that  of these sentences were marked as \u201dnon-sensical\u201d, and the rest could be used in the Indian context, and showed a gradation of gender bias999We do not annotate these sentences as biased or unbiased because Hada et al. (2023b  ###reference_b36###  ###reference_b36###) show in their work that the generations in FSB have a gradation of gender bias. Instead, upon initial examination, we found that some of the generated sentences did not make sense.."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "3.3. Results and Analysis",
            "text": "With the above datasets and methods, we saw varying yields of potentially gender-biased statements in the Indian context. Table 3  ###reference_### gives a summary of the yield for the different methods and datasets. In this section, we discuss the key challenges faced in using each of these methods and datasets:\nGathering data from the internet for natural language processing tasks, especially when focusing on Indian languages and topics such as gender bias, poses a significant hurdle. In the context of NLP applications, data collection often involves web scraping. However, it\u2019s essential to acknowledge that the majority of data available on the internet tends to align with dominant viewpoints and consists primarily of content in the English language (Bender et al., 2021  ###reference_b9###  ###reference_b9###). This prevalence of English-centric content on popular internet platforms makes it exceedingly challenging to obtain relevant data for languages other than English, particularly those spoken in regions like the global south. For our experiments, we used IndicCorp v2 (Doddapaneni et al., 2023  ###reference_b27###  ###reference_b27###) which was a large-scale effort to collect data in Indian languages. In our experiment of LaBSE mining of gender-biased Hindi statements, using 100 gender-biased statements from FSB (Hada et al., 2023b  ###reference_b36###  ###reference_b36###) as source we found only 20% of the examined statements to be biased. A higher yield from this method was expected because we sampled 10M Hindi sentences from IndicCorp v2 and picked 500 sentences that were most similar to very explicitly biased statements from FSB. The low yield could be attributed to the nature of sentences in IndicCorp v2. As mentioned earlier, the corpus was collected by scraping sentences from online news and media sources where the content might already be sanitized and censored.\nWhen data for a particular task and language is not available, the NLP community has often relied on the impressive performance of translation models to translate task data from a high-resource language to the target language.\nIn our experiments, we translated biased statements from the BUG (an English language) dataset (Levy et al., 2021  ###reference_b52###  ###reference_b52###) and CORGI-PM (a Chinese language) dataset (Zhang et al., 2023  ###reference_b91###  ###reference_b91###) to Hindi. From the biased statements we examined, we found only  of them to maintain their bias after translation. Out of the unbiased sentences, many sentences did not make any grammatical sense after translation or were not contextually relevant. A known challenge in the use of translation models is that they generate excessively formal renditions, potentially diluting the original colloquial or informal nuances present in the source statements(Liu et al., 2023  ###reference_b54###  ###reference_b54###). For example, \u201dteacher\u201d was translated to \u201dadhyapak\u201d, and \u201dwife\u201d was translated to \u201dgrihani\u201d. In colloquial Hindi these words are \u201dteacher\u201d and \u201dbahu\u201d respectively. Furthermore, the nuanced contextual element in the original statements are often not transpositioned completely during the translation process, leading to potential misinterpretations (Zhang and Toral, 2019  ###reference_b92###  ###reference_b92###; Gala et al., 2023  ###reference_b34###  ###reference_b34###; Vanmassenhove et al., 2021  ###reference_b85###  ###reference_b85###). Beyond these challenges, variations in linguistic structures, idiomatic expressions, and cultural nuances pose additional complexities, impeding the seamless transference of gender biases across linguistic boundaries (Ramesh et al., 2023  ###reference_b73###  ###reference_b73###).\nResearchers have traditionally turned to social media platforms like Twitter, Reddit, and Meta when studying concepts like offensive language, hate speech, and identity attacks (Hada et al., 2021  ###reference_b37###  ###reference_b37###; Mishra et al., 2019  ###reference_b59###  ###reference_b59###; Founta et al., 2018  ###reference_b33###  ###reference_b33###; Waseem and Hovy, 2016  ###reference_b89###  ###reference_b89###). Social media data has several advantages like a diverse user base, and users expressing them freely and spontaneously .\nHowever, the majority of this data is still in English. For instance, a survey conducted in June 2023 by Statista indicated that approximately 50% of the desktop traffic on Reddit originates from the United States (Statista, 2023  ###reference_b82###  ###reference_b82###). This prevalence of English-centric content on popular internet platforms makes it exceedingly challenging to obtain relevant data for languages other than English, particularly those spoken in regions like the global south (Joshi et al., 2020  ###reference_b41###  ###reference_b41###; Ahuja et al., 2023  ###reference_b3###  ###reference_b3###). Data that originates in India is still majorly in English as social media platforms are mostly used by urban population (Statista, 2022  ###reference_b81###  ###reference_b81###; Varghese, 2022  ###reference_b86###  ###reference_b86###). Moreover, recent trends in internet platforms have seen an increase in restrictions on data access, making it even more difficult to access social media data for research purposes (Davidson et al., 2023  ###reference_b20###  ###reference_b20###). This tightening of data accessibility exacerbates the already challenging task of procuring suitable data for gender bias. For our experiments, we extracted 7340 comments from YouTube spanning over 10 controversial/polarising search queries. Our experiment with the CORGI classifier shows that only  comments were classified as biased by the classifier, and a random sample of 100 comments showed that only 36% of those comments were correctly classified as biased. Our experiment with FSB scorer shows that comments from YouTube show a distribution of gender bias score, with a skew towards non-biased or neutral comments as observed in Figure 2  ###reference_###  ###reference_###. A sample of top-scoring 100 comments showed that only 21% of these comments are actually biased. Our analysis of random samples also revealed that the majority of the comments are in English, often target individuals over communities, and are highly profane in some cases. Therefore, collecting gender-biased Indian context data from social media platforms is a challenging task as it depends on various factors like appropriate selection of topics, choosing the right signal to boost the representation of biased comments, finding comments in a language other than English, and more.\n###figure_3### Recent works (Philippy et al., 2023  ###reference_b67###  ###reference_b67###; Artetxe et al., 2020  ###reference_b5###  ###reference_b5###; Lin et al., 2019  ###reference_b53###  ###reference_b53###; M\u2019hamdi et al., 2023  ###reference_b57###  ###reference_b57###; Gala et al., 2023  ###reference_b34###  ###reference_b34###; Doddapaneni et al., 2023  ###reference_b27###  ###reference_b27###) have shown the challenges and generalizability of cross-lingual transfer in multilingual language models. We hypothesize that this could be one of the reasons for the poor performance and yield of the CORGI classifier and FSB scorer. CORGI classifier was finetuned and evaluated on Chinese and Hindi respectively.\nAnother reason we attribute to their poor performance is the domain mismatch between the training and test data. CORGI was trained with non-social media data but was used to predict scores for YouTube comments. Similarly, the FSB scorer is finetuned with the limited FSB data, which are rudimentary and artificial generations in English created using GPT but is evaluated on the domain of social media content.\nIn the study of gender bias, researchers have used heuristics such as retrieval from raw corpora using gendered adjectives (Zhang et al., 2023  ###reference_b91###  ###reference_b91###), using professional words (Levy et al., 2021  ###reference_b52###  ###reference_b52###), and template-based sentence creation (Nadeem et al., 2021  ###reference_b61###  ###reference_b61###; Stanczak and Augenstein, 2021  ###reference_b79###  ###reference_b79###). Our experiment of retrieval of gender-biased statements from raw corpora using gendered adjectives returned only 3% true positive sentences when using mBERT and 0 true positives when using IndicBERT. We also tried topic-based retrieval from YouTube, coupled with classification using computational models as discussed previously. Both these methods returned a high number of false positives indicating the limitations and challenges of using heuristic-based approaches for retrieval. Additionally, a known limitation of the LLMs is their lack of cultural awareness. While models like LaBSE, IndicBERT, mBERT, and others might be linguistically aware of multiple languages, they lack cultural and social knowledge of different communities primarily because they are not exposed to topcially diverse data from different communities (Choi et al., 2023  ###reference_b17###  ###reference_b17###)\nOne of the main components of tasks like gender bias is to have diverse and representative data. The meaning of a sentence is perceived based on one\u2019s identity; expression of gender bias in language can often entail cultural nuances, hence, it is equally important that the data is contextually situated (Blodgett et al., 2020  ###reference_b12###  ###reference_b12###; Mostafazadeh Davani et al., 2022  ###reference_b60###  ###reference_b60###; Biester et al., 2022  ###reference_b10###  ###reference_b10###).\nIn our experiments, we tried various data sources such as IndicCorp v2 which is made primarily of news articles, comments from socia media sites, and text generation from GPT. Generated sentences from GPT showed the most promise in terms of getting a high proportion of biased statements. However, the generated data has limited diversity. We generate up to 4 sentences per seed provided in FSB. The themes are repetitive, sentences do not capture Indian cultural nuances, and are very simple hence do not have idiomatic or linguistic diversity either. The biggest challenge in generating data with this method is a careful selection of diverse seed sets, as the seed sentences can have a significant impact on the generation of biased sentences."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4. Crowdsourcing Contextually Relevant Gender Bias Definition and Annotations",
            "text": "Due to the difficulties encountered in developing a dataset on gender bias in Hindi using established mining techniques previously suggested for English, we conducted field studies to bootstrap the compilation of such sentences. We do this by adopting a community-centered approach for gender bias identification. This section explores two field studies we conducted in the pursuit of understanding how to define and identify gender bias in Hindi.\nDuring the initial stages of the workshop, participants expressed confidence in understanding gender bias, perceiving no need for discussions on the distinctions between sex and gender. As sessions progressed, however, it became evident that many participants were inadvertently conflating these concepts. Throughout the workshop, participant enthusiasm remained high, with active engagement and occasional challenges to facilitators on the concepts they brought forth.\nThe frog exercise worked well in exposing prevalent gender stereotypes as participants assigned stereotypical attributes to the frogs based on gender. For the \u201cfemale\u201d frog, the participants were more likely to use adjectives such as \u201cafraid\u201d, \u201cweak\u201d and \u201cunsure\u201d; whereas for the \u201cmale\u201d frog, words like \u201cbrave\u201d and \u201ccurious\u201d were assigned. Interestingly for the \u201cmale\u201d frog, perceptions were not as constant among the participants. Some participants assigned the male frog adjectives with more negative connotations such as \u201cstupid\u201d and \u201cconcerned only about seeming courageous\u201d.\nIn the skit and role-play activities, a shift towards more equitable portrayals emerged, indicating a loosening of rigid gender norms. About 75% of the paired teams decided to actively portray scenes that depicted equity and parity between men and women across various scenarios. Skits where \u201cbias of roles\u201d were still present were caveated through a warning that the skit represented the norms and ways their parents or people in the community would act. There became an interesting emphasis at this point on what was \u201cright\u201d and what was a \u201cwrong\u201d way to speak about people, with the overwhelming conclusion being that gender has nothing to do with why a person might behave a certain way.\nThe creation of a shared definition posed challenges, primarily due to participant fatigue, yet the exercise yielded valuable insights, uncovering over 80 unique sentences illustrating various gender biases, phrases, and words contributing to women\u2019s marginalization in everyday scenarios. Despite the difficulty in crystallizing a definitive definition, the workshop successfully captured nuanced perspectives on gender bias.\nThe annotation study revealed several critical insights surrounding the scalability of this type of work. Key challenges occurred in achieving agreement among annotators, with initial agreement values around 0.08, indicating random annotations by the participants. After filtering out annotations completed in less than 30 seconds, a slight improvement to 0.11 in agreement was observed. However, 30% of annotations selected the same item for both \u201dmost\u201d and \u201dleast\u201d biased sentences, suggesting issues with the task structure and understanding.\nInterviews with participants confirmed the supposition that participants had difficulty in completing the task.\nMost of the women highlighted that they did not understand the virtual training that occurred. While some were not prepared to be connected via a virtual mode facing challenges connecting through video conferencing, others found the language and overall explanation of the tasks too confusing and difficult to follow.\nFurther probing highlighted sentence construction and comprehension as a significant barrier; women either found the formal Hindi used in the sentences difficult to understand or felt that the sentences were \u201cnonsensical\u201d. A full manual review of the dataset by native Hindi speakers found that approximately 30% of sentences were identified as not aligned with the cultural and social contexts of rural India. Roughly 11% of the sentences did not have a clear meaning when translated, examples of these include \u201cthe country discovered new land\u201d. 17% included words like \u201cdating\u201d or \u201ccowboy\u201d, which are not commonly found in the Indian vocabulary. A precipitating point of confusion was the formality of translation, many of the words in this grouping were transliterated in the annotated corpora which led to misunderstanding amongst the annotators. A few sentences (2\u0303%) described scenes or turns of phrase that were unfamiliar to the Indian context in general, such as \u201cthanksgiving\u201d, \u201cbachelorette\u201d and \u201cmowing the lawn\u201d.\nResults from this field study indicate that critical revisions in design and process are required to make this type of task understandable and accessible to participants. The key areas identified for improvement are three-fold. First, establishing more intensive training elements and administering pre-testing to ensure task completion at quality. Second, ensuring that GPT-generated sentences match the social and cultural contexts of the Hindi-speaking community. Third, creating a simpler and more intuitive task structure and expectations."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1. Case Study 1: Gender Bias Awareness Workshop - Lucknow, UP",
            "text": "In this field study we aimed to gain a shared understanding of gender bias within a specific community and crowd-source gender bias sentences in Hindi that could inform our data collection process. Definitions of gender bias cannot be constant across cultures and contexts. Global North definitions of gender bias in particular, should not be used to assess situations and environments in the Global South. Braff and Nelson (2022  ###reference_b14###) highlight how the Global North has played a critical role in shaping gender norms and structures as they are often spoken about today. These are however neither universal nor natural. It is therefore crucial to consider the cultural, political, and economic contexts of each region when addressing gender issues.\nIn this case study where gender bias needed to be spoken about to women, a certain challenge was met in finding the right words and expressions to accurately convey messaging. As an example, the Hindi word for \u201cbias\u201d in the way it is understood in English does not exist; rather commonly used words for this topic instead indicate \u201cdiscrimination\u201d \u201cfavoritism\u201d or \u201cexclusion\u201d. To this end, we conducted a qualitative experiment to test whether definitions of gender bias in Indian languages could be generated through community-centered methods.\nWorking in collaboration with the Milaan Foundation, a prominent non-governmental organization in India that has been working on women\u2019s empowerment and gender bias for over 20 years, we developed a Gender Bias Awareness Workshop. The workshop was held in Lucknow district of Uttar Pradesh state in Northern India with 14 women aged 18-24. Most of these women were enrolled in their bachelor\u2019s degree programs and had also been a part of previous gender-related activities held by the Milaan Foundation. All workshop facilitation and engagement from participants occurred in Hindi to ensure the capture of language-relevant expressions of gender bias. The workshops were structured around two primary components: an exploration of fundamental concepts and the implementation of four key activities to ensure inputs and involvement from the community. In the conceptual exploration participants delved into the distinctions between sex and gender, analyzed gendered language within the context of Hindi, understood the nuances of gendered and non-gendered explanations, identified prevalent instances of gender bias in the daily lives of Indian women, and shared their personal experiences of gender bias. Complementing these discussions were four interactive activities: a storytelling exercise involving two frogs to uncover backstories, skit and role-play scenarios portraying gender dynamics in various situations, a collaborative compilation of a list highlighting bias towards women, the creation of a shared definition of gender bias using the simple majority voting method for bias determination."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2. Case Study 2: Annotation Study - Kannauj, UP",
            "text": "We conducted an annotation study in the Kannauj district of the Northern Indian state of Uttar Pradesh to investigate the identification of gender bias in sentences generated by GPT-3.5-Turbo. Hada et al. (2023b  ###reference_b36###) show in their work that humans can identify gender bias to varying degrees. They use an efficient comparative annotation framework called Best\u2013Worst Scaling (BWS) to obtain a degree of gender bias score per statement. They argue that BWS has shown to be very effective for annotation of subjective tasks such as offensive language and show it can be extended to gender bias. We followed the data generation and annotation process described in their work. Hada et al. (2023b  ###reference_b36###) have their data annotated by an urban population, and highlight the importance of incorporating subjective judgments for this task.\nIn contrast, we have our data annotated by a rural population, to include minority opinion for such tasks. Specifically, the study focused on employing women from rural and low-income communities to annotate a Hindi text corpus and identify sentences with gender bias. The overarching goal was to explore the feasibility of employing low-income women to generate foundational annotated datasets that could support the automation of gender bias detection and mitigation in Hindi corpora. This field study hypothesized that by employing women from Hindi-language speaking communities to highlight bias in GPT-generated corpora, identified sentences would be representative of the biases, stereotypes, and marginalization that the larger population of Hindi-speaking women may face.\nStudy Structure & Participants:\nThis study framed the annotation of gender bias sentences as digital work tasks that were deployed through the Karya App. 2000 English sentences were generated by GPT-3.5-Turbo using the prompting strategies described in (Hada et al., 2023b  ###reference_b36###) to create a corpus of sentences. Specifically, given a seed statement, we prompted GPT-3.5-Turbo to generate a gender-biased completion or conversion of the statement. We randomly sample sentences from COPA as our seed statements.\nThese gender-biased generations were then translated into Hindi. We initially used machine translation systems, however these did not yield positive results as the outputs did not pass manual quality checks. The final sentences for the task were manually translated for this study to ensure higher quality. This translated corpora was used to create 2N 4-tuples for the BWS setup as described in other works (Hada et al., 2023b  ###reference_b36###, 2021  ###reference_b37###; Louviere, 1991  ###reference_b55###; Kiritchenko and Mohammad, 2017  ###reference_b45###; Orme, 2009  ###reference_b65###; Flynn and Marley, 2014  ###reference_b31###; Pei and Jurgens, 2020  ###reference_b66###; Kiritchenko and Mohammad, 2016  ###reference_b46###). The tuples were randomly assigned to each participant.\nThe participants were shown a tuple and asked to identify which sentence was \u201cmost biased\u201d and which sentence was \u201cleast biased\u201d. Each participant was given 261 tuples to annotate.\nParticipants of this study involved 15 women from low-income backgrounds who underwent a two-hour virtual training delivered by the research team. All participants were native or fluent Hindi speakers who identified as women, and 46% were aged 18-25. The majority reported being part of a marginalized caste (93%), practicing Hinduism (94%), and being married (74%). Education levels varied significantly, with 33% of women not passing the 10th grade and 30% with a High School Diploma or Bachelor\u2019s degree. Income levels were low, with 66% coming from households earning less than INR 12,000 (USD 144) per month. Most women reported being unemployed and without a steady income; when working, these women were predominantly engaged in agricultural or domestic work."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "4.3. Results and Analysis",
            "text": "During the initial stages of the workshop, participants expressed confidence in understanding gender bias, perceiving no need for discussions on the distinctions between sex and gender. As sessions progressed, however, it became evident that many participants were inadvertently conflating these concepts. Throughout the workshop, participant enthusiasm remained high, with active engagement and occasional challenges to facilitators on the concepts they brought forth.\nThe frog exercise worked well in exposing prevalent gender stereotypes as participants assigned stereotypical attributes to the frogs based on gender. For the \u201cfemale\u201d frog, the participants were more likely to use adjectives such as \u201cafraid\u201d, \u201cweak\u201d and \u201cunsure\u201d; whereas for the \u201cmale\u201d frog, words like \u201cbrave\u201d and \u201ccurious\u201d were assigned. Interestingly for the \u201cmale\u201d frog, perceptions were not as constant among the participants. Some participants assigned the male frog adjectives with more negative connotations such as \u201cstupid\u201d and \u201cconcerned only about seeming courageous\u201d.\nIn the skit and role-play activities, a shift towards more equitable portrayals emerged, indicating a loosening of rigid gender norms. About 75% of the paired teams decided to actively portray scenes that depicted equity and parity between men and women across various scenarios. Skits where \u201cbias of roles\u201d were still present were caveated through a warning that the skit represented the norms and ways their parents or people in the community would act. There became an interesting emphasis at this point on what was \u201cright\u201d and what was a \u201cwrong\u201d way to speak about people, with the overwhelming conclusion being that gender has nothing to do with why a person might behave a certain way.\nThe creation of a shared definition posed challenges, primarily due to participant fatigue, yet the exercise yielded valuable insights, uncovering over 80 unique sentences illustrating various gender biases, phrases, and words contributing to women\u2019s marginalization in everyday scenarios. Despite the difficulty in crystallizing a definitive definition, the workshop successfully captured nuanced perspectives on gender bias.\nThe annotation study revealed several critical insights surrounding the scalability of this type of work. Key challenges occurred in achieving agreement among annotators, with initial agreement values around 0.08, indicating random annotations by the participants. After filtering out annotations completed in less than 30 seconds, a slight improvement to 0.11 in agreement was observed. However, 30% of annotations selected the same item for both \u201dmost\u201d and \u201dleast\u201d biased sentences, suggesting issues with the task structure and understanding.\nInterviews with participants confirmed the supposition that participants had difficulty in completing the task.\nMost of the women highlighted that they did not understand the virtual training that occurred. While some were not prepared to be connected via a virtual mode facing challenges connecting through video conferencing, others found the language and overall explanation of the tasks too confusing and difficult to follow.\nFurther probing highlighted sentence construction and comprehension as a significant barrier; women either found the formal Hindi used in the sentences difficult to understand or felt that the sentences were \u201cnonsensical\u201d. A full manual review of the dataset by native Hindi speakers found that approximately 30% of sentences were identified as not aligned with the cultural and social contexts of rural India. Roughly 11% of the sentences did not have a clear meaning when translated, examples of these include \u201cthe country discovered new land\u201d. 17% included words like \u201cdating\u201d or \u201ccowboy\u201d, which are not commonly found in the Indian vocabulary. A precipitating point of confusion was the formality of translation, many of the words in this grouping were transliterated in the annotated corpora which led to misunderstanding amongst the annotators. A few sentences (2\u0303%) described scenes or turns of phrase that were unfamiliar to the Indian context in general, such as \u201cthanksgiving\u201d, \u201cbachelorette\u201d and \u201cmowing the lawn\u201d.\nResults from this field study indicate that critical revisions in design and process are required to make this type of task understandable and accessible to participants. The key areas identified for improvement are three-fold. First, establishing more intensive training elements and administering pre-testing to ensure task completion at quality. Second, ensuring that GPT-generated sentences match the social and cultural contexts of the Hindi-speaking community. Third, creating a simpler and more intuitive task structure and expectations."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5. Discussion",
            "text": "Bring all findings together, our work suggests that studying gender bias especially in the context of the global south or more specifically India can be extremely challenging. Our extensive efforts, spanning experiments and on-field studies, showcase the intricate landscape of gender bias, revealing the need for nuanced strategies and inclusive approaches. We tried several methods of mining gender-biased data from different sources. Our experiments with mining gender-biased sentences highlighted various difficulties. Internet data being very Anglo-centric does not serve as a good source for mining gender biased data for the global south. Extracting data from social media has become increasingly difficult due to growing restrictions. The data we extracted from YouTube based on certain topics had comments majorly in English. From the random sample we annotated based on results from classifiers or gender-biased scoring, only a small portion was actually biased indicating the need for a more careful selection of topics, and domain-specific computational models. We used IndicCorp v2 to mine gender-biased sentences in Hindi. Our experiments with LaBSE embedding and explicitly gender-biased statements, a common method to retrieve similar sentences in two different languages, returned many false positives. This could be attributed to the fact that IndicCorp v2 has been sourced primarily from news outlets containing sentences that are sanitized and censored. Our experiments also highlighted the limitations of current language embeddings, showing limited performance in cross-lingual and cross-domain transfer capabilities. We found using translation systems to be a bottleneck due to their inability to translate sentences contextually and colloquially. We also found keyword-based approaches to retrieval to be ineffective. Finally, the generation of gender biased sentences from GPT showed some promise, however, had a very limited diversity.\nChallenges faced with mining gender biased sentences in Hindi motivated us to conduct field studies to crowd-source such sentences. To incorporate real-world perspectives we delve into the nuanced interpretation of gender bias by conducting two on-field studies. Our case studies bridge the gap between controlled experiments and the dynamic realities of crowd-sourced, culturally relevant insights. We conducted on-field studies to crowd-source gender biased Hindi statements with culturally relevant definition of gender bias and a comparative annotation task for identifying gender-biased statements. The gender bias awareness workshop revealed several key takeaways. The initial overconfidence expressed by participants in understanding gender bias, juxtaposed with their evolving comprehension throughout the workshop, underscores the subtle complexity of the issue. An interactive gamified frog exercise helped us tap into the tacit knowledge of the workshop participants by revealing ingrained gender stereotypes, emphasizing the pervasive nature of biased attributions. The skit and role-play activities provided a platform for a positive shift, with a significant majority actively portraying scenes that challenged traditional gender norms, signaling a growing inclination towards equitable representations. Of notable interest was the acknowledgment of variability in perceptions. Discussions within the workshop revealed personalization of gender bias for participants. Overall, the workshop highlights that gender bias has ill-defined boundaries and a highly subjective interpretation of the concept based on lived experiences, background, education, and other factors. Our workshop not only unveiled the intricacies of biased language and perceptions but also highlighted the need for continued efforts in fostering inclusive dialogue with the community. Our pilot annotation study for annotating gender-biased statements in a comparative annotation setup highlighted challenges with crowdsourcing annotations for this task. One key takeaway was that low-income or rural crowd workers present an additional challenge when dealing with gender bias. While urban audiences can complete such tasks with high accuracy, it is important to think about how we can include minority voices and non-dominant viewpoints for subjective tasks such as gender bias (Hada et al., 2023a  ###reference_b35###; Bender et al., 2021  ###reference_b9###). Post-annotation interviews with participants provided valuable context, unveiling a spectrum of challenges encompassing task understanding, virtual training, and language barriers. Women participants, facing connectivity challenges and linguistic complexities, expressed their difficulties with the virtual mode and the formal Hindi used in the sentences. A manual review by native Hindi speakers identified 30% sentences as misaligned with the cultural context, indicating the necessity for culturally sensitive content. The complexity of translation as also revealed by our experiments, including transliterations and unfamiliar phrases, contributed to misunderstandings among annotators. This annotation study underscores the importance of designing annotation tasks while keeping a variety of audiences in mind. While such tasks have been tested predominantly with the WEIRD or an urban population, not including viewpoints from a rural population can lead to cultural erasure and propagation of hegemonic viewpoints (Bender et al., 2021  ###reference_b9###; Prabhakaran et al., 2022  ###reference_b68###).\nOverall, our study highlights several challenges faced with mining gender-biased data in the Indian context, subjective understanding of the concept with ill-defined boundaries, and including minority opinions for this task. In the future, it would be interesting to explore a more participatory approach for this task. Works leveraging \u201dgames with a purpose\u201d (GWAP) (Balayn et al., 2022  ###reference_b6###; von Ahn et al., 2006  ###reference_b88###) can be adopted for crowdsourcing generation of gender-biased sentences in target language with cultural nuances, as these methods have shown promise to tap into the tacit knowledge of the crowd. For annotating gender-biased statements a more careful task design is required depending on the target audience, and effort should be made to include minority opinions. It would also be interesting to study the concept of intersectionality introduced by Crenshaw in her foundational work (Crenshaw, 1991  ###reference_b19###). She emphasized that social categories like race, gender, and class are interlinked, thereby mutually creating unique dimensions of oppression that are not adequately addressed by frameworks that consider such social categories separately. Drawing on Crenshaw\u2019s work on intersectionality, it is pivotal that the datasets designed to capture gender bias in Indic languages contain sentences that account for bias emerging from these intersecting marginalized identities to authentically capture not just gender bias but its intersection with structural embeddings of caste, religion, and rurality, among others."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6. Limitations",
            "text": "Efforts to mine gender-biased data faced obstacles stemming from the dominance of Anglo-centric content on the internet. Additionally, the growing restrictions on social media further limited our ability to extract diverse and representative datasets. In future, it would be interesting to explore other social media sites like Meta and Twitter if necessary permissions can be obtained. Despite employing various methods, our experiments in mining gender-biased sentences revealed significant difficulties. The low prevalence of biased sentences in the annotated sample suggests challenges in topic selection and the need for domain-specific computational models. The use of translation systems as a bridge between languages revealed substantial limitations. Inability to translate sentences contextually and colloquially hindered our understanding of gender bias in diverse linguistic contexts. The complexities of translation, including transliterations and unfamiliar phrases, contributed to misunderstandings among annotators, underscoring the need for culturally sensitive content. Although the generation of gender bias showed promise, the method exhibited a lack of diversity. In future, it would be interesting to explore GPT generations with more contextual seeds and in-context examples, and changes in the prompt for increasing diversity and including culturally relevant information.\nThe on-field studies faced challenges in annotating gender-biased statements due to the complex nature of the task. Connectivity issues, linguistic complexities, and cultural misalignments highlighted the difficulties faced by participants. These methodological limitations collectively point to the intricate nature of studying gender bias and emphasize the necessity for continuous refinement of our methods to better capture the complexities inherent in diverse cultural contexts, such as India."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7. Ethical Considerations",
            "text": "We use the framework by Bender and Friedman (2018  ###reference_b8###) to discuss the ethical considerations for our work.\nInstitutional Review: All aspects of this research were reviewed and approved by Microsoft Research IRB.\nData: Several publicly available datasets were examined in this study. We also prompt GPT-3.5-Turbo to generate gender biased statements. No personally identifiable information (PII) was collected or distribued in this process. No PII was included in prompt or in-context examples to GPT.\nAnnotator Demographics: Annotations for the mining experiments were done by researchers interested in studying gender bias in language technologies. These annotators were from India and had native proficiency in Hindi and English. The annotators had at least an undergraduate degree as their minimum educational qualification. The field studies were conducted with rural, low income women. The demographics are described in section 4  ###reference_###. The participants were recruited via a data annotation platform. The pay was adjusted after discussion with the company. The company particularly ensures fair pay (20 times the minimum wage) to low-income communities in India.\nAnnotation Guidelines: For the gender bias awareness workshop, as described in section 4.1  ###reference_###, we partner with an organization working in the domain of women\u2019s empowerment and gender bias for a long time. All modules were created by this organization. For gender bias annotation workshop we draw from the guidelines described by Hada et al. (2023b  ###reference_b36###). These guidelines were written in Hindi, and the task was explained verbally during a training session conducted online. Annotators were given detailed instructions, and walk-through of the task, with examples.\nImpact on Annotators: For the crowd-sourced annotation study we limited the number of annotations per participant, provided a mix of explicitly, implicitly, and neutrally biased sentences, asked annotators to skip and report instances they were not comfortable annotating, and lastly, encouraged open-dialogue with the workshop facilitator.\nMethods: In this study we explore several methods to mine gender biased data in Hindi. We discuss the challenges and limitations of these methods. We also trained computational models for automatic classification or scoring of gender bias. While these methods can be easily misused, our intent with this study is to highlight the limitations of these methods when used in the context of Global South."
        }
    ],
    "url": "http://arxiv.org/html/2405.06346v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2"
        ],
        "main_experiment_and_results_sections": [
            "3.2",
            "3.3",
            "4",
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "3.2",
            "3.3"
        ]
    },
    "research_context": {
        "paper_id": "2405.06346v1",
        "paper_title": "Akal Badi ya Bias: An Exploratory Study of Gender Bias in Hindi Language Technology",
        "research_background": "**Paper's Motivation:**\n\nThe motivation for this paper lies in the increasing integration of Large Language Models (LLMs) into various linguistic and real-world applications. As these technologies have become prevalent, it has become imperative to comprehend the biases they may propagate, especially gender biases, which can result in the underrepresentation, stereotyping, or misrepresentation of women and gender minorities. The existing research on gender bias predominantly focuses on English and contexts of the Global North, leaving a significant gap in understanding and addressing gender biases within the Global South, particularly in Indic languages like Hindi. This study seeks to bridge this gap by presenting a comprehensive examination of gender bias within Hindi language technology and promoting an inclusive and equitable digital society.\n\n**Research Problem:**\n\nThe research problem tackled by this study involves the identification, understanding, and mitigation of gender bias in Hindi language technology. The main issues addressed include:\n1. The difficulty of using parameters, benchmarks, and guidelines developed for English to measure and mitigate gender bias in Indic languages.\n2. The challenges related to mining gender-biased data in Hindi due to Anglo-centric datasets, social media restrictions, heuristic-based approaches leading to high false positives, limited transfer capabilities of computational models, and inadequate translations from industrial systems.\n3. Incorporating localized and culturally relevant perspectives from rural, low-income women to gain a nuanced understanding of gender bias and its manifestation in Hindi.\n\n**Relevant Prior Work:**\n\nThe paper builds on a vast corpus of prior research, particularly focusing on how LLMs and Natural Language Generation (NLG) models can generate or intensify biases, negatively impacting marginalized communities (references such as Lucy et al., 2023  ###reference_b56###; Trajanovski et al., 2021  ###reference_b84###; Mieczkowski et al., 2021  ###reference_b58###; Buschek et al., 2021  ###reference_b15###; Kirk et al., 2021  ###reference_b47###). \n\nThe study further aligns with works emphasizing the critical nature of addressing gender bias in technologies to promote inclusivity and equity (Stanczak and Augenstein, 2021  ###reference_b79###; for Economic Co-operation and, OECD  ###reference_b32###). \n\nAdditionally, the study references notable works concerning the characterization of Indic languages as a superset (Gala et al., 2023  ###reference_b34###; Doddapaneni et al., 2023  ###reference_b27###) and acknowledges differences encountered in transferring methodologies developed for English bias detection to Hindi. It draws from grassroots approaches and community-centered research highlighting the significance of local context and minority opinions in AI development (Queerinai et al., 2023  ###reference_b71###; Birhane et al., 2022  ###reference_b11###; Kormilitzin et al., 2023  ###reference_b49###; Suresh et al., 2022  ###reference_b83###). \n\nIn conclusion, the study presents a foundational effort to address gender bias in Hindi language technology comprehensively, considering the unique cultural and societal nuances of the Global South.",
        "methodology": "### Methodology Summary\n\n**Proposed Method for Mining Gender-Biased Data in Hindi Context**\n\n1. **Dataset Exploration**:\n   - **CORGI-PM**: A Chinese corpus with 32.9K sentences annotated for gender bias. The methodology includes automated sentence extraction using gendered vocabulary, followed by manual annotation.\n   - **IndicCorp v2**: A collection of 20.9 billion tokens across 23 Indian languages and English, sourced from contemporary news articles, blogs, etc.\n   - **Kathbath**: A read speech corpus derived from IndicCorp, consisting of short sentences (8-15 words).\n   - **BUG**: An English dataset for gender bias in coreference and machine translation with 108K sentences labeled as stereotypical, anti-stereotypical, or neutral.\n   - **Social Media Data**: YouTube comments related to gender-biased/polarizing topics.\n   - **LOIT**: A dataset of Hindi and Telugu tweets.\n   - **FSB**: A GPT-generated English dataset annotated for gender bias scores.\n\n2. **Methodologies**:\n   - **LaBSE Model**: Used to mine biased sentences from a Hindi subset of IndicCorp v2. The steps involved:\n     - Embedding 10M sampled Hindi sentences using LaBSE.\n     - Querying these embeddings using biased English sentences from FSB.\n     - Collecting top 5 similar Hindi sentences per query and manually annotating a random sample for bias.\n   - **Translation Approach**: \n     - Translating 200 biased sentences from CORGI-PM and BUG to Hindi using Azure Translate, followed by manual annotation for relevance and retained bias.\n   - **Binary Classification**:\n     - Fine-tuning mBERT on the CORGI-PM dataset to classify sentences as \"biased\" or \"not biased\".\n     - Applying this classifier to YouTube comments and validating with manual annotations.\n   - **Regression Model**:\n     - Fine-tuning IndicBERT v2 with LoRA using FSB sentences, aiming for gender bias score prediction. A regression head predicts scores within [-1, 1] range.\n     - Evaluating on YouTube comments with manual annotation of top-scoring biased comments.\n   - **Keyword-based Filtering**:\n     - Creating a list of gendered Hindi adjectives and filtering sentences from Kathbath and LOIT datasets.\n     - Comparing effectiveness using mBERT and IndicBERT thresholds.\n   - **GPT-Guided Sentence Generation**:\n     - Using FSB's methods to generate potentially gender-biased English sentences, followed by manual validation and contextual adaptation for the Indian context.\n   \n3. **Challenges Addressed**:\n   - **Cultural and Linguistic Nuances**: Difficulty in translating and retaining the bias contextually relevant to Indian culture.\n   - **Data Sanitization in Sources**: IndicCorp v2's potentially sanitized content leading to low yield of biased sentences.\n   - **Cross-lingual Transfer Issues**: Performance drop in models fine-tuned on other languages/domains when applying to Hindi or social media content.\n   - **Social Media Data Constraints**: Prevalence of English, domain mismatches, and data accessibility issues.\n   - **Diversity in Generated Data**: Limited diversity and cultural nuance in GPT-generated sentences despite higher biased content yield.\n\n4. **Findings**:\n   - **Manual Annotations**: Only a small percentage of mined or translated sentences retained gender bias.\n   - **Classifier Accuracy**: Achieved 0.81 accuracy with mBERT on CORGI-PM; varied effectiveness on social media data.\n   - **Bias Retention in Translation**: Significant loss of gender bias and contextual relevance post-translation.\n   - **Performance Consistency**: Disparities in model accuracy attributable to domain mismatches and training data limitations.\n   - **Heuristics and Retrieval Limitations**: Low true positives demonstrating heuristic-based retrieval constraints in detecting gender bias.\n   \nIn summary, the methodology involved exploring multiple datasets, experimenting with translations, embeddings, binary classification, and regression models, coupled with data generation and manual validation to handle gender bias in Hindi language content, highlighting the complexities and limitations of cross-cultural and cross-linguistic NLP tasks.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n**Datasets:**\n- **IndicCorp v2 (Hindi subset)**: Used to analyze the abundance of biased sentences in existing large-scale corpora.\n- **CORGI-PM** and **BUG** datasets: Used to analyze the retention of gender bias upon translation to Hindi.\n- **YouTube comments**: 7340 comments extracted for analysis.\n- **FSB (Fairness Sentence Bank)**: Used for training a model to predict gender bias scores.\n\n**Baselines:**\n- **LaBSE model**: Used for mining biased sentences. LaBSE is a language-agnostic embedding model based on BERT.\n\n**Evaluation Metrics:**\n- **Accuracy**: For the binary classifier trained on CORGI-PM to classify sentences as \"biased\" or \"not biased\".\n- **Mean Squared Error (MSE)** and **Pearson correlation**: For the regression model to predict gender bias scores.\n\n**Main Experimental Results:**\n1. **Mining Biased Sentences**: \n   - From the 10M sampled sentences in IndicCorp v2, 500 potentially biased sentences were identified.\n   - Upon manual inspection, 20% of these sentences were found to be biased.\n\n2. **Translation Analysis**:\n   - 24% of the translated BUG sentences and 33% of the translated CORGI-PM sentences maintained gender bias in Hindi.\n\n3. **Binary Classifier Performance**:\n   - A binary classifier trained on CORGI-PM achieved an accuracy of 0.81 on the test set.\n   - This classifier was used to analyze YouTube comments, identifying 343 out of 7340 comments as biased. Upon further random sampling and inspection, 36% of the sampled comments were found to be biased.\n\n4. **Gender Bias Score Prediction**:\n   - An IndicBERT v2 model fine-tuned on FSB achieved an MSE of 0.057 and a Pearson correlation of 0.85.\n   - This model was used to score the YouTube comments, with a further sample of the top 100 gender bias-scored comments showing that 21% were biased.\n\nOverall, the experiments indicated notable occurrences of gender bias in Hindi texts and showed that some English biases persist in translation to Hindi. The models and approaches used were effective in identifying and quantifying gender bias to a significant degree."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To analyze the abundance of gender-biased sentences in existing large-scale corpora specific to the Hindi language.",
            "experiment_process": "They sampled 10M sentences from the Hindi subset of IndicCorp v2. To mine biased sentences, the LaBSE model was utilized with the top 100 biased sentences from FSB as queries. The embeddings of all sampled sentences were cached and queried using cosine similarity against the source English sentences from FSB. The top 500 similar sentences were examined, and 200 samples were manually classified by authors for bias.",
            "result_discussion": "They found that 20% of the sentences were biased. It was noted that a low yield could be due to sanitized content from the IndicCorp v2 source.",
            "ablation_id": "2405.06346v1.No1"
        },
        {
            "research_objective": "To determine if biased sentences in English and Chinese maintain their bias upon translation to Hindi and their relevance to the Indian context.",
            "experiment_process": "100 biased sentences each from CORGI-PM and BUG datasets were translated into Hindi using Azure Translate. One author annotated the 200 translated sentences for gender bias and context relevance.",
            "result_discussion": "24% of the translated BUG sentences and 33% of the translated CORGI-PM sentences maintained gender bias, showing contextual relevance for the Indian context topics.",
            "ablation_id": "2405.06346v1.No2"
        },
        {
            "research_objective": "To classify sentences from YouTube comments as biased or not biased using a fine-tuned model originally trained on non-social media data.",
            "experiment_process": "A binary classifier trained on the CORGI-PM dataset with mBERT was fine-tuned. This model was applied to 7340 YouTube comments collected using specific search queries. A random sample of the comments predicted as biased was manually labeled.",
            "result_discussion": "Only 36% of the 100 sampled YouTube comments were classified correctly as biased by the model. The poor performance is due to domain mismatch and the limitations of interpreting social media context.",
            "ablation_id": "2405.06346v1.No3"
        },
        {
            "research_objective": "To score gender bias using a regressive prediction model on YouTube comments extracted using the FSB scorer.",
            "experiment_process": "The IndicBERT v2 model was fine-tuned using English sentences from FSB with optimal hyperparameters. Gender bias scores for the 7340 YouTube comments were predicted and the top 100 were manually classified.",
            "result_discussion": "Only 21% of the top 100 comments were actually biased. There was a skew towards non-biased or neutral comments, revealing the difficulties in identifying bias from YouTube comments using this scoring model.",
            "ablation_id": "2405.06346v1.No4"
        },
        {
            "research_objective": "To retrieve biased statements from raw corpora using gendered adjectives as a heuristic.",
            "experiment_process": "A list of female-associated Hindi adjectives was compiled and applied to 10M Hindi tweets from LOIT and Hindi sentences from Kathbath. Sentences containing these adjectives were filtered using mBERT and IndicBERT thresholds, yielding 400 sentences that were manually inspected for bias.",
            "result_discussion": "Only 3% of the sentences retrieved with mBERT were biased, and none with the IndicBERT model. This demonstrates the ineffectiveness of heuristic-based adjective retrieval methods in identifying gender bias in raw corpora.",
            "ablation_id": "2405.06346v1.No5"
        }
    ]
}