{
    "title": "Towards Reliable Latent Knowledge Estimation in LLMs: In-Context Learning vs. Prompting Based Factual Knowledge Extraction",
    "abstract": "We propose an approach for estimating the latent knowledge embedded inside large language models (LLMs).\nWe leverage the in-context learning (ICL) abilities of LLMs to estimate the extent to which an LLM knows the facts stored in a knowledge base.\nOur knowledge estimator avoids reliability concerns with previous prompting-based methods, is both conceptually simpler and easier to apply, and we demonstrate that it can surface more of the latent knowledge embedded in LLMs.\nWe also investigate how different design choices affect the performance of ICL-based knowledge estimation.\nUsing the proposed estimator, we perform a large-scale evaluation of the factual knowledge of a variety of open source LLMs, like OPT, Pythia, Llama(2), Mistral, Gemma, etc. over a large set of relations and facts from the Wikidata knowledge base.\nWe observe differences in the factual knowledge between different model families and models of different sizes, that some relations are consistently better known than others but that models differ in the precise facts they know, and differences in the knowledge of base models and their finetuned counterparts.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Conversational chatbots (e.g., OpenAI\u2019s ChatGPT) built around large language models (e.g., OpenAI\u2019s GPT) are increasingly being used for a variety of information retrieval tasks such as searching for information or seeking recommendations related to real world entities like people or places (Wu et al., 2023  ###reference_b29###; Zhu et al., 2023  ###reference_b36###). A worrisome concern in such scenarios is the factual correctness of information generated by the LLMs (Peng et al., 2023  ###reference_b20###; Hu et al., 2023a  ###reference_b10###; Snyder et al., 2023  ###reference_b23###; Yao et al., 2023  ###reference_b30###; Ji et al., 2023  ###reference_b12###; Zhang et al., 2023  ###reference_b34###; Wang et al., 2023  ###reference_b27###).\nThe latent knowledge estimation problem: To avoid making false assertions about a real-world entity, an LLM first needs to have factual (true) knowledge about the entity. Given a prompt like \u201cEinstein was born in the year\u201d, LLMs may generate both the correct answer (\u201c1879\u201d) and wrong answers (e.g., \u201c1878\u201d or \u201c1880\u201d) with some probabilities. If an LLM knows the fact, one can hope that the probability with which it would generate the correct answer would be much higher than the wrong answers Jiang et al. (2021  ###reference_b13###).\nAs LLMs are typically pretrained over a Web corpus (including Wikipedia data) with millions of facts about real-world entities, they have the opportunity to learn factual knowledge about our world and latently embed the knowledge in their parameters. But, how can we estimate the extent to which LLMs have knowledge of real-world facts?\nReliability of latent knowledge estimates:\nPrior works\nJiang et al. (2020  ###reference_b14###); Bouraoui et al. (2020  ###reference_b3###)\nfollowed Petroni et al. (2019  ###reference_b21###),\nand represented factual knowledge\nin the form of triplets , where the subject  has a relation of type  with the object  (e.g., ). The central challenge of latent knowledge estimation is to infer  given  and  by only using information extracted from the LLM. Typically, the inference relies on probing the LLM with prompts constructed using  and  and analyzing the responses.\nCurrent approaches\nhave few well-defined rules to avoid prompt engineering and prompt hacking,\nraising serious concerns about the reliability of their estimates.\nAgainst this background, in this paper, we make four primary contributions:\n1. A simple yet reliable latent knowledge estimator (LKE) leveraging in-context learning (ICL):\nWe propose a latent knowledge estimator (LKE) that leverages in-context learning (ICL), called IC-LKE, in a simple yet clever way to avoid the many reliability concerns with prompting based previous knowledge estimators.\n2. Exploring the nuances of using ICL for knowledge estimation:\nWe investigate the impact of different ICL design choices on the estimation of latent knowledge, such as the number of in-context examples, when some of the examples are unknown to the model or simply incorrect, as well as the sequence in which they appear. While we focus on knowledge estimation, our findings can inform the application of ICL in other contexts.\n3. A comparison of IC-LKE with previous approaches:\nWe empirically demonstrate that IC-LKE outperforms previous knowledge estimation approaches that rely on human-generated or machine-mined prompts across a variety of different open-source models and different types of factual relations.\nIn contrast to prompting based methods, which are relation-specific and LLM-specific, IC-LKE\u2019s design is straightforward to apply.\n4. A systematic comparison of latent knowledge of open source LLMs at scale:\nWe use IC-LKE to evaluate the knowledge of 49 open-source LLMs spanning many families such as Llama(2), Gemma, Mistral, OPT, Pythia, etc. across a wide range of sizes, both with and without instruction-finetuning over 50 different relations and 20,000 facts from Wikidata.\nWe find that models from some families such as Llama2, Mistral and Gemma and larger models know more facts than others,\nthat models within the same family differ in the specific facts they know, despite being trained on the same data,\nand that fine-tuning reduces the amount of factual knowledge that can be extracted from the models.\nResearchers have proposed several approaches to estimate latent knowledge from LLMs, which can be categorized into two ways: (i) Model-internals based approaches leverage the LLM attention map Wang et al. (2020  ###reference_b26###), activation function Burns et al. (2022  ###reference_b5###), or model parameters Kazemnejad et al. (2023  ###reference_b15###) to decide whether factual information can be extracted from the LLM.\nIn our study, we rely on the probability distribution of generated tokens in an LLM \u2013 thereby our method belongs to the model-responses based approach.\n(ii) Model-responses based approaches \u2013 generally applicable to a wide range of LLM models \u2013 often propose different prompting techniques to nudge the LLM to validate whether a target fact is stored in it Chern et al. (2023  ###reference_b6###); Sun et al. (2023  ###reference_b24###); Wang et al. (2020  ###reference_b26###); Petroni et al. (2019  ###reference_b21###); Jiang et al. (2021  ###reference_b13###); Newman et al. (2022  ###reference_b18###); Jiang et al. (2020  ###reference_b14###). Prompt-based methods differ subtly by the choice of prompts and evaluation criteria. Besides, the prompts are often brittle Zamfirescu-Pereira et al. (2023  ###reference_b32###); Arora et al. (2023  ###reference_b1###); Sclar et al. (2023  ###reference_b22###) \u2013 their success depends on the hypothesis that the LLM indeed understands the prompts. In our study, we instead seek a minimal understanding of prompts by an LLM and design a knowledge estimation method based on the in-context learning. As a test bed Elsahar et al. (2018  ###reference_b7###); Hu et al. (2023b  ###reference_b11###); Sun et al. (2023  ###reference_b24###); Petroni et al. (2019  ###reference_b21###); Zhu and Li (2023  ###reference_b37###); Kry\u015bci\u0144ski et al. (2019  ###reference_b16###), we consider facts from existing knowledge graphs for performing knowledge estimation of LLMs."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Designing Reliable LKEs",
            "text": "Today, there exist many general-purpose as well as domain-specific factual knowledge bases that contain a very large number (millions to billions) of facts. The facts can be encapsulated as triplets, represented as subject (), relation (), object (). These triplets offer a general way to represent factual knowledge about real-world entities in knowledge graphs or other structured knowledge bases. The goal of latent knowledge estimation is to infer what fraction of the facts are known to a LLM.\nWe call methods that estimate the amount of latent knowledge inside an LLM latent knowledge estimators (LKEs)."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Reliability concerns with existing LKEs",
            "text": "Existing approaches to estimating latent knowledge in LLMs use a variety of factual knowledge tests. Below, we identify several reliability concerns with current designs that motivate our new LKE design.\n1. LLM-specific restrictions on test topics: Many prior works Petroni et al. (2019  ###reference_b21###); Jiang et al. (2020  ###reference_b14###) limit the choice of facts that can be used in tests to those where the surface form of the objects () is represented by a single token by the LLM\u2019s tokenizer. As different LLMs use different tokenizers, this limitation prevents us from comparing the latent knowledge across different LLMs. Furthermore, only popular objects tend to be represented by a single token and so the resulting estimates are not representative of the LLM\u2019s knowledge of facts with multi-token object representations.\n2. Unrestricted choice of test prompts: Many past works have attempted to use test prompts without any restrictions, including both human-generated or machine-mined prompts\n (Jiang et al., 2020  ###reference_b14###; Zamfirescu-Pereira et al., 2023  ###reference_b32###; Arora et al., 2023  ###reference_b1###; Sclar et al., 2023  ###reference_b22###).\nThey typically intersperse the subject  and object  between additional relationship context-communicating tokens.\nSome analyze the performance of a variety of prompts and then pick the best-performing or use an ensemble of the best-performing prompts (Jiang et al., 2020  ###reference_b14###; Newman et al., 2022  ###reference_b18###; Fernando et al., 2023  ###reference_b8###).\nHowever, these approaches raise two important concerns: First, the generated prompts, particularly those that are machine-mined, may include tokens that can implicitly or explicitly introduce additional (side-channel) information that makes it easier to answer the question. As a specific example, in a prior work Jiang et al. (2020  ###reference_b14###), for the relation \u201cposition held\", the prompt \u201c has the position of \" performed worse than \u201c is elected \". But, note that the second prompt potentially introduces a side-channel: it implicitly rules out answer choices for unelected positions like Professor and favors elected positions like President. Second,\nselecting from an unbounded number of potential prompt choices raises concerns about the complexity of LKEs (the size of the set of all considered prompts) and the potential for over-fitting, which in turn brings the reliability of estimates into question.\n3. Reliance on LLMs\u2019 meta-linguistic judgments:\nPrior works used prompts Chern et al. (2023  ###reference_b6###); Sun et al. (2023  ###reference_b24###); Wang et al. (2020  ###reference_b26###); Petroni et al. (2019  ###reference_b21###); Jiang et al. (2021  ###reference_b13###); Newman et al. (2022  ###reference_b18###); Jiang et al. (2020  ###reference_b14###) for communicating the question as well as the expected format of answers. But, the scores (estimates) resulting from such prompt-based testing conflate an LLM\u2019s latent knowledge of the facts with the LLM\u2019s meta-linguistic judgments, i.e., the LLM\u2019s ability to comprehend the prompt, understand the question embedded within the prompt and output the answer in some expected format (Hu and Levy, 2023  ###reference_b9###).\nThe impact on meta-linguistic judgments can be seen from the fact that multiple semantically-equivalent prompts result in different responses from an LLM and thereby, different estimates of latent knowledge (Hu and Levy, 2023  ###reference_b9###).\nMotivated from the above, we derive the following three design principles for LKEs. A reliable LKE design should:\nDP1: generate estimates for any factual topic and tokenization scheme.\nDP2: limit arbitrary prompt engineering to minimize over-fitting & side-channels.\nDP3: minimize reliance on meta-linguistic prompts."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "A new In Context learning based LKE (IC-LKE)",
            "text": "Our goal is to estimate whether an LLM knows a fact.\nThe challenge is to probe the LLM and evaluate its responses in a way compatible with the design principles set in Section 2.1  ###reference_###.\nKey idea: Leverage using a single fixed prompt for all queries.\nLLMs have shown to exhibit In-Context Learning (ICL) abilities (Brown et al., 2020  ###reference_b4###) that allow them to infer and extrapolate patterns in their inputs.\nWe leverage this ability to communicate information about relation without additional instructions to the LLM (DP3) by providing it with a list of facts based on .\nAssume that we want to probe for whether an LLM knows the fact  Einstein, birth-year, 1879.\nWe can use other facts for the birth-year relation such as  Feynman, birth-year, 1918  Heisenberg, birth-year, 1901 to construct an input \u201cFeynman 1918 Heisenberg 1901 Einstein\u201d.\nBy providing in-context examples to the model, we communicate the relation between subjects and objects.\nTo correctly extrapolate the pattern, the model needs to retrieve Einstein\u2019s birth-year as the completion of the sequence.\nMore formally, given a training dataset of facts  for relation , as well as a test fact , we leverage ICL to construct prompts that elicit information about  as\nWe use  to pick facts from  and concatenate the tokens corresponding to the subjects and objects, but do not include any other information about  (DP2).\nWe use space \u201c \u201d as the separator token and discuss this choice in more detail in Section 4.1  ###reference_###.\nWe discuss other design choices for IC-LKE construction in Section 3  ###reference_###.\nWhen further details are not needed, we simply refer to some input as .\nEvaluating model outputs.\nWe evaluate the output of model  for input  based on the probabilities  assigns to the tokens of the corresponding object .\nTo allow for objects  consisting of multiple tokens and to be independent of the specific tokenization scheme (DP1), we compute the object probability over multiple tokens as follows:\nwhere  denotes the number of tokens in  and  is the conditional probability of predicting the -th token  of  given the preceding tokens , and .\nMultiple-choice testing.\nTo determine whether model  knows a fact , we test whether given input ,  can choose the correct object  from among a set of  unique alternatives.\nSpecifically, given fact , we derive a test instance called choice , where  is a set of  plausible but incorrect alternatives.\nWe discuss the choice of  in Section 4  ###reference_###.\ndenotes the prediction of  for choice .\nThe predicted object has the maximal object probability within .\nEvaluation Metric.\nWe evaluate the factual knowledge of model  over a dataset of choices  using multiple choice accuracy:\nwhere  is the indicator function.\nThe IC-LKE design satisfies the knowledge estimation design principles.\nThe IC-LKE design proposed here satisfies the design principles from Section 2.1  ###reference_###, since\nDP1: its relative probability comparisons between different answer-options make it applicable to arbitrary types of facts.\nDP2: it uses the same, minimal prompt design based on ICL across all relations.\nDP3: its only requirement is that the LLM is able to use ICL, no further assumptions about any metalinguistic abilities are made."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Exploring the design space of IC-LKE",
            "text": "###figure_1### ###figure_2### ###figure_3### ###figure_4### ###figure_5### ###figure_6### By design, IC-LKE avoids many limitations of prior works. However, IC-LKE introduces a few design choices for the input, i.e.,  in Equation (1  ###reference_###).\nOne must decide the right , the number of in-context examples included in . Further, it is unclear how IC-LKE would be impacted when some of the chosen examples are unknown to the model or are incorrect.\nWe study both these factors in detail by varying  and introducing unknown or incorrect examples within these  examples. These experiments allows us to better understand the number of in-context examples needed and how robust IC-LKE is to several types of noise in these in-context examples.\nWe perform an in-depth empirical analysis on a Nobel Laureate dataset for the relation \u2018birth year\u2019 (details in A.1  ###reference_###). The dataset consists of facts formatted as .\nMore knowledgeable models need fewer in-context examples, but a small number suffices for most models.\nIn Figure 1  ###reference_###, we report knowledge estimation accuracy (Eq. (5  ###reference_###)) for different LLMs evaluated on 900 test samples, with varying numbers of in-context examples () by randomly sampling from the training set using five random seeds.\nWith an increasing number of in-context examples, the mean accuracy increases while the standard deviation decreases in different LLMs, i.e., the models gradually converge to a stable performance.\nUsing dashed vertical lines, we report the minimum number of examples required by different LLMs to achieve  of the accuracy at  in-context examples.\nInterestingly, LLMs with higher estimation accuracy tend to require fewer in-context examples compared to those with lower accuracy.\nA potential explanation for this behavior is that in order to infer the relation , models need to comprehend the examples presented in the prompt. Therefore, less knowledgeable models need to see more examples in order to infer .\nTo further investigate which individual facts may be known or unknown to a model, we look at the generation probability of in-context objects in  correct subject ()-object () pairs using the Mistral-7B model, as shown in Figure 2a  ###reference_sf1###. Similar results for additional models are presented in Appendix E  ###reference_###. Note that here we are only looking at probabilities of the object () for in-context examples given previous  pairs in the input to understand which of these samples are known by the LLM.\nThe Mistral-7B model demonstrates a gradual increase in probability for generating correct objects as we go from left to right on the x-axis (note that for a point on the x-axis, points before it are in context, thus points on the right have more context to leverage) in Figure 2a  ###reference_sf1###, stabilizing at a mean probability of approximately 85%. We also see that some objects at later positions have a lower generation probability.\nThis suggests that the LLM may be less confident about its knowledge of the facts corresponding to them.\nWe can leverage the token generation probability as a signal of LLM\u2019s confidence when evaluating LKEs (see Appendix D  ###reference_###).\nModels are robust to unknown examples.\nNext, we investigate the robustness of estimates to occurrence of unknown examples. We insert unknown examples in two distinct ways: one where we randomly distribute the occurrence of unknown examples throughout , and another more extreme scenario where we replace a continuous block of examples with unknown ones.\nWe chose  out of the  examples and replaced them with unknown examples created using fictitious names and birth years 111generated via https://en.namefake.com/api  ###reference_en.namefake.com/api###.\nOur findings are shown in Figures 2b  ###reference_sf2### and 2c  ###reference_sf3### for random and continuous replacement respectively. Unknown examples are marked by red dots, examples immediately following unknown ones in cyan dots and the rest in blue dots. The unknown examples show generation probabilities close to zero, confirming the LLM\u2019s tendency to assign low probabilities to unknown data. However, interestingly, unknown examples minimally impact surrounding data in both settings.\nModels are vulnerable to incorrect examples. We investigate the impact of including incorrect examples in . Similar to the setup for unknown examples, we also insert 40 (out of 200) incorrect examples randomly (Figure 2d  ###reference_sf4###) and simultaneously (Figure 2e  ###reference_sf5###). In our experiments, these incorrect examples are created by altering the birth years of known Nobel laureates and are marked by red dots in the plots.\nIn contrast to inserting unknown examples, the LLM significantly struggles with incorrect examples. Injection of such examples detrimentally affects the LLM\u2019s performance in both settings. We highlight one randomly marked yellow star example in Figure 2a  ###reference_sf1###, Figure 2b  ###reference_sf2###, and Figure 2d  ###reference_sf4### to show how the presence of incorrect samples brings down the probability of surrounding points.\nSummary: LLMs can identify the relation pattern of subject-object pairs even with a small set of in-context examples in the prompt. LLMs are relatively robust to unknown examples, but their ability to recollect factual knowledge is vulnerable to incorrect examples, particularly when they appear in a continuous sequence.\nOur findings allude to the effectiveness of designing an IC-LKE, where we carefully place correct examples from a training dataset and proceed to estimate the latent knowledge of the LLM on examples from the test set.\nFurthermore, the findings also motivate us to design a more efficient in-context learning based LKE, called EIC-LKE, that can process multiple test examples simultaneously in a single prompt where training examples are placed preceding each test example, see more details in the Appendix F  ###reference_###."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments and Results",
            "text": "We present the empirical findings of IC-LKE (as well as the efficient version, EIC-LKE) on the knowledge-estimation task on 49 open-source (pre-trained and fine-tuned) LLMs across different LLM families and sizes.\nWe enlist models and their simplified names used in this paper in Appendix 6  ###reference_###, Table 6  ###reference_###, and provide a leader-board of models based on IC-LKE in Table LABEL:table:model_order.\nDataset:\nWe evaluate the knowledge of models on a large set of facts from the T-REx dataset222https://huggingface.co/datasets/relbert/t_rex  ###reference_rex### Elsahar et al. (2018  ###reference_b7###).\nWe selected relations from T-REx with at least 500 samples and linked to a minimum of 100 unique objects.\nThis filtering leads to 50 distinct relations spanning categories like birth dates, directorial roles, parental relationships, and educational lineage.\nThe resulting T-REx Multiple Choice (T-REx-MC) dataset comprises 5,000 training and 20,000 test facts. Appendix A  ###reference_### contains detailed information on the dataset and relations.\nChoosing the set  & its impact on test difficulty:\nFor each fact subject (), relation (), object (), we generate alternative objects  to create multiple choices.\nNote that the alternative objects in  are viable choices and cannot be easily eliminated.\nTherefore, for each fact  we select  from other facts in the dataset that share the same relationship .\nFor computational feasibility, we sample  alternative objects per fact, so that a random guess between  has a  probability of being correct."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "IC-LKE vs. prompt-based approaches",
            "text": "###figure_7### We compare the performance of IC-LKE and EIC-LKE with the existing prompt-based approaches  (Jiang et al., 2020  ###reference_b14###) and report two key takeaways.\nIC-LKE outperforms prompt-based approaches.\nWe randomly sample three human-generated prompts (HGP) and machine-mined prompts (MMP) from (Jiang et al., 2020  ###reference_b14###) for 12 common relations between T-REx-MC and (Jiang et al., 2020  ###reference_b14###). The HGPs and MMPs for all relations are in Appendix G  ###reference_###.\nIn Figure 3  ###reference_###, IC-LKE and EIC-LKE outperform HGP and MMP in terms of higher mean accuracy across different models and 12 relations.\nAlso, IC-LKE and EIC-LKE have lower standard deviation than HGP and MMP, indicating a higher consistency of IC-LKE and EIC-LKE on knowledge estimation tasks. In Appendix H.2  ###reference_###, we report relation specific results, where IC-LKE and EIC-LKE estimate higher factual knowledge than the existing works in most relations, thereby demonstrating the superiority of IC-LKE and EIC-LKE over existing methods.\n###figure_8### IC-LKE is a flexible and effective knowledge estimator. We adapt IC-LKE by replacing the separator \u2018[space]\u2019 with three separators from HGP and MMP each for the relation \u2018original broadcaster\u2019 and report estimation accuracy in Figure 4  ###reference_###. We can observe that \u2018[space]\u2019 token demonstrates an equivalent performance with semantically meaningful prompts via HGP and MMP. Therefore, adding relation specific separators has a limited impact on factual knowledge estimation, as long as the subject-object pairs are correctly presented. Furthermore, finding relation-specifc prompts often require hand-crafted efforts vs. an automatic in-context based approach like ours where (subject, object) pairs are used. Therefore, IC-LKE can potentially extend to any facts from knowledge graphs over any LLM while HGP and MMP requires additional supervision and relation-specific validation."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Evaluating Diverse Models and Relations",
            "text": "We investigate the performance of 35 pre-trained LLMs and 14 fine-tuned LLMs across 50 relations using the IC-LKE framework.\nOur analysis is designed to uncover nuanced insights into the knowledge levels and structures within these models. We will examine the results through two primary lenses: (1) the variations in knowledge across different model families, and (2) the influence of model size and fine-tuning within the same model family on their knowledge attributes."
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1 Comparing different LLMs families",
            "text": "###figure_9### Some model families are consistently more knowledgeable than the rest.\nWe sort the model families based on the performance of the model closest to 7B parameters 3337B parameters is a good reference point since all model families except GPT-NEO-X have models within a gap of  1B parameters: Mistral-7B, Gemma-7B, Llama-7B, Falcon-7B, MPT-7B, OPT-6.7B, GPT-J-6B, Pythia-6.9B, and Bloom-7.1B., and the models within each family based on average accuracy across 50 relations.\nFigure 5  ###reference_### shows that the Mistral, Llama2, Gemma, and Llama families have higher performance on most of the relations than Pythia, Bloom, and OPT, indicating their lower factual knowledge.\n###figure_10### Different model families align in their relative factual knowledge.\nWe investigate the correlations between each model pair\u2019s performance over 50 relations to assess the agreement in their knowledge levels of the 50 relations. We compute the average correlations within each model family (e.g. Llama2 7B, 13B, 70B) in Figure 6  ###reference_###. Despite differences in architecture and training datasets among model families, there is a significant consensus (correlation > 0.6, see Figure 14  ###reference_###) regarding the hierarchy of knowledge across various relations. We also compile the three best and worst-performing relations for each model in Table 9  ###reference_###, illustrating the consensus among all models."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2 Comparing within the same LLM family",
            "text": "Larger models embed more knowledge.\nWe show in Figure 5  ###reference_### that, within each model family, bigger models (e.g. Llama-65B) generally outperform their smaller counterparts (e.g. Llama-13B) in terms of accuracy with an exception in the OPT family.\nModels within the same family are typically pre-trained on the same datasets (Biderman et al., 2023  ###reference_b2###; Zhang et al., 2022  ###reference_b33###; Touvron et al., 2023  ###reference_b25###).\nThus, this observation suggests that, when trained on identical datasets, the larger models capture a broader set of facts.\nDespite being trained on the same data, models might remember different facts.\nFrom these results, however, it is not clear if the larger models are subsuming smaller models in their factual knowledge, i.e., are the larger models also correct on the facts that the smaller models are correct on?\nTo assess this, we compute the subsumption rate :\ni.e., the fraction of facts from  known by smaller model  that larger model  also knows.\nA subsumption rate of  1 indicates that all of the\nsmaller model\u2019s knowledge is also contained in the larger model.\nTo ensure a meaningful comparison across scales, we only consider models that were pre-trained using the same training data.\nTable 1  ###reference_### shows the average subsumption rate () between the largest and smallest models in a family, as well as the average accuracy, over all relations for different model families.\nInterestingly,  is relatively low (< 0.5) for OPT, Pythia and Bloom (i.e., the larger models know less than 50% of what the smaller models know) and only reaching up to 0.8 for Gemma, Llama and Llama-2. Therefore, even though models within each family are trained on the same datasets and generally agree on the relative knowledge of different relations (Figure 6  ###reference_###), there are differences in the knowledge of specific facts they retain from their training data.\n###figure_11### Fine-tuning reduces latent knowledge.\n\nFinally, we investigate the effects of chat-based fine-tuning on the factual knowledge of models. Base language models are often fine-tuned (using a mix of supervised and reinforcement learning Ouyang et al. (2022  ###reference_b19###)) to make them better at following instructions. While prior works have shown that this makes the models better at various benchmarks, it\u2019s unclear how such fine-tuning affects latent knowledge.\nFigure 7  ###reference_### illustrates the comparative accuracy of pre-trained models and their fine-tuned counterparts.\nIn almost all cases, the fine-tuned models obtain lower accuracy than their base versions.\nThis suggests that fine-tuning reduces the amount of extractable latent knowledge in the models. A similar observation was also made by Yu et al. (2024  ###reference_b31###). We observe a similar trend using EIC-LKE in Appendix H.6  ###reference_###, Figure 15  ###reference_###. Additional results on evaluating generated outputs (using 50 tokens) in Figure 16  ###reference_### reveal the same pattern. To further assess if the fine-tuned models are acquiring new knowledge, we compute the subsumption rate between pre-trained and fine-tuned versions (Table 10  ###reference_###). We find that most of the latent knowledge in fine-tuned models is already present in base models (high ), thus indicating, that fine-tuned models may not be obtaining additional knowledge."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Concluding Discussion",
            "text": "In this work, we investigate a new way to estimate latent factual knowledge from an LLM.\nUnlike prior approaches that use prompting, our method relies on in-context learning. Our method not only addresses many reliability concerns with prompting, but it also recollects (at time significantly) more factual knowledge than prompting.\nIn contrast to prompting, which requires relationship-specific and LLM-specific prompt engineering, our method can be applied with minimal effort to test factual knowledge of relations across a variety of structured knowledge bases and LLMs.\nThis ability enables us to compare the latent knowledge captured by many different families of open-source LLMs; we expect our results to be of interest to designers of these LLMs.\nFinally, to design our in-context learning based LKE, we explore the impact of the number and ordering of correct, incorrect, and unknown examples used as inputs; our findings may be of independent interest to developing a better understanding of in-context learning.\nA fundamental question posed by our and prior work on estimating latent knowledge in LLMs: What does it mean for an LLM to know a fact? Suppose we tried to infer if an LLM knows the capital of Germany using the input \"France Paris; Spain Madrid; Germany \" and suppose the answer were Berlin. What we have learnt is that the LLM knows that the relationship  between Germany and Berlin is similar to that between France and Paris or Spain and Madrid. What we have not learned is whether the LLM knows that the relation  is called \"capital\" in English or \"hauptstadt\" in German. The latter is revealed by prompts such as \"The capital of Germany is \".\nBut, such prompts don\u2019t reveal whether the LLM knows that what Berlin means to Germany is similar to what Paris means to France.\nIs one type of knowing facts better than other? It is difficult to answer in general. Neither type of knowing guarantees that the knowledge can be put to use in different contexts and tasks, such as when we ask the LLM where the parliament of Germany is located.\nNevertheless, one clear takeaway from our study is related to how factual knowledge is latently embedded in an LLM.\nWe show that more factual knowledge can be recollected using in-context learning, i.e., the representations of subjects and objects that share the same relationship, than by prompting with the name of their relationship."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "This study contributes to advancing our understanding of latent factual knowledge in LLMs through an innovative in-context learning approach.\nHowever, it is essential to acknowledge the inherent limitations of our work.\nWhile the use of in-context learning aims to mitigate the influence of prompt engineering and the reliability issues associated with previous prompting methods, it introduces its own biases based on the selection and formulation of in-context examples. We discus these in detail in Section 3  ###reference_###. For example, the choice of which examples to include, their order, and their factual accuracy can influence model responses, and thus these in-context examples must be carefully curated for reliable latent knowledge estimation. Additionally, our study\u2019s limitation in testing simple-format facts underlines a critical gap in assessing LLMs\u2019 complex reasoning abilities. The knowledge estimation framework employed predominantly hinges on the LLM\u2019s capacity to correctly recall or recognize factual information from a given set of triplets or structured prompts. This narrows the scope of evaluation to straightforward factual recall, thereby overlooking the models\u2019 capability to engage in more sophisticated cognitive processes such as reasoning, synthesis, and inference, which we leave as open avenues for future work."
        }
    ],
    "url": "http://arxiv.org/html/2404.12957v1",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "2.2",
            "3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.2.1",
            "4.2.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "4.1",
            "4.2",
            "4.2.1",
            "4.2.2"
        ]
    },
    "research_context": {
        "paper_id": "2404.12957v1",
        "paper_title": "Towards Reliable Latent Knowledge Estimation in LLMs: In-Context Learning vs. Prompting Based Factual Knowledge Extraction",
        "research_background": "**Paper's Motivation:**\nThe paper is motivated by the growing use of large language models (LLMs) in conversational chatbots for information retrieval tasks involving real-world entities. A significant concern in these applications is the factual correctness of information generated by the LLMs. Existing methods for estimating whether LLMs possess correct factual knowledge face reliability issues, prompting the need for new approaches.\n\n**Research Problem:**\nThe central research problem is to develop a reliable method for estimating the latent factual knowledge of LLMs. This involves determining the extent to which LLMs know real-world facts and addressing the reliability concerns of current prompting-based techniques used for latent knowledge estimation.\n\n**Relevant Prior Work:**\n1. **Latent Knowledge Representation:**\n   - Petroni et al. (2019) introduced representing factual knowledge as triplets (subject, relation, object) and probing LLMs to infer the missing element using various prompts.\n   - Jiang et al. (2020) and Bouraoui et al. (2020) followed this framework, representing factual knowledge in triplet form and probing LLMs accordingly.\n\n2. **Model-Internals Based Approaches:**\n   - Wang et al. (2020) utilized the LLM attention maps.\n   - Burns et al. (2022) focused on LLM activation functions.\n   - Kazemnejad et al. (2023) analyzed LLM model parameters.\n\n3. **Model-Responses Based Approaches:**\n   - Chern et al. (2023), Sun et al. (2023), Wang et al. (2020), Petroni et al. (2019), Jiang et al. (2021), Newman et al. (2022), and Jiang et al. (2020) proposed different prompting techniques to nudge LLMs to validate the factual knowledge stored.\n   - These studies focused on the choice of prompts and evaluation criteria but faced issues due to the brittleness of prompts, as LLMs might not always understand them correctly (Zamfirescu-Pereira et al., 2023; Arora et al., 2023; Sclar et al., 2023).\n\nThe current work proposes an alternative method leveraging in-context learning (ICL) to improve the reliability of latent knowledge estimation, extending beyond the limitations of prompt-based methods.",
        "methodology": "The proposed method aims to estimate whether a Language Learning Model (LLM) knows a specific fact. This methodology hinges on leveraging the model's In-Context Learning (ICL) abilities to infer and extrapolate patterns in its inputs, thereby communicating factual knowledge without needing additional instructions.\n\n### Key Components and Innovations\n\n1. **In-Context Learning (ICL):**\n   - The core idea is to use the LLM's ability to learn relationships between data points provided in context, called ICL. This technique allows the model to infer missing pieces of information by recognizing patterns within provided examples.\n   \n2. **Constructing the Input:**\n   - To determine if an LLM knows a fact such as \"Einstein, birth-year, 1879\":\n     - Provide related facts in the context like \"Feynman, birth-year, 1918\" and \"Heisenberg, birth-year, 1901\".\n     - Construct the input in the format \"Feynman 1918 Heisenberg 1901 Einstein\" for the model to complete with \"1879\".\n\n3. **Minimal Prompt Design:**\n   - The method relies on using a minimalistic, consistent prompt that concatenates tokens corresponding to subjects and their objects separated by spaces.\n   - No additional instructions about the relationship between subjects and objects are given, adhering to the design principle of minimal prompt modifications.\n\n4. **Evaluating Model Outputs:**\n   - The evaluation of the model's output is based on the probabilities assigned to each token of the correct answer.\n   - To account for multi-token objects and ensure independence from specific tokenization schemes, object probability is calculated over all tokens that constitute the object.\n\n5. **Multiple-Choice Testing:**\n   - This involves presenting the model with a set of plausible but incorrect alternatives.\n   - The model has to choose the correct fact from among these alternatives based on the objects' probabilities.\n\n6. **Evaluation Metric:**\n   - The factual knowledge of the model is evaluated using multiple-choice accuracy.\n   - The accuracy is determined by whether the model correctly identifies the true fact among a set of alternatives, aggregated over a given dataset.\n\n### Design Principles:\n\n- **DP1 (Arbitrary Fact Types):** The method's reliance on relative probability comparisons between different answer options makes it universally applicable to various fact types.\n- **DP2 (Minimal Prompts):** By utilizing a consistent, minimal prompt design based on ICL across different relations, the method simplifies the criteria for testing across different fact types.\n- **DP3 (No Metalinguistic Assumptions):** The method only assumes that the LLM can use ICL, with no additional requirements about its metalinguistic capabilities.\n\nThis methodology seamlessly integrates ICL abilities of LLMs, minimal yet effective prompt designs, and a robust evaluation mechanism to estimate the factual knowledge stored within language models.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n#### Datasets:\nThe main experiment evaluates the knowledge estimation capabilities of large language models (LLMs) using a dataset derived from the T-REx dataset (referenced as ###reference_rex###, Elsahar et al., 2018 ###reference_b7###). Specific criteria were applied to the T-REx dataset:\n- Relations must have at least 500 samples.\n- Relations must be linked to a minimum of 100 unique objects.\n\nThis filtering procedure produced a subset of the T-REx dataset consisting of:\n- 50 distinct relations covering categories such as birth dates, directorial roles, parental relationships, and educational lineage.\n- The resultant dataset (T-REx Multiple Choice or T-REx-MC) consists of 5,000 training facts and 20,000 test facts.\n\nFor each fact, alternative object choices are generated to ensure multiple viable options, raising the test's difficulty. Each fact's alternative objects are sampled from other facts sharing the same relation, with four alternatives per fact, resulting in a 20% random guess probability.\n\n#### Models and Baselines:\nThe main experiment assesses IC-LKE (In-Context Learning Knowledge Estimation) and its efficient version, EIC-LKE, across 49 open-source large language models that belong to various LLM families and sizes. The specific models and their simplified names are listed in the appendix (###reference_###, Table 6 ###reference_###).\n\n#### Evaluation Metrics:\nWhile the specific evaluation metrics are not explicitly stated in the provided text, it is implicit that the main experiment involved measuring the accuracy of models in selecting the correct object among multiple choices. Success rates in answering based on alternatives offered provide a straightforward metric, given the percentage chance of correct random guessing is 20%.\n\n### Main Experimental Results:\nThe results of the main experiment are presented in a leaderboard format, ranking the models based on their performance with IC-LKE. This leaderboard is available in Table LABEL:table:model_order.\n\nIn summary, the main experiment setup reflects a robust and comprehensive approach to evaluating factual knowledge extraction capabilities across various LLM models. The T-REx-MC dataset serves as a challenging and diversified testbed, while the performance results are clearly documented through a comparative leaderboard."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To understand the design choices affecting IC-LKE, particularly the impact of the number of in-context examples and the introduction of unknown or incorrect examples in the estimation process.",
            "experiment_process": "The study varied the number of in-context examples and introduced unknown or incorrect examples within these examples. The dataset consisted of 'birth year' facts for Nobel Laureates. Models were evaluated on 900 test samples with randomly sampled in-context examples from the training set across five seeds. The mean knowledge estimation accuracy and the standard deviation were observed as measures. The robustness was tested by inserting unknown and incorrect examples in different patterns within the in-context examples, and the token generation probabilities were analyzed to understand the models' confidence.",
            "result_discussion": "More knowledgeable models required fewer in-context examples for high accuracy, while less knowledgeable ones needed more examples. The models were robust to unknown examples but struggled significantly with incorrect ones. LLMs could identify the relation pattern of pairs even with a small set of in-context examples, suggesting that correct placement of examples could yield effective latent knowledge estimation. Incorrect examples impacted performance much more than unknown examples, prompting potential improvements in designing IC-LKE approaches by carefully placing training examples.",
            "ablation_id": "2404.12957v1.No1"
        },
        {
            "research_objective": "To compare the performance of IC-LKE and EIC-LKE with existing prompt-based approaches for factual knowledge extraction.",
            "experiment_process": "The study compared IC-LKE and EIC-LKE with human-generated prompts (HGP) and machine-mined prompts (MMP) from previous works for 12 relations between T-REx-MC and Jiang et al., 2020. Random samples of three HGPs and MMPs for each relation were used, with results shown in terms of mean accuracy and standard deviation across different models and relations. Additionally, IC-LKE was adapted by replacing the default separator '[space]' with other separators from HGP and MMP for the relation 'original broadcaster' to test its flexibility.",
            "result_discussion": "IC-LKE and EIC-LKE outperformed HGP and MMP in terms of higher mean accuracy and lower standard deviation, demonstrating higher consistency and superiority in knowledge estimation tasks. The results showed that IC-LKE's performance was not significantly impacted by the change of separators, indicating its flexibility and potential to be extended to various facts without requiring additional supervision.",
            "ablation_id": "2404.12957v1.No2"
        },
        {
            "research_objective": "To investigate the performance differences in factual knowledge estimation across different pre-trained and fine-tuned LLMs using the IC-LKE framework.",
            "experiment_process": "The study evaluated 35 pre-trained and 14 fine-tuned LLMs across 50 relations. The analysis focused on two primary aspects: variations in knowledge across different model families and the influence of model size and fine-tuning on the knowledge attributes within the same model family. The performance was analyzed based on average accuracy across relations and the correlation of knowledge levels between different models.",
            "result_discussion": "Certain model families like Mistral, Llama2, Gemma, and Llama were found to be more knowledgeable, with models showing higher accuracy in most relations. Larger models generally showed better performance than smaller ones within the same family, indicating that model size helps capture a broader set of facts. However, the subsumption rate analysis indicated that larger models did not fully subsume the factual knowledge of smaller models. Fine-tuning was found to reduce the extractable latent knowledge, as fine-tuned models had lower accuracy compared to their base versions, suggesting a potential trade-off between fine-tuning for specific tasks and retaining general factual knowledge.",
            "ablation_id": "2404.12957v1.No3"
        }
    ]
}