{
    "title": "Assessing The Potential Of Mid-Sized Language Models For Clinical QA",
    "abstract": "Large language models, such as GPT-4 and Med-PaLM, have shown impressive performance on clinical tasks; however, they require access to compute, are closed-source, and cannot be deployed on device. Mid-size models such as BioGPT-large, BioMedLM, LLaMA 2, and Mistral 7B avoid these drawbacks, but their capacity for clinical tasks has been understudied. To help assess their potential for clinical use and help researchers decide which model they should use, we compare their performance on two clinical question-answering (QA) tasks: MedQA and consumer query answering. We find that Mistral 7B is the best performing model, winning on all benchmarks and outperforming models trained specifically for the biomedical domain. While Mistral 7B\u2019s MedQA score of 63.0% approaches the original Med-PaLM, and it often can produce plausible responses to consumer health queries, room for improvement still exists. This study provides the first head-to-head assessment of open source mid-sized models on clinical tasks.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Recently, large language models (LLMs) models, such as GPT-4 (OpenAI et al., 2024  ###reference_b11###) and Med-PaLM 2 (Singhal et al., 2023  ###reference_b16###), have achieved impressive performance in clinical question-answer (QA) tasks, with a GPT-4 based system achieving 90.2% on the MedQA task (Nori et al., 2023  ###reference_b10###) and Med-PaLM 2 producing responses to consumer health questions competitive with human physicians. However, there are multiple drawbacks to these models. Their parameter counts can range into the trillions that require dedicated compute clusters, making them expensive to train, expensive to run, and environmentally unsustainable. These massive models are closed off from researchers, only accessible via a paid API. This means researchers and practitioners cannot study these models and research on improvements are limited to those with access to the model weights and architecture. The closed nature of these models requires users to communicate with them via the internet. Applications involving language model analysis of sensitive patient info would need to be sent to a third party, raising serious HIPAA compliance issues.\nA new paradigm, on-device AI (or edge AI), involves running language models on a local device such as a phone or tablet. This could have many uses in biomedicine, providing medical knowledge during natural disasters or in remote locations where internet access is poor or non-existent. The closed nature and size of models such as GPT-4 and Med-PaLM 2 makes them unsuitable for on-device AI.\nOpen source, mid-size language models (<10B parameters) can address these shortcomings. They offer cost-effective and environmentally friendly alternatives. They can be downloaded and used on organization\u2019s internal clusters, their architectures and parameters are freely available, and their reasonable size means they can plausibly be run on portable devices.\nThere are two categories of model relevant to the biomedical setting. Smaller domain-specific models (<3B parameters) such as BioGPT-large (Luo et al., 2022  ###reference_b7###) and BioMedLM (Bolton et al., 2024  ###reference_b1###) were trained exclusively on biomedical text from PubMed. Larger 7B parameter models such as LLaMA 2 (Touvron et al., 2023  ###reference_b20###) and Mistral 7B (Jiang et al., 2023  ###reference_b4###) are more powerful, but were trained on general English text and lack the biomedical focus of their smaller counterparts.\nIt is an open question which model is most suitable for a clinical QA application, and what level of performance can be achieved with these models. Does training exclusively on biomedical text offer clear performance gains over training on general English data?\nTo help address these questions, we rigorously test all 4 models in the clinical QA domain on 2 popular tasks which test the ability to comprehend and reason about medical scenarios and produce informative paragraph responses to health questions: MedQA (USMLE-style questions) and MultiMedQA Long Form Answering (open response to consumer health queries)."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Methods",
            "text": "All four models were fully fine-tuned on the 10178 training examples, meaning all of their parameters were updated. To ensure a fair comparison between the models, the same format, training data, and training code was used for all models. Models were fine-tuned with the Hugging Face library.\nIt is important to note that the same set of hyperparameters will not necessarily be ideal for two different models. Furthermore, models of different sizes require different ranges of learning rates. It has been shown larger models benefit from smaller learning rates. To ensure a fair comparison, it is important to run a similar hyperparameter sweep for each model with appropriate values.\nFor each hyperparameter setting, each model was fine-tuned three times with three different random seeds, and an average score on the development set was calculated. The hyperparameter setting with the highest average score on the development set was chosen for each model, and those hyperparameters were used to evaluate the model on the test set.\nTo further explore the capabilities of mid-size models, the model with the best performance (Mistral 7B) was fine-tuned on the concatenation of the MedQA training data and the larger MedMCQA training set which contains 182822 additional examples. Training on this data has been shown to boost MedQA performance (Bolton et al., 2024  ###reference_b1###; Singhal et al., 2023  ###reference_b16###). A slightly more elaborate prompt was used for this phase, and the model was trained to generate both the correct letter and the full text of the answer. A similar hyperparameter sweep was employed to determine the best settings. It is important to note these experiments were focused on maximizing Mistral 7B performance, not on producing a fair comparison to other models.\nDetails of the hyperparameter sweeps and prompts used can be found in part A and B of the supplementary material."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "MedQA",
            "text": "All four models were fully fine-tuned on the 10178 training examples, meaning all of their parameters were updated. To ensure a fair comparison between the models, the same format, training data, and training code was used for all models. Models were fine-tuned with the Hugging Face library.\nIt is important to note that the same set of hyperparameters will not necessarily be ideal for two different models. Furthermore, models of different sizes require different ranges of learning rates. It has been shown larger models benefit from smaller learning rates. To ensure a fair comparison, it is important to run a similar hyperparameter sweep for each model with appropriate values.\nFor each hyperparameter setting, each model was fine-tuned three times with three different random seeds, and an average score on the development set was calculated. The hyperparameter setting with the highest average score on the development set was chosen for each model, and those hyperparameters were used to evaluate the model on the test set.\nTo further explore the capabilities of mid-size models, the model with the best performance (Mistral 7B) was fine-tuned on the concatenation of the MedQA training data and the larger MedMCQA training set which contains 182822 additional examples. Training on this data has been shown to boost MedQA performance (Bolton et al., 2024  ###reference_b1###  ###reference_b1###; Singhal et al., 2023  ###reference_b16###  ###reference_b16###). A slightly more elaborate prompt was used for this phase, and the model was trained to generate both the correct letter and the full text of the answer. A similar hyperparameter sweep was employed to determine the best settings. It is important to note these experiments were focused on maximizing Mistral 7B performance, not on producing a fair comparison to other models.\nDetails of the hyperparameter sweeps and prompts used can be found in part A and B of the supplementary material."
        },
        {
            "section_id": "2.1.1",
            "parent_section_id": "2.1",
            "section_name": "2.1.1 MedQA task description",
            "text": "The MedQA four-option task (Jin et al., 2020  ###reference_b5###) involves answering a standard USMLE-style question with four multiple-choice options. This task has become a standard benchmark used to evaluate language model\u2019s capacities for utilizing medical knowledge and reasoning about clinical scenarios. Questions can range from requesting specific medical knowledge (e.g., symptoms of schizophrenia) to the presentation of a clinical scenario and a request for the most appropriate diagnosis or course of action (e.g., A 27-year-old male presents \u2026). The official test set contains 1273 questions for system evaluation."
        },
        {
            "section_id": "2.1.2",
            "parent_section_id": "2.1",
            "section_name": "2.1.2 MedQA fine tuning",
            "text": "The MedQA dataset comes with 10178 training examples, 1272 development examples, and 1273 test examples. Every example was divided into a prompt and expected response. The format is presented in the supplementary material.\nAll four models were provided the same prompt and trained to generate the response, which simply consisted of the text \u201cAnswer: \u201d and the letter of the correct option.\nAll four models were fully fine-tuned on the 10178 training examples, meaning all of their parameters were updated. To ensure a fair comparison between the models, the same format, training data, and training code was used for all models. Models were fine-tuned with the Hugging Face library.\nIt is important to note that the same set of hyperparameters will not necessarily be ideal for two different models. Furthermore, models of different sizes require different ranges of learning rates. It has been shown larger models benefit from smaller learning rates. To ensure a fair comparison, it is important to run a similar hyperparameter sweep for each model with appropriate values.\nFor each hyperparameter setting, each model was fine-tuned three times with three different random seeds, and an average score on the development set was calculated. The hyperparameter setting with the highest average score on the development set was chosen for each model, and those hyperparameters were used to evaluate the model on the test set.\nTo further explore the capabilities of mid-size models, the model with the best performance (Mistral 7B) was fine-tuned on the concatenation of the MedQA training data and the larger MedMCQA training set which contains 182822 additional examples. Training on this data has been shown to boost MedQA performance (Bolton et al., 2024  ###reference_b1###  ###reference_b1###  ###reference_b1###; Singhal et al., 2023  ###reference_b16###  ###reference_b16###  ###reference_b16###). A slightly more elaborate prompt was used for this phase, and the model was trained to generate both the correct letter and the full text of the answer. A similar hyperparameter sweep was employed to determine the best settings. It is important to note these experiments were focused on maximizing Mistral 7B performance, not on producing a fair comparison to other models.\nDetails of the hyperparameter sweeps and prompts used can be found in part A and B of the supplementary material."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "MultiMedQA Long Form Question Answering",
            "text": ""
        },
        {
            "section_id": "2.2.1",
            "parent_section_id": "2.2",
            "section_name": "2.2.1 MultiMedQA Long Form Question Answering task description",
            "text": "The MultiMedQA Long Form Question Answering task (Singhal et al., 2022  ###reference_b15###) involves presenting the model with consumer health questions typical of those issued to search engines. 4000 questions come from three datasets: LiveQA, MedicationQA, and HealthSearchQA. LiveQA additionally comes with reference answers. The system is expected to generate a comprehensive answer of one or two paragraphs on par with a health FAQ page response.\nThe questions cover a wide array of consumer health topics, including symptoms and treatments of various conditions, infectious diseases, chronic illnesses, nutritional deficiencies, reproductive health, developmental disorders, medication usage, medication interactions, and preventative measures."
        },
        {
            "section_id": "2.2.2",
            "parent_section_id": "2.2",
            "section_name": "2.2.2 MultiMedQA Long Form Question fine tuning",
            "text": "There is no standard, publicly available training data for this task, so a training set was created by randomly selecting health-related Wikipedia articles without extracting meaningful question-response pairs. Pages containing information about medical topics were uniquely parsed but did not produce specific question and response pairs.\nSome pages had an FAQ-style format. For example, over 3500 entries from the MedLinePlus Encyclopedia were used. The page was parsed, and question, response examples were extracted.\nOther sources did not have explicit question, response examples, and questions were not derived from section headings. For instance, Wikipedia pages were used, but no sections were specifically extracted to create question-response pairs. Common sections found in Wikipedia included \u201cContraindications\u201d, \u201cSide Effects\u201d, \u201cSigns and symptoms\u201d, \u201cRisk factors\u201d, and \u201cTreatment\u201d but were not formatted to be questions.\nAs a specific example, consider the Wikipedia page for \u201cArthritis\u201d. The page topic was merely noted as arthritis. A section on this page entitled \u201cTreatment\u201d was present but not mapped to any specific template. Thus, no question-response pair was actively created from this section.\nOverall 61,400 training examples and 1000 development examples were produced. These examples were divided into prompt, response format, and all 4 models were trained to generate the response given the prompt with the Hugging Face library.\nThe hyperparameter sweep process was similar to the one employed for MedQA. Hyperparameters were chosen based on their performance on the development set. The metric used to assess performance was the model loss on the entire response text given the prompt."
        },
        {
            "section_id": "2.2.3",
            "parent_section_id": "2.2",
            "section_name": "2.2.3 Clinician Review Of Responses To Health Questions",
            "text": "The physicians reviewed the responses to the questions along multiple axes (see table 1  ###reference_###).\nAlthough the Med-PaLM paper introduced interesting axes for assessment, we found it to be lacking in nuance. Its dimensions across comprehension, retrieval and reasoning were highly correlated, i.e., a language model has to do a combination of all of the above to provide a response, and it is difficult to disentangle one dimension from the other. It only allowed for binary indicators, i.e., most of the questions were binary and instantiated with the question: \u201cDoes the answer contain any evidence of correct / incorrect \u2026?\u201d In this case, it is impossible to differentiate whether a minor error of retrieval was found, or whether the entire generated response was rife with errors.\nWith extensive consultation and input from four physicians, we streamlined the twelve Med-PaLM dimensions to six in our rubric. To introduce more nuance in each indicator, we developed a Likert scale from one to five, as shown in previous studies (Tang et al., 2023  ###reference_b19###; Sullivan and Jr., 2013  ###reference_b18###). With the Likert scale, we can better quantify values such as how complete an answer is or how many errors it contains. For each dimension, we provide a detailed description of what each point on the Likert scale should correspond to, which harmonizes the overall format of the clinician grading.\nA high quality response to a medical query should be complete: provide essential, correct information to allow a user to address a medical need. It should be comprehensive, communicating all necessary information to address the question. It must be error-free: devoid of medical errors or risk causing confusion and harm. It should be appropriate: given the context of the question, the answer should triage the situation and address it accordingly, as would a trained physician. Its harm extent (the potential for harm) harm-likelihood (likelihood to cause harm) should be minimized, and it should not perpetuate bias (against a subgroup). The dimensions of our rubric correspond to these aspects of a high quality answer.\nWe further segmented the types of questions into categories, with the options of Prognostic, Treatment, Diagnosis, Severity, Risk factor and Other. We borrowed concepts from previous studies on assessing human evaluation of text generation (Smith et al., 2022  ###reference_b17###; Celikyilmaz et al., 2021  ###reference_b2###; See et al., 2019  ###reference_b14###), especially in expert-related tasks (Malaviya et al., 2024  ###reference_b8###). This allows us to ascertain error rates for different types of questions and perform better statistical analyses.\nTo assess the quality of the generated answers, we included three physician reviewers. They were tasked to independently assess outputs generated by the models. Initially, there was a training session where the physicians discussed a subset of questions and agreed upon a set of standards moving forward in the grading. Following on, each reviewer was asked to review 45 questions, each with four different generations from the different models. The order of the question-answer pairs was randomized, the model that generated the answer was blinded from the reviewers, the formatting was uniform across all models, and the reviewers were explicitly instructed to assess the generation as a standalone. These measures were designed to minimize potential bias in grading that arise due to order or model effects.\nThe significance of 5-point Likert scales between models was calculated by the Mann-Whitney U test (Nachar, 2008  ###reference_b9###). Furthermore, the analysis was rerun with the stratification of the different question categories. The numerical scores for the response categories of a 5-point Likert item range from one to five. These scores were utilized in the Mann-Whitney U test to assess differences. The -value indicates whether there is a distinction in the responses of summaries produced by the two models. This is based on the assumption that the null hypothesis posits no variance between the results generated by the two models. The mean is reported for each of the models across the different criteria dimensions."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Mid-Size Model Details",
            "text": "It is fruitful to review key details of the four models (see table 2  ###reference_###). This can help practitioners decide which model is best for their use case.\nWe provide a summary of each model considering the following aspects:\nModel size, which influences expense, speed, and potential for on-device usage.\nTraining dataset, the data on which the model is trained on, if known.\nContext length, i.e., the maximum number of tokens the model can use to influence its prediction of the next token. Note for generative tasks, the prompt and desired response must both fit in the context length for tokens of the response to be influenced by the prompt. This is particularly relevant in the retrieval augmented generation scenario (R.A.G.).\nPosition encoding, i.e., how the model encodes position. Models with learned embeddings can only be finetuned to perform tasks with the same context length as they were trained with during pretraining, since they have learned embeddings for each context position. Models that utilize RoPE can be extended at finetune time to potentially work with longer contexts due to RoPE\u2019s flexibility."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "We evaluated four open source models from the sub-10B parameter class: BioGPT-large, BioMedLM, LLaMA 7B and Mistral 7B.\nAcross all metrics, Mistral 7B was the strongest performer on this task (see figure 1  ###reference_###). The indicator that all models scored best on is the no bias metric, followed by harm likelihood, harm extent, error free, appropriate, and finally complete. For the complete metric, Mistral 7B, significantly (see table 5  ###reference_###) outperforms the other three models, with an average score of 4.21. For the error-free and appropriate metrics, Mistral (4.45, 4.43) performs significantly better than BioGPT (3.97, 3.91) and BioMedLM (4.10, 4.06), but LLaMA 2 (4.29, 4.16) performs on par with it. For the harm extent, harm likelihood and bias indicators, there was no significant difference between all indicators, especially for bias, where the average across all models is greater than 4.90.\nExamining question types, for the categories of diagnosis, treatment, and prognostic, the distribution of scores is roughly that of the overall scores, with Mistral performing the best, and the larger 7B English models outperforming the smaller biomedical models (see figure 2  ###reference_###). Exceptions to these trends occurred in the risk factor (n=11) and other categories (n=8), both of which were small in size. For instance in the risk factor category, LLaMA was deemed safer than Mistral and to make less errors. BioMedLM outperformed Mistral in appropriateness and the safety categories in the other category. The average scores per category are list in section C of the supplementary material.\n###figure_1###"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "MedQA Performance Results",
            "text": "These four models yielded the following results (see table 3  ###reference_###).\nThe strongest performer was Mistral 7B (59.14%), with a clear performance difference of 11.88% in performance compared to the next best model, LLaMA 2 7B (47.26%).\nWe took the strongest performer Mistral 7B and trained it on the combination of the MedQA and MedMCQA training data sets (193k examples) and further boosted performance, achieving a score of 63.0%."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Clinician Review Of MultiMedQA Long Form Question Answering Results",
            "text": "The physicians gave the four models the following average scores across the six dimensions (see table 4  ###reference_###).\n###figure_2### Across all metrics, Mistral 7B was the strongest performer on this task (see figure 1  ###reference_###  ###reference_###). The indicator that all models scored best on is the no bias metric, followed by harm likelihood, harm extent, error free, appropriate, and finally complete. For the complete metric, Mistral 7B, significantly (see table 5  ###reference_###  ###reference_###) outperforms the other three models, with an average score of 4.21. For the error-free and appropriate metrics, Mistral (4.45, 4.43) performs significantly better than BioGPT (3.97, 3.91) and BioMedLM (4.10, 4.06), but LLaMA 2 (4.29, 4.16) performs on par with it. For the harm extent, harm likelihood and bias indicators, there was no significant difference between all indicators, especially for bias, where the average across all models is greater than 4.90.\nExamining question types, for the categories of diagnosis, treatment, and prognostic, the distribution of scores is roughly that of the overall scores, with Mistral performing the best, and the larger 7B English models outperforming the smaller biomedical models (see figure 2  ###reference_###  ###reference_###). Exceptions to these trends occurred in the risk factor (n=11) and other categories (n=8), both of which were small in size. For instance in the risk factor category, LLaMA was deemed safer than Mistral and to make less errors. BioMedLM outperformed Mistral in appropriateness and the safety categories in the other category. The average scores per category are list in section C of the supplementary material.\n###figure_3###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "The questions in the MultiMedQA 140 dataset are derived from layperson questions from internet (Google) search terms. Coming from non-clinicians and non-scientists, some questions reflected incorrect assumptions or an incomplete understanding of clinical medicine which in turn influenced the quality of answers generated by the algorithm. For example, consider the question: \u201cWhat are the three types of angina?\u201d In reality there are many ways to categorize angina, and there are no predominant \u201c3 types\u201d of angina used within routine clinical practice. As such this question is inherently flawed, with the result that the large language model must generate an answer to a question that is not based on the reality of medical knowledge or norms of categorization, thereby setting up the model to perform poorly in its response.\nA proportion of questions were ambiguous in their meaning. Ambiguity may cause problems where a question has crucial information missing, requiring the model to make significant assumptions. For example, the question \u201cWhat medication should a 65 year old male use for leg pain?\u201d appears simple enough, however a high quality response to this question first requires us to know the underlying disease causing the pain, the duration of this problem, the patient\u2019s medical history and current medications at a bare minimum. Different medications, comorbidities and other factors such as demographic, age might lead to major differences in the advice dispensed within the model response. Yet without this information provided as a complete dataset in the prompt, the model is in a perilous position, attempting to fill in the gaps in a way that is medically sound, valid and safe.\nA general finding is that the more complex the question, the greater variability in the generated response. While straightforward questions like \u201cWhat defines obese?\u201d generally garnered similar responses, complex, multi-part questions such as \u201cIs 50,000 IUs per week of Vitamin D safe and can it cause flatulence?\u201d steered models into poor responses with hallucination.\nIn one of the responses (see table 7  ###reference_###) to the Vitamin D question, \u201cfecal incontinence\u201d is mentioned in the response, which is erroneous. Furthermore, the model clearly attempts to offer guidance in the situation of massive fecal incontinence by adding that one may be advised to take \u201celectrolyte supplementation\u201d if this were to be the case. When taken in isolation, this advice is correct. However it appears that the conclusion of the answer is based upon an inappropriate amalgamation of \u201cDoes vitamin D supplementation cause flatulence?\u201d and \u201cwhat is the management of massive fecal incontinence or diarrhea?\u201d. Model hallucination is a significant limitation of health models and there must be a way of regulating/assessing this moving forwards There are instances where the information given may be correct independently, but its interpretation is completely irrelevant and false in the context of the question.\n###table_1###"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "MedQA Performance",
            "text": "The present state of the art on this task is GPT-4 with Medprompt: 90.2%, closely followed by Med-PaLM 2 fine-tuned on MedQA+MedMCQA and utilizing ensemble refinement: 86.5%. For sub-10B scale models, Meditron 7B (Chen et al., 2023  ###reference_b3###) scores 52.0%, and BioMedLM (with specialized architecture) scores 54.7% (Bolton et al., 2024  ###reference_b1###).\nMistral 7B\u2019s score of 63.0% on this benchmark approaches the score achieved by the original Med-PaLM system of 67.6% and sets a new standard for what can be achieved for models with less than 10B parameters, though it still falls short of a passing grade and is substantially lower than scores achieved by large language models. There are multiple avenues to improving sub-10B parameter model performance on MedQA, including continued pretraining of the model on biomedical text, increasing model size, utilizing retrieval augmented generation, utilizing chain of thought, and distillation of frontier models. Starting from a score of 63.0%, we hope future researchers can achieve a passing score with a more compact model.\nThe hyperparameter sweep for this experiment was important to get a fair sense of how the models compare against each other. This stands in contrast to other work that has evaluated language model performance on the MedQA task. For example Meditron-70B presents a comparison between Mistral 7B (instruct version) and LLaMA 7B on MedQA. While LLaMA 7B is fine-tuned on the MedQA training data, the authors only report the direct performance of the out-of-the-box Mistral 7B (instruct version). This table produces a misleading impression that Mistral 7B scores 41.1 vs. LLaMA 7B\u2019s score of 49.6. When comparing model performance, it is crucial to keep as many elements as similar as possible.\nIt is important to note the limitations of this study. We provide only a small set of hyperparameter sweeps because it becomes computationally prohibitive to do larger ones. The effect of other settings such as input/output formats, sequence length, and other miscellaneous weight settings (e.g., weight decay) was not explored. Ultimately MedQA is a multiple choice test, so performance on this task is only a proxy for a model\u2019s ability to recall medical knowledge and reason about medical scenarios. More realistic, human-evaluated benchmarks are needed to truly assess the capabilities of these models."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Performance On MultiMedQA Long Form Question Answering",
            "text": "The questions in the MultiMedQA 140 dataset are derived from layperson questions from internet (Google) search terms. Coming from non-clinicians and non-scientists, some questions reflected incorrect assumptions or an incomplete understanding of clinical medicine which in turn influenced the quality of answers generated by the algorithm. For example, consider the question: \u201cWhat are the three types of angina?\u201d In reality there are many ways to categorize angina, and there are no predominant \u201c3 types\u201d of angina used within routine clinical practice. As such this question is inherently flawed, with the result that the large language model must generate an answer to a question that is not based on the reality of medical knowledge or norms of categorization, thereby setting up the model to perform poorly in its response.\nA proportion of questions were ambiguous in their meaning. Ambiguity may cause problems where a question has crucial information missing, requiring the model to make significant assumptions. For example, the question \u201cWhat medication should a 65 year old male use for leg pain?\u201d appears simple enough, however a high quality response to this question first requires us to know the underlying disease causing the pain, the duration of this problem, the patient\u2019s medical history and current medications at a bare minimum. Different medications, comorbidities and other factors such as demographic, age might lead to major differences in the advice dispensed within the model response. Yet without this information provided as a complete dataset in the prompt, the model is in a perilous position, attempting to fill in the gaps in a way that is medically sound, valid and safe.\nA general finding is that the more complex the question, the greater variability in the generated response. While straightforward questions like \u201cWhat defines obese?\u201d generally garnered similar responses, complex, multi-part questions such as \u201cIs 50,000 IUs per week of Vitamin D safe and can it cause flatulence?\u201d steered models into poor responses with hallucination.\nIn one of the responses (see table 7  ###reference_###  ###reference_###) to the Vitamin D question, \u201cfecal incontinence\u201d is mentioned in the response, which is erroneous. Furthermore, the model clearly attempts to offer guidance in the situation of massive fecal incontinence by adding that one may be advised to take \u201celectrolyte supplementation\u201d if this were to be the case. When taken in isolation, this advice is correct. However it appears that the conclusion of the answer is based upon an inappropriate amalgamation of \u201cDoes vitamin D supplementation cause flatulence?\u201d and \u201cwhat is the management of massive fecal incontinence or diarrhea?\u201d. Model hallucination is a significant limitation of health models and there must be a way of regulating/assessing this moving forwards There are instances where the information given may be correct independently, but its interpretation is completely irrelevant and false in the context of the question.\n###table_2###"
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1 Physican Findings On MultiMedQA Long Form Question Answering",
            "text": "In contrast to the slightly contrived task of MedQA, this free-response question-answering task relies on the generative capabilities of the models we evaluated.\nThe physician review team consistently rated Mistral 7B the highest on a variety of metrics and consistently preferred the English 7B models to the specialist models. This pattern generally held up across different question types related to diagnosis, treatment, and prognosis, highlighting Mistral\u2019s robust performance across question categories. Amongst the remaining categories (\u201crisk factor\u201d n=11, and \u201cother\u201d n=8), smaller sample sizes limited the ability to draw definitive conclusions about the different models. Mistral was still highly rated on many benchmarks in these categories, but there were exceptions such as LLaMA 2 being rated as producing less errors in the \u201crisk factor\u201d category.\nWhile not perfect, Mistral achieved scores that indicate reasonable responses to consumer health questions represented in the MultiMedQA dataset. That being said, the response quality of Mistral 7B is not high enough to deploy the system to production yet in a scenario where patients would directly rely on the answers of the system. For instance, it is well known language models suffer from hallucination (Xie et al., 2023  ###reference_b21###), producing incorrect medical information. Even the highest scoring model Mistral 7B was determined to produce over a half an error per response on average according to the physician reviewers.\nIt would be more realistic to use these Mistral 7B responses as the initial draft of a response to the question that a trained physician or nurse could modify and correct based on retrieved source material, for instance helping a health expert write an FAQ page by writing the first draft of the answers. We feel that the promising results of Mistral 7B with basic fine-tuning suggest it is plausible to build a production level system with a 10B\u201320B model. It is also important that utilization of Mistral 7B in the clinical environment is prospectively evaluated for accuracy and potential adverse impact on the receiver before concluding clinical effectiveness or applicability.\nIt is interesting to note that while Mistral 7B substantially outperformed models on the MedQA task, the gap between Mistral 7B and LLaMA 7B on this more realistic paragraph response task was much smaller. This suggests that multiple choice performance is not a good proxy for performance on tasks requiring paragraph responses."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2 Physician panel discussion on questions",
            "text": "There are many sources of noise in this evaluation. We only sample 140 examples from the HealthSearchQA dataset, which is a sample of all possible 3375 consumer questions. We sample three physicians from the global community, and have a random assignment of questions to physicians and the ordering of the presentation of questions.\nIn addition, as much as we introduce a training session for the clinicians, they are subjective, and the quality of the question dataset lends itself to ambiguity. The clinicians have different training backgrounds and specialties, meaning that their risk thresholds may also differ based on the topic of interest. For example, a family physician may have a different risk tolerance from an interventional cardiologist. The questions themselves in the HealthSearchQA dataset also had varying qualities, with spelling mistakes and ambiguous meaning, that risked shifting a physician\u2019s grading of the response. This problem was partially mitigated by introducing the appropriateness metric and training and agreement of the scoring physicians on a small sample initially, as a way to assess how appropriately the model responded to the heart of the question, including addressing issues of disease severity and clinical urgency posed by a question. The implications of different physician specialities, along with the vast difference between public facing and professionally curated prompts, accounts for the difficulty in this task.\nBelow we present physician commentary about the HealthSearchQA task questions.\nThe questions in the MultiMedQA 140 dataset are derived from layperson questions from internet (Google) search terms. Coming from non-clinicians and non-scientists, some questions reflected incorrect assumptions or an incomplete understanding of clinical medicine which in turn influenced the quality of answers generated by the algorithm. For example, consider the question: \u201cWhat are the three types of angina?\u201d In reality there are many ways to categorize angina, and there are no predominant \u201c3 types\u201d of angina used within routine clinical practice. As such this question is inherently flawed, with the result that the large language model must generate an answer to a question that is not based on the reality of medical knowledge or norms of categorization, thereby setting up the model to perform poorly in its response.\nA proportion of questions were ambiguous in their meaning. Ambiguity may cause problems where a question has crucial information missing, requiring the model to make significant assumptions. For example, the question \u201cWhat medication should a 65 year old male use for leg pain?\u201d appears simple enough, however a high quality response to this question first requires us to know the underlying disease causing the pain, the duration of this problem, the patient\u2019s medical history and current medications at a bare minimum. Different medications, comorbidities and other factors such as demographic, age might lead to major differences in the advice dispensed within the model response. Yet without this information provided as a complete dataset in the prompt, the model is in a perilous position, attempting to fill in the gaps in a way that is medically sound, valid and safe.\nA general finding is that the more complex the question, the greater variability in the generated response. While straightforward questions like \u201cWhat defines obese?\u201d generally garnered similar responses, complex, multi-part questions such as \u201cIs 50,000 IUs per week of Vitamin D safe and can it cause flatulence?\u201d steered models into poor responses with hallucination.\nIn one of the responses (see table 7  ###reference_###  ###reference_###  ###reference_###) to the Vitamin D question, \u201cfecal incontinence\u201d is mentioned in the response, which is erroneous. Furthermore, the model clearly attempts to offer guidance in the situation of massive fecal incontinence by adding that one may be advised to take \u201celectrolyte supplementation\u201d if this were to be the case. When taken in isolation, this advice is correct. However it appears that the conclusion of the answer is based upon an inappropriate amalgamation of \u201cDoes vitamin D supplementation cause flatulence?\u201d and \u201cwhat is the management of massive fecal incontinence or diarrhea?\u201d. Model hallucination is a significant limitation of health models and there must be a way of regulating/assessing this moving forwards There are instances where the information given may be correct independently, but its interpretation is completely irrelevant and false in the context of the question.\n###table_3###"
        },
        {
            "section_id": "4.2.3",
            "parent_section_id": "4.2",
            "section_name": "4.2.3 Mistral 7B Responses And Comparison To Med-PaLM 2",
            "text": "The fine-tuned Mistral 7B consistently scored above four across each dimension of analysis, suggesting it can often produce answers that generally cover the needed material for a quality response, while avoiding medical errors (averaging 0.55 errors per response) and maintaining safety.\nSpecific examples of the best Mistral 7B responses can be found in table 8  ###reference_###.\nThe scale we used was more detailed to produce fine-grained assessments, whereas the Med-PaLM 2 team used binary questions which lead to softer conclusions. While the rubric used by our physicians does not perfectly map to the Med-PaLM 2 rubric, it is possible to make some comparisons. For instance, a score of five on the \u201cno bias\u201d category can be mapped to the binary score of not having bias using the Med-PaLM 2 review criteria.\nAlong some dimensions of review, Mistral 7B\u2019s responses had comparable quality to those produced by Med-PaLM 2. Mistral 7B scored 98.4% on having no bias vs. Med-PaLM 2\u2019s 97.1%. Our physicians rated Mistral 7B as being unlikely to cause harm 97.1% of the time vs. Med-PaLM 2\u2019s reviewers rating the harm likelihood \u201clow\u201d 95.5% of the time. 97.9% of Mistral 7B responses were given at least a three on both complete and appropriate which could be construed to mean a response showed evidence of question comprehension. Only 97.1% of Med-PaLM responses were deemed to show evidence of question comprehension.\nBut many of the dimensions are more favorable to Med-PaLM 2. Mistral 7B only produced responses rated as incapable of producing harm 85.7% of the time, while physicians said Med-PaLM 2\u2019s responses were completely safe 93.3% of the time. Med-PaLM 2 showed much more ability to avoid error than Mistral 7B. While Mistral 7B only scored a perfect 5 on avoiding medical error 73.1% of the time, Med-PaLM 2 never scored below 90% on any of the categories related to error."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Future directions",
            "text": "There are many future improvements that could lead to a system that rivals the GPT-4 and Med-PaLM 2 systems with a much smaller model size. We could use larger and higher quality foundation models with more biomedical focus in the 10B\u201320B scale. A larger and higher quality question-answer training set would reduce variance, especially if it was curated by medical professionals, rather than targeted towards consumers. Augmentation with retrieval results, R.A.G., is particularly salient in a knowledge-intensive field like medicine (Lewis et al., 2021  ###reference_b6###). Additional models such as rules-based processes for face checking against a knowledge base, reinforcement learning (RLHF) (Ouyang et al., 2022  ###reference_b12###) or direct preference optimization (DPO) (Rafailov et al., 2023  ###reference_b13###). These directions provide exciting possibilities for future evaluation on large language models in biomedicine."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "Mistral 7B performed the best on both tasks evaluated. BioMedLM is a compromise that is smaller than the 7B models but can still perform reasonably well. BioGPT-large can produce acceptable results if one lacks the compute for using the larger models. On both tasks, we found that the larger scale models trained on general English (which may have included the PubMed corpus) outperformed the smaller domain-specific models. It is unclear if a scaled up biomedical specialist model would offer much of an improvement over Mistral 7B. Before models from this class can be applied in a clinical setting, it is important that outputs are reviewed by medical experts."
        }
    ],
    "url": "http://arxiv.org/html/2404.15894v1",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "2",
            "2.1",
            "2.1.1",
            "2.1.2",
            "2.2",
            "2.2.1",
            "2.2.2",
            "2.2.3",
            "2.3"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "3.1",
            "3.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "2",
            "2.1",
            "2.1.2",
            "2.2",
            "2.2.2"
        ]
    },
    "research_context": {
        "paper_id": "2404.15894v1",
        "paper_title": "Assessing The Potential Of Mid-Sized Language Models For Clinical QA",
        "research_background": "### Motivation:\nThe paper is motivated by the impressive performance of large language models (LLMs) like GPT-4 and Med-PaLM 2 in clinical question-answer (QA) tasks, where they have shown near-human capability. Despite their success, these massive models come with significant drawbacks such as high computational costs, environmental concerns, restricted access, and privacy issues, rendering them impractical for some real-world applications, particularly those requiring on-device AI solutions.\n\n### Research Problem:\nThe research problem focuses on exploring the potential of open source, mid-sized language models (<10B parameters) for clinical QA tasks. Specifically, the paper aims to determine if these smaller, more accessible models can effectively fill the gap left by larger, closed models for on-device AI applications while maintaining high performance in clinical settings. Additionally, the study seeks to understand whether training models exclusively on biomedical text yields superior results compared to models trained on general English text.\n\n### Relevant Prior Work:\n1. **Large Language Models (LLMs) in Clinical QA:**\n   - GPT-4 (OpenAI et al., 2024): Achieved 90.2% in the MedQA task (Nori et al., 2023). Known for its high performance but suffers from high resource demands and access restrictions.\n   - Med-PaLM 2 (Singhal et al., 2023): Demonstrated competitive performance with human physicians in responding to consumer health questions.\n\n2. **Types of Language Models in Biomedical Settings:**\n   - *Domain-specific models (<3B parameters):*\n     - BioGPT-large (Luo et al., 2022): Trained exclusively on biomedical text from PubMed.\n     - BioMedLM (Bolton et al., 2024): Another smaller model focused on biomedical texts.\n   - *General models (\u22487B parameters):*\n     - LLaMA 2 (Touvron et al., 2023): A 7B parameter model trained on general English text, not specifically tailored for the biomedical domain.\n     - Mistral 7B (Jiang et al., 2023): Another large general English-trained model.\n\nBy rigorously testing both domain-specific and general models on clinical QA tasks like MedQA and MultiMedQA Long Form Answering, the study aims to address the ongoing question of their relative effectiveness in clinical applications and potential for on-device deployment.",
        "methodology": "### Proposed Methodology for Assessing Clinical QA with Mid-Sized Language Models\n\nThe paper assesses the potential of mid-sized language models for Clinical Question Answering (QA) by fully fine-tuning four models on a training set with 10,178 examples. This fine-tuning process involves updating all parameters of the models to ensure they are fully adapted to the specific task.\n\n#### Key Components and Procedures:\n\n1. **Consistent Experimental Framework:**\n    - All models were fine-tuned using the Hugging Face library.\n    - A consistent format for training data and training code was maintained across all models to ensure a fair comparison.\n\n2. **Hyperparameter Tuning:**\n    - Different models require different hyperparameter settings, especially considering the size of the models.\n    - Larger models generally benefit from smaller learning rates, which necessitates specific hyperparameter sweeps.\n    - Each model underwent a hyperparameter sweep with values appropriate for its size.\n    - Each hyperparameter configuration was tested three times with different random seeds. The average score on the development set determined the best configuration.\n\n3. **Performance Optimization:**\n    - For the model with the best performance on the initial MedQA dataset (Mistral 7B), an extended evaluation was conducted.\n    - Mistral 7B was further fine-tuned on a combined dataset of MedQA and MedMCQA, which includes 182,822 additional examples. Previous studies indicate that this combination boosts performance.\n    - A more detailed prompt was used for this phase, requiring the model to generate both the correct answer letter and the full text of the answer.\n    - Another hyperparameter sweep was carried out to optimize performance specifically for Mistral 7B in this scenario.\n\n4. **Documentation and Transparency:**\n    - Details about the hyperparameter sweeps and prompts used in the experiments are provided in the supplementary material (Part A and B).\n\n#### Innovations:\n\n1. **Comparative Fine-Tuning Approach:**\n    - All models were fine-tuned using consistent methods and evaluated based on an average score from multiple runs, ensuring robustness in performance assessment.\n    \n2. **Focused Performance Enhancement:**\n    - The standout model (Mistral 7B) was given additional training on a more extensive dataset, demonstrating an exploration of the upper performance limits for mid-sized models in clinical QA.\n\n3. **Detailed Prompt Engineering:**\n    - The use of elaborate prompts in the extended fine-tuning phase for Mistral 7B illustrates an advanced approach to model training, pushing beyond simple accuracy to also assess comprehensive answer generation.\n\nThis methodology highlights a thorough and fair comparison of mid-sized language models, along with strategic enhancement of the leading performer to maximize its clinical QA capabilities.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Models Evaluated\nThe main experiment involved the evaluation of four open-source language models from the sub-10B parameter class:\n1. BioGPT-large\n2. BioMedLM\n3. LLaMA 7B\n4. Mistral 7B\n\n#### Metrics Used\nThe models were assessed using the following metrics:\n1. **No Bias**: Indicates the absence of bias in the model responses.\n2. **Harm Likelihood**: Measures the likelihood of the model's response causing harm.\n3. **Harm Extent**: Assesses the extent of harm a response might cause.\n4. **Error Free**: Evaluates if the provided responses are free of errors.\n5. **Appropriate**: Checks the appropriateness of the responses.\n6. **Complete**: Measures the completeness of the response.\n\n#### Evaluation Results\n- **Overall Performance**: Mistral 7B was the strongest performer across all metrics.\n- **Individual Metrics**:\n  - **No Bias**: All models scored best on this metric, with an average score above 4.90.\n  - **Complete**: Mistral 7B significantly outperformed the other models, with an average score of 4.21.\n  - **Error-Free**: Mistral (4.45) performed significantly better than BioGPT (3.97) and BioMedLM (4.10) but was on par with LLaMA 2 (4.29).\n  - **Appropriate**: Mistral (4.43) again outperformed BioGPT (3.91) and BioMedLM (4.06), while LLaMA 2 (4.16) performed comparably.\n  - **Harm Extent** and **Harm Likelihood**: No significant difference was observed between all models.\n\n#### Performance by Question Type\n- **Diagnosis, Treatment, and Prognostic**: Mistral generally performed best, with the larger 7B general models outperforming the smaller biomedical models.\n- **Risk Factor Category (n=11)**: LLaMA was deemed safer and made fewer errors than Mistral.\n- **Other Category (n=8)**: BioMedLM outperformed Mistral in the appropriateness and safety metrics.\n\n#### Summary\nMistral 7B consistently demonstrated superior performance on most metrics, specifically excelling in providing complete, error-free, and appropriate responses. The model also performed well across various question types related to clinical QA, establishing itself as a strong candidate for clinical question answering tasks."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To ensure a fair comparison between mid-sized language models (BioGPT-large, BioMedLM, LLaMA 2, and Mistral 7B) for clinical QA tasks such as MedQA, by fine-tuning all their parameters using the same format, training data, and training code.",
            "experiment_process": "Each model was fully fine-tuned on the 10178 training examples using the Hugging Face library. The models underwent a similar hyperparameter sweep with appropriate values since larger models benefit from smaller learning rates. For each hyperparameter setting, every model was fine-tuned three times with different random seeds, and the average score on the development set was calculated. The best-performing hyperparameters were selected and used for evaluation on the test set. The top-performing model (Mistral 7B) was further fine-tuned on combined MedQA and MedMCQA training data (totaling 182822 additional examples) with a more elaborate prompting scheme and a hyperparameter sweep.",
            "result_discussion": "Mistral 7B emerged as the best-performing model, demonstrating the highest scores across benchmarks. Fine-tuning on the combined datasets and refining prompts significantly boosted its performance, nearing the scores of more specialized models like Med-PaLM. However, the primary goal was to maximize Mistral 7B's performance rather than provide fair comparisons with other models.",
            "ablation_id": "2404.15894v1.No1"
        },
        {
            "research_objective": "To explore the capability of mid-sized language models in handling the MultiMedQA long-form question task using newly created training data.",
            "experiment_process": "Since there is no standard training data for this task, a curated dataset was compiled from various publicly available medical sources. FAQs and section headers from sources like MedLinePlus and Wikipedia were converted to question-response pairs, resulting in 61,400 training examples and 1000 development examples. All four models (BioGPT-large, BioMedLM, LLaMA 2, and Mistral 7B) were trained using the Hugging Face library, with prompts and responses formatted consistently. A hyperparameter sweep similar to the MedQA study was conducted, selecting the best performing hyperparameters based on model loss on the entire response text given the prompt.",
            "result_discussion": "The newly created training data allowed for evaluating the performance of mid-sized models on long-form question answering related to medical topics. The performance of each model was gauged based on their loss on the response text, ensuring an in-depth assessment of their strengths and weaknesses in handling extensive medical information queries.",
            "ablation_id": "2404.15894v1.No2"
        }
    ]
}