{
    "title": "Can\u2019t say cant? Measuring and Reasoning of Dark Jargons in Large Language Models",
    "abstract": "Ensuring the resilience of Large Language Models (LLMs) against malicious exploitation is paramount, with recent focus on mitigating offensive responses. Yet, the understanding of cant or dark jargon remains unexplored. This paper introduces a domain-specific Cant dataset and CantCounter evaluation framework, employing Fine-Tuning, Co-Tuning, Data-Diffusion, and Data-Analysis stages. Experiments reveal LLMs, including ChatGPT, are susceptible to cant bypassing filters, with varying recognition accuracy influenced by question types, setups, and prompt clues. Updated models exhibit higher acceptance rates for cant queries. Moreover, LLM reactions differ across domains, e.g., reluctance to engage in racism versus LGBT topics. These findings underscore LLMs\u2019 understanding of cant and reflect training data characteristics and vendor approaches to sensitive topics. Additionally, we assess LLMs\u2019 ability to demonstrate reasoning capabilities. Access to our datasets and code is available at https://github.com/cistineup/CantCounter.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large Language Models (LLMs), exemplified by ChatGPT[1  ###reference_b1###], redefine information acquisition, communication, and problem-solving[2  ###reference_b2###]. These models are trained on extensive datasets or fine-tuned from pre-existing models, necessitating vast amounts of data. However, LLMs also pose security and ethical concerns as attackers can exploit their generative capabilities for malicious purposes [3  ###reference_b3###]. Such misuse encompasses disinformation dissemination [4  ###reference_b4###], AI-driven crime [5  ###reference_b5###], privacy breaches [6  ###reference_b6###], and social engineering [7  ###reference_b7###]. Despite efforts by regulators like OpenAI to implement content filters [8  ###reference_b8###], there remains a risk of attackers disguising malicious content using \u201ccant\u201d or \u201cdark jargon\u201d - concealed language elements requiring deeper comprehension [9  ###reference_b9###]. LLMs excel in understanding and generating natural language responses, fostering user trust. While research evaluates their efficacy in providing accurate responses [10  ###reference_b10###], little attention has been paid to LLMs\u2019 interaction with cant in specific domains. Prior studies often lack depth in understanding the intricacies of cant [11  ###reference_b11###], especially its varied representations in domains like politics and drugs. In this paper, we investigate LLMs\u2019 ability to recognize and reason about cant, particularly in domains prone to offensive content like politics and drugs. Despite progress in filtering harmful content, attackers can still exploit cant to evade detection. Understanding LLMs\u2019 response to cant in specific domains is essential for addressing emerging security challenges. Additionally, we assess LLMs\u2019 ability to demonstrate reasoning capabilities.\nResearch Questions. To address the above issues, in this paper, we evaluate the reasoning abilities of current LLMs involving cant or dark jargon from the following four perspectives:\nRQ1: Do different types of questions help LLM understand the cant?\nRQ2: Do different question setups and prompt clues help LLM understand cant?\nRQ3: Do different LLMs have the same understanding of the same cant?\nRQ4: How well does LLM understand cant in different domains?\nCantCounter: Addressing past shortcomings[11  ###reference_b11###], CantCounter is a system crafted to evaluate LLM\u2019s grasp of cant within specific domains. We compile Cant and Scene datasets from various sources to form adversarial texts. These datasets fine-tune the GPT-2 model and generate Scene fragments for assessing LLM comprehension. Co-Tuning methods align the Cant dataset and Scene fragments, while Data-Diffusion techniques augment and refine adversarial text. Employing Type, Sample learning, and Clue approaches enrich our experiments. Finally, Data-Analysis methods systematically evaluate 1.67 million data points. CantCounter is locally deployable and adaptable to any open-world dialogue system. Its replication has both advantages and drawbacks, aiding attackers in bypassing LLM classifiers while facilitating safety filter development. We define \u201centities\u201d as distinct objects or concepts and \u201cscenes\u201d as related events in specific environments.\nEthical Considerations: CantCounter draws from public datasets such as Reddit [12  ###reference_b12###] and 4chan [13  ###reference_b13###], avoiding direct user interaction. However, its misuse poses risks, despite its benefits in addressing LLM\u2019s challenges. Despite these potential risks, we believe that the benefits of CantCounter far outweigh the risks. LLM has become a hot topic [14  ###reference_b14###], and we need to fully recognize the potential problems of LLM and promote its safer development and application. We caution that this paper may contain sensitive content, including drug and violence-related examples, which could cause discomfort. Comprehensive data is available upon request. We have only open sourced part of the dataset.\nContributions. This paper introduces three key contributions:\nWe present the Cant and Scene datasets, addressing data scarcity in domains like drugs, weapons, and racism, laying groundwork for future large language model assessment.\nCantCounter, our framework, assesses large language models\u2019 understanding of domain-specific cants through four stages: Fine-Tuning for scene fragment generation, Co-Tuning for cross-matching, Data-Diffusion for text expansion, and Data-Analysis for simplifying complex calculations.\nOur evaluation of CantCounter reveals its efficacy in bypassing security filters of mainstream dialogue LLMs, providing insights into LLM reasoning within specific domains and guiding future research."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Large Language Model Security Issues",
            "text": "ChatGPT, developed by OpenAI in November 2022 [1  ###reference_b1###], has undergone upgrades and fine-tuning [15  ###reference_b15###] to prevent harmful content generation. However, users can still provoke negative responses by using specific prompts [16  ###reference_b16###]. Researchers are investigating security risks, including the generation of toxic outputs from benign inputs [17  ###reference_b17###]. Recent studies have shown that attackers can bypass detection by encrypting inputs with methods like Caesar ciphers and exploiting language nuances [18  ###reference_b18###]. This paper proposes a Q&A query approach to evaluate LLMs\u2019 reasoning abilities in handling such content."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Cant",
            "text": "Cant, a specialized language used by social groups for secrecy [19  ###reference_b19###], varies in names like argot [20  ###reference_b20###], slang [21  ###reference_b21###], and secret language across history. While LLMs excel in traditional cant analysis, understanding criminal cant poses challenges. Criminal groups use innocuous terms to hide illegal activities, necessitating mastery for law enforcement [22  ###reference_b22###]. Our study explores cant in politics, drugs, racism, weapons, and LGBT issues. These cants share ambiguity, indirect messaging, and potential for social harm. Political cant conveys biases, drug cant evades regulation, racism cant reinforces biases, weapons cant enables illegal dealings, and LGBT cant discriminates. Mastering these cants is vital for addressing societal and security concerns."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Question Answering (Q&A) Task",
            "text": "Dialogue systems fall into task-oriented and non-task-oriented categories. Task-oriented systems serve specific purposes like reservations, while non-task-oriented systems engage in free conversation. Examples include ChatGPT, Bard, ERNIE, and Claude, offering services in entertainment, social interaction, and information retrieval [23  ###reference_b23###].Question-answering (Q&A) tasks in NLP evaluate language processing capabilities [24  ###reference_b24###], including reading comprehension and logical reasoning. Q&A formats include abstractive, Yes/No, and Multiple-Choice, each requiring specific evaluation metrics [25  ###reference_b25###]. We employ Zero-shot/One-shot learning for testing."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "CantCounter",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "High-level Idea",
            "text": "We observe that the responses generated by LLMs vary with different cants, allowing adversaries to bypass filters or security restrictions. Thus, understanding how LLMs react to different cants is very important. However, exhaustively trying different cants queries with different scenes across numerous domains to find those capable of bypassing LLM restrictions and generating harmful outputs would be time-consuming and impractical. Therefore, we investigate whether adversaries can independently combine different cants and scenes to generate context that is reasonable and coherent, bypassing LLM filters or restrictions. To this end, we introduce CantCounter, the first evaluation (attack) framework targeting open-world dialogue systems (LLM)."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Threat Model",
            "text": "We adopt a threat model similar to \u201cWhy so toxic\u201d [17  ###reference_b17###], targeting deployed dialogue LLMs like ChatGPT. Firstly, the adversary requires scene data different from the target LLM\u2019s training data. Secondly, they interact with the LLM, combining cants and scenarios to extract detectable cants. Finally, they access the victim LLM via CantCounter in a black-box manner, querying it through an API-like interface."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Dataset",
            "text": "In our study, we extensively gathered cant related to five domains: politics, drugs, racism, weapons, and LGBT. The cant, comprising common and less common usages, holds practical meanings in real life. This Cant dataset forms a robust basis for evaluating the veracity and reliability of LLMs across specific domains. These five areas were chosen to address pressing societal issues impacting fundamental values such as social justice and human rights. Exploration of politics, drugs, racism, weapons, and homosexuality enables LLMs to tackle real-world challenges effectively. While other domains like hacking and fraud are significant, we focused on these due to data availability and processing feasibility, leaving room for future research on sensitive topics.\n###figure_1### In constructing the Cant dataset (Figure 2  ###reference_### \\scriptsize{2}\u20dd), we crawled or manually screened multiple sources, including government agency websites [26  ###reference_b26###], online forums like Reddit [12  ###reference_b12###], 4chan [13  ###reference_b13###], and X [27  ###reference_b27###], publicly available datasets from Kaggle [28  ###reference_b28###] and Hugging Face [29  ###reference_b29###], dark web, and public compilations of cant. Multi-source data encompasses various text types closely related to specific domains. CantCounter utilizes information networks [30  ###reference_b30###] to address redundancy challenges between cants, capturing their interdependency.\nThe Cant dataset covers five domains, totaling 1,778 cants across 187 entities. We randomly selected 53 entities, totaling 692 cants, ensuring even representation across domains and prevalence in the open world. Selected entities and cants were cross-validated with authoritative sources [31  ###reference_b31###, 32  ###reference_b32###, 33  ###reference_b33###, 34  ###reference_b34###, 35  ###reference_b35###] to ensure wide presence and reflection in publicly accessible information sources. Criteria like content relevance and topic specificity guided information selection and filtering, aiming for transparency and consistency. The resulting high-quality data forms the Scene dataset, laying the groundwork for subsequent simulation scene generation models.\nDuring information selection and filtering (Figure 2  ###reference_### \\scriptsize{1}\u20dd), explicit criteria were used to judge relevance and adherence to study definitions. Decisions were reached through participatory discussion to mitigate subjectivity and ensure alignment with research objectives. This rigorous process yields a refined dataset for accurate and relevant analysis."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Pipeline",
            "text": "The CantCounter pipeline (Figure 2  ###reference_###) consists of four stages: Fine-Tuning, Co-Tuning, Data-Diffusion, and Data-Analysis, as detailed below.\n###figure_2### Cant is prevalent in the open world, so we aggregate raw text data from various sources to construct Cant and Scene datasets (Section 3.3). Although Cant and Scene datasets provide specific entities and scenes, they may not align well with the domain\u2019s requirements. Therefore, in Stage \\scriptsize{3}\u20dd, we fine-tune GPT-2 using the Scene dataset to build five scene generation models for large-scale scenes, tailored to our specific domains. However, the fine-tuned scenes may not match the entities in the Cant dataset. In Stage \\scriptsize{4}\u20dd, we address this issue by using entities from the Cant dataset to constrain the output of the generated model, ensuring scenes closely relate to the cant entities. Next, we conduct semi-automatic screening of the generated simulation scenes to form a set of Scene fragments. While these fragments contain entities, linking them with specific questions requires a method we have not yet discovered. Hence, in Steps \\scriptsize{5}\u20dd-\\scriptsize{6}\u20dd, we devise the Co-Tuning stage, where Scene fragments cross-match with cants from the Cant dataset to form Fragments. To enable multi-task comparison, we construct detection tests through different combinations of specific domains, question types, learning methods, and prompt clue methods in Stage \\scriptsize{7}\u20dd. This completes and diffuses Fragments to form Q&A-Query datasets.\nFinally, in Stages \\scriptsize{8}\u20dd-\\scriptsize{9}\u20dd, Q&A-Queries are sent to the target model API for completion, and a segmented data statistics algorithm is applied to obtain and analyze test results, conducting analyses in the Data-Analysis stage."
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "Stage 1: Fine-Tuning",
            "text": "During the fine-tuning stage, we use the Scene dataset to guide GPT-2 in generating tailored scenarios for specific domains. Despite more advanced models like GPT-3.5 and GPT-4 being available, we opt for GPT-2 due to its open-source nature, facilitating better control over training details. The fine-tuning code is publicly accessible for replication. The fine-tuning process is outlined in Algorithm 1  ###reference_###.\nThe Transformer model [36  ###reference_b36###] forms the basis for GPT-2, featuring encoders and decoders with identical modules. GPT-2 employs a partially masked self-attention mechanism and positional coding to understand sequence relationships. It has been successfully applied in various tasks like AI detection and text summarization. Overall, GPT-2\u2019s fine-tuning with the Scene dataset enables the generation of Question-Answer patterns tailored to specific domains, aiding in simulated scene generation tasks."
        },
        {
            "section_id": "3.6",
            "parent_section_id": "3",
            "section_name": "Stage 2: Co-Tuning",
            "text": "To solve the problem of many intersecting data processes in CantCounter, we use the Cant dataset and Scene fragments to collaborate and design a Co-Tuning method. Co-Tuning realizes the generation and collaboration of cross-matching and solves the problem of detection data insufficiency. The Cant dataset provides detailed entity information for the generated model. The entities could constrain the generative model and make the Scene Fragments more consistent and coherent in the need for a specific domain during the Co-Tuning stage. In the end, we also manually review the results to ensure the relevance of cants to scenes and the distinctiveness of all scenes corresponding to the same cant.\nIn this paper, we design formulas in the Co-Tuning to mathematically represent this part of the stage. The generation model is specified as , and it includes five fine-tuned models, which are denoted as , , , , and .\nAs shown in Figure 3  ###reference_###, entity  represents the -th entity () in the Cant dataset, and cant  represents the -th cant of  (). For example, in the case of the politics domain, there are 10 entities used in our experiments, each entity has twenty cants,  is taken as . The entity  can constrain the fine-tuned model \u2019s output, and the result of the constraint is the Scene fragment; this part corresponds to Eq. (1). The Scene is  . The Scene  represents the -th scene fragment (, ) that the -th entity enters into the output of the fine-tuning model ().\nEq. (2) denotes the cross-match of Cant and Scene fragment and was saved in .\nThere are  orange boxes in the  Scene fragment. These orange boxes represent the -generated text containing the Cant dataset\u2019s entities. The function of Eq. 2 is to replace the entities in the Scene fragments with cant in the Cant dataset. As shown in Figure 3  ###reference_###, for example, from  Scene fragment to Fragment 1. We replace entities in Scene  with the cant (), forming Fragment 1. By analogy, we built  Fragments in the Co-Tuning stage.\n###figure_3### In the Co-Tuning stage, we can obtain scene fragments related to entities in specific domains that have a high degree of context consistency and express various characteristics of the entities in different contexts. At the same time, our fine-tuned model is flexible enough to introduce multiple entities during the generation process and allow scene fragments to describe the relationships among multiple entities. This stage generates diverse scene fragments. While the scene fragments are generated through a generative process, the Scene dataset we provide undergoes manual review to mitigate errors in both the generated content and the language utilized within the experimental environment.\n###figure_4###"
        },
        {
            "section_id": "3.7",
            "parent_section_id": "3",
            "section_name": "Stage 3: Data-Diffusion",
            "text": "At this stage, Fragments from the Co-Tuning stage are transformed into Q&A-Queries to enhance interaction with LLM and diversify evaluation. We employ three diffusion methods: two sample learning techniques, three question types, and four prompt clue methods. Each Fragment generates 24 Q&A-Queries. First, we introduce sample learning techniques for zero-shot and one-shot learning transformations of Fragments. Second, we categorize Fragments into Abstractive, Yes/No, and Multiple-choice question types. Finally, prompts are classified into None-tip, Tip-1, Tip-2, and All-tip categories, considering information retrieval difficulty and situational prompting.\nThe introduction of Data-Diffusion in extended Fragments has significantly increased Q&A queries, providing diverse test cases for evaluating the generation model\u2019s performance comprehensively. This approach promises to establish a diverse database for future research and applications."
        },
        {
            "section_id": "3.8",
            "parent_section_id": "3",
            "section_name": "Stage 4: Data-Analysis",
            "text": "As shown \\scriptsize{8}\u20dd and \\scriptsize{9}\u20dd in Figure 2  ###reference_###, \\scriptsize{8}\u20dd means sending the data expanded by Data-Diffusion to ChatGPT and other target models. \\scriptsize{9}\u20dd shows data analysis of the output results of LLMs such as ChatGPT. After completing the Data-Diffusion, we submit the generated Q&A-Queries to the LLM API interface to obtain a large number of data results. These data results are complex and diverse, including the interplay of relationships. Therefore, we devise a data analysis algorithm to yield both numerical and analysis outcomes.\nAfter the Co-Tuning and Data-Diffusion stages, the test data generated by CantCounter is very complex. Therefore, in the Data-Analysis stage, we implement Algorithm 2  ###reference_### to conduct data statistics from various angles. During analysis, when the entity  is modified in the Co-Tuning stage (see Figure 3  ###reference_###), Algorithm 2  ###reference_### will be called accordingly. We analyze the results based on different tasks. We learn and analyze data features from Question Type Method (See 4.2 QTM) and Sample Learning Method (See 4.3 SLM) based on different question types and samples learning to get ; we analyze the data based on different prompt clues from Prompt Clue Method (See 4.4 PCM) to get . In Algorithm 2  ###reference_###, we set the matching conditions, calculate the number of fragments, and obtain  and accuracy . At the same time, we set eleven intervals: 0, 1-10, 11-20, \u2026, 91-101 to distinguish different feedbacks and obtain .\nAs shown in the Algorithm 2  ###reference_###, we put Zero-shot learning, One-shot learning, and three tasks together as a loop. We define that in the Abstractive task, the output is  in the Zero-shot learning input; the output is  in the One-shot learning input. In the Yes/NO task, the output is expressed as  in the Zero-shot learning input; the output is expressed as  in the One-shot learning input. In the Multiple-choice task, the output is represented as  in the Zero-shot learning input; the output is expressed as  in the One-shot learning input. The above content has been integrated into our code to form semi-automation."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experimental Design and Results",
            "text": "To explore our research questions, we conducted experiments in CantCounter, outlined sequentially in this section. We examined various question types in RQ1 (Section 4.1  ###reference_###), different question setups in RQ2 (Section 4.2  ###reference_###), and diverse prompt clues in RQ2 (Section 4.3  ###reference_###). Focusing primarily on ChatGPT-3.5 (version gpt-3.5-turbo-0613) due to its convenience and wide usage, similar experiments were also conducted with other language models. All experiments were performed on a server equipped with an RTX 3090 Ti GPU. In this section, we analyze using cant and scene to bypass the LLM filter in the CantCounter framework quantitatively. We conduct open-world query experiments across five domains: politics, drugs, racism, weapons, and LGBT. Initially setting  to 101, we match 692 cants to 53 entities, resulting in 69,892 scenes. These undergo Data-Diffusion, expanding to 1,677,408 scenes. This study enables a comprehensive analysis of corpus performance and language changes within specific domains."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Question Type Method (QTM)",
            "text": "In the Q&A task, we conduct three types of tasks:\nAbstractive Task: Models generate responses freely, without relying on specific information extraction.\nYes/No Task: Models provide binary responses, \u201cTrue\u201d or \u201cFalse,\u201d based solely on the presented question and existing knowledge.\nTrue/False Quiz without options: Models determine the truthfulness of a statement without selecting from predefined options, demonstrating comprehension of semantics and accurate identification.\nTable 1  ###reference_### shows that True/False Quiz without options tasks achieve the highest accuracy (45.38%), while Yes/No tasks have the lowest (22.91%). The discovery that ChatGPT performs well in multiple-choice questions is intriguing. In this task, there are five options (A) to (E), with (A) to (D) relevant to a specific domain, and (E) set as \u201cI don\u2019t know.\u201d \u201cOther\u201d signifies an answer unrelated to these options, with (A) as the correct choice. Figure 5  ###reference_### displays the box plot analysis results. Analyzing the True/False Quiz without options task results, we find key factors for its success. Firstly, it offers a set of answers with one correct option and distractors, aiding comprehension. Secondly, its structured format simplifies the process of eliminating incorrect options, improving accuracy. Lastly, the inclusion of an \u201cI don\u2019t know\u201d option enhances accuracy in uncertain situations.\n###figure_5### We also explore the low accuracy in the Yes/No task. Comparing ChatGPT-3.5\u2019s \u201cFalse\u201d answers with True/False Quiz without options task data, we find they often include option (E) and incorrect choices from the True/False Quiz without options task due to the clarity of options. Additionally, differences in response styles and keyword detection criteria impact ChatGPT-3.5\u2019s performance across Abstractive and Yes/No tasks, where Yes/No tasks restrict responses to \u201cTrue\u201d or \u201cFalse.\u201d Overall, our analysis highlights how different Q&A types affect ChatGPT-3.5\u2019s accuracy in specific domains, with True/False Quiz without options tasks showing higher performance. Further research is needed to improve ChatGPT-3.5\u2019s accuracy and adaptability in these domains."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Sample Learning Method (SLM)",
            "text": "In our experiments, we explore two sample setups: Zero-shot and One-shot learning.\nZero-shot learning. No examples are provided in the prompt, which only includes instructions and questions.\nOne-shot learning. The prompt includes an example relevant to the discussion, consisting of a sample message and user information.\nZero-shot learning involves a single user message, while One-shot learning processes a sample message and a user message. These methods help understand LLM\u2019s performance in different sample learning approaches and reveal its inference capabilities in information-poor settings. Further investigation uncovers learning patterns and effects of the model in specific domains, with default hyper-parameter settings used to avoid extensive tuning.\nIn this section, we explore how Zero-shot and One-shot learning methods affect LLM accuracy in recognizing cant scenes for RQ2. Traditionally, One-shot learning often outperforms Zero-shot learning due to more available data [37  ###reference_b37###]. However, our cross-domain analysis, depicted in Figure 6  ###reference_### and reflected in Table 1  ###reference_### (red section), reveals a trend favoring Zero-shot learning overall. We find this trend varies by domain.\n###figure_6### In the politics domain, One-shot learning performs better due to ample data and contextual understanding. Conversely, in the LGBT domain, Zero-shot learning outperforms One-shot learning due to limited publicly available examples. One-shot learning aids ChatGPT-3.5 in better contextual comprehension of sensitive topics, but it may also introduce biases, leading to lower overall accuracy in specific domains. Similar analyses across other domains yield consistent results."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Prompt Clue Method (PCM)",
            "text": "In this part of the study, the purpose of CantCounter is to explore the impact of different clues on LLM recognition and reasoning abilities. To this end, we provide four different clues to experiment with:\nNone-tip. Keeps the same as the original prompt and does not add any additional clues.\nTip 1. Add relevant tip for \u201cNone-tip\u201d. For example, when describing Trump\u2019s cant, we can add the clue \u201cPolitician\u201d in the political domain to make the prompt more directional.\nTip 2. Add another relevant tip for \u201cNone-tip\u201d. For example, when describing Trump\u2019s cant, add the \u201cUnited States\u201d prompt in the domain of politics to enrich the prompt content.\nAll-tip. Add both Tip 1 and Tip 2 on the basis of \u201cNone-tip\u201d; for example, when describing Trump\u2019s cant, add both \u201cpolitician\u201d and \u201cAmerican\u201d in the political domain to make the prompt more appropriate.\nBy observing the effects of these different clues on LLMs, CantCounter can assess the fluctuating changes they induce in recognition and reasoning abilities. This study will help further understand the influence of cues on LLM and provide directions for improving its application and performance.\nTo answer RQ2, Table 1  ###reference_### displays ChatGPT-3.5\u2019s accuracy across five domains using different prompt clues. Generally, more clue-related information improves recognition accuracy, as seen in the political domain where All-tip prompts perform significantly better. However, increasing clues doesn\u2019t always lead to higher accuracy, possibly due to information redundancy or LLM filter triggering.\nToo many clues may reduce accuracy, as seen in the LGBT domain where Tip 1 prompts were less accurate than none-tip prompts.\nOur analysis stresses the importance of a balanced clue selection approach to maximize external information usage without compromising accuracy. Thus, choosing appropriate clues in moderate quantities is key to enhancing ChatGPT-3.5\u2019s domain-specific performance."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Comparison with other LLMs",
            "text": "In our study, we examine several LLMs alongside ChatGPT-3.5 to address RQ3, including GPT-4[1  ###reference_b1###], New Bing [40  ###reference_b40###], Bard [39  ###reference_b39###], Claude [42  ###reference_b42###], ERNIE [43  ###reference_b43###], and SparkDesk [41  ###reference_b41###]. While ERNIE is optimized for Chinese content, translating cant prompts may compromise their subtlety and effectiveness. Moreover, ERNIE\u2019s frequent account suspensions hindered extensive trials [44  ###reference_b44###]. Claude\u2019s sensitive content handling also led to account suspensions [42  ###reference_b42###]. Thus, we focus on comparing and validating four other LLMs: GPT-4, Bard, New Bing, and SparkDesk. Table 2  ###reference_### presents ratios of correct answers, refused answers, and \u201cI don\u2019t know\u201d responses. Interestingly, GPT-4 consistently responds in all situations, avoiding refusal to answer. This contrasts with other models that often refuse to respond due to content filtering. GPT-4\u2019s tendency to use \u201cI don\u2019t know\u201d may stem from our controlled comparisons in the QTM and PCM methods, particularly in Multiple-choice scenarios. Conversely, other LLMs tend to refuse to answer, likely due to content categorization by filters and classifiers. SparkDesk exhibits the highest refusal rate, possibly due to overly strict filters. Furthermore, One-shot learning models are more prone to refusal to answer, as they rely on context understanding, potentially triggering filters. These findings offer insights into the performance of these LLMs across different learning tasks, informing future research directions."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Takeaways",
            "text": "We observe varying accuracy across different Q&A-Query types (RQ1), with Multiple-choice tasks being most accurate and Yes/No tasks the least. In sensitive domains, Zero-shot learning performs better than One-shot learning (RQ2). Increasing prompt clues improves cant identification accuracy (RQ2). More recent LLM models consistently avoid refusing to answer (RQ3), but they are more likely to refuse answering questions related to racism compared to LGBT (RQ4)."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "This paper presents the first comprehensive evaluation of LLM\u2019s reasoning capability using cants or dark jargons. We created two domain-specific datasets: Cant and Scene datasets, and developed an evaluation framework to assess LLM\u2019s reasoning abilities through cant comprehension. We proposed a four-stage strategy - Fine-Tuning, Co-Tuning, Data-Diffusion, and Data-Analysis - to address cross-matching and complex data calculation problems. Our experiments reveal varying comprehension levels of LLM under different question types (Abstractive, Yes/No, Multiple-choice), sample learning methods (Zero-shot/One-shot learning), and prompt clues (None-tip, Tip1, Tip2, All-tip). Additionally, across different domains (Politics, Drugs, Racism, Weapons, LGBT), different LLMs (GPT-3.5, GPT-4, New Bing, Bard, SparkDesk) demonstrate varying refusal rates to answer questions. Our findings provide insights for the security research community into LLM\u2019s reasoning capabilities regarding \u201ccant\u201d, emphasizing the importance of implementing effective safety filters and measures for screening potentially hazardous LLM-generated content."
        }
    ],
    "url": "http://arxiv.org/html/2405.00718v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2",
            "2.3"
        ],
        "methodology_sections": [
            "3.1",
            "3.2",
            "3.3",
            "3.4",
            "3.5",
            "3.6",
            "3.7",
            "3.8"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4",
            "4.5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.4",
            "3.5",
            "3.6",
            "3.7",
            "3.8",
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "research_context": {
        "paper_id": "2405.00718v1",
        "paper_title": "Can\u2019t say cant? Measuring and Reasoning of Dark Jargons in Large Language Models",
        "research_background": "Based on the provided sections, the paper addresses the following aspects:\n\n### Motivation:\nLarge Language Models (LLMs) such as ChatGPT have significantly transformed areas like information acquisition and problem-solving. Despite their advantages, LLMs present notable security and ethical challenges, particularly in how they can be manipulated for nefarious purposes, such as disinformation, AI-driven crime, and social engineering. There is a specific concern regarding \"cant,\" or \"dark jargon,\" which attackers use to disguise malicious content and evade content filters. Sparse research exists on how well LLMs understand and interpret cant, especially in domains like politics and drugs where such obfuscated language is prevalent. Hence, the motivation stems from the necessity to investigate LLMs' comprehension of cant to enhance security mechanisms.\n\n### Research Problem:\nThe primary research problem revolves around evaluating and enhancing the ability of LLMs to recognize and reason about cant. This includes:\n- Assessing how different types of questions impact LLM understanding of cant.\n- Exploring the effect of various question setups and prompt clues on LLM comprehension.\n- Comparing understanding of the same cant across different LLMs.\n- Evaluating LLMs' grasp of cant across diverse domains prone to offensive content.\n\n### Relevant Prior Work:\nThe paper builds upon previous studies which:\n- Highlight the general capabilities of LLMs, like ChatGPT, in understanding and generating natural language responses efficiently.\n- Emphasize the ethical and security issues linked to the misuse of LLMs for spreading disinformation and facilitating AI-driven crimes.\n- Fail to delve deeply into the intricacies of cant and its varied representations across different domains, thus leaving a gap in understanding which the present study aims to fill.\n- Current efforts by regulators, such as OpenAI, to implement content filters, although insufficient to fully mitigate the misuse employing cant.\n\n### Summary of Framework and Contributions:\nThe paper introduces \"CantCounter,\" a system designed to evaluate how LLMs understand cant within specific domains using compiled Cant and Scene datasets derived from various sources. CantCounter's methodology includes fine-tuning models, co-tuning datasets, data diffusion to enhance text, and systematic data analysis. The contributions of the paper are:\n1. Introduction of Cant and Scene datasets to help address data scarcity in sensitive domains.\n2. Development of the CantCounter framework to assess LLMs' domain-specific cant understanding through a multi-stage process.\n3. Demonstration of CantCounter\u2019s efficacy in bypassing security filters in mainstream dialogue LLMs, offering insights into LLM reasoning and guiding future research.",
        "methodology": "The proposed method or model in this paper involves the development and implementation of **CantCounter**, an evaluation framework aimed at examining how Large Language Models (LLMs) respond to various cants. Here are the key components and innovations of this methodology:\n\n1. **Observation of Variability in Responses**: The study starts with the observation that responses generated by LLMs differ when presented with different cants. This variability allows adversaries to potentially bypass filters or security measures embedded in the LLMs.\n\n2. **Challenge of Exhaustive Querying**: The authors note that exhaustively testing different cants across multiple scenarios and domains to identify those that can bypass LLM restrictions and produce harmful outputs is not feasible due to the time and effort required.\n\n3. **Combining Cants and Scenes**: Instead of exhaustive querying, the research investigates whether adversaries can create contexts that are both reasonable and coherent by independently combining various cants and scenes. The goal is to see if these combinations can bypass LLM filters and restrictions.\n\n4. **Introduction of CantCounter**: To facilitate this investigation, the authors propose **CantCounter**, which they claim is the first evaluation (attack) framework specifically targeting open-world dialogue systems or LLMs.\n\nIn summary, **CantCounter** is an innovative framework designed to evaluate the susceptibility of LLMs to manipulations involving different cants and scenes, aiming to understand and mitigate the risk of adversaries generating harmful outputs.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\nTo explore the research questions, the primary experiments were conducted within the CantCounter framework. The goal was to quantitatively analyze the use of \"cant\" and \"scene\" to bypass the filter of large language models (LLMs) like ChatGPT-3.5 (version gpt-3.5-turbo-0613), with additional experiments performed on other language models for comparison. The experiments were focused on evaluating the models' responses to different types of jargons across various sensitive domains, ensuring a thorough investigation into their behavior and performance.\n\n**Datasets and Domains:**\n- The experiment spanned across five distinct domains:\n  1. Politics\n  2. Drugs\n  3. Racism\n  4. Weapons\n  5. LGBT\n\nThe dataset initiated with 101 initial entities, which were expanded through a matching process, resulting in 692 cants associated with 53 entities. Consequently, this initial dataset yielded 69,892 unique scenes. Using a process termed Data-Diffusion, these scenes were further expanded, culminating in a comprehensive dataset of 1,677,408 scenes. This extensive dataset allowed for an in-depth analysis of language model performance and linguistic adaptability across these sensitive domains.\n\n**Baselines and Evaluation Metrics:**\nWhile the specific baselines and evaluation metrics are not explicitly described in the provided text, it is implied that the focus was on evaluating how well different models could filter or respond to dark jargons within these structured queries and scenes. Metrics likely included the accuracy and robustness of the models in identifying and managing sensitive content, as well as their adaptability to new or expanding linguistic patterns introduced in the dataset.\n\n**Experimental Results:**\nThe main results highlight the efficacy of the CantCounter framework in generating a vast array of scenes that challenge the models' filtering capabilities. By systematically analyzing the responses of ChatGPT-3.5 and other LLMs across diverse domains, the study provides insights into the strengths and weaknesses of these models when confronted with domain-specific jargons designed to bypass content filters. The results underscore the importance of continuous updates and training of LLMs to better handle evolving linguistic tricks and dark jargons.\n\nIn summary, the main experiment successfully demonstrated the CantCounter framework's ability to create a robust testbed for evaluating LLMs, revealing significant insights into their current capabilities and areas needing improvement."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Investigate how large language models (LLMs), including ChatGPT, recognize and respond to various types of questions across specific domains, focusing on the influence of different question types (abstractive, yes/no, and multiple-choice).",
            "experiment_process": "In the Q&A task, three types of tasks were conducted: Abstractive Task (models generate responses freely without relying on specific information extraction), Yes/No Task (models provide binary responses based on the presented question and existing knowledge), and Multiple-choice Task (models select the correct answer from a set of multiple options). The tasks were evaluated using specific combinations of options, and the accuracy of these tasks was analyzed.",
            "result_discussion": "The multiple-choice tasks achieved the highest accuracy (45.38%), while yes/no tasks had the lowest (22.91%). Multiple-choice tasks benefiting from structured formats aid comprehension, improve accuracy through the elimination process, and provide a clear answer set. The complexity and clarity of options presented in multiple-choice tasks aid in their higher accuracy in the model's understanding and selection abilities.",
            "ablation_id": "2405.00718v1.No1"
        },
        {
            "research_objective": "Evaluate the performance of LLMs in recognizing cant scenes under different sample learning conditions (zero-shot and one-shot learning) across varied domains.",
            "experiment_process": "Zero-shot learning involves prompts with instructions and questions without examples, whereas one-shot learning includes relevant examples in the prompt. The accuracy of recognizing cant scenes by ChatGPT-3.5 under these sample learning methods was compared across different domains such as politics and LGBT.",
            "result_discussion": "Zero-shot learning generally performed better across most domains, contradicting traditional expectations that one-shot learning should outperform due to additional context from examples. This trend varied by domain, with one-shot learning achieving better results in politics due to richer data, while zero-shot learning outperformed in the LGBT domain due to lesser biases and better recognition in data-scarce contexts.",
            "ablation_id": "2405.00718v1.No2"
        },
        {
            "research_objective": "Examine the impact of different prompt clues on the recognition and reasoning capabilities of LLMs (like ChatGPT) in various domains.",
            "experiment_process": "Four different prompt clues were tested: None-tip (no additional clue), Tip 1 (a relevant clue), Tip 2 (another relevant clue), and All-tip (both Tip 1 and Tip 2). These clues were meant to guide the model more effectively with additional context. The accuracy of recognition and reasoning was evaluated across five domains using these clues.",
            "result_discussion": "Generally, the inclusion of more clue-related information improved accuracy, particularly in the political domain where all-tip prompts showed significantly better performance. However, adding too many clues did not always enhance accuracy, as seen in the LGBT domain where tip 1 prompts were less accurate than none-tip prompts due to possible information redundancy or triggering of LLM filters. A balanced clue selection approach was emphasized for optimizing performance.",
            "ablation_id": "2405.00718v1.No3"
        }
    ]
}