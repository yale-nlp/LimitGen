{
    "title": "Watermark-based Detection and Attribution of AI-Generated Content",
    "abstract": "Several companies\u2013such as Google, Microsoft, and OpenAI\u2013have deployed techniques to watermark AI-generated content to enable proactive detection. However, existing literature mainly focuses on user-agnostic detection. Attribution aims to further trace back the user of a generative-AI service who generated a given content detected as AI-generated. Despite its growing importance, attribution is largely unexplored. In this work, we aim to bridge this gap by providing the first systematic study on watermark-based, user-aware detection and attribution of AI-generated content. Specifically, we theoretically study the detection and attribution performance via rigorous probabilistic analysis. Moreover, we develop an efficient algorithm to select watermarks for the users to enhance attribution performance. Both our theoretical and empirical results show that watermark-based detection and attribution inherit the accuracy and (non-)robustness properties of the watermarking method.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Generative AI (GenAI)\u2013such as DALL-E 3, Midjourney, and ChatGPT\u2013can synthesize very realistic-looking content such as images, texts, and audios. Beyond its societal benefits, GenAI also raises many ethical concerns. For instance, they can be misused to generate harmful content; they can be used to aid disinformation and propaganda campaigns by generating realistic-looking content [1  ###reference_bx1###]; and people can falsely claim copyright ownership of content generated by them [2  ###reference_bx2###].\nWatermark-based detection and attribution of AI-generated content is a promising technique to mitigate these ethical concerns. For instance, several companies\u2013such as Google, OpenAI, Stability AI, and Microsoft\u2013have deployed such techniques to watermark their AI-generated images. Specifically, OpenAI inserts a visible watermark into the images generated by its DALL-E 2 [3  ###reference_bx3###]; Google\u2019s SynthID [4  ###reference_bx4###] inserts an invisible watermark into images generated by its Imagen; Stability AI deploys a watermarking method in its Stable Diffusion [5  ###reference_bx5###];\nand Microsoft watermarks all AI-generated images in Bing [6  ###reference_bx6###].\nHowever, existing literature mainly focuses on user-agnostic detection of AI-generated content. In particular, the same watermark is inserted into all the content generated by a GenAI service; and a content is detected as generated by the GenAI service if a similar watermark can be decoded from it. Attribution aims to further trace back the registered user of the GenAI service who generated a given content.111Attribution could also refer to tracing back the GenAI service that generated a given content, which we discuss in Section 7  ###reference_###. Such attribution can aid the GenAI service provider or law enforcement in forensic analysis of cyber-crimes, such as disinformation and propaganda campaigns, that involve a given AI-generated content. Despite the growing importance of attribution, it is largely unexplored. In this work, we aim to bridge this gap by providing a systematic study on watermark-based detection and attribution of AI-generated content.\nWe note that a relevant but orthogonal research direction is to develop watermarking methods that are robust against post-processing of AI-generated content. We stress that it is still an ongoing effort to develop robust watermarking and the community has already made significant progress in the past several years. For instance, non-learning-based image watermarking [7  ###reference_bx7###, 8  ###reference_bx8###, 9  ###reference_bx9###], which has been studied for decades, is not robust against common post-processing such as JPEG compression, Gaussian blur, and Brightness/Contrast. However, recent learning-based image watermarking [10  ###reference_bx10###, 11  ###reference_bx11###, 12  ###reference_bx12###, 13  ###reference_bx13###, 14  ###reference_bx14###] is robust against such common post-processing [11  ###reference_bx11###] because it can leverage adversarial training [15  ###reference_bx15###]. Although learning-based image watermarking is not robust yet against adversarial post-processing in the white-box setting [16  ###reference_bx16###]; it has good robustness against adversarial post-processing when an attacker can only query the detection API for a small number of times in the black-box setting or does not have access to the detection API [16  ###reference_bx16###]. For instance, Google restricts access of its detection API to only trusted customers [17  ###reference_bx17###]. Since our detection and attribution method relies on watermarking techniques, it inherits their (non-)robustness properties.\n###figure_1### Our work:  In this work, we conduct the first systematic study on the theory, algorithm, and evaluation of watermark-based detection and attribution of AI-generated content. Figure 1  ###reference_### illustrates our method. When a user registers in a GenAI service, the service provider selects a watermark (i.e., a bitstring) for him/her and stores it in a watermark database. When a user generates a content using the GenAI service, the user\u2019s watermark is inserted into the content using the watermark encoder. A content is detected as generated by the GenAI service if the watermark decoded from the content is similar enough to at least one user\u2019s watermark in the watermark database. Moreover, the content is further attributed to the user whose watermark is the most similar to the decoded one.\nTheory. We theoretically analyze the performance of watermark-based detection and attribution. Specifically, we define three key evaluation metrics: true detection rate (TDR), false detection rate (FDR), and true attribution rate (TAR). TDR (or TAR) is the probability that an AI-generated content is correctly detected (or attributed), while FDR is the probability that a non-AI-generated content is falsely detected as AI-generated. We show that other relevant evaluation metrics can be derived from these three. Based on a formal quantification of a watermarking method\u2019s behavior, we derive lower bounds of TDR and TAR, and an upper bound of FDR no matter how the users\u2019 watermarks are selected. We also discuss multiple theoretical insights about the detection/attribution performance based on our derived bounds.\nAlgorithm. Selecting watermarks for the users is a key component of watermark-based detection and attribution. Intuitively, attribution is hard if the users\u2019 watermarks are similar to each other. In fact, our derived lower bound of TAR also aligns with such intuition. Therefore, to enhance attribution performance, we aim to select dissimilar watermarks for the users. Formally, we formulate a watermark selection problem, which aims to select a watermark for a new registered user via minimizing the maximum similarity between the selected watermark and the existing users\u2019 watermarks. We find that our watermark selection problem is equivalent to the well-known farthest string problem [18  ###reference_bx18###], which has been studied extensively in the theoretical computer science community. Moreover, since the farthest string problem is NP-hard, our watermark selection problem is also NP-hard, which implies the challenges of developing efficient, exact solutions. Thus, we resort to efficient, approximate solutions. In particular, we adapt the bounded search tree algorithm [19  ###reference_bx19###], a state-of-the-art inefficient, exact solution to the farthest string problem, as an efficient, approximate algorithm to select watermarks.\nEmpirical evaluation. We empirically evaluate our method for AI-generated images on three GenAI models, i.e., Stable Diffusion, Midjourney, and DALL-E 2. We use HiDDeN [11  ###reference_bx11###], the state-of-the-art learning-based watermarking method. Note that our detection and attribution inherit the (non-)robustness properties of HiDDeN. In particular, our results show that detection and attribution are very accurate, i.e., TDR/TAR is close to 1 and FDR is close to 0, when AI-generated images are not post-processed; detection and attribution are still accurate when common post-processing, such as JPEG compression, Gaussian blur, and Brightness/Contrast, is applied to AI-generated images; and adversarial post-processing [16  ###reference_bx16###] with a small number of queries to the detection API degrades the image quality substantially in order to evade detection/attribution. Moreover, we show our watermark selection algorithm outperforms baselines, and our method is also applicable to AI-generated texts.\nTo summarize, our contributions are as follows:\nWe provide the first systematic study on watermark-based, user-aware detection and attribution of AI-generated content.\nTheory. We theoretically analyze the detection and attribution performance for any watermarking method and no matter how the watermarks are selected for the users.\nAlgorithm. We formulate a watermark selection problem, which is inspired by our theoretical results; and we develop an efficient, approximate solution for it.\nEvaluation. We conduct extensive evaluation of our method in different scenarios."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Watermarking Methods",
            "text": "A watermarking method typically consists of three components: watermark, encoder, and decoder. We consider a watermark  to be a bitstring. An encoder  embeds a watermark into a content, while a decoder  decodes a watermark from a (watermarked or unwatermarked) content. When a content has watermark , the decoded watermark is similar to . Note that the encoder  and watermark  can also be embedded into the parameters of a GenAI model such that its generated content is inherently watermarked with  [14  ###reference_bx14###].\nNon-learning-based vs. learning-based:  Watermarking methods can be categorized into two groups based on the design of the encoder and decoder: non-learning-based and learning-based. Non-learning-based methods [7  ###reference_bx7###, 8  ###reference_bx8###, 9  ###reference_bx9###, 20  ###reference_bx20###, 21  ###reference_bx21###] design the encoder and decoder based on some hand-crafted heuristics, while learning-based methods [10  ###reference_bx10###, 11  ###reference_bx11###, 22  ###reference_bx22###, 12  ###reference_bx12###, 13  ###reference_bx13###, 14  ###reference_bx14###] use neural networks as the encoder/decoder and automatically learn them using a content dataset. For instance, Tree-Ring [20  ###reference_bx20###] and LM-watermarking [21  ###reference_bx21###] respectively are non-learning-based watermarking methods for images and texts; while HiDDeN [11  ###reference_bx11###] and AWT [22  ###reference_bx22###] respectively are learning-based methods for images and texts. Our watermark-based detection and attribution method, theory, and algorithm are applicable to both categories of watermarking methods. However, since learning-based watermarking methods are more robust due to adversarial training [11  ###reference_bx11###], we adopt a learning-based watermarking method in our experiments.\nStandard training vs. adversarial training: \nIn learning-based watermarking methods, the encoder and decoder are automatically learnt using a content dataset. Specifically, given a content  and a random watermark , the decoded watermark  for the watermarked content  should be similar to , i.e., . Based on this intuition, standard training aims to learn an encoder  and decoder  such that  is similar to  for a content dataset [10  ###reference_bx10###]. A watermarked content  may be post-processed, e.g., a watermarked image may be post-processed by JPEG compression during transmission on the Internet. Zhu et al. [11  ###reference_bx11###] extended adversarial training [15  ###reference_bx15###, 23  ###reference_bx23###], a standard technique to train robust classifiers, to train watermarking encoder and decoder that are more robust against post-processing. Specifically, adversarial training aims to learn an encoder  and decoder  such that  is similar to , where  stands for a post-processing operation and  is a post-processed watermarked content. In each epoch of adversarial training, a  is randomly sampled from a given set of them for each content in the content dataset.\nRobustness of watermarking:  We stress that building robust watermarking methods is orthogonal to our work and is still an ongoing effort. Non-learning-based watermarking methods [7  ###reference_bx7###, 8  ###reference_bx8###, 9  ###reference_bx9###, 20  ###reference_bx20###, 21  ###reference_bx21###] are known to be non-robust to common post-processing such as JPEG compression for images [14  ###reference_bx14###, 11  ###reference_bx11###] and paraphrasing for texts [24  ###reference_bx24###], i.e., such common post-processing can remove the watermark from a watermarked content. Learning-based watermarking methods [10  ###reference_bx10###, 11  ###reference_bx11###, 22  ###reference_bx22###, 12  ###reference_bx12###, 13  ###reference_bx13###, 14  ###reference_bx14###] are more robust to such common post-processing because they can leverage adversarial training. For instance, common post-processing has to substantially decrease the quality of a watermarked image in order to remove the watermark [12  ###reference_bx12###, 13  ###reference_bx13###].\nJiang et al. [16  ###reference_bx16###] proposed adversarial post-processing to image watermarking, which strategically perturbs a watermarked image to remove the watermark. According to Jiang et al., learning-based image watermarking methods are not yet robust to adversarial post-processing in the white-box setting where an attacker has access to the decoder. However, they have good robustness to adversarial post-processing when an attacker can only query the detection API for a small number of times in the black-box setting or does not have access to the detection API. In particular, adversarial post-processing substantially decreases the quality of a watermarked image in order to remove the watermark in such scenarios. To enhance robustness, a GenAI service can keep its watermarking encoder/decoder private and restrict the access of its detection API to a small number of trusted customers. For instance, Google\u2019s SynthID [4  ###reference_bx4###] adopts such strategy.\nWe acknowledge that our watermark-based detection and attribution inherit the watermarking method\u2019s (non-)robustness properties discussed above."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Watermark-based Detection",
            "text": "Watermark has been used for proactive detection of AI-generated content [21  ###reference_bx21###]. In particular, multiple companies\u2013such as Stability AI, OpenAI, Google, and Microsoft\u2013have deployed watermark-based detection as discussed in Introduction. However, existing literature mainly focuses on user-agnostic detection. Specifically, a GenAI service provider picks a watermark; whenever a content is generated by the GenAI service, the watermark is embedded into it before returning it to a user; and a content is detected as generated by the GenAI service if a similar watermark can be decoded from it. In this work, we study watermark-based, user-aware detection and attribution of AI-generated content. After detecting a content as generated by the GenAI service, we further trace back the user of the GenAI service who generated it."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Problem Formulation",
            "text": "Problem setup:  Suppose we are given a generative AI model, which is deployed as a GenAI service. A registered user sends a prompt (i.e., a text) to the GenAI service, which returns an AI-generated content to the user. The content can be image, text, or audio. In this work, we consider detection and attribution of AI-generated content. Detection aims to decide whether a given content was generated by the GenAI service or not; while attribution further traces back the user of the GenAI service who generated a content detected as AI-generated. Such attribution can aid the GenAI service provider or law enforcement in forensic analysis of cyber-crimes, e.g., disinformation or propaganda campaigns, that involve a given AI-generated content. We formally define the detection and attribution problems as follows:\nGiven a content and a GenAI service, detection aims to infer whether the content was generated by the GenAI service or not.\nGiven a content, a GenAI service, and  users  of the GenAI service, attribution aims to further infer which user used the GenAI service to generate the content after it is detected as AI-generated.\nWe note that the set of  users  in attribution could include all registered users of the GenAI service, in which  may be very large. Alternatively, this set may consist of a smaller number of registered users if the GenAI service provider has some prior knowledge on its registered users. For instance, the GenAI service provider may exclude the registered users, who are verified offline as trusted, from the set  to reduce its size. How to construct the set of users  in attribution is out of the scope of this work. Given any set , our method aims to infer which user in  may have generated a given content. We also note that another relevant attribution problem is to trace back the GenAI service that generated a given content. Our method can also be used for such GenAI-service attribution, which we discuss in Section 7  ###reference_###.\nThreat model:  An AI-generated, watermarked content may be post-processed by some common post-processing techniques in non-adversarial settings. For instance, an image may be post-processed by JPEG compression during transmission on the Internet, or a user may use Gaussian blur or Brightness/Contrast to edit an image in an image editor. In adversarial settings, a malicious user may post-process an AI-generated content to evade detection and/or attribution. Other than the common post-processing techniques, a malicious user may also use adversarial post-processing [16  ###reference_bx16###] to remove the watermark in an AI-generated content. We assume the watermark encoder/decoder is private and the malicious user has limited access to the detection API, in which state-of-the-art watermarking methods have good robustness to post-processing [16  ###reference_bx16###]. Such threat model arises when a GenAI service provider restricts the access of its detection API to a small set of trusted customers, e.g., Google\u2019s SynthID adopts this threat model. Note that our theoretical analysis in Section 5  ###reference_### can explicitly quantify and incorporate the impact of post-processing on the detection and attribution performance."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Watermark-based Detection and Attribution",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Overview",
            "text": "We propose a watermark-based detection and attribution method, which is illustrated in Figure 1  ###reference_###. When a user registers in the GenAI service, the service provider selects a unique watermark for the user. We denote by  the watermark selected for user , where  is the user index. During generation, when a user  sends a prompt to the GenAI service to generate a content, the provider uses the watermark encoder  to embed watermark  into the content. During detection and attribution, a watermark is decoded from a given content; the given content is detected as generated by the GenAI service if the decoded watermark is similar enough to at least one of the users\u2019 watermarks; and the given content is further attributed to the user whose watermark is the most similar to the decoded watermark after it is detected as AI-generated.\nNext, we describe the details of detection and attribution. Moreover, we discuss how to select watermarks for the users to maximize the attribution performance."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Detection",
            "text": "Recall that we denote by  the set of  users of the GenAI service for atribution. Each user  has a watermark , where . For convenience, we denote by  the set of  watermarks. Given a content , we use the decoder  to decode a watermark  from it. If there exists a user\u2019s watermark that is similar enough to , we detect  as AI-generated. We use bitwise accuracy to measure similarity between two watermarks, which we formally define as follows:\nBitwise Accuracy (BA):  Given any two watermarks  and , their bitwise accuracy (denoted as ) is the fraction of matched bits in them. Formally, we have the following:\nwhere  is the watermark length,  is the th bit of , and  is the indicator function that has a value 1 if  and 0 otherwise. A content  is detected as AI-generated if and only if the following satisfies:\nwhere  is the detection threshold."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Attribution",
            "text": "Attribution is applied only after a content  is detected as AI-generated. Intuitively, we attribute the content to the user whose watermark is the most similar to the decoded watermark . Formally, we attribute content  to user , where  is as follows:"
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Watermark Selection",
            "text": "A key component of watermark-based detection and attribution is how to select watermarks for the users. Next, we first formulate watermark selection as an optimization problem, and then propose a method to approximately solve it."
        },
        {
            "section_id": "4.4.1",
            "parent_section_id": "4.4",
            "section_name": "4.4.1 Formulating a Watermark Selection Problem",
            "text": "Intuitively, if two users have similar watermarks, then it is hard to distinguish between them for the attribution. An extreme example is that two users have the same watermark, making it impossible to attribute either of them. In fact, our theoretical analysis in Section 5 ###reference_### shows that attribution performance is better if the maximum pairwise bitwise accuracy between the users\u2019 watermarks is smaller. Thus, to enhance attribution, we aim to select watermarks for the users to minimize their maximum pairwise bitwise accuracy. Formally, we formulate watermark selection as the following optimization problem: where stands for bitwise accuracy between two watermarks. This optimization problem jointly optimizes the watermarks simultaneously. As a result, it is very challenging to solve the optimization problem because the GenAI service provider does not know the number of registered users (i.e., ) in advance. In practice, users register in the GenAI service at very different times. To address the challenge, we select a watermark for a user at the time of his/her registration in the GenAI service. For the first user , we choose a watermark sequentially from a predefined list without assessing similarity. Suppose we have selected watermarks for users. Then, the th user registers and we aim to select a watermark whose maximum bitwise accuracy with the existing watermarks is minimized. Formally, we formulate a watermark selection problem as follows:"
        },
        {
            "section_id": "4.4.2",
            "parent_section_id": "4.4",
            "section_name": "4.4.2 Solving the Watermark Selection Problem",
            "text": "NP-hardness:  We can show that our watermark selection problem in Equation 5  ###reference_### is NP-hard. In particular, we can reduce the well-known farthest string problem [18  ###reference_bx18###], which is NP-hard, to our watermark selection problem. In the farthest string problem, we aim to find a string that is the farthest from a given set of strings. We can view a string as a watermark in our watermark selection problem, the given set of strings as the watermarks of the  users, and the similarity metric between two strings as our bitwise accuracy. Then, we can reduce the farthest string problem to our watermark selection problem, which means that our watermark selection problem is also NP-hard. This NP-hardness implies that it is very challenging to develop an efficient exact solution for our watermark selection problem. We note that efficiency is important for watermark selection as we aim to select a watermark for a user at the time of registration. Therefore, we aim to develop an efficient algorithm that approximately solves the watermark selection problem.\nRandom:  The most straightforward method to approximately solve the watermark selection problem in Equation 5  ###reference_### is to generate a watermark uniformly at random as . We denote this method as Random. The limitation of this method is that the selected watermark  may be very similar to some existing watermarks, i.e.,  is large, making attribution less accurate, as shown in our experiments.\nDecision problem:  To develop an efficient algorithm to approximately solve our watermark selection problem, we first define its decision problem. Specifically, given the maximum number of matched bits between  and the existing  watermarks as , the decision problem aims to find such a  if there exists one and return NotExist otherwise. Formally, the decision problem is to find any watermark  in the following set if the set is nonempty: ,\nwhere  is the watermark length. Next, we discuss how to solve the decision problem and then turn the algorithm to solve our watermark selection problem.\nBounded search tree algorithm (BSTA) [19  ###reference_bx19###]:  Recall that our watermark selection problem is equivalent to the farthest string problem. Thus, our decision problem is equivalent to that of the farthest string problem, which has been studied extensively in the theoretical computer science community. In particular, BSTA is the state-of-the-art exact algorithm to solve the decision problem version of the farthest string problem. We apply BSTA to solve the decision problem version of our watermark selection problem exactly, which is shown in Algorithm 1  ###reference_### in Appendix. The key idea of BSTA is to initialize  as  (i.e., each bit of  flips), and then reduce the decision problem to a simpler problem recursively until it is easily solvable or there does not exist a solution . In particular, given an initial , BSTA first finds the existing watermark  that has the largest bitwise accuracy with . If , then  is already a solution to the decision problem and thus BSTA returns . Otherwise, BSTA chooses any  bits that  and  match. For each of the chosen  bits, BSTA flips the corresponding bit in  and recursively solves the decision problem using the new  as an initialization. The recursion is applied  times at most, i.e., the recursion depth  is set as  when calling Algorithm 1  ###reference_###.\nA key limitation of BSTA is that it has an exponential time complexity [19  ###reference_bx19###]. In fact, since the decision problem is NP-hard, all known exact solutions have exponential time complexity. Therefore, to enhance computation efficiency, we resort to approximate solutions. Next, we discuss the state-of-the-art approximate solution that adapts BSTA and a new approximate solution that we propose.\nNon Redundant Guess (NRG) [25  ###reference_bx25###]:  Like BSTA, this approximate solution also first initializes  as  and finds the existing watermark  that has the largest bitwise accuracy with . If , then NRG returns . Otherwise, NRG samples  bits that  and  match uniformly at random. Then, NRG flips these bits in  and recursively solve the decision problem using the new  as an initialization. Note that NRG stops the recursion when  bits of the initial  have been flipped. Algorithm 2  ###reference_### in Appendix shows NRG.\nApproximate bounded search tree algorithm (A-BSTA):  We adapt BSTA as an efficient approximate solution to our decision problem. Specifically, A-BSTA makes two adaptions of BSTA. First, we constrain the recursion depth  to be a constant (e.g., 8 in our experiments) instead of , which makes the algorithm approximate but improves the efficiency substantially. Second, instead of initializing  as , we initialize  as an uniformly random watermark. As our experiments in Table 3  ###reference_### in Appendix show, our initialization further improves the performance of A-BSTA. This is because a random initialization is more likely to have small bitwise accuracy with all existing watermarks. Note that BSTA, NRG, and A-BSTA all return NotExist if they cannot find a solution  to the decision problem.\nSolving our watermark selection problem:  Given an algorithm (e.g., BSTA, NRG, or A-BSTA) to solve the decision problem, we turn it as a solution to our watermark selection problem. Specifically, our idea is to start from a small , and then solve the decision problem. If we cannot find a watermark  for the given , we increase it by 1 and solve the decision problem again. We repeat this process until finding a watermark . Note that we start from , i.e., the maximum number of matched bits between  and the other  watermarks. This is because an  smaller than this value is unlikely to produce a watermark  as it failed to do so when selecting . Algorithm 3  ###reference_### in Appendix shows our method.\nNote that binary search is another way to find a proper . Specifically, we start with a small  (denoted as ) that does not produce a  and a large  (denoted as ) that does produce a . If  produces a , we update ; otherwise we update . The search process stops when . However, we found that increasing  by 1 as in our Algorithm 3  ###reference_### is more efficient than binary search. This is because increasing  by 1 expands the search space of  substantially, which often leads to a valid . On the contrary, binary search would require solving the decision problem multiple times with different  until finding that  is enough.\nTime complexity:  We analyze the time complexity of the algorithms to solve the decision problem. For Random, the time complexity is . For BSTA, the time complexity to solve the decision problem with parameter  is  according to [19  ###reference_bx19###]. For NRG, the time complexity is  according to [25  ###reference_bx25###]. For A-BSTA, the time complexity is , where  is a constant."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Theoretical Analysis",
            "text": "We theoretically analyze the detection and attribution performance of our watermark-based method. We first formally define several key metrics to evaluate the performance of detection and attribution. Then, we theoretically analyze the evaluation metrics. All our proofs are shown in Appendix."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Content Distributions",
            "text": "Suppose we are given  users , each of which has an unique watermark , where . We denote the  watermarks as a set . When a user  generates content via the GenAI service, the service provider uses the encoder  to embed the watermark  into the content. We denote by  the probability distribution of the watermarked content generated by . Note that two users  and  may have different AI-generated, watermarked content distributions  and . This is because the two users have different watermarks and they may be interested in generating different types of content. Moreover, we denote by  the probability distribution of non-AI-generated content."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": " Evaluation Metrics",
            "text": "(User-dependent) True Detection Rate (TDR):  TDR is the probability that an AI-generated content is correctly detected. Note that different users may have different AI-generated content distributions. Therefore, TDR depends on users. We denote by TDR the true detection rate for the watermarked content generated by user , i.e., TDR is the probability that a content  sampled from the probability distribution  uniformly at random is correctly detected as AI-generated.\nFormally, we have:\nwhere  is the bitwise accuracy between two watermarks,  is the decoder, , and  is the detection threshold. The notation  indicates a content is sampled from a distribution uniformly at random.\nFalse Detection Rate (FDR):  FDR is the probability that a content  sampled from the non-AI-generated content distribution  uniformly at random is detected as AI-generated. Note that FDR does not depend on users. Formally, we have:\nwhere .\n(User-dependent) True Attribution Rate (TAR):  TAR is the probability that an AI-generated content is correctly attributed to the user that generated the content. Like TDR, TAR also depends on users. We denote by TAR the true attribution rate for the watermarked content generated by user , i.e., TAR is the probability that a content sampled from  uniformly at random is correctly attributed to user . Formally, we have:\nwhere , the first term  means that  is detected as AI-generated, and the second term  means that  is attributed to user . Note that we have the first term because attribution is only applied after detecting a content as AI-generated.\nOther evaluation metrics can be derived from TDR, FDR, and TAR:  We note that there are also other relevant detection and attribution metrics, e.g., the probability that an AI-generated content is incorrectly attributed to a user.\nWe show that other relevant detection and attribution metrics can be derived from TDR, FDR, and TAR, and thus we focus on these three metrics in our work. Specifically, Figure 2  ###reference_### shows the taxonomy of detection and attribution results for non-AI-generated content and AI-generated content generated by user . In the taxonomy trees, the first-level nodes represent ground-truth labels of content; the second-level nodes represent possible detection results; and the third-level nodes represent possible attribution results (note that attribution is only performed after a content is detected as AI-generated).\nIn the taxonomy trees, there are 5 branches in total, which are labeled as \u2460, \u2461, \u2462, \u2463, and \u2464 in the figure. Each branch starts from a root node and ends at a leaf node, and corresponds to a metric that may be of interest. For instance, our TDR is the probability that a content  goes through branches \u2463 or \u2464; FDR is the probability that a content  goes through branch \u2461; and TAR is the probability that a content  goes through branch \u2463. The probability that a content goes through other branches can be calculated using TDR, FDR, and/or TAR. For instance, the probability that a non-AI-generated content  is correctly detected as non-AI-generated is the probability that  goes through the branch \u2460, which can be calculated as FDR. The probability that an AI-generated content  is incorrectly detected as non-AI-generated is the probability that  goes through the branch \u2462, which can be calculated as TDR. The probability that a user \u2019s AI-generated content  is correctly detected as AI-generated but incorrectly attributed to a different user  is the probability that  goes through the branch \u2464, which can be calculated as TDRTAR.\n###figure_2###"
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Formal Quantification of Watermarking",
            "text": "Intuitively, to theoretically analyze the detection and attribution performance (i.e., TDR, FDR, and TAR), we need a formal quantification of a watermarking method\u2019s behavior at decoding watermarks in AI-generated content and non-AI-generated content. Towards this end, we formally define -accurate and -random watermarking as follows:\nFor a randomly sampled AI-generated content  embedded with watermark , the bits of the decoded watermark  are independent and each bit matches with that of  with probability , where . Formally, we have , where ,  is the decoder, and  represents the th bit of a watermark. We say a watermarking method is -accurate if it satisfies the above condition.\nFor a randomly sampled non-AI-generated content  without any watermark embedded, the bits of the decoded watermark  are independent and each bit is 1 with probability at least  and at most , where . Formally, we have , where  and  represents the th bit of a watermark. We say a watermarking method is -random if it satisfies the above condition.\nThe parameter  is used to characterize the accuracy of the watermarking method at encoding/decoding a watermark in an AI-generated content. In particular, the watermarking method is more accurate when  is closer to 1. For a -accurate watermarking method, the number of matched bits between the decoded watermark  for a watermarked content  and the ground-truth watermark follows a binomial distribution with parameters  and , where  is the watermark length. The parameter  characterizes the behavior of the watermarking method for non-AI-generated content. In particular, the decoded watermark for a non-AI-generated (i.e., unwatermarked) content is close to a uniformly random watermark, where  quantifies the difference between them. The watermarking method is more random for non-AI-generated content if  is closer to 0.\nUser-dependent :  Since the users\u2019 AI-generated content may have different distributions , the same watermarking method may have different  for different users. To capture this phenomena, we consider the watermarking method is -accurate for user \u2019s AI-generated content embedded with watermark . Note that the same  is used across different users since it is used to characterize the behavior of the watermarking method for non-AI-generated content, which is user-independent. The parameters  and  can be estimated using a set of AI-generated and non-AI-generated content, as shown in our experiments.\nIncorporating post-processing:  Our definition of -accurate and -random watermarking can also incorporate post-processing (e.g., JPEG compression) that an attacker/user may apply to AI-generated or non-AI-generated content. In particular, we can replace  as  in our definitions, where  stands for post-processing of content . When AI-generated content is post-processed, the watermarking method may become less accurate, i.e.,  may decrease."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Detection Performance",
            "text": "Deriving a lower bound of TDR:  Intuitively, an user \u2019s AI-generated content  can be correctly detected as AI-generated in two cases:\nCase I. The decoded watermark  is similar enough to the user \u2019s watermark .\nCase II. The decoded watermark  is dissimilar to  but similar enough to some other user\u2019s watermark.\nCase II is more likely to happen when  is more dissimilar to some other user\u2019s watermark, i.e., when  is smaller. This is because the fact that  is dissimilar to  and  is dissimilar to some other user\u2019s watermark implies that  is similar to some other user\u2019s watermark. Formally, we can derive a lower bound of TDR as follows:\nSuppose we are given  users with any  watermarks . When the watermarking method is -accurate for user \u2019s AI-generated content, we have a lower bound of TDR as follows:\nwhere  follows a binomial distribution with parameters  and , i.e., , ,  is the watermark length, and .\nThe two terms in the lower bound respectively bound the probabilities for Case I and Case II of correctly detecting user \u2019s AI-generated content. Based on Theorem 1  ###reference_orem1###, we have the following corollary.\nWhen the watermarking method is more accurate, i.e.,  is closer to 1, the lower bound of TDR is larger.\nDeriving an upper bound of FDR:  Intuitively, a non-AI-generated content  is also incorrectly detected as AI-generated in two cases: 1) the decoded watermark  is similar enough with some user\u2019s watermark, e.g., ; and 2) the decoded watermark  is dissimilar to  but similar enough to some other user\u2019s watermark. Based on this intuition, we can derive an upper bound of FDR as follows:\nSuppose we are given  users with  watermarks  and watermark  is selected uniformly at random. We have an upper bound of FDR as follows:\nwhere  follows a binomial distribution with parameters  and , i.e., , and .\nNote that the upper bound of FDR in Theorem 2  ###reference_orem2### does not depend on -random watermarking since we consider  is picked uniformly at random. However, we found such upper bound is loose. This is because the second term of the upper bound considers the worst-case scenario of the  watermarks. The next theorem shows that when the  watermarks are constrained, in particular selected independently, we can derive a tighter upper bound of FDR.\nSuppose we are given  users with  watermarks  selected independently. When the watermarking method is -random for non-AI-generated content, we have an upper bound of FDR as follows:\nwhere .\nBased on Theorem 3  ###reference_orem3###, we have the following corollary.\nWhen the watermarking method is more random for non-AI-generated content, i.e.,  is closer to 0, the upper bound of FDR is smaller.\nImpact of  on the bounds:  Intuitively, when there are more users, i.e.,  is larger, it is more likely to have at least one user whose watermark has a bitwise accuracy with the decoded watermark  that is no smaller than . As a result, both TDR and FDR may increase as  increases, i.e.,  controls a trade-off between TDR and FDR. Our theoretical results align with this intuition. On one hand, our Theorem 1  ###reference_orem1### shows that the lower bound of TDR is larger when  is larger. In particular, when  increases, the parameter  may become smaller. Therefore, the second term of the lower bound increases, leading to a larger lower bound of TDR. On the other hand, the upper bound of FDR in both Theorem 2  ###reference_orem2### and Theorem 3  ###reference_orem3### increases as  increases. In particular, in Theorem 2  ###reference_orem2###, the parameter  becomes larger when  increases, leading to a larger second term of the upper bound.\nUser-agnostic vs. user-aware detection: \nExisting watermark-based detection is user-agnostic, i.e., it does not distinguish between different users when embedding a watermark into an AI-generated content. The first term of the lower bound in our Theorem 1  ###reference_orem1### is a lower bound of TDR for user-agnostic detection; the first term of the upper bound in our Theorem 2  ###reference_orem2### is an upper bound of FDR for user-agnostic detection; and the upper bound with  in our Theorem 3  ###reference_orem3### is an alternative upper bound of FDR for user-agnostic detection. Therefore, compared to user-agnostic detection, our user-aware detection achieves larger TDR but also larger FDR."
        },
        {
            "section_id": "5.5",
            "parent_section_id": "5",
            "section_name": "Attribution Performance",
            "text": "Suppose we are given a user \u2019s AI-generated content .\nIntuitively, if the watermark  is very dissimilar to the other  watermarks, i.e.,  is small, then  can be correctly attributed to  once  is detected as AI-generated, i.e., the decoded watermark  is similar enough to . If the watermark  is similar to some other watermark, i.e.,  is large, then the decoded watermark  has to be very similar to  in order to correctly attribute  to . Formally, we can derive a lower bound of TAR in the following theorem.\nSuppose we are given  users with any  watermarks . When the watermarking method is -accurate for user \u2019s AI-generated content, we have a lower bound of TAR as follows:\nwhere  follows a binomial distribution with parameters  and , i.e., , ,  is the watermark length, and  is the detection threshold.\nOur Theorem 4  ###reference_orem4### shows that the lower bound of TAR is larger when  is closer to 1, i.e., attribution performance is better when the watermarking method is more accurate. Moreover, the lower bound is larger when  is smaller because it is easier to distinguish between users. This is a theoretical motivation on why our watermark selection problem aims to select watermarks for the users such that they have small pairwise bitwise accuracy.\nDetection implies attribution:  When , the lower bound of TAR in Theorem 4  ###reference_orem4### becomes TAR. The second term of the lower bound of TDR in Theorem 1  ###reference_orem1### is usually much smaller than the first term. In other words, the lower bound of TDR is also roughly . Therefore, when  is large enough (i.e., ), TDR and TAR are very close, which is also confirmed in our experiments. This result indicates that once an AI-generated content is correctly detected, it would also be correctly attributed."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In our major experiments, we focus on detection and attribution of AI-generated images. In Section 7  ###reference_###, we also show results for AI-generated texts."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Experimental Setup",
            "text": "Datasets:  We consider both AI-generated and non-AI-generated images as follows:\nAI-generated. We consider three GenAI models, i.e., Stable Diffusion, Midjourney, and DALL-E 2, which correspond to three datasets of AI-generated images. For Stable Diffusion, we use publicly available dataset DiffusionDB [26  ###reference_bx26###]. For Midjourney, we collect its generated images from a website [27  ###reference_bx27###]. For DALL-E 2, we also collect its generated images from a website [28  ###reference_bx28###]. Following HiDDeN [11  ###reference_bx11###], for each dataset, we sample 10,000 images for training watermark encoders and decoders; and we sample 1,000 images for testing the performance of watermark-based detection and attribution.\nNon-AI-generated. To evaluate the likelihood that a non-AI-generated image is falsely detected as AI-generated, we need non-AI-generated images. For this purpose, we combine the images in three benchmark datasets, including COCO [29  ###reference_bx29###], ImageNet [30  ###reference_bx30###], and Conceptual Caption [31  ###reference_bx31###], and sample 1,000 images from the combined set uniformly at random as our non-AI-generated image dataset.\nWe scale the image size in all datasets to be 128  128.\nWatermarking method:  We use the state-of-the-art learning-based watermarking method HiDDeN [11  ###reference_bx11###]. Unless otherwise mentioned, we use standard training with the default parameter settings in the publicly available code, except that we use ResNet18 as the decoder to enlarge the capacity to encode/decode longer watermarks. For each GenAI model, we train a watermark encoder/decoder using the corresponding AI-generated image training set and evaluate the detection and attribution performance on the testing set.\nWatermark selection methods:  We evaluate Random, NRG, and A-BSTA watermark selection methods. Unless otherwise mentioned, we use A-BSTA. Note that we do not use BSTA because it is not scalable. For instance, it takes BSTA more than 8 hours to generate even 16 watermarks.\n###figure_3### Evaluation metrics:  As discussed in Section 5.2  ###reference_###, we mainly use three evaluation metrics, i.e., True Detection Rate (TDR), False Detection Rate (FDR), and True Attribution Rate (TAR). FDR is the fraction of the 1,000 non-AI-generated images that are falsely detected as AI-generated. FDR does not depend on users. In contrast, TDR and TAR depend on users because they use different watermarks, leading to different distributions of AI-generated images. For each of the  users, we embed its watermark into 100 images randomly sampled from a testing AI-generated image dataset; and then we calculate the TDR and TAR for the user.\nIn most of our experiments, we report the average TDR and average TAR, which respectively are the TDR and TAR averaged among the  users. However, average TDR and average TAR cannot reflect the detection/attribution performance for the worst-case users, i.e., some users may have quite small TDR/TAR, but the average TDR/TAR may still be very large. Therefore, we further consider the 1% users (at least 1 user) with the smallest TDR (or TAR) and report their average TDR (or TAR), which we call worst 1% TDR (or worst 1% TAR).\nParameter settings:  By default, we set the number of users , watermark length , and detection threshold . To compute TAR of an user, we need to compute the bitwise accuracy between the decoded watermark and each user\u2019s watermark for each watermarked image, and thus we set  due to our limited computation resources, but we will also explore  in one of our experiments to show the results when the number of users in attribution is very large. When post-processing methods are applied to watermarked images, the watermarking method may become less accurate (i.e.,  may decrease) and thus we reduce  to be 0.85. Unless otherwise mentioned, we show results for the Stable Diffusion dataset."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Without Post-processing",
            "text": "In this section, we show results when the AI-generated, watermarked images are not post-processed. Specifically, we explore the impact of the three parameters, including the number of users , watermark length , and detection threshold , on the detection and attribution performance. When exploring the impact of one parameter, we fix the other two parameters as their default settings.\nMain results:  For each GenAI model, we compute the TDR/TAR of each user and the FDR. The FDRs for the three GenAI models are nearly 0. Then, we rank the users\u2019 TARs (or TDRs) in a non-descending order. Figure 3  ###reference_### shows the ranked TARs of the 100,000 users for the three GenAI models. Note that the curve of TDR overlaps with that of TAR for a GenAI model and thus is omitted in the figure for simplicity. TDR and TAR overlap because   (0.89 in our experiments), which is consistent with our theoretical analysis in Section 5.5  ###reference_### that shows detection implies attribution in such settings. Our results show that watermark-based detection and attribution are accurate when the AI-generated, watermarked images are not post-processed. Specifically, the worst TAR or TDR of a user is larger than 0.94; less than 0.1% of users have TARs/TDRs smaller than 0.98; and 85% of users have TARs/TDRs of 1 for Midjourney and DALL-E 2, and 60% of users have TARs/TDRs of 1 for Stable Diffusion.\n###table_1### ###figure_4### ###figure_5### ###figure_6### Impact of number of users : \nFigure 4a  ###reference_sf1### shows the average TDR, average TAR, worst 1% TDR, worst 1% TAR, and FDR when  varies from 10 to 1,000,000. We have two observations. First, both average TDR and average TAR are consistently close to 1, and FDR is consistently close to 0, which means our detection and attribution are accurate. Second, worst 1% TDR and worst 1% TAR decrease as  increases. This is because when there are more users, the worst 1% of them have smaller TDRs and TARs. Moreover, these worst 1% of users also make the average TDR and average TAR decrease slightly when  increases from 100,000 to 1,000,000.\nImpact of watermark length : \nFigure 4b  ###reference_sf2### shows the average TDR, average TAR, worst 1% TDR, worst 1% TAR, and FDR when the watermark length  varies from 32 to 80. The average TDR and average TAR slightly decrease when  increases from 64 to 80, while the worst 1% TDR/TAR slightly increases as  increases from 32 to 48 and then decreases as  further increases. Table 1  ###reference_### shows the estimated average  of all users and average  of the worst 1% of users in -accurate watermarking. We observe that the patterns of average TDR/TAR and worst 1% TDR/TAR are consistent with those of average  and worst 1% , respectively. These observations are consistent with our theoretical analysis which shows that TDR or TAR increases as  increases. Our result also implies that HiDDeN watermarking may be unable to accurately encode/decode very long watermarks.\nImpact of detection threshold : \nFigure 4c  ###reference_sf3### shows the average TDR, average TAR, worst 1% TDR, worst 1% TAR, and FDR when the detection threshold  varies from 0.7 to 0.95. When  increases, both TDR and TAR decrease, while FDR also decreases. Such trade-off of  is consistent with Theorem 1  ###reference_orem1###, 3  ###reference_orem3###, and 4  ###reference_orem4###.\n###figure_7### ###figure_8### ###figure_9### ###figure_10###"
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "Common Post-processing",
            "text": "Common post-processing methods:  Common post-processing methods are often used to evaluate the robustness of watermarking in non-adversarial settings. Each post-processing method has specific parameters that govern the extent of perturbation introduced to an image. In particular, we consider common post-processing methods as follows.\nJPEG. JPEG [32  ###reference_bx32###] method compresses an image via a discrete cosine transform. The perturbation introduced to an image is determined by the quality factor . An image is perturbed more when  is smaller.\nGaussian noise. This method perturbs an image via adding a random Gaussian noise to each pixel. In our experiments, the mean of the Gaussian distribution is 0. The perturbation introduced to an image is determined by the parameter standard deviation .\nGaussian blur. This method blurs an image via a Gaussian function. In our experiments, we fix kernel size . The perturbation introduced to an image is determined by the parameter standard deviation .\nBrightness/Contrast. This method perturbs an image via adjusting the brightness and contrast. Formally, the method has contrast parameter  and brightness parameter , where each pixel  is converted to . In our experiments, we fix  and vary  to control the perturbation.\nAdversarial training [11  ###reference_bx11###]:  We use adversarial training to train HiDDeN. Specifically, during training, we randomly sample a post-processing method from no post-processing and common post-processing with a random parameter to post-process each watermarked image in a mini-batch. Following previous work [11  ###reference_bx11###], we consider the following range of parameters during adversarial training:  [10, 99] for JPEG,  [0, 0.5] for Gaussian noise,  [0, 1.5] for Gaussian blur, and  [1, 20] for Brightness/Contrast.\nResults:  Figure 5  ###reference_### shows the detection/attribution results when a common post-processing method with different parameters is applied to the (AI-generated and non-AI-generated) images. SSIM [33  ###reference_bx33###] is a popular metric to measure visual similarity between two images. The SSIM in Figure 5  ###reference_### is the average between (AI-generated and non-AI-generated) images and their post-processed versions.\nWe note that when HiDDeN is trained using standard training, detection and attribution become inaccurate after AI-generated images are post-processed, as shown in Figure 10  ###reference_### in Appendix. Our results show that detection and attribution using an adversarially trained HiDDeN are robust to common post-processing. In particular, the average TDR and TAR are still high when a common post-processing does not sacrifice image quality substantially. For instance, average TDR and TAR start to decrease when the quality factor  of JPEG is smaller than 90. However, the average SSIM between watermarked images and their post-processed versions also drops quickly. Note that Gaussian blur with  already influences visual quality substantially even if SSIM is larger than 0.75. Figure 11  ###reference_### in Appendix shows a watermarked image and the versions post-processed by different methods.\n###figure_11###"
        },
        {
            "section_id": "6.4",
            "parent_section_id": "6",
            "section_name": "Adversarial Post-processing",
            "text": "In adversarial settings, an attacker may apply adversarial post-processing [16  ###reference_bx16###] to perturb a watermarked image to evade detection/attribution. HiDDeN is not robust to adversarial post-processing in the white-box setting [16  ###reference_bx16###], i.e., adversarial post-processing can remove the watermark from a watermarked image without sacrificing its visual quality. Thus, HiDDeN-based detection/attribution is also not robust to adversarial post-processing in the white-box setting, i.e., TDR/TAR can be reduced to 0 while maintaining image quality.\nFigure 6  ###reference_### shows the average SSIM between watermarked images and their adversarially post-processed versions as a function of query budget in the black-box setting (i.e., WEvade-B-Q [16  ###reference_bx16###]), where the query budget is the number of queries to the detection API for each watermarked image. HiDDeN is trained via adversarial training in these experiments. Both TDR and TAR are 0 in these experiments since WEvade-B-Q always guarantees evasion [16  ###reference_bx16###]. However, adversarial post-processing substantially sacrifices image quality in the black-box setting (i.e., SSIM is small) even if an attacker can query the detection API for a large number of times. Figure 12  ###reference_### in Appendix shows several examples of adversarially post-processed images with degraded visual quality. Our results show that HiDDeN and thus our HiDDeN-based detection/attribution have good robustness to adversarial post-processing in the black-box setting.\nWe note that Jiang et al. [16  ###reference_bx16###] showed adversarial post-processing does not sacrifice image visual quality in the black-box setting when evading HiDDeN, which we can reproduce using their publicly available code and the same parameter setting. However, they use watermark length 30, while we use 64; and they use a simple neural network as the decoder, while we use ResNet18 as the decoder. Moreover, we use stronger adversarial training with a larger range of parameters for the post-processing. Our results show that longer watermarks, more expressive decoder, and stronger adversarial training can further enhance robustness of HiDDeN.\n###figure_12### ###figure_13###"
        },
        {
            "section_id": "6.5",
            "parent_section_id": "6",
            "section_name": "Different Watermark Selection Methods",
            "text": "Running time:  Table 2  ###reference_### shows the running time to generate a watermark averaged among the 100,000 watermarks. Although A-BSTA is slower than Random and NRG, the running time is acceptable, i.e., it takes only 24ms to generate a watermark on average.\n###table_2### Distribution of :  Recall that TAR of a user depends on the maximum bitwise accuracy between the watermark  and the remaining watermarks, i.e., . Figure 7a  ###reference_sf1### shows the cumulative distribution function of  among the  watermarks generated by different watermark selection methods. Our results show that all watermarks generated by A-BSTA have  smaller than 0.74. However, Random and NRG generate many watermarks with larger , and Random is the worst among the three methods. This is because Random selection does not explicitly minimize  when generating watermarks.\nTARs:  Figure 7b  ###reference_sf2### shows the ranked TARs of the worst 1,000 users, where the AI-generated images are post-processed by JPEG compression with quality factor  and HiDDeN is adversarially trained. The results indicate that A-BSTA outperforms NRG, which outperforms Random. This is because A-BSTA selects watermarks with smaller , while Random selects watermarks with larger  as shown in Figure 7a  ###reference_sf1###."
        },
        {
            "section_id": "6.6",
            "parent_section_id": "6",
            "section_name": " Theoretical vs. Empirical Results",
            "text": "The theoretical lower bounds of TDR and TAR of a user are respectively calculated using Theorem 1  ###reference_orem1### and 4  ###reference_orem4###, while the theoretical upper bound of FDR is calculated using Theorem 3  ###reference_orem3###. We estimate  as the bitwise accuracy between the decoded watermark and  averaged among the testing AI-generated images, and estimate  using the fraction of bits in the decoded watermarks that are 1 among the non-AI-generated images. Figure 8  ###reference_### shows the average theoretical vs. empirical TDR/TAR, and theoretical vs. empirical FDR, when no post-processing or JPEG with  is applied. The results show that our theoretical lower bounds of TDR and TAR match with empirical results well, which indicates that our derived lower bounds are tight. The theoretical upper bound of FDR is notably higher than the empirical FDR. This is because some bits may have larger probabilities to be 1 or 0 in the experiments, but our theoretical analysis treats the bits equally, leading to a loose upper bound of FDR.\n###figure_14### ###figure_15###"
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": " Discussion and Limitations",
            "text": "AI-generated texts:  Our method can also be used for the detection and attribution of AI-generated texts. For text watermarking, we use a learning-based method called Adversarial Watermarking Transformer (AWT) [22  ###reference_bx22###]. Given a text, AWT encoder embeds a bitstring watermark into it; and given a (watermarked or unwatermarked) text, AWT decoder decodes a watermark from it. Following the original paper [22  ###reference_bx22###], we train AWT on the word-level WikiText-2 dataset, which is derived from Wikipedia articles [34  ###reference_bx34###]. We use most of the hyperparameter settings in the publicly available code of AWT except the weight of the watermark decoding loss. To optimize watermark decoding accuracy, we increase this weight during training. The detailed hyperparameter settings for training can be found in Table 4  ###reference_### in Appendix.\n###figure_16### ###figure_17### We use A-BSTA to select users\u2019 watermarks. For each user, we sample 10 text segments from the test corpus uniformly at random, and perform watermark-based detection and attribution. Moreover, we use the unwatermarked test corpus to calculate FDR. Figure 9  ###reference_### shows the detection and attribution results when there is no post-processing and paraphrasing [35  ###reference_bx35###] is applied to texts, where , , and  ranges from 10 to 100,000. Due to the fixed-length nature of AWT\u2019s input, we constrain the output length of the paraphraser to a certain range. When paraphrasing is used, we extend adversarial training to train AWT, and Section G  ###reference_### in Appendix shows the details. Note that the average TDR/TAR and FDR are all nearly 0 when AWT is trained by standard training and paraphrasing is applied to texts.\nThe results show that our method is also applicable for AI-generated texts, and adversarially trained AWT has better robustness to paraphrasing.\nAttribution of GenAI services:  In this work, we focus on attribution of content to users for a specific GenAI service. Another relevant attribution problem is to trace back the GenAI service (e.g., Google\u2019s Imagen, OpenAI\u2019s DALL-E 3, or Stable Diffusion) that generated a given content. Our method can also be applied to such GenAI-service-attribution problem by assigning a different watermark to each GenAI service. Moreover, we can perform attribution to GenAI service and user simultaneously. Specifically, we can divide the watermark space into multiple subspaces; and each GenAI service uses a subspace of watermarks and assigns watermarks in its subspace to its users. In this way, we can trace back both the GenAI service and its user that generated a given content."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Conclusion and Future Work",
            "text": "We find that watermark can be used for user-aware detection and attribution of AI-generated content. Moreover, via both theoretical analysis and empirical evaluation, we find that such detection and attribution inherit the accuracy/(non-)robustness properties of the watermarking method. For instance, learning-based watermarking methods [11  ###reference_bx11###] are accurate and robust to common post-processing; and thus detection and attribution based on such a watermarking method are also accurate and robust to common post-processing. However, since watermarking is not yet robust to adversarial post-processing in the white-box setting [16  ###reference_bx16###], detection and attribution are not yet robust in such adversarial settings. We also find that selecting dissimilar watermarks for the users enhances attribution performance. An important future work is to develop robust watermarking methods in adversarial settings."
        }
    ],
    "url": "http://arxiv.org/html/2404.04254v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "3",
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4",
            "4.4.1",
            "4.4.2"
        ],
        "main_experiment_and_results_sections": [
            "6",
            "6.1",
            "6.2",
            "6.3",
            "6.4",
            "6.5",
            "6.6"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "6.2",
            "6.3",
            "6.4",
            "6.5"
        ]
    },
    "research_context": {
        "paper_id": "2404.04254v1",
        "paper_title": "Watermark-based Detection and Attribution of AI-Generated Content",
        "research_background": "**Motivation:**\n\nThe motivation behind this paper stems from the growing ethical concerns associated with the use of Generative AI (GenAI) technologies such as DALL-E 3, Midjourney, and ChatGPT. While these technologies have significant societal benefits, they also have the potential to be misused. For example, they can generate harmful content and aid disinformation and propaganda campaigns by producing realistic-looking media. Additionally, there is the risk of individuals falsely claiming copyright ownership of AI-generated content. These issues necessitate effective mechanisms for detecting and attributing AI-generated content to prevent misuse and assist in forensic analysis of cyber-crimes.\n\n**Research Problem:**\n\nThe primary research problem addressed in this paper is the need for effective watermark-based detection and attribution of AI-generated content. While watermark-based detection techniques have been deployed by several companies to identify AI-generated content, these techniques typically do not account for user-specific attribution. Attribution, in this context, is the ability to trace back the AI-generated content to the specific registered user of the GenAI service who created it. This capability is important for forensic analysis but has been largely unexplored in existing literature. Therefore, the paper aims to systematically study watermark-based detection and attribution methods that can attribute AI-generated content to individual users.\n\n**Relevant Prior Work:**\n\n1. **Existing Watermarking Deployments:** Companies like Google, OpenAI, Stability AI, and Microsoft have already employed watermarking methods to mark AI-generated images. For example:\n   - OpenAI uses a visible watermark in DALL-E 2.\n   - Google's SynthID inserts an invisible watermark into images from its Imagen.\n   - Stability AI deploys watermarking in its Stable Diffusion.\n   - Microsoft watermarks all AI-generated images in Bing.\n\n2. **User-Agnostic Detection:** Current literature and industry practices predominantly focus on detecting AI-generated content using a universal watermark, applied uniformly across all generated content. The detection process checks if a similar watermark can be decoded from the content, but does not provide user-specific attribution.\n\n3. **Robust Watermarking Methods:** There is also significant prior work on developing robust watermarking methods, especially learning-based approaches, to resist common post-processing techniques like JPEG compression, Gaussian blur, and brightness/contrast adjustments. The robustness of these methods against adversarial post-processing, particularly in black-box settings, is also under active investigation.\n\n4. **Theoretical Foundations:** The niche field of non-learning-based image watermarking has been explored for decades, with recent advancements in learning-based methods making significant progress in creating robust watermarks against common and adversarial post-processing techniques.\n\nIn summary, the paper builds on existing watermarking techniques while extending them to incorporate user-specific attribution, which is a largely unexplored area with significant implications for forensic analysis and the mitigation of AI-generated content misuse.",
        "methodology": "### Methodology: \n\n**Problem setup:**  \nSuppose we are given a generative AI model, which is deployed as a GenAI service. A registered user sends a prompt (i.e., a text) to the GenAI service, which returns AI-generated content to the user. The content can be image, text, or audio. In this work, we consider detection and attribution of AI-generated content. \n\n- **Detection** aims to decide whether a given content was generated by the GenAI service or not.\n- **Attribution** further traces back the user of the GenAI service who generated content detected as AI-generated.\n\nSuch attribution can aid the GenAI service provider or law enforcement in forensic analysis of cyber-crimes, such as disinformation or propaganda campaigns that involve a given AI-generated content. \n\nWe formally define the detection and attribution problems as follows:\n\n1. **Detection:** Given a content and a GenAI service, detection aims to infer whether the content was generated by the GenAI service or not.\n   \n2. **Attribution:** Given a content, a GenAI service, and users of the GenAI service, attribution aims to further infer which user used the GenAI service to generate the content after it is detected as AI-generated.\n\nThe set of users in attribution could include all registered users of the GenAI service, which may be very large. Alternatively, this set may consist of a smaller number of registered users if the GenAI service provider has some prior knowledge of its registered users. For instance, the GenAI service provider may exclude the registered users, who are verified offline as trusted, to reduce its size. How to construct the set of users in attribution is out of the scope of this work. Given any set, our method aims to infer which user in the set may have generated a given content. \n\nWe also note that another relevant attribution problem is to trace back the GenAI service that generated a given content. Our method can also be used for such GenAI-service attribution, which we discuss in Section 7.\n\n**Threat model:**  \nAn AI-generated, watermarked content may be post-processed by some common post-processing techniques in non-adversarial settings. For instance, an image may be post-processed by JPEG compression during transmission on the Internet, or a user may use Gaussian blur or Brightness/Contrast to edit an image in an image editor. In adversarial settings, a malicious user may post-process an AI-generated content to evade detection and/or attribution. \n\nOther than the common post-processing techniques, a malicious user may also use adversarial post-processing to remove the watermark in an AI-generated content. We assume the watermark encoder/decoder is private and the malicious user has limited access to the detection API, in which state-of-the-art watermarking methods have good robustness to post-processing. Such a threat model arises when a GenAI service provider restricts the access of its detection API to a small set of trusted customers. For example, Google\u2019s SynthID adopts this threat model. Note that our theoretical analysis in Section 5 can explicitly quantify and incorporate the impact of post-processing on the detection and attribution performance.",
        "main_experiment_and_results": "Main Experiment Setup\n\n**Datasets:** \nFor the primary experiments, we utilize a diverse collection of datasets comprising both AI-generated and real-world images. These datasets encompass a variety of imagery aimed at testing the robustness and accuracy of our watermark-based detection and attribution system.\n\n**Baselines:** \nTo ensure comprehensive evaluation, we compare our proposed method against several baseline methods. These include other state-of-the-art watermarking techniques and non-watermark-based detection mechanisms. The baselines are carefully chosen to represent a wide array of current methodologies in AI-generated content detection and attribution.\n\n**Evaluation Metrics:** \nTo objectively assess the performance of our watermark-based system, we employ several evaluation metrics:\n- **Precision and Recall:** These metrics help measure the accuracy of detection, with precision indicating the proportion of true positive identifications, and recall indicating the proportion of actual positives correctly identified.\n- **F1 Score:** The harmonic mean of precision and recall, offering a single metric that balances the two.\n- **Detection Rate:** The rate at which AI-generated images are correctly detected.\n- **Attribution Accuracy:** The accuracy rate at which the source AI model of an image is correctly identified when a watermark is detected.\n\nMain Experiment Results\n\nThe results of our main experiments demonstrate the effectiveness of our watermark-based system in both detecting and attributing AI-generated images. Our methodology outperforms traditional and state-of-the-art baselines across all key metrics:\n\n- **Detection Rate:** Our system achieves a significantly high detection rate, identifying over 95% of AI-generated images accurately. This is a notable improvement over baseline methods, which range between 70% to 85%.\n- **Precision and Recall:** The precision and recall of our system are both above 90%, resulting in high F1 scores that reflect the balanced and robust performance of our detection mechanism.\n- **Attribution Accuracy:** Our watermark-based system attributes the correct source AI model with an accuracy rate exceeding 92%. This ensures effective traceability of AI-generated content back to its origin.\n\nThese results substantiate the reliability and efficacy of our proposed watermark-based detection and attribution framework for AI-generated images, setting a new benchmark in the field.\n\n(Please note that results for AI-generated texts are included in Section 7 and are not covered in this summary of the main experiment.)"
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To explore how different parameters (number of users, watermark length, and detection threshold) impact the detection and attribution performance when AI-generated, watermarked images are not post-processed.",
            "experiment_process": "For each Generative AI (GenAI) model, the True Detection Rate (TDR) and True Attribution Rate (TAR) of each user were computed, along with the False Detection Rate (FDR). Various parameters were explored by fixing two parameters and varying the third: the number of users (ranging from 10 to 1,000,000), watermark length (from 32 to 80), and detection threshold (from 0.7 to 0.95). The performance metrics were plotted to observe their trends.",
            "result_discussion": "The main findings showed that the watermark-based detection and attribution were accurate without post-processing. For Midjourney and DALL-E 2, 85% of users recorded a TAR/TDR of 1, while Stable Diffusion had 60% users with a TAR/TDR of 1. The study revealed that the worst 1% TAR and TDR decrease as the number of users increases, longer watermarks impact encoding/decoding accuracy, and higher detection thresholds result in decreasing TDR/TAR and FDR.",
            "ablation_id": "2404.04254v1.No1"
        },
        {
            "research_objective": "To evaluate the robustness of watermark-based detection and attribution against common post-processing methods applied to AI-generated images.",
            "experiment_process": "Common post-processing methods such as JPEG compression, Gaussian noise, Gaussian blur, and Brightness/Contrast adjustments were applied to both AI-generated and non-AI-generated images. Each method involved varying a specific parameter to control the extent of image perturbation. Additionally, adversarial training was used to enhance robustness during these experiments.",
            "result_discussion": "Detection and attribution became inaccurate for standard-trained HiDDeN when images were post-processed. However, with adversarial training, HiDDeN was robust against common post-processing. Performance decline was observed only when post-processing significantly affected image quality, such as with very high levels of JPEG compression or Gaussian blur.",
            "ablation_id": "2404.04254v1.No2"
        },
        {
            "research_objective": "To determine the robustness of HiDDeN against adversarial post-processing methods applied to AI-generated images in adversarial settings.",
            "experiment_process": "HiDDeN's robustness was tested against adversarial post-processing methods aiming to evade detection/attribution. The study observed the Structural Similarity Index Measure (SSIM) between original and post-processed images, with a focus on the black-box setting where the attacker could query the detection API multiple times.",
            "result_discussion": "HiDDeN-based detection/attribution was found to be non-robust in white-box settings, as adversarial post-processing could remove watermarks without degrading image quality. However, in black-box settings, adversarial post-processing significantly reduced image quality, even with extensive querying of the detection API. Enhanced by stronger adversarial training and more sophisticated decoders, longer watermarks proved to bolster robustness against such attacks.",
            "ablation_id": "2404.04254v1.No3"
        },
        {
            "research_objective": "To compare different watermark selection methods and their impact on detection and attribution performance.",
            "experiment_process": "Three watermark selection methods\u2014A-BSTA, Random, and NRG\u2014were compared in terms of their running time to generate watermarks, the distribution of maximum bitwise accuracy (\u03b7), and the resulting TAR of users. The experiment involved 100,000 generated watermarks, with the performance assessed when images were post-processed by JPEG compression.",
            "result_discussion": "A-BSTA showcased the most efficient watermark selection, generating watermarks with \u03b7 smaller than 0.74 consistently, compared to Random and NRG which produced watermarks with larger \u03b7. In terms of TAR, A-BSTA performed the best, followed by NRG and Random, confirming that the method explicitly minimizing \u03b7 results in improved attribution accuracy.",
            "ablation_id": "2404.04254v1.No4"
        }
    ]
}