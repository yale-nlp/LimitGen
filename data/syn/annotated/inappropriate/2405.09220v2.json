{
    "title": "ALPINE: Unveiling the Planning Capability of Autoregressive Learning in Language Models",
    "abstract": "In this paper, we present the findings of our Project ALPINE which stands for \u201cAutoregressive Learning for Planning In NEtworks.\u201d\nProject ALPINE initiates a theoretical investigation\ninto the development of planning capabilities in Transformer-based language models through their autoregressive learning mechanisms,\naiming to identify any potential limitations in their planning abilities.\nWe abstract planning as a network path-finding task where the objective is to generate a valid path from a specified source node to a designated target node.\nIn terms of expressiveness,\nwe show that the Transformer is capable of executing path-finding by embedding the adjacency and reachability matrices within its weights.\nOur theoretical analysis of the gradient-based learning dynamic of the Transformer reveals\nthat the Transformer is capable of learning both the adjacency matrix and a limited form of the reachability matrix.\nThese theoretical insights\nare then validated through experiments, which demonstrate that the Transformer indeed learns the adjacency matrix and an incomplete reachability matrix,\nwhich aligns with the predictions made in our theoretical analysis.\nAdditionally, when applying our methodology to a real-world planning benchmark, called Blocksworld, our observations remain consistent.\nOur theoretical and empirical analyses further unveil a potential limitation of Transformer in path-finding: it cannot identify reachability relationships through transitivity, and thus would fail when path concatenation is needed to generate a path.\nIn summary, our findings shed new light on how the internal mechanisms of autoregressive learning enable planning in networks. This study may contribute to our understanding of the general planning capabilities in other related domains.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large language models (LLMs), such as ChatGPT, have impressed everyone with their powerful capabilities in multi-faceted tasks spanning language processing, knowledge extraction, reasoning, planning, coding, tool use, and more.\nThe broad spectrum of intelligent capabilities exhibited by LLMs\nreflects promising signs of\nartificial general intelligence (AGI) [BCE+23  ###reference_bx5###] and catalyzes an AI revolution: Individuals and organizations are now striving to develop more powerful and adaptive\nAI models towards AGI, while also integrating LLM-based AI models into\nvarious aspects of our work and daily lives.\nHowever, at the same time, we are still\nintrigued by the underlying mechanisms\nthat fuel the power of LLMs.\nWhile all current LLMs are\nbuilt upon the Transformer neural network architecture,\nwhich employs autoregressive learning to predict the next word in a language sequence, the question remains:\nWhy does the Transformer-based autoregressive learning architecture produce such exceptional performance across a wide range of intelligent tasks?\nTo put it in plain English: Why does next word prediction generate intelligence?\nThere is no definite answer to this question yet. But researchers are tackling this problem from various angles,\naiming to provide explanations to the power of LLMs.\nIn this paper, we focus on the planning capability of LLMs.\nPlanning is a fundamental\nconstruct\nof human intelligence and is involved in almost every aspect of our daily life, e.g., planning a task at work, organizing a trip,\nseeking a mathematical proof of a theorem, etc.\nUnderstanding how LLMs completes a planning task\ncan shed light on\nthe transformation of the seemingly low-level statistical task of next word prediction\ninto a high-level intelligent task like planning.\nThis understanding may serve as a potential pathway to\ncomprehend and explain other intelligent behaviors\nexhibited by LLMs.\nThere are already a number of studies on the planning capabilities of LLMs.\nBut most of them\nmainly focus on the empirical evaluation of LLMs.\nAlthough these studies may show some evidences that LLMs have planning capabilities\nto a limited extent, the results are partial and\ndo not explain why LLMs can or cannot\nsuccessfully accomplish specific planning tasks\n(see Section 7  ###reference_### for more detailed discussions on the related work).\nIn light of this context, we initiated Project ALPINE, which stands for Autoregressive Learning for Planning In NEtworks.\nProject ALPINE aims to not only empirically evaluate the planning performance of LLMs but also provide theoretical interpretations on how LLMs accomplish such tasks.\nTo provide a solid foundation for our investigation, we must define a specific task that serves as a representative example of a planning task.\nGiven that planning often entails making\nsequential selections of next steps within a multi-step procedure to achieve a desired goal, it naturally\nrelates to the path-finding task in networks.\nA complex task\nis often represented as a task graph, where nodes\ncorrespond to subtasks or intermediate stages, and edges represent the ordering relationships\nbetweeb these subtasks.\nTask planning\ninvolves finding a valid path\nwithin the task graph to reach a pre-determined goal.\nFor example, project planning can be viewed as navigating through the multi-stage project planning graph, while a mathematical proof can be seeing as a path from the axioms to the final theorem,\nwith lemmas serving as intermediate nodes.\nMany previous studies on LLM planning capabilities are also related to path finding.\nFor example, a benchmark planning game called Blocksworld [VMSK24  ###reference_bx21###] used for evaluating LLMs\ncan be viewed as\npath finding from the initial blocks\u2019 state to the final blocks\u2019 state in a state transition graph.\nSimilarly, HuggingGPT [SST+23  ###reference_bx18###] for scheduling API calls can\nbe likened to finding a call path in the API call graph.\nIn Project ALPINE, we investigate the following path-finding task: given an underlying graph,\nthe training data\nconsists of a collection of paths in the graph\nthat specify the source node , the target node  and a path from \nto .\nThe objective of the test is to generate a path from  to , given new source-target pair .\nNote that, for this path-finding task, the generative model\nmust generate a valid path in one shot without relying on trial and error.\nIn this case,\nthe key challenge, when giving the current node on the path, lies in correctly identifying the next node on the path, and\nthis node should be both adjacent to the current node and reachable to the target node\n(see Algorithm 1  ###reference_###).\nThis suggests that\nin order to accomplish the path-finding task, it is essential to extract the information\nabout the adjacency and reachability of the graph from the training data.\nOur investigation below demonstrates that the Transformer model is indeed performing this extraction to a certain extent.\nMore specifically, we investigate how the Transformer-based autoregressive learning architecture achieves the path-finding task by examining the following aspects.\nFirst, we show that the Transformer architecture has the expressive power to complete the task by manually constructing a Transformer that encodes the adjacency matrix and reachability matrix of the network as part of its model.\nSecond, we conduct theoretical analysis\nto further explore the capabilities of the Transformer model.\nFor a simplified Transformer model, when applying gradient descent to minimize the cross-entropy loss on the path training data, our analysis reveals that\nthe model\ncan extract the adjacency matrix and a limited form of the reachability matrix.\nSpecifically, the feed-forward layer encodes the adjacency matrix,\nwhile the attention layer captures a partial form of the reachability matrix.\nThis process mimics human intelligence in generating the next node that is both adjacent to the current node and reachable to the target node.\nHowever, our theoretical analysis also reveals a limitation of the Transformer model. It cannot learn the full\ncomplete reachability relationship.\nParticularly the reachability\nderived from transitivity cannot be learned by the model as the Transformer falls short in capturing these complex reachability patterns.\nThird, we conduct extensive experiments that train Transformer models on the path language through autoregressive learning, and test its capability\nfor generating valid paths for new pairs of source and target nodes.\nOur empirical results\nprovide compelling evidence that the Transformer can excel in achieving high accuracy in the path-finding task.\nThese findings also align with our theoretical analysis as they show\nthat the trained Transformer model generates the next node on the path by focusing its attention on the target node, and effectively learns the adjacency and reachability matrices in its feed-forward layer and attention layer, respectively. Moreover, we observe\na significant drop in test accuracy\nwhen the source and target can only be connected through concatenation of path segments in the training data.\nThis indicates the requirement for\nhigher-order transitive relationship to establish reachability.\nThis matches our theoretical analysis, showing that the Transformer indeed has limitation in learning transitive reachability relationships.\nFinally, we demonstrate that the Transformer can successfully learn a task planning benchmark called Blocksworld [VMSK24  ###reference_bx21###], a planning task that corresponds directly to the path-finding problem.\nIn summary, our investigation in Project ALPINE\nmakes the following contributions:\n(a) We\nhave initiated a theoretical analysis\nthat explains how Transformer achieves a path-planning task through its gradient descent autoregressive learning mechanism;\n(b) Our empirical evaluation corroborates\nour theoretical analysis and clearly demonstrates how the Transformer\naccomplishes path planning by extracting the adjacency and reachability information,\nwhile focusing\nattention on the target node.\n(c) Both our theoretical and empirical analyses\nuncover a limitation of the Transformer architecture, highlighting its inability to handle transitive reachability relationship in the path-finding task, which\nholds significant implications.\nOur findings\nrepresent an initial step toward\nunraveling the underlying mechanism that empowers the intelligent\nof LLMs.\nWe believe that\nthis meaningful first step\nbrings us closer to achieving our ultimate\nobjective.\nWe hope that our findings,\nalong with our integrated theoretical and\nempirical approach, will prove valuable\nto the community, facilitating collective\nprogress\nin understanding\nLLMs and\ndriving improvements future generations of\nthese models.\nThe rest of the paper is organized as follows.\nIn Section 2  ###reference_###, we provide the preliminaries for the study.\nSection 3  ###reference_### presents\nan overview of our technical results, including the expressive power of the Transformer model in the path-finding task.\nIn Section 4  ###reference_###,\nwe present our theoretical analysis on how a simplified Transformer model solves the path-finding task.\nSection 5  ###reference_### provides the detailed empirical evaluation results that reinforce\nour theoretical analysis.\nSection 6  ###reference_### highlights our findings on the Blocksworld task, which are consistent with our main findings.\nFollowing that, in Section 7  ###reference_###, we provide an overview of related work and discuss the implications of our results.\nFinally, we conclude the paper in Section 8  ###reference_###."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Preliminaries",
            "text": "Throughout this paper,\nwe use the following notations for matrices and vectors:  and  stand for a column vector and a matrix, respectively.\nNotations  and  denote the  entry of vector  and the  entry in\nmatrix , respectively.\nWe also denote the  row of matrix  by  and the transpose of  by ."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Auto-regressive Transformer Architecture and Loss Function",
            "text": "In this paper, we adopt the NanoGPT architecture. Let  denote the sequence length,  the embedding size,  the number of heads,  the embedding size per head, and  the vocabulary size.\nOne key component of the architecture is its attention mechanism, which is formulated as\nwhere , ,  are the query, key, and value matrices, respectively.\nFunction softmax takes a vector  and transforms it into  with .\nWhen softmax applies to a matrix, it applies to every row of the matrix.\nDenoting  as input, the multi-head attention is computed as\nwhere , ,  are the learnable weight matrices for the query, key, and value matrices of the  head.\nThe feed-forward layer is a two-layer multi-layer perceptron (MLP) defined as\nwhere , , , and  are the learnable weight matrices of FFN, and\n denotes the all-one matrix with dimension .\nFinally, one-layer Transformer is defined as\nwhere  and  are two layer normalizations. The layer normalization is defined as , where the expectation  and standard deviation  are averaged across all terms in , , and  are two learnable scalars.\nWith these essential components\nin place, we\nproceed to introduce the procedures of GPT. The training data consists of a sequence of tokens , where  is the token id for the  token. We first represent the tokens by the one-hot embedding matrix , where  and  elsewhere. Then there is a learnable token embedding matrix \nand positional embedding matrix , and the input .\nThis input  is fed into an -layer Transformer\nto obtain the predicted next word111The learnable weight matrices of different layers are different, and thus the layer index  should be\nadded as a subscript to these matrices. But our later analysis is only on a one-layer Transformer.\nThus we omitted this layer index.:\nFinally, the output embedding goes through another layer normalization , and then it is multiplied by a learnable output weight matrix  to convert back to probability weights\nover all possible tokens.\nWe calculate the output probability vector at position , denoted as , to predict the next token for position , which reflects the auto-regressive learning paradigm:\nThe actual token  is sampled\naccording to the probability vector  and a temperature parameter.\nWhen temperature parameter is set to , which is what we use throughout the paper, the sample is directly\nproportionally to\nthe probability value in .\nThe adopted loss function is the cross-entropy loss for the next token prediction\ngiven by"
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Path Planning Dataset",
            "text": "The dataset is designed to test GPT\u2019s path planning capability on simple graphs.\nWe consider a directed graph , where  is the set of nodes, and  is the set of directed edges, i.e.,\nfor any ,  means that there is an edge from node  to node  in .\nA pair of source node  and target node  is considered as a valid pair if\n contains least one path from  to .\nWe\nallocate a\nportion\nof valid  pairs to the training dataset and\nassign the remaining pairs to the test dataset.\nThe samples in the training dataset  is sequences of the format \u201c       n\u201d, where  is the source node,  is the target node,      is a valid path in  from  to , and n indicates the end of the sequence. In the test dataset, we provide only the source and target nodes in the format \u201c \u201d. The model is tasked with completing the remaining tokens in the sequence.\nThe completion is\ndeemed correct if the model generates a valid path in graph  with the correct syntax."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Path-Finding Transformers: Expressive Power and Learning Capacity",
            "text": "In this section, we present an overview of our main results.\nFirstly, in Theorem 2  ###reference_orem2### below,\nwe establish the mathematical existence of a Transformer model capable of effective path finding in any given network.\nNext, in Theorem 3  ###reference_orem3###\u2014\nthe proof of which will be the main\nfocus of Section 4  ###reference_###\u2014we characterize the learning capacity and limitations of path-finding transformer models in a data-driven gradient descent framework.\nThe empirical evaluation that supports these theoretical analyses will be discussed in Section 5  ###reference_###.\nIn our path-finding task, the essential step for completing a path is to predict the next node based on the current information. It is apparent that to predict the subsequent node on the path, only the information related to the current node and the target node is necessary.\nAlgorithm 1  ###reference_### introduces a handcrafted algorithm that utilizes both the adjacency matrix and the reachability matrix of the graph.\nThe true adjacency matrix follows the standard definition in graph theory, i.e.,\nThe true reachability matrix is defined as:\nAssuming that  is reachable by , then Algorithm 1  ###reference_### is guaranteed to output a valid path with input  and .\nTo Illustrate the expressive\ncapacities of the Transformer model in path finding,\nwe first\nshow that we can manually construct a Transformer model to perform the path planning task by simulating the idealized Algorithm 1  ###reference_###. In the manual construction, the task for the model is to find a path from a start node  to a target node  with the format \u201c\u201d. Consider every time that the Transformer takes \u201c\u201d as the input and outputs  for  (assuming  and ): if  is an out-neighbor of  and can reach  in , then we say that the Transformer outputs a correct response.\nGiven a graph  (with adjacency matrix  and reachability matrix ), for every , there exists a -layer, -head, and -embedding-size Transformer that makes correct response in every step of the above task with probability\nat least .\nFor simplicity, we omit all layer normalizations in this construction.\nBefore presenting the detailed proof, we provide a summary of our construction.\nIn essence, we\nutilize the attention layer to attend the output\nsolely\nto the target node .\nThis approach allows the distribution of next token  to become a function of both the current node  and the target node  (as formulated in Section 2  ###reference_###).\nThen, by integrating the adjacency matrix  into the MLP layer and the reachability matrix  into the matrix  in the attention layer, we extract row vectors\n and  from  and , respectively, corresponding to the target node  and current node .\nSpecifically,  and  are stored by  and , respectively.\nBy selecting proper coefficients, we can ignore the effect of the remaining term  in  and only keep a weighted sum of  and .\nFollowing the softmax layer, the non-negligible entries in the final vector correspond to the feasible next nodes.\nWith this encoding, the Transformer serves as a simulator of the idealized Algorithm 1  ###reference_### with input  and .\nWe now provide the detailed proof. Suppose the input token sequence is \u201c\u201d with , where  () and\n are the tokens of the source and target nodes,\nrespectively, and nodes \nform a path that can reach node  in graph .\nOur objective is to construct a -layer, -head Transformer model\nthat consistently generate an out-neighbor  of ,\nenabling a path from \nto  in .\nFollowing our notation in Section 2.1  ###reference_###, we adopt ,  and .\nIn the Transformer, there are  tokens representing the  nodes and the end-of-line \u2018n\u2019.\nHence, the input tokens can be represented by the one-hot embedding matrix .\nWe let \nand\n,\nhere\n represents the second unit column vector of dimension ,  is the notation for matrix concatenation by column,\nand  is a positive parameter to be decided.\nAccording to the definition of the Transformer, we now have a matrix  such that the first  columns\nare the tokens of nodes in the sequence and the last column indicates the positions of the target node .\nMore specifically, we have\nhere  represents the one-hot token vector for node  (with dimension ).\nThen we construct the attention layer of our Transformer. We only have one head and let  and . Then we can compute  (i.e. second rows are all \u2019s and other rows are all \u2019s)\nand .\nTherefore,\nAnd we can compute the first part of the attention layer as\nBy setting , we obtain:\nFurthermore, we set ,\nwhere  is also a parameter to be decided later. Then after the attention layer, we have a matrix as\nNow we construct the feed-forward layer, which is a two-layer MLP.\nFor the first layer, the weight matrix  is set to be,\nand the bias , which implies that . When  is large enough, the  row of the matrix\n is .\nSince  can reach , in , only the entry for node  is  while all other entries are  or .\nTherefore, the  row of the matrix\n can be arbitrarily close to\n. Here  represents the one-hot token vector for node  (with dimension ).\nFor the second layer, we set\nwhere  are positive parameters to be decided, and .\nBy this way, we have\nTherefore,\nwhere  represents the one-hot token vector for node  (with dimension ).\nThen we fix  and let them be large enough.\nIn this case, the dominant entries in  represent the nodes that are both the out-neighbor of  and reachable to , since those entries will\nhave the value of  while others entries are at most .\nThis means that  can correctly indicates the next node .\nSpecifically, let \nThen the final output approaches the following vector\nwhere  is the number of nodes that are both the out-neighbor of  and reachable to .\nThus, this encoding guarantees that this is exactly the correct output of the next node.\nHence, for any , we can always find a -layer, -head, and -embedding-size Transformer that provides the correct response with probability at least  by selecting large enough parameters .\nFinally, there are two different rules (other than output a correct next node): i) when the input sequence is only \u201c \u201d, the prediction of the next token should be the source node ; ii) when the input sequence is only \u201c      \u201d, the prediction of the next token should be n. Case i) can be constructed using the Transformer architecture utilizing the position information and attention to the first position; and case ii) can be constructed by using the Transformer architecture utilizing the position information and attention to the second position.\nTo maintain focus on the main construction corresponding to Algorithm 1  ###reference_###, we omit the detailed construction for these two boundary cases.\n\u220e\nHaving established the mathematical existence of a Transformer model capable of accomplishing path finding in a given network, as demonstrated in Theorem 2  ###reference_orem2###, we now shift our focus to the following fundamental question\nCan the Transformer architecture, trained on sufficient path data\nwith an auto-regressive loss as in Equation (7  ###reference_###) and using the gradient descent (GD) method,\nlearn the adjacency and reachability matrices and carry out path finding similar to the idealized Algorithm 1  ###reference_###?\nThrough a combination of theoretical analysis and empirical evaluation presented in the following section, our primary investigation aims to address the aforementioned question.\nFirst,\nit is important to note that the Transformer may not be capable to learn the exact true adjacency and reachability matrices of the underlying graph.\nInstead, it can only learn the relevant information that\nis directly encoded in the observed training data .\nTherefore, we define the\nobserved adjacency and reachability matrices based on the training data  as follows.\nNaturally, the observed adjacency matrix  only records the edges  that appears in some path\nwithin the training data . On the other hand, the observed reachability matrix \nexhibits more nuanced distinctions from\nthe true reachability matrix.\nIt only records that  is reachable from node , if\nthe training data \ncontains a path (sequence) whose destination is  and  appears as a non-source node on the path.\nWe call such pairs  observed reachable pairs.\nTherefore, the observed reachability matrix would miss the following types of reachable pairs  in  (referred as non-observed reachable pairs):\n(i) there is no path in  that contains a sub-path from  to , even if a path from  to  can be obtained by concatenating several sub-paths appeared in ;\n(ii) there are some paths in  that contains a sub-path from  to , however,  is not the target node in these paths;\n(iii) there are some paths in  that contains a sub-path from  to  and  is the target node in these paths, however,  is always the source node of these paths.\nIn Section 4  ###reference_###, we show that in a simplified Transformer model, the learning is limited to the observed adjacency and reachability matrices,\nrather than the true underlying matrices.\nThe following\npresents an informal version of the result:\nBy using auto-regressive loss and training with gradient descent, a simplified Transformer architecture with -layer, -head, and -embedding-size simulates Algorithm 1  ###reference_### with\n and .\nThe formal analytical result is presented as Theorem 4  ###reference_orem4###, which\ngives\ncaptures the direction of changes of the parameters in the learnable matrices of the simplified Transformer when following the\ngradient descent calculation.\nWe then discuss that how this result indicates that the simplified Transformer\neffective learns the observed adjacency matrix and the observed reachability matrix, and its inference procedure\nindeed\nalign with the workings of Algorithm 1  ###reference_###.\nSpecifically, in the simplified Transformer, the observed adjacency matrix is encoded within the\nweights of the feed-forward network (FFN)\nas illustrated in Figure 1  ###reference_###,\nwhile the observed reachability matrix is encoded in the value matrix, as depicted\nin Figure 2  ###reference_###.\nNext, in Section 5  ###reference_###, we present\nthe results of our empirical evaluation\nwhich is based on\nextensive experiments.\nWe report the accuracy achieved by the Transformer models with various hyperparameters (Figure 3  ###reference_###).\nFurthermore, we provide\nvisualizations that demonstrate the Transformer\u2019s ability to learn attention\n(Figure 4  ###reference_###)\nas well as the information about adjacency and reachability matrices (Figures 5  ###reference_### and 6  ###reference_###).\nNotably, our findings\nreveal\nthat even a large Transformer model fails to learn the reachability matrix beyond , resulting poor path-finding accuracy for those unobserved reachable pairs (Figure 6  ###reference_###).\nTo further validate our approach, we conduct experiments on a realistic planning dataset called Blockswords. The accuracy, attention, adjacency matrix and reachability matrix are shown in Figure 7  ###reference_### and Figure 8  ###reference_###. Importantly, our empirical results align closely\nwith our theoretical findings discussed in Section 4  ###reference_###."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Gradient-based Analysis for Path Finding",
            "text": "Let  be the path dataset as described in Section 2.2  ###reference_###.\nIn this section, we show analytically that even with only one layer and one head, the Transformer architecture could learn\nboth the adjacency matrix and the reachability matrix from the dataset  and then predict the next node on a path, similar to what is done in Algorithm 1  ###reference_###.\nLet  be the number of times in  that i) the current node is ; ii) the destination node is  and iii) the next node is , and let .\nTo simplify the analysis, we consider the following simplified one layer and one head Transformer structure without any layer normalizations. The embedding size is the same as the vocabulary size (), and we only consider the cross-entropy loss of predicting the next node, i.e., only when  (hence it is not repeating the source node) and the token  is not the target node (hence it is not predicting \u201cn\u201d).\nThe attention weight is only on the target node (the second token), i.e., we manually set every row in  (in Eq. (1  ###reference_###)) to be a one-hot vector with the second coordinate being . Moreover, we set the positional embedding matrix , since it is usually used to adjust the attention weights.\nWe remove all the non-linear layers (e.g., the layer normalizations), and use\n instead of Eq. (3  ###reference_###), and use\n instead of Eq. (4  ###reference_###).\nThe token embedding matrix  and the output weight matrix  are set to be identity, i.e., .\nSince there is only one layer and one head, for simplicity, we use  to represent the weight of the value matrix in the attention layer.\nUnder the above Transformer structure,\nwhere  is the manually set attention weight matrix (every row is a one-hot vector with the second coordinate being ).\nTherefore, the weight vector when predicting the  token is , and\nthe prediction probability is\nWe prove the following theorem.\nUnder the cross-entropy loss , for all possible  pairs, i) if , then  is always 0;\nii) if  but , then  is always positive;\niii) if , then  is negative when  converges to .\nSimilarly, for all possible  pairs, i) if , then  is always 0; ii) if  but , then  is always positive; iii) if , then  is negative when  converges to .\nWe only prove the first part of this theorem, since the proof of the second part is almost the\nidentical.\nBy the definition of the cross-entropy loss in Eq.(7  ###reference_###), and the prediction weight vector in Eq.(9  ###reference_###) for our simplified model,\nthe total cross-entropy loss of the model (with matrices , ) is\nThen we have that\nIn case i),  implies that . Hence  is always zero.\nIn case ii),  implies that the second term in Eq. (10  ###reference_###) is positive, while  implies that the first term in Eq. (10  ###reference_###) is 0. Hence  is always positive.\nIn case iii), when  and  converges to , then the second term in Eq. (10  ###reference_###) converges to zero, and it is smaller than . Hence,  is negative when  converges to .\n\u220e\nThe theorem directly leads to a theoretical explanation on how the model learns the adjacency and reachability matrices, as explained below.\n###figure_1### ###figure_2### ###figure_3### ###figure_4### Let  denote the set of edges appearing in the training dataset , which corresponds to the observed adjacency matrix .\nFor any , , and for any , . Then from the above theorem, under the random guessing of the next node learning paradigm,  will keep decreasing (since its gradient is always positive), while  will not (since its gradient becomes negative when its value is sufficiently negative). This tends to make  higher than  after training.\nNote that these terms are weights when predicting the next node: a higher  means that \u201cthe edge  exists\u201d, and a lower  means that \u201cthe edge  does not exist\u201d. By this way, the Transformer model learns the information about the observed adjacency matrix\nwith weight matrix .\nTo facilitate comprehension, we\nconducted a simple experiment, and\npresent the results in Figure 1  ###reference_### (the structure of the Transformer aligns with the description\nprovided in this section).\nIn this experiment, we generate a 10-node graph, and use 3 different training datasets  based on this graph:  contains all the paths with length 1;  contains all the paths with length 1 and  of the paths with length higher"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Empirical Evaluations: Peeking into a Trained Transformer",
            "text": "We conduct extensive experiments on the path-finding task using the general Transformer architecture as described in Section 2.1  ###reference_###.\nThe experiments include tests on the overall accuracy of the Transfomer model for the path-finding task, as well as investigation on how well the model learns the attention and the structural information about the adjacency and reachability matrices.\nIn this section, we present these empirical evaluation results, which show that the results derived from our theoretical analysis in Section 4  ###reference_### can be carried over to the general\nTransformer architecture."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Datasets",
            "text": ""
        },
        {
            "section_id": "5.1.1",
            "parent_section_id": "5.1",
            "section_name": "5.1.1 Graphs",
            "text": "The graph is generated randomly on two parameters: the number of nodes , and the probability of edge . Given these two parameters, we generate a DAG with  nodes as follows: for any , there is an edge  with probability , and the randomness for different edges are independent."
        },
        {
            "section_id": "5.1.2",
            "parent_section_id": "5.1",
            "section_name": "5.1.2 Training Data and Test Data",
            "text": "Given the DAG, we first find all the possible reachable pairs  (i.e.,  and there exists at least one path that starts at  and ends at ). Then these reachable pairs are separated into\nthe training set (w.p. 0.5) and the test set (w.p. 0.5), but if edge , we always put  in the training set.\nFor a reachable pair  in the training set, we generate  random paths that start at  and end at , and put these  paths into the training dataset.\nWhen generating the random path, at each current node , we find all the possible  such that  and  (i.e., there is an edge , and  could also reach the target ), and uniformly choose a random one from them.\nMoreover, we always put the one-edge path \u201c    n\u201d in the training dataset for each\n,\nto guarantee that all edges appear at least once in the training data."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Accuracy on Test Dataset",
            "text": "We train Transformer models on the\nthe aforementioned training dataset and subsequently evaluate performance of\nthese models\nusing the pairs in the test dataset.\nFor test pair , The correctness of a\nmodels\u2019 output is determined based on its validity in terms of syntax and\nwhether it corresponds to a valid path from  to .\nIn our experiments, we employ Transformer models with an embedding size of .\nWe conduct tests using various configurations, ranging from\n1-layer and 1-head to 6-layer and 6-head, while considering different graph sizes,\nwith number of nodes  ranging\nfrom 100 to 500.\nThe accuracy results on all these tests are\npresented in Figure 3  ###reference_###.\nFrom these results, we\nmake the following observation:\nWhen comparing the five figures, we note that the accuracy tends to decrease as the number of nodes increases.\nFor  and , the accuracy\nconsistently remains\nabove .\nHowever for , the accuracy drops to a range between  and\n,\nand for  and , the accuracy further declines to a\nrange between  and  in\nmost cases.\nWhen examining at each row,\nwe observe that the accuracy\nremains relatively stable\neven as the number of attention heads increases.\nUpon examining each column,\nwe observe that when the embedding size is\nsufficiently large in comparison\nto the graph size (e.g., ), the accuracy\nremains relatively stable as the number of layers increases.\nConversely, when the embedding size is small comparing to the graph size (e.g., ), the accuracy\nshows a slight improvement as the number of layers increases.\nSpecifically, the accuracy with  layers tends to be slightly higher than the accuracy achieved with  and  layers.\n###figure_12### ###figure_13### ###figure_14### ###figure_15### ###figure_16### The above observations suggest that both the embedding size and the number of layers have an impact on the model\u2019s accuracy, as they influence the number of parameters in the model.\nOn the one hand, when the embedding size is sufficiently large compared to the graph size, even 1-layer 1-head models would perform well.\nThis coincides with our theoretical analysis in the previous section, which shows that when the embedding size equals to the graph size, the 1-layer and 1-head structure is enough to predict the next nodes accurately.\nOn the other hand, our empirical results show that when the embedding size is small comparing to the graph size, the Transformer architecture may need more parameters to increase the accuracy, which could be achieved by increasing the number of layers in the model.\nHowever, increasing the number of layers does not efficiently increase the accuracy a lot, e.g., the accuracy for Transformer with 3 layers is almost the same as the accuracy for Transformer with 6 layers.\n###figure_17### ###figure_18### ###figure_19### ###figure_20### ###figure_21###"
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Attention",
            "text": "We now look inside the Transformer models, and try to find more evidence that our theoretical analysis reflects the reality.\nIn our theoretical analysis, we assume that the attention is fixed on the target node.\nIs it true for the Transformer models learned from read data?\nThe corresponding results are shown in Figure 4  ###reference_###.\nThese results are obtained by looking into the attention mechanism of the five 1-layer and 1-head Transformer models,\nand showing the average (taking on the test dataset) matrix of ,\nof which the  row represents the attention vector for predicting the  token.\nNote that the second column in these figures represents the attention weights on the second token, which corresponding to the target node in our test data.\nWe can see that, when predicting the next tokens, almost all the attention weights are concentrated on this column, especially for those models\nwith higher accuracy (Figure 4(a)  ###reference_sf1### for  and Figure 4(b)  ###reference_sf2### for ).\nThis demonstrates that indeed the Transformer model learns the correct attention for the path-finding task, and our assumption on the attention for the theoretical analysis is reasonable.\n###figure_22### ###figure_23### ###figure_24### ###figure_25### ###figure_26### ###figure_27###"
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Adjacency Matrix",
            "text": "Our analysis in Section 4  ###reference_### shows that the observed adjacency matrix and the observed reachability matrix are stored in the feed-forward layer and the attention layer. We now verify this on the general 1-layer and 1-head Transformer model that includes all the Transformer mechanisms, such as attention weights, non-linear transformation,\ntoken and position embedding.\nNote that in the Transformer layer, the output can be written as 333For simplicity, though the layer normalizations , and  are used in our experiments, we omit them in the equations.\nAlso noting that we have verified that the attention is concentrated at the second token, then we let , representing the token embedding of the target node, and , representing the token embedding of the current node. Then we know that\nIt is straightforward that  contains the information of current node, and  contains the information of the target node. As for , we choose to use its linear approximation as . As shown by Table 1  ###reference_### (which takes average over all possible \u2019s and \u2019s), this is a good approximation.\nThen we can treat  as the information of the target node, and  as the information of the current node.\n###table_1### In this subsection, we want to verify that the adjacency matrix is stored in the feed-forward layer, i.e., in the sum of two terms .\nLet  be the matrix whose -th row is , where  represents the one-hot column vector for node  (with dimension ).\nNote that in the simplified Transformer model of Section 4  ###reference_###,  is the same as matrix .\nThe results of  are shown in Figure 5  ###reference_###.\nAs we can see, in Figure 5(a)  ###reference_sf1###, the  matrix and the adjacency matrix are highly aligned: almost all the large entries in the  matrix correspond to real edges, and almost all real edges correspond to large entries in the  matrix.\nThis high accuracy is because the embedding size  is higher than the number of nodes . If the embedding size is lower than the graph size (Figures 5(b)  ###reference_sf2###, 5(c)  ###reference_sf3###, 5(d)  ###reference_sf4### and 5(e)  ###reference_sf5###), we inevitably lose some accuracy when approximating the adjacency matrix by the product of matrices with rank smaller than the graph size, let alone the non-linear layers\u2019 influence. Even so, there is still high relevance between  and the adjacency matrix: most real edges correspond to large entries in the  matrix.\nIn Figure 5(f)  ###reference_sf6###, we show the gap between the average weight corresponds to edges (i.e., the average of \u2019s with  and ) and the average weight corresponds to non-edges (i.e., the average of \u2019s with  and ). We can see that in all these five graphs, their gaps keep increasing until convergence, suggesting that\nweights between edges and non-edges are more easily separated as the learning process proceeds. Moreover, with lower number of nodes, the gap is higher. We believe this is because that when the embedding size is fixed, one can approximate the adjacency matrix better for smaller graphs."
        },
        {
            "section_id": "5.5",
            "parent_section_id": "5",
            "section_name": "Reachability Matrix",
            "text": "In this subsection, we want to verify that the reachability matrix is stored in the attention layer, i.e., in the two terms .\nLet  be the matrix whose  row is , where  represents the one-hot column vector for node  (with dimension ).\nNote that in the simplified Transformer model of Section 4  ###reference_###,  is the same as matrix .\nIn Figure 6(a)  ###reference_sf1###, we show the average weights of three different sets in the graphs: \u201cobs\u201d corresponds to the \u2019s with  and ; \u201crealobs\u201d corresponds to the \u2019s with ,  but ; and \u201cnon\u201d corresponds to the \u2019s with  and .\nHere we only show the results of graphs with 100 nodes and 200 nodes, since i) their accuracy is high enough (about 0.96, and does not become higher even if there are more layers/heads, as shown in Figures 3(a)  ###reference_sf1### and 3(b)  ###reference_sf2###); ii) their attention is quite close to be concentrate on the target node (see Figures 4(a)  ###reference_sf1### and 4(b)  ###reference_sf2###). When there are more\nnodes, the ability of approximating the reachability matrix is not enough for us to distinguish it.\nFrom these average weights, we can see that the Transformer learns  quite well, as for those terms in \u201crealobs\u201d, their weights are almost the same as those in \u201cnon\u201d.\nThis echos our analysis in Section 4  ###reference_###.\nTo further demonstrate that  is not learned as good as , we divide the source-target node pair  in the test dataset into four categories:\nDegree 0: .\nDegree 1:  is not of degree 0, while  has at least one out-neighbor node  such that  is of degree , i.e. .\nDegree 2:  is not of degree 0 and 1, while  has at least one out-neighbor node  such that  is of degree 1.\nDegree 3 or more: the remaining  pairs in the test dataset.\nRoughly speaking, in our analysis, when the Transformer is predicting the next node of  for the source-target pair ,\nit will add the adjacency vector of  and the reachability vector of , and use the sum as the weight vector of the next node.\nFor  pairs of degree 0 or 1, we know that there is a node  such that\n and\n. Then node  will have a large weight, indicating a high accuracy.\nOn the other hand, for  pairs of degree 2 or more, there is no node  such that both its corresponding entry in the adjacency vector of  and\nits corresponding entry in the reachability vector of  are large.\nIn this case, the high-weight entry when predicting the next node is either an adjacent node of  or a recorded node that can reach .\nSince none of the recorded nodes reachable to  is adjacent to , the accuracy for  pairs of degree 2 or more should be much lower than those of degree 0 or 1.\nTo see this, we check the accuracy of the Transformer on the  pairs of the four different categories. The results are shown in Figure 6  ###reference_### (b)-(f).\nIn this figure, each row of the accuracy matrix is further divided into four sub-rows corresponding to the accuracy of degree-0 pairs, degree-1 pairs, degree-2 pairs, and degree-3 or more pairs\nrespectively (in the graph with 100 nodes, there are no test  pairs in the degree-3 or more category).\nFrom these results, we can see that the accuracy for degree-2 pairs and degree-3 or more pairs is much lower than the two other categories in most cases.\nIt indicates that, even with a larger model (e.g. 6-layer 6-head Transformer), the model has a fundamental difficulty to generate paths for high-degree source-target pairs,\nnamely those pairs who can only be connected by concatenating several path segments in the training dataset.\nIn short, the model fails to learn reachability through the transitivity among the reachability relations.\nThis result demonstrates the validity of our theoretical analysis, which shows that the Transformer can only learn observed reachability, and will miss those unobserved reachability\ndeduced from the transitivity of the reachability relation."
        },
        {
            "section_id": "5.6",
            "parent_section_id": "5",
            "section_name": "Summary of the Empirical Results",
            "text": "In summary, our extensive empirical evaluation leads to the following conclusions about the auto-regressive Transformer model in achieving the path-finding task:\n(a) With large enough embedding size, the model can achieve high accuracy in general; (b) The model achieves its performance by concentrating attentions on the target nodes as intended, and learning the information on adjacency and reachability matrices, just as\nwhat a human would do and as predicted by our theoretical analysis; and\n(c) The model may have limitations and fail to learn high-order reachability relations through transitivity, and thus fail to generate paths derived from high-order reachability,\nas human would expect.\n###figure_28### ###figure_29### ###figure_30### ###figure_31### ###figure_32### ###figure_33###"
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Path-planning in Blocksworld",
            "text": "To further validate the theoretical results in Section 4  ###reference_### and the practicability of the proposed path-finding task, we consider Blocksworld benchmark [VMSK24  ###reference_bx21###]. Blocksworld is a scenario consisting of a set of blocks identified by different colors. The blocks are either placed on table or on top of another block and the task is to plan a block manipulation from the given state to the target state.\nWe formulate Blocksworld as a path-finding task.\nHere we construct a graph  for the case with  blocks, where each node represents a state of the blocks.\nFor example, node  refers to the state that \u201cthe red block is on top of the blue block, the blue block is on top of the orange block, the orange block is on top of the yellow block, and the yellow block is on the table\u201d.\n is a directed graph with  nodes, and the adjacency matrix of  is presented in Figure 7(a)  ###reference_sf1###. Specifically, the states corresponding to the first  nodes are the states where the four blocks are stacked one on top of another in a single stack, and thus each of these nodes only has one out-neighbor\ncorresponding to removing the top block and putting it on the table.\nThe last node refers to the state where all blocks are on the table, so it has 12 out-neighbors, corresponding to the  states where three blocks are on the table and the fourth block is on top of one of the three blocks.\nIn the original Blocksworld task, the answer is a sequence of actions, which is equivalent to the notion of edges in .\nWe reformulated it to let the model output a path from the given state to the target state, only consisting of the nodes.\nThis can be seen as a simplified version and a pure planning task.\nWe randomly select  of all node pairs for training and the rest  for testing and generate the  training data in the same format as introduced in Section 5  ###reference_###.\nWe mainly use Transformers with  layer and  head for the convenience of visualization.\n###figure_34### ###figure_35### ###figure_36### ###figure_37### ###figure_38### ###figure_39### ###figure_40### ###figure_41###"
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Results",
            "text": "We first present the accuracy during the training when using different embedding sizes . As is shown in Figure 7(d)  ###reference_sf4###, although a smaller embedding size results in a longer time to converge, all models reach an accuracy near  at the end of the training.\nThen, we use the same method introduced in Section 5  ###reference_### to visualize the attention map and the  matrix for the model with  after the entire iterations. In Figure 7(c)  ###reference_sf3###, we can see that when predicting the tokens on the path, almost all the attention weights are on the second token which represents for the target node, demonstrating the capability of the model to learn a correct attention. For adjacency matrix, we find that the  matrix in Figure 7(b)  ###reference_sf2### almost perfectly aligns to the real adjacency matrix of . And the weight gap (average edge weight minus average non-edge weight) for all models keeps increasing in the training process until convergence, as is shown in Figure 7(e)  ###reference_sf5###.\nIn addition, we present the results related to reachability matrix in Figure 8  ###reference_###. Figure 8(a)  ###reference_sf1### shows the observed reachability in the training dataset. Although  is fully-connected, some reachability are not observed since we request that all training data has no cycle. Specifically, each of the first  nodes is not observed reachable to any nodes other than itself. To validate whether Transformer has captured this information, we construct  matrix through the same method presented in Section 5  ###reference_###. As shown in Figure 8(b)  ###reference_sf2###, the  matrix has represented darkness for the first  columns, which aligns to the real observed reachability matrix. Furthermore, we plot the average gap between the weight of  on observed reachability and those exist but not observed in Figure 8(c)  ###reference_sf3###. We find that this gap keeps increasing for all models444Since there does not exist any test pairs that are with degree 2 or more (as defined in Section 5  ###reference_###), we do not compare the accuracy between different degrees in Blocksworld..\nIn summary, our experimental results on the Blocksworld benchmark confirms our theoretical analyses (Theorem 4  ###reference_orem4###) and empirical results on the synthetic data, and it\nat least partially explains the planning capability of the Transformer on the Blocksworld scenario."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Related Works",
            "text": ""
        },
        {
            "section_id": "7.1",
            "parent_section_id": "7",
            "section_name": "LLMs for Planning",
            "text": "Several recent studies have empirically evaluated the planning abilities of large language models (LLMs). For instance, CogEval has introduced a variety of planning tasks set in mazes and graphs, ultimately finding no evidence to suggest LLMs grasp the intricacies of the maps or the planning tasks themselves [MHVF+23  ###reference_bx15###]. Similarly, another study explored the Blocksworld game, a planning challenge where humans typically achieve success rates above , in stark contrast to GPT-3\u2019s mere  success rate [VMSK24  ###reference_bx21###]. Our paper proposes a novel approach by formulating a class of planning problems as path finding on graphs, applying this model to the blocksworld game and uncovering significant insights, as detailed in Section 6  ###reference_###.\nDespite these seemingly negative evaluations, LLMs have shown remarkable aptitude in executing real-world planning tasks, creating the field of autonomous agents [WMF+24  ###reference_bx23###].\nCertain applications of autonomous agents feature explicit graphs.\nIn the tool agent HuggingGPT [SST+23  ###reference_bx18###], LLMs are deployed to trigger a sequence of external APIs in response to user requests. Here, APIs are conceptualized as graph nodes, with their interrelations represented as edges, and the selection process akin to identifying a path or subgraph that aligns with user demands.\nThis scenario is an extension of the settings discussed in this paper, where the graph is text-attributed and the objective function is evaluated through textual analysis.\nThe application of graph search techniques has been shown to enhance the performance of tool agents significantly [LLG+23  ###reference_bx11###, LPY+24  ###reference_bx12###].\nThe math agent AlphaGeometry utilizes LLMs to solve geometry problems [TWL+24  ###reference_bx19###].\nBy treating lemmas as nodes and their interdependencies as edges, the process of finding a proof of a theorem is analogous to finding a path to the theorem node in the above graph formed by possible lemma nodes and their interdependency edges.\nHowever, [TWL+24  ###reference_bx19###] focuses on using LLM to generate auxiliary constructions, and the reasoning tasks are done by a non-LLM engine. This is very different to our research.\nThere are no explicit graphs in other agents, such as game agents [WXJ+23  ###reference_bx24###], embodied agents [HAPM22  ###reference_bx10###], and code agents [SCG+24  ###reference_bx17###]. The core strategy in these domains is to employ verbal reinforcement learning within LLMs. It is noteworthy that any dynamic programming problem characterized by deterministic state transitions can be reformulated as a shortest path problem on a graph, with states and transitions represented as nodes and edges, respectively. As a result, the area of autonomous agents is closely related to the path-planning task investigated in this paper."
        },
        {
            "section_id": "7.2",
            "parent_section_id": "7",
            "section_name": "LLM for Graphs",
            "text": "GPT4Graph [GDL23  ###reference_bx8###] and NLGraph [WFH+23  ###reference_bx22###] have developed extensive frameworks for assessing LLMs in the context of graph tasks. These frameworks encompass a broad spectrum of challenges, including classic graph problems (e.g., connectivity, cycle detection, and topological sorting), graph neural network (GNN) tasks (e.g., node and graph classification), and semantic graph question answering (e.g., knowledge graph inquiries). They also explore various input formats, such as adjacency lists, edge lists, GML, and GraphML, alongside innovative prompting techniques like few-shot, role prompting, chain-of-thought, and algorithmic prompting (e.g., stating \u201cwe are using DFS\u201d).\nThese studies demonstrate that LLMs possess basic graph processing capabilities, and the choice of prompts and formats influence performance significantly.\nYet, they also reveal the models\u2019 susceptibility to spurious correlations within graphs.\nGPT-4, for instance, only achieves around  accuracy on shortest path tasks, even when utilizing complex prompts.\nTo our knowledge, this paper presents the first theoretical analysis that identifies and explains the spurious correlations learned by LLMs (Theorem 3  ###reference_orem3###), supporting the negative outcomes reported in these studies.\nThere has also been a surge in efforts aiming at bolstering LLMs\u2019 performance on graph tasks. Innovations like GraphGPT [TYW+23  ###reference_bx20###] and GraphLLM [CZW+23  ###reference_bx6###], which incorporate an additional GNN encoder, have shown notable improvements across the aforementioned graph tasks. GraphInstruct [LSH+24  ###reference_bx13###] seeks to enhance LLMs\u2019 capabilities using pure LLM approaches. This involves meticulously documenting the steps of classical algorithms (e.g., BFS and DFS) and fine-tuning LLMs to learn these graph algorithms. This method of procedural supervision has extended the capacity of LLMs in graph tasks from the complexity class  to P/poly [FZG+23  ###reference_bx7###].\nHowever, while this approach has yielded performance improvements in simpler tasks such as topological sorting and connectivity, it has proven less effective for more complex challenges, like finding Hamiltonian Paths."
        },
        {
            "section_id": "7.3",
            "parent_section_id": "7",
            "section_name": "Algorithm Simulation with Transformers",
            "text": "Recent theoretical investigations have shed light on the capability of Transformer to simulate algorithms, a topic that has garnered considerable interest. This discussion begins with discrete algorithms. From a circuit complexity standpoint, Transformer models are likened to parallel circuits characterized by polynomial width and constant depth, which places them within the  complexity class. It is also noticed that . On the other hand, despite their impressive expressiveness, Transformer is theoretically incapable of addressing a range of P-complete problems, including the testing of Context-Free Grammar Membership [MS23  ###reference_bx16###]. However, the advent of chain-of-thought prompting has enabled Transformer to sequentially simulate algorithms, thereby equipping them to tackle P-complete problems in domains such as arithmetic and decision-making [FZG+23  ###reference_bx7###]. The exploration extends to continuous algorithms, where it has been demonstrated that Transformer can approximate functions such as matrix inversion, Stochastic Gradient Descent (SGD), and power iterations [GRS+23  ###reference_bx9###].\nOur study specifically applies GPT models to simulate path-finding algorithms, presenting evidence that their expressiveness is sufficient for such tasks (Theorem 2  ###reference_orem2###).\nNevertheless, the usage of auto-regressive loss and gradient descent introduces certain limitations (Theorem 3  ###reference_orem3###), which have not been studied in existing works."
        },
        {
            "section_id": "7.4",
            "parent_section_id": "7",
            "section_name": "Mechansims of LLMs",
            "text": "LLMs have demonstrated capabilities that exceed the theoretically predicted lower bounds of expressiveness. To demystify this paradox, numerous studies have employed experimental methodologies akin to those used in the physical and biological sciences. Their aim is to decode the mechanisms of LLMs. The foundational strategy is to generate controlled synthetic datasets to analyze how language models (not necessarily the LLMs) complete various tasks. Standard methods for this analysis include visualizing attention patterns to examine computational properties (such as locality and time invariance) and employing linear probing on the hidden states to determine the extent of learning. Given that the data is synthetic and the ground-truth mappings are generally known, it becomes feasible to isolate the influence of various factors (e.g., prompting strategies, chain-of-thought reasoning, and data formatting). For example, a dataset designed for learning group operations, as detailed in [ZBB+22  ###reference_bx25###], facilitates the exploration of how pretraining, data composition, and neural architecture influence reasoning tasks within LLMs. Similarly, the generation of synthetic context-free grammar (CFG) data, as described in [AZL23a  ###reference_bx1###], enables training GPT-2 models, uncovering their capacity to learn dynamic programming algorithms for parsing CFGs. Synthetic datasets focusing on biographical knowledge, presented in [AZL23b  ###reference_bx2###, AZL23c  ###reference_bx3###, AZL24  ###reference_bx4###], probe into the mechanisms of knowledge storage, retrieval, manipulation, and the implications of scaling laws. Moreover, the work in [LSL+23  ###reference_bx14###] introduces synthetic datasets aimed at understanding how smaller LLMs tackle basic arithmetic operations, like addition, and examines the effects of few-shot prompting, pretraining, and model scaling [LSL+23  ###reference_bx14###]. This paper builds upon these investigations by conducting controlled experiments with a path planning dataset, thereby shedding light on the complexities and challenges of planning in LLMs."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Discussion and Conclusion",
            "text": "In this paper, we present our investigation on how the Transformer architecture executes the path-finding task, which abstracts a number of planning tasks one may encounter in practical scenarios.\nWe first show that the Transformer has the expressive power of path finding by manually construct a 1-layer 1-head Transformer that encodes the adjacency matrix and the reachability matrix\nwith its weight matrices.\nNext we provide a theoretical analysis demonstrating that the Transformer can indeed learn the adjacency matrix and the observed reachability matrix (a limited form of reachability) from the path\ntraining data through the common gradient descent mechanism on cross-entropy loss function.\nWe then verify our theoretical analysis through comprehensive experiments, both on synthetical network data and on a concrete Blocksworld planning benchmark.\nOur empirical findings\nsupport our theoretical analysis and demonstrate that the Transformer can achieve high accuracy in many cases, it can concentrate its attention correctly to the target node,\nand it extracts adjacency matrix and observed reachability matrix as predicted by our theoretical analysis.\nOur theoretical analysis further points out a limitation of the Transformer \u2014 it cannot learn well the transitive reachability, and this point is validated by our experimental results.\nOur study is the first to combine theoretical analysis with empirical evaluation on such planning tasks.\nIn particular, we are the first to apply theoretical analysis using the gradient descent learning method of the Transformer on a concrete task and provide theoretical interpretation, which is validated by\nour experimental results.\nMore importantly, our theoretical analysis is able to unveil a potential limitation of the Transformer in completing the path-finding task, which guides us to verify that indeed this limitation occurs in the\nexperiments.\nThis demonstrates the power of theoretical analysis as a guidance for understanding practical Transformers and LLMs.\nAlthough the path-finding task is a simple task on a network and our investigation is still the first attempt\nto understand Transformer\u2019s capability on this task,\nour findings may already provide some insights and implications to path finding and planning in general.\nWe can see that, due to the autoregressive nature of learning, the Transformer cannot execute path finding by a simple trial-and-error search such as depth-first or breadth-first search, and thus\nit has to plan ahead well in generating the next node on the path correctly.\nWe demonstrate that the Transformer can adjust its effort accordingly by concentrating its attention to the target, and generating the next node that is both adjacent to the current node and reachable\nto the target, mimicking the human intelligence.\nIt suggests that for many planning tasks in general, the Transformer architecture is adjusting its learning by balancing the consideration between the immediate continuation of the next step and\nits final goal of reaching the target.\nOur investigation opens up many possible future directions, and we list several of them below.\n(a) Extend our study to hyper-graphs and hyper-paths, where a hyper-edge models the situation in which several preconditions need to be satisfied together in order to carry out the next step, which\nis common in task planning and mathematical proofs.\n(b) Further understand the limitation of the Transformer on path finding: the current inability of transitive reachability may be due to the limited expressive power of the path language, and it may also\nsuggest the fundamental difference between Transformer\u2019s interpretation and human\u2019s interpretation on path finding.\nThis requires further study, perhaps by enriching the languages, or by an investigation on the deployed LLMs such as GPT4.\nIt can also stimulates new improvement to the Transformer architecture to achieve better performance.\n(c) Study the connection between the abstract path-finding task and some concrete planning tasks (e.g. block moving in Blocksworld, etc.), to see if Transformer has any capability\nof abstracting a concrete task to the more abstract path-finding task, or find if what Transformer does for concrete planning tasks has any commonalities with the abstract path-planning task.\n(d) Study deployed LLMs or fine-tune LLMs to connect their planning capabilities with our path-finding investigation.\nWe are planning to deepen our investigation on the above fronts.\nWe also hope that our work could inspire more people to study LLMs with combined theoretical and empirical analysis, with the ultimate goal of understanding and explaining the power of LLMs."
        }
    ],
    "url": "http://arxiv.org/html/2405.09220v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "7.1"
        ],
        "methodology_sections": [
            "2",
            "2.1",
            "2.2",
            "3",
            "4"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.2",
            "5.3",
            "5.4",
            "5.5",
            "5.6",
            "6",
            "6.1"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "5",
            "5.1",
            "5.1.1",
            "5.1.2",
            "5.2",
            "5.3",
            "5.4",
            "5.5",
            "5.6",
            "6",
            "6.1"
        ]
    },
    "research_context": {
        "paper_id": "2405.09220v2",
        "paper_title": "ALPINE: Unveiling the Planning Capability of Autoregressive Learning in Language Models",
        "research_background": "**Motivation:**\n\nLarge language models (LLMs), such as ChatGPT, have demonstrated exceptional capabilities across various intelligent tasks, hinting at the potential achievement of artificial general intelligence (AGI). Despite their powerful outputs, the underlying mechanisms enabling these capabilities, particularly from a theoretical standpoint, remain largely unexplained. This paper targets a fundamental aspect of intelligence\u2014planning\u2014to explore how LLMs can transform the next word prediction in a sequence into high-level intelligent tasks such as planning. By doing so, the study aims to uncover deeper insights into the innate mechanisms of LLMs that could explain their broad capabilities.\n\n**Research Problem:**\n\nThe core research problem this paper addresses is understanding how the Transformer-based autoregressive learning architecture of LLMs achieves planning tasks. Specifically, it investigates the planning capability by examining the ability of LLMs to solve path-finding tasks in networks. This task requires the model to generate a valid path from a starting node to a target node using a sequence of nodes, a challenge that reflects real-world planning problems. The study seeks both empirical evidence and theoretical interpretations of how and why LLMs can or cannot successfully complete these planning tasks.\n\n**Relevant Prior Work:**\n\n1. **Empirical Studies on LLM Planning Capabilities:** Existing research has primarily been empirical, assessing the ability of LLMs to complete planning tasks with some success. However, these studies have not fully explained why these models perform well or poorly on specific aspects of planning.\n2. **Benchmarks and Tasks Related to Planning:**\n   - **Blocksworld Game [VMSK24]:** Used as a benchmark for evaluating LLM planning capability, viewed as a path-finding problem from initial to final block states.\n   - **HuggingGPT [SST+23]:** Demonstrates scheduling API calls akin to finding a path in an API call graph, another type of planning-related task.\n3. **Path-Finding Tasks:** These tasks involve navigating a graph to find a valid path between nodes, considering immediate adjacency and overall reachability\u2014key components essential in planning and intelligent task management across various applications.\n\n**Conclusion:**\n\nThis paper proposes Project ALPINE to bridge the gap between empirical results and theoretical understanding by not only evaluating LLM performance in planning tasks but also dissecting the underlying mechanisms enabling this capability. Through the study of path-finding tasks, it aims to provide vital insights and potential pathways to explain broader intelligent behaviors exhibited by LLMs.",
        "methodology": "I'm sorry, but from the given text, it appears that the section you have provided is focused on the notation and symbol conventions used in the paper, rather than the methodology itself. To describe the proposed method or model, including key components and innovations, I would need more specific details about the method or model described in the paper. This would typically include descriptions of the algorithm or procedure, key steps, components involved, and any novel contributions or improvements introduced.\n\nIf you can provide more detailed information from the methodology section or other parts of the paper, I'd be happy to help you describe the proposed method or model accurately.",
        "main_experiment_and_results": "**Main Experiment Setup:**\n\n**Datasets:**\n- The experiments are conducted on a path-finding task dataset. The precise nature of the dataset (e.g., its size, the format of the graphs, or the specific tasks) isn't detailed here, but it likely involves various graph structures where the model must predict paths.\n\n**Baselines:**\n- The general Transformer architecture serves as the primary model. No specific mention of additional baseline models is given, implying the primary focus might be comparing the Transformer model performance against the theoretical expectations.\n\n**Evaluation Metrics:**\n- Overall accuracy on the path-finding task.\n- Measurements of how well the model learns attention.\n- Evaluation of how well the model understands the structural information, specifically the adjacency and reachability matrices.\n\n**Main Experimental Results:**\n- The empirical evaluation results support the theoretical analysis presented earlier in the paper (Section 4). Specifically:\n  - The Transformer model shows good performance on the path-finding task.\n  - The model effectively learns the attention mechanisms required for this task.\n  - It also grasps the structural information related to adjacency and reachability matrices well.\n  \nBy confirming that the theoretical assertions hold in practice, the study demonstrates the planning capability of the Transformer model in path-finding tasks."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To investigate whether Transformer models can efficiently learn the attention and structural information about the adjacency and reachability matrices for the path-finding task.",
            "experiment_process": "The path-finding task experiments involved randomly generating a DAG with nodes and edges, splitting reachable pairs into training and test sets, and training Transformer models with varying configurations (1-layer 1-head to 6-layer 6-head) on this data. The accuracy of the models was measured based on the validity of generated paths from the source to target nodes.",
            "result_discussion": "Accuracy decreased as the number of nodes increased. Embedding size and the number of layers both affect accuracy, with a large enough embedding size helping to maintain high accuracy even with fewer layers. These results coincide with the theoretical analysis, suggesting that larger models are more accurate but increasing layers beyond three provides marginal benefits.",
            "ablation_id": "2405.09220v2.No1"
        },
        {
            "research_objective": "To validate if the Transformer models learn the assumed attention, concentrating on the target node for path-finding tasks.",
            "experiment_process": "Evaluated the attention mechanism inside five 1-layer and 1-head Transformer models by observing the attention weights on the target node for the test dataset, visualized in attention vectors.",
            "result_discussion": "The attention weights were concentrated on the target node's column, especially in higher accuracy models, thus validating the hypothesis that Transformers can learn the correct attention for path-finding.",
            "ablation_id": "2405.09220v2.No2"
        },
        {
            "research_objective": "To verify if the observed adjacency matrix is stored in the feed-forward layer of the Transformer.",
            "experiment_process": "Analysis was performed on the 1-layer and 1-head Transformer model by representing token embeddings of current and target nodes and their linear approximations. The alignment of the adjacency matrix with these representations was assessed.",
            "result_discussion": "The adjacency matrix and the product of embeddings were highly aligned, especially when the embedding size was higher than the number of nodes. This alignment diminished with lower embedding sizes but still indicated significant relevance between real edges and the matrix entries.",
            "ablation_id": "2405.09220v2.No3"
        },
        {
            "research_objective": "To validate if the observed reachability matrix is stored in the attention layer of the Transformer.",
            "experiment_process": "Compared average weights of different sets within graphs (observed, real observed, and non-edges) for models with varying numbers of nodes, examining how well the Transformer learns the reachability matrix.",
            "result_discussion": "The Transformer learned the observed reachability well but struggled with high-degree source-target pairs requiring path concatenation, as indicated by much lower accuracy for such pairs. This aligns with theoretical analysis suggesting the model's limitations in learning reachability through transitivity.",
            "ablation_id": "2405.09220v2.No4"
        },
        {
            "research_objective": "To assess the applicability of theoretical findings on Transformer planning capabilities using a real-world benchmark, Blocksworld, reformulated as a path-finding task.",
            "experiment_process": "A graph representing states of blocks was constructed, with node pairs randomly split into training and test sets. Transformer models with various embedding sizes were trained and evaluated on accuracy, attention, adjacency, and reachability matrices.",
            "result_discussion": "All models reached high accuracy eventually, with attention directed almost entirely on target nodes and adjacency matrices aligning well with ground truth. Reachability matrices also demonstrated increased weight separation for observed edges, confirming theoretical findings.",
            "ablation_id": "2405.09220v2.No5"
        }
    ]
}