{
    "title": "Beyond Memorization: The Challenge of Random Memory Access in Language Models",
    "abstract": "Recent developments in Language Models (LMs) have shown their effectiveness in NLP tasks, particularly in knowledge-intensive tasks.\nHowever, the mechanisms underlying knowledge storage and memory access within their parameters remain elusive.\nIn this paper, we investigate whether a generative LM (e.g., GPT-2) is able to access its memory sequentially or randomly.\nThrough carefully-designed synthetic tasks, covering the scenarios of full recitation, selective recitation and grounded question answering, we reveal that LMs manage to sequentially access their memory while encountering challenges in randomly accessing memorized content. We find that techniques including recitation and permutation improve the random memory access capability of LMs.\nFurthermore, by applying this intervention to realistic scenarios of open-domain question answering, we validate that enhancing random access by recitation leads to notable improvements in question answering. The code to reproduce our experiments can be found at https://github.com/sail-sg/lm-random-memory-access.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Language models (LMs) have recently showcased outstanding abilities in NLP tasks with a large amount of memory stored in their parameters (Brown et al., 2020  ###reference_b7###; Ouyang et al., 2022  ###reference_b27###). Through pre-training on large text corpora, LMs memorize factual knowledge about the world (Zhou et al., 2023  ###reference_b54###). Consequently, they show great performance in knowledge-intensive tasks (Petroni et al., 2021  ###reference_b31###) such as open-domain question answering (Kamalloo et al., 2023  ###reference_b15###; Ziems et al., 2023  ###reference_b58###; Mallen et al., 2023  ###reference_b23###). There is a growing interest in considering LMs as knowledge bases (Wang et al., 2021  ###reference_b47###; Heinzerling and Inui, 2021  ###reference_b11###; Petroni et al., 2019  ###reference_b32###; Cao et al., 2021  ###reference_b8###; AlKhamissi et al., 2022  ###reference_b1###). Despite the recent advances in applying LMs to solve downstream tasks, the fundamentals of how LMs store knowledge and access memory in their parameters remains a subject of ongoing research and intrigue (Tirumala et al., 2022  ###reference_b43###; Zhu and Li, 2023  ###reference_b56###; Allen-Zhu and Li, 2023  ###reference_b2###; Berglund et al., 2023  ###reference_b4###).\n###figure_1### In this paper, we draw inspiration from memory-accessing patterns observed in computer systems to explore whether LMs can access their parametric memory in a sequential or random manner.\nWe extrapolate these concepts to investigate LMs and delineate two memory access patterns: sequential memory access means that the model starts from the beginning of a memorized sequence, progressing through the content in consecutive order. Conversely, random memory access denotes that the model can commence from any location within the memorized content, without needing to start from the beginning. For instance, reciting a memorized poem line by line is considered sequential access, while directly starting from the third line involves random access.\nWith these concepts, we design experiments with both synthetic and real data to evaluate the language model\u2019s ability to perform sequential or random access to memorized content, as illustrated in Figure 1  ###reference_###.\nWe limit our study to\ndecoder-only language models due to their increasing popularity and capability (Radford et al., 2019  ###reference_b34###; Brown et al., 2020  ###reference_b7###; Touvron et al., 2023a  ###reference_b44###, b  ###reference_b45###; Jiang et al., 2023  ###reference_b13###). We first ask the model to memorize key\u2013value pairs of various types and show that the model is able to sequentially read memorized content to a satisfying degree. Next, we test the model\u2019s random access ability by training it to recite a sentence or find an answer to a question in a memorized passage. In such tasks, the model\u2019s performance falls drastically when it is required to extract a span in the middle of a passage, revealing its incapability to randomly access its memory.\nGiven that language models struggle to perform random access to their memory, we pursue two means for mitigation: recitation at inference time, and permutation during training. Recitation enables the model to sequentially read its parametric memory first before performing a task. The model\u2019s performance can thus be enhanced by utilizing the recited content in its context window. We also show that simply permuting sentences in a passage during training to memorize content also improves performance.\nWe finally verify the challenge of random access through a case study on open-domain question answering. We reduce the difficulty of the task by allowing the model to memorize passages with ground-truth answers, yet we find that the model benefits the most from such memorization when it is allowed to recite a relevant passage and then answer the question. Overall, we make several contributions to further understand the memory access mechanisms of decoder-only language models:\nWe show that language models can access their memory sequentially and can reproduce memorized content. In contrast, they encounter significant challenges in random memory access.\nWe find solutions to mitigate the challenge of random access by permuting memorized content or explicitly reciting the memory before performing tasks.\nWe demonstrate the effect of poor random memory access ability in open-domain question answering, showing that the challenge could have broader implications on the applications of language models."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Large language models store a considerable amount of knowledge in their parameters (Petroni et al., 2019  ###reference_b32###; Heinzerling and Inui, 2021  ###reference_b11###). They memorize useful knowledge such as facts and commonsense (Zhao et al., 2023  ###reference_b53###), but also sensitive personal information such as emails or phone numbers (Carlini et al., 2020  ###reference_b9###; Huang et al., 2022  ###reference_b12###). Existing approaches to understanding memorization include fine-grained analysis to locate the neuron that is associated with the knowledge (Meng et al., 2022  ###reference_b25###; Liu et al., 2024  ###reference_b22###) or macro analysis to understand the overall dynamics of memorization (Tirumala et al., 2022  ###reference_b43###; Speicher et al., 2024  ###reference_b39###). In this study, we do not aim to analyze the mechanisms of writing to language model\u2019s memory. Instead, we consider the language model as a black-box memory store and focus mainly on how the model accesses its memory.\nOur investigation requires writing new content to the model\u2019s parametric memory. There are mainly two ways to perform such knowledge injection without changing the model architecture (Ovadia et al., 2024  ###reference_b28###; Balaguer et al., 2024  ###reference_b3###): fine-tuning or retrieval augmentation. Retrieval augmentation (Lewis et al., 2020  ###reference_b20###; Shi et al., 2023  ###reference_b38###) retrieves relevant information and puts it into the model\u2019s context while fine-tuning directly updates the model parameters. As the goal of our study is to investigate how the model accesses its parametric memory after writing to the memory, we choose finetuning as the method for introducing new knowledge to the model.\nPrevious works have shown that using prompts can effectively retrieve knowledge stored in large language models (Bouraoui et al., 2019  ###reference_b6###; Jiang et al., 2021  ###reference_b14###; Wang et al., 2021  ###reference_b47###). We follow earlier work to use prompts to query the model to access and regenerate memorized content. However, a notable difference is that prior work focuses on finding optimised methods to elicit the model\u2019s knowledge obtained during pretraining (Youssef et al., 2023  ###reference_b51###; Liu et al., 2023  ###reference_b21###), while we directly use unique keys for memorizing and retrieving content.\nWe consider the language model as a memory store for passages, which is related to the recent advances in adopting a language model as an index for document storage and retrieval (Metzler et al., 2021  ###reference_b26###; Tay et al., 2022  ###reference_b42###; Wang et al., 2023  ###reference_b48###; Zeng et al., 2023  ###reference_b52###). In such indexes, each document is associated with a document identifier (ID), which could be keywords (Ren et al., 2023  ###reference_b36###; Bevilacqua et al., 2022  ###reference_b5###; Lee et al., 2023b  ###reference_b19###, a  ###reference_b18###) or numbers (Tay et al., 2022  ###reference_b42###; Wang et al., 2023  ###reference_b48###; Zhuang et al., 2022  ###reference_b57###; Zhou et al., 2022  ###reference_b55###). We also follow the practice and assign an ID to each document for storing and retrieving the documents. However, we do not ask the model to retrieve a relevant ID to a question. Instead, we provide the ID in the input, and investigate the possibility of sequentially or randomly accessing the corresponding document content."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Investigating Sequential and Random Memory Access",
            "text": "In this section, we investigate the ability of a language model to sequentially or randomly access its memory stored in the parameters. First, we provide formulations of language models serving as a memory bank of passages (\u00a73.1  ###reference_###). Within this framework, we define sequential memory access as the process of starting from the beginning of a memorized passage and progressively generating subsequent content. In contrast, we conceptualize random memory access as the model\u2019s ability to initiate recall from any chosen location in a memorized passage and accurately regenerate the subsequent content. Based on these definitions, we first investigate the model\u2019s sequential memory access ability by requiring it to recite full passages word by word (\u00a73.2  ###reference_###). Next, we test the random memory access ability of the model by asking it to recite selected sentences from memorized passages (\u00a73.3  ###reference_###). We further assess the model\u2019s random access proficiency through a more challenging task involving question answering (\u00a73.4  ###reference_###).\nTo investigate whether the model can handle identifiers and passage content of different types, we set  and  and consider the following variations. For the type of passage content , we examine two categories: (1) natural language (NL), comprising Wikipedia paragraphs from SQuAD (Rajpurkar et al., 2016  ###reference_b35###), and (2) random strings (Rand), where each NL passage is substituted with a space-separated alphanumeric string maintaining the same number of tokens. Regarding the type of  (i.e., passage IDs), we explore three forms: (1) numerical strings (Num), such as \u2018#123\u2019; (2) rare random tokens (Rare), adopting the approach of Ruiz et al. (2022  ###reference_b37###) by random sampling three infrequent tokens; (3) article title (Title) of the Wikipedia page to which the passage belongs.\nWe adopt the GPT2-large model (Radford et al., 2019  ###reference_b34###) with 774M parameters as the base model. For better string memorization ability (Stevens and Su, 2023  ###reference_b40###), we use a pretrained checkpoint222https://huggingface.co/gpt2  ###reference_huggingface.co/gpt2### instead of training the model from scratch. We fine-tune the model for 100 epochs to ensure that the model fully converges, with a learning rate of . We measure memorization using both the BLEU score (Papineni et al., 2002  ###reference_b30###) and the Exact Match (EM) score, indicating the similarity between the generated content and the ground-truth passage.\nTable 1  ###reference_### shows that the model is able to sequentially access memorized content, with high BLEU and EM on validation passages. The model\u2019s sequential access capability is further demonstrated by its adaptability to varying types of IDs and passages. Specifically, using titles or numbers as keys for natural language passages achieves higher performance than using rare tokens. We suspect that models might have difficulty associating rare tokens with the natural language content. Remarkably, the model\u2019s access ability extends to passages composed of random characters ().\n###figure_2### To further test the memory capacity of the model, we carry out an additional experiment where we set the passage type to  and identifier type to  and construct passages each with 25 random tokens. As illustrated in Figure 2  ###reference_###, we fix  as 1k and increase  gradually from 1k to 500k to examine the ability of sequential memory access.\nWe observe that even with a training passage count of 50k, the model could accurately reproduce over 70% of memorized validation passages.\nHowever, there is also a bottleneck in parametric memory: the performance drops to nearly zero when the passage count exceeds 100k.\nWe attribute this bottleneck to the difficulty in training, as the model fails to converge on memorizing all the passages.\nTherefore, in subsequent experiments, we carefully manage the corpus size to ensure that the model memorizes all passages.\nWe follow Mallick et al. (2023  ###reference_b24###) to place markers at the boundaries of each sentence, obtained by the NLTK sentence splitter333https://www.nltk.org/api/nltk.tokenize.sent_tokenize.html  ###reference_t_tokenize.html###: a passage is formatted as \u201c[0] sent0 [0] [1] sent1 [1], \u2026,\u201d. In this case, the model only needs to learn to copy the content between these markers. Our selective recitation task requires the model to recite the -th sentence of passage  based on the given passage ID . The reading function is now , such as \u201cWhat is sentence [1] of Document #2033?\u201d. For reference, we also test the model\u2019s performance in a baseline where the passage content is provided in the context window.\nAs we are testing for exact memorization, we use BLEU and EM scores to evaluate the model. Similar to \u00a73.2  ###reference_.SSS0.Px1###, we use  training and  validation passages, with 1994 sentences and 200 sentences respectively. We set the type of ID to be  and only include passages with more than 3 sentences. All other hyperparameters stay the same as \u00a73.2  ###reference_.SSS0.Px1###.\n###figure_3### We find that providing the passage ID does not enable the model to selectively recite the requested sentences. It scores poorly with a low EM of 34.5 and a 47.1 BLEU score, in contrast to the much higher 97.0 EM and 97.3 BLEU when the passage content is included in the context. A detailed analysis in Figure 3  ###reference_### reveals that the correct predictions are largely reciting the first sentence (). This verifies that the model can sequentially access the content to reproduce the first sentence. However, as the marker index increases, the model is required to skip preceding sentences and directly access a sentence in the middle of a passage. The model\u2019s performance sharply declines, indicating its inability to randomly access middle or later sentences in memorized passages.\nWe experiment with the well-known SQuAD-v1 (Rajpurkar et al., 2016  ###reference_b35###) dataset because many of its questions are closely dependent on the passage, such as \u201cHow did the war start?\u201d. This design compels the model to depend on the memorized IDs and passages rather than pre-existing knowledge. We explore the grounded QA task with variants of providing (1) the ID of the golden passage with the answer, (2) a random non-golden ID and (3) no ID. For comparison, we also consider the setups that do not involve writing passages to the model\u2019s parametric memory. These include (1) closed-book QA, where the model is fine-tuned solely on QA pairs, serving as a baseline to assess the model\u2019s reliance on prior knowledge for answering questions, and (2) open-book QA, where the golden passage content is concatenated with the question, setting the upper limit of extractive QA performance.\nWe experiment with different types of passage IDs. To ensure the uniqueness of using titles as passage IDs, we select  passages and  passages from the full SQuAD dataset, with over 2,000 and 300 questions respectively. The model is evaluated on F1 and EM following the original SQuAD evaluation script. The other hyperparameters are the same as mentioned in \u00a73.2  ###reference_.SSS0.Px1###.\nThe results are presented in Table 2  ###reference_### (the settings with \u201c+Recitation\u201d are discussed in later sections).\nAs expected, the model performs the best in the open-book setting, as it only needs to locate the answer in the golden passage. In contrast, the closed-book QA setup\nyields the worst performance, as the model has no access to passages and relies solely on its parametric knowledge stored during pretraining.\nInterestingly, the form of the provided passage ID has minimal impact on performance. We observe similar performance regardless of whether the golden ID is provided, except when the type of ID is Title. In this case, providing a random incorrect ID harms performance. We suspect that this is because the title is usually an entity related to the passage topic, therefore offering useful clues. In cases where the ID does not carry semantic meaning (i.e., Rare and Num), the correctness or presence of the ID does not significantly affect the performance, which remains substantially below the open-book setting, despite the model memorizing all passages. This further validates the model\u2019s inability to effectively access random memory, as it struggles to extract the answer even when provided with a correct passage ID.\nIn summary, our findings validate the hypothesis that LMs can effectively function as a memory bank, enabling sequential access to its memory.\nHowever, there are significant limitations in the model\u2019s ability to randomly access its memory. Across both the simple selective recitation and the complex grounded question-answering tasks, the model consistently fails to accomplish the tasks by leveraging its memory, despite being explicitly provided with the corresponding passage IDs."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Task Formulation",
            "text": "We abstract the language model as a memory bank and investigate its sequential or random access ability. We adopt a simple definition of a memory bank as a key\u2013value store , where  represents a unique identifier (ID) assigned to the content of the -th passage111We use the term \u201cdocument\u201d and \u201cpassage\u201d interchangeably in this paper, referring to a chunk of text..\nThere are two core functions that a memory bank needs to support: reading and writing.\nGiven that our memory bank is embodied as a language model, it is not straightforward to write and read the model\u2019s memory.\nFollowing previous work (Zhu and Li, 2023  ###reference_b56###; Wang et al., 2021  ###reference_b47###), for writing to the memory bank, we use fine-tuning to update the model\u2019s parameters.\nFor reading, we use prompting to elicit the model\u2019s memory.\nSpecifically, for each passage  with its corresponding identifier , we create two types of data instances: writing,  and reading, , where  and  denote the prompts detailed in Appendix A.1  ###reference_###.\nAs the primary goal of our study is to test whether the model can read (access) its stored content sequentially or randomly, we mainly vary the reading function across different experiments.\nGiven a corpus consisting of  passages, we split the corpus into two subsets:  training passages and  validation passages. We adopt a mixed training strategy as described by Zhu and Li (2023  ###reference_b56###): During the training stage, we include  and  instances of T training passages, as well as  instances of V validation passages. Our objective is for the model to learn to associate each identifier with its passage content by training on the reading and writing instances of the training passages. During evaluation, we prompt the model with the  instances of the V validation passages to test the model\u2019s memory access pattern."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Sequential Access: Full Recitation",
            "text": "We test the sequential access ability of the language model by asking it to reproduce the full passage content. Specifically, given an ID, the model is prompted to start from the beginning of the corresponding memorized passage and generate tokens consecutively. We evaluate the model\u2019s performance to reproduce the content on the  validation passages, which requires the model to both memorize the passage content and sequentially access the memory with the provided key.\nTo investigate whether the model can handle identifiers and passage content of different types, we set  and  and consider the following variations. For the type of passage content , we examine two categories: (1) natural language (NL), comprising Wikipedia paragraphs from SQuAD (Rajpurkar et al., 2016  ###reference_b35###  ###reference_b35###), and (2) random strings (Rand), where each NL passage is substituted with a space-separated alphanumeric string maintaining the same number of tokens. Regarding the type of  (i.e., passage IDs), we explore three forms: (1) numerical strings (Num), such as \u2018#123\u2019; (2) rare random tokens (Rare), adopting the approach of Ruiz et al. (2022  ###reference_b37###  ###reference_b37###) by random sampling three infrequent tokens; (3) article title (Title) of the Wikipedia page to which the passage belongs.\nWe adopt the GPT2-large model (Radford et al., 2019  ###reference_b34###  ###reference_b34###) with 774M parameters as the base model. For better string memorization ability (Stevens and Su, 2023  ###reference_b40###  ###reference_b40###), we use a pretrained checkpoint222https://huggingface.co/gpt2  ###reference_huggingface.co/gpt2###  ###reference_huggingface.co/gpt2### instead of training the model from scratch. We fine-tune the model for 100 epochs to ensure that the model fully converges, with a learning rate of . We measure memorization using both the BLEU score (Papineni et al., 2002  ###reference_b30###  ###reference_b30###) and the Exact Match (EM) score, indicating the similarity between the generated content and the ground-truth passage.\nTable 1  ###reference_###  ###reference_### shows that the model is able to sequentially access memorized content, with high BLEU and EM on validation passages. The model\u2019s sequential access capability is further demonstrated by its adaptability to varying types of IDs and passages. Specifically, using titles or numbers as keys for natural language passages achieves higher performance than using rare tokens. We suspect that models might have difficulty associating rare tokens with the natural language content. Remarkably, the model\u2019s access ability extends to passages composed of random characters ().\n###figure_4### To further test the memory capacity of the model, we carry out an additional experiment where we set the passage type to  and identifier type to  and construct passages each with 25 random tokens. As illustrated in Figure 2  ###reference_###  ###reference_###, we fix  as 1k and increase  gradually from 1k to 500k to examine the ability of sequential memory access.\nWe observe that even with a training passage count of 50k, the model could accurately reproduce over 70% of memorized validation passages.\nHowever, there is also a bottleneck in parametric memory: the performance drops to nearly zero when the passage count exceeds 100k.\nWe attribute this bottleneck to the difficulty in training, as the model fails to converge on memorizing all the passages.\nTherefore, in subsequent experiments, we carefully manage the corpus size to ensure that the model memorizes all passages."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Random Access: Selective Recitation",
            "text": "Selective recitation is a straightforward synthetic task: asking the language model to reproduce a specific sentence of a memorized passage. This task is designed for its simplicity, as it does not require the model\u2019s understanding of passage content. The focus is solely on the model\u2019s capacity to access segments in a memorized passage. Successful random access would be indicated by the model\u2019s ability to reproduce any sentence from within memorized passages, regardless of position.\nWe follow Mallick et al. (2023  ###reference_b24###  ###reference_b24###) to place markers at the boundaries of each sentence, obtained by the NLTK sentence splitter333https://www.nltk.org/api/nltk.tokenize.sent_tokenize.html  ###reference_t_tokenize.html###  ###reference_t_tokenize.html###: a passage is formatted as \u201c[0] sent0 [0] [1] sent1 [1], \u2026,\u201d. In this case, the model only needs to learn to copy the content between these markers. Our selective recitation task requires the model to recite the -th sentence of passage  based on the given passage ID . The reading function is now , such as \u201cWhat is sentence [1] of Document #2033?\u201d. For reference, we also test the model\u2019s performance in a baseline where the passage content is provided in the context window.\nAs we are testing for exact memorization, we use BLEU and EM scores to evaluate the model. Similar to \u00a73.2  ###reference_.SSS0.Px1###  ###reference_.SSS0.Px1###, we use  training and  validation passages, with 1994 sentences and 200 sentences respectively. We set the type of ID to be  and only include passages with more than 3 sentences. All other hyperparameters stay the same as \u00a73.2  ###reference_.SSS0.Px1###  ###reference_.SSS0.Px1###.\n###figure_5### We find that providing the passage ID does not enable the model to selectively recite the requested sentences. It scores poorly with a low EM of 34.5 and a 47.1 BLEU score, in contrast to the much higher 97.0 EM and 97.3 BLEU when the passage content is included in the context. A detailed analysis in Figure 3  ###reference_###  ###reference_### reveals that the correct predictions are largely reciting the first sentence (). This verifies that the model can sequentially access the content to reproduce the first sentence. However, as the marker index increases, the model is required to skip preceding sentences and directly access a sentence in the middle of a passage. The model\u2019s performance sharply declines, indicating its inability to randomly access middle or later sentences in memorized passages."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Random Access: Grounded Question Answering",
            "text": "Building on our earlier finding \u00a73.2  ###reference_### that the model can memorize many passages each linked to a unique ID, we embark on a more pragmatic task: question answering grounded in a specific passage ID.\nThis task aims to evaluate whether the model can provide answers to questions by extracting a span from its memory.\nFor instance, a question might be framed as \u201cAccording to document #3022, in what year did Chopin become a French citizen?\u201d and the answer is \u201c1835\u201d in the passage with ID #3022.\nWe hypothesize that if LMs are capable of random memory access, they should navigate to the corresponding passage using the provided ID and extract the relevant span to answer the questions.\nWe experiment with the well-known SQuAD-v1 (Rajpurkar et al., 2016  ###reference_b35###  ###reference_b35###) dataset because many of its questions are closely dependent on the passage, such as \u201cHow did the war start?\u201d. This design compels the model to depend on the memorized IDs and passages rather than pre-existing knowledge. We explore the grounded QA task with variants of providing (1) the ID of the golden passage with the answer, (2) a random non-golden ID and (3) no ID. For comparison, we also consider the setups that do not involve writing passages to the model\u2019s parametric memory. These include (1) closed-book QA, where the model is fine-tuned solely on QA pairs, serving as a baseline to assess the model\u2019s reliance on prior knowledge for answering questions, and (2) open-book QA, where the golden passage content is concatenated with the question, setting the upper limit of extractive QA performance.\nWe experiment with different types of passage IDs. To ensure the uniqueness of using titles as passage IDs, we select  passages and  passages from the full SQuAD dataset, with over 2,000 and 300 questions respectively. The model is evaluated on F1 and EM following the original SQuAD evaluation script. The other hyperparameters are the same as mentioned in \u00a73.2  ###reference_.SSS0.Px1###  ###reference_.SSS0.Px1###.\nThe results are presented in Table 2  ###reference_###  ###reference_### (the settings with \u201c+Recitation\u201d are discussed in later sections).\nAs expected, the model performs the best in the open-book setting, as it only needs to locate the answer in the golden passage. In contrast, the closed-book QA setup\nyields the worst performance, as the model has no access to passages and relies solely on its parametric knowledge stored during pretraining.\nInterestingly, the form of the provided passage ID has minimal impact on performance. We observe similar performance regardless of whether the golden ID is provided, except when the type of ID is Title. In this case, providing a random incorrect ID harms performance. We suspect that this is because the title is usually an entity related to the passage topic, therefore offering useful clues. In cases where the ID does not carry semantic meaning (i.e., Rare and Num), the correctness or presence of the ID does not significantly affect the performance, which remains substantially below the open-book setting, despite the model memorizing all passages. This further validates the model\u2019s inability to effectively access random memory, as it struggles to extract the answer even when provided with a correct passage ID.\nIn summary, our findings validate the hypothesis that LMs can effectively function as a memory bank, enabling sequential access to its memory.\nHowever, there are significant limitations in the model\u2019s ability to randomly access its memory. Across both the simple selective recitation and the complex grounded question-answering tasks, the model consistently fails to accomplish the tasks by leveraging its memory, despite being explicitly provided with the corresponding passage IDs."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Mitigating Random Access Challenge",
            "text": "The earlier experiments show that in general, language models perform well in sequentially accessing their parametric memory, but encounter challenges in random memory access. This naturally raises the question: How can we mitigate the shortcomings in random memory access?\nWe extend the earlier experiments\nby integrating recitation and permutation into the respective reading and writing stages.\nFirst, we add a setup to the selective sentence recitation task: Based on the given ID, the model is tasked to first recite the entire content of the corresponding passage and then the specific sentence, altering the reading operation to . Similarly, for the grounded QA task, we ask the model to recite the passage associated with the input passage ID before answering the question. In the setup without an ID, the model is still trained to recite the golden passage.\nTo explore the effect of permutation during the writing stage, we perform permutation among sentences in a passage to create diverse  instances. For a -sentence passage we tested:\n(1) first, moving each sentence to the passage\u2019s beginning to create  unique instances;\n(2) random, randomly shuffling the sentences  times to create  instances, where  is set to 4 by default;\nReciting the passage content effectively boosts the performance of selective recitation, as evidenced in Table 3  ###reference_###. With recitation, the model first sequentially accesses the content from its memory using the provided passage ID and subsequently loads this passage in the context to allow for random access. Conditioned on the recited content in the context, the model can therefore easily identify the correct sentence.\nSimilarly, explicitly reciting the golden passages markedly enhances question-answering performance, as shown in Table 2  ###reference_###. This observation is consistent across all three types of passage IDs. Conversely,\nintentionally prompting the model to recite a random passage leads to a decline in performance. This is likely because random passages introduce irrelevant information and confuse the model. Surprisingly, the recitation of relevant passages benefits performance even without an ID, although the improvement is smaller than with the golden ID. This verifies the effectiveness of recitation in more general settings of question answering.\nAnother way of enhancing random access is to perform permutation of sentences, as presented in Table 3  ###reference_###. Simply bringing every sentence to the start of the passage once (first) or randomly permuting the sentences many times (random) helps to solve the challenge of accessing the middle content of a passage. We also find that permutation enhances grounded QA performance (Appendix C  ###reference_###) from 26.7 to 31.3 in terms of EM. However, it is noteworthy that permutation does not alter the inherent sequential access pattern of parametric memory. Rather, by permuting the sentences and disrupting their original order, we allow more sentences to be sequentially accessible via the ID."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Proposed Method",
            "text": "To address the challenge, we start from the two operations supported by LMs as a memory store: reading and writing. During the writing phase, we hypothesize that performing permutation on the passage content can naturally enhance the model\u2019s random access ability: any part of the content can be the starting point of a memorized sequence. In this setup, we change the sequential order of passage content to achieve random access.\n###figure_6### On the other hand, during the reading phase, leveraging the model\u2019s context window presents a viable strategy. The attention mechanism (Vaswani et al., 2017  ###reference_b46###) enables the model to access any token within the context window, thereby inherently supporting random access (Packer et al., 2023  ###reference_b29###; Ge et al., 2023  ###reference_b10###). For tasks with a given ID, we could ask the model to sequentially recite the passage first, place it within the context, and subsequently query the model to perform span extraction tasks utilizing this context, as illustrated in Figure 4  ###reference_###.\nOur subsequent experiments are designed to evaluate the effectiveness of these two methods. Through empirical evaluation, we validate that content permutation during writing or recitation during reading can largely mitigate the challenge of random memory access and enhance performance.\nWe extend the earlier experiments by integrating recitation and permutation into the respective reading and writing stages.\nFirst, we add a setup to the selective sentence recitation task: Based on the given ID, the model is tasked to first only provide the passage ID without any recitation and then the specific sentence, altering the reading operation to . Similarly, for the grounded QA task, we ask the model to recite the passage associated with the input passage ID before answering the question. In the setup without an ID, the model is still trained to recite the golden passage.\nTo explore the effect of permutation during the writing stage, we perform permutation among sentences in a passage to create diverse instances. For a -sentence passage we tested:\n(1) first, moving each sentence to the passage\u2019s beginning to create unique instances;\n(2) random, randomly shuffling the sentences times to create instances, where is set to 4 by default;\nReciting the passage content effectively boosts the performance of selective recitation, as evidenced in Table 3  ###reference_###  ###reference_###. With recitation, the model first sequentially accesses the content from its memory using the provided passage ID and subsequently loads this passage in the context to allow for random access. Conditioned on the recited content in the context, the model can therefore easily identify the correct sentence.\nSimilarly, explicitly reciting the golden passages markedly enhances question-answering performance, as shown in Table 2  ###reference_###  ###reference_###. This observation is consistent across all three types of passage IDs. Conversely,\nintentionally prompting the model to recite a random passage leads to a decline in performance. This is likely because random passages introduce irrelevant information and confuse the model. Surprisingly, the recitation of relevant passages benefits performance even without an ID, although the improvement is smaller than with the golden ID. This verifies the effectiveness of recitation in more general settings of question answering.\nAnother way of enhancing random access is to perform permutation of sentences, as presented in Table 3  ###reference_###  ###reference_###. Simply bringing every sentence to the start of the passage once (first) or randomly permuting the sentences many times (random) helps to solve the challenge of accessing the middle content of a passage. We also find that permutation enhances grounded QA performance (Appendix C  ###reference_###  ###reference_###) from 26.7 to 31.3 in terms of EM. However, it is noteworthy that permutation does not alter the inherent sequential access pattern of parametric memory. Rather, by permuting the sentences and disrupting their original order, we allow more sentences to be sequentially accessible via the ID."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Case Study: Open-Domain Question Answering",
            "text": "###table_1### Our findings indicate that language models struggle with random memory access, unless the memory is explicitly recited and thus loaded into the context which can be accessed randomly. Building on this insight, we extend our study to the task of open-domain question answering, a challenging task that requires the model to first retrieve relevant memories and reason over them. This is different from previous experiments as the passage IDs are no longer provided as the input: The reading operation becomes . The model therefore needs to find relevant passages to the query without the aid of passage IDs, which is a non-trivial task (Pradeep et al., 2023  ###reference_b33###). As the goal of our study is not on retrieval performance and our earlier results (\u00a73.3  ###reference_.SSS0.Px2###) show that the model has limited memorization capacity, we reduce the difficulty of retrieval by limiting the number of passages written to the model\u2019s memory: we only include positive passages that contain answers to at least one question.\nWe aim to test the model\u2019s ability to perform random access in real applications. Specifically, we investigate whether the model, having memorized many passages, can accurately extract answers from its memory. Similar to the previous experiments, we also aim to observe the difference in the model\u2019s performance when it is trained to recite relevant passages and subsequently answer the question.\nWe opt not to experiment with permutation due to the high training cost associated with sentence permutation across a large number of passages, and leave this avenue for future work."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Experimental Setup",
            "text": "We use Natural Questions (Kwiatkowski et al., 2019  ###reference_b17###) processed by Karpukhin et al. (2020  ###reference_b16###) for single-hop QA, selecting 6000 training and all of the 6489 validation questions, with a total of 10.9k passages. For multi-hop question answering, we use HotpotQA (Yang et al., 2018  ###reference_b50###) where each question has two golden passages. We select 8k training and all the 7405 validation questions in the distractor subset, with a total of 26.9k passages.\nWe start from a baseline setup where the training only involves QA pairs, i.e., closed-book QA. Next, we consider two types of training strategies to write the passages into the memory. In the mixed setting, the model is fine-tuned on a mixture of the  instances of all passages and training QA pairs. In the continual setting, the model is fine-tuned on  of all passages first, followed by fine-tuning on QA. To test the effectiveness of recitation, we also include settings where the model is trained to recite the golden passage(s) before answering.\nAs the task requires the model to perform both passage retrieval and question answering, we expect that the model size should be sufficiently large. Therefore, we opt for GPT2-XL with 1.5B parameters. In the mixed setting, we train the model for 20 epochs with a learning rate of 3e-5. In the continual setting, we first train 20 epochs on the passages, followed by another 20 epochs on QA pairs. We report the best performance based on the EM score on validation questions."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Results and Discussion",
            "text": "The results presented in Table 4  ###reference_### demonstrate that writing golden passages into the model\u2019s memory, with either mixed or continual training, leads to improved performance over the baseline closed-book setting. This aligns with our expectations, as we deliberately inject passages containing the answers to the questions into the memory, enriching the model\u2019s knowledge.\nMoreover, recitation significantly enhances the model\u2019s ability to utilize and access memorized passages, leading to a noticeable improvement in performance. This is observed in both the mixed and continual training settings. The exact match score increases significantly by more than 3% in both single and multi-hop QA. When the model explicitly recites the passages and loads them into the context for random access, the original open-domain QA task is reduced to an easier task of extractive QA. However, the low recitation BLEU score suggests that the model does not always accurately recite the golden passage. We expect that the performance could be further enhanced if it can accurately retrieve relevant passages from memory.\nThe mixed training strategy outperforms the continual training setup. This is likely because the model\u2019s memory of passage content is refreshed constantly in mixed training. In continual training, however, the second stage only involves QA pairs on training passages, potentially leading to fading memory of validation passages. Consequently, the recitation becomes less accurate, as shown by a decrease in the BLEU score.\nOur results are consistent and complementary to the findings of Wei et al. (2023  ###reference_b49###) and Sun et al. (2023  ###reference_b41###): introducing intermediate steps or generating relevant passages helps to improve model performance on various tasks. We provide an alternative interpretation for this phenomenon: loading the parametric memory into the context window facilitates enhanced random access to memorized information, and the model benefits from such enhancements."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We empirically study how language models access their parametric memory. Our experiments on both synthetic and realistic data demonstrate that while language models can adequately reproduce memorized content in a sequential manner, they struggle with the random access of segments in the middle of memorized content. We identify two effective strategies of recitation and permutation to mitigate the limitation of random memory access. Furthermore, through a controlled case study on open-domain question answering, we illustrate that allowing the model to recite and randomly access its memory significantly improves performance. Overall, our study not only provides a deeper understanding of memory access patterns in language models, but also highlights the implications of limited random memory access ability in practical application of language models."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A Prompts",
            "text": ""
        },
        {
            "section_id": "Appendix 2",
            "parent_section_id": null,
            "section_name": "Appendix B Additional Selective Recitation Experiments",
            "text": "We provide additional experimental results for our selective recitation task of reciting sentences. All of the experiments lead to a consistent conclusion that the model is unable to randomly extract a sentence from a memorized passage.\nIn both of the experiments below, we include setups of (1) in-context: the passage is included in the context window. (2) ID-guided: the basic version of the selective recitation task where a passage ID is provided. and (3) with passage recitation: the passage is recited first before sentence recitation."
        },
        {
            "section_id": "Appendix 3",
            "parent_section_id": null,
            "section_name": "Appendix C Additional Grounded Question Answering Experiments with Permutation",
            "text": "We conduct additional experiments showing the effect of performing permutations of sentences of memorized passages in the grounded question-answering task.\nIn Table 7  ###reference_###, we show the effect of augmenting the  instances to include sentence permutations of the original passage. We observe that the model\u2019s performance generally improves as we perform permutation. This validates our findings that performing permutation enhances random access to the passage content."
        },
        {
            "section_id": "Appendix 4",
            "parent_section_id": null,
            "section_name": "Appendix D Additional Open-Domain Question Answering Experiments",
            "text": "To ensure that our conclusion is consistent with different dataset sizes, we vary the number of training and validation documents and questions to observe the performance difference. For NQ, we select 5k training and 5k validation QA pairs, forming a corpus containing around 9k passages. For Hotpot QA, we select 5k training and 5k validation questions in the distractor subset, with a total of 18.2k passages.\nIn Table 8  ###reference_###, we obtain similar conclusions that recitation greatly enhances question-answering performance, and using a mixed training strategy is better than continual training because of the increase in recitation score.\n###table_2###"
        },
        {
            "section_id": "Appendix 5",
            "parent_section_id": null,
            "section_name": "Appendix E Additional Training Details",
            "text": "We conduct all experiments in a cluster with NVIDIA Tesla A100 GPUs (with 40G or 80G memory). Experiments in \u00a73.2  ###reference_### take a total of 48 hours on 4 GPUs. Selective sentence recitation experiments in \u00a73.3  ###reference_### and \u00a74  ###reference_### take a total of 41 hours on 4 GPUs. Grounded QA experiments take a total of 132 hours on 4 GPUs. The open-domain QA experiments need 3 days to complete with 32 GPUs.\nWe use the Huggingface transformers library for all experiments.\nWe use a learning rate of 3e-5. We set a constant learning rate schedule for the open-domain QA experiments. For all other experiments, we use a warmup ratio of 0.05 and a linear decay learning rate. We evaluate the model\u2019s performance on the validation set at the end of each epoch."
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T1\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.1\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" id=\"S3.T1.1.1.1.1\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.2.1\">Title (ID)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T1.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.3.1\">Num (ID)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T1.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.4.1\">Rare (ID)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.1.2.1.1\">psg=<span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.1.2.1.1.1\">NL</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.2.1.2\">96.2\u2009/\u200985.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.2.1.3\">96.7\u2009/\u200995.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.2.1.4\">73.4\u2009/\u200972.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S3.T1.1.3.2.1\">psg=<span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.1.3.2.1.1\">Rand</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.1.3.2.2\">96.7\u2009/\u200995.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.1.3.2.3\">96.7\u2009/\u200995.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.1.3.2.4\">96.7\u2009/\u200995.0</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>BLEU\u2009/\u2009Exact Match scores of reading from memory with different types of IDs and passage content. </figcaption>\n</figure>",
            "capture": "Table 1: BLEU\u2009/\u2009Exact Match scores of reading from memory with different types of IDs and passage content. "
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T2\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T2.3\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T2.3.4.1\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" id=\"S3.T2.3.4.1.1\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S3.T2.3.4.1.2\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.3.4.1.2.1\">Title (ID Type)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S3.T2.3.4.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.3.4.1.3.1\">Rare (ID Type)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S3.T2.3.4.1.4\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.3.4.1.4.1\">Num (ID Type)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.3.5.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.3.5.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.3.5.2.1.1\">Setup</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.3.5.2.2\">EM</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.3.5.2.3\">F1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.3.5.2.4\">EM</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.3.5.2.5\">F1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.3.5.2.6\">EM</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.3.5.2.7\">F1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.3.6.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"7\" id=\"S3.T2.3.6.3.1\">\n<span class=\"ltx_text ltx_font_italic\" id=\"S3.T2.3.6.3.1.1\">w.o. passage memorization</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.3.7.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.3.7.4.1\">Closed-Book QA</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.7.4.2\">9.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.7.4.3\">16.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.7.4.4\">9.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.7.4.5\">16.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.7.4.6\">9.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.7.4.7\">16.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.3.8.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.3.8.5.1\">Open-Book QA</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.8.5.2\">73.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.8.5.3\">79.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.8.5.4\">73.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.8.5.5\">79.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.8.5.6\">73.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.8.5.7\">79.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.3.9.6\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"7\" id=\"S3.T2.3.9.6.1\">\n<span class=\"ltx_text ltx_font_italic\" id=\"S3.T2.3.9.6.1.1\">w. passage memorization</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.3.10.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.3.10.7.1\">Grounded QA <span class=\"ltx_text ltx_font_italic\" id=\"S3.T2.3.10.7.1.1\">w. Golden ID</span>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.10.7.2\">26.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.10.7.3\">35.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.10.7.4\">20.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.10.7.5\">28.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.10.7.6\">24.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.10.7.7\">32.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.1.1.1\">\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Recitation</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.2.1\">59.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.3.1\">68.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.4.1\">54.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.5.1\">62.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.6.1\">57.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.7.1\">66.2</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.3.11.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.3.11.8.1\">Grounded QA <span class=\"ltx_text ltx_font_italic\" id=\"S3.T2.3.11.8.1.1\">w. Random ID</span>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.11.8.2\">20.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.11.8.3\">28.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.11.8.4\">20.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.11.8.5\">28.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.11.8.6\">23.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.11.8.7\">31.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.2.2.1\">\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Recitation</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.2.2.2\">16.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.2.2.3\">20.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.2.2.4\">18.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.2.2.5\">23.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.2.2.6\">18.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.2.2.7\">23.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.3.12.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.3.12.9.1\">Grounded QA <span class=\"ltx_text ltx_font_italic\" id=\"S3.T2.3.12.9.1.1\">w.o. ID</span>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.12.9.2\">22.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.12.9.3\">31.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.12.9.4\">22.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.12.9.5\">31.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.12.9.6\">22.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.12.9.7\">31.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S3.T2.3.3.1\">\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Recitation</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.3.3.2\">26.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.3.3.3\">33.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.3.3.4\">26.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.3.3.5\">33.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.3.3.6\">26.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.3.3.7\">33.1</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>EM and F1 scores for grounded question answering tasks, as well as baselines on closed-book and open-book QA. Numbers in bold represent the best performance in the grounded QA setting.</figcaption>\n</figure>",
            "capture": "Table 2: EM and F1 scores for grounded question answering tasks, as well as baselines on closed-book and open-book QA. Numbers in bold represent the best performance in the grounded QA setting."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T3\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T3.3\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T3.3.4.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S3.T3.3.4.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.3.4.1.1.1\">Setup</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.3.4.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.3.4.1.2.1\">BLEU</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.3.4.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.3.4.1.3.1\">EM</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T3.3.5.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T3.3.5.1.1\">Baseline</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T3.3.5.1.2\">47.1</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T3.3.5.1.3\">34.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T3.1.1.1\">\u00a0\u00a0\u00a0 Recitation</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T3.1.1.2\">99.3</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T3.1.1.3\">98.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T3.2.2.1\">\u00a0\u00a0\u00a0 Permutation (<span class=\"ltx_text ltx_font_italic\" id=\"S3.T3.2.2.1.1\">first</span>)</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T3.2.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.2.2.2.1\">100.0</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T3.2.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.2.2.3.1\">100.0</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S3.T3.3.3.1\">\u00a0\u00a0\u00a0 Permutation (<span class=\"ltx_text ltx_font_italic\" id=\"S3.T3.3.3.1.1\">random</span>)</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T3.3.3.2\">98.0</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T3.3.3.3\">97.0</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>BLEU score and EM score of selective sentence recitation experiments after introducing passage recitation and permutation.</figcaption>\n</figure>",
            "capture": "Table 3: BLEU score and EM score of selective sentence recitation experiments after introducing passage recitation and permutation."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T4\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S5.T4.2\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T4.2.3.1\">\n<td class=\"ltx_td ltx_border_tt\" id=\"S5.T4.2.3.1.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" id=\"S5.T4.2.3.1.2\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.2.3.1.2.1\">NQ</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" id=\"S5.T4.2.3.1.3\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.2.3.1.3.1\">Hotpot QA</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.2.4.2\">\n<td class=\"ltx_td\" id=\"S5.T4.2.4.2.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T4.2.4.2.2\">EM</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T4.2.4.2.3\">F1</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T4.2.4.2.4\">Recite BLEU</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T4.2.4.2.5\">EM</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T4.2.4.2.6\">F1</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T4.2.4.2.7\">Recite BLEU</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.2.5.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T4.2.5.3.1\">Closed-Book QA</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T4.2.5.3.2\">10.1</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T4.2.5.3.3\">14.8</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T4.2.5.3.4\">-</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T4.2.5.3.5\">13.1</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T4.2.5.3.6\">20.1</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T4.2.5.3.7\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.2.6.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T4.2.6.4.1\">Closed-Book QA <span class=\"ltx_text ltx_font_italic\" id=\"S5.T4.2.6.4.1.1\">w. Mix Training</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T4.2.6.4.2\">12.6</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T4.2.6.4.3\">18.2</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T4.2.6.4.4\">-</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T4.2.6.4.5\">15.7</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T4.2.6.4.6\">22.8</td>\n<td class=\"ltx_td ltx_border_t\" id=\"S5.T4.2.6.4.7\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.1\">\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Recitation</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T4.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.2.1\">16.1</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T4.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.3.1\">20.1</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T4.1.1.4\">28.6</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T4.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.5.1\">21.0</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T4.1.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.6.1\">28.4</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T4.1.1.7\">51.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.2.7.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.2.7.5.1\">Closed-Book QA <span class=\"ltx_text ltx_font_italic\" id=\"S5.T4.2.7.5.1.1\">w. Continual Training</span>\n</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T4.2.7.5.2\">10.3</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T4.2.7.5.3\">15.5</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T4.2.7.5.4\">-</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T4.2.7.5.5\">15.1</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T4.2.7.5.6\">22.4</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T4.2.7.5.7\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.2.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T4.2.2.1\">\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Recitation</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S5.T4.2.2.2\">13.4</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S5.T4.2.2.3\">16.9</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S5.T4.2.2.4\">25.6</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S5.T4.2.2.5\">18.1</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S5.T4.2.2.6\">25.2</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S5.T4.2.2.7\">48.3</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>EM and F1 of open-domain question answering datasets. We report the BLEU score of the recitation when the model is trained to recite the passage first and then offer an answer. Best performances are in bold.</figcaption>\n</figure>",
            "capture": "Table 4: EM and F1 of open-domain question answering datasets. We report the BLEU score of the recitation when the model is trained to recite the passage first and then offer an answer. Best performances are in bold."
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"A2.T5\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A2.T5.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A2.T5.1.2.1\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" id=\"A2.T5.1.2.1.1\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"A2.T5.1.2.1.2\">Recite First</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"A2.T5.1.2.1.3\">Recite Second</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"A2.T5.1.2.1.4\">Recite Last</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T5.1.3.2\">\n<th class=\"ltx_td ltx_th ltx_th_row\" id=\"A2.T5.1.3.2.1\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T5.1.3.2.2\">BLEU</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T5.1.3.2.3\">EM</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T5.1.3.2.4\">BLEU</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T5.1.3.2.5\">EM</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T5.1.3.2.6\">BLEU</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T5.1.3.2.7\">EM</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A2.T5.1.4.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A2.T5.1.4.1.1\">In-context</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T5.1.4.1.2\">97.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T5.1.4.1.3\">95.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T5.1.4.1.4\">94.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T5.1.4.1.5\">95.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T5.1.4.1.6\">91.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T5.1.4.1.7\">87.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T5.1.5.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T5.1.5.2.1\">ID-guided</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T5.1.5.2.2\">99.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T5.1.5.2.3\">97.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T5.1.5.2.4\">14.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T5.1.5.2.5\">5.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T5.1.5.2.6\">17.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T5.1.5.2.7\">0.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T5.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"A2.T5.1.1.1\">\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Recitation</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T5.1.1.2\">99.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T5.1.1.3\">95.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T5.1.1.4\">98.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T5.1.1.5\">87.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T5.1.1.6\">98.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T5.1.1.7\">85.0</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>BLEU and EM score of reciting the first, second or last sentence of a memorized passage. </figcaption>\n</figure>",
            "capture": "Table 5: BLEU and EM score of reciting the first, second or last sentence of a memorized passage. "
        },
        "6": {
            "table_html": "<figure class=\"ltx_table\" id=\"A2.T6\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A2.T6.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A2.T6.1.2.1\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" id=\"A2.T6.1.2.1.1\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"A2.T6.1.2.1.2\">Recite Next Sentence</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"A2.T6.1.2.1.3\">Recite Previous Sentence</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T6.1.3.2\">\n<th class=\"ltx_td ltx_th ltx_th_row\" id=\"A2.T6.1.3.2.1\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T6.1.3.2.2\">BLEU</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T6.1.3.2.3\">EM</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T6.1.3.2.4\">BLEU</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T6.1.3.2.5\">EM</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A2.T6.1.4.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A2.T6.1.4.1.1\">In-context</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.1.4.1.2\">98.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.1.4.1.3\">96.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.1.4.1.4\">82.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.1.4.1.5\">79.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T6.1.5.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T6.1.5.2.1\">ID-guided</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T6.1.5.2.2\">86.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T6.1.5.2.3\">81.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T6.1.5.2.4\">20.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T6.1.5.2.5\">18.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T6.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"A2.T6.1.1.1\">\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Recitation</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T6.1.1.2\">98.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T6.1.1.3\">85.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T6.1.1.4\">96.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T6.1.1.5\">81.0</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 6: </span>BLEU and EM score of reciting the next or previous sentence given an input sentence. </figcaption>\n</figure>",
            "capture": "Table 6: BLEU and EM score of reciting the next or previous sentence given an input sentence. "
        },
        "7": {
            "table_html": "<figure class=\"ltx_table\" id=\"A3.T7\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A3.T7.5\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A3.T7.5.6.1\">\n<td class=\"ltx_td ltx_border_tt\" id=\"A3.T7.5.6.1.1\"></td>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"A3.T7.5.6.1.2\">ID=Title</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"A3.T7.5.6.1.3\">ID=Rare</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"A3.T7.5.6.1.4\">ID=Num</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T7.5.7.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column\" id=\"A3.T7.5.7.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T7.5.7.2.1.1\">Setup</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"A3.T7.5.7.2.2\">EM</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"A3.T7.5.7.2.3\">F1</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"A3.T7.5.7.2.4\">EM</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"A3.T7.5.7.2.5\">F1</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"A3.T7.5.7.2.6\">EM</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"A3.T7.5.7.2.7\">F1</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T7.5.8.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A3.T7.5.8.3.1\">Grounded QA w. <span class=\"ltx_text ltx_font_italic\" id=\"A3.T7.5.8.3.1.1\">Golden ID</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A3.T7.5.8.3.2\">26.7</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A3.T7.5.8.3.3\">35.6</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A3.T7.5.8.3.4\">20.7</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A3.T7.5.8.3.5\">28.7</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A3.T7.5.8.3.6\">24.3</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A3.T7.5.8.3.7\">32.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T7.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"A3.T7.1.1.1\">\u00a0\u00a0\u00a0\u00a0 Permutation (<span class=\"ltx_text ltx_font_italic\" id=\"A3.T7.1.1.1.1\">first</span>)</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A3.T7.1.1.2\">27.7</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A3.T7.1.1.3\">39.8</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A3.T7.1.1.4\">27.0</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A3.T7.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T7.1.1.5.1\">37.7</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"A3.T7.1.1.6\">27.7</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A3.T7.1.1.7\">37.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T7.2.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"A3.T7.2.2.1\">\u00a0\u00a0\u00a0\u00a0 Permutation (<span class=\"ltx_text ltx_font_italic\" id=\"A3.T7.2.2.1.1\">rand-1</span>)</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A3.T7.2.2.2\">25.7</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A3.T7.2.2.3\">35.0</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A3.T7.2.2.4\">19.0</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A3.T7.2.2.5\">27.5</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A3.T7.2.2.6\">19.7</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A3.T7.2.2.7\">28.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T7.3.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"A3.T7.3.3.1\">\u00a0\u00a0\u00a0\u00a0 Permutation (<span class=\"ltx_text ltx_font_italic\" id=\"A3.T7.3.3.1.1\">rand-2</span>)</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A3.T7.3.3.2\">26.0</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A3.T7.3.3.3\">35.6</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A3.T7.3.3.4\">25.7</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A3.T7.3.3.5\">33.7</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A3.T7.3.3.6\">23.7</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A3.T7.3.3.7\">32.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T7.4.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"A3.T7.4.4.1\">\u00a0\u00a0\u00a0\u00a0 Permutation (<span class=\"ltx_text ltx_font_italic\" id=\"A3.T7.4.4.1.1\">rand-4</span>)</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A3.T7.4.4.2\">29.7</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A3.T7.4.4.3\">38.5</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A3.T7.4.4.4\">25.3</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A3.T7.4.4.5\">35.6</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A3.T7.4.4.6\">25.0</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A3.T7.4.4.7\">34.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T7.5.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A3.T7.5.5.1\">\u00a0\u00a0\u00a0\u00a0 Permutation (<span class=\"ltx_text ltx_font_italic\" id=\"A3.T7.5.5.1.1\">rand-8</span>)</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"A3.T7.5.5.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T7.5.5.2.1\">31.3</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"A3.T7.5.5.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T7.5.5.3.1\">40.1</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"A3.T7.5.5.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T7.5.5.4.1\">27.7</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"A3.T7.5.5.5\">36.7</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"A3.T7.5.5.6\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T7.5.5.6.1\">29.0</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"A3.T7.5.5.7\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T7.5.5.7.1\">38.3</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 7: </span>The EM and F1 score of performing sentence permutation during the writing phase. <span class=\"ltx_text ltx_font_italic\" id=\"A3.T7.9.1\">rand-k</span> means that permutation is performed  times. </figcaption>\n</figure>",
            "capture": "Table 7: The EM and F1 score of performing sentence permutation during the writing phase. rand-k means that permutation is performed  times. "
        },
        "8": {
            "table_html": "<figure class=\"ltx_table\" id=\"A4.T8\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"A4.T8.2\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A4.T8.2.3.1\">\n<td class=\"ltx_td ltx_border_tt\" id=\"A4.T8.2.3.1.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" id=\"A4.T8.2.3.1.2\">\n<span class=\"ltx_text ltx_font_bold\" id=\"A4.T8.2.3.1.2.1\">NQ</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" id=\"A4.T8.2.3.1.3\">\n<span class=\"ltx_text ltx_font_bold\" id=\"A4.T8.2.3.1.3.1\">Hotpot QA</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T8.2.4.2\">\n<td class=\"ltx_td\" id=\"A4.T8.2.4.2.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T8.2.4.2.2\">EM</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T8.2.4.2.3\">F1</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T8.2.4.2.4\">Recite BLEU</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T8.2.4.2.5\">EM</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T8.2.4.2.6\">F1</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T8.2.4.2.7\">Recite BLEU</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T8.2.5.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T8.2.5.3.1\">Closed-Book QA</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T8.2.5.3.2\">9.1</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T8.2.5.3.3\">13.7</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T8.2.5.3.4\">-</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T8.2.5.3.5\">13.3</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T8.2.5.3.6\">20.4</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T8.2.5.3.7\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T8.2.6.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T8.2.6.4.1\">Closed-Book QA <span class=\"ltx_text ltx_font_italic\" id=\"A4.T8.2.6.4.1.1\">w. Mix Training</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T8.2.6.4.2\">11.5</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T8.2.6.4.3\">17.2</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T8.2.6.4.4\">-</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T8.2.6.4.5\">15.9</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T8.2.6.4.6\">23.6</td>\n<td class=\"ltx_td ltx_border_t\" id=\"A4.T8.2.6.4.7\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T8.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T8.1.1.1\">\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Recitation</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A4.T8.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A4.T8.1.1.2.1\">15.7</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"A4.T8.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A4.T8.1.1.3.1\">19.7</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"A4.T8.1.1.4\">29.1</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A4.T8.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"A4.T8.1.1.5.1\">20.8</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"A4.T8.1.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"A4.T8.1.1.6.1\">28.4</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"A4.T8.1.1.7\">50.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T8.2.7.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T8.2.7.5.1\">Closed-Book QA <span class=\"ltx_text ltx_font_italic\" id=\"A4.T8.2.7.5.1.1\">w. Continual Training</span>\n</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A4.T8.2.7.5.2\">10.3</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A4.T8.2.7.5.3\">15.5</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A4.T8.2.7.5.4\">-</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A4.T8.2.7.5.5\">15.1</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A4.T8.2.7.5.6\">22.8</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A4.T8.2.7.5.7\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T8.2.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A4.T8.2.2.1\">\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Recitation</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"A4.T8.2.2.2\">12.3</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"A4.T8.2.2.3\">15.8</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"A4.T8.2.2.4\">24.2</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"A4.T8.2.2.5\">18.2</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"A4.T8.2.2.6\">25.6</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"A4.T8.2.2.7\">49.2</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 8: </span>EM and F1 of the model\u2019s QA performance on different subsets of NQ and Hotpot QA datasets. We report the BLEU score of the recitation when the model is trained to recite the passage first and then provide an answer. The bold numbers are the best-performing setup.</figcaption>\n</figure>",
            "capture": "Table 8: EM and F1 of the model\u2019s QA performance on different subsets of NQ and Hotpot QA datasets. We report the BLEU score of the recitation when the model is trained to recite the passage first and then provide an answer. The bold numbers are the best-performing setup."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.07805v2_figure_1.png",
            "caption": "Figure 1: A illustration of our investigation of memory access pattern in language models. We find that the model accesses its parametric memory largely in a sequential manner, and faces difficulty in randomly accessing the content in the middle of memorized strings."
        },
        "2": {
            "figure_path": "2403.07805v2_figure_2.png",
            "caption": "Figure 2: EM and BLEU for reading validation passages, varying the number of training passages.\nWe calculate EM using only the first 25 tokens, as the model often generates beyond the\nmax passage length (25)."
        },
        "3": {
            "figure_path": "2403.07805v2_figure_3.png",
            "caption": "Figure 3: A stacked bar plot showing the accuracy of ID-guided sentence recitation with different marker numbers. The performance decreases significantly as the sentence index grows."
        },
        "4": {
            "figure_path": "2403.07805v2_figure_4.png",
            "caption": "Figure 4: A illustration of the recitation method. The model first recites the corresponding passage content and subsequently extracts the answer in the context, in contrast to directly answering the question."
        }
    },
    "references": [
        {
            "1": {
                "title": "A review on language models as knowledge bases.",
                "author": "Badr AlKhamissi, Millicent Li, Asli Celikyilmaz, Mona T. Diab, and Marjan Ghazvininejad. 2022.",
                "venue": "ArXiv, abs/2204.06031.",
                "url": "https://api.semanticscholar.org/CorpusID:248157206"
            }
        },
        {
            "2": {
                "title": "Physics of language models: Part 3.2, knowledge manipulation.",
                "author": "Zeyuan Allen-Zhu and Yuanzhi Li. 2023.",
                "venue": "ArXiv, abs/2309.14402.",
                "url": "https://api.semanticscholar.org/CorpusID:262898066"
            }
        },
        {
            "3": {
                "title": "RAG vs fine-tuning: Pipelines, tradeoffs, and a case study on agriculture.",
                "author": "Angels Balaguer, Vinamra Benara, Renato Luiz de Freitas Cunha, Roberto de M. Estev\u00e3o Filho, Todd Hendry, Daniel Holstein, Jennifer Marsman, Nick Mecklenburg, Sara Malvar, Leonardo O. Nunes, Rafael Padilha, Morris Sharp, Bruno Silva, Swati Sharma, Vijay Aski, and Ranveer Chandra. 2024.",
                "venue": null,
                "url": "http://arxiv.org/abs/2401.08406"
            }
        },
        {
            "4": {
                "title": "The reversal curse: LLMs trained on \"A is B\" fail to learn \"B is A\".",
                "author": "Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2309.12288"
            }
        },
        {
            "5": {
                "title": "Autoregressive search engines: Generating substrings as document identifiers.",
                "author": "Michele Bevilacqua, Giuseppe Ottaviano, Patrick Lewis, Scott Yih, Sebastian Riedel, and Fabio Petroni. 2022.",
                "venue": "In Advances in Neural Information Processing Systems, volume 35, pages 31668\u201331683. Curran Associates, Inc.",
                "url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/cd88d62a2063fdaf7ce6f9068fb15dcd-Paper-Conference.pdf"
            }
        },
        {
            "6": {
                "title": "Inducing relational knowledge from bert.",
                "author": "Zied Bouraoui, Jos\u00e9 Camacho-Collados, and Steven Schockaert. 2019.",
                "venue": "In AAAI Conference on Artificial Intelligence.",
                "url": "https://api.semanticscholar.org/CorpusID:208512764"
            }
        },
        {
            "7": {
                "title": "Language models are few-shot learners.",
                "author": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020.",
                "venue": "In Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc.",
                "url": "https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf"
            }
        },
        {
            "8": {
                "title": "Knowledgeable or educated guess? revisiting language models as knowledge bases.",
                "author": "Boxi Cao, Hongyu Lin, Xianpei Han, Le Sun, Lingyong Yan, Meng Liao, Tong Xue, and Jin Xu. 2021.",
                "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1860\u20131874, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.acl-long.146"
            }
        },
        {
            "9": {
                "title": "Extracting training data from large language models.",
                "author": "Nicholas Carlini, Florian Tram\u00e8r, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom B. Brown, Dawn Xiaodong Song, \u00dalfar Erlingsson, Alina Oprea, and Colin Raffel. 2020.",
                "venue": "In USENIX Security Symposium.",
                "url": "https://api.semanticscholar.org/CorpusID:229156229"
            }
        },
        {
            "10": {
                "title": "LLM as OS, agents as apps: Envisioning AIOS, agents and the AIOS-agent ecosystem.",
                "author": "Yingqiang Ge, Yujie Ren, Wenyue Hua, Shuyuan Xu, Juntao Tan, and Yongfeng Zhang. 2023.",
                "venue": "ArXiv, abs/2312.03815.",
                "url": "https://api.semanticscholar.org/CorpusID:266694338"
            }
        },
        {
            "11": {
                "title": "Language models as knowledge bases: On entity representations, storage capacity, and paraphrased queries.",
                "author": "Benjamin Heinzerling and Kentaro Inui. 2021.",
                "venue": "In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 1772\u20131791, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.eacl-main.153"
            }
        },
        {
            "12": {
                "title": "Are large pre-trained language models leaking your personal information?",
                "author": "Jie Huang, Hanyin Shao, and Kevin Chen-Chuan Chang. 2022.",
                "venue": "In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 2038\u20132047, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2022.findings-emnlp.148"
            }
        },
        {
            "13": {
                "title": "Mistral 7b.",
                "author": "Albert Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u2019elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. 2023.",
                "venue": "ArXiv, abs/2310.06825.",
                "url": "https://api.semanticscholar.org/CorpusID:263830494"
            }
        },
        {
            "14": {
                "title": "How can we know when language models know? on the calibration of language models for question answering.",
                "author": "Zhengbao Jiang, Jun Araki, Haibo Ding, and Graham Neubig. 2021.",
                "venue": "Transactions of the Association for Computational Linguistics, 9:962\u2013977.",
                "url": "https://doi.org/10.1162/tacl_a_00407"
            }
        },
        {
            "15": {
                "title": "Evaluating open-domain question answering in the era of large language models.",
                "author": "Ehsan Kamalloo, Nouha Dziri, Charles Clarke, and Davood Rafiei. 2023.",
                "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5591\u20135606, Toronto, Canada. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.acl-long.307"
            }
        },
        {
            "16": {
                "title": "Dense passage retrieval for open-domain question answering.",
                "author": "Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020.",
                "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769\u20136781, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2020.emnlp-main.550"
            }
        },
        {
            "17": {
                "title": "Natural questions: A benchmark for question answering research.",
                "author": "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.",
                "venue": "Transactions of the Association for Computational Linguistics, 7:452\u2013466.",
                "url": "https://doi.org/10.1162/tacl_a_00276"
            }
        },
        {
            "18": {
                "title": "Nonparametric decoding for generative retrieval.",
                "author": "Hyunji Lee, JaeYoung Kim, Hoyeon Chang, Hanseok Oh, Sohee Yang, Vladimir Karpukhin, Yi Lu, and Minjoon Seo. 2023a.",
                "venue": "In Findings of the Association for Computational Linguistics: ACL 2023, pages 12642\u201312661, Toronto, Canada. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.findings-acl.801"
            }
        },
        {
            "19": {
                "title": "GLEN: Generative retrieval via lexical index learning.",
                "author": "Sunkyung Lee, Minjin Choi, and Jongwuk Lee. 2023b.",
                "venue": "In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7693\u20137704, Singapore. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.emnlp-main.477"
            }
        },
        {
            "20": {
                "title": "Retrieval-augmented generation for knowledge-intensive nlp tasks.",
                "author": "Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, and Douwe Kiela. 2020.",
                "venue": "In Advances in Neural Information Processing Systems, volume 33, pages 9459\u20139474. Curran Associates, Inc.",
                "url": "https://proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf"
            }
        },
        {
            "21": {
                "title": "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing.",
                "author": "Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023.",
                "venue": "ACM Comput. Surv., 55(9).",
                "url": "https://doi.org/10.1145/3560815"
            }
        },
        {
            "22": {
                "title": "The devil is in the neurons: Interpreting and mitigating social biases in language models.",
                "author": "Yan Liu, Yu Liu, Xiaokang Chen, Pin-Yu Chen, Daoguang Zan, Min-Yen Kan, and Tsung-Yi Ho. 2024.",
                "venue": "In The Twelfth International Conference on Learning Representations.",
                "url": "https://openreview.net/forum?id=SQGUDc9tC8"
            }
        },
        {
            "23": {
                "title": "When not to trust language models: Investigating effectiveness of parametric and non-parametric memories.",
                "author": "Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023.",
                "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9802\u20139822, Toronto, Canada. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.acl-long.546"
            }
        },
        {
            "24": {
                "title": "Adapting pre-trained generative models for extractive question answering.",
                "author": "Prabir Mallick, Tapas Nayak, and Indrajit Bhattacharya. 2023.",
                "venue": "ArXiv, abs/2311.02961.",
                "url": "https://api.semanticscholar.org/CorpusID:265033906"
            }
        },
        {
            "25": {
                "title": "Locating and editing factual associations in gpt.",
                "author": "Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022.",
                "venue": "In Advances in Neural Information Processing Systems, volume 35, pages 17359\u201317372. Curran Associates, Inc.",
                "url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/6f1d43d5a82a37e89b0665b33bf3a182-Paper-Conference.pdf"
            }
        },
        {
            "26": {
                "title": "Rethinking search: making domain experts out of dilettantes.",
                "author": "Donald Metzler, Yi Tay, Dara Bahri, and Marc Najork. 2021.",
                "venue": "SIGIR Forum, 55(1).",
                "url": "https://doi.org/10.1145/3476415.3476428"
            }
        },
        {
            "27": {
                "title": "Training language models to follow instructions with human feedback.",
                "author": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022.",
                "venue": "Advances in Neural Information Processing Systems, 35:27730\u201327744.",
                "url": null
            }
        },
        {
            "28": {
                "title": "Fine-tuning or retrieval? comparing knowledge injection in LLMs.",
                "author": "Oded Ovadia, Menachem Brief, Moshik Mishaeli, and Oren Elisha. 2024.",
                "venue": null,
                "url": "http://arxiv.org/abs/2312.05934"
            }
        },
        {
            "29": {
                "title": "MemGPT: Towards LLMs as operating systems.",
                "author": "Charles Packer, Vivian Fang, Shishir G. Patil, Kevin Lin, Sarah Wooders, and Joseph E. Gonzalez. 2023.",
                "venue": "ArXiv, abs/2310.08560.",
                "url": "https://api.semanticscholar.org/CorpusID:263909014"
            }
        },
        {
            "30": {
                "title": "Bleu: a method for automatic evaluation of machine translation.",
                "author": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.",
                "venue": "In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.",
                "url": "https://doi.org/10.3115/1073083.1073135"
            }
        },
        {
            "31": {
                "title": "KILT: a benchmark for knowledge intensive language tasks.",
                "author": "Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rockt\u00e4schel, and Sebastian Riedel. 2021.",
                "venue": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2523\u20132544, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.naacl-main.200"
            }
        },
        {
            "32": {
                "title": "Language models as knowledge bases?",
                "author": "Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019.",
                "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463\u20132473, Hong Kong, China. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/D19-1250"
            }
        },
        {
            "33": {
                "title": "How does generative retrieval scale to millions of passages?",
                "author": "Ronak Pradeep, Kai Hui, Jai Gupta, Adam Lelkes, Honglei Zhuang, Jimmy Lin, Donald Metzler, and Vinh Tran. 2023.",
                "venue": "In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1305\u20131321, Singapore. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.emnlp-main.83"
            }
        },
        {
            "34": {
                "title": "Language models are unsupervised multitask learners.",
                "author": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019.",
                "venue": "OpenAI blog, 1(8):9.",
                "url": null
            }
        },
        {
            "35": {
                "title": "SQuAD: 100,000+ questions for machine comprehension of text.",
                "author": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016.",
                "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383\u20132392, Austin, Texas. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/D16-1264"
            }
        },
        {
            "36": {
                "title": "TOME: A two-stage approach for model-based retrieval.",
                "author": "Ruiyang Ren, Wayne Xin Zhao, Jing Liu, Hua Wu, Ji-Rong Wen, and Haifeng Wang. 2023.",
                "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6102\u20136114, Toronto, Canada. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.acl-long.336"
            }
        },
        {
            "37": {
                "title": "Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation.",
                "author": "Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. 2022.",
                "venue": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 22500\u201322510.",
                "url": "https://api.semanticscholar.org/CorpusID:251800180"
            }
        },
        {
            "38": {
                "title": "Replug: Retrieval-augmented black-box language models.",
                "author": "Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen tau Yih. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2301.12652"
            }
        },
        {
            "39": {
                "title": "Understanding the mechanics and dynamics of memorisation in large language models: A case study with random strings.",
                "author": "Till Speicher, Aflah Mohammad Khan, Qinyuan Wu, Vedant Nanda, Soumi Das, Bishwamittra Ghosh, Krishna P. Gummadi, and Evimaria Terzi. 2024.",
                "venue": null,
                "url": "https://openreview.net/forum?id=ILStlRb1Sp"
            }
        },
        {
            "40": {
                "title": "Memorization for good: Encryption with autoregressive language models.",
                "author": "Samuel Stevens and Yung-Chun Su. 2023.",
                "venue": "ArXiv, abs/2305.10445.",
                "url": "https://api.semanticscholar.org/CorpusID:258762168"
            }
        },
        {
            "41": {
                "title": "Recitation-augmented language models.",
                "author": "Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. 2023.",
                "venue": "In International Conference on Learning Representations.",
                "url": "https://openreview.net/forum?id=-cqvvvb-NkI"
            }
        },
        {
            "42": {
                "title": "Transformer memory as a differentiable search index.",
                "author": "Yi Tay, Vinh Q. Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, Tal Schuster, William W. Cohen, and Donald Metzler. 2022.",
                "venue": null,
                "url": "http://arxiv.org/abs/2202.06991"
            }
        },
        {
            "43": {
                "title": "Memorization without overfitting: Analyzing the training dynamics of large language models.",
                "author": "Kushal Tirumala, Aram H. Markosyan, Luke Zettlemoyer, and Armen Aghajanyan. 2022.",
                "venue": "ArXiv, abs/2205.10770.",
                "url": "https://api.semanticscholar.org/CorpusID:248986465"
            }
        },
        {
            "44": {
                "title": "Llama: Open and efficient foundation language models.",
                "author": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a.",
                "venue": null,
                "url": "http://arxiv.org/abs/2302.13971"
            }
        },
        {
            "45": {
                "title": "Llama 2: Open foundation and fine-tuned chat models.",
                "author": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023b.",
                "venue": null,
                "url": "http://arxiv.org/abs/2307.09288"
            }
        },
        {
            "46": {
                "title": "Attention is all you need.",
                "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017.",
                "venue": "Advances in neural information processing systems, 30.",
                "url": null
            }
        },
        {
            "47": {
                "title": "Can generative pre-trained language models serve as knowledge bases for closed-book QA?",
                "author": "Cunxiang Wang, Pai Liu, and Yue Zhang. 2021.",
                "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3241\u20133251, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.acl-long.251"
            }
        },
        {
            "48": {
                "title": "A neural corpus indexer for document retrieval.",
                "author": "Yujing Wang, Yingyan Hou, Haonan Wang, Ziming Miao, Shibin Wu, Hao Sun, Qi Chen, Yuqing Xia, Chengmin Chi, Guoshuai Zhao, Zheng Liu, Xing Xie, Hao Allen Sun, Weiwei Deng, Qi Zhang, and Mao Yang. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2206.02743"
            }
        },
        {
            "49": {
                "title": "Chain-of-thought prompting elicits reasoning in large language models.",
                "author": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2201.11903"
            }
        },
        {
            "50": {
                "title": "HotpotQA: A dataset for diverse, explainable multi-hop question answering.",
                "author": "Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018.",
                "venue": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369\u20132380, Brussels, Belgium. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/D18-1259"
            }
        },
        {
            "51": {
                "title": "Give me the facts! a survey on factual knowledge probing in pre-trained language models.",
                "author": "Paul Youssef, Osman Kora\u015f, Meijie Li, J\u00f6rg Schl\u00f6tterer, and Christin Seifert. 2023.",
                "venue": "In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 15588\u201315605, Singapore. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.findings-emnlp.1043"
            }
        },
        {
            "52": {
                "title": "Scalable and effective generative information retrieval.",
                "author": "Hansi Zeng, Chen Luo, Bowen Jin, Sheikh Muhammad Sarwar, Tianxin Wei, and Hamed Zamani. 2023.",
                "venue": "ArXiv, abs/2311.09134.",
                "url": "https://api.semanticscholar.org/CorpusID:265213270"
            }
        },
        {
            "53": {
                "title": "Large language models as commonsense knowledge for large-scale task planning.",
                "author": "Zirui Zhao, Wee Sun Lee, and David Hsu. 2023.",
                "venue": "In RSS 2023 Workshop on Learning for Task and Motion Planning.",
                "url": null
            }
        },
        {
            "54": {
                "title": "LIMA: Less is more for alignment.",
                "author": "Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2305.11206"
            }
        },
        {
            "55": {
                "title": "Ultron: An ultimate retriever on corpus with a model-based indexer.",
                "author": "Yujia Zhou, Jing Yao, Zhicheng Dou, Ledell Yu Wu, Peitian Zhang, and Ji rong Wen. 2022.",
                "venue": "ArXiv, abs/2208.09257.",
                "url": "https://api.semanticscholar.org/CorpusID:251710261"
            }
        },
        {
            "56": {
                "title": "Physics of language models: Part 3.1, knowledge storage and extraction.",
                "author": "Zeyuan Allen Zhu and Yuanzhi Li. 2023.",
                "venue": "ArXiv, abs/2309.14316.",
                "url": "https://api.semanticscholar.org/CorpusID:262825178"
            }
        },
        {
            "57": {
                "title": "Bridging the gap between indexing and retrieval for differentiable search index with query generation.",
                "author": "Shengyao Zhuang, Houxing Ren, Linjun Shou, Jian Pei, Ming Gong, Guido Zuccon, and Daxin Jiang. 2022.",
                "venue": "arXiv preprint arXiv:2206.10128.",
                "url": null
            }
        },
        {
            "58": {
                "title": "Large language models are built-in autoregressive search engines.",
                "author": "Noah Ziems, Wenhao Yu, Zhihan Zhang, and Meng Jiang. 2023.",
                "venue": "In Findings of the Association for Computational Linguistics: ACL 2023, pages 2666\u20132678, Toronto, Canada. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.findings-acl.167"
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.07805v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "4",
            "4.1"
        ],
        "main_experiment_and_results_sections": [
            "3.2",
            "3.3",
            "3.4",
            "5",
            "5.1",
            "5.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.4",
            "4",
            "4.1",
            "5",
            "5.1",
            "5.2"
        ]
    },
    "research_context": {
        "paper_id": "2403.07805v2",
        "paper_title": "Beyond Memorization: The Challenge of Random Memory Access in Language Models",
        "research_background": "### Motivation:\nThe paper is motivated by the recent advancements in the abilities of language models (LMs) to handle knowledge-intensive tasks by storing vast amounts of factual knowledge in their parameters. Despite this progress, there remains an open question about how these models access their stored knowledge. Specifically, the study draws inspiration from memory-accessing patterns observed in computer systems to explore whether LMs access their parametric memory in a sequential or random manner. Addressing this question would enhance the understanding of the fundamental mechanisms underlying knowledge retrieval in LMs.\n\n### Research Problem:\nThe core research problem the paper addresses is the challenge language models face with random memory access. While LMs can sequentially access memorized content effectively, they show a marked decline in performance when required to perform random memory access. This limitation poses significant challenges for tasks that involve retrieving specific pieces of information from a broader context without sequentially reading through the entire content.\n\n### Related Work:\n1. **Language Models as Knowledge Bases**:\n   - There has been a growing interest in regarding LMs as repositories of knowledge due to their high performance in knowledge-intensive tasks, such as open-domain question answering (Petroni et al., 2019; Wang et al., 2021; Heinzerling and Inui, 2021; Cao et al., 2021; AlKhamissi et al., 2022).\n\n2. **Fundamentals of Memory Storage in LMs**:\n   - Recent research has focused on the underlying mechanisms of how LMs store and access knowledge (Tirumala et al., 2022; Zhu and Li, 2023; Allen-Zhu and Li, 2023; Berglund et al., 2023). This ongoing exploration aims to decode the efficiency and patterns of memory retrieval embedded in the parameters of these models.\n\n3. **LMs Performance and Challenges in Downstream Tasks**:\n   - Understanding how LMs access their memory is crucial as it influences their performance on various downstream tasks (Petroni et al., 2021; Kamalloo et al., 2023; Ziems et al., 2023; Mallen et al., 2023).\n\nBy drawing from previous works and introducing experiments to test memory access patterns in LMs, the paper seeks to bridge the gap in understanding random memory access challenges and pose potential solutions to improve LM performance in practical applications.",
        "methodology": "The paper examines the ability of a language model to access its parametrically stored memory, focusing on both sequential and random memory access. Here\u2019s a detailed breakdown of the proposed methodology:\n\n1. **Memory Access Definitions**:\n   - **Sequential Memory Access**: This involves the model starting from the beginning of a memorized passage and progressively generating the subsequent content.\n   - **Random Memory Access**: This involves the model initiating recall from any chosen location within a memorized passage and accurately regenerating the subsequent content from that point.\n\n2. **Experiments and Evaluations**:\n   - **Sequential Memory Access**:\n     1. The model is asked to recite full passages word by word to evaluate its sequential access ability.\n   - **Random Memory Access**:\n     1. The model is asked to recite selected sentences from memorized passages.\n     2. A more challenging task of question answering is conducted to further assess random access proficiency.\n\n3. **Variations in Experiments**:\n   - **Passage Content Types**:\n     - Natural Language (NL): Wikipedia paragraphs from the SQuAD dataset.\n     - Random Strings (Rand): Natural language passages substituted with space-separated alphanumeric strings of the same token length.\n   - **Passage ID Types**:\n     - Numerical Strings (Num): E.g., '#123'.\n     - Rare Random Tokens (Rare): Infrequent tokens as identifiers.\n     - Article Titles (Title): Titles of the related Wikipedia pages.\n\n4. **Model and Training Details**:\n   - **Base Model**: GPT2-large with 774M parameters.\n   - **Pretrained Checkpoint**: Used instead of training from scratch for enhanced string memorization.\n   - **Fine-Tuning**: Conducted for 100 epochs with a specified learning rate.\n   - **Evaluation Metrics**: BLEU and Exact Match (EM) scores to measure memorization and recall accuracy.\n\n5. **Results**:\n   - **Sequential Access**: The model shows high BLEU and EM scores on validation passages, demonstrating good sequential access ability.\n   - **Adaptability**: Better performance with titles or numbers as IDs for natural language passages, while rare tokens pose a challenge.\n   - **Random Strings**: The model can also sequentially access passages composed of random characters.\n   - **Bottleneck**: Performance drops when the passage count exceeds 100k due to training difficulties.\n\n6. **Experimental Detail**:\n   - **Selective Recitation**: Tests involved reciting the specific '-th' sentence of a passage based on a provided ID.\n     - Poor performance with selective recitation, especially for non-initial sentences, indicating weakness in random access.\n   - **Grounded QA Task**: Performance evaluated in different setups (closed-book, open-book, and variants involving passage IDs).\n\n7. **Findings**:\n   - **Open-book QA**: Best performance due to direct access to golden passages.\n   - **Closed-book QA**: Worst performance, reliant solely on the model's pre-existing knowledge.\n   - **Passage ID Impact**: Minimal impact on performance, except when the ID type is 'Title', which can aid in accessing related content.\n   - **General Insight**: Models can effectively serve as a memory bank with robust sequential access but significant limitations in random memory access.\n\n### Key Innovations:\n- The study meticulously compares the model\u2019s efficacy in sequential vs. random recall.\n- It introduces a dual evaluation mechanism, combining selective sentence recitation and grounded question-answering.\n- The methodology considers multiple variations in passage content and identifier types to comprehensively test the model's memory capabilities.\n- It highlights the practical constraints and challenges in the parametric memory capacity of large language models.\n\nOverall, the proposed methodology provides a thorough examination of sequential and random memory access capabilities in language models, unveiling critical insights into their performance and limitations.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n**Objective**: To evaluate the language model's ability to reproduce entire memorized passages given a prompt with an identifier.\n\n**Datasets**: The experiments utilize two types of passage content:\n1. **Natural Language (NL)**: Wikipedia paragraphs from SQuAD.\n2. **Random Strings (Rand)**: Space-separated alphanumeric strings corresponding to the original NL passage's token count.\n\n**Passage IDs**: The models are prompted with three different forms of identifiers:\n1. **Numerical Strings (Num)**: e.g., \u2018#123\u2019.\n2. **Rare Random Tokens (Rare)**: Three infrequent tokens, randomly sampled.\n3. **Article Titles (Title)**: Titles of the Wikipedia pages.\n\n**Model**: \n- **GPT2-large** with 774M parameters, using a pretrained checkpoint from Hugging Face, fine-tuned for 100 epochs.\n\n**Evaluation Metrics**:\n- **BLEU Score**: To measure the similarity between the generated content and the ground-truth passage.\n- **Exact Match (EM) Score**: To determine if the generated content matches the ground-truth passage exactly.\n\n### Main Experimental Results\n\nThe results indicate that the GPT2-large model has a robust capability for sequentially accessing memorized content:\n\n1. **Performance**: High BLEU and EM scores on validation passages suggest that the model can effectively recall and generate the entire passage content.\n2. **Adaptability**: The model performs better when using titles or numbers as prompts for natural language passages compared to rare tokens. This could be because associating rare tokens with natural language content is more challenging for the model.\n3. **Random Strings**: The model's memory access ability extends to passages composed of random alphanumeric strings, demonstrating versatility beyond natural language content.\n\nThe summarization of these results highlights the model's adaptability and effectiveness in sequential access across different types of identifiers and passage contents."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To investigate whether language models (LMs) can sequentially or randomly access their memory stored in the parameters and understand the challenges involved.",
            "experiment_process": "The study adopts GPT2-large model with 774M parameters, fine-tunes it for 100 epochs using a learning rate of . The tasks include full recitation of passages, selective recitation of sentences, and grounded question answering using SQuAD dataset. Types of passage IDs and contents (Natural Language, random strings) are varied. Sequential memory access is tested by reciting full passages; random memory access by selective recitation and grounded QA. Performance is measured using BLEU and Exact Match (EM) scores.",
            "result_discussion": "The model effectively performs sequential access, achieving high BLEU and EM scores. However, it struggles with random access, showing a sharp decline in performance when asked to access specific segments or sentences. The model shows poor performance in selective recitation and grounded QA tasks without including the passage content in the context, indicating significant limitations in random memory access.",
            "ablation_id": "2403.07805v2.No1"
        },
        {
            "research_objective": "To enhance the random memory access capability of LMs by integrating recitation and permutation techniques.",
            "experiment_process": "For selective sentence recitation, the model first recites the entire passage and then the specific sentence. For grounded QA, the model recites the passage associated with the input ID. Additionally, permutation of sentences is performed during the writing stage in two methods: moving each sentence to the beginning or random shuffling.",
            "result_discussion": "Recitation improves performance in selective recitation and grounded QA tasks by enabling the model to load passage content into the context, allowing for easier random access. Permutation also enhances random access by making more sentences accessible via the ID. These techniques do not change the inherent sequential access pattern but improve memory utilization.",
            "ablation_id": "2403.07805v2.No2"
        },
        {
            "research_objective": "To test the model's ability to perform random access in open-domain question answering scenarios without passage IDs.",
            "experiment_process": "Using Natural Questions (NQ) and HotpotQA datasets, the study limits passages to those containing answers. Training involves closed-book QA, mixed training (passages and QA pairs), and continual training (passages followed by QA pairs). GPT2-XL model with 1.5B parameters is used, trained for 20 epochs with a 3e-5 learning rate.",
            "result_discussion": "Writing golden passages into the model's memory improves performance over closed-book QA. Recitation significantly enhances the model's ability to access memorized passages, improving EM scores in both single and multi-hop QA. Mixed training outperforms continual training, likely due to constant refreshing of passage content in memory.",
            "ablation_id": "2403.07805v2.No3"
        }
    ]
}