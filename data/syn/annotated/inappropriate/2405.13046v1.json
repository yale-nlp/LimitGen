{
    "title": "LeaPformer: Enabling Linear Transformers for Autoregressive and Simultaneous Tasks via Learned Proportions",
    "abstract": "A promising approach to preserving model performance in linearized transformers is to employ position-based re-weighting functions. However, state-of-the-art re-weighting functions rely heavily on target sequence lengths, making it difficult or impossible to apply them to autoregressive and simultaneous tasks, where the target and sometimes even the input sequence length are unknown. To address this issue, we propose Learned Proportions (LeaP) and LeaPformers222https://github.com/OSU-STARLAB/LeaPformer. Our contribution is built on two major components. First, we generalize the dependence on explicit positional representations and sequence lengths into dependence on sequence proportions for re-weighting. Second, we replace static positional representations with dynamic proportions derived via a compact module, enabling more flexible attention concentration patterns. We evaluate LeaPformer against eight representative efficient transformers on the Long-Range Arena benchmark, showing that LeaPformer achieves the best quality-throughput trade-off, as well as LeaPformer to Wikitext-103 autoregressive language modeling and simultaneous speech-to-text translation for two language pairs, achieving competitive results.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Transformers (Vaswani et al., 2017  ###reference_b35###) are dominant in the natural language processing (NLP) solution space, demonstrating state-of-the-art performance for a range of applications. With the advent of widely accessible large language models (LLMs), transformers as a class of models are being studied more closely than ever. Unfortunately, the quadratic complexity of the attention mechanisms of typical transformers limits the lengths of the sequences that they can process, rendering them sub-optimal or even impossible to apply for tasks with long sequences.\n###figure_1### Naturally, an active area of possible improvement for classical transformers are efficient attention mechanisms that reduce the sometimes prohibitive quadratic run-time and memory complexity of softmax attention with respect to sequence lengths. Many efficient transformer variants have been proposed, including both sub-quadratic attention mechanisms, usually with key assumptions or experimental bounds surrounding their construction, and truly linear attention mechanisms with no prior environmental assumptions (Katharopoulos et al., 2020  ###reference_b14###; Choromanski et al., 2020  ###reference_b8###; Peng et al., 2021  ###reference_b25###; Chen et al., 2021  ###reference_b6###; Qin et al., 2022b  ###reference_b29###). While the aforementioned linear transformers are often effective for specific tasks, they tend to exhibit varying degrees of performance degradation when generalized.\nTo address this issue, re-weighting functions have been recently formalized (Su et al., 2022  ###reference_b31###; Qin et al., 2022b  ###reference_b29###) in linear transformers and serve to concentrate attention scores. Although promising, state-of-the-art position-based re-weighting functions rely on explicit token positions and sequence lengths (Qin et al., 2022b  ###reference_b29###). This reliance on knowing the sequence length beforehand make it difficult to apply those re-weighting functions and linear transformers to autoregressive tasks without specialized solutions (Agostinelli & Chen, 2023  ###reference_b1###) and renders it impossible to apply them to simultaneous tasks. Furthermore, existing re-weighting functions\u2019 reliance on explicit positional representations usually produce static attention concentration patterns, which can severely limit their generalizability when an attention concentration pattern is ill-suited to a given task.\nTo solve this reliance on explicit positional representations and enable linear transformers for a wider range of tasks, we propose a novel approach that we refer to as Learned Proportions (LeaP) and call models we apply it to LeaPformers. This contribution is composed of two major aspects: generalization to proportions and learned behavior. First, we generalize the dependence on explicit positional representations and sequence lengths into an intuitive dependence on proportions of a sequence for re-weighting, removing theoretical dependence on sequence lengths. Second, instead of employing static positional representations, we construct and deploy a compact module that dynamically derives sequence proportions for a given token during training and inference. These straightforward, but critical, contributions ultimately remove any reliance that current position-based re-weighting functions may have on sequence length, enabling them for tasks where the sequence length is not known beforehand (and cannot be estimated) and/or where attention concentration patterns are more complex.\nTo validate our proposed approach, we primarily test LeaPformer against cosFormer, the state-of-the-art position-based linear transformer, by adapting cosFormer\u2019s cosine-based re-weighting function via LeaP. We also evaluate and compare with eight other representative attention mechanisms on the Long-Range Arena (LRA) benchmark (Tay et al., 2021  ###reference_b33###), a competitive benchmark for efficient attention mechanisms on long sequences. In addition, we validate LeaPformers on autoregressive language modeling on Wikitext-103b (Merity et al., 2016  ###reference_b22###) and on multiple language pairs for simultaneous speech-to-text translation (SimulST) (Ma et al., 2020c  ###reference_b21###). When compared to popular, previously proposed efficient attention mechanisms on the LRA benchmark, the proposed LeaPformer achieves the best accuracy-throughput trade-off, balanced performance across tasks, small memory footprint, and notably beats cosFormer\u2019s inference quality (see Figure 1  ###reference_###). During autoregressive language modeling, LeaPformer achieves the lowest perplexity out of a limited set of efficient attention mechanisms, beating out the next closest mechanism by 0.13 perplexity on the test set. Finally, when applied to simultaneous translation, LeaPformer demonstrates competitive results with a reasonable accuracy-throughput trade-off compared to classical softmax attention for critical ablations, with variations achieving quality loss of only 0.26 BLEU-4 (Post, 2018  ###reference_b26###) for English to German and 0.23 BLEU-4 for French to English while being completely linear in complexity. To our knowledge, this is the first time that an explicit position-based re-weighting function for linear transformers is successfully applied to simultaneous tasks."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background and Motivation",
            "text": "Here we provide an overview of the background knowledge required to understand our work and motivate LeaPformers."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Softmax Attention Mechanisms",
            "text": "Multi-headed self-attention in transformers (Vaswani et al., 2017  ###reference_b35###) can generally be described as follows:\nwhere query is , key , and value , with  being the input sequence for each attention head that divides the model embedding space  into some  (denoted as  hereafter for simplicity) and ,  and . In cases where the concatenation of the attention head outputs differs in dimensionality from , an optional output projection layer is commonly applied via .\nFor long sequences, the quadratic complexity of the mechanism in Equation 1  ###reference_### can prove to be a throughput bottleneck during training and inference."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Efficient and Linear Transformers",
            "text": "Efficient and linear transformers have emerged over the past few years as an active area of research for particularly resource or latency-constrained environments, exhibiting notable inference speedups and smaller memory footprints. These transformer variants focus on alternative attention mechanisms that reduce the quadratic complexity of typical softmax attention. A plethora of efficient transformer options exist that can be classified into a few groups: sliding-window or localized attention mechanisms (Parmar et al., 2018  ###reference_b24###; Dai et al., 2019  ###reference_b9###; Wu et al., 2020  ###reference_b40###; Beltagy et al., 2020  ###reference_b4###), pattern or sparsity-based attention mechanisms (Child et al., 2019  ###reference_b7###; Zaheer et al., 2020  ###reference_b42###), kernel-based and truly linear attention mechanisms with no priors (Katharopoulos et al., 2020  ###reference_b14###; Choromanski et al., 2020  ###reference_b8###; Peng et al., 2021  ###reference_b25###; Chen et al., 2021  ###reference_b6###; Qin et al., 2022b  ###reference_b29###), and some unique outliers (Wang et al., 2020b  ###reference_b38###; Kitaev et al., 2020  ###reference_b15###).\nWhile many approaches linearize the computations, truly-linear transformers, such as the kernel-based substitutions for the softmax mechanism, do not make any prior assumptions of the environments (e.g., no assumed sparsity or local dependencies).\nThis can be described\nvia row-wise outputs (represented by ) for each attention head in Equations 3  ###reference_###, 4  ###reference_###, and 5  ###reference_###, with  corresponding to any similarity function that transforms the product of the query and key matrices. If  becomes , Equation 3  ###reference_### is an accurate representation of softmax attention.\nIf we decompose  into  and ,\nas shown in Equation 4  ###reference_###, computation can be reordered such that the attention complexity reduces from  in Equation 3  ###reference_### to  in Equation 5  ###reference_###.  corresponds to the sequence length of the query matrix and  corresponds to those of the key and value matrices (a generalization for encoder-decoder cross-attention). When  or  are significantly larger than , this rearrangement of the attention calculation leads to linear complexity with respect to the sequence length."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Position-Based Re-weighting Functions",
            "text": "While reducing the computational complexity, linearizing multi-headed self-attention\nleads to varying degrees of degraded model performance. To address this shortcoming, re-weighting functions have been recently proposed.\nThey introduce an additional function to augment , with the goal of concentrating/adjusting the probability distribution of the normalized  (Qin et al., 2022b  ###reference_b29###). Re-weighting functions  are commonly based on token positions, and we multiply as shown in Equation 6  ###reference_###:\nNote that even though  is placed at the end of the equation and multiplied, that particular placement and operation can be arbitrary. For example, placing  in between or before the transformed query and key matrices would also be valid as a re-weighting function application.  can also map to any number of possible concentration methods, such as a matrix modifying  by multiplication or element-wise operations (e.g. addition).\nElaborate position-based encoding schemes (Raffel et al., 2020  ###reference_b30###; Wang et al., 2019  ###reference_b36###; Wang & Chen, 2020  ###reference_b39###; Liutkus et al., 2021  ###reference_b17###; Press et al., 2022  ###reference_b27###), using absolute or relative token positions, have advanced the scheme utilized by the initial work (Vaswani et al., 2017  ###reference_b35###)\nand many provide what can be intuited as position-based re-weighting functions. However, those schemes are specifically designed for a  formulation and do not work for the decomposed  linearized formulation.\nRotary Positional Embeddings (RoPE) (Su et al., 2022  ###reference_b31###), with some minor modifications, is closest to being a true position-based re-weighting function for linear transformers by using relative token positions333Some re-weighting functions, like Alibi (Press et al., 2022  ###reference_b27###), are not covered in detail because we consider them superseded by other options or they are not obviously usable for linear attention because they are not decomposable like RoPE.. However, RoPE is unaware of the total sequence length when it is applied, and this can cause potential\nproblems. For example, RoPE would treat two tokens that are 100 tokens apart in a 1k length sequence and a 200 length sequence the same, where the actual relationship of the two tokens could vary drastically between the two sequences. This lack of sequence length awareness renders RoPE\u2019s re-weighting ability inherently limited, especially for sequences that exhibit more than the aforementioned locality characteristic. We elaborate on RoPE\u2019s construction (and its linear attention variant tested in this paper) in Appendix A.6  ###reference_###.\nRecently introduced, cosFormer (Qin et al., 2022b  ###reference_b29###) is the state-of-the-art in position-based linear transformers that utilizes sequence length in addition to absolute token positions. cosFormer\u2019s proposed mechanism, with common-sense modifications (Agostinelli & Chen, 2023  ###reference_b1###), is described by Equation 7  ###reference_###. Here,  and  are set to  and their cosine-based re-weighting function is distributable via Ptolemy\u2019s method for expanding trigonometric expressions. Intuitively, when the positions  and  of two tokens are closer, the cosine\u2019s response is increased, emphasizing locality. Conversely, when the two positions are far apart, the response approaches zero, representing maximum attenuation via re-weighting.\nUnlike RoPE, cosFormer can recognize differences in token distances relative to the sequence length, re-weighting more dynamically in practice. Using our previous example, cosFormer would treat two tokens that are 100 positions apart differently in a 1k length sequence versus a 200 length sequence, an intuitive improvement."
        },
        {
            "section_id": "2.4",
            "parent_section_id": "2",
            "section_name": "Motivation of Our Study",
            "text": "Unfortunately, the reliance on sequence length makes it difficult to apply certain re-weighting functions towards autoregressive and simultaneous tasks. For instance, it can be challenging to apply position-based re-weighting functions to autoregressive tasks (e.g. text-to-speech translation) where target sequence lengths are usually not known beforehand. Although some effort has been made to address these issues (Liu et al., 2022  ###reference_b16###; Agostinelli & Chen, 2023  ###reference_b1###), mostly via target sequence length prediction based on the full input sequence, proposed solutions are prone to some level of approximation error. Furthermore, none of the prior approaches has discussed the impossibility of applying them to simultaneous tasks, where even the full input sequence is not available at decoding time-steps.\nMoreover, the static nature of the state-of-the-art re-weighting functions can cause issues from an inference quality standpoint. cosFormer\u2019s re-weighting function focuses on encouraging locality, but this can be problematic when locality bias is not important to a given application. RoPE and similar schemes suffer from the same problem. In such instances, dynamic flexibility in the re-weighting function to encourage strong, long-range connections would be preferred. An example of when this flexibility may be desirable can be found in a typical translation task, where languages like German that tend to exhibit subject-object-verb (SOV) structures as opposed to subject-verb-object (SVO) structures in languages like English may require diverse attention patterns and long-range dependencies. A verb near the end of a German sentence may attend strongly to the subject near the beginning of the sentence, but static re-weighting functions like the one employed by cosFormer would likely have trouble enabling this relationship."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "LeaPformer",
            "text": "We propose a novel re-weighting function and method for constructing such functions for linear transformers that resolves the issues in applying them to many autoregressive tasks and enables their application to simultaneous tasks.\nTo this end,\nwe first generalize the reliance on absolute token position and sequence length into a more direct, intuitive reliance on the relative placement of a token in the sequence which we refer to as a proportion. This generalization allows for easier analysis of re-weighting function behavior and removes theoretical dependence on sequence length. Second, we propose, construct, and deploy a compact module to learn proportional representations derived from each token, a technique that we call Learned Proportions (LeaP) and call the models it is applied to LeaPformers. LeaPformers can be applied to tasks where sequence lengths are unknown and, more importantly, capture dynamic attention patterns over position-based re-weighting functions."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Proportion-Based Re-weighting Functions",
            "text": "We introduce proportion-based re-weighting in Equation 8  ###reference_###, where  and  represent proportions of sequences from which queries and keys are derived from and  represents the re-weighting function with a reliance on the provided proportions. Technically,  and  can be set in any manner, but for the most straightforward proportion-based re-weighting implementations, they would correspond to the proportion of a sequence that a token is placed (e.g., at 20% of the sequence).\nUnder this definition, cosFormer\u2019s formulation in Equation 7  ###reference_### can be considered as a special case,\nwhere we replace  in Equation 8  ###reference_### with the cosine-based re-weighting function of cosFormer and define  and  as being explicit token positions divided by the sequence length, as shown in Equation 9  ###reference_###:"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Learned Proportions",
            "text": "In contrast to determining the proportions statically as in the case of cosFormer in Equation 9  ###reference_###, models can learn to derive these representative proportions via a module containing a compact network embedded within attention blocks. We call this method Learned Proportions (LeaP) and models utilizing this technique LeaPformers. The possible inference quality benefits of LeaP can be understood intuitively. Suppose that  is set in a static manner in accordance with explicit positional representations, but  is derived via a small module based on the query matrix. The module\u2019s learned behavior could produce derived elements of  equal to classical positional representations, thus replicating the behavior and performance of attention mechanisms like cosFormer, but could alternatively defer the inter-token relationships that cosFormer might otherwise emphasize (i.e. an emphasis on locality). Along these lines, we redefine the aforementioned proportions in accordance with Equation LABEL:eq:proportion_derivation, where  and  represent the proposed modules that derive proportions based on the query and key matrices, and  and  are redefined as  and .\nTo elaborate on potential inference quality benefits further, we can refer back to our example of translation to or from German and the SOV structure that cosFormer would likely struggle to model well. If  is derived from a small LeaP module in self-attention, models could effectively defer the locality bias inherent to cosFormer to elsewhere in the sequence. If correctly learned, this might allow models to defer their attention concentration from the verb at the end of the German sequence to the beginning of the sequence, where we might expect a typically strong attention score. Allowing derivations of both  and  would, naturally, afford maximum flexibility in attention patterns produced by the employed re-weighting function.\nBeyond the inference quality benefits of LeaP, our method removes any dependence that proportion-based re-weighting functions have on knowing the sequence length beforehand, widely enabling them for autoregressive tasks without target sequence length prediction and, for the first time, demonstrating the feasibility to apply them to simultaneous tasks.\n###figure_2###"
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Optimizing LeaP Module for Throughput and Analyzing Expressivity",
            "text": "It is critical that the addition of LeaP does not significantly affect the throughput of a given model or its memory footprint, as it is intended for resource-constrained and latency-sensitive environments. Given that, we recommend a module composed of a simple, two-layer feed-forward network that steps down the attention head embedding dimension with a ReLU activation between the layers and a sigmoid activation at the end of the network, along the lines of the augmentation highlighted in Figure 2  ###reference_###. The choice of a ReLU activation is based on empirical tests on the Long-Range Arena benchmark (Tay et al., 2021  ###reference_b33###) which determined that, as opposed to several other competitive options, ReLU generalized well to multiple tasks.\nWhile a separate LeaP module for each attention head would be straightforward, we found in our experiments that this made a very minor difference in terms of quality. For English to German SimulST, we observed that when replacing the decoder self-attention block with LeaPformer where a separate LeaP module was provided for each attention head the models were of similar quality (measured by validation perplexity, difference of 0.03). Given that and acknowledging that deploying multiple LeaP modules would drastically increase the parameter footprint of the module, we elect to share one LeaP module for all attention heads.\n###figure_3### Additionally, given the activation functions chosen for the LeaP module\u2019s architecture, it is important to examine the expressivity of the module. It is generally desirable that the LeaP module outputs a complex range of values as opposed to saturating to values of 0 or 1, as otherwise it is simply sparsifying the  matrix (were it to be directly calculated).\nIn Figure 3  ###reference_###, we compare the re-weighting matrices of baseline cosFormer and LeaPformer for the list-operations task in the LRA benchmark, a fairly difficult one. As can be observed in the example, and as we generally found in practice, the baseline cosFormer can only provide static re-weighting emphasizing locality (with the largest weights along diagonal for the same position). In contrast, cosFormer augmented with the LeaP modules is capable of generating complex re-weighting matrices that lightly attenuate between most positions while selectively attenuating harshly or not at all. The fact that there is wide-spread, light attenuation across several examples indicates that our LeaP module can also avoid saturation."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Empirical Evaluation",
            "text": "We validate the potential of LeaP by applying it to cosFormer on three major sets of tasks. All references to LeaPformers in the following sections refer to this augmentation of cosFormers. We first test LeaPformers on the popular Long-Range Arena (LRA) benchmark (Tay et al., 2021  ###reference_b33###), built specifically for validating the capabilities of efficient attention mechanisms. We also engage with basic autoregressive language modeling, employing Baevski & Auli 2019  ###reference_b3###\u2019s adaptive input/output architecture on Wikitext-103b (Merity et al., 2016  ###reference_b22###). Moreover, we evaluate LeaPformers on speech-to-text simultaneous translation (SimulST) via a wait-k read-write schedule (Ma et al., 2019  ###reference_b18###, 2020b  ###reference_b20###, 2020c  ###reference_b21###) across two language pairs. For our SimulST and autoregressive language modeling experiments, we employ Fairseq (Ott et al., 2019  ###reference_b23###) for training and validation alongside SimulEval (Ma et al., 2020a  ###reference_b19###) for SimulST evaluation. LRA results are compared via accuracy, autoregressive language modeling results are evaluated via validation and test set perplexity, and SimulST results are compared via detokenized BLEU-4 (called BLEU later) using sacreBLEU (Post, 2018  ###reference_b26###). Additional details related to employed hardware and hyperparameters can be found in the Appendix."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Long-Range Arena Benchmark",
            "text": "Instead of the Long-Range Arena (LRA) benchmark provided by Tay et al. 2021 ###reference_b33###, our implementation follows Skyformer\u2019s (Chen et al., 2021 ###reference_b6###) PyTorch framework and reuses their architectures and hyperparameters, which we hold static. We provide baseline results for various architectures, including the classical transformer (Vaswani et al., 2017 ###reference_b35###) and several seminal efficient transformers. Some auxiliary results and details related to the LRA benchmark are provided in Appendices A.4 ###reference_### and A.5 ###reference_### regarding controlling for increased parameter counts and alternatives to efficient transformers.\nIn addition to these results, we propose an evaluation metric that we call simple accuracy measure without consideration for throughput or memory footprint to more concretely evaluate efficient attention mechanisms. We treat softmax attention as an inference quality ceiling for the LRA benchmark as follows: with as a given efficient attention mechanism and as softmax attention and their corresponding accuracy values. The accuracy measure focuses on assessing the accuracy between two attention mechanisms. Adding by one in the computation smooths out the resulting values. While as depicted in Equation 11 ###reference_### favors equally prioritizing accuracy, one could prioritize other factors by changing the expressions (they are currently set to 1 as a default for equal prioritization).\nEquation 12 ###reference_### provides a version that considers memory footprint, called RCPmem that splits its reward between throughput increases and memory footprint reductions, where the weights for those rewards are similarly tunable:\nWe set these tunable weights to 0.5 as a default.\nRegarding the LeaPformers tested on the LRA benchmark, a minimal setup was initially employed with around a maximum of a 0.2% increase on the number of parameters for the LeaP module. We additionally test a larger module employed with a maximum increase of 1.5% to the number of parameters to investigate the effects of increased size. Some very limited fine-tuning was employed across a few possible module sizes on a per-task basis for the larger LeaPformer, depending on the perceived difficulty of the task.\nWe show\na holistic view of performance in Figure 1 ###reference_### (we refer the readers to \u00a71 ###reference_###), with kernel-based linear transformers tending to provide an excellent quality-throughput trade-off. It is clear from the figure that\nLeaPformer provides the best performance trade-off, exhibiting significant quality increases over Linear Transformer and overall supremacy compared to Performer, Linformer, Reformer, and Skyformer, with a reduced memory footprint. Details on inference quality are showcased in Table 1 ###reference_###, where both LeaPformer-0.2% and LeaPformer-1.5% exhibit a balanced performance profile. While classical softmax attention achieves the highest average score by a notable margin, it is beaten on a number of tasks by other methods.\nCompared to cosFormer, LeaPformer provides, at a minor throughput and memory footprint penalty, significant increases to scores across cosFormer\u2019s most problematic tasks, including improvements for text and image classification. Additionally, when compared to the score profiles of other efficient attention mechanisms, LeaPformer does not seem to specialize nearly as much as other architectures (aside from some difficulty on the pathfinding task), indicating its balanced performance. BigBird is the closest to providing a similarly balanced inference quality profile, but this comes with significant throughput reductions as shown in Table 2 ###reference_### and noticeable increases to memory footprint. Regarding the application of the simple accuracy measure to the results in Table 1 ###reference_### and Table 2 ###reference_###, LeaPformer beats out all other options in Table 3 ###reference_### by a wide margin (minimum increase of 0.81), demonstrating its very effective relative performance on the LRA benchmark compared to softmax attention and its efficient attention peers. Across the board, LeaPformer matches the general inference quality of task-balanced models with a massively reduced memory footprint while still exhibiting a minimum 1.52x throughput increase over those mechanisms."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Autoregressive Language Modeling",
            "text": "While autoregressive language modeling has advanced tremendously with the advent of LLMs, more accessible methods can still serve to validate architectural differences between attention mechanisms. Given that, we\u2019ve employed the adaptive setup of Baevski & Auli 2019  ###reference_b3### and reuse nearly their exact model hyperparameters for autoregressive language modeling on Wikitext-103b (Merity et al., 2016  ###reference_b22###). Hyperparameter differences are only related to batch sizes and the number of updates due to computational constraints, and are detailed in our Appendix. All sequences during training and evaluation were composed of 512 tokens (i.e. 511 tokens of context where possible for evaluation).\nWe contrast a few attention mechanisms in Table 4  ###reference_###, including a ReLU-based mechanism functioning similar to Linear Transformers (Katharopoulos et al., 2020  ###reference_b14###) as well as an ablation of cosFormer\u2019s re-weighting function (Qin et al., 2022b  ###reference_b29###). As observed in Table 4  ###reference_###, classical softmax attention outperforms all linear attention mechanisms by a wide margin, but amongst the linear attention mechanisms themselves there are distinctions in terms of quality. Notably, LeaPformer demonstrates significant improvement over its linear attention peers, beating out cosFormer by 0.13 perplexity on the test set while only requiring a parameter increase of approximately 3.13%, though it still falls significantly short of classical softmax attention by 2.37 perplexity."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Simultaneous Speech Translation (SimulST)",
            "text": "For the purposes of our SimulST related experiments, we employed a model inspired by the ESPnet-ST toolkit (Inaguma et al., 2020  ###reference_b13###) that focused on end-to-end speech-to-text (S2T) translation with a modified cross-attention block for a wait-k and fixed pre-decision paradigm (Ma et al., 2019  ###reference_b18###, 2020b  ###reference_b20###, 2020c  ###reference_b21###). All model encoders were pre-trained on automatic speech-recognition (ASR) and were trained on a wait-k of 5 and a fixed predecision ratio of 9 and were evaluated on a wait-k of 3 (a slightly larger k for training is suggested by prior work (Ma et al., 2019  ###reference_b18###)) with greedy decoding. Models are evaluated via validation set perplexity and by detokenized BLEU-4 (Post, 2018  ###reference_b26###) via SimulEval (Ma et al., 2020a  ###reference_b19###). Two language pairs and two datasets were employed to test the application of LeaPformer to simultaneous tasks. We utilized MuST-C\u2019s (Cattoni et al., 2021  ###reference_b5###) English to German (en-de) split and CoVoST 2\u2019s (Wang et al., 2020a  ###reference_b37###) French to English (fr-en) split. More comprehensive evaluation is provided for the en-de pair, comparing the results of LeaPformer to an ablation without a re-weighting function. The application of LeaP modules resulted in an approximate parameter increase of 0.03% for ablations that included all attention blocks being linearized.\nWe first seek to show LeaPformer outperforms cosFormer in terms of model quality, justifying its inclusion not only from the perspective of necessity but also as an overall improvement. Table 5  ###reference_### demonstrates the results of a brief comparison on en-de SimulST (note that cosFormer can still be employed for training, where sequence lengths are known), where significant quality improvement is observed. Having established the capability of the proposed method, we seek to validate it further on en-de SimulST while also providing several ablations for LeaPformer, representing a wide-range of quality-throughput trade-offs. Additionally, we seek to show that applying the LeaP-augmented re-weighting function of LeaPformer is consistently useful by testing models trained without any re-weighting functionality, operating as a variation on Linear Transformer (Katharopoulos et al., 2020  ###reference_b14###). Table 6  ###reference_### showcases the results of this study, where LeaPformer ablations consistently beat their simple ReLU-based alternative. The most competitive ablation in terms of translation quality emerges as a model with the decoder self-attention block replaced by LeaPformer, achieving only a 0.26 BLEU reduction compared to softmax attention.\n###table_1### Similar results are provided for the fr-en language pair in Table 7  ###reference_###, with trends from en-de persisting. The most competitive translation quality ablations continue to be replacements of the decoder self-attention blocks with LeaPformer, where only a 0.23 BLEU reduction was observed. While our analysis related to SimulST is focused on analyzing the possible translation quality benefits of employing LeaPformers, we also provide some latency analysis, with some qualifications, employing common SimulST latency metrics in Appendix A.7  ###reference_###."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we made two concrete contributions. We re-framed reliance on explicit positional representations and sequence lengths to reliance on sequence proportions, removing theoretical dependence on sequence lengths. Additionally, we proposed LeaPformers and applied them to the state-of-the-art in proportion-based linear transformers, cosFormer, achieving the best performance trade-off on the Long-Range Arena benchmark and competitive results in autoregressive language modeling on Wikitext-103b. Moreover, we applied proportion-based transformers for the first time to simultaneous translation, achieving minimal quality loss compared to softmax attention for two language pairs."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Impact Statement",
            "text": "We advance the efficiency of transformers in state-of-the-art deep learning. Any societal consequences or impacts that typically relate to work focused on increased efficiency also apply here, as such work necessarily improves the practicality of deep learning models for an array of applications."
        }
    ],
    "url": "http://arxiv.org/html/2405.13046v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2",
            "2.3",
            "2.4"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.2",
            "4.3"
        ]
    },
    "research_context": {
        "paper_id": "2405.13046v1",
        "paper_title": "LeaPformer: Enabling Linear Transformers for Autoregressive and Simultaneous Tasks via Learned Proportions",
        "research_background": "### Motivation\nThe primary motivation for this paper is to address the limitations of conventional transformers, particularly their quadratic complexity in attention mechanisms, which poses challenges for processing long sequences. Efficient transformer variants have been developed, yet they often either rely on specific assumptions or exhibit performance degradation when generalized to a broader range of tasks. Furthermore, state-of-the-art linear transformers with position-based re-weighting functions rely on explicit token positions and sequence lengths, which is problematic for autoregressive and simultaneous tasks. This necessitates a new approach that can overcome these constraints.\n\n### Research Problem\nThe paper aims to solve the reliance on explicit positional representations and sequence lengths in state-of-the-art re-weighting functions for linear transformers. This reliance hinders the application of linear transformers in autoregressive tasks, where sequence lengths are not known beforehand, and simultaneous tasks. The goal is to develop a novel attention mechanism that removes these dependencies, thereby enabling linear transformers to perform effectively across a wider range of tasks without pre-known sequence lengths and with adaptable attention concentration patterns.\n\n### Relevant Prior Work\n1. **Transfomers and Efficient Attention Mechanisms**:\n   - **Vaswani et al. (2017)** introduced the original transformer architecture with softmax attention mechanisms, which has quadratic complexity.\n   - **Katharopoulos et al. (2020), Choromanski et al. (2020), Peng et al. (2021), Chen et al. (2021), and Qin et al. (2022b)** developed various linear attention mechanisms that significantly reduce complexity but still face limitations in generalizability across different tasks.\n\n2. **Re-weighting Functions**:\n   - **Su et al. (2022) and Qin et al. (2022b)** formalized re-weighting functions in linear transformers to concentrate attention scores. These techniques, however, depend on explicit positional information and sequence lengths.\n\n3. **Challenges in Autoregressive and Simultaneous Tasks**:\n   - **Agostinelli & Chen (2023)** highlighted the difficulty of applying position-based re-weighting functions in autoregressive tasks without specialized solutions.\n   \n4. **Benchmarks and Evaluation**:\n   - **Tay et al. (2021)** introduced the Long-Range Arena (LRA) benchmark to evaluate efficient attention mechanisms on long sequences.\n   - **Merity et al. (2016)** provided the Wikitext-103b dataset for autoregressive language modeling.\n   - **Ma et al. (2020c)** contributed to the field of simultaneous speech-to-text translation, providing data for evaluating translation tasks.\n\nBy leveraging these insights and challenges, the paper proposes the LeaPformer model with a novel approach that uses learned proportions to enhance linear transformers\u2019 applicability across various tasks, particularly addressing the issues in sequence length dependence and adaptable attention concentration patterns.",
        "methodology": "The paper introduces \"LeaPformer,\" a method aimed at adapting linear transformers for both autoregressive and simultaneous tasks, which traditionally face challenges when applying conventional techniques. The proposed methodology includes two key innovations:\n\n1. **Generalization to Proportional Reliance:**\n   - Instead of relying on absolute token positions and the total sequence length, the method generalizes the attention mechanism to depend on the relative placement of tokens within the sequence, termed as a \"proportion.\" \n   - This shift to a proportional reliance simplifies the analysis of the re-weighting function's behavior and eliminates the theoretical dependency on the length of the sequence.\n\n2. **Learned Proportions (LeaP):**\n   - The authors propose and build a compact module that learns proportional representations for each token. This provides a more flexible and dynamic method of re-weighting attention compared to traditional position-based methods.\n   - The technique is referred to as \"Learned Proportions\" (LeaP). Models utilizing this technique are called \"LeaPformers.\"\n\n**Advantages of LeaPformers:**\n- They can be applied to tasks with unknown sequence lengths.\n- They capture evolving attention patterns more effectively than position-based re-weighting functions.\n\nTogether, these innovations allow linear transformers to be effectively utilized in autoregressive and simultaneous tasks, thus expanding their applicability and efficacy.",
        "main_experiment_and_results": "**Main Experiment Setup and Results:**\n\n**Setup:**\n\n1. **Tasks and Datasets:**\n   - **Long-Range Arena (LRA) Benchmark:** Used to validate the capabilities of efficient attention mechanisms.\n   - **Autoregressive Language Modeling:** Employed on the Wikitext-103b dataset using an adaptive input/output architecture.\n   - **Speech-to-Text Simultaneous Translation (SimulST):** Evaluated across two language pairs using a wait-k read-write schedule.\n\n2. **Frameworks and Tools:**\n   - **Fairseq**: Used for training and validation in SimulST and autoregressive language modeling experiments.\n   - **SimulEval**: Used for evaluation in SimulST experiments.\n\n3. **Evaluation Metrics:**\n   - **LRA:** Evaluated via accuracy.\n   - **Autoregressive Language Modeling:** Evaluated via validation and test set perplexity.\n   - **SimulST:** Evaluated via detokenized BLEU-4 (BLEU) using sacreBLEU.\n\n**Results:**\n\nThe results of LeaPformers on the three major tasks are evaluated in terms of their respective metrics (accuracy for LRA, validation and test set perplexity for language modeling, and BLEU for SimulST). Specific numerical results and further comparisons are detailed in the paper's appendix, where additional hardware and hyperparameter details are provided.\n\nThis setup allows for a comprehensive assessment of LeaPformers' potential across different tasks, validating their efficacy in improving the performance of cosFormers in various contexts."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To validate the architectural differences between attention mechanisms for autoregressive language modeling and evaluate the performance of LeaPformer against other linear attention mechanisms.",
            "experiment_process": "The study employs the adaptive setup of Baevski & Auli 2019 for autoregressive language modeling on Wikitext-103b, reusing nearly their exact model hyperparameters. Differences are only in batch sizes and number of updates due to computational constraints, detailed in the Appendix. All sequences during training and evaluation were composed of 512 tokens. The attention mechanisms contrasted include a ReLU-based mechanism similar to Linear Transformers, and an ablation of cosFormer's re-weighting function.",
            "result_discussion": "Classical softmax attention outperforms all linear attention mechanisms by a wide margin. Amongst linear attention mechanisms, LeaPformer shows significant improvement over its peers, beating cosFormer by 0.13 perplexity on the test set with only a 3.13% parameter increase, though still lagging behind classical softmax attention by 2.37 perplexity.",
            "ablation_id": "2405.13046v1.No1"
        },
        {
            "research_objective": "To evaluate the performance improvement of LeaPformer in simultaneous speech-to-text translation tasks and validate its utility over other linear attention mechanisms.",
            "experiment_process": "The study employs a model inspired by the ESPnet-ST toolkit, focusing on end-to-end S2T translation using a modified cross-attention block for a wait-k and fixed pre-decision paradigm. Model encoders were pre-trained on ASR and evaluated with greedy decoding on a wait-k of 3. Two datasets are used: MuST-C\u2019s English to German (en-de) split and CoVoST 2\u2019s French to English (fr-en) split. Models are evaluated via validation set perplexity and detokenized BLEU-4 using SimulEval. Comprehensive evaluation compares LeaPformer to an ablation without a re-weighting function.",
            "result_discussion": "LeaPformer significantly improves quality over cosFormer in en-de SimulST. Ablations demonstrate that LeaPformer consistently outperforms simple ReLU-based alternatives. The most competitive translation quality ablation is a model with the decoder self-attention block replaced by LeaPformer, showing only a 0.26 BLEU reduction compared to softmax attention. Similar trends are observed in the fr-en pair, with only a 0.23 BLEU reduction.",
            "ablation_id": "2405.13046v1.No2"
        }
    ]
}