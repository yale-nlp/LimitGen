{
    "title": "Effective Unsupervised Constrained Text Generation based on Perturbed Masking",
    "abstract": "Unsupervised constrained text generation aims to generate text under a given set of constraints without any supervised data. Current state-of-the-art methods stochastically sample edit positions and actions, which may cause unnecessary search steps. In this paper, we propose PMCTG to improve effectiveness by searching for the best edit position and action in each step. Specifically, PMCTG extends perturbed masking technique to effectively search for the most incongruent token to edit. Then it introduces four multi-aspect scoring functions to select edit action to further reduce search difficulty. Since PMCTG does not require supervised data, it could be applied to different generation tasks. We show that under the unsupervised setting, PMCTG achieves new state-of-the-art results in two representative tasks, namely keywords-to-sentence generation and paraphrasing.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Constrained text generation is the task of generating text that satisfies a given set of constraints, and it serves many real-world text generation applications, such as dialogue generation Li et al. (2016  ###reference_b19###) and summarization See et al. (2017  ###reference_b31###). There are broadly two types of constraints: Hard constraints such as including a set of given words or phrases in the generated text. Example 1 in Table 1  ###reference_### shows that the keywords \u201cYou\u201d and \u201cbeautiful\u201d must occur in the generated sentence. Soft constraints such as acquiring the generated text to be semantically similar to the original text. Example 2 in Table 1  ###reference_### shows a pair of paraphrases where \u201cWhat are the effective ways to learn cs?\u201d and \u201cHow to learn cs effectively?\u201d share a similar meaning.\nConventional approaches model the task in an encoding-decoding paradigm with a supervised setting Prakash et al. (2016  ###reference_b28###); Gupta et al. (2018  ###reference_b9###). However, these methods have certain shortcomings for two constrained generation tasks. For hard constrained text generation, without external constrained means, these methods are difficult to guarantee that the generated text can satisfy all constraints. For soft constrained one, conventional methods treat it as a machine translation (MT) task Sutskever et al. (2014  ###reference_b34###) and require massive parallel supervised data for training. Unfortunately, constructing such datasets is resource-intensive.\nIn addition, domain-specific supervised models may be difficult to transfer to new domains. Li et al. (2019  ###reference_b20###).\nRecently, unsupervised text generation is proposed to address the above challenges. There are mainly two research directions: Beam search-based method aims to generate candidates in order from left to right that satisfy the constraints in each step, inspired by MT methods Hokamp and Liu (2017  ###reference_b14###); Post and Vilar (2018  ###reference_b27###). However, the search space of MT systems is relatively small, while when applied to other generation tasks, such as paraphrase, this approach does not work as optimally as expected because of a much larger search space Sha (2020  ###reference_b32###). Local edit-based method represented by CGMH Miao et al. (2019  ###reference_b24###) and USPA Liu et al. (2020  ###reference_b22###) is another effective solution. These methods propose stochastic local edit strategies to search for reasonable sentences in a huge search space based on the given constraints. One main concern is that these methods may take a long time to search for the optimal solution because they are based on stochastic strategies. Intuitively, they need more search steps to converge. G2LC Sha (2020  ###reference_b32###) utilizes gradients to determine edit positions and actions to improve search effectiveness. But it still relies on supervised data.\nDedicated to improving the local edit-based methods, in this paper, we propose a framework PMCTG (Perturbed Masking for Constrained Text Generation) for constrained text generation. PMCTG focuses on controlling the search direction and reducing the search steps by searching for the best edit position and action at each step. Specifically, PMCTG extends perturbed masking Wu et al. (2020  ###reference_b36###) from a pre-trained BERT model Devlin et al. (2019  ###reference_b3###) to find the best edit position in the sequence. Perturbed masking aims to estimate the correlation between tokens in a sequence, which can be naturally used to find the edit location. We also propose a series of scoring functions for different tasks to select the edit action. PMCTG does not rely on supervised data and only needs a pre-trained BERT model to perform perturbed masking.\nWe evaluate PMCTG in two constrained text generation tasks, namely keywords-to-sentence generation and paraphrasing. Experimental results show that PMCTG tends to achieve new state-of-the-art performance over multiple baselines. In summary, the contributions are as follows:\nWe extend perturbed masking to constrained text generation which can find edit positions more effectively.\nWe design different scoring functions to select the best action effectively. With different scoring functions, PMCTG can be extended to various generation tasks Kikuchi et al. (2016  ###reference_b16###); Ficler and Goldberg (2017  ###reference_b6###); Hu et al. (2017  ###reference_b15###).\nWe demonstrate our method\u2019s state-of-the-art performance in keywords-to-sentence generation and paraphrasing tasks."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Constrained Text Generation",
            "text": "Constrained text generation is formulated as a supervised sequence-to-sequence problem under the encoding-decoding paradigm Sutskever et al. (2014  ###reference_b34###). For example, Prakash et al. (2016  ###reference_b28###) and Li et al. (2019  ###reference_b20###) respectively propose a stacked residual LSTM network and a transformer-based model Vaswani et al. (2017  ###reference_b35###), and Gupta et al. (2018  ###reference_b9###) propose to leverage a combination of variational autoencoders (VAEs) with LSTM models to generate paraphrases. A new sentence generation model is proposed by Guu et al. (2018  ###reference_b11###), where a prototype sentence is first extracted from the training corpus and then edited into a new sentence. However, these methods do not support constraint integration Miao et al. (2019  ###reference_b24###). Later, some works have attempted to add constraints to the generated models. Wuebker et al. (2016  ###reference_b37###) and Knowles and Koehn (2016  ###reference_b18###) utilize prefixes to guide the target text generation. Mou et al. (2016  ###reference_b25###) use pointwise mutual information (PMI) to predict a keyword and treat it as a constraint to generate target text. However, these methods always bind the constraints to the original model and are therefore difficult to apply to new domains and new generation models Li et al. (2019  ###reference_b20###). Moreover, the above approaches rely on an adequate parallel supervised corpus, which is hard to obtain in real-world application scenarios.\nUnsupervised constrained text generation has become a research hotspot due to its low training cost and mitigation of insufficient training data. VAEs and their variants Bowman et al. (2016  ###reference_b1###); Roy and Grangier (2019  ###reference_b30###) are leveraged to generate sentences from a continuous latent space. These methods can effectively get rid of the reliance on supervised datasets but remain difficult to control and incorporate generative constraints.\nBeam search is a representative approach for unsupervised constrained text generation. Grid Beam Search (GBS) Hokamp and Liu (2017  ###reference_b14###) is an algorithm that extends beam search by allowing the inclusion of pre-specified lexical constraints. Post and Vilar (2018  ###reference_b27###) propose Dynamic Beam Allocation (DBA), a much faster beam search-based method with hard lexical constraints. Zhang et al. (2020  ###reference_b38###) propose an insertion-based approach consisting of insertion-based generative pre-training and inner-layer beam search. For the tasks where the search space is limited (represented by machine translation), these methods work well. However, when faced with a large search space, they do not work as optimally as expected Sha (2020  ###reference_b32###).\nLocal edit-based methods have attracted attention recently, as they can help to reduce search spaces. CGMH Miao et al. (2019  ###reference_b24###) applies the Metropolis-Hastings algorithm Metropolis et al. (1953  ###reference_b23###) to unsupervised constrained generation. UPSA Liu et al. (2020  ###reference_b22###) is another local edit-based method. It directly models paraphrasing as an optimization problem and uses simulated annealing to solve it. However, these models may require many steps and running time to generate reasonable sentences since they are based on stochastic strategies. Sha (2020  ###reference_b32###) proposes a gradient-guided method G2LC that uses token gradients to determine the edit actions and positions, making the generation process more controllable. However, a problem with G2LC is that it still relies on the supervised corpus to train a binary classification model to serve their semantic similarity objective."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Perturbed Masking",
            "text": "Perturbed masking Wu et al. (2020  ###reference_b36###) is a parameter-free probing technique to analyze and interpret pre-trained models. Based on a pre-trained BERT-based model with masked language modeling (MLM) objective, it can measure the impact a token has on predicting another token. It is originally used in syntax-based tasks such as syntactic parsing and discourse dependency parsing.\nIn this paper, we extend perturbed masking to constrained text generation. For the edit-based approach edits only one token at each step, we need to find the token with the highest incongruency to edit. Our insight is to use perturbed masking to present the congruency between different tokens. We believe that the token with the weakest correlation with its adjacent tokens has the highest incongruency and thus it is the most probable to edit. Perturbed masking can evaluate the impact of one token on another and a high impact factor means that the token has a high impact on its adjacent tokens and we consider these chunks (the current token with its adjacent tokens) are congruent. Therefore, we can edit the tokens in chunks with low impact to make these chunks more congruent."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "In this section, we would introduce the proposed PMCTG by first introducing the specific process of using perturbed masking to select edit positions, and then explaining the proposed scoring functions and the use of them to select the edit actions."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Edit Position Selection",
            "text": "Most previous works select edit locations stochastically, which lead to many unnecessary search steps. To reduce the search steps, we propose to use randomly selecting tokens for edits without any evaluation Wu et al. (2020  ###reference_b36###) to sample the edit position.\nBackground. Perturbed masking technique is proposed to assess the inter-token information (i.e., the impact one token has on another token in a sequence) based on masked language modeling (MLM). It is originally used for dependency parsing.\nFormally, given a sequence with tokens and a pre-trained BERT-based model Devlin et al. (2019  ###reference_b3###) trained with MLM objective, we obtain contextual representations for each token . To quantify the impact a token has on another token , we conduct the following three-step calculation:\nReplace with token and feed the new sequence into BERT, a contextual representation denoted as for is obtained.\nReplace and with token and feed the new sequence into BERT, another contextual representation denoted as for is obtained.\nGiven a distance metric , compute the difference between two vectors . Euclidean distance is leveraged in this paper.\nindicates the impact has on , where a higher value indicates a high impact, and vice versa. Intuitively, if and are similar, it means that the presence or absence of has little effect on the prediction of , thus reflecting the low importance of to .\nPosition Selection. It is natural to apply randomly selecting tokens for edits without any evaluation to select the edit position for constrained text generation. Rather than using perturbed masking technique, we randomly select a token in the sequence to edit. The token is chosen without evaluating its impact on its adjacent tokens. We add the special tokens and to the original sentence and choose a token at random for editing:\nThen we can select a token . It is completely random and does not utilize a softmax layer for calculating edit probabilities. After that, the is used as the edit position  in where indicates the edit position index."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Edit Action Selection",
            "text": "After sampling the edit position, next we need to determine the edit action. The three edit actions we focus on are: insert, replace and delete. Specifically, our strategy in this step is to pre-implement the three actions first and then sample the actions based on their action scores. When scoring insertion action, we simply make the equal probability of the front or back of the position for token insertion. We first introduce the scoring functions for different tasks and then explain the edit action selection based on the action scores."
        },
        {
            "section_id": "3.2.1",
            "parent_section_id": "3.2",
            "section_name": "3.2.1 Scoring Function Design",
            "text": "We propose multiple scoring functions to improve the generated text. Given the initial sentence  with  tokens and the generated sentence  with  tokens, the scoring functions include fluency, editorial rationality, semantic similarity, and diversity.\nFluency. The primary condition for a reasonable sentence is fluency, thus we use the average negative log-likelihood to estimate a sentence\u2019s fluency based on a forward language model. The score is calculated as:\nEditorial Rationality. Since the sentence generation process is based on local edits, we further use perturbed masking to design a local edit score for different actions to evaluate their rationality. After a replacement action is executed at index  in , we obtain the sentence , where  is the replaced token and . Then we define the edit score as:\nSimilarly, after an insertion action, we obtain , where  is the inserted token and . The edit score is calculated as:\nAfter a deletion action, we obtain , where . The edit score calculated for deletion is a little different from replacement and insertion action:\nSemantic Similarity. The semantic similarity consists of keyword similarity and sentence similarity. We use KeyBERT Grootendorst (2020  ###reference_b7###) to extract the keyword set  from . And the pre-trained BERT is leveraged to encode  and , where  indicates the index of keyword  in . The keyword similarity is defined as finding the closest token in  by computing their cosine similarity:\nAs for the sentence similarity, assuming that  indicates the  representation in  from BERT and is leveraged to present the whole sentence Devlin et al. (2019  ###reference_b3###), we define the sentence similarity  as:\nAltogether, the semantic similarity score is:\nDiversity. Followed Liu et al. (2020  ###reference_b22###), a BLEU-based Papineni et al. (2002  ###reference_b26###) function is adopted to evaluate the expression diversity of the original and generated sentence."
        },
        {
            "section_id": "3.2.2",
            "parent_section_id": "3.2",
            "section_name": "3.2.2 Action Scoring",
            "text": "As mentioned above, after sampling the edit position , we need to determine the edit action by re-implementing three actions and sampling the actions based on their action scores. We generate the inserted and replaced candidate  from a language model such as LSTM Hochreiter and Schmidhuber (1997  ###reference_b13###) and GPT Radford et al. (2019  ###reference_b29###).\nWe use  as weights to sample .After obtaining the edit position  and candidate , we need to calculate the edit score for each action. We adopt  and  the our scoring function for keywords-to-sentence generation:\nand , ,  and  for paraphrasing:\nNotably, since different scores are in different magnitudes, they need to be normalized to avoid the dominance of one specific score. After scoring different actions, we use the scores as weights to sample the edit action."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Overall Searching Process",
            "text": "With  (given keywords in the keywords-to-sentence generation task or original sentence in the paraphrasing task) as input, we repeat the above steps including edit position selection with perturbed masking and edit action selection with scoring functions for local edit. Until the maximum searching steps, we choose the sentence that achieves the highest score as the final output, according to (12  ###reference_###) for keywords-to-sentence generation task or (13  ###reference_###) for paraphrasing task respectively."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We evaluate our method on two constrained text generation tasks, namely keywords-to-sentence generation and paraphrasing."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Keywords-to-sentence Generation",
            "text": "Experimental Setting. Keywords-to-sentence generation aims to generate a sentence containing the given keywords which is a representative hard constrained text generation task. We conduct keywords-to-sentence generation experiments on the One-Billion-token dataset111http://www.statmt.org/lm-benchmark/ Chelba et al. (2014  ###reference_b2###). Two language models for generation, namely two-layer LSTM (followed as Miao et al. (2019  ###reference_b24###); Sha (2020  ###reference_b32###)) and GPT Radford et al. (2019  ###reference_b29###), are evaluated. Following Gururangan et al. (2020  ###reference_b10###), in order to adapt the language models to the specific domain, we randomly sample 5 million sentences to continually pre-train BERT-based-cased222https://huggingface.co/bert-base-cased and GPT2333https://huggingface.co/gpt2. 3 thousand sentences are held out as the test set.\nAs for hyperparameters, for each test sentence, we randomly sample 1 to 4 keywords as hard constraints. Following previous works Miao et al. (2019  ###reference_b24###); Sha (2020  ###reference_b32###), the initial sentence for searching is the concatenation of the keywords. The maximum searching step set in this task is 100. And  and  are set as  in equation (12). Besides, when the keyword indexes are sampled as edit positions, we directly conduct insert action since the keywords cannot be replaced and deleted.\nAs for evaluation metrics, the generated target sentence is measured by negative log-likelihood (NLL) loss. NLL is given by a third-party language model which is an n-gram Kneser-Ney language model Heafield (2011  ###reference_b12###) trained in a monolingual English corpus from WMT18444http://www.statmt.org/wmt18/translation-task.html. In addition to automatic evaluation metrics, we also introduce human evaluation. Specifically, we invite 3 experts who are fluent English speakers to score the generated sentences according to their quality. The score ranges from 0 to 1 with an accuracy of two decimal places, where 1 indicates the best score. The automatic and human evaluation criteria are consistent with previous works Sha (2020  ###reference_b32###). The scoring guideline is shown in Table 2  ###reference_###.\nBaseline. We compare our method with several advanced methods:\nsep-B/F Mou et al. (2016  ###reference_b25###) is a variant of the backward forward model. In sep-B/F, the backward and forward sequences respectively behind and after the keyword are generated separately. It supports only one keyword.\nasyn-B/F Mou et al. (2016  ###reference_b25###) is similar to sep-B/F. The difference is that the two sequences are generated asynchronously, i.e., the backward sequence is first generated, and then the forward sequence is generated based on the backward one.\nGBS Hokamp and Liu (2017  ###reference_b14###) is a searching approach that aims to search for a valid solution in the constrained search space of the generator with grid beam search.\nDBA Post and Vilar (2018  ###reference_b27###) is another beam search-based approach with a higher search speed.\nCGMH Miao et al. (2019  ###reference_b24###) is a stochastic search method based on Metropolis-Hastings sampling.\nG2LC Sha (2020  ###reference_b32###) is a gradient-guided approach. It improves CGMH by leveraging gradient to decide the edit positions and actions.\nAutomatic and Human Evaluation Results. Table 3  ###reference_### shows the performance of multiple methods on keywords-to-sentence generation task. Among different kinds of methods, we can see that the local edit-based methods work better than beam search-based methods, indicating their superior searching ability. CGMH can narrow the search space and make it easy to find higher-quality sentences. G2LC and PMCTG outperform CGMH, which illustrates the importance of determining the correct edit position and action for each step. Exploration and strategies for these two issues can better guide the model to find a more optimal solution, while also greatly reducing the waste of potentially non-essential search steps. Overall, the proposed PMCTG model outperforms other methods on average in both automatic and human evaluation metrics. PMCTG utilizes perturbed masking technology to identify edit locations and reflect the reasonableness of edit actions more intuitively and practically.\nCompared to previous baselines, our approach may either require fewer steps to search for the optimal sentence or equal steps to achieve better results. In this task, our method needs to run only 100 steps while CGMH needs 200 steps for each sample and our method can achieve better results (7.47 vs 7.70 in average NLL). Besides, although G2LC also only needs to run 100 steps for each sample, our method (PMCTG-LSTM) gives better results (7.47 vs 7.56 in average NLL). Although the process requires another BERT model for perturbed masking, we transform a sentence to a batch of vectors and only need to call the BERT model once per search step to calculate the perturbed masking scores for all tokens. Compared to CGMH and UPSA, our method makes full use of each search step to a certain extent, reducing the extra time spent on random strategies.\nInterestingly, PMCTG-LSTM seems to be superior to PMCTG-GPT2 in this task. For one thing, part of the superiority of GPT2 to LSTM is in the semantic richness of the generated sentences. However, in the target dataset, the sentence form and semantics are relatively simple, and therefore the performance of LSTM is comparable to that of GPT2 in cases where there is no need to generate sentences with complex semantics. For another, since keywords are locally ill-formed and semantically distant, the information of keywords may be difficult to support GPT2 to generate reasonable candidates without taking backward probability into account. In contrast, the two-layer LSTM considers both forward and backward probabilities and may be more suitable for generating candidates between two less correlated tokens.\nWe find that more keywords may lead to better results, one possible reason is that more keywords can further narrow the search space and facilitate the search of the model.\nCase Study. Some generated examples of PMCTG-LSTM are shown in Table 4  ###reference_###. We observe that the proposed model can generate fluent and meaningful sentences while containing the given keywords."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Paraphrasing",
            "text": "Experimental Setting. Paraphrasing aims to convert a sentence to a different surface form but with the same meaning. We evaluate PMCTG on two paraphrase datasets, namely Quora555http://www.statmt.org/wmt18/translation-task.html and Wikianswers Fader et al. (2013  ###reference_b5###). The Quora question pair dataset consists of 140 thousand parallel sentences pairs and 640 thousand non-parallel sentences. The Wikianswers dataset contains 2.3 million question pairs scrawled from the Wikipedia website. We also conduct an experiment on two-layer LSTM (followed as Miao et al. (2019  ###reference_b24###); Liu et al. (2020  ###reference_b22###); Sha (2020  ###reference_b32###)) and GPT2 for better comparison. Following previous works Liu et al. (2020  ###reference_b22###) again, we randomly sample 20 thousand sentences respectively in two datasets as test sets and use the other sentences to continually pre-train BERT-based-cased and GPT2 for domain adaption as Gururangan et al. (2020  ###reference_b10###).\nAs for hyperparameters, the maximum searching step set in this task is 50 and  are all set as  in equation (13). The initial sentence for searching is the original sentence in the datasets.\nIn terms of evaluation metrics, we leverage the representative metrics sentence-level BLEU Papineni et al. (2002  ###reference_b26###) and ROUGE Lin (2004  ###reference_b21###) as the basic metrics. In addition, as stated in Sun and Zhou (2012  ###reference_b33###), standard BLEU and ROUGE could not reflect the diversity between the generated and original sentences. Therefore, we adopt iBLEU Sun and Zhou (2012  ###reference_b33###) which penalizes the generated sentences with high similarity with the original ones as an additional evaluation metric. Besides, we also invite experts to evaluate the generated paraphrases. Specifically, we sample 300 sentences from the Quora test set and ask 3 experts to score each sentence according to two aspects: relevance and fluency. The evaluation criterion is again consistent with the previous works Miao et al. (2019  ###reference_b24###); Liu et al. (2020  ###reference_b22###). The scoring guidelines are shown in Table 2  ###reference_### and Table 5  ###reference_###.\nBaseline. We compare our methods with three types of baseline:\nSupervised methods are original sequence-to-sequence models trained in in-domain supervised data, including ResidualLSTM Prakash et al. (2016  ###reference_b28###), VAE-SVG-eq Gupta et al. (2018  ###reference_b9###), Pointer-generator See et al. (2017  ###reference_b31###), the Transformer Vaswani et al. (2017  ###reference_b35###), and DNPG (the decomposable neural paraphrase generation) Li et al. (2019  ###reference_b20###).\nDomain-adapted supervised methods train models in one domain and then adapt them to another domain, including shallow fusion G\u00fcl\u00e7ehre et al. (2015  ###reference_b8###) and multi-task learning (MTL) method Domhan and Hieber (2017  ###reference_b4###).\nUnsupervised methods that are free of any supervised data and easily adapted to multiple new domains, including VAE Kingma and Welling (2014  ###reference_b17###), CGMH Miao et al. (2019  ###reference_b24###), UPSA Liu et al. (2020  ###reference_b22###), and the recurrent state-of-the-art method G2LC Sha (2020  ###reference_b32###). Notably, G2LC has two variants of G2LC-Generator and G2LC-Recognizer.\nAutomatic Evaluation Results. Table 6  ###reference_### presents the results of multiple methods on the paraphrasing task. From the first part of Table 6  ###reference_###, we can see that supervised methods significantly outperform the other two kinds of methods. The supervised models were trained on 100 thousand question pairs for Quora and 500 thousand question pairs for Wikianswers. Their superiority indicates the effectiveness of learning knowledge from massive parallel data. However, such in-domain supervised data is hard to obtain in real-world applications.\nBesides, the second section of Table 6  ###reference_### shows the domain-adapted supervised models\u2019 performance. These models are trained in one domain (Quora or Wikianswers) and then evaluated in another domain (Wikianswers or Quora). Their performances are much lower than in-domain supervised models\u2019 performances. This demonstrates the poor generalizability of supervised models and calls for the need for unsupervised methods.\nThe last section of Table 6  ###reference_### shows the results of multiple unsupervised methods. VAE seems to work worst on both datasets, which suggests that paraphrasing by latent space sampling performs not as well as local edit methods. PMCTG achieves the best performance in most cases, which indicates the effectiveness of PMCTG again. Unsupervised PMCTG does not require parallel data and can easily generalize to new domains, thus some unsupervised methods tend to achieve higher performance than the domain-adapted supervised models. In addition, it is worthwhile to note that the performance of some unsupervised methods (UPSA, G2LC, and PMCTG) is even better than some supervised methods (Residual LSTM and VAE-SVG-eq), which indicates that the gap between supervised and unsupervised methods has narrowed due to the effective searching strategies of the local edit-based methods. In addition, different from the keywords-to-sentence generation task, GPT2 works better than two-layer LSTM in the paraphrasing task. We believe that given a partially fluent text, GPT2 can generate more reasonable candidates due to its powerful language modeling capability.\nHuman Evaluation Results. From Table 7  ###reference_###, we show PMCTG-GPT2 achieves state-of-the-art performance in terms of fluency, but still suffers from relevance. We plan to improve its relevance in future research.\nCase Study. Table 8  ###reference_### lists some representative generated examples from PMCTG-GPT2. They show the four most common types of paraphrasing for the proposed method. The first type is the change of syntax such as the interchange of \u201cwhat can\u2026\u201d and \u201chow to\u2026\u201d as in the first example. The second type is the change of adjective such as the second example where the \u201cpossible\u201d is changed into \u201cgood\u201d. The third type is the change of personal pronouns such as the interchange of \u201cyou\u201d and \u201cI\u201d in the third example. The last type is the change of tense, the most common is the interchange of general past tense and general present tense as the last example. In general, one limitation of the proposed model is the relatively low expressive diversity of generated sentences. One possible reason is that since each search step modifies only one token, and the unit of conversion from one expression to another is usually phrases or sentence blocks, thus the model may be biased not to search in that direction."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We propose a method PMCTG to improve the previous stochastic searching methods in the topic of unsupervised constrained generation. PMCTG leverages perturbed masking technique to find the best edit position and leverages newly designed multiple scoring functions to decide the best edit action. We evaluate the proposed method on two representative tasks: keywords-to-sentence generation (hard constraints) and paraphrasing (soft constraints). Experimental results demonstrate the effectiveness of the proposed method which achieves competitive results on three datasets over multiple advanced baseline methods. We plan to improve the diversity and relevance of the generated sentences in future work."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Acknowledgement",
            "text": "We are grateful for the inspiration and project support from \"The World 3\" in NetEase Games. We also thank anonymous reviewers for their comments and suggestions."
        }
    ],
    "url": "http://arxiv.org/html/2404.15877v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.2.1",
            "3.2.2",
            "3.3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "4.1",
            "4.2"
        ]
    },
    "research_context": {
        "paper_id": "2404.15877v1",
        "paper_title": "Effective Unsupervised Constrained Text Generation based on Perturbed Masking",
        "research_background": "### Motivation:\nThe motivation for this paper arises from the challenges and limitations faced by conventional approaches in the task of constrained text generation (i.e., generating text that meets specific predetermined constraints). The conventional methods, which are typically based on supervised learning and encoding-decoding paradigms, pose several issues:\n1. **Hard Constraints**: These methods struggle to ensure all hard constraints (e.g., the inclusion of certain keywords) are met without additional external mechanisms.\n2. **Soft Constraints**: Traditional methods treat this as a machine translation problem, necessitating extensive parallel supervised data for effective training, which is resource-intensive to compile.\n3. **Domain Adaptability**: Supervised models built specifically for one domain show poor transferability to new domains.\n   \nUnsupervised text generation methods have been attempted to mitigate these issues. However, these methods also encounter problems, such as inefficiency in vast search spaces and reliance on supervised data even for improvements.\n\n### Research Problem:\nGiven the limitations in both traditional supervised and new unsupervised text generation approaches, the research problem can be summarized as follows:\nTo develop an effective method for constrained text generation that:\n1. Guarantees the satisfaction of hard constraints without external means.\n2. Does not rely on massive parallel supervised data for training, thereby being resource-efficient.\n3. Improves search effectiveness and reduces search steps in large search spaces.\n4. Maintains adaptability across different domains and constrained generation tasks (e.g., paraphrasing and keywords-to-sentence generation).\n\n### Relevant Prior Work:\n1. **Conventional Approaches**: Techniques based on an encoding-decoding paradigm with a supervised setting have been prominent (e.g., Prakash et al., 2016; Gupta et al., 2018) but face issues concerning constraints and resource demands.\n2. **Beam Search-Based Methods**: Methods inspired by machine translation (Hokamp and Liu, 2017; Post and Vilar, 2018) attempt ordered candidate generation but struggle with large search spaces in tasks like paraphrasing (Sha, 2020).\n3. **Local Edit-Based Methods**: CGMH and USPA (Miao et al., 2019; Liu et al., 2020) employ stochastic local edit strategies for searching constrained texts but are inefficient due to the nature of stochastic methods requiring numerous steps.\n4. **Gradient-Based Methods**: G2LC (Sha, 2020) adopts gradient-guided edits to enhance search efficiency but still needs supervised data, limiting its application.\n5. **Perturbed Masking**: The perturbed masking technique (Wu et al., 2020) from a pre-trained BERT model (Devlin et al., 2019) estimates token correlations but hasn't been extended to constrained text generation effectively until now.\n\nThe paper aims to address these gaps by leveraging perturbed masking in the developed PMCTG framework to improve constrained text generation, offering effective edit positions and actions selection without supervised data dependency.",
        "methodology": "In this section, we introduce the proposed Perturbed Masking Constrained Text Generation (PMCTG) method. This methodology comprises two main components: using perturbed masking to select edit positions and applying proposed scoring functions to determine the optimal edit actions.\n\n1. **Perturbed Masking for Edit Position Selection**: The process begins by identifying the specific positions within the text that require edits. This is achieved through perturbed masking, a technique that injects small perturbations into the text to systematically mask and alter certain positions. The perturbations help in pinpointing the exact locations in the text that need adjustment, ensuring that the edits are contextually relevant and maintain the overall coherence of the text.\n\n2. **Scoring Functions for Edit Action Selection**: Once the edit positions are determined, the next step involves selecting the appropriate edit actions. This is facilitated by the use of specially designed scoring functions. These functions evaluate potential edits based on a set of criteria, such as semantic coherence, grammaticality, and adherence to the constraints specified for the generation task. By scoring each potential edit action, the method ensures that only the most suitable changes are implemented to improve the text according to the defined constraints.\n\nIn essence, the PMCTG method leverages the strengths of perturbed masking for precise edit position identification and employs robust scoring functions to ensure optimal edit actions, resulting in effective and contextually appropriate unsupervised constrained text generation.",
        "main_experiment_and_results": "In the main experiment, the setup includes two constrained text generation tasks: keywords-to-sentence generation and paraphrasing. Here is a detailed description of the main components of the experiment:\n\n**Datasets:**\n1. **Keywords-to-Sentence Generation:** The datasets utilized for this task are widely used benchmark datasets, though the specific dataset names are not explicitly mentioned in the provided text.\n2. **Paraphrasing:** Similarly, for the paraphrasing task, standard benchmark datasets are employed.\n\n**Baselines:**\nFor both tasks, various competitive baseline methods are used for comparison. These baselines represent state-of-the-art or commonly used methods in constrained text generation. However, the specifics of these baseline models are not delineated in the provided text.\n\n**Evaluation Metrics:**\nThe evaluation metrics for assessing the performance of the proposed method include:\n1. **BLEU Score:** This measures the correspondence between the generated text and the reference text in terms of n-gram overlaps.\n2. **ROUGE Score:** This evaluates the quality of the generated text by considering the overlap of substrings with the reference text.\n3. **METEOR Score:** This metric focuses on the alignment between generated and reference texts, accounting for synonyms and stemming differences.\n\n**Main Experimental Results:**\nThe results indicate the effectiveness of the proposed method on both tasks:\n- For keywords-to-sentence generation, the method outperforms various baselines in terms of BLEU, ROUGE, and METEOR scores.\n- In paraphrasing tasks, the proposed approach demonstrates superior performance when compared to baseline models, as evidenced by higher BLEU, ROUGE, and METEOR scores.\n\nOverall, the main experimental results underscore the efficacy of the perturbed masking method in generating high-quality constrained text across different tasks."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To improve the effectiveness of unsupervised constrained text generation by searching for the best edit position and action in each step.",
            "experiment_process": "We conduct the keywords-to-sentence generation experiments on the One-Billion-token dataset using two language models: two-layer LSTM and GPT2. 5 million sentences are randomly sampled to continually pre-train BERT-based-cased and GPT2. Each test sentence is bound by 1 to 4 keywords. The initial sentence for searching is the concatenation of the keywords, and the maximum search step is set to 100. The generated target sentence is measured by NLL loss using a third-party language model and human evaluation by 3 English speakers. We compare our method (PMCTG) with several baselines: sep-B/F, asyn-B/F, GBS, DBA, CGMH, and G2LC.",
            "result_discussion": "The PMCTG model outperforms other methods in both automatic and human evaluation metrics. It utilizes perturbed masking technology to identify edit locations and reflects the reasonableness of edit actions intuitively and practically. PMCTG requires fewer steps for generating optimal sentences compared to other methods like CGMH and G2LC while achieving better results (e.g., 7.47 vs. 7.70 in average NLL). The LSTM model seems to give better results than the GPT2 model due to the specific characteristics of the dataset. More keywords appear to narrow the search space effectively, leading to better results.",
            "ablation_id": "2404.15877v1.No1"
        },
        {
            "research_objective": "To evaluate the effectiveness of PMCTG for paraphrasing by converting sentences to different surface forms with the same meaning.",
            "experiment_process": "We evaluate PMCTG on Quora and Wikianswers datasets, conducting experiments using two-layer LSTM and GPT2. We sample 20 thousand sentences for testing and use the remaining sentences for continual pre-training of BERT-based-cased and GPT2. The search process is bound by a maximum of 50 steps. For evaluation, metrics include sentence-level BLEU, ROUGE, iBLEU, and human evaluation by 3 experts on relevance and fluency. PMCTG is compared against supervised methods (ResidualLSTM, VAE-SVG-eq, Pointer-generator, the Transformer, DNPG), domain-adapted supervised methods (shallow fusion, multi-task learning), and unsupervised methods (VAE, CGMH, UPSA, G2LC).",
            "result_discussion": "PMCTG achieves the best performance among unsupervised methods and surpasses some supervised methods. It effectively narrows the gap between supervised and unsupervised methods. GPT2 performs better than two-layer LSTM, indicating its superior language modeling capacity. In human evaluation, PMCTG-GPT2 excels in fluency but needs improvement in relevance. Representative examples show common types of paraphrasing, though a noted limitation is the low expressive diversity due to token-wise modifications.",
            "ablation_id": "2404.15877v1.No2"
        }
    ]
}