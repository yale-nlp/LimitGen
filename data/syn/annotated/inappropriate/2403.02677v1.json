{
    "title": "Finetuned Multimodal Language Models Are High-Quality Image-Text Data Filters",
    "abstract": "We propose a novel framework for filtering image-text data by leveraging fine-tuned Multimodal Language Models (MLMs). Our approach outperforms predominant filtering methods (e.g., CLIPScore) via integrating the recent advances in MLMs. We design four distinct yet complementary metrics to holistically measure the quality of image-text data. A new pipeline is established to construct high-quality instruction data for fine-tuning MLMs as data filters. Comparing with CLIPScore, our MLM filters produce more precise and comprehensive scores that directly improve the quality of filtered data and boost the performance of pre-trained models. We achieve significant improvements over CLIPScore on popular foundation models (i.e., CLIP and BLIP2) and various downstream tasks. Our MLM filter can generalize to different models and tasks, and be used as a drop-in replacement for CLIPScore. An additional ablation study is provided to verify our design choices for the MLM filter.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large-scale image-text datasets [31  ###reference_b31###, 32  ###reference_b32###, 38  ###reference_b38###, 30  ###reference_b30###, 3  ###reference_b3###] have been the major driving force for the recent breakthrough in Vision-Language Models (VLMs) and Text-to-Image generation models. The ever-growing size of such datasets allows researchers to scale the models to unprecedented capacities with billions or even trillions of parameters. These humongous foundation models lead to significant improvements in many down-stream tasks, such as image classification, text-to-image retrieval, image captioning, visual question answering, image generation and editing, etc. One great example is the OpenAI CLIP [29  ###reference_b29###] model, which is trained with 400M web-crawled image-text pairs. The CLIP model demonstrates impressive zero-shot learning capability across a wide range of different tasks.\nThe quality of image-text data plays a decisive role in the final performance of foundation models. But web-crawled image-text data are often very noisy, e.g., the corresponding text data is low quality or does not match the content of the image. How to build high-quality image-text datasets is a challenging research problem that attracts lots of interests recently. [48  ###reference_b48###] try to re-create the data curation process from CLIP. [25  ###reference_b25###] advocate that data quality is more important than quantity for model robustness. The DataComp challenge [11  ###reference_b11###] is introduced to systematically evaluate different data-filtering techniques.\nEach successful foundation model have their own secret recipes for data filtering. Before the invention of CLIP, most techniques are hand-designed or rule-based. For example, CC3M and CC12M design a series of heuristics for image-based, text-based and image&text-based filtering. Model-based filtering becomes popular since the introduction of CLIPScore [14  ###reference_b14###], which leverages the CLIP model to compute the cosine similarity between image and text to measure their alignment. CLIPScore has become the predominant method for filtering image-text data.\nHowever, recent research [40  ###reference_b40###, 41  ###reference_b41###] finds that visual features from CLIP are blind to subtle differences in the image, e.g., object number, shape and position. Because the contrastive loss is applied to the whole image, CLIPScore is less sensitive to capture the fine-grained object-level alignment information, shown in Figure 1  ###reference_###.\nAdditionally, the text encoder of CLIP can only process up to 77 tokens. The information loss from the text encoder can limit CLIPScore to process data with long captions. This limitation can be serious for Text-to-Image generation models [2  ###reference_b2###] that rely on long and highly-descriptive captions.\n###figure_1### Compared with the contrastively trained CLIP model, Multimodal Language Models (MLMs) have demonstrated promising capability in predicting the quality of generated images or text and aligning well with human preferences.\nMore specifically, the image-text matching scores generated by GPT-4Vision [26  ###reference_b26###] are more consistent with human experts compared with CLIPScore in recent MLM-based evaluation [49  ###reference_b49###, 52  ###reference_b52###].\nThis motivates us to integrate recent advances in MLMs for high-quality data filtering:\n\u201cCan we adapt strong MLMs to generate scores for assessing image-text data quality and outperform CLIPScore for image-text data filtering?\u201d\nThough GPT-4V is better at measuring image-text alignment, directly applying GPT-4V-scale MLMs in filtering billions of image-text data is computationally too costly. A good filtering method should be both effective and efficient due to the sheer amount of data we need to process.\nThere are smaller MLMs (e.g., LLaVA [19  ###reference_b19###], MiniGPT-4 [51  ###reference_b51###], etc), which are more efficient but fail to generate scores at a granularity that can reflect the subtle changes in the image-text data, since they are mainly instruction-tuned on task completion data. In this paper, we propose to combine the best of both worlds, leveraging proprietary LLMs or MLMs to construct high-quality instruction tuning data for effectiveness, and fine-tuning more accessible open-source MLMs to inject the knowledge from the high-quality data for efficiency.\nWe summarize our major contributions as follows:\nWe propose the MLM filter which incorporates the recent progress from MLMs for image-text data filtering and can be used as a drop-in replacement to the popular CLIPScore.\nWe design four diverse metrics to measure the image-text data quality from different perspectives, and a new pipeline to construct high-quality instruction data to harvest the information from proprietary models.\nFoundation models trained with our MLM filtered data demonstrate significant improvements, e.g., 1.7% better on 38 downstream tasks from DataComp comparing with CLIPScore."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Data Filters. Initial work, such as ImageNet [8  ###reference_b8###], relies on manual data filtering to select high-quality images and captions. More recent work [29  ###reference_b29###, 16  ###reference_b16###] pushes the size of image-text dataset to the order of hundreds of millions, and thus employs fixed rules and heuristics for filtering. LAION [38  ###reference_b38###] introduce the CLIPScore metric computed by the pre-trained CLIP model in filtering high-quality image-text pairs. CLIPScore filtering then becomes a widespread method of constructing large-scale web-crawled datasets [3  ###reference_b3###, 30  ###reference_b30###, 11  ###reference_b11###]. Based on that, DataComp [11  ###reference_b11###] is the first work to propose a benchmark for evaluating data filtering methods. [50  ###reference_b50###] introduce a set of tools to improve data filtering including CLIP-FLIP, distribution matching, de-duplication and clustering. Similarly, [21  ###reference_b21###] propose text masking to improve filtering. On the other hand, [10  ###reference_b10###] use high quality image-text pairs to train a new CLIP filtering network instead of using OpenAI\u2019s original CLIPScore. These papers all build upon CLIP filtering and introduce various techniques to improve it. In contrast, we investigate an alternate approach to CLIP-based Filtering, which employs fine-tuned Multimodal Language Models for large-scale image-text data filtering. Additionally, various works [6  ###reference_b6###, 45  ###reference_b45###] deploys proprietary LLMs like GPT-4 to score and filter text-only and visual instruction data.\nMultimodal Language Models. Recent Multimodal Language Models [1  ###reference_b1###, 13  ###reference_b13###, 44  ###reference_b44###, 18  ###reference_b18###, 51  ###reference_b51###, 19  ###reference_b19###] concatenate vision encoders with the latest LLMs via cross-model adapters to enable LLMs [39  ###reference_b39###, 5  ###reference_b5###, 42  ###reference_b42###] to take visual inputs. The most typical vision encoders deployed in MLMs are still the vision transformer models in CLIP pre-trained models [29  ###reference_b29###] for extracting visual features of input images. Moreover, various adapter architectures are proposed to connect the feature space of different modalities, including Q-former proposed by BLIP-2 [18  ###reference_b18###], a simple MLP layer used in LLaVA [19  ###reference_b19###], and Visual Experts of CogVLM [46  ###reference_b46###].\nMultimodal Instruction Tuning. Instruction tuning [22  ###reference_b22###, 43  ###reference_b43###, 27  ###reference_b27###] is a fine-tuning paradigm that enables LLMs to perform unseen tasks. This zero-shot performance is enabled by training LLMs using natural language instructions to explain the goal of the task. Instruction tuning is much more computationally efficient than full-set fine-tuning, and can enable LLMs to achieve zero-shot performance scores that are competitive with fully supervised models. LLaVA [19  ###reference_b19###] introduces multimodal instruction tuning via fine-tuning MLMs on a set of visual instructions. MLMs that use instruction tuning [9  ###reference_b9###, 17  ###reference_b17###] achieve SOTA performance on various vision-language tasks, such as visual question answering and visual reasoning."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Fine-Tuned Multimodal Language Models as Data Filters",
            "text": "To investigate the effects of each design choice, we keep the selection of the other three design choices the same and only change one design choice for each experiment group. As we propose four different metrics to assess data quality, we only adopt the metric of Object Detail Fulfillment as the filtering metric to select a high-quality subset from the 128M medium scale data pool. The ablation results for all four design choices are presented in Table 1  ###reference_###.\nThe first two lines in Table 1  ###reference_### demonstrate that adopting LLaVA as the captioning model to transform images into detailed descriptions for instruction data construction leads to better filtering performance. Next, adopting CC12M to sample image-text pairs for data construction outperforms the design choice of using DataComp-Medium dataset. We suppose it is because the image quality of CC12M is significantly better than that of DataComp, enabling the instruction tuning process more knowledge intensive. Thirdly, grouping the initial instructions into 10 buckets for sampling illustrates priority over using 100 buckets. In terms of the selection of teacher models, the MLM filters learned from different teacher models exhibit distinct strengths across different tasks. The MLM filter learned from GPT-4 performs better in VTAB [53  ###reference_b53###] classification and retrieval datasets, while the MLM filter learned from GPT-4V obtains higher scores in ImageNet [8  ###reference_b8###] related datasets. Finally, we decide to fix the other three choices as LLaVA captioner, CC12M data resources, and 10 sampling buckets. We report the two versions of MLM-based filters with different teacher models GPT4 and GPT-4V for future experiments, denoted as MLM-Filter-GPT4 and MLM-Filter-GPT4V respectively."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Overview",
            "text": "We propose to adopt fine-tuned Multimodal Language Model as effective data filters to select high-quality image-text data to promote the VLM pre-training, which involves three stages: 1) constructing multimodal instruction tuning data on proposed quality scoring tasks to fine-tune MLM to realize accurate quality assessment; 2) adopt the fine-tuned MLM Filter to generate quality scores for each data point in the data pool and then select the high-quality data; 3) pre-train VLMs using the filtered dataset and evaluate the pre-trained VLMs on downstream tasks to demonstrate the effectiveness of the proposed filtering method. The detailed pipeline for the three stages is shown in Figure 2  ###reference_###.\n###figure_2###"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Constructing Multimodal Instruction Tuning Data for Scoring Tasks",
            "text": "In order to work as an effective data filter, the MLM must generate quality scores for every single image-text pair for data selection and filtering. To enable MLMs like LLaVA to reason accurately on the quality score, we propose to fine-tune such MLMs on a set of scoring tasks to enhance their scoring capability. The multimodal instruction tuning data needed for scoring tasks are hard and expensive to collect via human labeling, and thus we leverage proprietary models GPT-4 or GPT-4V to construct such multimodal instruction data for scoring tasks.\nDefining Metrics for Image-Text Quality Assessment.\nConventional data filters like CLIPScore focus on the overall holistic matching of image and text via computing the cosine similarity between hidden features of image and text. However, such implicit scoring is poor in discriminating hard or ambiguous samples, leading to the false negative score predictions shown in Figure 1  ###reference_###. We propose to leverage strong Multimodal Language Models to predict the quality scores towards image-text pairs. Beyond the overall image-text alignment assessment, the fine-tuned MLM filters can evaluate the quality of image-text pairs from multiple perspectives. We propose four quality evaluation metrics to comprehensively evaluate the data quality:\nImage-Text Matching (ITM): the ITM metric focuses on evaluating whether the image caption accurately represents the main features and objects of the image and captures its primary theme. The fine-tuned MLM data filter can explicitly generate the ITM score on a scale of 100.\nObject Detail Fulfillment (ODF): the ODF metric focuses on evaluating whether the image caption provides detailed descriptions of objects that align with the image. Specifically, ODF assesses if the caption sufficiently describes the properties of the objects in the image, e.g., number, color, size, position, shape, etc. Compared with the ITM metric, the ODF metric focuses more on the fine-grained alignment between the detailed object properties in the image and the ones described in the corresponding caption.\nCaption Text Quality (CTQ): the CTQ metric focuses on evaluating the text quality of image caption based on the grammatical correctness, diversity of vocabulary (e.g., the range and uniqueness of words), fluency (e.g., smoothness and natural flow of sentences), readability, length, and structure. Previous data-centric research [50  ###reference_b50###] finds that web-crawled data is poor in its text quality, as it contains various bad text patterns, such as repeated words or textual noise. Thus, we propose to fine-tune MLMs to assess the text quality of image captions for data filtering.\nSemantic Understanding (SU): the SU metric focuses on determining if the image caption provides additional semantic information that is not readily apparent just from the image itself. Such auxiliary semantic information can be 1) the professions of persons in the image; 2) the locations, addresses, festivals, country names, city names; 3) the names or entities of buildings, people, bird species, animal breeds, car models, engines in the image; 4) the social relationships between the people in the image, i.e., lovers, parent, or child. We suggest that adopting SU metric for data filtering can select image-text pairs with auxiliary semantics, which can further enhance the commonsense reasoning capability of pre-trained VLMs.\nPrompting the Teacher Models. We select two state-of-the-art teacher models, GPT-4 and GPT-4V, to construct the multimodal instruction data for quality scoring tasks. Constructing multimodal instruction data with GPT-4V is much easier as GPT-4V can directly take visual inputs. As GPT-4 is a text-only LLM, we transform the image into a detailed text description to prompt a text-only GPT-4. The prompt for such dense captioning process is Please generate a dense caption in 4-6 sentences for describing the image in detail as much as you can. These comprehensive image descriptions are generated using a SOTA image captioning models, such as LLaVA or ShareGPT4V [4  ###reference_b4###]. With the prompt to the teacher model and the generated output, the visual instruction data can be simply formatted as User: {Prompt} Assistant: {Output}.\n###figure_3### ###figure_4### Prompting Strategies. As the scoring tasks involve a reasoning process to predict final accurate quality metrics for an image-text pair, we consider two prompting strategies to ensure the reasoning accuracy of the fine-tuned multimodal language model: Chain-of-Thought (CoT) Reasoning [47  ###reference_b47###], and Rationalization Reasoning [7  ###reference_b7###]. The major difference between the two prompting strategies are the generation order of the score and the generated reasoning steps. The exemplar prompts for two prompting strategies are presented in Appendix B  ###reference_### Table 7  ###reference_###. Between these two prompting strategies, we select the rationalization reasoning as we find it to be the most efficient and accurate. Computational efficiency is a concern as the scoring MLM should be able to score billions of image-text pairs. If the MLM is fine-tuned to output the score value first, the model\u2019s text generation process can be stopped early in the inference stage as only the score value is needed for filtering. Secondly, the experimental results of LLaVA demonstrate that the instruction tuning with rationalization reasoning leads to better performance on the ScienceQA benchmark [34  ###reference_b34###] than CoT reasoning. Four final prompts for different scoring metrics are presented in Appendix A  ###reference_###.\nSelecting Image-Text Pairs for Data Collection.\nThe multimodal instruction data used for fine-tuning should contain image-text pairs of varying quality.\nThus, data diversity is essential to enhance the fine-tuned MLM filter, enabling it to effectively score image-text data across all quality levels. We select two different image-text dataset as the data pool for constructing instruction tuning data: the Conceptual Captions 12M (CC12m) [32  ###reference_b32###], and the DataComp Medium 128M Dataset [11  ###reference_b11###].\nTo enhance the diversity of the instruction set, we perform clustering and uniform-sampling on the sentence embeddings of each captioning text.\nThe sentence embedding model we use is the pre-trained MPNet [37  ###reference_b37###] encoder model, which is contrastively pre-trained on a mixture of retrieval and natural language inference datasets. We directly use the pre-trained MPNet provided by Sentence Transformers [28  ###reference_b28###] to generate the sentence embedding towards each image caption. We set the number of clusters as  and  for CC12M and Datacomp-Medium, respectively. The image-text pairs for constructing instruction tuning data are uniformly sampled from each cluster, in which only one data point closest to the cluster centroid is selected.\nSampling Final Instructions for Scoring Tasks.\nAs we find that the initial  instruction data generated by teacher models are not uniformly distributed on the score scale of  in Figure 3  ###reference_###, we need to sample the initial instruction data into a balanced instruction set to avoid learning bias. Considering that the ideal size of multi-task instruction tuning dataset is  instructions [5  ###reference_b5###, 42  ###reference_b42###], we decide to sample  instructions from  initial generated instruction data for each scoring tasks, which ensure the generalization capability of instruction-tuned MLM. Thus, there are  instruction data of quality scoring tasks to be included in the total  instruction dataset, such that there is 1k instruction data for each proposed quality metric. We experiment with two sampling methods to ensure that the instruction data distribution is balanced on the scoring scale of : 1) grouping all data into  buckets and uniformly sampling  instructions from each bucket; 2) grouping all data into  buckets and uniformly sampling  instructions from each bucket. The score distribution of sampled 10k instruction in Figure 3  ###reference_### are more diverse and uniform than the original score distribution in Figure 3  ###reference_###. The code for sampling the final  instruction is presented in Appendix C  ###reference_###.\nMixture with instruction data of multi-tasks.\nThe multimodal instruction tuning process should involve a diverse set of tasks [9  ###reference_b9###, 17  ###reference_b17###] to enhance the zero-shot reasoning capability of fine-tuned MLMs. In addition to 4k multimodal instruction data of the proposed data quality scoring tasks, we sample another 46k multimodal instructions from LLaVA-665k instruction datasets. We allocate a larger portion of our data mixture to reasoning tasks, such as complex reasoning [19  ###reference_b19###] and GQA [15  ###reference_b15###] as we regard that enhancing reasoning capabilities will improve the scoring capability of our fine-tuned MLM.\nThe detailed statistics on the size of each dataset sampled for data mixture are presented in Appendix D  ###reference_### Table 8  ###reference_###."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Instruction-Tuning on Multimodal Language Models",
            "text": "We adopt LLaVA-1.5 based on Vicuna-13B LLM [5  ###reference_b5###, 17  ###reference_b17###] as the Multimodal Language Model architecture for instruction tuning on the mixed instructions of data quality scoring tasks and other multimodal tasks. The training process of LLaVA-1.5 involves pre-training on image-text pairs and performing a manual review of image-text pairs for quality assessment with our mixed instruction set. We directly take the pre-trained checkpoint and only reimplement the instruction tuning stage with our mixed instruction set."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Creating Optimal MLM Data Filters",
            "text": "We propose various different design choices for constructing instruction data for data quality scoring tasks in Section 3.2  ###reference_###. These design choices may make a significant difference in the effectiveness of instruction tuning. To create the optimal fine-tuned MLM data filter, we conduct comprehensive ablation studies to investigate the effects of different design choices on the filtering performance. Four major design choices for constructing the instruction data for scoring tasks are investigated: 1) we experiment with two captioning models to transform image into text-base detailed description for prompting GPT-4, including LLaVA and ShareGPT4V [4  ###reference_b4###]; 2) we experiment with two different image-text datasets for constructing visual instructions, including CC12M and DataComp Medium 128M; 3) we experiment with two different numbers of grouping buckets, 10 and 100, for sampling the final 4k instructions; 4) we experiment with different teacher models to get multimodal instructions, including GPT-4 and GPT-4 Vision. Additionally, we use the DataComp benchmark to evaluate the effectiveness of different data filtering hyperparameters.\nDataComp Benchmark. The DataComp benchmark [11  ###reference_b11###] has been introduced to systematically compare the performance of different data filtering methods. In this benchmark, the training code and computational budget is fixed across all competing methods to facilitate direct comparison between methods. The DataComp provides a fixed original image-text data pool for different filtering methods to ensure a fair comparison. The performance is measured by training a CLIP model on the filtered dataset and then testing the zero-shot capabilities of this CLIP model on a suite of 38 classification and retrieval tasks. We select the Medium scale training setting to train ViT-B/32 CLIP models on datasets resulting from various MLM data filter configurations.\nTo investigate the effects of each design choice, we keep the selection of the other three design choices the same and only change one design choice for each experiment group. As we propose four different metrics to assess data quality, we only adopt the metric of Object Detail Fulfillment as the filtering metric to select a high-quality subset from the 128M medium scale data pool. The ablation results for all four design choices are presented in Table 1  ###reference_###  ###reference_###.\nThe first two lines in Table 1  ###reference_###  ###reference_### demonstrate that adopting LLaVA as the captioning model to transform images into detailed descriptions for instruction data construction leads to better filtering performance. Next, adopting CC12M to sample image-text pairs for data construction outperforms the design choice of using DataComp-Medium dataset. We suppose it is because the image quality of CC12M is significantly better than that of DataComp, enabling the instruction tuning process more knowledge intensive. Thirdly, grouping the initial instructions into 10 buckets for sampling illustrates priority over using 100 buckets. In terms of the selection of teacher models, the MLM filters learned from different teacher models exhibit distinct strengths across different tasks. The MLM filter learned from GPT-4 performs better in VTAB [53  ###reference_b53###  ###reference_b53###] classification and retrieval datasets, while the MLM filter learned from GPT-4V obtains higher scores in ImageNet [8  ###reference_b8###  ###reference_b8###] related datasets. Finally, we decide to fix the other three choices as LLaVA captioner, CC12M data resources, and 10 sampling buckets. We report the two versions of MLM-based filters with different teacher models GPT4 and GPT-4V for future experiments, denoted as MLM-Filter-GPT4 and MLM-Filter-GPT4V respectively."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this section, we evaluate the effectiveness of adopting fine-tuned MLMs as high-quality image-text data filters. We compare the performance of vision-language models pre-trained on datasets filtered using a baseline filter with their performance using our MLM filter. We select two different VLM architectures for comprehensive evaluation: CLIP pre-training and BLIP-2 pre-training. Additionally, we conduct human evaluation to compute the correlation between the scoring generated by our proposed MLM filter model and the baseline CLIP model."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "CLIP Pre-Training on DataComp Medium and Large Scales",
            "text": "Evaluation Setup. We select the DataComp benchmark to evaluate the effectiveness of adopting fine-tuned MLM as data filter. The evaluation process involves the data filtering stage and evaluation stage, which are shown in Figure 2  ###reference_###. During the data filtering stage, we adopt the MLM-Filter to generate quality scores on all 128M medium-scale data and 1.28B large-scale data. After that, an integer filtering threshold is calculated based on the closest value that retains 30% of the overall data pool, 38.4M for Medium and 384M for Large. Such threshold is set up to select all the image-text pairs, of which the quality score is larger or equal to the threshold.\nWe report the results using each defined metric to filter data separately and we consider two MLM filters learning from different teacher models. Additionally, we also report the results of experiments with a combination of two metrics for data filtering.\nFinally, we select a high-quality subset from the medium or large scale image-text data pools based on different proposed quality metrics. During the evaluation stage, we adopt the selected high-quality data subset to pre-train a CLIP model and compare the performance of our CLIP model with CLIP models pre-trained on datasets filtered by other methods.\nBaselines. We compare the proposed MLM filter with other baseline filtering methods from DataComp, including applying no filtering, basic filtering, LAION filtering and CLIPScore filtering. The basic filtering method adopts three rule-based filters, filtering English only, filtering by caption length, and filtering by image size. The LAION filtering adopts both the CLIPScore filtering using ViT-B/32 CLIP model and the English filtering. The CLIPScore filtering utilizes a larger ViT-L/14 CLIP model for score generation and data filtering.\nTraining Details.\nWe strictly follow the training setup provided by DataComp. The computational budget and hyperparameters are fixed for pre-training CLIP using different filters. The CLIP model architecture is determined by the data scale, in which the ViT-B/32 model is pre-trained on the medium scale setting and ViT-B/16 model is on the large scale setting. We use  Nvidia A100 GPUs to train our models.\nResults on DataComp Medium and Large Scale. The DataComp results between the proposed MLM filter and other baselines are presented in Table 2  ###reference_### and Table 3  ###reference_### for Medium and Large scale respectively. On the medium-scale DataComp benchmark, the proposed MLM Filter significantly outperforms the CLIPScore baseline on different task subgroups, achieving notable improvements of +3.2 accuracy on ImageNet-1k, +2.6 average accuracy on 6 ImageNet shifted datasets, +2.3 average accuracy on 13 VTAB datasets, and +4.9 average scores on 3 retrieval datasets. Moreover, the proposed MLM Filter surpasses CLIPScore baseline by +1.7 and +1.3 improvements on the average scores over 38 datasets on DataComp Medium and Large Scale benchmarks, which demonstrates the proposed MLM Filter can work as more effective filtering method than CLIPScore filter. Additionally, we can draw the following auxiliary conclusions from the results:\nThe MLM Filter learned from GPT-4V performs better on ImageNet related datasets than the MLM Filter learned from GPT-4. The MLM-Filter-GPT4V achieves the best performance on both ImageNet-1k and 6 ImageNet Shifted datasets. Both filtering metrics of Image Text Matching and Object Detail Fulfillment generated by MLM-Filter-GPT4V outperforms the best ImageNet-1k accuracy of MLM-Filter-GPT4, achieving a notable improvement of +1.1 accuracy.\nThe optimal filtering metric varies for fine-tuned MLM Filter learned from different teacher models. For the proposed MLM Filter learned from different teacher models, the optimal filtering metric under single metric filtering setting is different. The Image-Text Matching is the optimal filtering metric for MLM-Filter-GPT4V, while the Object Detail Fulfillment metric helps the MLM-Filter-GPT4 most. The other two metrics of Caption Text Quality and Semantic Understanding cannot work as effective filtering quality metrics in DataComp benchmark, leading to worse performance than CLIPScore baseline. We regard that it is because the most of DataComp evaluation datasets are image classification datasets, which did not aligh with the filtering directions and objectives of CTQ and SU metrics.\nImage-Text Matching is the best filtering metric for retrieval tasks. Our proposed MLM Filter achieves the SOTA performance on the three image-to-text and text-to-image datasets under DataComp Medium setting. The two types of MLM Filters achieves 30.0 and 29.7 average performance on three retrieval tasks using the ITM filtering metric, surpassing the CLIPScore baseline by 4.9 average scores. We also observe in results of both MLM Filter variants that the image-text matching metric leads to better performance on retrieval tasks compared with other three filtering metrics.\nCombing different quality metrics effectively filters and identifies image-text pairs of better quality.  The AND operation to combine ITM and ODF quality metrics means that the ITM and ODF score of selected datapoints should exceed the filtering thresholds of both metrics, while the OR operation to combine two metrics means that the selected datapoints should either exceed the threshold for ITM metric or that for ODF metric. The combination of ITM and ODF metrics using AND operation outperforms all the baseline filtering methods and other variants of MLM Filters, achieving the best average performance of 34.5 over 38 datasets.\nITM\n8.2\n10.3\n9.2\nODF\n14.6\n19.3\n16.9\nITM\n15.4\n8.3\n11.8\nODF\n9.0\n6.8\n7.9\nAND\n12.9\n11.6\n12.3\nThe worse performance on digit classification tasks prevents MLM-Filter-GPT4V from remarkably outperforming MLM-Filter-GPT4. Even if MLM-Filter-GPT4V outperforms MLM-Filter-GPT4 on 23 ImageNet, VTAB and retrieval datasets, it only achieves the same average performance over 38 datasets as MLM-Filter-GPT4. It is because the performance of MLM-Filter-GPT4V on the two digit classification datasets significantly lags behind MLM-Filter-GPT4 by 5.1 average score, shown in Table 4  ###reference_###, which leads to 0.27 average score behind on 38 datasets. The combination of two quality metrics promotes the digit classification performance of MLM-Filter-GPT4V, but does not resolve it."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "BLIP2 Pre-Training",
            "text": "To demonstrate the effectiveness of our proposed MLM Filter across various VLM model architectures, we pre-train BLIP-2 VLM on the filtered dataset and evaluate the zero-shot performance of such BLIP-2 model on VQA datasets to compare the effectiveness of filtering methods on high-level vision-language tasks.\nTraining setup.  We directly use the filtered dataset from DataComp Large 1.28B data pool using CLIPScore filtering and our proposed MLM Filtering. The batch size and number of pre-training steps are kept as the same as original implementation [18  ###reference_b18###] for both the CLIPScore filtered dataset and MLM filtered dataset, in which both BLIP-2 models are iterated on 420M images for pre-training stage 1 and 154M images for stage 2. We use the same hyperparameters and number of GPUs for training. The visual encoder and LLM we used for BLIP-2 architecture are Eva-CLIP ViT-g/14 [33  ###reference_b33###] and Vicuna-7b [5  ###reference_b5###] respectively. More training details are available in Appendix E  ###reference_### Table 9  ###reference_###.\nResults. Two BLIP-2 models pre-trained on different filtered datasets are evaluated on VQAv2 [12  ###reference_b12###] and GQA [15  ###reference_b15###] datasets in zero-shot manner and the results of zero-shot VQA performance are shown in Table 5  ###reference_###. The BLIP-2 pre-trained with MLM-Filter-GPT4 filtered image-text data achieves +1.7 and + 1.4 improvements on VQAv2 and GQA datasets than the BLIP-2 pre-trained on CLIPSCore filtered dataset.\n55.1\n56.8"
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Correlation with Human Scoring",
            "text": "We follow [52  ###reference_b52###] to compute the correlation between human scoring and model scoring to evaluate the alignment between human and the filtering model. A set of 100 image-text pairs are sampled from CC12M and MSCOCO [20  ###reference_b20###] and labeled with human scores in terms of the image-text matching. CLIPScore and fine-tuned MLM filters are used to generate the image-text matching scores for the selected image-text pairs. Then, the Pearson and Spearman scores are reported between the human scores and model scores, as presented in Table 6  ###reference_###. Our proposed MLM-Filter scores are significantly aligned and correlated with human quality scores, while CLIPScore does not demonstrate such correlations. The two quality metrics Image-Text Matching and Object Detail Fulfillment all demonstrate significant correlations in similar levels.\n0.164\n0.410\n0.328\n0.368"
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Analysis",
            "text": "Effects of filtering fraction. We perform an ablation study to investigate the effects of the fraction of samples selected for pre-training CLIP on DataComp Medium benchmark performance. We select five fractions  of the total 128M images of DataComp medium pool. The results are presented in Table 4  ###reference_###. The top-30% of images selected for CLIP training achieve the best performance, which is also observed in [11  ###reference_b11###]. Even adding 5% poison data leads to a huge performance drop on both ImageNet and average over 38 datasets.\n###figure_5### Efficiency of MLM Filters.\nThe MLM Filter used for quality score generation is LLaVA-1.5 with 14B model parameters , while CLIPScore adopts a CLIP ViT-L/14 model with 492M parameter in total. Even if the model size of the proposed MLM Filter is much larger than that of CLIPScore, due to the computation redundancy of the CLIP\u2019s dual-encoder architecture, the timecost for generating scores for 10k image-text pairs is average 24.3 mins for MLM Filter versus 11.2 mins for CLIPScore-ViT/L using one A100 GPU. Additionally, with the help of the latest techniques in language model inference acceleration, the TensorRT-LLM toolkit111https://github.com/NVIDIA/TensorRT-LLM, we accelerate the score generation of our MLM Filter 4 times over, resulting in 6.1 mins in average for 10k samples. Thus, the proposed MLM Filter can achieve much better efficiency than CLIPScore."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We propose to instruction-tune Multimodal Language Model on quality scoring tasks and further leverage these fine-tuned MLM as effective data filters to select high-quality image-text pairs from large-scale web-crawled dataset. We find that, on CLIP and BLIP-2 models, pre-training on datasets filtered by our proposed MLM Filter significantly outperforms pre-training on CLIPScore-filtered datasets, demonstrating the superiority of our proposed MLM Filter over CLIPScore filtering."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A Prompt Construction",
            "text": "After manually writing the first version of prompts, we leverage the GPT-4 to refine the human-written prompts. The final prompts for four quality scoring tasks are shown below:"
        },
        {
            "section_id": "Appendix 2",
            "parent_section_id": null,
            "section_name": "Appendix B Examples for Two Prompting Strategies",
            "text": "Example for Chain-of-Thought Reasoning\nExample for Rationalization\nPlease evaluate if the provided text caption accurately represents the main features and objects of the image. The caption doesn\u2019t need to detail every aspect of the image, but it should capture its primary theme. Rate the overall quality of the text caption\u2019s match to the image on a scale of 1-100, considering the criteria mentioned.\nPlease evaluate if the provided text caption accurately represents the main features and objects of the image. The caption doesn\u2019t need to detail every aspect of the image, but it should capture its primary theme. Rate the overall quality of the text caption\u2019s match to the image on a scale of 1-100, considering the criteria mentioned.\nPlease think step by step to first output your reasons to give such a score. In the subsequent line, please output a single line containing the value indicating the scores.\nPlease first output a single line containing the value indicating the scores. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias."
        },
        {
            "section_id": "Appendix 3",
            "parent_section_id": null,
            "section_name": "Appendix C Sampling Final Instructions for Scoring Tasks",
            "text": ""
        },
        {
            "section_id": "Appendix 4",
            "parent_section_id": null,
            "section_name": "Appendix D Data Mixture of Multi-Task Multimodal Instructions",
            "text": ""
        },
        {
            "section_id": "Appendix 5",
            "parent_section_id": null,
            "section_name": "Appendix E BLIP-2 Training Details",
            "text": "The detailed training hypperparamters and settings of BLIP-2 stage 1 and stage 2 are presented in Table 9  ###reference_###."
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T1\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S3.T1.1\" style=\"width:330.1pt;height:153.9pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-33.8pt,15.8pt) scale(0.83,0.83) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S3.T1.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt ltx_border_t\" id=\"S3.T1.1.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.1.1.1\" style=\"font-size:90%;\">Captioner</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt ltx_border_t\" id=\"S3.T1.1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.1.2.1\" style=\"font-size:90%;\">Data Resource</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt ltx_border_t\" id=\"S3.T1.1.1.1.1.3\"><span class=\"ltx_text\" id=\"S3.T1.1.1.1.1.3.1\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.1.1.1.1.3.1.1\">\n<span class=\"ltx_tr\" id=\"S3.T1.1.1.1.1.3.1.1.1\">\n<span class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.1.1.1.3.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.1.3.1.1.1.1.1\">#Sampling</span></span></span>\n<span class=\"ltx_tr\" id=\"S3.T1.1.1.1.1.3.1.1.2\">\n<span class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.1.1.1.3.1.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.1.3.1.1.2.1.1\">Buckets</span></span></span>\n</span></span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt ltx_border_t\" id=\"S3.T1.1.1.1.1.4\"><span class=\"ltx_text\" id=\"S3.T1.1.1.1.1.4.1\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.1.1.1.1.4.1.1\">\n<span class=\"ltx_tr\" id=\"S3.T1.1.1.1.1.4.1.1.1\">\n<span class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.1.1.1.4.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.1.4.1.1.1.1.1\">Teacher</span></span></span>\n<span class=\"ltx_tr\" id=\"S3.T1.1.1.1.1.4.1.1.2\">\n<span class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.1.1.1.4.1.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.1.4.1.1.2.1.1\">Model</span></span></span>\n</span></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t\" id=\"S3.T1.1.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.1.5.1\" style=\"font-size:90%;\">ImageNet-1k</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t\" id=\"S3.T1.1.1.1.1.6\"><span class=\"ltx_text\" id=\"S3.T1.1.1.1.1.6.1\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.1.1.1.1.6.1.1\">\n<span class=\"ltx_tr\" id=\"S3.T1.1.1.1.1.6.1.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.1.1.6.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.1.6.1.1.1.1.1\">ImageNet</span></span></span>\n<span class=\"ltx_tr\" id=\"S3.T1.1.1.1.1.6.1.1.2\">\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.1.1.6.1.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.1.6.1.1.2.1.1\">dist. shifts</span></span></span>\n</span></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t\" id=\"S3.T1.1.1.1.1.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.1.7.1\" style=\"font-size:90%;\">VTAB</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t\" id=\"S3.T1.1.1.1.1.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.1.8.1\" style=\"font-size:90%;\">Retrieval</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t\" id=\"S3.T1.1.1.1.1.9\"><span class=\"ltx_text\" id=\"S3.T1.1.1.1.1.9.1\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.1.1.1.1.9.1.1\">\n<span class=\"ltx_tr\" id=\"S3.T1.1.1.1.1.9.1.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.1.1.9.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.1.9.1.1.1.1.1\">Average over</span></span></span>\n<span class=\"ltx_tr\" id=\"S3.T1.1.1.1.1.9.1.1.2\">\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.1.1.9.1.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.1.9.1.1.2.1.1\">38 datasets</span></span></span>\n</span></span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.1.1.2.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.2.1.1.1\" style=\"font-size:90%;\">LLaVA</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.1.1.2.1.2\"><span class=\"ltx_text\" id=\"S3.T1.1.1.2.1.2.1\" style=\"font-size:90%;\">CC12M</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.1.1.2.1.3\"><span class=\"ltx_text\" id=\"S3.T1.1.1.2.1.3.1\" style=\"font-size:90%;\">10</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.1.1.2.1.4\"><span class=\"ltx_text\" id=\"S3.T1.1.1.2.1.4.1\" style=\"font-size:90%;\">GPT-4</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.2.1.5\"><span class=\"ltx_text ltx_framed_underline\" id=\"S3.T1.1.1.2.1.5.1\" style=\"font-size:90%;\">29.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.2.1.6\"><span class=\"ltx_text\" id=\"S3.T1.1.1.2.1.6.1\" style=\"font-size:90%;\">24.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.2.1.7\"><span class=\"ltx_text\" id=\"S3.T1.1.1.2.1.7.1\" style=\"font-size:90%;\">35.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.2.1.8\"><span class=\"ltx_text ltx_framed_underline\" id=\"S3.T1.1.1.2.1.8.1\" style=\"font-size:90%;\">29.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.2.1.9\"><span class=\"ltx_text ltx_framed_underline\" id=\"S3.T1.1.1.2.1.9.1\" style=\"font-size:90%;\">34.2</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T1.1.1.3.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.3.2.1.1\" style=\"font-size:90%;\">ShareGPT4V</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T1.1.1.3.2.2\"><span class=\"ltx_text\" id=\"S3.T1.1.1.3.2.2.1\" style=\"font-size:90%;\">CC12M</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T1.1.1.3.2.3\"><span class=\"ltx_text\" id=\"S3.T1.1.1.3.2.3.1\" style=\"font-size:90%;\">10</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T1.1.1.3.2.4\"><span class=\"ltx_text\" id=\"S3.T1.1.1.3.2.4.1\" style=\"font-size:90%;\">GPT-4</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.3.2.5\"><span class=\"ltx_text\" id=\"S3.T1.1.1.3.2.5.1\" style=\"font-size:90%;\">28.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.3.2.6\"><span class=\"ltx_text ltx_framed_underline\" id=\"S3.T1.1.1.3.2.6.1\" style=\"font-size:90%;\">24.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.3.2.7\"><span class=\"ltx_text ltx_framed_underline\" id=\"S3.T1.1.1.3.2.7.1\" style=\"font-size:90%;\">35.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.3.2.8\"><span class=\"ltx_text\" id=\"S3.T1.1.1.3.2.8.1\" style=\"font-size:90%;\">28.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.3.2.9\"><span class=\"ltx_text\" id=\"S3.T1.1.1.3.2.9.1\" style=\"font-size:90%;\">33.7</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.1.1.4.3.1\"><span class=\"ltx_text\" id=\"S3.T1.1.1.4.3.1.1\" style=\"font-size:90%;\">N/A</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.1.1.4.3.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.4.3.2.1\" style=\"font-size:90%;\">DataComp</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.1.1.4.3.3\"><span class=\"ltx_text\" id=\"S3.T1.1.1.4.3.3.1\" style=\"font-size:90%;\">10</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.1.1.4.3.4\"><span class=\"ltx_text\" id=\"S3.T1.1.1.4.3.4.1\" style=\"font-size:90%;\">GPT-4V</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.4.3.5\"><span class=\"ltx_text\" id=\"S3.T1.1.1.4.3.5.1\" style=\"font-size:90%;\">29.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.4.3.6\"><span class=\"ltx_text\" id=\"S3.T1.1.1.4.3.6.1\" style=\"font-size:90%;\">24.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.4.3.7\"><span class=\"ltx_text ltx_framed_underline\" id=\"S3.T1.1.1.4.3.7.1\" style=\"font-size:90%;\">34.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.4.3.8\"><span class=\"ltx_text\" id=\"S3.T1.1.1.4.3.8.1\" style=\"font-size:90%;\">26.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.4.3.9\"><span class=\"ltx_text\" id=\"S3.T1.1.1.4.3.9.1\" style=\"font-size:90%;\">33.2</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T1.1.1.5.4.1\"><span class=\"ltx_text\" id=\"S3.T1.1.1.5.4.1.1\" style=\"font-size:90%;\">N/A</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T1.1.1.5.4.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.5.4.2.1\" style=\"font-size:90%;\">CC12M</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T1.1.1.5.4.3\"><span class=\"ltx_text\" id=\"S3.T1.1.1.5.4.3.1\" style=\"font-size:90%;\">10</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T1.1.1.5.4.4\"><span class=\"ltx_text\" id=\"S3.T1.1.1.5.4.4.1\" style=\"font-size:90%;\">GPT-4V</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.5.4.5\"><span class=\"ltx_text ltx_framed_underline\" id=\"S3.T1.1.1.5.4.5.1\" style=\"font-size:90%;\">30.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.5.4.6\"><span class=\"ltx_text ltx_framed_underline\" id=\"S3.T1.1.1.5.4.6.1\" style=\"font-size:90%;\">25.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.5.4.7\"><span class=\"ltx_text\" id=\"S3.T1.1.1.5.4.7.1\" style=\"font-size:90%;\">33.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.5.4.8\"><span class=\"ltx_text ltx_framed_underline\" id=\"S3.T1.1.1.5.4.8.1\" style=\"font-size:90%;\">28.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.5.4.9\"><span class=\"ltx_text ltx_framed_underline\" id=\"S3.T1.1.1.5.4.9.1\" style=\"font-size:90%;\">33.7</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.6.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.1.1.6.5.1\"><span class=\"ltx_text\" id=\"S3.T1.1.1.6.5.1.1\" style=\"font-size:90%;\">ShareGPT4V</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.1.1.6.5.2\"><span class=\"ltx_text\" id=\"S3.T1.1.1.6.5.2.1\" style=\"font-size:90%;\">CC12M</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.1.1.6.5.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.6.5.3.1\" style=\"font-size:90%;\">10</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.1.1.6.5.4\"><span class=\"ltx_text\" id=\"S3.T1.1.1.6.5.4.1\" style=\"font-size:90%;\">GPT-4</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.6.5.5\"><span class=\"ltx_text ltx_framed_underline\" id=\"S3.T1.1.1.6.5.5.1\" style=\"font-size:90%;\">28.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.6.5.6\"><span class=\"ltx_text ltx_framed_underline\" id=\"S3.T1.1.1.6.5.6.1\" style=\"font-size:90%;\">24.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.6.5.7\"><span class=\"ltx_text ltx_framed_underline\" id=\"S3.T1.1.1.6.5.7.1\" style=\"font-size:90%;\">35.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.6.5.8\"><span class=\"ltx_text\" id=\"S3.T1.1.1.6.5.8.1\" style=\"font-size:90%;\">28.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.6.5.9\"><span class=\"ltx_text ltx_framed_underline\" id=\"S3.T1.1.1.6.5.9.1\" style=\"font-size:90%;\">33.7</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.7.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T1.1.1.7.6.1\"><span class=\"ltx_text\" id=\"S3.T1.1.1.7.6.1.1\" style=\"font-size:90%;\">ShareGPT4V</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T1.1.1.7.6.2\"><span class=\"ltx_text\" id=\"S3.T1.1.1.7.6.2.1\" style=\"font-size:90%;\">CC12M</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T1.1.1.7.6.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.7.6.3.1\" style=\"font-size:90%;\">100</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T1.1.1.7.6.4\"><span class=\"ltx_text\" id=\"S3.T1.1.1.7.6.4.1\" style=\"font-size:90%;\">GPT-4</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.7.6.5\"><span class=\"ltx_text\" id=\"S3.T1.1.1.7.6.5.1\" style=\"font-size:90%;\">27.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.7.6.6\"><span class=\"ltx_text\" id=\"S3.T1.1.1.7.6.6.1\" style=\"font-size:90%;\">23.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.7.6.7\"><span class=\"ltx_text\" id=\"S3.T1.1.1.7.6.7.1\" style=\"font-size:90%;\">34.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.7.6.8\"><span class=\"ltx_text ltx_framed_underline\" id=\"S3.T1.1.1.7.6.8.1\" style=\"font-size:90%;\">28.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.7.6.9\"><span class=\"ltx_text\" id=\"S3.T1.1.1.7.6.9.1\" style=\"font-size:90%;\">33.2</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.8.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.1.1.8.7.1\"><span class=\"ltx_text\" id=\"S3.T1.1.1.8.7.1.1\" style=\"font-size:90%;\">LLaVA</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.1.1.8.7.2\"><span class=\"ltx_text\" id=\"S3.T1.1.1.8.7.2.1\" style=\"font-size:90%;\">CC12M</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.1.1.8.7.3\"><span class=\"ltx_text\" id=\"S3.T1.1.1.8.7.3.1\" style=\"font-size:90%;\">10</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.1.1.8.7.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.8.7.4.1\" style=\"font-size:90%;\">GPT-4</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.8.7.5\"><span class=\"ltx_text\" id=\"S3.T1.1.1.8.7.5.1\" style=\"font-size:90%;\">29.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.8.7.6\"><span class=\"ltx_text\" id=\"S3.T1.1.1.8.7.6.1\" style=\"font-size:90%;\">24.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.8.7.7\"><span class=\"ltx_text ltx_framed_underline\" id=\"S3.T1.1.1.8.7.7.1\" style=\"font-size:90%;\">35.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.8.7.8\"><span class=\"ltx_text ltx_framed_underline\" id=\"S3.T1.1.1.8.7.8.1\" style=\"font-size:90%;\">29.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.8.7.9\"><span class=\"ltx_text ltx_framed_underline\" id=\"S3.T1.1.1.8.7.9.1\" style=\"font-size:90%;\">34.2</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.9.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_b\" id=\"S3.T1.1.1.9.8.1\"><span class=\"ltx_text\" id=\"S3.T1.1.1.9.8.1.1\" style=\"font-size:90%;\">N/A</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_b\" id=\"S3.T1.1.1.9.8.2\"><span class=\"ltx_text\" id=\"S3.T1.1.1.9.8.2.1\" style=\"font-size:90%;\">CC12M</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_b\" id=\"S3.T1.1.1.9.8.3\"><span class=\"ltx_text\" id=\"S3.T1.1.1.9.8.3.1\" style=\"font-size:90%;\">10</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_b\" id=\"S3.T1.1.1.9.8.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.9.8.4.1\" style=\"font-size:90%;\">GPT-4V</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b\" id=\"S3.T1.1.1.9.8.5\"><span class=\"ltx_text ltx_framed_underline\" id=\"S3.T1.1.1.9.8.5.1\" style=\"font-size:90%;\">30.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b\" id=\"S3.T1.1.1.9.8.6\"><span class=\"ltx_text ltx_framed_underline\" id=\"S3.T1.1.1.9.8.6.1\" style=\"font-size:90%;\">25.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b\" id=\"S3.T1.1.1.9.8.7\"><span class=\"ltx_text\" id=\"S3.T1.1.1.9.8.7.1\" style=\"font-size:90%;\">33.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b\" id=\"S3.T1.1.1.9.8.8\"><span class=\"ltx_text\" id=\"S3.T1.1.1.9.8.8.1\" style=\"font-size:90%;\">28.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b\" id=\"S3.T1.1.1.9.8.9\"><span class=\"ltx_text\" id=\"S3.T1.1.1.9.8.9.1\" style=\"font-size:90%;\">33.7</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Ablations on different design choices for constructing multimodal instruction data for quality scoring tasks.</figcaption>\n</figure>",
            "capture": "Table 1: Ablations on different design choices for constructing multimodal instruction data for quality scoring tasks."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T2\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S3.T2.1\" style=\"width:347.4pt;height:249.4pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-30.7pt,22.0pt) scale(0.85,0.85) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S3.T2.1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt ltx_border_t\" id=\"S3.T2.1.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.1.1.1.1\" style=\"font-size:90%;\">Filter</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt ltx_border_t\" id=\"S3.T2.1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.1.1.2.1\" style=\"font-size:90%;\">Metrics</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt ltx_border_t\" id=\"S3.T2.1.1.1.1.3\"><span class=\"ltx_text\" id=\"S3.T2.1.1.1.1.3.1\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S3.T2.1.1.1.1.3.1.1\">\n<span class=\"ltx_tr\" id=\"S3.T2.1.1.1.1.3.1.1.1\">\n<span class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.1.1.1.3.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.1.1.3.1.1.1.1.1\">Teacher</span></span></span>\n<span class=\"ltx_tr\" id=\"S3.T2.1.1.1.1.3.1.1.2\">\n<span class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.1.1.1.3.1.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.1.1.3.1.1.2.1.1\">Model</span></span></span>\n</span></span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt ltx_border_t\" id=\"S3.T2.1.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.1.1.4.1\" style=\"font-size:90%;\">ImageNet-1k</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt ltx_border_t\" id=\"S3.T2.1.1.1.1.5\"><span class=\"ltx_text\" id=\"S3.T2.1.1.1.1.5.1\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S3.T2.1.1.1.1.5.1.1\">\n<span class=\"ltx_tr\" id=\"S3.T2.1.1.1.1.5.1.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.1.1.5.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.1.1.5.1.1.1.1.1\">ImageNet</span></span></span>\n<span class=\"ltx_tr\" id=\"S3.T2.1.1.1.1.5.1.1.2\">\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.1.1.5.1.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.1.1.5.1.1.2.1.1\">dist. shifts</span></span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt ltx_border_t\" id=\"S3.T2.1.1.1.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.1.1.6.1\" style=\"font-size:90%;\">VTAB</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt ltx_border_t\" id=\"S3.T2.1.1.1.1.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.1.1.7.1\" style=\"font-size:90%;\">Retrieval</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt ltx_border_t\" id=\"S3.T2.1.1.1.1.8\"><span class=\"ltx_text\" id=\"S3.T2.1.1.1.1.8.1\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S3.T2.1.1.1.1.8.1.1\">\n<span class=\"ltx_tr\" id=\"S3.T2.1.1.1.1.8.1.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.1.1.8.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.1.1.8.1.1.1.1.1\">Average over</span></span></span>\n<span class=\"ltx_tr\" id=\"S3.T2.1.1.1.1.8.1.1.2\">\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.1.1.8.1.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.1.1.8.1.1.2.1.1\">38 datasets</span></span></span>\n</span></span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T2.1.1.2.2.1\"><span class=\"ltx_text\" id=\"S3.T2.1.1.2.2.1.1\" style=\"font-size:90%;\">No Filtering</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T2.1.1.2.2.2\"><span class=\"ltx_text\" id=\"S3.T2.1.1.2.2.2.1\" style=\"font-size:90%;\">-</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T2.1.1.2.2.3\"><span class=\"ltx_text\" id=\"S3.T2.1.1.2.2.3.1\" style=\"font-size:90%;\">-</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.1.2.2.4\"><span class=\"ltx_text\" id=\"S3.T2.1.1.2.2.4.1\" style=\"font-size:90%;\">17.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.1.2.2.5\"><span class=\"ltx_text\" id=\"S3.T2.1.1.2.2.5.1\" style=\"font-size:90%;\">15.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.1.2.2.6\"><span class=\"ltx_text\" id=\"S3.T2.1.1.2.2.6.1\" style=\"font-size:90%;\">25.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.1.2.2.7\"><span class=\"ltx_text\" id=\"S3.T2.1.1.2.2.7.1\" style=\"font-size:90%;\">21.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.1.2.2.8\"><span class=\"ltx_text\" id=\"S3.T2.1.1.2.2.8.1\" style=\"font-size:90%;\">25.8</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.1.1.3.3.1\"><span class=\"ltx_text\" id=\"S3.T2.1.1.3.3.1.1\" style=\"font-size:90%;\">Basic Filtering</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.1.1.3.3.2\"><span class=\"ltx_text\" id=\"S3.T2.1.1.3.3.2.1\" style=\"font-size:90%;\">Rules</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.1.1.3.3.3\"><span class=\"ltx_text\" id=\"S3.T2.1.1.3.3.3.1\" style=\"font-size:90%;\">-</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.3.3.4\"><span class=\"ltx_text\" id=\"S3.T2.1.1.3.3.4.1\" style=\"font-size:90%;\">22.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.3.3.5\"><span class=\"ltx_text\" id=\"S3.T2.1.1.3.3.5.1\" style=\"font-size:90%;\">19.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.3.3.6\"><span class=\"ltx_text\" id=\"S3.T2.1.1.3.3.6.1\" style=\"font-size:90%;\">28.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.3.3.7\"><span class=\"ltx_text\" id=\"S3.T2.1.1.3.3.7.1\" style=\"font-size:90%;\">25.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.3.3.8\"><span class=\"ltx_text\" id=\"S3.T2.1.1.3.3.8.1\" style=\"font-size:90%;\">28.5</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.1.1.4.4.1\"><span class=\"ltx_text\" id=\"S3.T2.1.1.4.4.1.1\" style=\"font-size:90%;\">LAION Filtering</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.1.1.4.4.2\"><span class=\"ltx_text\" id=\"S3.T2.1.1.4.4.2.1\" style=\"font-size:90%;\">CLIPScore+Rules</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.1.1.4.4.3\"><span class=\"ltx_text\" id=\"S3.T2.1.1.4.4.3.1\" style=\"font-size:90%;\">-</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.4.4.4\"><span class=\"ltx_text\" id=\"S3.T2.1.1.4.4.4.1\" style=\"font-size:90%;\">23.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.4.4.5\"><span class=\"ltx_text\" id=\"S3.T2.1.1.4.4.5.1\" style=\"font-size:90%;\">19.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.4.4.6\"><span class=\"ltx_text\" id=\"S3.T2.1.1.4.4.6.1\" style=\"font-size:90%;\">30.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.4.4.7\"><span class=\"ltx_text\" id=\"S3.T2.1.1.4.4.7.1\" style=\"font-size:90%;\">23.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.4.4.8\"><span class=\"ltx_text\" id=\"S3.T2.1.1.4.4.8.1\" style=\"font-size:90%;\">29.2</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.1.1.5.5.1\"><span class=\"ltx_text\" id=\"S3.T2.1.1.5.5.1.1\" style=\"font-size:90%;\">CLIPScore</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.1.1.5.5.2\"><span class=\"ltx_text\" id=\"S3.T2.1.1.5.5.2.1\" style=\"font-size:90%;\">CLIPScore</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.1.1.5.5.3\"><span class=\"ltx_text\" id=\"S3.T2.1.1.5.5.3.1\" style=\"font-size:90%;\">-</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.5.5.4\"><span class=\"ltx_text\" id=\"S3.T2.1.1.5.5.4.1\" style=\"font-size:90%;\">27.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.5.5.5\"><span class=\"ltx_text\" id=\"S3.T2.1.1.5.5.5.1\" style=\"font-size:90%;\">23.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.5.5.6\"><span class=\"ltx_text\" id=\"S3.T2.1.1.5.5.6.1\" style=\"font-size:90%;\">33.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.5.5.7\"><span class=\"ltx_text\" id=\"S3.T2.1.1.5.5.7.1\" style=\"font-size:90%;\">25.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.5.5.8\"><span class=\"ltx_text\" id=\"S3.T2.1.1.5.5.8.1\" style=\"font-size:90%;\">32.8</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.6.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T2.1.1.6.6.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T2.1.1.6.6.1.1\" style=\"font-size:90%;\">MLM-Filter</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T2.1.1.6.6.2\"><span class=\"ltx_text\" id=\"S3.T2.1.1.6.6.2.1\" style=\"font-size:90%;\">Image-Text Matching</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T2.1.1.6.6.3\"><span class=\"ltx_text\" id=\"S3.T2.1.1.6.6.3.1\" style=\"font-size:90%;\">GPT-4</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.1.6.6.4\"><span class=\"ltx_text\" id=\"S3.T2.1.1.6.6.4.1\" style=\"font-size:90%;\">28.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.1.6.6.5\"><span class=\"ltx_text\" id=\"S3.T2.1.1.6.6.5.1\" style=\"font-size:90%;\">23.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.1.6.6.6\"><span class=\"ltx_text\" id=\"S3.T2.1.1.6.6.6.1\" style=\"font-size:90%;\">34.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.1.6.6.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.6.6.7.1\" style=\"font-size:90%;\">30.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.1.6.6.8\"><span class=\"ltx_text\" id=\"S3.T2.1.1.6.6.8.1\" style=\"font-size:90%;\">33.4</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.7.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.1.1.7.7.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T2.1.1.7.7.1.1\" style=\"font-size:90%;\">MLM-Filter</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.1.1.7.7.2\"><span class=\"ltx_text\" id=\"S3.T2.1.1.7.7.2.1\" style=\"font-size:90%;\">Object Detail Fulfillment</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.1.1.7.7.3\"><span class=\"ltx_text\" id=\"S3.T2.1.1.7.7.3.1\" style=\"font-size:90%;\">GPT-4</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.7.7.4\"><span class=\"ltx_text\" id=\"S3.T2.1.1.7.7.4.1\" style=\"font-size:90%;\">29.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.7.7.5\"><span class=\"ltx_text\" id=\"S3.T2.1.1.7.7.5.1\" style=\"font-size:90%;\">24.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.7.7.6\"><span class=\"ltx_text\" id=\"S3.T2.1.1.7.7.6.1\" style=\"font-size:90%;\">35.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.7.7.7\"><span class=\"ltx_text\" id=\"S3.T2.1.1.7.7.7.1\" style=\"font-size:90%;\">29.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.7.7.8\"><span class=\"ltx_text ltx_framed_underline\" id=\"S3.T2.1.1.7.7.8.1\" style=\"font-size:90%;\">34.2</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.8.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.1.1.8.8.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T2.1.1.8.8.1.1\" style=\"font-size:90%;\">MLM-Filter</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.1.1.8.8.2\"><span class=\"ltx_text\" id=\"S3.T2.1.1.8.8.2.1\" style=\"font-size:90%;\">Caption Text Quality</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.1.1.8.8.3\"><span class=\"ltx_text\" id=\"S3.T2.1.1.8.8.3.1\" style=\"font-size:90%;\">GPT-4</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.8.8.4\"><span class=\"ltx_text\" id=\"S3.T2.1.1.8.8.4.1\" style=\"font-size:90%;\">25.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.8.8.5\"><span class=\"ltx_text\" id=\"S3.T2.1.1.8.8.5.1\" style=\"font-size:90%;\">20.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.8.8.6\"><span class=\"ltx_text\" id=\"S3.T2.1.1.8.8.6.1\" style=\"font-size:90%;\">32.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.8.8.7\"><span class=\"ltx_text\" id=\"S3.T2.1.1.8.8.7.1\" style=\"font-size:90%;\">26.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.8.8.8\"><span class=\"ltx_text\" id=\"S3.T2.1.1.8.8.8.1\" style=\"font-size:90%;\">30.9</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.9.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.1.1.9.9.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T2.1.1.9.9.1.1\" style=\"font-size:90%;\">MLM-Filter</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.1.1.9.9.2\"><span class=\"ltx_text\" id=\"S3.T2.1.1.9.9.2.1\" style=\"font-size:90%;\">Semantic Understanding</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.1.1.9.9.3\"><span class=\"ltx_text\" id=\"S3.T2.1.1.9.9.3.1\" style=\"font-size:90%;\">GPT-4</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.9.9.4\"><span class=\"ltx_text\" id=\"S3.T2.1.1.9.9.4.1\" style=\"font-size:90%;\">20.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.9.9.5\"><span class=\"ltx_text\" id=\"S3.T2.1.1.9.9.5.1\" style=\"font-size:90%;\">16.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.9.9.6\"><span class=\"ltx_text\" id=\"S3.T2.1.1.9.9.6.1\" style=\"font-size:90%;\">28.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.9.9.7\"><span class=\"ltx_text\" id=\"S3.T2.1.1.9.9.7.1\" style=\"font-size:90%;\">20.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.9.9.8\"><span class=\"ltx_text\" id=\"S3.T2.1.1.9.9.8.1\" style=\"font-size:90%;\">27.0</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.10.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T2.1.1.10.10.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T2.1.1.10.10.1.1\" style=\"font-size:90%;\">MLM-Filter</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T2.1.1.10.10.2\"><span class=\"ltx_text\" id=\"S3.T2.1.1.10.10.2.1\" style=\"font-size:90%;\">Image-Text Matching</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T2.1.1.10.10.3\"><span class=\"ltx_text\" id=\"S3.T2.1.1.10.10.3.1\" style=\"font-size:90%;\">GPT-4V</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.1.10.10.4\"><span class=\"ltx_text\" id=\"S3.T2.1.1.10.10.4.1\" style=\"font-size:90%;\">29.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.1.10.10.5\"><span class=\"ltx_text\" id=\"S3.T2.1.1.10.10.5.1\" style=\"font-size:90%;\">24.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.1.10.10.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.10.10.6.1\" style=\"font-size:90%;\">36.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.1.10.10.7\"><span class=\"ltx_text ltx_framed_underline\" id=\"S3.T2.1.1.10.10.7.1\" style=\"font-size:90%;\">29.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.1.10.10.8\"><span class=\"ltx_text ltx_framed_underline\" id=\"S3.T2.1.1.10.10.8.1\" style=\"font-size:90%;\">34.2</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.11.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.1.1.11.11.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T2.1.1.11.11.1.1\" style=\"font-size:90%;\">MLM-Filter</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.1.1.11.11.2\"><span class=\"ltx_text\" id=\"S3.T2.1.1.11.11.2.1\" style=\"font-size:90%;\">Object Detail Fulfillment</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.1.1.11.11.3\"><span class=\"ltx_text\" id=\"S3.T2.1.1.11.11.3.1\" style=\"font-size:90%;\">GPT-4V</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.11.11.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.11.11.4.1\" style=\"font-size:90%;\">30.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.11.11.5\"><span class=\"ltx_text ltx_framed_underline\" id=\"S3.T2.1.1.11.11.5.1\" style=\"font-size:90%;\">25.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.11.11.6\"><span class=\"ltx_text\" id=\"S3.T2.1.1.11.11.6.1\" style=\"font-size:90%;\">33.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.11.11.7\"><span class=\"ltx_text\" id=\"S3.T2.1.1.11.11.7.1\" style=\"font-size:90%;\">28.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.11.11.8\"><span class=\"ltx_text\" id=\"S3.T2.1.1.11.11.8.1\" style=\"font-size:90%;\">33.7</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.12.12\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.1.1.12.12.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T2.1.1.12.12.1.1\" style=\"font-size:90%;\">MLM-Filter</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.1.1.12.12.2\"><span class=\"ltx_text\" id=\"S3.T2.1.1.12.12.2.1\" style=\"font-size:90%;\">Caption Text Quality</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.1.1.12.12.3\"><span class=\"ltx_text\" id=\"S3.T2.1.1.12.12.3.1\" style=\"font-size:90%;\">GPT-4V</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.12.12.4\"><span class=\"ltx_text\" id=\"S3.T2.1.1.12.12.4.1\" style=\"font-size:90%;\">24.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.12.12.5\"><span class=\"ltx_text\" id=\"S3.T2.1.1.12.12.5.1\" style=\"font-size:90%;\">20.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.12.12.6\"><span class=\"ltx_text\" id=\"S3.T2.1.1.12.12.6.1\" style=\"font-size:90%;\">32.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.12.12.7\"><span class=\"ltx_text\" id=\"S3.T2.1.1.12.12.7.1\" style=\"font-size:90%;\">24.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.12.12.8\"><span class=\"ltx_text\" id=\"S3.T2.1.1.12.12.8.1\" style=\"font-size:90%;\">30.9</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.13.13\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.1.1.13.13.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T2.1.1.13.13.1.1\" style=\"font-size:90%;\">MLM-Filter</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.1.1.13.13.2\"><span class=\"ltx_text\" id=\"S3.T2.1.1.13.13.2.1\" style=\"font-size:90%;\">Semantic Understanding</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.1.1.13.13.3\"><span class=\"ltx_text\" id=\"S3.T2.1.1.13.13.3.1\" style=\"font-size:90%;\">GPT-4V</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.13.13.4\"><span class=\"ltx_text\" id=\"S3.T2.1.1.13.13.4.1\" style=\"font-size:90%;\">16.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.13.13.5\"><span class=\"ltx_text\" id=\"S3.T2.1.1.13.13.5.1\" style=\"font-size:90%;\">13.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.13.13.6\"><span class=\"ltx_text\" id=\"S3.T2.1.1.13.13.6.1\" style=\"font-size:90%;\">23.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.13.13.7\"><span class=\"ltx_text\" id=\"S3.T2.1.1.13.13.7.1\" style=\"font-size:90%;\">18.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.13.13.8\"><span class=\"ltx_text\" id=\"S3.T2.1.1.13.13.8.1\" style=\"font-size:90%;\">24.0</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.14.14\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T2.1.1.14.14.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T2.1.1.14.14.1.1\" style=\"font-size:90%;\">MLM-Filter</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T2.1.1.14.14.2\"><span class=\"ltx_text\" id=\"S3.T2.1.1.14.14.2.1\" style=\"font-size:90%;\">ITM AND ODF</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T2.1.1.14.14.3\"><span class=\"ltx_text\" id=\"S3.T2.1.1.14.14.3.1\" style=\"font-size:90%;\">GPT-4V</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.1.14.14.4\"><span class=\"ltx_text ltx_framed_underline\" id=\"S3.T2.1.1.14.14.4.1\" style=\"font-size:90%;\">30.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.1.14.14.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.14.14.5.1\" style=\"font-size:90%;\">25.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.1.14.14.6\"><span class=\"ltx_text ltx_framed_underline\" id=\"S3.T2.1.1.14.14.6.1\" style=\"font-size:90%;\">36.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.1.14.14.7\"><span class=\"ltx_text\" id=\"S3.T2.1.1.14.14.7.1\" style=\"font-size:90%;\">29.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.1.14.14.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.14.14.8.1\" style=\"font-size:90%;\">34.5</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.15.15\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_b\" id=\"S3.T2.1.1.15.15.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T2.1.1.15.15.1.1\" style=\"font-size:90%;\">MLM-Filter</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_b\" id=\"S3.T2.1.1.15.15.2\"><span class=\"ltx_text\" id=\"S3.T2.1.1.15.15.2.1\" style=\"font-size:90%;\">ITM OR ODF</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_b\" id=\"S3.T2.1.1.15.15.3\"><span class=\"ltx_text\" id=\"S3.T2.1.1.15.15.3.1\" style=\"font-size:90%;\">GPT-4V</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b\" id=\"S3.T2.1.1.15.15.4\"><span class=\"ltx_text\" id=\"S3.T2.1.1.15.15.4.1\" style=\"font-size:90%;\">28.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b\" id=\"S3.T2.1.1.15.15.5\"><span class=\"ltx_text\" id=\"S3.T2.1.1.15.15.5.1\" style=\"font-size:90%;\">24.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b\" id=\"S3.T2.1.1.15.15.6\"><span class=\"ltx_text\" id=\"S3.T2.1.1.15.15.6.1\" style=\"font-size:90%;\">35.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b\" id=\"S3.T2.1.1.15.15.7\"><span class=\"ltx_text\" id=\"S3.T2.1.1.15.15.7.1\" style=\"font-size:90%;\">29.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b\" id=\"S3.T2.1.1.15.15.8\"><span class=\"ltx_text\" id=\"S3.T2.1.1.15.15.8.1\" style=\"font-size:90%;\">33.9</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Zero-shot performance of CLIP models pre-trained using baseline filtering methods and proposed <span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T2.8.1\">MLM-Filter</span> on <span class=\"ltx_text ltx_font_italic\" id=\"S3.T2.9.2\">Medium</span> scale pools of the DataComp benchmark. AND represents the combination of ITM and ODF metrics using AND operation.</figcaption>\n</figure>",
            "capture": "Table 2: Zero-shot performance of CLIP models pre-trained using baseline filtering methods and proposed MLM-Filter on Medium scale pools of the DataComp benchmark. AND represents the combination of ITM and ODF metrics using AND operation."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T3\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T3.1\" style=\"width:344.5pt;height:111.7pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-30.4pt,9.9pt) scale(0.85,0.85) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T3.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt ltx_border_t\" id=\"S4.T3.1.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1.1\" style=\"font-size:90%;\">Filter</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt ltx_border_t\" id=\"S4.T3.1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.2.1\" style=\"font-size:90%;\">Metrics</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt ltx_border_t\" id=\"S4.T3.1.1.1.1.3\"><span class=\"ltx_text\" id=\"S4.T3.1.1.1.1.3.1\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T3.1.1.1.1.3.1.1\">\n<span class=\"ltx_tr\" id=\"S4.T3.1.1.1.1.3.1.1.1\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T3.1.1.1.1.3.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.3.1.1.1.1.1\">Teacher</span></span></span>\n<span class=\"ltx_tr\" id=\"S4.T3.1.1.1.1.3.1.1.2\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T3.1.1.1.1.3.1.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.3.1.1.2.1.1\">Model</span></span></span>\n</span></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t\" id=\"S4.T3.1.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.4.1\" style=\"font-size:90%;\">ImageNet-1k</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t\" id=\"S4.T3.1.1.1.1.5\"><span class=\"ltx_text\" id=\"S4.T3.1.1.1.1.5.1\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T3.1.1.1.1.5.1.1\">\n<span class=\"ltx_tr\" id=\"S4.T3.1.1.1.1.5.1.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.1.1.5.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.5.1.1.1.1.1\">ImageNet</span></span></span>\n<span class=\"ltx_tr\" id=\"S4.T3.1.1.1.1.5.1.1.2\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.1.1.5.1.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.5.1.1.2.1.1\">dist. shifts</span></span></span>\n</span></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t\" id=\"S4.T3.1.1.1.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.6.1\" style=\"font-size:90%;\">VTAB</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t\" id=\"S4.T3.1.1.1.1.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.7.1\" style=\"font-size:90%;\">Retrieval</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t\" id=\"S4.T3.1.1.1.1.8\"><span class=\"ltx_text\" id=\"S4.T3.1.1.1.1.8.1\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T3.1.1.1.1.8.1.1\">\n<span class=\"ltx_tr\" id=\"S4.T3.1.1.1.1.8.1.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.1.1.8.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.8.1.1.1.1.1\">Average over</span></span></span>\n<span class=\"ltx_tr\" id=\"S4.T3.1.1.1.1.8.1.1.2\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.1.1.8.1.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.8.1.1.2.1.1\">38 datasets</span></span></span>\n</span></span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T3.1.1.2.1.1\"><span class=\"ltx_text\" id=\"S4.T3.1.1.2.1.1.1\" style=\"font-size:90%;\">No Filtering</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T3.1.1.2.1.2\"><span class=\"ltx_text\" id=\"S4.T3.1.1.2.1.2.1\" style=\"font-size:90%;\">-</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T3.1.1.2.1.3\"><span class=\"ltx_text\" id=\"S4.T3.1.1.2.1.3.1\" style=\"font-size:90%;\">-</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.2.1.4\"><span class=\"ltx_text\" id=\"S4.T3.1.1.2.1.4.1\" style=\"font-size:90%;\">45.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.2.1.5\"><span class=\"ltx_text\" id=\"S4.T3.1.1.2.1.5.1\" style=\"font-size:90%;\">37.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.2.1.6\"><span class=\"ltx_text\" id=\"S4.T3.1.1.2.1.6.1\" style=\"font-size:90%;\">42.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.2.1.7\"><span class=\"ltx_text\" id=\"S4.T3.1.1.2.1.7.1\" style=\"font-size:90%;\">41.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.2.1.8\"><span class=\"ltx_text\" id=\"S4.T3.1.1.2.1.8.1\" style=\"font-size:90%;\">43.7</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T3.1.1.3.2.1\"><span class=\"ltx_text\" id=\"S4.T3.1.1.3.2.1.1\" style=\"font-size:90%;\">Basic Filtering</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T3.1.1.3.2.2\"><span class=\"ltx_text\" id=\"S4.T3.1.1.3.2.2.1\" style=\"font-size:90%;\">Rules</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T3.1.1.3.2.3\"><span class=\"ltx_text\" id=\"S4.T3.1.1.3.2.3.1\" style=\"font-size:90%;\">-</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.3.2.4\"><span class=\"ltx_text\" id=\"S4.T3.1.1.3.2.4.1\" style=\"font-size:90%;\">51.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.3.2.5\"><span class=\"ltx_text\" id=\"S4.T3.1.1.3.2.5.1\" style=\"font-size:90%;\">42.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.3.2.6\"><span class=\"ltx_text\" id=\"S4.T3.1.1.3.2.6.1\" style=\"font-size:90%;\">44.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.3.2.7\"><span class=\"ltx_text\" id=\"S4.T3.1.1.3.2.7.1\" style=\"font-size:90%;\">48.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.3.2.8\"><span class=\"ltx_text\" id=\"S4.T3.1.1.3.2.8.1\" style=\"font-size:90%;\">45.8</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T3.1.1.4.3.1\"><span class=\"ltx_text\" id=\"S4.T3.1.1.4.3.1.1\" style=\"font-size:90%;\">LAION Filtering</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T3.1.1.4.3.2\"><span class=\"ltx_text\" id=\"S4.T3.1.1.4.3.2.1\" style=\"font-size:90%;\">CLIPScore+Rules</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T3.1.1.4.3.3\"><span class=\"ltx_text\" id=\"S4.T3.1.1.4.3.3.1\" style=\"font-size:90%;\">-</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.4.3.4\"><span class=\"ltx_text\" id=\"S4.T3.1.1.4.3.4.1\" style=\"font-size:90%;\">55.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.4.3.5\"><span class=\"ltx_text\" id=\"S4.T3.1.1.4.3.5.1\" style=\"font-size:90%;\">45.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.4.3.6\"><span class=\"ltx_text\" id=\"S4.T3.1.1.4.3.6.1\" style=\"font-size:90%;\">51.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.4.3.7\"><span class=\"ltx_text\" id=\"S4.T3.1.1.4.3.7.1\" style=\"font-size:90%;\">49.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.4.3.8\"><span class=\"ltx_text\" id=\"S4.T3.1.1.4.3.8.1\" style=\"font-size:90%;\">50.1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T3.1.1.5.4.1\"><span class=\"ltx_text\" id=\"S4.T3.1.1.5.4.1.1\" style=\"font-size:90%;\">CLIPScore</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T3.1.1.5.4.2\"><span class=\"ltx_text\" id=\"S4.T3.1.1.5.4.2.1\" style=\"font-size:90%;\">CLIPScore</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T3.1.1.5.4.3\"><span class=\"ltx_text\" id=\"S4.T3.1.1.5.4.3.1\" style=\"font-size:90%;\">-</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.5.4.4\"><span class=\"ltx_text\" id=\"S4.T3.1.1.5.4.4.1\" style=\"font-size:90%;\">57.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.5.4.5\"><span class=\"ltx_text\" id=\"S4.T3.1.1.5.4.5.1\" style=\"font-size:90%;\">47.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.5.4.6\"><span class=\"ltx_text\" id=\"S4.T3.1.1.5.4.6.1\" style=\"font-size:90%;\">53.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.5.4.7\"><span class=\"ltx_text\" id=\"S4.T3.1.1.5.4.7.1\" style=\"font-size:90%;\">46.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.5.4.8\"><span class=\"ltx_text\" id=\"S4.T3.1.1.5.4.8.1\" style=\"font-size:90%;\">52.9</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.6.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_b ltx_border_t\" id=\"S4.T3.1.1.6.5.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T3.1.1.6.5.1.1\" style=\"font-size:90%;\">MLM-Filter</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_b ltx_border_t\" id=\"S4.T3.1.1.6.5.2\"><span class=\"ltx_text\" id=\"S4.T3.1.1.6.5.2.1\" style=\"font-size:90%;\">Object Detail Fulfillment</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_b ltx_border_t\" id=\"S4.T3.1.1.6.5.3\"><span class=\"ltx_text\" id=\"S4.T3.1.1.6.5.3.1\" style=\"font-size:90%;\">GPT-4</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_t\" id=\"S4.T3.1.1.6.5.4\"><span class=\"ltx_text\" id=\"S4.T3.1.1.6.5.4.1\" style=\"font-size:90%;\">58.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_t\" id=\"S4.T3.1.1.6.5.5\"><span class=\"ltx_text\" id=\"S4.T3.1.1.6.5.5.1\" style=\"font-size:90%;\">48.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_t\" id=\"S4.T3.1.1.6.5.6\"><span class=\"ltx_text\" id=\"S4.T3.1.1.6.5.6.1\" style=\"font-size:90%;\">57.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_t\" id=\"S4.T3.1.1.6.5.7\"><span class=\"ltx_text\" id=\"S4.T3.1.1.6.5.7.1\" style=\"font-size:90%;\">52.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_t\" id=\"S4.T3.1.1.6.5.8\"><span class=\"ltx_text\" id=\"S4.T3.1.1.6.5.8.1\" style=\"font-size:90%;\">54.2</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Zero-shot performance of CLIP models pre-trained using baseline filtering methods and proposed <span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T3.8.1\">MLM-Filter</span> on <span class=\"ltx_text ltx_font_italic\" id=\"S4.T3.9.2\">Large</span> scale pools of the DataComp benchmark.</figcaption>\n</figure>",
            "capture": "Table 3: Zero-shot performance of CLIP models pre-trained using baseline filtering methods and proposed MLM-Filter on Large scale pools of the DataComp benchmark."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T4\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T4.1\" style=\"width:180.3pt;height:102.6pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-4.7pt,2.7pt) scale(0.95,0.95) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T4.1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T4.1.1.1.1\">\n<td class=\"ltx_td ltx_align_justify ltx_border_tt ltx_border_t\" id=\"S4.T4.1.1.1.1.1\" style=\"width:82.5pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S4.T4.1.1.1.1.1.1\" style=\"font-size:90%;\">Filter</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt ltx_border_t\" id=\"S4.T4.1.1.1.1.2\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S4.T4.1.1.1.1.2.1\" style=\"font-size:90%;\">Metrics</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt ltx_border_t\" id=\"S4.T4.1.1.1.1.3\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S4.T4.1.1.1.1.3.1\" style=\"font-size:90%;\">SVHN</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt ltx_border_t\" id=\"S4.T4.1.1.1.1.4\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S4.T4.1.1.1.1.4.1\" style=\"font-size:90%;\">MNIST</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt ltx_border_t\" id=\"S4.T4.1.1.1.1.5\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S4.T4.1.1.1.1.5.1\" style=\"font-size:90%;\">Avg.</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.1.2.2\">\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S4.T4.1.1.2.2.1\" style=\"width:82.5pt;\"><span class=\"ltx_text ltx_font_smallcaps ltx_align_top\" id=\"S4.T4.1.1.2.2.1.1\" style=\"font-size:90%;\">MLM-Filter-GPT4</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S4.T4.1.1.2.2.2\" style=\"width:22.8pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.1.2.2.2.1\"><span class=\"ltx_text\" id=\"S4.T4.1.1.2.2.2.1.1\" style=\"font-size:90%;\">ITM</span></p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S4.T4.1.1.2.2.3\" style=\"width:22.8pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.1.2.2.3.1\"><span class=\"ltx_text\" id=\"S4.T4.1.1.2.2.3.1.1\" style=\"font-size:90%;\">8.2</span></p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S4.T4.1.1.2.2.4\" style=\"width:22.8pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.1.2.2.4.1\"><span class=\"ltx_text\" id=\"S4.T4.1.1.2.2.4.1.1\" style=\"font-size:90%;\">10.3</span></p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S4.T4.1.1.2.2.5\" style=\"width:22.8pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.1.2.2.5.1\"><span class=\"ltx_text\" id=\"S4.T4.1.1.2.2.5.1.1\" style=\"font-size:90%;\">9.2</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.1.3.3\">\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T4.1.1.3.3.1\" style=\"width:82.5pt;\"><span class=\"ltx_text ltx_font_smallcaps ltx_align_top\" id=\"S4.T4.1.1.3.3.1.1\" style=\"font-size:90%;\">MLM-Filter-GPT4</span></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T4.1.1.3.3.2\" style=\"width:22.8pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.1.3.3.2.1\"><span class=\"ltx_text\" id=\"S4.T4.1.1.3.3.2.1.1\" style=\"font-size:90%;\">ODF</span></p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T4.1.1.3.3.3\" style=\"width:22.8pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.1.3.3.3.1\"><span class=\"ltx_text\" id=\"S4.T4.1.1.3.3.3.1.1\" style=\"font-size:90%;\">14.6</span></p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T4.1.1.3.3.4\" style=\"width:22.8pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.1.3.3.4.1\"><span class=\"ltx_text\" id=\"S4.T4.1.1.3.3.4.1.1\" style=\"font-size:90%;\">19.3</span></p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T4.1.1.3.3.5\" style=\"width:22.8pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.1.3.3.5.1\"><span class=\"ltx_text\" id=\"S4.T4.1.1.3.3.5.1.1\" style=\"font-size:90%;\">16.9</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.1.4.4\">\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S4.T4.1.1.4.4.1\" style=\"width:82.5pt;\"><span class=\"ltx_text ltx_font_smallcaps ltx_align_top\" id=\"S4.T4.1.1.4.4.1.1\" style=\"font-size:90%;\">MLM-Filter-GPT4V</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S4.T4.1.1.4.4.2\" style=\"width:22.8pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.1.4.4.2.1\"><span class=\"ltx_text\" id=\"S4.T4.1.1.4.4.2.1.1\" style=\"font-size:90%;\">ITM</span></p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S4.T4.1.1.4.4.3\" style=\"width:22.8pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.1.4.4.3.1\"><span class=\"ltx_text\" id=\"S4.T4.1.1.4.4.3.1.1\" style=\"font-size:90%;\">15.4</span></p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S4.T4.1.1.4.4.4\" style=\"width:22.8pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.1.4.4.4.1\"><span class=\"ltx_text\" id=\"S4.T4.1.1.4.4.4.1.1\" style=\"font-size:90%;\">8.3</span></p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S4.T4.1.1.4.4.5\" style=\"width:22.8pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.1.4.4.5.1\"><span class=\"ltx_text\" id=\"S4.T4.1.1.4.4.5.1.1\" style=\"font-size:90%;\">11.8</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.1.5.5\">\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T4.1.1.5.5.1\" style=\"width:82.5pt;\"><span class=\"ltx_text ltx_font_smallcaps ltx_align_top\" id=\"S4.T4.1.1.5.5.1.1\" style=\"font-size:90%;\">MLM-Filter-GPT4V</span></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T4.1.1.5.5.2\" style=\"width:22.8pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.1.5.5.2.1\"><span class=\"ltx_text\" id=\"S4.T4.1.1.5.5.2.1.1\" style=\"font-size:90%;\">ODF</span></p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T4.1.1.5.5.3\" style=\"width:22.8pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.1.5.5.3.1\"><span class=\"ltx_text\" id=\"S4.T4.1.1.5.5.3.1.1\" style=\"font-size:90%;\">9.0</span></p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T4.1.1.5.5.4\" style=\"width:22.8pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.1.5.5.4.1\"><span class=\"ltx_text\" id=\"S4.T4.1.1.5.5.4.1.1\" style=\"font-size:90%;\">6.8</span></p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T4.1.1.5.5.5\" style=\"width:22.8pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.1.5.5.5.1\"><span class=\"ltx_text\" id=\"S4.T4.1.1.5.5.5.1.1\" style=\"font-size:90%;\">7.9</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.1.6.6\">\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_b ltx_border_t\" id=\"S4.T4.1.1.6.6.1\" style=\"width:82.5pt;\"><span class=\"ltx_text ltx_font_smallcaps ltx_align_top\" id=\"S4.T4.1.1.6.6.1.1\" style=\"font-size:90%;\">MLM-Filter-GPT4V</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_b ltx_border_t\" id=\"S4.T4.1.1.6.6.2\" style=\"width:22.8pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.1.6.6.2.1\"><span class=\"ltx_text\" id=\"S4.T4.1.1.6.6.2.1.1\" style=\"font-size:90%;\">AND</span></p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_b ltx_border_t\" id=\"S4.T4.1.1.6.6.3\" style=\"width:22.8pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.1.6.6.3.1\"><span class=\"ltx_text\" id=\"S4.T4.1.1.6.6.3.1.1\" style=\"font-size:90%;\">12.9</span></p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_b ltx_border_t\" id=\"S4.T4.1.1.6.6.4\" style=\"width:22.8pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.1.6.6.4.1\"><span class=\"ltx_text\" id=\"S4.T4.1.1.6.6.4.1.1\" style=\"font-size:90%;\">11.6</span></p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_b ltx_border_t\" id=\"S4.T4.1.1.6.6.5\" style=\"width:22.8pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.1.6.6.5.1\"><span class=\"ltx_text\" id=\"S4.T4.1.1.6.6.5.1.1\" style=\"font-size:90%;\">12.3</span></p>\n</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>\nZero-shot performance of pre-trained CLIP on SVHN and MNIST digit classification datasets. Avg. represents the average performance on two digit datasets. AND represents the combination of ITM and ODF metrics using AND operation.\n</figcaption>\n</figure>",
            "capture": "Table 4: \nZero-shot performance of pre-trained CLIP on SVHN and MNIST digit classification datasets. Avg. represents the average performance on two digit datasets. AND represents the combination of ITM and ODF metrics using AND operation.\n"
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T5\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T5.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt ltx_border_t\" id=\"S4.T5.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.1.1.1\">Filter</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt ltx_border_t\" id=\"S4.T5.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.1.2.1\">Metric</span></th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt ltx_border_t\" id=\"S4.T5.1.1.1.3\" style=\"width:28.5pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S4.T5.1.1.1.3.1\">VQA</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t\" id=\"S4.T5.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.1.4.1\">GQA</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T5.1.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T5.1.2.1.1\">CLIPScore</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T5.1.2.1.2\">CLIPScore</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S4.T5.1.2.1.3\" style=\"width:28.5pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T5.1.2.1.3.1\">55.1</p>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.2.1.4\">34.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.3.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_b ltx_border_t\" id=\"S4.T5.1.3.2.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T5.1.3.2.1.1\">MLM-Filter-GPT4</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_b ltx_border_t\" id=\"S4.T5.1.3.2.2\">ODF</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_b ltx_border_t\" id=\"S4.T5.1.3.2.3\" style=\"width:28.5pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T5.1.3.2.3.1\">56.8</p>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_t\" id=\"S4.T5.1.3.2.4\">36.2</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>\nZero-shot VQA performance of BLIP-2 models pre-trained on dataset filtered by different filtering methods.\n</figcaption>\n</figure>",
            "capture": "Table 5: \nZero-shot VQA performance of BLIP-2 models pre-trained on dataset filtered by different filtering methods.\n"
        },
        "6": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T6\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T6.8\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T6.8.9.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt ltx_border_t\" id=\"S4.T6.8.9.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.8.9.1.1.1\" style=\"font-size:90%;\">Filter</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt ltx_border_t\" id=\"S4.T6.8.9.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.8.9.1.2.1\" style=\"font-size:90%;\">Metric</span></th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt ltx_border_t\" id=\"S4.T6.8.9.1.3\" style=\"width:28.5pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S4.T6.8.9.1.3.1\" style=\"font-size:90%;\">Pearson</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t\" id=\"S4.T6.8.9.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.8.9.1.4.1\" style=\"font-size:90%;\">Spearman</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.8.10.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T6.8.10.2.1\"><span class=\"ltx_text\" id=\"S4.T6.8.10.2.1.1\" style=\"font-size:90%;\">CLIPScore</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T6.8.10.2.2\"><span class=\"ltx_text\" id=\"S4.T6.8.10.2.2.1\" style=\"font-size:90%;\">-</span></th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t\" id=\"S4.T6.8.10.2.3\" style=\"width:28.5pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T6.8.10.2.3.1\"><span class=\"ltx_text\" id=\"S4.T6.8.10.2.3.1.1\" style=\"font-size:90%;\">0.164</span></p>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T6.8.10.2.4\"><span class=\"ltx_text\" id=\"S4.T6.8.10.2.4.1\" style=\"font-size:90%;\">0.072</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T6.2.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T6.2.2.3\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T6.2.2.3.1\" style=\"font-size:90%;\">MLM-Filter-GPT4</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T6.2.2.4\"><span class=\"ltx_text\" id=\"S4.T6.2.2.4.1\" style=\"font-size:90%;\">ITM</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S4.T6.1.1.1\" style=\"width:28.5pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S4.T6.1.1.1.1.1.1\" style=\"font-size:90%;\">0.452</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.2.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.2.2.2.1\" style=\"font-size:90%;\">0.430</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.4.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T6.4.4.3\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T6.4.4.3.1\" style=\"font-size:90%;\">MLM-Filter-GPT4</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T6.4.4.4\"><span class=\"ltx_text\" id=\"S4.T6.4.4.4.1\" style=\"font-size:90%;\">ODF</span></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T6.3.3.1\" style=\"width:28.5pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T6.3.3.1.1.1\"><span class=\"ltx_text\" id=\"S4.T6.3.3.1.1.1.1\" style=\"font-size:90%;\">0.410</span></p>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.4.2\">\n<span class=\"ltx_text\" id=\"S4.T6.4.4.2.1\" style=\"font-size:90%;\">0.384</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.6.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T6.6.6.3\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T6.6.6.3.1\" style=\"font-size:90%;\">MLM-Filter-GPT4V</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T6.6.6.4\"><span class=\"ltx_text\" id=\"S4.T6.6.6.4.1\" style=\"font-size:90%;\">ITM</span></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T6.5.5.1\" style=\"width:28.5pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T6.5.5.1.1.1\"><span class=\"ltx_text\" id=\"S4.T6.5.5.1.1.1.1\" style=\"font-size:90%;\">0.328</span></p>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.6.6.2\">\n<span class=\"ltx_text\" id=\"S4.T6.6.6.2.1\" style=\"font-size:90%;\">0.331</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.8.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_b\" id=\"S4.T6.8.8.3\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T6.8.8.3.1\" style=\"font-size:90%;\">MLM-Filter-GPT4V</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_b\" id=\"S4.T6.8.8.4\"><span class=\"ltx_text\" id=\"S4.T6.8.8.4.1\" style=\"font-size:90%;\">ODF</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_b\" id=\"S4.T6.7.7.1\" style=\"width:28.5pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T6.7.7.1.1.1\"><span class=\"ltx_text\" id=\"S4.T6.7.7.1.1.1.1\" style=\"font-size:90%;\">0.368</span></p>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b\" id=\"S4.T6.8.8.2\">\n<span class=\"ltx_text\" id=\"S4.T6.8.8.2.1\" style=\"font-size:90%;\">0.374</span>\n</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 6: </span>\nPearson and Spearman correlations between human-labeled quality scores and scores generated by MLM-Filter and CLIP. Images are scored on a scale of 100 for our MLMFilter, while CLIPScore is also normalized to the scale of 100. The  denotes significant correlations at .\n</figcaption>\n</figure>",
            "capture": "Table 6: \nPearson and Spearman correlations between human-labeled quality scores and scores generated by MLM-Filter and CLIP. Images are scored on a scale of 100 for our MLMFilter, while CLIPScore is also normalized to the scale of 100. The  denotes significant correlations at .\n"
        },
        "7": {
            "table_html": "<figure class=\"ltx_table\" id=\"A2.T7\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A2.T7.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A2.T7.1.1.1\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"A2.T7.1.1.1.1\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A2.T7.1.1.1.1.1\"><span class=\"ltx_text\" id=\"A2.T7.1.1.1.1.1.1\" style=\"font-size:90%;\">Example for </span><span class=\"ltx_text ltx_font_bold\" id=\"A2.T7.1.1.1.1.1.2\" style=\"font-size:90%;color:#FF0000;\">Chain-of-Thought Reasoning</span></p>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\" id=\"A2.T7.1.1.1.2\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A2.T7.1.1.1.2.1\"><span class=\"ltx_text\" id=\"A2.T7.1.1.1.2.1.1\" style=\"font-size:90%;\">Example for </span><span class=\"ltx_text ltx_font_bold\" id=\"A2.T7.1.1.1.2.1.2\" style=\"font-size:90%;color:#FF0000;\">Rationalization</span></p>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A2.T7.1.2.1\">\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"A2.T7.1.2.1.1\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A2.T7.1.2.1.1.1\"><span class=\"ltx_text\" id=\"A2.T7.1.2.1.1.1.1\" style=\"font-size:90%;\">Please evaluate if the provided text caption accurately represents the main features and objects of the image. The caption doesn\u2019t need to detail every aspect of the image, but it should capture its primary theme. Rate the overall quality of the text caption\u2019s match to the image on a scale of 1-100, considering the criteria mentioned.</span></p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"A2.T7.1.2.1.2\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A2.T7.1.2.1.2.1\"><span class=\"ltx_text\" id=\"A2.T7.1.2.1.2.1.1\" style=\"font-size:90%;\">Please evaluate if the provided text caption accurately represents the main features and objects of the image. The caption doesn\u2019t need to detail every aspect of the image, but it should capture its primary theme. Rate the overall quality of the text caption\u2019s match to the image on a scale of 1-100, considering the criteria mentioned.</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T7.1.3.2\">\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_b ltx_border_r\" id=\"A2.T7.1.3.2.1\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A2.T7.1.3.2.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T7.1.3.2.1.1.1\" style=\"font-size:90%;color:#000000;\">Please</span><span class=\"ltx_text\" id=\"A2.T7.1.3.2.1.1.2\" style=\"font-size:90%;\"> </span><span class=\"ltx_text ltx_font_bold\" id=\"A2.T7.1.3.2.1.1.3\" style=\"font-size:90%;color:#0000FF;\">think step by step to first output your reasons</span><span class=\"ltx_text\" id=\"A2.T7.1.3.2.1.1.4\" style=\"font-size:90%;\"> </span><span class=\"ltx_text ltx_font_bold\" id=\"A2.T7.1.3.2.1.1.5\" style=\"font-size:90%;color:#000000;\">to give such a score. In the subsequent line, please output a single line containing the value indicating the scores.</span></p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_b\" id=\"A2.T7.1.3.2.2\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A2.T7.1.3.2.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T7.1.3.2.2.1.1\" style=\"font-size:90%;color:#000000;\">Please</span><span class=\"ltx_text\" id=\"A2.T7.1.3.2.2.1.2\" style=\"font-size:90%;\"> </span><span class=\"ltx_text ltx_font_bold\" id=\"A2.T7.1.3.2.2.1.3\" style=\"font-size:90%;color:#0000FF;\">first output a single line containing the value indicating the scores</span><span class=\"ltx_text\" id=\"A2.T7.1.3.2.2.1.4\" style=\"font-size:90%;\">. </span><span class=\"ltx_text ltx_font_bold\" id=\"A2.T7.1.3.2.2.1.5\" style=\"font-size:90%;color:#000000;\">In the subsequent line, please</span><span class=\"ltx_text\" id=\"A2.T7.1.3.2.2.1.6\" style=\"font-size:90%;\"> </span><span class=\"ltx_text ltx_font_bold\" id=\"A2.T7.1.3.2.2.1.7\" style=\"font-size:90%;color:#0000FF;\">provide a comprehensive explanation of your evaluation</span><span class=\"ltx_text\" id=\"A2.T7.1.3.2.2.1.8\" style=\"font-size:90%;\">, </span><span class=\"ltx_text ltx_font_bold\" id=\"A2.T7.1.3.2.2.1.9\" style=\"font-size:90%;color:#000000;\">avoiding any potential bias.</span></p>\n</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 7: </span>Prompts for zero-shot Chain-of-Thought reasoning and Rationalization reasoning for assessing the image-text matching score.</figcaption>\n</figure>",
            "capture": "Table 7: Prompts for zero-shot Chain-of-Thought reasoning and Rationalization reasoning for assessing the image-text matching score."
        },
        "8": {
            "table_html": "<figure class=\"ltx_table\" id=\"A4.T8\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"A4.T8.1\" style=\"width:318.9pt;height:270pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"A4.T8.1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A4.T8.1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt ltx_border_t\" id=\"A4.T8.1.1.1.1.1\"><span class=\"ltx_text\" id=\"A4.T8.1.1.1.1.1.1\" style=\"font-size:90%;\">Data</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_tt ltx_border_t\" id=\"A4.T8.1.1.1.1.2\"><span class=\"ltx_text\" id=\"A4.T8.1.1.1.1.2.1\" style=\"font-size:90%;\">Size</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt ltx_border_t\" id=\"A4.T8.1.1.1.1.3\"><span class=\"ltx_text\" id=\"A4.T8.1.1.1.1.3.1\" style=\"font-size:90%;\">Task</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T8.1.1.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"A4.T8.1.1.2.2.1\">\n<span class=\"ltx_text\" id=\"A4.T8.1.1.2.2.1.1\" style=\"font-size:90%;\">Visual Conversation\u00a0</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"A4.T8.1.1.2.2.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02677v1#bib.bib19\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">19</span></a><span class=\"ltx_text\" id=\"A4.T8.1.1.2.2.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T8.1.1.2.2.2\"><span class=\"ltx_text\" id=\"A4.T8.1.1.2.2.2.1\" style=\"font-size:90%;\">5K</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T8.1.1.2.2.3\"><span class=\"ltx_text\" id=\"A4.T8.1.1.2.2.3.1\" style=\"font-size:90%;\">Conversation</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T8.1.1.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A4.T8.1.1.3.3.1\">\n<span class=\"ltx_text\" id=\"A4.T8.1.1.3.3.1.1\" style=\"font-size:90%;\">Complex Reasoning\u00a0</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"A4.T8.1.1.3.3.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02677v1#bib.bib19\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">19</span></a><span class=\"ltx_text\" id=\"A4.T8.1.1.3.3.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T8.1.1.3.3.2\"><span class=\"ltx_text\" id=\"A4.T8.1.1.3.3.2.1\" style=\"font-size:90%;\">16k</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T8.1.1.3.3.3\"><span class=\"ltx_text\" id=\"A4.T8.1.1.3.3.3.1\" style=\"font-size:90%;\">Visual Reasoning</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T8.1.1.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A4.T8.1.1.4.4.1\">\n<span class=\"ltx_text\" id=\"A4.T8.1.1.4.4.1.1\" style=\"font-size:90%;\">Detail Description\u00a0</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"A4.T8.1.1.4.4.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02677v1#bib.bib19\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">19</span></a><span class=\"ltx_text\" id=\"A4.T8.1.1.4.4.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T8.1.1.4.4.2\"><span class=\"ltx_text\" id=\"A4.T8.1.1.4.4.2.1\" style=\"font-size:90%;\">5k</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T8.1.1.4.4.3\"><span class=\"ltx_text\" id=\"A4.T8.1.1.4.4.3.1\" style=\"font-size:90%;\">Captioning</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T8.1.1.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"A4.T8.1.1.5.5.1\">\n<span class=\"ltx_text\" id=\"A4.T8.1.1.5.5.1.1\" style=\"font-size:90%;\">ShareGPT\u00a0</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"A4.T8.1.1.5.5.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02677v1#bib.bib35\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">35</span></a><span class=\"ltx_text\" id=\"A4.T8.1.1.5.5.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T8.1.1.5.5.2\"><span class=\"ltx_text\" id=\"A4.T8.1.1.5.5.2.1\" style=\"font-size:90%;\">10K</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T8.1.1.5.5.3\"><span class=\"ltx_text\" id=\"A4.T8.1.1.5.5.3.1\" style=\"font-size:90%;\">Language-Only Instructions</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T8.1.1.6.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"A4.T8.1.1.6.6.1\">\n<span class=\"ltx_text\" id=\"A4.T8.1.1.6.6.1.1\" style=\"font-size:90%;\">VQAv2\u00a0</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"A4.T8.1.1.6.6.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02677v1#bib.bib12\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">12</span></a><span class=\"ltx_text\" id=\"A4.T8.1.1.6.6.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T8.1.1.6.6.2\"><span class=\"ltx_text\" id=\"A4.T8.1.1.6.6.2.1\" style=\"font-size:90%;\">2K</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T8.1.1.6.6.3\"><span class=\"ltx_text\" id=\"A4.T8.1.1.6.6.3.1\" style=\"font-size:90%;\">VQA</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T8.1.1.7.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A4.T8.1.1.7.7.1\">\n<span class=\"ltx_text\" id=\"A4.T8.1.1.7.7.1.1\" style=\"font-size:90%;\">GQA\u00a0</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"A4.T8.1.1.7.7.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02677v1#bib.bib15\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">15</span></a><span class=\"ltx_text\" id=\"A4.T8.1.1.7.7.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T8.1.1.7.7.2\"><span class=\"ltx_text\" id=\"A4.T8.1.1.7.7.2.1\" style=\"font-size:90%;\">3K</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T8.1.1.7.7.3\"><span class=\"ltx_text\" id=\"A4.T8.1.1.7.7.3.1\" style=\"font-size:90%;\">Visual Reasoning</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T8.1.1.8.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A4.T8.1.1.8.8.1\">\n<span class=\"ltx_text\" id=\"A4.T8.1.1.8.8.1.1\" style=\"font-size:90%;\">OKVQA\u00a0</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"A4.T8.1.1.8.8.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02677v1#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a><span class=\"ltx_text\" id=\"A4.T8.1.1.8.8.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T8.1.1.8.8.2\"><span class=\"ltx_text\" id=\"A4.T8.1.1.8.8.2.1\" style=\"font-size:90%;\">2K</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T8.1.1.8.8.3\"><span class=\"ltx_text\" id=\"A4.T8.1.1.8.8.3.1\" style=\"font-size:90%;\">Knowledge Grounded VQA</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T8.1.1.9.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A4.T8.1.1.9.9.1\">\n<span class=\"ltx_text\" id=\"A4.T8.1.1.9.9.1.1\" style=\"font-size:90%;\">OCRVQA\u00a0</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"A4.T8.1.1.9.9.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02677v1#bib.bib24\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">24</span></a><span class=\"ltx_text\" id=\"A4.T8.1.1.9.9.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T8.1.1.9.9.2\"><span class=\"ltx_text\" id=\"A4.T8.1.1.9.9.2.1\" style=\"font-size:90%;\">1K</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T8.1.1.9.9.3\"><span class=\"ltx_text\" id=\"A4.T8.1.1.9.9.3.1\" style=\"font-size:90%;\">OCR</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T8.1.1.10.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A4.T8.1.1.10.10.1\">\n<span class=\"ltx_text\" id=\"A4.T8.1.1.10.10.1.1\" style=\"font-size:90%;\">TextCaption\u00a0</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"A4.T8.1.1.10.10.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02677v1#bib.bib36\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">36</span></a><span class=\"ltx_text\" id=\"A4.T8.1.1.10.10.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T8.1.1.10.10.2\"><span class=\"ltx_text\" id=\"A4.T8.1.1.10.10.2.1\" style=\"font-size:90%;\">2K</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T8.1.1.10.10.3\"><span class=\"ltx_text\" id=\"A4.T8.1.1.10.10.3.1\" style=\"font-size:90%;\">Captioning</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T8.1.1.11.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"A4.T8.1.1.11.11.1\"><span class=\"ltx_text\" id=\"A4.T8.1.1.11.11.1.1\" style=\"font-size:90%;\">ITM Scoring</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T8.1.1.11.11.2\"><span class=\"ltx_text\" id=\"A4.T8.1.1.11.11.2.1\" style=\"font-size:90%;\">1k</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T8.1.1.11.11.3\" rowspan=\"4\"><span class=\"ltx_text\" id=\"A4.T8.1.1.11.11.3.1\" style=\"font-size:90%;\">Data Quality Scoring</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T8.1.1.12.12\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A4.T8.1.1.12.12.1\"><span class=\"ltx_text\" id=\"A4.T8.1.1.12.12.1.1\" style=\"font-size:90%;\">ODF Scoring</span></th>\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T8.1.1.12.12.2\"><span class=\"ltx_text\" id=\"A4.T8.1.1.12.12.2.1\" style=\"font-size:90%;\">1k</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T8.1.1.13.13\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A4.T8.1.1.13.13.1\"><span class=\"ltx_text\" id=\"A4.T8.1.1.13.13.1.1\" style=\"font-size:90%;\">CTQ Scoring</span></th>\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T8.1.1.13.13.2\"><span class=\"ltx_text\" id=\"A4.T8.1.1.13.13.2.1\" style=\"font-size:90%;\">1k</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T8.1.1.14.14\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A4.T8.1.1.14.14.1\"><span class=\"ltx_text\" id=\"A4.T8.1.1.14.14.1.1\" style=\"font-size:90%;\">SU Scoring</span></th>\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T8.1.1.14.14.2\"><span class=\"ltx_text\" id=\"A4.T8.1.1.14.14.2.1\" style=\"font-size:90%;\">1k</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T8.1.1.15.15\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_b ltx_border_r ltx_border_t\" id=\"A4.T8.1.1.15.15.1\"><span class=\"ltx_text\" id=\"A4.T8.1.1.15.15.1.1\" style=\"font-size:90%;\">Total</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_b ltx_border_t\" id=\"A4.T8.1.1.15.15.2\"><span class=\"ltx_text\" id=\"A4.T8.1.1.15.15.2.1\" style=\"font-size:90%;\">50k</span></td>\n<td class=\"ltx_td ltx_border_bb ltx_border_b ltx_border_t\" id=\"A4.T8.1.1.15.15.3\"></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 8: </span>\nMultimodal instruction data mixture of the data quality scoring tasks and other multimodal tasks.\n</figcaption>\n</figure>",
            "capture": "Table 8: \nMultimodal instruction data mixture of the data quality scoring tasks and other multimodal tasks.\n"
        },
        "9": {
            "table_html": "<figure class=\"ltx_table\" id=\"A5.T9\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A5.T9.2\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A5.T9.2.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt ltx_border_t\" id=\"A5.T9.2.3.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A5.T9.2.3.1.1.1\">Hyperparameter</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt ltx_border_t\" id=\"A5.T9.2.3.1.2\">BLIP-2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.4.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"2\" id=\"A5.T9.2.4.2.1\">\n<span class=\"ltx_text ltx_font_bold\" id=\"A5.T9.2.4.2.1.1\">Stage-1 Pre-training</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.5.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A5.T9.2.5.3.1\"># Trainable Parameters</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T9.2.5.3.2\">188M</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.6.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A5.T9.2.6.4.1\">Precision</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T9.2.6.4.2\"><span class=\"ltx_text ltx_font_typewriter\" id=\"A5.T9.2.6.4.2.1\">float16</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.7.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A5.T9.2.7.5.1\">Global Batch Size</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T9.2.7.5.2\">1680</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.8.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A5.T9.2.8.6.1\"># Training Steps</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T9.2.8.6.2\">250k</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.9.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A5.T9.2.9.7.1\"># GPUs</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T9.2.9.7.2\">16</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.10.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A5.T9.2.10.8.1\"># Gradient Accumulation Steps</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T9.2.10.8.2\">1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.11.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A5.T9.2.11.9.1\">Min LR</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T9.2.11.9.2\">1e-5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.12.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A5.T9.2.12.10.1\">Peak LR</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T9.2.12.10.2\">1e-4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.13.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A5.T9.2.13.11.1\"># Warmup Steps</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T9.2.13.11.2\">2000</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.14.12\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A5.T9.2.14.12.1\">LR Scheduler</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T9.2.14.12.2\">Cosine LR Decay</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.15.13\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A5.T9.2.15.13.1\">Weight Decay</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T9.2.15.13.2\">0.05</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A5.T9.1.1.1\">Adam \n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T9.1.1.2\">(0.9, 0.98)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.16.14\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"2\" id=\"A5.T9.2.16.14.1\">\n<span class=\"ltx_text ltx_font_bold\" id=\"A5.T9.2.16.14.1.1\">Stage-2 Pre-training</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.17.15\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A5.T9.2.17.15.1\"># Trainable Parameters</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T9.2.17.15.2\">188M</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.18.16\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A5.T9.2.18.16.1\">Precision</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T9.2.18.16.2\"><span class=\"ltx_text ltx_font_typewriter\" id=\"A5.T9.2.18.16.2.1\">float16</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.19.17\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A5.T9.2.19.17.1\">Global Batch Size</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T9.2.19.17.2\">1920</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.20.18\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A5.T9.2.20.18.1\"># Training Steps</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T9.2.20.18.2\">80k</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.21.19\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A5.T9.2.21.19.1\"># GPUs</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T9.2.21.19.2\">16</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.22.20\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A5.T9.2.22.20.1\"># Gradient Accumulation Steps</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T9.2.22.20.2\">4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.23.21\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A5.T9.2.23.21.1\">Min LR</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T9.2.23.21.2\">5e-5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.24.22\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A5.T9.2.24.22.1\">Peak LR</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T9.2.24.22.2\">1e-4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.25.23\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A5.T9.2.25.23.1\"># Warmup Steps</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T9.2.25.23.2\">2000</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.26.24\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A5.T9.2.26.24.1\">LR Scheduler</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T9.2.26.24.2\">Cosine LR Decay</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.27.25\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A5.T9.2.27.25.1\">Weight Decay</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T9.2.27.25.2\">0.05</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_b ltx_border_r\" id=\"A5.T9.2.2.1\">Adam \n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b\" id=\"A5.T9.2.2.2\">(0.9, 0.98)</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 9: </span>Training details for BLIP-2 pre-training stage 1 and stage 2.</figcaption>\n</figure>",
            "capture": "Table 9: Training details for BLIP-2 pre-training stage 1 and stage 2."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.02677v1_figure_1.png",
            "caption": "Figure 1: CLIPScore fails in differentiating the fine-grained object-level image-text alignment, while the image-text matching score generated by MLM Filter significantly captures such alignment."
        },
        "2": {
            "figure_path": "2403.02677v1_figure_2.png",
            "caption": "Figure 2: Illustration of the pipeline of fine-tuning MLM Filter and employing it for data filtering."
        },
        "3": {
            "figure_path": "2403.02677v1_figure_3.png",
            "caption": "Figure 3: (a) image text matching score distribution of initial 10k instructions using GPT-4V on CC12M; (b) image text matching score distribution of final 1k instructions uniformly sampled from 10 buckets."
        },
        "4": {
            "figure_path": "2403.02677v1_figure_4.png",
            "caption": "Figure 3: (a) image text matching score distribution of initial 10k instructions using GPT-4V on CC12M; (b) image text matching score distribution of final 1k instructions uniformly sampled from 10 buckets."
        },
        "5": {
            "figure_path": "2403.02677v1_figure_5.png",
            "caption": "Figure 4: Effects of fraction of images selected for training CLIP."
        }
    },
    "references": [
        {
            "1": {
                "title": "Flamingo: a visual language model for few-shot learning.",
                "author": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al.",
                "venue": "Advances in Neural Information Processing Systems, 35:23716\u201323736, 2022.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Improving image generation with better captions.",
                "author": "James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al.",
                "venue": "Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3), 2023.",
                "url": null
            }
        },
        {
            "3": {
                "title": "Coyo-700m: Image-text pair dataset.",
                "author": "Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim.",
                "venue": "https://github.com/kakaobrain/coyo-dataset, 2022.",
                "url": null
            }
        },
        {
            "4": {
                "title": "Sharegpt4v: Improving large multi-modal models with better captions.",
                "author": "Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin.",
                "venue": "arXiv preprint arXiv:2311.12793, 2023.",
                "url": null
            }
        },
        {
            "5": {
                "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.",
                "author": "Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing.",
                "venue": null,
                "url": null
            }
        },
        {
            "6": {
                "title": "Alpagasus: Training a better alpaca with fewer data.",
                "author": "Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et al.",
                "venue": "arXiv preprint arXiv:2307.08701, 2023.",
                "url": null
            }
        },
        {
            "7": {
                "title": "e-snli: Natural language inference with natural language explanations.",
                "author": "Oana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, and Phil Blunsom.",
                "venue": "Advances in Neural Information Processing Systems, 31, 2018.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Imagenet: A large-scale hierarchical image database.",
                "author": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.",
                "venue": "In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.",
                "url": null
            }
        },
        {
            "9": {
                "title": "Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023.",
                "author": "Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi.",
                "venue": null,
                "url": null
            }
        },
        {
            "10": {
                "title": "Data filtering networks.",
                "author": "Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar.",
                "venue": "arXiv preprint arXiv:2309.17425, 2023.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Datacomp: In search of the next generation of multimodal datasets.",
                "author": "Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al.",
                "venue": "arXiv preprint arXiv:2304.14108, 2023.",
                "url": null
            }
        },
        {
            "12": {
                "title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering.",
                "author": "Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.",
                "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6904\u20136913, 2017.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Language is not all you need: Aligning perception with language models.",
                "author": "Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, et al.",
                "venue": "Advances in Neural Information Processing Systems, 36, 2024.",
                "url": null
            }
        },
        {
            "14": {
                "title": "Clipscore: A reference-free evaluation metric for image captioning.",
                "author": "Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi.",
                "venue": "arXiv preprint arXiv:2104.08718, 2021.",
                "url": null
            }
        },
        {
            "15": {
                "title": "Gqa: A new dataset for real-world visual reasoning and compositional question answering.",
                "author": "Drew A Hudson and Christopher D Manning.",
                "venue": "In CVPR, 2019.",
                "url": null
            }
        },
        {
            "16": {
                "title": "Scaling up visual and vision-language representation learning with noisy text supervision.",
                "author": "Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig.",
                "venue": "In International conference on machine learning, pages 4904\u20134916. PMLR, 2021.",
                "url": null
            }
        },
        {
            "17": {
                "title": "Improved baselines with visual instruction tuning.",
                "author": "Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.",
                "venue": "arXiv preprint arXiv:2310.03744, 2023.",
                "url": null
            }
        },
        {
            "18": {
                "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.",
                "author": "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.",
                "venue": "arXiv preprint arXiv:2301.12597, 2023.",
                "url": null
            }
        },
        {
            "19": {
                "title": "Visual instruction tuning.",
                "author": "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.",
                "venue": "arXiv preprint arXiv:2304.08485, 2023.",
                "url": null
            }
        },
        {
            "20": {
                "title": "Microsoft coco: Common objects in context.",
                "author": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick.",
                "venue": "In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740\u2013755. Springer, 2014.",
                "url": null
            }
        },
        {
            "21": {
                "title": "T-mars: Improving visual representations by circumventing text feature learning.",
                "author": "Pratyush Maini, Sachin Goyal, Zachary C Lipton, J Zico Kolter, and Aditi Raghunathan.",
                "venue": "arXiv preprint arXiv:2307.03132, 2023.",
                "url": null
            }
        },
        {
            "22": {
                "title": "Cross-task generalization via natural language crowdsourcing instructions.",
                "author": "Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi.",
                "venue": "arXiv preprint arXiv:2104.08773, 2021.",
                "url": null
            }
        },
        {
            "23": {
                "title": "Ok-vqa: A visual question answering benchmark requiring external knowledge.",
                "author": "Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi.",
                "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR), 2019.",
                "url": null
            }
        },
        {
            "24": {
                "title": "Ocr-vqa: Visual question answering by reading text in images.",
                "author": "Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty.",
                "venue": "In 2019 international conference on document analysis and recognition (ICDAR), pages 947\u2013952. IEEE, 2019.",
                "url": null
            }
        },
        {
            "25": {
                "title": "Quality not quantity: On the interaction between dataset design and robustness of clip.",
                "author": "Thao Nguyen, Gabriel Ilharco, Mitchell Wortsman, Sewoong Oh, and Ludwig Schmidt.",
                "venue": "Advances in Neural Information Processing Systems, 35:21455\u201321469, 2022.",
                "url": null
            }
        },
        {
            "26": {
                "title": "Gpt-4v(ision) technical work and authors.",
                "author": "OpenAI.",
                "venue": "2023.",
                "url": null
            }
        },
        {
            "27": {
                "title": "Training language models to follow instructions with human feedback.",
                "author": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al.",
                "venue": "Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.",
                "url": null
            }
        },
        {
            "28": {
                "title": "Making monolingual sentence embeddings multilingual using knowledge distillation.",
                "author": "Nils Reimers and Iryna Gurevych.",
                "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2020.",
                "url": null
            }
        },
        {
            "29": {
                "title": "Learning transferable visual models from natural language supervision.",
                "author": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.",
                "venue": "In ICML, 2021.",
                "url": null
            }
        },
        {
            "30": {
                "title": "Laion-5b: An open large-scale dataset for training next generation image-text models.",
                "author": "Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al.",
                "venue": "Advances in Neural Information Processing Systems, 35:25278\u201325294, 2022.",
                "url": null
            }
        },
        {
            "31": {
                "title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning.",
                "author": "Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.",
                "venue": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556\u20132565, 2018.",
                "url": null
            }
        },
        {
            "32": {
                "title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning.",
                "author": "Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.",
                "venue": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556\u20132565, 2018.",
                "url": null
            }
        },
        {
            "33": {
                "title": "Eva-clip: Improved training techniques for clip at scale.",
                "author": "Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao.",
                "venue": "arXiv preprint arXiv:2303.15389, 2023.",
                "url": null
            }
        },
        {
            "34": {
                "title": "Scienceqa: A novel resource for question answering on scholarly articles.",
                "author": "Tanik Saikh, Tirthankar Ghosal, Amish Mittal, Asif Ekbal, and Pushpak Bhattacharyya.",
                "venue": "International Journal on Digital Libraries, 23(3):289\u2013301, 2022.",
                "url": null
            }
        },
        {
            "35": {
                "title": "https://sharegpt.com/, 2023.",
                "author": "ShareGPT.",
                "venue": null,
                "url": null
            }
        },
        {
            "36": {
                "title": "Textcaps: a dataset for image captioning with reading comprehension.",
                "author": "Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh.",
                "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part II 16, pages 742\u2013758. Springer, 2020.",
                "url": null
            }
        },
        {
            "37": {
                "title": "Mpnet: Masked and permuted pre-training for language understanding.",
                "author": "Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu.",
                "venue": "Advances in Neural Information Processing Systems, 33:16857\u201316867, 2020.",
                "url": null
            }
        },
        {
            "38": {
                "title": "Laion-400m: Open dataset of clip-filtered 400 million image-text pairs.",
                "author": "Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki.",
                "venue": "arXiv preprint arXiv:2111.02114, 2021.",
                "url": null
            }
        },
        {
            "39": {
                "title": "Stanford alpaca: An instruction-following llama model.",
                "author": "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto.",
                "venue": "https://github.com/tatsu-lab/stanford_alpaca, 2023.",
                "url": null
            }
        },
        {
            "40": {
                "title": "Mass-producing failures of multimodal systems with language models.",
                "author": "Shengbang Tong, Erik Jones, and Jacob Steinhardt.",
                "venue": "arXiv preprint arXiv:2306.12105, 2023.",
                "url": null
            }
        },
        {
            "41": {
                "title": "Eyes wide shut? exploring the visual shortcomings of multimodal llms.",
                "author": "Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie.",
                "venue": "arXiv preprint arXiv:2401.06209, 2024.",
                "url": null
            }
        },
        {
            "42": {
                "title": "Llama 2: Open foundation and fine-tuned chat models.",
                "author": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.",
                "venue": "arXiv preprint arXiv:2307.09288, 2023.",
                "url": null
            }
        },
        {
            "43": {
                "title": "Finetuned language models are zero-shot learners.",
                "author": "Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le.",
                "venue": "arXiv preprint arXiv:2109.01652, 2021.",
                "url": null
            }
        },
        {
            "44": {
                "title": "Visually-augmented language modeling.",
                "author": "Weizhi Wang, Li Dong, Hao Cheng, Haoyu Song, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and Furu Wei.",
                "venue": "arXiv preprint arXiv:2205.10178, 2022.",
                "url": null
            }
        },
        {
            "45": {
                "title": "Instructiongpt-4: A 200-instruction paradigm for fine-tuning minigpt-4.",
                "author": "Lai Wei, Zihao Jiang, Weiran Huang, and Lichao Sun.",
                "venue": "arXiv preprint arXiv:2308.12067, 2023.",
                "url": null
            }
        },
        {
            "46": {
                "title": "Cogvlm: Visual expert for pretrained language models, 2023.",
                "author": "Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang.",
                "venue": null,
                "url": null
            }
        },
        {
            "47": {
                "title": "Chain of thought prompting elicits reasoning in large language models.",
                "author": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou.",
                "venue": "arXiv preprint arXiv:2201.11903, 2022.",
                "url": null
            }
        },
        {
            "48": {
                "title": "Demystifying clip data.",
                "author": "Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer.",
                "venue": "arXiv preprint arXiv:2309.16671, 2023.",
                "url": null
            }
        },
        {
            "49": {
                "title": "The dawn of lmms: Preliminary explorations with gpt-4v (ision).",
                "author": "Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang.",
                "venue": "arXiv preprint arXiv:2309.17421, 9(1):1, 2023.",
                "url": null
            }
        },
        {
            "50": {
                "title": "The devil is in the details: A deep dive into the rabbit hole of data filtering.",
                "author": "Haichao Yu, Yu Tian, Sateesh Kumar, Linjie Yang, and Heng Wang.",
                "venue": "arXiv preprint arXiv:2309.15954, 2023.",
                "url": null
            }
        },
        {
            "51": {
                "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models.",
                "author": "Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.",
                "venue": "arXiv preprint arXiv:2304.10592, 2023.",
                "url": null
            }
        },
        {
            "52": {
                "title": "Gpt-4v (ision) as a generalist evaluator for vision-language tasks.",
                "author": "Xinlu Zhang, Yujie Lu, Weizhi Wang, An Yan, Jun Yan, Lianke Qin, Heng Wang, Xifeng Yan, William Yang Wang, and Linda Ruth Petzold.",
                "venue": "arXiv preprint arXiv:2311.01361, 2023.",
                "url": null
            }
        },
        {
            "53": {
                "title": "The visual task adaptation benchmark.",
                "author": "Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al.",
                "venue": "2019.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.02677v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.4"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "3.4",
            "4.4"
        ]
    },
    "research_context": {
        "paper_id": "2403.02677v1",
        "paper_title": "Finetuned Multimodal Language Models Are High-Quality Image-Text Data Filters",
        "research_background": "### Paper's Motivation\n\nThe paper is motivated by the crucial role that the quality of image-text data plays in the performance of Vision-Language Models (VLMs) and Text-to-Image generation models. Despite the advancements driven by large-scale datasets, such as those used to train models like OpenAI's CLIP, the noise and low quality in web-crawled image-text data persist as significant challenges. Traditional data filtering methods, like CLIPScore, show limitations in subtle visual details and long captions alignment, prompting the need for more refined filtering approaches.\n\n### Research Problem\n\nThe main research problem addressed by the paper is:\n\"Can we adapt strong Multimodal Language Models (MLMs) to generate scores for assessing image-text data quality and outperform CLIPScore for image-text data filtering?\"\n\nThe research further tackles the challenge of finding a balance between the effectiveness and efficiency of filtering methods, especially given the computational constraints of processing billions of image-text pairs.\n\n### Relevant Prior Work\n\nThe paper reviews several pieces of prior work relevant to the research problem:\n1. **Large-Scale Image-Text Datasets and Foundation Models**: Previous work has shown the influence of large-scale datasets on the development of models with extensive capabilities. The success of the CLIP model trained on 400M web-crawled image-text pairs is highlighted as a notable example (e.g., [reference_b29###]).\n\n2. **Data Filtering Techniques**: Traditional filtering techniques before the advent of CLIP were often hand-designed or rule-based (e.g., CC3M and CC12M). The introduction of model-based filtering using CLIPScore [###reference_b14###], measuring cosine similarity between image and text, represented a significant advance.\n\n3. **Limitations of CLIPScore**: Studies [###reference_b40###, ###reference_b41###] have identified shortcomings in CLIPScore, such as its insensitivity to fine-grained visual details and its limited ability to process lengthy text captions.\n\n4. **Advances in Multimodal Language Models (MLMs)**: Recent advancements, especially with models like GPT-4Vision [###reference_b26###], have demonstrated better alignment with human judgment but are computationally prohibitive for large-scale data filtering. The potential of smaller, instruction-tuned MLMs (e.g., LLaVA [###reference_b19###], MiniGPT-4 [###reference_b51###]) is also noted, though these models lack the granularity in their scoring needed for high-quality filtering.\n\nBuilding on these insights, the paper proposes a novel approach combining high-quality instruction tuning data from proprietary models with fine-tuning more accessible, open-source MLMs, aiming to achieve both efficiency and effectiveness in filtering image-text datasets.",
        "methodology": "The proposed method focuses on fine-tuning multimodal language models to act as high-quality image-text data filters. The methodology involves assessing and fine-tuning design choices within this framework to achieve optimal data quality filtering.\n\n### Key Components and Innovations:\n\n1. **Object Detail Fulfillment Metric**:\n   - Among the proposed four metrics to assess data quality, the Object Detail Fulfillment metric is used for filtering. This metric is critical in selecting high-quality subsets from a medium-scale data pool of 128 million entries.\n\n2. **Captioning Model**:\n   - Using LLaVA as the captioning model for transforming images into detailed descriptions results in superior filtering performance compared to other models. \n\n3. **Data Pool Source**:\n   - The research finds that using the CC12M dataset (which contains high-quality images) for sampling image-text pairs outperforms the usage of the DataComp-Medium dataset. This is attributed to the better image quality in CC12M, which enhances the knowledge-intensive instruction tuning process.\n\n4. **Instruction Sampling Buckets**:\n   - Grouping initial instructions into 10 buckets for sampling is more effective than using 100 buckets. This design choice helps in effectively organizing and sampling instructions for further processing.\n\n5. **Teacher Models**:\n   - Different teacher models influence the strengths of the MLM (Multimodal Language Model) filters:\n     - The MLM filter learned from GPT-4 excels in VTAB classification and retrieval tasks.\n     - The MLM filter learned from GPT-4V achieves higher performance in ImageNet-related datasets.\n\n6. **Final Configuration for Future Experiments**:\n   - Two versions of MLM-based filters are reported for future experiments, named MLM-Filter-GPT4 and MLM-Filter-GPT4V, based on the teacher models used.\n\nThis thorough and systematic exploration of design choices ensures that the proposed methodology not only improves data quality filtering but also provides a robust foundation for future experimentation and model development.",
        "main_experiment_and_results": "### Main Experiment Setup:\n\n**Datasets:**\n- The datasets for the main experiment include those on which the vision-language models (VLMs) are pre-trained, filtered using both a baseline filter and the proposed Multimodal Language Model (MLM) filter.\n\n**Baselines:**\n- **Baseline Filter:** This is the existing filter used to preprocess the datasets before fine-tuning the VLMs.\n- **Proposed MLM Filter:** This is the newly introduced filter leveraging fine-tuned MLMs for selecting high-quality image-text data.\n\n**Vision-Language Models (VLM) Architecture:**\n- **CLIP Pre-Training:** A well-known VLM architecture where the baseline filter and the proposed MLM filter are applied to preprocess datasets for model training.\n- **BLIP-2 Pre-Training:** Another VLM architecture adopted for the same filtering and pre-training process to ensure comprehensive evaluation across different models.\n\n**Evaluation Metrics:**\n- Performance of VLMs pre-trained on datasets filtered by the baseline filter versus those filtered by the MLM filter.\n- Human evaluation to measure the correlation between the scores provided by both the baseline CLIP model and the proposed MLM filter model.\n\n### Main Experimental Results:\n\n- **Performance Comparison:** Vision-language models pre-trained on datasets filtered by the proposed MLM filter showed improvements in quality compared to those filtered by the baseline method across different architectures.\n- **Human Evaluation:** There was a significant correlation between the scores generated by the proposed MLM filter model and the baseline CLIP model, suggesting the authenticity and reliability of the MLM filter in selecting high-quality image-text pairs."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To investigate the effects of each design choice on the effectiveness of MLM data filters and determine the optimal configuration for improving filtering performance.",
            "experiment_process": "In the experiment, four major design choices were investigated: (1) adopting either LLaVA or ShareGPT4V as the captioning model; (2) using either CC12M or DataComp Medium 128M as the image-text dataset; (3) using either 10 or 100 grouping buckets for sampling; and (4) selecting either GPT-4 or GPT-4 Vision as the teacher model. The DataComp benchmark was used to evaluate the effectiveness by training a CLIP model on datasets filtered using different MLM configurations and testing zero-shot capabilities across 38 tasks. Only the 'Object Detail Fulfillment' metric was used for filtering during ablation tests.",
            "result_discussion": "Adopting LLaVA as the captioning model and CC12M as the image-text dataset yielded better filtering results. Grouping into 10 buckets outperformed 100 buckets, and different teacher models showed distinct strengths: GPT-4 excelled in VTAB tasks, while GPT-4V was superior for ImageNet-related tasks. Consequently, LLaVA captioner, CC12M datasets, and 10 buckets were set as constants, and further experiments were based on two configurations: MLM-Filter-GPT4 and MLM-Filter-GPT4V.",
            "ablation_id": "2403.02677v1.No1"
        },
        {
            "research_objective": "To analyze the impact of different fractions of selected image samples on pre-training performance of the CLIP model using the DataComp Medium benchmark.",
            "experiment_process": "Five different fractions of the total 128M images from the DataComp medium pool were selected for CLIP model training. The resulting model's performance was measured across ImageNet and an average of 38 datasets to determine the optimal fraction for data selection.",
            "result_discussion": "The top-30% of selected images resulted in the best performance. Even a small addition (5%) of lower quality 'poison' data led to a significant performance drop. This highlights the sensitivity of the model to data quality and reinforces the importance of careful data filtering.",
            "ablation_id": "2403.02677v1.No2"
        },
        {
            "research_objective": "To assess the efficiency differences between the proposed MLM filter and the CLIPScore method.",
            "experiment_process": "The study compared the time costs of generating quality scores for 10k image-text pairs using the proposed MLM filter and the CLIPScore method. The MLM Filter used was the LLaVA-1.5 model with 14B parameters, whereas the CLIPScore employed a CLIP ViT-L/14 model with 492M parameters. The benchmark was conducted using an A100 GPU, and further acceleration was introduced using the TensorRT-LLM toolkit.",
            "result_discussion": "Despite the MLM Filter's larger model size, its score generation time (24.3 mins) was not excessively longer than the CLIPScore method (11.2 mins) due to CLIP's computational redundancy. With the TensorRT-LLM toolkit, the MLM Filter's time was reduced significantly to 6.1 mins for 10k samples, making its efficiency comparable or better than the CLIPScore method.",
            "ablation_id": "2403.02677v1.No3"
        }
    ]
}