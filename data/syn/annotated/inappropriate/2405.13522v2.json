{
    "title": "Beyond Trend and Periodicity: Guiding Time Series Forecasting with Textual Cues",
    "abstract": "This work introduces a novel Text-Guided Time Series Forecasting (TGTSF) task. By integrating textual cues, such as channel descriptions and dynamic news, TGTSF addresses the critical limitations of traditional methods that rely purely on historical data. To support this task, we propose TGForecaster, a robust baseline model that fuses textual cues and time series data using cross-attention mechanisms. We then present four meticulously curated benchmark datasets to validate the proposed task, ranging from simple periodic data to complex, event-driven fluctuations. Our comprehensive evaluations demonstrate that TGForecaster consistently achieves state-of-the-art performance, highlighting the transformative potential of incorporating textual information into time series forecasting. This work not only pioneers a novel forecasting task but also establishes a new benchmark for future research, driving advancements in multimodal data integration for time series models.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Time series forecasting (TSF) is crucial in many fields, thereby attracting significant attention from both academia and industry. Despite extensive research dedicated to this task, recent studies have shown that simple linear models (Zeng et al., 2023  ###reference_b17###; Xu et al., 2023  ###reference_b16###; Toner & Darlow, 2024  ###reference_b10###) that barely extract trend and periodicity information from time series data frequently achieve performance close to that of state-of-the-art complex models (Nie et al., 2023  ###reference_b8###; Liu et al., 2023  ###reference_b7###; Jin et al., 2023  ###reference_b3###), indicating that current approaches have reached a saturation point, possibly overfitting historical data.\nTo achieve higher forecasting accuracy, it is essential to provide models with additional, critical information\u2014such as holiday information, consumer sentiment, and climate changes (depending on the specific forecasting task)\u2014that are not present in the time series data itself. Without this additional information, models cannot extract the necessary causal relationships, leading to oversimplification or overfitting. Moreover, the lack of insight about channels and underlying systems prevents the modeling of inter-channel relationships, often resulting in unreliable forecasts.\nAs such valuable information is usually hard to quantify as auxiliary time series data, we consider them as textual cues and introduce a novel task for TSF: Text-Guided Time Series Forecasting (TGTSF). TGTSF leverages two main components: channel descriptions and dynamic news messages. Channel descriptions provide static knowledge about the underlying systems, enabling the model to differentiate between channels and better understand inter-channel correlations. News messages offer dynamic and external insights, helping the model adjust to shifts in data distribution caused by external events. By modeling the joint distribution of future values conditioned on these textual cues, TGTSF has the potential to enhance forecasting accuracy and reliability. Furthermore, TGTSF enables scenario-based forecasting, which is particularly valuable for business decision-making. For instance, a company can forecast sales based on the assumption of a successful marketing campaign, incorporating relevant news and channel-specific descriptions to refine predictions.\nWe then introduce TGForecaster, a simple Transformer-based baseline multimodal model designed to realize the potential of the proposed TGTSF framework. Utilizing a cross-attention mechanism, TGForecaster fuses information from textual cues and time series data to guide the forecasting process. To rigorously evaluate and demonstrate the efficacy of our model, we carefully craft four benchmark datasets categorized into synthetic, captioned existing, and real-world datasets. Each dataset is uniquely designed to validate the task and test the model\u2019s capability from different perspectives. Specifically, the synthetic dataset features simple yet variable time series patterns, such as sinusoidal waves with random frequency changes, to test the model\u2019s ability to utilize textual cues in forecasting and mitigating distribution shifts. Two existing datasets are captioned with different levels of textual information, used to challenge the model in complex, non-periodic environments by integrating detailed external data. Lastly, our real-world dataset, collected from actual sales scenarios, showcases the practical application value of TGTSF.\nExperimental results show that TGForecaster consistently demonstrates state-of-the-art performance across our datasets, affirming the soundness of the TGTSF task definition. Ablation studies reveal that without textual assistance, TGForecaster\u2019s performance reverts to that of PatchTST (Nie et al., 2023  ###reference_b8###), its time series encoder backbone, underscoring that the performance enhancement is driven by the additional information provided by textual data, not merely by a more sophisticated architecture.\nOur contributions are summarized as follows:\nWe identify the roadblock of TSF as information insufficiency and propose TGTSF as a new forecasting approach that integrates textual data to enrich the models with external causal information and system knowledge.\nWe establish the first TGTSF benchmark containing four uniquely designed datasets.\nWe design a simple baseline model for TGTSF, TGForecaster. TGForecaster achieves state-of-the-art performance by utilizing textual information and effectively validates our proposed TGTSF task."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Insights and Motivation",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Existing TSF Solutions Suffer from Information Insufficiency",
            "text": "Time series forecasting typically involves predicting future segments based on historical data, specifically forecasting subsequent time series segments from prior ones. However, the inherent sparsity of information within time series data makes achieving accurate forecasts challenging, or even impossible at times. Standard decomposition of time series data identifies three main components: trend, periodicity, and noise (Box et al., 2015  ###reference_b2###). The noise component is intrinsically unpredictable. Trends, which are slowly changing patterns, are often impacted by external events that influence the underlying system, making it less predictable without external information. As a result, the periodicity component is usually the main source of reliable prediction.\nRecent advancements in linear models have validated these challenges. For instance, the DLinear (Zeng et al., 2023  ###reference_b17###) model utilizes a simple linear layer to effectively capture basic periodic patterns, outperforming most complex Transformer-based models. Similarly, FITS (Xu et al., 2023  ###reference_b16###), designed to simply extract periodicity, achieves comparable or superior results to state-of-the-art (SOTA) methods using fewer than  parameters. Further research demonstrates that employing a closed-form forecasting matrix (Toner & Darlow, 2024  ###reference_b10###), calculated from training data, can produce SOTA outcomes. These findings indicate that the minimal information in time series data allows models to recognize patterns with few or no learnable parameters, suggesting that training larger models with insufficient information can lead to severe overfitting.\nThe issue of information insufficiency extends beyond the data itself to the absence of external information. External factors like holidays, consumer sentiment, or climate changes can significantly alter patterns in time series data, affecting variables such as electricity usage, sales trends, or regional weather conditions. However, these factors are not contained within the time series data, hindering models from incorporating causal relationships. This deficiency can compromise model training, causing models to either learn overly simplified average shortcut or overfit on the training data, as shown in Fig. 1  ###reference_###. For example, if a dataset frequently presents days without rain, the model may consistently predict no rain for future days. However, rainfall predictions are largely influenced by external factors like climate change, not included in the time series, leading models to opt for shortcuts that minimize overall loss by predicting the most common outcome.\nMoreover, the lack of knowledge about the underlying systems exacerbates the problem of information insufficiency, especially in datasets with multiple time series channels. Without a deeper understanding of these systems or the characteristics of each channel, it is challenging to leverage such information for improved modeling performance. The indistinctiveness among channels complicates modeling efforts, making it difficult to accurately capture and model correlations.\nThese challenges underscore the need to integrate external information to address the fundamental limitations of time series forecasting methods."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "RIN & Weight Sharing are Compromises under Information Insufficiency",
            "text": "Researchers have developed various methods to tackle information insufficiency in time series forecasting. One such method is Reversible Instance Normalization (RIN) (Kim et al., 2021  ###reference_b4###), which normalizes each instance to align mean and variance across the dataset, as shown in Fig. 1  ###reference_###. RIN effectively reduces distribution shifts caused by trends shifting or external events, forcing the model to learn a unified distribution. While it standardizes data inputs, RIN also strips away essential information such as trend intensity and amplitude variations, which diminishes the model\u2019s capacity to detect relative biases or amplitude shifts beyond the training scope. However, RIN merely hides the issue of information insufficiency by neutralizing the effects of external events.\n\n###figure_1### Another strategy is weight sharing (Nie et al., 2023  ###reference_b8###; Zeng et al., 2023  ###reference_b17###; Xu et al., 2023  ###reference_b16###). This technique treats all input channels as equivalent, leveraging common patterns across all channels to alleviate the problem of limited training samples. In other words, this approach enhances performance by learning from the shared periodicities across all the channels, typical within a single dataset. However, since the model is trained to minimize loss across all channel distributions with a single set of weights, it prioritizes common patterns, failing to capture unique distributions for each channel, as illustrated in Fig. 1  ###reference_###, which may lead to averaged results. These average results further reveal that the model lacks sufficient information to distinguish between different channels effectively.\nDespite these advancements, both methods provide only temporary solutions to the pervasive issue of information insufficiency. They do not resolve the core problem and still suffer from the limitations imposed by insufficient information."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Motivation for The Proposed TGTSF Framework",
            "text": "External factors significantly impact time series behavior such as trending and periodicity. A line of work apply and investigate how to embed the external information as a new channel, such as Autoformer (Wu et al., 2021  ###reference_b14###), FedFormer (Zhou et al., 2022a  ###reference_b19###) add auxilary channels to indicate the day-of-week or even the exact date. Temporal Fusion Transformer (TFT) (Lim et al., 2021  ###reference_b6###) and TimeXer (Wang et al., 2024  ###reference_b13###), however, focus on how to select the given variables and how to embed and fuse them with other channels. However, these methods only bring limited performance boost. Basically, they are still trying to squeeze out the information in the original time series dataset which contains limited information itself.\nHowever, other forms of information such as news articles, social media updates, global climate change report are crucial for accurately predicting time series pattern in domains like sales and local weather report. These information are often in forms of text and hard to be quantified as numerical data alone. Moreover, the real-world application of forecasting frequently requires conditional predictions based on specific scenarios. For example, businesses may need to assess the impact of a marketing campaign on future sales, necessitating a deep understanding of external textual factors to forecast outcomes effectively.\nTo address these challenges, it is essential to include text as an additional input modality. This approach not only allows models to react to historical data but also to proactively adapt to potential future events and trends. It enhances forecasting by providing insights into the underlying dynamics and causal relationships often missing from time series data alone. Guiding time series forecasting with textual data also helps clarify inter-channel relationships, reflecting systemic correlations or domain-specific knowledge that are crucial for accurate predictions. This textual context enhances models\u2019 ability to interpret complex data interactions, vital in environments where data channels are interconnected.\nIn summary, guiding time series forecasting with textual data addresses the critical issue of information insufficiency. This approach transcends the limitations of traditional TSF methodologies, enhancing model accuracy and effectiveness. By incorporating textual insights, forecasting becomes more robust and practically valuable for real-world applications."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "TGTSF Task Formulation",
            "text": "We introduce a novel task within the domain of time series analysis, termed Text-Guided Time Series Forecasting (TGTSF), designed to address the prevalent issue of information insufficiency in time series forecasting.\nAs illustrated in Fig. 1  ###reference_###, text guidance in TGTSF operates on two aspects. Firstly, channel descriptions serve as identifiers for each channel, aiding the model in distinguishing between them while learning shared features. These descriptions can also incorporate static knowledge about the underlying system, enhancing the model\u2019s ability to recognize inter-channel correlations. Secondly, news messages provide dynamic and external insights into known (in training) or hypothesized (in inference) future events, which assist the model in adapting to event-driven distribution shifts. 111In creating a TGTSF dataset, it is advisable to avoid directly generating the description out of the forecasting horizon time series pattern as news messages, as this could lead to information leakage. News messages should instead contain relevant, known information from other sources.\nThe TGTSF task is defined by three principal inputs: the time series data, news messages, and channel descriptions. The challenge lies in the fact that news items are not directly linked to specific time series channels. Consequently, the model must infer the impact of each news item on individual channels based on their descriptions. This setup not only capitalizes on dynamic events but also integrates domain knowledge via channel descriptions. Such integration is crucial for the model to comprehend and learn the spatial correlations among channels, thereby enhancing the forecasting accuracy with nuanced contextual understanding.\nFormally, the TGTSF task seeks to model the potentially complex joint distribution of future values in a multi-channel sequence. This modeling involves the integration of historical time series data, textual information from news messages, and channel descriptions over a specified look-back window:\nwhere  denotes the look-back window length,  stands for the prediction length, the  means the channel number,  is the total number of channels."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "TGForecaster: A Baseline Model for The TGTSF Task",
            "text": "To validate the TGTSF task, we developed TGForecaster, a streamlined transformer model designed for multimodal fusion with cross-attention. Illustrated in Fig. 2  ###reference_###, this model harnesses textual information to enhance the accuracy and relevance of time series forecasting. It employs a reimplemented PatchTST encoder for processing time series data and leverages off-the-shelf, pretrained text models for pre-embedding textual inputs into vector sequences across the time dimension, allowing for effective modality fusion in the embedding space.\n\n###figure_2### A key innovation in TGForecaster is the integration of simple linear regression without external input to achieve the \u2019text guided channel independent\u2019. This layer is pivotal in fusing news content with channel descriptions. Taking cues from the attention mechanism\u2019s original use in recommendation systems, we treat the news as both the key and value, with the channel descriptions serving as queries. Through this mechanism, we calculate the relevance of each news item to every channel. The resulting scores are then used to aggregate the news, culminating in a composite embedding for each channel. Another crucial aspect of our design is the modality fusion, achieved through a separate layer of simple linear regression without external input where text embeddings function as query, while temporal embeddings act as the key and value. This ensures that the output dimensions align naturally with the query which has the length of forecasting horizon. The attention maps for these two simple linear regressions are provided in Appendix E  ###reference_###. These maps compellingly demonstrate that the layers perform in accordance with our design specifications.\nAs decoder, we use the token-wise decoding to avoid the overfitting on the last layer as observed in previous work (Lee et al., 2023  ###reference_b5###). Each token is project back to its original patch length and weighted sumed to get the final forecasting result. Notably, TGForecaster does not incorporate the RIN. Our experiments, detailed in Appendix D  ###reference_###, demonstrate that the inclusion of RIN degrades performance when sufficient information is already available.\nThis design strategy not only enables the integration of textual data into the time series forecasting model but also facilitates a direct comparison of the impact of text on forecasting accuracy."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "TGTSF Benchmark Datasets",
            "text": "We have designed four TGTSF benchmark datasets across three categories: synthetic, captioned existing, and real-world datasets. We plan to release these TGTSF datasets along with captions, captioning scripts, and pre-embeddings, adhering to all applicable intellectual property constraints."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Synthetic Toy Dataset",
            "text": "The Synthetic Toy Dataset evaluates models\u2019 ability to utilize textual information for time series forecasting. It features diverse patterns, including segments of sinusoidal waves with variations in frequency, amplitude, and trend. Textual descriptions resembling news reports precede each change point, outlining upcoming alterations to prompt the TGTSF model to adjust its forecasts accordingly. This setup tests the model\u2019s effectiveness in mitigating distribution shifts and harnessing textual cues for improved forecasting accuracy."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Electricity-Captioned Dataset",
            "text": "The Electricity-Captioned Dataset builds on a commonly-used electricity utility dataset (Zhou et al., 2021  ###reference_b18###) that tracks appliance usage in an office building, with daily patterns largely influenced by whether it is a workday. This dataset has been enhanced with straightforward, publically-known textual information such as the type of day (e.g., day of the week, public holiday, workday), using the channel name as the descriptor. This enhancement serves a dual purpose: to illustrate whether minimal textual data can improve prediction accuracy and to facilitate a direct performance comparison with traditional time series models."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Weather-Captioned Dataset",
            "text": "The Weather-Captioned Dataset is designed to overcome the limitations of commonly used datasets in time series forecasting (TSF), especially the lack of predictability and periodicity in variables like rainfall, wind speed, and direction. By incorporating external climate information, this dataset demonstrates how integrating textual data can address the challenges of information insufficiency in traditional TSF, enabling enhanced control over forecasting outcomes.\nOriginally limited to a single year (Zhou et al., 2021  ###reference_b18###), we have expanded the weather dataset to encompass a decade of detailed weather data from 2014 to 2023, sourced consistently from the same weather station. The Weather-Captioned dataset now includes weather forecasting reports from a publicly accessible service, detailing the climate conditions in Jena, Germany, where the weather station is approximately located. These reports, updated every six hours and daily, cover a range of parameters including weather conditions, temperature, humidity, wind speed, and direction, providing a comprehensive overview for our analysis. We directly use the channel name as channel description.\nTo transform this quantitative data into actionable insights for TSF, we employed GPT-4 to summarize each forecasting report into a set of seven thematic sentences, as demonstrated in Tab. 1  ###reference_###. GPT-4 is prompted to avoid specific numerical details from the original reports to prevent information leakage and to simplify the model\u2019s learning process. To further enrich the dataset, GPT-4 generated three distinct summary versions for each report, resulting in  possible unique textual captions for each time step. The full details of the prompt used for generating these summaries are provided in the appendix.\nWe divided this extensive dataset into two subsets for detailed analysis: Weather-Captioned-Medium, covering data from 2014 to 2018, and Weather-Captioned-Large, which includes the entire dataset spanning ten years. This dataset encompasses over 525,600 weather data records across 21 different channels, with more than 85,000 unique sentences. This vast corpus provides millions of potential training samples for captioned time series forecasting. Further information is detailed in the appendix."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Steam Game Dataset",
            "text": "A key application of TGTSF is in sales forecasting and decision-making. To illustrate this, we compiled data on online gamer counts from several of the most popular games on Steam, the leading online video game distribution platform. This dataset also includes records of all game updates and events. The time series exhibits a weekly periodicity, with gamer activity typically peaking on weekends. Announcements from game developers frequently trigger spikes in activity, reflecting player enthusiasm for new content. However, the varied management strategies of game developers and the diverse reactions of gamers to updates introduce significant distribution shifts, even when textual information is considered. This variability makes the Steam Dataset the most challenging and intricate dataset within the TGTSF framework. 222Due to intellectual property constraints, we may not be able to directly release this dataset. Detailed instructions are available in the appendix for researchers interested in replicating or extending our data."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We evaluate the TGForecaster model across four datasets to demonstrate the feasibility of the TGTSF task on the proposed datasets. As discussed in Section 5  ###reference_###, each dataset is uniquely designed to test the task from different perspectives.\nTGForecaster is benchmarked against state-of-the-art (SOTA) methods including DLinear, FITS, PatchTST, iTransformer, and Time-LLM (Zeng et al., 2023  ###reference_b17###; Xu et al., 2023  ###reference_b16###; Nie et al., 2023  ###reference_b8###; Liu et al., 2023  ###reference_b7###; Jin et al., 2023  ###reference_b3###). We specifically contrast it with the linear-based models DLinear and FITS to illustrate how TGForecaster surpasses periodicity-focused models. Comparisons with PatchTST highlight how textual information can enhance forecasting performance, even when using the same time series encoder. Time-LLM \"reprograms\" the LLaMa2 (Touvron et al., 2023  ###reference_b11###), a large language model, which gives it capability of understanding text information. We use it as a baseline of multi-modal model. Specific settings differences, if any, are noted accordingly.\nAll of the models are following the same experimental setup with prediction length  and LBW length .\nTable 2  ###reference_### presents the performance comparison of various models on the toy dataset, with TGForecaster significantly outperforming all baseline models. Specifically, TGForecaster achieved an 80% improvement in Mean Squared Error (MSE) over the best performing transformer-based model, and a 96% increase over DLinear. These results underscore the considerable benefits of integrating textual guidance in TSF. Notably, a version of TGForecaster without news data, which lacks the ability to utilize auxiliary textual information, demonstrated substantially lower performance. This underscores the essential role of text in enhancing forecast accuracy.\nFigure 3  ###reference_### illustrates a segment of the toy dataset where the frequency changes within the forecasting horizon. In this visualization, the PatchTST model maintains the frequency observed in the look-back window, indicating an inability to adapt to new frequencies. Similarly, DLinear displays a collapsed pattern. Notably, without news input, TGForecaster shows the same behavior of PatchTST, suggesting that it relies solely on its time series encoder in the absence of textual cues. The models without external data also highlights the decreasing amplitude of predictions, a tendency to revert to safer, average predictions when faced with uncertainty in the far future\u2014an issue known as the \u2019average shortcut\u2019 phenomenon discussed in Section 2.1  ###reference_###. Conversely, with the integration of external text information eliminating uncertainty, TGForecaster adeptly adapts to new frequencies at the appropriate moments, demonstrating the substantial benefits of incorporating textual data into the forecasting process.\n###figure_3### ###figure_4### ###figure_5### ###figure_6### We follow the experiment settings in previous works as follows: forecasting horizon , look back window length of 288. For fair comparison, we directly compare with the results report in the baseline original paper. And the TGForecaster is trained with the captioned version. On this dataset, we also test the impact of a shorter look-back window on the TGForecaster. We will report accordingly in Tab. 2  ###reference_###.\nAs depicted in Tab. 2  ###reference_###, TGForecaster demonstrates SOTA performance on the Electricity dataset, particularly effective at shorter forecasting horizons using minimal textual cues. However, TimeLLM slightly outperforms TGForecaster over longer forecasting periods. Given TimeLLM\u2019s backbone is a large language model that also incorporates textual information, its edge may stem from its use of basic date and statistical data within its text inputs.\nWe follow the experiment settings in previous works as follows: forecasting horizon , look back window length of 360. We trained all other models on the Weather-Medium and -large dataset with their setting on the original weather dataset accordingly. And the TGForecaster is trained with the captioned version. We will report accordingly in Tab. 3  ###reference_###.\nAs demonstrated in Tab. 3  ###reference_###, TGForecaster significantly outperforms other models across both the Weather-Medium and Weather-Large datasets, substantiating the efficacy of incorporating external text information in addressing the information insufficiency inherent in TSF models. Notably, despite its simplicity, the FITS model performs exceptionally well, second only to TGForecaster. This suggests that simply adding more training time series to complex models does not necessarily lead to improved performance, as more time series does not necessarily contain more information. The results highlight that strategic integration of textual data can provide a more substantial performance boost than merely increasing the quantity of time series data. Further, we also report the channel-wise performance in Appendix B  ###reference_###. We notice a groundbreaking performance boost of over 60% on certain channels which were not predictable with historical time series data alone.\nWe conducted ablation studies to evaluate the impact of different embedding models and the integration of textual inputs on the performance of TGForecaster. We tested three embedding models: OpenAI Embedding (ope,  ###reference_b1###), paraphrase-MiniLM-L6 (Wang et al., 2020  ###reference_b12###), and all-mpnet-base (Song et al., 2020  ###reference_b9###)333Sentence Transformer: https://huggingface.co/sentence-transformers  ###reference_rs###. The results, presented in Tab. 4  ###reference_###, indicate minimal performance differences between the embedding models. Notably, the removal of channel descriptions led to a significant performance decrease, underscoring the model\u2019s reliance on this feature for distinguishing between channels. Similarly, omitting news text resulted in performance dropping to levels comparable to the baseline PatchTST model, confirming that the improvements in forecasting accuracy are primarily driven by the inclusion of external textual information. This observation validates our hypothesis that textual data plays a crucial role in compensating for information deficiencies in traditional time series forecasting.\nFig. 4  ###reference_### visualizes three channels from the Weather-Caption-Medium dataset, full result see Appendix C  ###reference_###. The first channel, atmospheric pressure (p), which is influenced by regional climate conditions, typically exhibits slowly changing trends that are challenging for TSF models to predict due to their subtle fluctuations without periodicity. However, with the integration of external information, TGForecaster accurately predicts these trends, whereas PatchTST tends to predict a constant average value, failing to capture the gradual changes.\n\n###figure_7### The second channel, time of raining, measured in seconds per 10 minutes, presents unique challenges: it lacks periodicity, displays no causality, and often exists in a binary state. Traditional models like PatchTST often fail, typically predicting no rainfall - the averaged shortcut. TGForecaster adjusts its forecasts based on text prompts about upcoming rain. However, it sometimes misses predictions when the text weather forecasting report does not perfectly align with actual weather events, as noted with the missed second rainfall period.\nThe third channel, SWDR (solar radiation), directly reflects the amount of solar power reaching the ground. TGForecaster accurately forecasts SWDR, even though these are not directly mentioned in the news texts. It effectively infers inter-channel dependencies, enabling it to predict shifts in SWDR associated with weather changes, in contrast to PatchTST\u2019s basic waveform predictions.\nAdditionally, a controllability test involving the swapping of news inputs for the second and fourth days illustrates TGForecaster\u2019s responsive accuracy: it predicts rain on the fourth day and clear conditions on the second, aligning forecasts with the altered news data. This test underscores the model\u2019s adaptability and the effectiveness of text-guided forecasting."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Evaluation on Toy Dataset",
            "text": "All of the models are following the same experimental setup with prediction length  and LBW length .\nTable 2  ###reference_###  ###reference_### presents the performance comparison of various models on the toy dataset, with TGForecaster significantly outperforming all baseline models. Specifically, TGForecaster achieved an 80% improvement in Mean Squared Error (MSE) over the best performing transformer-based model, and a 96% increase over DLinear. These results underscore the considerable benefits of integrating textual guidance in TSF. Notably, a version of TGForecaster without news data, which lacks the ability to utilize auxiliary textual information, demonstrated substantially lower performance. This underscores the essential role of text in enhancing forecast accuracy.\nFigure 3  ###reference_###  ###reference_### illustrates a segment of the toy dataset where the frequency changes within the forecasting horizon. In this visualization, the PatchTST model maintains the frequency observed in the look-back window, indicating an inability to adapt to new frequencies. Similarly, DLinear displays a collapsed pattern. Notably, without news input, TGForecaster shows the same behavior of PatchTST, suggesting that it relies solely on its time series encoder in the absence of textual cues. The models without external data also highlights the decreasing amplitude of predictions, a tendency to revert to safer, average predictions when faced with uncertainty in the far future\u2014an issue known as the \u2019average shortcut\u2019 phenomenon discussed in Section 2.1  ###reference_###  ###reference_###. Conversely, with the integration of external text information eliminating uncertainty, TGForecaster adeptly adapts to new frequencies at the appropriate moments, demonstrating the substantial benefits of incorporating textual data into the forecasting process.\n###figure_8### ###figure_9### ###figure_10### ###figure_11###"
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Evaluation on Electricity-Captioned Dataset",
            "text": "We follow the experiment settings in previous works as follows: forecasting horizon , look back window length of 288. For fair comparison, we directly compare with the results report in the baseline original paper. And the TGForecaster is trained with the captioned version. On this dataset, we also test the impact of a shorter look-back window on the TGForecaster. We will report accordingly in Tab. 2  ###reference_###  ###reference_###.\nAs depicted in Tab. 2  ###reference_###  ###reference_###, TGForecaster demonstrates SOTA performance on the Electricity dataset, particularly effective at shorter forecasting horizons using minimal textual cues. However, TimeLLM slightly outperforms TGForecaster over longer forecasting periods. Given TimeLLM\u2019s backbone is a large language model that also incorporates textual information, its edge may stem from its use of basic date and statistical data within its text inputs."
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "Evaluation on Weather-Captioned Dataset",
            "text": "We follow the experiment settings in previous works as follows: forecasting horizon , look back window length of 360. We trained all other models on the Weather-Medium and -large dataset with their setting on the original weather dataset accordingly. And the TGForecaster is trained with the captioned version. We will report accordingly in Tab. 3  ###reference_###  ###reference_###.\nAs demonstrated in Tab. 3  ###reference_###  ###reference_###, TGForecaster significantly outperforms other models across both the Weather-Medium and Weather-Large datasets, substantiating the efficacy of incorporating external text information in addressing the information insufficiency inherent in TSF models. Notably, despite its simplicity, the FITS model performs exceptionally well, second only to TGForecaster. This suggests that simply adding more training time series to complex models does not necessarily lead to improved performance, as more time series does not necessarily contain more information. The results highlight that strategic integration of textual data can provide a more substantial performance boost than merely increasing the quantity of time series data. Further, we also report the channel-wise performance in Appendix B  ###reference_###  ###reference_###. We notice a groundbreaking performance boost of over 60% on certain channels which were not predictable with historical time series data alone.\nWe conducted ablation studies to evaluate the impact of different embedding models and the integration of textual inputs on the performance of TGForecaster. We tested three embedding models: OpenAI Embedding (ope,  ###reference_b1###  ###reference_b1###), paraphrase-MiniLM-L6 (Wang et al., 2020  ###reference_b12###  ###reference_b12###), and all-mpnet-base (Song et al., 2020  ###reference_b9###  ###reference_b9###)333Sentence Transformer: https://huggingface.co/sentence-transformers  ###reference_rs###  ###reference_rs###. The results, presented in Tab. 4  ###reference_###  ###reference_###, indicate minimal performance differences between the embedding models. Notably, the removal of channel descriptions led to a significant performance decrease, underscoring the model\u2019s reliance on this feature for distinguishing between channels. Similarly, omitting news text resulted in performance dropping to levels comparable to the baseline PatchTST model, confirming that the improvements in forecasting accuracy are primarily driven by the inclusion of external textual information. This observation validates our hypothesis that textual data plays a crucial role in compensating for information deficiencies in traditional time series forecasting.\nFig. 4  ###reference_###  ###reference_### visualizes three channels from the Weather-Caption-Medium dataset, full result see Appendix C  ###reference_###  ###reference_###. The first channel, atmospheric pressure (p), which is influenced by regional climate conditions, typically exhibits slowly changing trends that are challenging for TSF models to predict due to their subtle fluctuations without periodicity. However, with the integration of external information, TGForecaster accurately predicts these trends, whereas PatchTST tends to predict a constant average value, failing to capture the gradual changes.\n\n###figure_12### The second channel, time of raining, measured in seconds per 10 minutes, presents unique challenges: it lacks periodicity, displays no causality, and often exists in a binary state. Traditional models like PatchTST often fail, typically predicting no rainfall - the averaged shortcut. TGForecaster adjusts its forecasts based on text prompts about upcoming rain. However, it sometimes misses predictions when the text weather forecasting report does not perfectly align with actual weather events, as noted with the missed second rainfall period.\nThe third channel, SWDR (solar radiation), directly reflects the amount of solar power reaching the ground. TGForecaster accurately forecasts SWDR, even though these are not directly mentioned in the news texts. It effectively infers inter-channel dependencies, enabling it to predict shifts in SWDR associated with weather changes, in contrast to PatchTST\u2019s basic waveform predictions.\nAdditionally, a controllability test involving the swapping of news inputs for the second and fourth days illustrates TGForecaster\u2019s responsive accuracy: it predicts rain on the fourth day and clear conditions on the second, aligning forecasts with the altered news data. This test underscores the model\u2019s adaptability and the effectiveness of text-guided forecasting."
        },
        {
            "section_id": "6.4",
            "parent_section_id": "6",
            "section_name": "Evaluation on Steam-100 Dataset",
            "text": "We evaluate the performance of the TGForecaster on our Steam-100 dataset, utilizing an input window of 60 days and an output horizon of 14 days. The findings indicate that the TGForecaster outperforms baseline models such as PatchTST, achieving a performance enhancement of over . This superior performance is consistently observed across over  of all games, ranking as the best among all evaluated methods. Given the significant stylistic variations among different game developers and the potential risk of temporal distribution shifts, traditional time series forecasting methods often struggle to capture the commonalities in temporal features. In contrast, the TGForecaster leverages textual information to significantly augment its predictive capabilities. We report the full result in the Appendix G  ###reference_###."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusions and Discussion",
            "text": "This paper addresses a critical roadblock in time series forecasting: information insufficiency. We introduced Text-Guided Time Series Forecasting (TGTSF), a new approach that integrates textual cues to enrich the models with external information and system knowledge. We developed and released four TGTSF datasets, each crafted to validate different aspects of the task and model. Our straightforward yet effective TGForecaster model demonstrates that textual guidance can significantly enhance time series modeling by mitigating the average predictions typically resulting from information scarcity.\nWhile the TGForecaster effectively validates the TGTSF task, it does not fully comprehend the semantics of the text, such as extracting correlations among channels automatically. Future work will focus on advancing the model\u2019s semantic understanding and its ability to autonomously discern intricate relationships within the data."
        }
    ],
    "url": "http://arxiv.org/html/2405.13522v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2",
            "2.3"
        ],
        "methodology_sections": [
            "3",
            "4"
        ],
        "main_experiment_and_results_sections": [
            "6",
            "6.1",
            "6.2",
            "6.3",
            "6.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "1",
            "3",
            "6",
            "6.3"
        ]
    },
    "research_context": {
        "paper_id": "2405.13522v2",
        "paper_title": "Beyond Trend and Periodicity: Guiding Time Series Forecasting with Textual Cues",
        "research_background": "**Paper Motivation:**\nThe motivation behind this paper arises from the observation that state-of-the-art time series forecasting (TSF) models, despite their complexity, often perform similarly to simple linear models that only extract basic trend and periodicity information. This similarity in performance suggests that current TSF methodologies may have hit a saturation point and might be overfitting to historical data. There is a need to go beyond these limitations and improve forecasting accuracy by incorporating essential external information such as holiday schedules, consumer sentiment, and climatic changes, which are not present in the time series data itself. The paper is driven by the idea of enriching TSF models with additional, critical information via textual cues to move beyond the limitations of current models.\n\n**Research Problem:**\nThe central research problem this paper addresses is the limitation of current TSF approaches that rely solely on historical data, leading to oversimplification or overfitting and lacking the necessary causal insights. To tackle this, the paper introduces a novel task, Text-Guided Time Series Forecasting (TGTSF), which incorporates textual cues such as channel descriptions and dynamic news messages to enhance the models' ability to forecast accurately. These textual cues provide both static and dynamic insights, enabling the model to better adjust to distribution shifts and understand inter-channel relationships.\n\n**Relevant Prior Work:**\nPrevious studies have demonstrated that simple linear models focusing on trend and periodicity often achieve performance comparable to more complex models, indicating a saturation in current TSF methodologies:\n- **Zeng et al. (2023)**, **Xu et al. (2023)**, and **Toner & Darlow (2024)** showed that simple linear models perform close to state-of-the-art models.\n  \nComplex models, while advanced, still face challenges such as overfitting and lack of causal information from external sources:\n- **Nie et al. (2023)**, **Liu et al. (2023)**, and **Jin et al. (2023)** developed state-of-the-art TSF models that might be reaching performance saturation.\n\nThe paper builds on this foundational understanding by proposing a new methodology that integrates additional textual cues to provide richer contextual information and improve TSF performance.",
        "methodology": "The proposed methodology in \"Beyond Trend and Periodicity: Guiding Time Series Forecasting with Textual Cues\" introduces a new paradigm in time series forecasting termed **Text-Guided Time Series Forecasting (TGTSF)**. This innovative approach aims to enrich traditional time series analysis by addressing the prevalent issue of information insufficiency. Here\u2019s a detailed breakdown of the key components and innovations:\n\n### Key Components:\n\n1. **Channel Descriptions**:\n   - **Function**: Serve as identifiers for each channel in the time series data.\n   - **Purpose**: Aid the model in distinguishing between different channels while learning shared features.\n   - **Static Knowledge Integration**: These descriptions can encapsulate static knowledge about the underlying system, allowing the model to recognize inter-channel correlations.\n\n2. **News Messages**:\n   - **Function**: Provide dynamic, external insights about known (in training) or hypothesized (in inference) future events.\n   - **Purpose**: Help the model adapt to event-driven distribution shifts, incorporating dynamic event information.\n\n### Methodology:\n\n1. **Text Guidance Operation**:\n   - **Dual Aspect**:\n     - **Channel Descriptions**: Help in identifying channels and infer shared features, integrating static domain knowledge.\n     - **News Messages**: Offer dynamic event information for adapting to distribution shifts in the time series data.\n   - **Avoidance of Information Leakage**: Ensure news messages are derived from relevant external sources rather than the pattern of the forecasting horizon time series to prevent information leakage.\n\n2. **TGTSF Task Definition**:\n   - **Three Principal Inputs**:\n     - **Time Series Data**: Historical numerical data across multiple channels.\n     - **News Messages**: External textual data providing context about events affecting the time series.\n     - **Channel Descriptions**: Textual information characterizing each channel and facilitating the learning of spatial correlations among channels.\n   \n   - **Challenge**: \n     - News items are not directly linked to specific time series channels.\n     - The model must infer the impact of each news item on individual channels by using the channel descriptions, thereby integrating domain knowledge and contextual understanding.\n\n3. **Joint Distribution Modeling**:\n   - **Objective**: Model the potentially complex joint distribution of future values in a multi-channel sequence.\n   - **Inputs**: \n     - **Look-Back Window Length** (): Span of historical time series data considered.\n     - **Prediction Length** (): The duration of the future that needs to be forecasted.\n     - **Channel Number** (): Identifier for each individual channel.\n     - **Total Number of Channels** (): Represents the comprehensive set of time series channels being modeled.\n\n### Innovation:\n\n- **Textual Cues Integration**: Innovative integration of textual information (both static and dynamic) with traditional time series data, providing richer contextual insights for forecasting.\n- **Enhanced Forecasting Accuracy**: By understanding and leveraging both inter-channel correlations (through descriptions) and dynamic external events (through news), the model achieves a more precise and contextually informed forecast.\n\nOverall, the TGTSF methodology represents a significant advancement in time series forecasting, emphasizing the nuanced contextual understanding driven by enriched, multi-modal data inputs.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Experimental Setup\nThe main experiment evaluates the TGForecaster model across four datasets to demonstrate its feasibility in the TGTSF task. The datasets are:\n1. Toy dataset\n2. Electricity dataset\n3. Weather-Medium dataset\n4. Weather-Large dataset\n\n**Baselines**: TGForecaster is benchmarked against state-of-the-art (SOTA) models, including:\n- **DLinear**\n- **FITS**\n- **PatchTST**\n- **iTransformer**\n- **Time-LLM**\n\nEach model is evaluated under the same setup with specified prediction length and look-back window length (LBW).\n\n**Evaluation Metrics**:\n- **Mean Squared Error (MSE)** is used as the primary metric to compare model performance.\n\n**Notable Details**:\n- **Toy Dataset**: TGForecaster shows significant improvement, achieving an 80% improvement in MSE over the best transformer-based model, and a 96% increase over DLinear.\n- **Electricity Dataset**: TGForecaster demonstrates SOTA performance, especially effective at shorter forecasting horizons with minimal textual cues. However, TimeLLM slightly surpasses TGForecaster over longer periods.\n- **Weather-Medium and Weather-Large Datasets**: TGForecaster significantly outperforms other models on these datasets, emphasizing the benefits of integrating external text information.\n\n#### Main Experimental Results\n**Toy Dataset**:\n- TGForecaster achieves substantial improvement in MSE compared to baseline models.\n\n**Electricity Dataset**:\n- indicates TGForecaster\u2019s SOTA performance, particularly for shorter forecasting horizons.\n- TimeLLM edges out TGForecaster slightly over longer periods due to its utilization of basic date and statistical text inputs.\n\n**Weather-Medium and Weather-Large Datasets**:\n- TGForecaster dramatically outperforms other models, demonstrating the high effectiveness of incorporating external text.\n- FITS model, despite its simplicity, performs well but is still second to TGForecaster.\n\n**Additional Observations**:\n- Specific visualizations show PatchTST and DLinear's limitations in adapting to frequency changes without textual cues, a limitation TGForecaster overcomes with the incorporation of text.\n- Channel-wise performance highlights a performance boost of over 60% on some channels, emphasizing the critical role of textual guidance.\n\nThe results showcase TGForecaster's robustness and capability in integrating textual data to significantly enhance time series forecasting accuracy across different datasets and conditions."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the impact of different embedding models and the integration of textual inputs on the performance of TGForecaster.",
            "experiment_process": "Three embedding models were tested: OpenAI Embedding, paraphrase-MiniLM-L6, and all-mpnet-base. The experiments included removing channel descriptions and omitting news text to examine their effect on model performance. The evaluations were based on the TGForecaster's ability to differentiate between channels and utilize textual data effectively.",
            "result_discussion": "The results indicated minimal performance differences between the embedding models. Removing channel descriptions or news text resulted in significant performance drops. This suggests that textual data, particularly channel descriptions and news text, play a crucial role in enhancing forecasting accuracy by compensating for information deficiencies in traditional time series forecasting.",
            "ablation_id": "2405.13522v2.No1"
        },
        {
            "research_objective": "To demonstrate the practical application and performance benefits of integrating textual information with historical time series data in forecasting models, specifically evaluating TGForecaster's performance compared to other state-of-the-art models.",
            "experiment_process": "TGForecaster was tested on synthetic, captioned existing, and real-world datasets. Comparisons were made with other models across various datasets and experiment settings, including different look-back window lengths and prediction horizons. Specific tests involved replacing news inputs to validate model's adaptability to textual guidance.",
            "result_discussion": "TGForecaster consistently outperformed baseline models, especially in scenarios that involved frequency changes within the forecast horizon. A version without news input performed similarly to PatchTST, highlighting the importance of textual cues. The model demonstrated substantial benefits in adapting to new frequencies and reducing uncertainties, thus effectively incorporating textual data into the forecasting process.",
            "ablation_id": "2405.13522v2.No2"
        }
    ]
}