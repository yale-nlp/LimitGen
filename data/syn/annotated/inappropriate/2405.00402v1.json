{
    "title": "Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models",
    "abstract": "The alignments of reasoning abilities between smaller and larger Language Models are largely conducted via Supervised Fine-Tuning (SFT) using demonstrations generated from robust Large Language Models (LLMs). Although these approaches deliver more performant models, they do not show sufficiently strong generalization ability as the training only relies on the provided demonstrations.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Previous works have demonstrated that Chain-of-Thought (CoT) prompting can improve the Large Language Models (LLMs) 111(e.g., with more than 60B parameters Wei et al. (2023  ###reference_b40###)) capacity to perform complex reasoning tasks by decomposing a reasoning task into a sequence of intermediate steps Wei et al. (2022  ###reference_b39###), where the generation of multi-step controlled reasoning can improve results in commonsense Bubeck et al. (2023  ###reference_b4###), symbolic and mathematical Gaur and Saunshi (2023  ###reference_b9###); Liu et al. (2023  ###reference_b17###) reasoning datasets.\nSince the size of LLMs represents an adoption barrier for many use cases and smaller models do not seem to have the same emergent reasoning abilities as LLMs, several state-of-the-art alignment approaches for solving mathematical problems have emerged, where Supervised Fine-Tuning (SFT) has been used to train Small Language Models (SLMs) using CoT annotations.\nHowever, these annotations outline the intermediate reasoning steps for solving a given problem, which consists of a reasoning pathway generated by the LLM for the specific case. This phenomenon can lead to a relatively weak generalization capacity of tuned models that have a few and limited number of samples. Indeed, there are often multiple valid CoT annotations for the same question Cobbe et al. (2021  ###reference_b5###); Zhang et al. (2023  ###reference_b44###), which underlines the need for a more general CoT-based fine-tuning approach.\nIn this paper, we propose Self-refine Instruction-tuning, which is a method to enable CoT reasoning over SLMs.\nOur approach starts by performing Instruction-tuning on SLMs via demonstrations delivered by LLMs and then applies preference optimization based on reinforcement learning (RL) heuristics to let the SLMs refine their abilities to solve a task in a step-wise manner.\nHence, proposing a teacher-student alignment method, we investigate the impact of transferring Chain-of-Thought reasoning abilities through the support of Demonstrations \"taught\" by LLMs to SLMs as a warm-up to the Self-refine process.\nTherefore, to reinforce the Instruction-tuning phase, we analyze whether preference optimization methods could strengthen students\u2019 step-wise reasoning abilities.\n###figure_1### Complementing the foundation work of Wang et al. (2023c  ###reference_b37###, d  ###reference_b38###), we introduce Self-refinement based on reinforcement learning, and in contrast to Uesato et al. (2022  ###reference_b34###); Luo et al. (2023  ###reference_b18###); Luong et al. (2024  ###reference_b19###); Paul et al. (2024  ###reference_b24###), we use an Instruction-tuning via Demonstrations approach Ranaldi and Freitas (2024  ###reference_b26###) (i.e., a task-oriented specialization of Supervised Fine-Tuning) through which we instruct SLMs using Demonstrations delivered from different teachers prompted via a CoT mechanism.\nThis leads to the target research questions, which are the focus of this paper:\nRQ1: How does Instruction-tuning via Demonstrations initialize the SLMs\u2019 reasoning abilities?\nRQ2: What is the effect of the preference optimization algorithm on the alignment between teacher and student models?\nRQ3: How much does the ability to solve tasks in a multi-step manner improve across different scenarios?\nTo answer these questions, we select three different SLMs: Llama2-7b, -13b Touvron et al. (2023  ###reference_b33###), Mistral-7b Jiang et al. (2023  ###reference_b13###); and three LLMs Llama2-70b, Mixtral Jiang et al. (2024  ###reference_b14###) and GPT-3.5 OpenAI (2023  ###reference_b22###).\nIn the teacher-student alignment phase, we use LLMs (teachers) to deliver Demonstrations at the core of the Instruction-tuning process (see Figure 1  ###reference_###) used to instruct SLMs (students).\nIn the Self-refine phase, the students improve their step-wise reasoning abilities via Direct Preference Optimization (DPO) Rafailov et al. (2023  ###reference_b25###). This allows the students to sample different reasoning paths and CoT Demonstrations and learn from them (Figure 1  ###reference_###). Moreover, differently from previous works, preferences are self-generated, and there is no need for a separately trained reward model as in the previous approaches Ouyang et al. (2022  ###reference_b23###).\nWe demonstrate the effectiveness of the proposed refinement technique in aligning teacher-student models (overcoming the differences highlighted by Ranaldi and Freitas (2024  ###reference_b26###)) from the same family and in maximizing efficiency in in-domain and out-domain tasks.\nOur contributions can be summarized as follows:\nWe propose the Self-refined Instruction-tuning approach that is a task-oriented Supervised Fine-Tuning (SFT), which utilizes DPO heuristics to conduct a self-refinement process starting from instructed SLMs.\nWe analyze the impact of different configurations of Instruction-tuning on the SLMs before and after the Self-refining phase by conducting in-depth experiments on mathematical problems and common sense question-answering tasks using Demonstrations delivered by teacher of the same family (in-family) or not (out-family). Hence, we show the downstream functionalities in both scenarios.\nFinally, we display the generalization abilities acquired via Self-refined Instruction-tuning through a systematic evaluation using Demonstrations provided by in-family and out-family teachers, both within in-domain and out-domain tasks."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Method",
            "text": "To transfer the step-wise reasoning properties from Large Language Models (LLMs) to Small Language Models (SLMs), we propose Self-refine Instruction-tuning, a two-step approach as shown in Figure 1  ###reference_###. In the first phase, there is a transfer of step-wise (CoT) reasoning via Instruction-tuning, where LLMs systematically generate Demonstrations which are used by SLMs to initialize their step-wise (CoT) alignment (Section 2.1  ###reference_###). In the second phase, the instructed SLMs Self-refine their internal CoT model via the preference optimization technique presented in Section 2.2  ###reference_###.\nIn the standard DPO approach Rafailov et al. (2023  ###reference_b25###), a human annotator ranks the outputs from a reference policy, labeling winning and losing pairs  and . However, we propose an optimization step via Self-generated annotation by the students , which, after Instruction-tuning, should have more robust performances and reliably follow the demands of the questions.\nFor each Demonstration , we prompt the students using the input  ( or ) (blue block in Figure 1  ###reference_###). Hence, for each instance within the Demonstrations we collect the Answers () that are the answers generated by the student given the input , and the CoT-Answers () are the answers that deliver CoT generated by the student elicited via CoT mechanism .\nIn particular, assuming it is preferable for the model to generate responses that provide a CoT when elicited with  and responses when prompted with  just as the corresponding LLM teacher would do, we propose an alignment by exploiting DPO optimization. This aims to move the default style of our model (response generated by the student) towards the desired style (answers that deliver CoT). Different configurations are proposed depending on the desired result. Starting from the standard equation 1  ###reference_###:\nwhere  is the sigmoid function, and\nwhere  is a hyperparameter.\nWe propose the Self-refine Instruction-tuning that uses as optimization technique DPOCoT (described in details in Appendix B  ###reference_### in Equation 3  ###reference_###. In particular, in DPOCoT the answers that deliver a CoT response which is self-generated from the students are referred to as the preferred response."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Instruction-tuning Phase",
            "text": "A significant part of the state-of-the-art works employs standard Supervised Fine-Tuning (SFT) performed on annotations produced by a single LLM (Large Language Model) as a mechanism to improve SLMs. In our contribution, we take a step further and use Random Sampling of Outputs, which is a task-oriented specialization of SFT (Supervised Fine-Tuning), in coordination with a teacher-student alignment approach (detailed in Appendix A ###reference_###).\nIn this phase, the SLM (student) is fine-tuned on a dataset produced by LLM (teacher) comprising a set of tuples in the form of , where  represents a specific instruction,  is the input question (e.g., math-word problem), and  is the expected output and CoT answers generated from the teacher in response to the instruction and input. This setup is intended to transfer to the student models foundational problem-solving abilities, emphasizing the generation of outputs that conform to the provided instructions. The CoT answer  is articulated as:\nwith  indicating the sequence length. At each timestep , the action  is derived from the policy , where  can be any token from the models vocabulary, and the state  encapsulates the concatenation of all previously generated tokens and the optional input  if provided. The state transition is defined as:\nThe Random Sampling of Outputs loss function explicitly integrates the instruction , aligning the models\u2019 learning process with the instructional context. This loss function is formulated as:\nHere,  is conditioned on both the state , the input , and the instruction , ensuring that the model prioritizes instruction compliance in its output generation. This methodological shift from SFT to Random Sampling of Outputs underlines the principle of enhancing the models\u2019 ability to accurately interpret and execute complex instructions."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Self-refinement Phase",
            "text": "In the second phase, the instructed SLMs (students) that have improved CoT properties via Instruction-tuning (Section 2.1  ###reference_###) self-refine these properties with the support of Direct Preference Optimization (DPO) Rafailov et al. (2023  ###reference_b25###). This refinement can be conducted in an SFT style, relying exclusively on labeled preference data. The policy model, defined as , learns by repeatedly sampling the answers generated by teachers and students.\nIn the standard DPO approach Rafailov et al. (2023  ###reference_b25###  ###reference_b25###), a human annotator ranks the outputs from a reference policy, labeling winning and losing pairs  and . However, we propose an optimization step via Self-generated annotation by the students , which, after Instruction-tuning, should have more robust performances and reliably follow the demands of the questions.\nFor each Demonstration , we prompt the students using the input  ( or ) (blue block in Figure 1  ###reference_###  ###reference_###). Hence, for each instance within the Demonstrations we collect the Answers () that are the answers generated by the student given the input , and the CoT-Answers () are the answers that deliver CoT generated by the student elicited via CoT mechanism .\nIn particular, assuming it is preferable for the model to generate responses that provide a CoT when elicited with  and responses when prompted with  just as the corresponding LLM teacher would do, we propose an alignment by exploiting DPO optimization. This aims to move the default style of our model (response generated by the student) towards the desired style (answers that deliver CoT). Different configurations are proposed depending on the desired result. Starting from the standard equation 1  ###reference_###  ###reference_###:\nwhere  is the sigmoid function, and\nwhere  is a hyperparameter.\nWe propose the Self-refine Instruction-tuning that uses as optimization technique DPOCoT (described in details in Appendix B  ###reference_###  ###reference_### in Equation 3  ###reference_###  ###reference_###. In particular, in DPOCoT the answers that deliver a CoT response which is self-generated from the students are referred to as the preferred response."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experimental Setup",
            "text": "In order to evaluate the proposed model, we use both commonsense and mathematical reasoning tasks (introduced in Section 3.1  ###reference_###) that are generally used to assess the step-wise inference properties of Large Language Models (LLMs). Regarding the Self-refine Instruction-tuning on the Small Language Models (SLMs), we use the approach presented in Section 3.2  ###reference_###.\nWe adopt two benchmarks to evaluate commonsense reasoning: CommonSenseQA Talmor et al. (2019  ###reference_b31###) (CSQA) and OpenBookQA Mihaylov et al. (2018  ###reference_b21###) (OBQA) are two multi-choice commonsense question-answering tasks.\nWe adopt two benchmarks to evaluate reasoning in the context of everyday situations, aiming to establish the most reasonable solution: Interaction Question Answering (PIQA) Bisk et al. (2019  ###reference_b2###) and Social Interaction Question Answering (SIQA) Sap et al. (2019  ###reference_b28###), which emphasizes people\u2019s actions and social implications.\nWe use two math word problem benchmarks to evaluate the models of mathematical reasoning. MultiArith Roy and Roth (2015  ###reference_b27###) covers a set of multi-step arithmetic reasoning tasks, while GSM8k Cobbe et al. (2021  ###reference_b5###) covers a set of primary school-level mathematical problems.\nFinally, to evaluate the adaptability of our proposal, we conduct further analysis on two additional evaluation benchmarks: MATH Hendrycks et al. (2021b  ###reference_b11###), and MMLU Hendrycks et al. (2021a  ###reference_b10###).\nSince the test split is not prescribed for all the benchmarks, we adopt the following strategy: for SIQA, PIQA, CSQA, and OBQA, we use 4000 examples with equally distributed target classes as training data and the validation versions found on huggingface as test data, while for GSM8K and MultiArith we use the full huggingface datasets. In Table 9  ###reference_###, we report the descriptive statistics and splitting ratios, while in Table 9  ###reference_###, we report one example for each benchmark. The supporting datasets are publicly accessible as described in Table 9  ###reference_###."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Tasks & Datasets",
            "text": "In this paper, we selected different tasks that focus on reasoning tasks:\nWe adopt two benchmarks to evaluate commonsense reasoning: CommonSenseQA Talmor et al. (2019  ###reference_b31###  ###reference_b31###) (CSQA) and OpenBookQA Mihaylov et al. (2018  ###reference_b21###  ###reference_b21###) (OBQA) are two multi-choice commonsense question-answering tasks.\nWe adopt two benchmarks to evaluate reasoning in the context of everyday situations, aiming to establish the most reasonable solution: Interaction Question Answering (PIQA) Bisk et al. (2019  ###reference_b2###  ###reference_b2###) and Social Interaction Question Answering (SIQA) Sap et al. (2019  ###reference_b28###  ###reference_b28###), which emphasizes people\u2019s actions and social implications.\nWe use two math word problem benchmarks to evaluate the models of mathematical reasoning. MultiArith Roy and Roth (2015  ###reference_b27###  ###reference_b27###) covers a set of multi-step arithmetic reasoning tasks, while GSM8k Cobbe et al. (2021  ###reference_b5###  ###reference_b5###) covers a set of primary school-level mathematical problems.\nFinally, to evaluate the adaptability of our proposal, we conduct further analysis on two additional evaluation benchmarks: MATH Hendrycks et al. (2021b  ###reference_b11###  ###reference_b11###), and MMLU Hendrycks et al. (2021a  ###reference_b10###  ###reference_b10###).\nSince the test split is not prescribed for all the benchmarks, we adopt the following strategy: for SIQA, PIQA, CSQA, and OBQA, we use 4000 examples with equally distributed target classes as training data and the validation versions found on huggingface as test data, while for GSM8K and MultiArith we use the full huggingface datasets. In Table 9  ###reference_###  ###reference_###, we report the descriptive statistics and splitting ratios, while in Table 9  ###reference_###  ###reference_###, we report one example for each benchmark. The supporting datasets are publicly accessible as described in Table 9  ###reference_###  ###reference_###."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Self-refine Instruction-tuning Pipeline",
            "text": "The Self-refine Instruction-tuning comprises the annotation process conducted by the LLMs teachers that are prompted in the zero-shot scenario (as shown in Table 6  ###reference_###), as explained in Appendix A  ###reference_###.\nWe selected Llama-2-70 Touvron et al. (2023  ###reference_b33###), Mixtral7x8 Jiang et al. (2024  ###reference_b14###) and GPT-3.5 OpenAI (2023  ###reference_b22###) as LLMs (teachers) and Llama2-7, -13 Touvron et al. (2023  ###reference_b33###) and Mistral-7 Jiang et al. (2023  ###reference_b13###) SMLs (students) models.\nHence, the students models are tuned, as proposed in Taori et al. (2023  ###reference_b32###) and evaluated with probing pipelines (detailed in Section 3.3  ###reference_###). The students are instructed via Demonstrations that contain the answers generated by the teachers, as explained in Section 2.1  ###reference_###.\nDownstream of the teacher-student CoT transference process, the optimization technique (proposed in Section 2.2  ###reference_### and detailed in Appendix B  ###reference_###) is employed to improve alignment and self-refine the quality of the generation.\n###figure_2### ###figure_3### ###figure_4### ###figure_5### ###figure_6### ###figure_7### ###figure_8###"
        },
        {
            "section_id": "3.2.1",
            "parent_section_id": "3.2",
            "section_name": "3.2.1 Models Setup",
            "text": "We conduct the Self-refined Instruction-tuning in two different phases. Firstly, we start with Instruction-tuning phase using QLoRA Dettmers et al. (2023  ###reference_b6###). This approach allows Instruction-tuning to be performed while reducing memory usage. In particular, Dettmers et al. (2023  ###reference_b6###) propose several techniques for tuning models with many parameters on GPUs with limited resources while preserving 16-bit tuning performance.\nWe follow the training approach proposed in Taori et al. (2023  ###reference_b32###), setting four training epochs using a learning rate of 2e-5 with a 1e-4 weight decay. We use the cosine learning rate scheduler with a warm-up ratio of 0.03.\nFurthermore, we conduct the Self-refine phase following the approach proposed in Rafailov et al. (2023  ###reference_b25###). In particular, we use the huggingface  to support its reproducibility. We follow the parameters proposed in Rafailov et al. (2023  ###reference_b25###). Hence, for the DPO policy, our work employs a learning rate of 1e-6,  set at , and a warm-up step count of 100. The batch size is configured to 128. The optimization process is capped at a maximum of 1000 steps, where we save the checkpoint corresponding to the lowest loss on the validation set. The experiments were conducted on a workstation equipped with four Nvidia RTX A6000 with 48GB of VRAM.\n###figure_9### ###figure_10### ###figure_11### ###figure_12### ###figure_13### ###figure_14### ###figure_15###"
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Evaluation",
            "text": "The most commonly used evaluation methods for question-answering tasks are language-model probing, in which the option with the highest probability is selected Brown et al. (2020  ###reference_b3###), and multiple-choice probing, in which the models are asked to commit to an answer. The evaluation in the first case is performed with a function taking the argmax and, in the second case, with a direct string matching. The second method is more widely used in recent evaluations as it can be inclusive to the larger GPT family modelsOpenAI (2023  ###reference_b22###), where probability values are not readily accessible. In the experiments, we chose the latter to have a comparable and scalable pipeline (Details provided in Appendix C.2  ###reference_###). Finally, string matching is performed between the generated outputs and the target choice to evaluate the percentages of the correct answers."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Results & Discussion",
            "text": "The Self-refine Instruction-tuning improves the alignment between Large Language Models (LLMs) and Small (SLMs) in both in-family and out-family settings. These conclusions can be observed in Figure 2  ###reference_### and Figure 3  ###reference_###, which reports the downstream accuracies without tuning (see the Baselines), with only the Instruction-tuning phase on Demonstrations and after the Self-refine phase. As discussed in Section 4.1  ###reference_###, the models with only Instruction-tuning on Demonstrations (generated by LLMs) transfers the reasoning properties in a marginal way (see Instruction-tuned in Figures 2  ###reference_###).\nHowever, although teacher-student alignment via Instruction-tuning produces better students, an improved alignment is achieved through the Self-refine phase, as discussed in 4.2  ###reference_###. In particular, the \u2019Self-refine Instruction-tuning\u2019 bars in Figure 2  ###reference_### show that the students self-refined outperformed the students tuned only with Instruction-tuning (\u2019Instruction-tuning\u2019 bars on Figure 2  ###reference_###). Furthermore, the alignment via Demonstrations generated by teachers outside the same family (out-family) delivers more robust students (see Figure 3  ###reference_### the Self-refine Instruction-tuning and (in-family) bars).\nFinally, students models behind the self-refine phase outperformed others in both in-domain and out-domain tasks (discussed in Section 4.3  ###reference_###). Hence, the self-refine mechanism effectively aligns teacher-student capabilities in out-domain tasks by enhancing performance even in the presence of fewer Demonstrations (Section 4.4  ###reference_###)."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "The Instruction-tuning alignment",
            "text": "Instruction-tuning led by Larger Language Models (teachers models), which are able to deliver multi-step reasoned answers, induces this property within Smaller Language Models (students models). This can be seen in the experiments in Figure 2  ###reference_###, Figure 3  ###reference_### and additional evaluations in Appendix I  ###reference_###. The student models behind instruction-tuning on demonstrations produced by teacher models outperformed the baselines of the proposed benchmarks.\nWhile one can observe consistent improvements in performance across the board, there are moderate variations across models and tasks.\nThe teacher models that generate Demonstrations stem from different families and perform differently, as shown in Table 5  ###reference_###. The consequence of this phenomenon can be seen in Figure 2  ###reference_### and Figure 3  ###reference_### (horizontal lines that are the reported performance of the teachers and bars \u2019Instruction-tuning\u2019 that are the performance of the students). Therefore, the teacher-student alignment is not complete as there is a gap between the performances of the teachers and the students tuned via Instruction-tuning (only phase presented in Section 2.1  ###reference_###). In addition, it is possible to differentiate between in-family and out-family alignment. In the in-family, where students are instructed with Demonstrations delivered by the teachers of the same family, performances vary from 6.3 points on average in question-answering (QA) tasks and 8.2 points on average in math word problems (MWP) tasks. Meanwhile, in the out-family alignment, the performances vary by 8.5 on the QA and 8.7 on the MWP.\nHence, to improve the alignment both in-family and consistently out-family, we have proposed an optimization technique based on a self-refinement approach (introduced in Section 2.2  ###reference_###), the results of which we discuss in Section 4.2  ###reference_###."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "The Self-refine Impact",
            "text": "The Self-refine process enables complete in-family student-teacher alignment by consistently increasing performance in out-family settings and improving the qualities of generated answers. The results obtained in Figure 2  ###reference_### show that the students (SLMs instructed with Self-refine Instruction-tuning) outperform the non-self-refined students and perform comparably to their teachers. The same behaviour can be observed from the out-family setting shown in Figure 3  ###reference_###. In particular, the teacher GPT-3.5 showed a more robust baseline performance (Table 5  ###reference_###). Although Instruction-tuning alone transfers some of the abilities to the student models, they were significantly lower when compared to the out-family teacher models. In contrast, the teacher-student performances significantly converged after the self-refine phase, leading to the alignment completion. Finally, a positive impact can also be observed on the quality of students\u2019 generations, as shown in the additional experiment discussed in Appendix H  ###reference_###.\nThe performances appear completely aligned, but the students were tested only for in-domain tasks. The proposed approach could cause students to over-specialize in in-domain tasks, running the risk of losing the ability to solve out-domain tasks. For this reason, we performed a set of assessments evaluating students on in-domain and out-domain tasks and discussed the results in Section 4.3  ###reference_###."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "In-Domain and Out-Domain",
            "text": "The Self-refine Instruction-tuning approach complements student-teacher alignment and improves students\u2019 generalization abilities in out-domain tasks. These results can be observed in Table 1  ###reference_### with Llama2-7 as students and Llama2-70 as teachers (in Appendix Table 10  ###reference_### with Llama2-13 Table 11  ###reference_### with Mistral-7). In particular, behind the evaluations performed on in-domain and out-domain tasks, the students Self-refine Instruction-tuned outperform the baselines and the Instruction-tuned models.\nFurthermore, to observe the impact of the optimization phase (introduced in Section 2.2  ###reference_###) on the downstream performance, we conducted a further experiment by fixing the Instruction-tuning phase and switching the Self-refine ones across different evaluation tasks (e.g., we instructed a student on OBQA and then optimized via self-refine approach on CSQA). As shown in lines Cross Self-refine of Table 1  ###reference_###, students warmed up on tasks other than those they are optimized, outperformed the others, and obtained similar performances to those obtained from in-domain models. This shows that optimization positively impacts the alignment of generalization abilities in out-domain tasks.\nFinally, following evaluations in out-domain tasks and across scenarios, we evaluate the performance of the proposed approach by reducing the number of demonstrations available for alignment in Section 4.4  ###reference_###."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Low-resource Optimization",
            "text": "Self-refine Instruction-tuning achieves sustainable performances in low-resource settings. In fact, in Figure 4  ###reference_###, it is possible to observe that the performance achieved by the self-refined students consistently outperforms that of the non-self-refined students (where only phase 1 described in Section 2.1  ###reference_### was performed) (technical details on the breakdown can be found in Appendix C.1  ###reference_###).\nAlthough it emerges that only the optimization process via DPO is more performant than the instruction-tuning process alone, the combination of the two phases achieves the best results in both in-family and out-family alignment in each proposed splitting that are described in Appendix C.1  ###reference_###."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Multi-step Reasoning",
            "text": "Previous works focus on Chain-of-Thought (CoT) prompting techniques, studying the impact of prompting design and engineering, proposing specialized interventions to improve CoT generalization and fine-grained multi-step reasoning properties Wei et al. (2022  ###reference_b39###); Fu et al. (2023  ###reference_b7###).\nOn the prompting design side, Gao et al. (2023  ###reference_b8###) proposed using Python programs as a CoT prompt, demonstrating more accurate reasoning steps and significant improvements behind CoT prompting Wei et al. (2022  ###reference_b39###). Zhou et al. (2023  ###reference_b45###) introduced a code generation approach to verify the intermediate reasoning step OpenAI (2023  ###reference_b22###).\nIn parallel, there have been improvements in the accessibility of lower-parameter versions of Large Language Models (LLMs), which we define as Small Language Models (SLMs), on which previous CoT improvements cannot be fully observed Shridhar et al. (2023  ###reference_b30###); Ho et al. (2023  ###reference_b12###). Therefore, several works are emerging at this gap, aiming to transfer LLM reasoning properties to SLMs. Pioneering proposals in this direction proposed teacher-student alignment methods through a series of approaches geared towards the distillation of the knowledge generated by the teacher for the fine-tuning of the student Li et al. (2023b  ###reference_b16###); Magister et al. (2023  ###reference_b20###); Shridhar et al. (2023  ###reference_b30###).\nLater, Yue et al. (2023  ###reference_b42###) proposed specialized Instruction-tuning using Alpaca-like style demonstrations Taori et al. (2023  ###reference_b32###) specialized for mathematical tasks, while Luo et al. (2023  ###reference_b18###); Xu et al. (2023  ###reference_b41###) proposed supervised fine-tuning reinforced with rewarding algorithms."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Reinforcement Learning (RL)",
            "text": "A significant component that promotes the generative reasoning delivering CoT is provided by refinement via RL methods. Recent work that applies Proximal Policy Optimization (PPO) Schulman et al. (2017  ###reference_b29###) for aligning human preferences Ouyang et al. (2022  ###reference_b23###). Several methods have been proposed to improve the efficiency of alignment Azar et al. (2023  ###reference_b1###), including Direct Preference Optimization (DPO) Rafailov et al. (2023  ###reference_b25###).\nIn this work, we adopt RL to refine performance over conventional SFT. For mathematical problem solving, Uesato et al. (2022  ###reference_b34###) trained an outcome- or process-based reward model to perform re-ranking Cobbe et al. (2021  ###reference_b5###), achieving better performance than SFT and majority voting Wang et al. (2023b  ###reference_b36###).\nLuong et al. (2024  ###reference_b19###) adopted reinforcement learning as an extension of traditional supervised tuning.\nWe adopt DPO and automate the reward process in a teacher-student context. We focus on the transfer of CoT-style, step-wise reasoning and propose a refinement technique applied to models downstream of the instruction-tuning phase."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Self-refined Instruction-tuning",
            "text": "Complementing and enhancing foundational approaches Magister et al. (2023  ###reference_b20###); Uesato et al. (2022  ###reference_b34###); Li et al. (2023a  ###reference_b15###); Ho et al. (2023  ###reference_b12###),\nseveral papers have been published simultaneously Wang et al. (2023d  ###reference_b38###); Luo et al. (2023  ###reference_b18###); Wang et al. (2023a  ###reference_b35###); Paul et al. (2024  ###reference_b24###); Luong et al. (2024  ###reference_b19###); Ranaldi and Freitas (2024  ###reference_b26###) (Table 15  ###reference_### summarises the main features). These works prove the effect of supervised fine-tuning to transfer the ability to produce multi-step reasoned answers from larger to smaller models, as described in Section 5.2  ###reference_###.\nOur work goes beyond the state-of-the-art by:\nproposing a method for aligning CoT abilities by introducing Instruction-tuning via Demonstrations produced by answers generated by different LLMs, decentralizing the unique teacher model (in many cases GPT-3.5,4).\nanalyzing the alignment performance between in-family and out-family models on different tasks related to commonsense and math reasoning, identifying crucial alignment factors that arise between teachers and students.\ninvestigating the impact of teacher-student alignment by adapting and promoting DPO Rafailov et al. (2023  ###reference_b25###) as a cornerstone method for eliminating performance gaps."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "This paper proposes a novel approach for aligning multi-step CoT reasoning between teacher Large Language Models (LLMs) and student Smaller LMs (SLMs). In particular, our Self-refine Instruction-tuning is framed as an instruction tuning via Chain-of-Thought Demonstrations method based on explanations delivered by LLMs prompted by the CoT mechanism, which is then reinforced via the Self-refine phase that uses Direct Preference Optimization. We also contrast the impact of in-family and out-family alignment across teacher and student models.\nThe results highlight the impact of teacher-student Instruction-tuning interventions as a mechanism to improve the multi-wise reasoning properties of smaller language models and promote the self-refinement abilities of instructed models to complete the alignment."
        }
    ],
    "url": "http://arxiv.org/html/2405.00402v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "5",
            "5.1"
        ],
        "methodology_sections": [
            "2",
            "2.1",
            "2.2"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "3.1",
            "3.2",
            "3.2.1",
            "3.3",
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ]
    },
    "research_context": {
        "paper_id": "2405.00402v1",
        "paper_title": "Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models",
        "research_background": "### Paper's Motivation\nThe motivation for this paper stems from the challenges associated with the size and usability of Large Language Models (LLMs) for complex reasoning tasks. Although Chain-of-Thought (CoT) prompting has been beneficial in enhancing the reasoning capabilities of LLMs, smaller language models (SLMs) do not exhibit the same emergent reasoning abilities. Furthermore, the current Supervised Fine-Tuning (SFT) methods relying on CoT annotations generated by LLMs face limitations due to the variety of valid intermediate reasoning steps, leading to a weak generalization capacity of the tuned models. The desire to overcome these limitations while making complex reasoning tasks accessible to SLMs drives the development of this research.\n\n### Research Problem\nThe core research problem addressed in this paper is how to effectively align the reasoning capabilities of SLMs with those of LLMs, particularly through a method that can generalize better across different reasoning tasks. Specifically, the paper seeks to answer the following research questions:\n1. How does Instruction-tuning via Demonstrations initialize the SLMs\u2019 reasoning abilities?\n2. What is the effect of the preference optimization algorithm on the alignment between teacher and student models?\n3. How much does the ability to solve tasks in a multi-step manner improve across different scenarios?\n\n### Relevant Prior Work\n1. **Chain-of-Thought (CoT) Prompting:** Prior works such as Wei et al. (2023) and Wei et al. (2022) have demonstrated the efficacy of CoT prompting for LLMs in complex reasoning tasks. These studies show that breaking down problems into intermediate steps improves performance in commonsense, symbolic, and mathematical reasoning tasks.\n\n2. **Supervised Fine-Tuning (SFT) with CoT Annotations:** Existing SFT techniques have been explored to train SLMs using reasoning pathways generated by LLMs. However, as highlighted by Cobbe et al. (2021) and Zhang et al. (2023), multiple valid CoT annotations for the same question lead to a need for a more general CoT-based fine-tuning approach.\n\n3. **Preference Optimization and Self-Refinement:** The concept of self-refinement using reinforcement learning heuristics to enhance model capabilities is complemented by foundational works like Wang et al. (2023c, d). Uesato et al. (2022), Luo et al. (2023), and Luong et al. (2024) have explored similar techniques, but the current approach distinctively uses Instruction-tuning via Demonstrations.\n\n4. **Direct Preference Optimization (DPO):** Rafailov et al. (2023) have contributed to the DPO heuristic methods, which the paper leverages for refining reasoning abilities in SLMs without the need for a separately trained reward model, differing from approaches like those by Ouyang et al. (2022).\n\nThe culmination of these prior works sets the stage for the proposed Self-refine Instruction-tuning method, which seeks to blend these elements into a cohesive strategy to enhance the reasoning performance of SLMs.",
        "methodology": "**Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models**\n\n**Methodology:** To transfer the step-wise reasoning properties from Large Language Models (LLMs) to Small Language Models (SLMs), we propose Self-refine Instruction-tuning, a two-step approach. In the first phase, there is a transfer of step-wise (CoT) reasoning via Instruction-tuning, where LLMs systematically generate Demonstrations which are used by SLMs to initialize their step-wise (CoT) alignment (Section 2.1  ###reference_###). In the second phase, the instructed SLMs Self-refine their internal CoT model via the preference optimization technique presented in Section 2.2  ###reference_###.\n\nIn the standard DPO approach Rafailov et al. (2023  ###reference_b25###), a human annotator ranks the outputs from a reference policy, labeling winning and losing pairs \\( (x_w, x_l) \\). However, we propose an optimization step via Self-generated annotation by the students, which, after Instruction-tuning, should have more robust performances and reliably follow the demands of the questions.\n\nFor each Demonstration \\( D_i \\), we prompt the students using the input \\( x_i \\) (e.g., \\( x_+ \\) or \\( x_- \\)). Hence, for each instance within the Demonstrations we collect the Answers \\( A_i(x_i) \\) that are the answers generated by the student given the input \\( x_i \\), and the CoT-Answers \\( A_i^{CoT}(x_i) \\) are the answers that deliver CoT generated by the student elicited via CoT mechanism \\( \\mathcal{T} \\).\n\nIn particular, assuming it is preferable for the model to generate responses that provide a CoT when elicited with \\( x_+ \\) and typical responses when prompted with \\( x_- \\) just as the corresponding LLM teacher would do, we propose an alignment by exploiting DPO optimization. This aims to move the default style of our model (response generated by the student) towards the desired style (answers that deliver CoT). Different configurations are proposed depending on the desired result. Starting from the standard equation 1  ###reference_###:\n\\[ \\pi(x_+ | x) = \\sigma(f(x_+/x) - f(x_-/x)) \\]\nwhere \\( \\sigma \\) is the sigmoid function, and\n\\[ J(\\theta) = \\mathbb{E}_{x \\sim D}[\\log \\pi(x_+ | x; \\theta)] - \\lambda_{\\text{reg}} R(x), \\]\nwhere \\( \\lambda_{\\text{reg}} \\) is a hyperparameter.\n\nWe propose the Self-refine Instruction-tuning that uses as optimization technique DPOCoT (described in details in Appendix B  ###reference_### in Equation 3  ###reference_###). In particular, in DPOCoT the answers that deliver a CoT response which is self-generated from the students are referred to as the preferred response.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\n#### Datasets:\n\nThe main experiment evaluates the proposed model using the following datasets for commonsense and mathematical reasoning tasks:\n\n1. **Commonsense Reasoning Tasks:**\n   - **CommonSenseQA (CSQA)**: Talmor et al. (2019)\n   - **OpenBookQA (OBQA)**: Mihaylov et al. (2018)\n\n2. **Reasoning in Context of Everyday Situations:**\n   - **Physical Interaction Question Answering (PIQA)**: Bisk et al. (2019)\n   - **Social Interaction Question Answering (SIQA)**: Sap et al. (2019)\n\n3. **Mathematical Reasoning Tasks:**\n   - **MultiArith**: Roy and Roth (2015)\n   - **GSM8k**: Cobbe et al. (2021)\n\n4. **Additional Evaluation Benchmarks for Adaptability:**\n   - **MATH**: Hendrycks et al. (2021b)\n   - **MMLU**: Hendrycks et al. (2021a)\n\n#### Data Splitting Strategy:\n\n- **SIQA, PIQA, CSQA, and OBQA:** \n   - Training Data: 4000 examples with equally distributed target classes.\n   - Test Data: Validation versions available on HuggingFace.\n  \n- **GSM8K and MultiArith:**\n   - Full HuggingFace datasets are used.\n\n#### Baselines and Evaluation Metrics:\n\nThe main experiment assesses the proposed model's performance compared to standard approaches on the selected benchmarks. While the specifics of the baselines and metrics are not detailed in the provided text, typically such evaluations compare the accuracy, precision, recall, and other relevant metrics specific to question-answering and problem-solving tasks.\n\n#### Main Experimental Results:\n\nThe details of the main experimental results are not provided in the text. However, these results would generally include comparisons of model performance against baselines across the selected datasets. The results section would highlight the effectiveness of the proposed Self-refine Instruction-tuning approach when applied to Small Language Models (SLMs) in the context of the various reasoning tasks.\n\nNote: The specific results, including numerical values and comparative analysis, would be found in the referenced sections not included in the provided text."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the impact of Instruction-tuning in aligning reasoning abilities between smaller and larger language models, and to determine the extent of the performance gap.",
            "experiment_process": "The ablation study utilizes teacher models to generate demonstrations for student models through Instruction-tuning. The evaluated models belong to different families and their performances are compared using question-answering and math word problem tasks. The performances are observed in both in-family and out-family settings, considering horizontal lines (teacher's performance) and Instruction-tuning bars (student's performance) as shown in Figures 2 and 3. Metrics for comparison include average performance variations in QA and MWP tasks, noted in Table 5.",
            "result_discussion": "Instruction-tuned student models generally outperformed the baseline benchmarks but still exhibited a performance gap compared to their teacher models. The average performance variations showed smaller discrepancies in the in-family setting compared to the out-family setting.",
            "ablation_id": "2405.00402v1.No1"
        },
        {
            "research_objective": "To examine the effect of the Self-refine process on improving in-family and out-family alignment between student and teacher models.",
            "experiment_process": "The Self-refine process is applied to students who have undergone Instruction-tuning, and their performances are evaluated and compared to non-self-refined students and their teacher models. This includes observing the results in Figures 2 and 3, particularly focusing on robust performance baselines of models like GPT-3.5 (as shown in Table 5). In addition, comparisons are made based on evaluations discussed in Appendix H.",
            "result_discussion": "The Self-refine process considerably increased student model performance, aligning closer to their teacher models both in-family and out-family. The alignment completion is evident, and there was also an observed improvement in the quality of generated answers. However, the students were only tested on in-domain tasks, implying a potential risk of over-specialization.",
            "ablation_id": "2405.00402v1.No2"
        },
        {
            "research_objective": "To evaluate the generalization abilities of student models in out-domain tasks following Self-refine Instruction-tuning.",
            "experiment_process": "The Student models (e.g., Llama2-7, Llama2-13, Mistral-7) are evaluated on both in-domain and out-domain tasks as demonstrated in Table 1 and additional tables in the Appendix (Table 10 and Table 11). Students were fixed in the Instruction-tuning phase and underwent cross Self-refine optimization on different tasks (e.g., OBQA optimized on CSQA), measuring downstream performance impacts.",
            "result_discussion": "Students that underwent Self-refine Instruction-tuning outperformed baseline and sole Instruction-tuned models in out-domain tasks. The cross Self-refine students achieved performance levels similar to in-domain models, indicating the positive impact of this optimization phase on out-domain generalization abilities.",
            "ablation_id": "2405.00402v1.No3"
        },
        {
            "research_objective": "To assess the effectiveness of Self-refine Instruction-tuning in low-resource settings.",
            "experiment_process": "Performance of self-refined students is compared to non-self-refined students under reduced availability of demonstrations as depicted in Figure 4. Technical details and breakdowns are provided in Appendix C.1. Performance metrics focus on in-family and out-family alignment across different task splits (details in Appendix C.1).",
            "result_discussion": "Self-refined students in low-resource settings consistently outperformed non-self-refined students. The combined Instruction-tuning and Self-refine phases achieved the best results in both in-family and out-family alignments, showcasing the sustainability of the approach in resource-constrained environments.",
            "ablation_id": "2405.00402v1.No4"
        }
    ]
}