{
    "title": "A Unified Framework for Model Editing",
    "abstract": "We introduce a unifying framework that brings two leading \"locate-and-edit\" model editing techniques \u2013 ROME and MEMIT \u2013 under a single conceptual umbrella, optimizing for the same goal, which we call the preservation-memorization objective. ROME uses an equality constraint to perform one edit at a time, whereas MEMIT employs a more flexible least-square constraint that allows for batched edits. Following the preservation-memorization objective, we present Equality-constrained Mass Model Editing algorithm for Transformers or EMMET, a new batched memory-editing algorithm that uses a closed-form solution for the equality-constrained version of the preservation-memorization objective. EMMET is a batched-version of ROME and is able to perform batched-edits up to a batch-size of 10,000 with very similar performance to MEMIT across multiple dimensions. With EMMET, we unify and achieve symmetry within the \"locate-and-edit\" algorithms, allowing batched-editing using both objectives.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "As new facts emerge constantly, it\u2019s crucial to keep models up-to-date with the latest knowledge. Model editing Yao et al. (2023  ###reference_b30###) gives us the ability to edit facts stored inside a model as well as update incorrectly stored facts. In this paper, we focus on two popular parameter modifying model editing methods which infuse knowledge within models without needing an additional hypernetwork Chauhan et al. (2023  ###reference_b3###). These methods are ROME (Rank-One Model Editing) (Meng et al., 2022a  ###reference_b17###) and MEMIT (Mass Editing Memory in Transformer) (Meng et al., 2022b  ###reference_b18###). These methods directly update specific \"knowledge-containing\" parts of the model without requiring the need to train additional models and can be applied to any transformer based large language model (LLMs). MEMIT also uniquely allows for batched edits (appendix A.1  ###reference_###).\n###figure_1### In this paper, we present a unifying conceptual framework for ROME and MEMIT and show that both methods optimize the same objective. We call this the preservation-memorization objective of model editing, where new knowledge is injected or memorized such that representations of certain vectors are preserved through the editing process. We show that ROME optimizes an equality-constrained version of the objective whereas MEMIT optimizes a more relaxed least-squares version of the objective, which allows for a simple closed-form solution for making batched edits. We then highlight that MEMIT consists of two separate steps - an optimization objective and an algorithm that distributes the edits into multiple layers. The power of MEMIT in many cases comes from these edit-distribution algorithms.\nFinally, we present a closed-form solution for making batched edits with the equality-constraint under the preservation-memorization objective in the form of EMMET - an Equality-constrained Mass Model Editing algorithm for Transformers. With EMMET, batched edits can be performed for batch sizes upto 10,000 with its performance matching MEMIT across multiple dimensions. We evaluate EMMET on three models - GPT2-XL Radford et al. (2019  ###reference_b21###), GPT-J Wang and Komatsuzaki (2021  ###reference_b28###) and Llama-2-7b Touvron et al. (2023  ###reference_b25###) on standard model editing datasets - CounterFact Meng et al. (2022a  ###reference_b17###, b  ###reference_b18###) and zsRE Levy et al. (2017  ###reference_b16###). The code for EMMET can be found here111https://github.com/myanonymousrepo/unified_model_editing  ###reference__model_editing###.\nThe main contributions of our paper are:\nWe unify two popular model editing techniques (ROME and MEMIT) under the same conceptual framework called the preservation-memorization.\nWe disentangle the MEMIT objective from the MEMIT algorithm which distributes edits within multiple layers. We hope this sparks further research in edit-distribution algorithms.\nWe present a closed-form solution to equality-constrained memorization in the form of EMMET, a batched version of ROME. EMMET is a new batched-editing algorithm that achieves symmetry between the two objectives of \"locate-and-edit\" class of algorithms and shows that batched edits can be made using both objectives."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background",
            "text": "Facts for model editing are usually represented in a key-value format where the key vector has maximal correspondence to retrieval of a fact and the value vector enables us to get the target output after editing (Meng et al., 2022a  ###reference_b17###; Geva et al., 2020  ###reference_b9###). As an example, let us say we are editing a new fact into the model - \"The president of USA is John Cena\". In this fact,  is the vector representation of the phrase - \"The president of USA is,\" and  is the vector representation of the output at the layer being edited such that \"John Cena\" is produced as output at the final layer of the model. This is pictorially represented in step 2 in Figure 1  ###reference_###. For a more detailed explanation of the creation of key-value vectors, we refer readers to (Meng et al., 2022a  ###reference_b17###).\nThe success of model editing is measured using standard model editing metrics (Meng et al., 2022a  ###reference_b17###; Yao et al., 2023  ###reference_b30###) described below:\nEfficacy Score (ES) indicates if an edit has been successfully made to a model. It is measured as the percentage of edits where  for a query prompt used to edit the model.\nParaphrase Score (PS) represents the generalization ability of model under an edit. It is measured as the percentage of edits where  under paraphrases of the query prompt.\nNeighborhood Score (NS) represents locality of model editing. In other words, it measures if editing of a fact affects other facts stored inside a model. NS represents the percentage of facts in the neighborhood of the edited fact that remain unaltered post-edit.\nGeneration Entropy (GE) represents the fluency of a model post edit. It is calculated by measuring the weighted average of bi-gram and tri-gram entropies of text generated by an edited model. This quantity drops if the generated text is repetitive, a common failure case of model editing (Meng et al., 2022a  ###reference_b17###; Gupta and Anumanchipalli, 2024  ###reference_b12###).\nScore (S) is a quantify defined by (Meng et al., 2022a  ###reference_b17###) to represent a combination of edit success, generalization and locality. It is the harmonic mean of ES, PS, and NS.\n###figure_2###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Preservation-Memorization : A Unifying Framework for ROME and MEMIT",
            "text": "Both ROME and MEMIT base their work on viewing the weights of the feed-forward layer in a transformer as linear associative memories (Kohonen, 1972  ###reference_b15###; Anderson, 1972  ###reference_b1###). Under this paradigm, linear operations in a transformer (feed-forward layers) are viewed as a key-value store for information. In this section, we re-introduce both ROME and MEMIT in a new light - a unifying conceptual framework of the preservation-memorization objective.\nLet  represent the weights of the feed-forward layer we want to edit222These layers are found by causal tracing methods (Meng et al., 2022a  ###reference_b17###, b  ###reference_b18###), and let  be a key-vector representative of a fact that we are either editing or preserving, and is the input vector to . The layers being edited are shown in an expanded diagram of a transformer layer Vaswani et al. (2017  ###reference_b26###) in Figure 2  ###reference_###. The output of a single transformer layer for a general decoder-only LLM can be written as:\nHere, Att is the multi-head attention function, NL refers to the non-linearity used in the MLP module of the model and LN refers to layer normalization. The keys are outputs of the first linear layer, or , whereas . A detailed explanation on creation of key-vectors and value-vectors is given in Appendix A.3  ###reference_###.\nIn the model editing process, the weights of an intermediate layer of the model are changed from  to , where  is used to indicate a key-vector representing facts we want to preserve from the original model, and  being key-vectors representing facts we want to insert into the model. Let  be the desired output at the layer being edited corresponding to input  such that the correct fact is recalled by the model when finally generating text.\nOur objective is then to preserve the representations of selected input vectors before and after editing, or in other words, minimize the error between  and , while forcing the output representation of the vector  to be , or in other words - memorizing the fact represented by (, ). This process is shown pictorially in Figure 1  ###reference_###.\nIn ROME-style, this objective of model editing is optimized by the following equation:\nwhere  is a matrix containing all the vectors whose representations we want to preserve in a row.\nWe call this the preservation-memorization objective of model editing because it allows us to retain existing knowledge or skills of a model by keeping the same representations of selected key-vectors before and after editing, while memorizing a new fact , whose representation are forced to be , where  is by definition the output representation for  that generates the target answer at final layer.\nThe solution for ROME can then be written as:\nHere,  is assumed to be an invertible matrix and the denominator  is a scalar.\nMEMIT on the other hand optimizes a relaxed version of the same objective:\nwhere  is a matrix containing a row of vectors representing the edits we are making and  represents their target representations.\nThe above optimization objective aims to modify the output representations of vectors in  to  by minimizing the least square error between them instead of requiring them to be equal with an equality constraint. This is the major difference between the objectives of ROME and MEMIT, where ROME poses the memorization part of the objective as an equality constraint whereas MEMIT relaxes the equality constraint to a least-square objective. This allows Meng et al. (2022b  ###reference_b18###) to find a closed-form solution for making  edits to the model in a single update, represented by the matrix . The solution for the MEMIT objective is:\nWe deliberately write the first term in both solutions in a similar form. The first term in  represents the residual error (represented by ) of the new associations () when evaluated on the old weights .  is a vector in case of ROME since we are only able to make singular edits, whereas  is a matrix for MEMIT consisting of a row of vectors corresponding to each edit in the batch.\nTo summarize, in this section we show that ROME and MEMIT can be seen as two realizations of the preservation-memorization (PM) objective of model editing, where ROME enforces memorization using an equality constraint whereas MEMIT enforces memorization as a least square objective. The least-square constraint in MEMIT allows to reach a closed form solution for batch updates.\n###figure_3### ###figure_4### ###figure_5###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Edit-Distribution Algorithms",
            "text": "The difference in objectives is not the only difference between ROME and MEMIT. MEMIT (Meng et al., 2022b  ###reference_b18###) also additionally distributes its edits into multiple layers, which has been one of the reasons for success of MEMIT at large batch sizes. This distribution is done by using the formula:\nwhere  represents the change in weights at layer , where  represents one of the  layers being edited.  are the representations of the fact being edited at the final edit layer, which is represented by . All other representations of  and  are calculated at the layer  being edited. For , the formula reduces to equation 8  ###reference_###. We call this algorithm a type of edit-distribution algorithm, which is applied post-hoc after finding the closed-form solutions to the PM-objective.\nThe edit-distribution algorithm is separate from the solutions of the ROME and MEMIT objectives, therefore, we can apply the edit-distribution algorithm when using ROME, as well as use MEMIT without distributing the edits into multiple layers. The formula for using the MEMIT edit-distribution algorithm on ROME is as follows:\nPrior works on model editing do not differentiate between the MEMIT-objective and the edit-distribution algorithm, and as a consequence we never see edits using ROME being distributed to multiple layers or MEMIT being used on only a single layer."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Impact of edit-distribution Algorithms",
            "text": "The key advantage of single-layer editing without distribution is apparent when making batched edits. In this section, we perform two experiments to analyze this. First, we compare single edits in ROME and MEMIT with and without edit distribution on 1k randomly selected facts from the CounterFact dataset. Following that, we compare batched editing in MEMIT with and without edit distribution. Both experiments are performed on three different models - GPT2-XL (1.5B), GPT-J (6B) and Llama2-7B. The results are shown in Table 1  ###reference_### for edits without distribution and Table 3  ###reference_### for edits with distribution. We use the more stable version of ROME called r-ROME as presented in (Gupta and Anumanchipalli, 2024  ###reference_b12###) that does not lead to model collapse and improves generalization. We see that solutions to both ROME and MEMIT objectives perform equally well at making singular edits across different metrics, without needing to distribute the edits to multiple layers. To highlight the usefulness of single-layer editing, we compare MEMIT when making batched edits. The results are shown in Figure 3  ###reference_###. When only editing a single layer, we see that MEMIT is able to successfully make batched edits up to a batch size of 1024 for GPT2-XL, 256 for Llama-2-7b and a batch-size as large as 4096 for GPT-J333In our experiments we find GPT-J to be an easier model to edit compared to other models. This is both intriguing but also not the best model to evaluate model editing success.. After this point, the performance of model editing increases when making edits on multiple layers, except for Llama-2-7b. All hyperparameters for all models were chosen as is from prior work (Meng et al., 2022a  ###reference_b17###, b  ###reference_b18###; Yao et al., 2023  ###reference_b30###; Zhang et al., 2024  ###reference_b31###) (appendix A.2  ###reference_###). With these experiments, we want to highlight two key points - firstly, when comparing the effectiveness of two optimization objectives, the evaluation should not be conflated with the edit distribution algorithms. Secondly, the MEMIT edit-distribution algorithm is not perfect and currently is the only way to distribute edits into multiple layers, where the residual in the update is distributed with specific ratios through different layers. We hope these experiments will bring more focus to edit distribution algorithms and boost further research in these methods. ###figure_6### ###figure_7### ###figure_8### ###figure_9### ###figure_10###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Introducing EMMET",
            "text": "In section 3  ###reference_###, we show that ROME and MEMIT are both algorithms optimizing the preservation-memorization objective of model editing, where ROME does memorization using an equality constraint wherease MEMIT uses a least-square objective for memorization. Thus, we ask the question - can we perform batched-editing under an equality constraint for memorization?\nIn this section, we provide a closed-form solution for batched-editing where memorization is done with equality constraints under the presevation-memorization objective, and thus present a batched-version of ROME, a method we call EMMET - Equality-constrained Mass Model Editing in a Transformer.\nLet  represent  key-vectors whose representations we want to preserve. Additionally, let  represent key-vectors for  facts we want to edit in the model at the same time. Then according to the preservation-memorization objective, we want to find new weights  for a weight matrix  such that:\nAs can be seen in the above equation, the preservation of representations happens in the first term whereas memorization of all the new facts are forced using an equality constrain in the second term. The above equation is solved using lagrange-multipliers. The Lagrangian for the above equation for multiple equality constraints requires a summation of lagrange multipliers and equals:\nTo solve the system of equations, we put  to get:\nwhich is same as:\n###figure_11### ###figure_12### ###figure_13### ###figure_14### ###figure_15### where  and . Here,  and  are matrices created using a row of vectors. We set  (assuming that  is invertible444In practice, we find that  is always invertible as long as the number of key-vectors in  are large enough) to get the update equation of EMMET:\nwhere ,  and .\nThe unknown matrix of lagrange multipliers () can be found using the constraint  in the previous equation. It comes out to be:\nReplacing the above equation in equation 15  ###reference_### gives us the update equation for EMMET:\nWe write the update equation of EMMET in a familiar form, where the residual  is modified by some matrix operations to update the models with new edits. Additionally, when we put , the  matrix reduces to a single vector  and equation 17  ###reference_### reduces to the ROME update equation (equation 5  ###reference_###). With EMMET, we complete the unification of ROME and MEMIT under the preservation-memorization objective and achieve a symmetry with the usage of these algorithms. EMMET allows for making batched-edits as well as singular when using equality constraints for memorization, much similar to MEMIT with least-square based memorization."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Stabilizing EMMET",
            "text": "There are two important matrices that are being inverted in EMMET and MEMIT. The first one is , which is defined identically in both algorithms, whereas  is only inverted in EMMET. While the invertibility of both matrices are assumed, they are not always guaranteed. Each of the matrices  or  can be written as a row of column vectors as explained in section 3  ###reference_###, and thus  can be written as a sum of outer products:\nwhere  represent a key-vector we want to preserve. For an LLM of dimension , the dimensionality of a key-vector is usually  (Figure 2  ###reference_###), which is the dimensionality of the square matrix . If  is a -dimensional square matrix which is a summation of rank-1 matrices, it is invertible as long as there are atleast -independent vectors in the summation, or -independent vectors in . For example, for Llama-2-7b with hidden dimension of 4096, the dimensionality of key vectors are 16384. So as long as representations of atleast 16384 independent key-vectors are being preserved while editing,  will be an invertible matrix. In practice, we preserve representations of a much larger number of vectors, and hence this condition is almost always satisfied.\nThe matrix  is a square matrix of dimensionality equal to the number of edits. If given that  is invertible,  is invertible as long as  is full-rank, which means all key-vectors corresponding to facts being memorized are independent of each other. While this is not guaranteed, it can be verified before editing and facts corresponding to non-independent keys can be removed from a batch. In practice, we do not find invertibility of D being an issue. However, we find that  is often ill-conditioned, which means that the ratio of the largest and smallest eigenvalues of  explodes. This doesn\u2019t necessarily mean that the matrix is singular (non-invertible), but it does mean that numerical computations involving the matrix inverse are unstable and can lead to large numerical errors. To counter this, we set , where  is set to 0.1 after an ablation over multiple batch sizes. This allows for stable batched edits using EMMET."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Batch Editing with EMMET",
            "text": "We begin by experimenting with EMMET for model editing with varied batch sizes on GPT2-XL, GPT-J and Llama-2-7b on the CounterFact and zsRE Levy et al. (2017  ###reference_b16###) datasets. The exact implementation details can be found in section A.2  ###reference_###. We compare the performance of EMMET and MEMIT on batch sizes up to 10,000 while editing both single (to directly compare the optimization objectives) and multiple layers. The single layer editing comparison between EMMET and MEMIT can be found in Figure 4  ###reference_###. We see that both methods have almost identical performance in practice across different metrics. MEMIT performs slightly better than EMMET for Llama-2-7b, as indicated by ES, PS and S metrics. We then apply the MEMIT edit-distribution on EMMET and compare it with MEMIT. The results are shown in Figure 5  ###reference_###. We see that in this case, EMMET performs slightly better than MEMIT for Llama-2-7b. The results on the zsRE dataset tell a similar story and can be seen in Figure 7  ###reference_### and 8  ###reference_###. These results present EMMET as a viable new batched-editing algorithm.\n###figure_16### ###figure_17### Previous work (Gu et al., 2024  ###reference_b11###; Gupta et al., 2024  ###reference_b13###) has shown that model editing is often accompanied by model degradation. Gupta et al. (2024  ###reference_b13###) show this by evaluating the edited model on downstream tasks from the popular GLUE benchmark Wang et al. (2018  ###reference_b27###). We adopt their evaluation setting and evaluate both EMMET and MEMIT on four downstream tasks - sentiment analysis (SST2) Socher et al. (2013  ###reference_b22###), paraphrase detection (MRPC) Dolan and Brockett (2005  ###reference_b8###), natural language inference (NLI) (Dagan et al., 2005  ###reference_b5###; Haim et al., 2006  ###reference_b14###; Giampiccolo et al., 2007  ###reference_b10###; Bentivogli et al., 2009  ###reference_b2###) and linguistic acceptability classification Warstadt et al. (2019  ###reference_b29###) for doing downstream evaluation. The results are shown in Figure 6  ###reference_### for a batch size of 256. The results for other batch sizes can be found in Appendix A.2  ###reference_###. We find that both EMMET and MEMIT also degrade the model similarly.\nAlthough EMMET is unable outperform MEMIT, it is an important piece in unifying model editing under the preservation-memorization framework. Both algorithms are able to make successful batched edits upto a batch-size of 10,000 and lead to similar model degradation. EMMET imposes a \"theoretically\" stronger memorization constraint, yet we do not see an improvement in editing efficacy. This indicates that we may be reaching the limit of model editing capabilities under the preservation-memorization objective."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper we unite two popular model editing techniques, ROME and MEMIT, under the preservation-memorization objective, with ROME performing equality-constrained edits and MEMIT operating under a least-square constraint. We disentangle the edit-distribution algorithm proposed in MEMIT from the optimization objective, presenting them as separate entities, emphasizing that a fair comparison of future model editing techniques with MEMIT should be based on the objective of MEMIT rather than conflating it with the edit-distribution algorithm.\nFinally, we present EMMET - Equality-constrained Mass Model Editing in a Transformer, a new batched-editing algorithm based on the preservation-memorization objective where batched-memorization happens under an equality constraint. Our experiments show that EMMET performs similarly to MEMIT across multiple dimensions and metrics. EMMET completes batched editing using both types of objectives and truly unifies model editing under the preservation memorization framework. We hope that this unifying framework improves the intuitive understanding of these algorithms and fuels future research based on both intuition and mathematics."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "While our technique may streamline error correction processes, it does not address deeper structural limitations within models, such as edited models inadvertently amplifying existing errors or introducing new inaccuracies. Furthermore, the effectiveness of our method varies depending on the complexity of the model architecture and the nature of the edited knowledge as evidenced by our experiments. Despite having a theoretically \u2018stronger\u2019 memorization objective, EMMET is not able to outperform MEMIT, which also indicates that we might have reached a saturation point for model editing using naive implementations of the preservation-memorization objective, underscoring the fact that significant progress is yet to be made in understanding edit distribution and its implications. Thus, while our work contributes to a deeper understanding of model behavior, it is essential to recognize and account for these limitations in the interpretation and application of our findings."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Ethical Considerations",
            "text": "While our model editing method allows users to effectively correct for errors or update facts in models, caution is warranted. Our technique also introduces concerns for potential misuse such as malicious actors inserting harmful or false knowledge in LLMs that is absent from the original training data. As such, we warn readers that LLMs should not be considered reliable knowledge bases."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A Appendix",
            "text": "###table_1###"
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T1\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T1.56\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T1.56.57.1\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S3.T1.56.57.1.1\" rowspan=\"2\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T1.56.57.1.1.1\">Algorithm</span></th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S3.T1.56.57.1.2\" rowspan=\"2\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T1.56.57.1.2.1\">Model</span></th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S3.T1.56.57.1.3\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">Efficacy</th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S3.T1.56.57.1.4\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">Generalization</th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S3.T1.56.57.1.5\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">Locality</th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T1.56.57.1.6\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">Fluency</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T1.56.57.1.7\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">Score</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.8.8\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S3.T1.1.1.1\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">ES \n</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S3.T1.2.2.2\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">EM \n</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S3.T1.3.3.3\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">PS \n</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S3.T1.4.4.4\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">PM \n</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S3.T1.5.5.5\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">NS \n</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S3.T1.6.6.6\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">NM \n</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S3.T1.7.7.7\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">GE \n</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S3.T1.8.8.8\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">S \n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T1.16.16\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.16.16.9\" rowspan=\"2\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" id=\"S3.T1.16.16.9.1\">ROME</span></th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.16.16.10\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T1.16.16.10.1\">GPT2-XL (1.5B)</span></th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S3.T1.9.9.1\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S3.T1.10.10.2\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S3.T1.11.11.3\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S3.T1.12.12.4\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S3.T1.13.13.5\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S3.T1.14.14.6\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S3.T1.15.15.7\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S3.T1.16.16.8\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.24.24\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row\" id=\"S3.T1.24.24.9\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T1.24.24.9.1\">GPT-J (6B)</span></th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S3.T1.17.17.1\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S3.T1.18.18.2\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S3.T1.19.19.3\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S3.T1.20.20.4\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S3.T1.21.21.5\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S3.T1.22.22.6\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S3.T1.23.23.7\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S3.T1.24.24.8\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.32.32\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_th ltx_th_row\" id=\"S3.T1.32.32.9\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row\" id=\"S3.T1.32.32.10\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T1.32.32.10.1\">Llama-2 (7B)</span></th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S3.T1.25.25.1\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S3.T1.26.26.2\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S3.T1.27.27.3\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S3.T1.28.28.4\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S3.T1.29.29.5\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S3.T1.30.30.6\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S3.T1.31.31.7\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S3.T1.32.32.8\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.40.40\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.40.40.9\" rowspan=\"2\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" id=\"S3.T1.40.40.9.1\">MEMIT</span></th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.40.40.10\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T1.40.40.10.1\">GPT2-XL (1.5B)</span></th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S3.T1.33.33.1\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S3.T1.34.34.2\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S3.T1.35.35.3\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S3.T1.36.36.4\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S3.T1.37.37.5\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S3.T1.38.38.6\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S3.T1.39.39.7\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S3.T1.40.40.8\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.48.48\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row\" id=\"S3.T1.48.48.9\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T1.48.48.9.1\">GPT-J (6B)</span></th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S3.T1.41.41.1\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S3.T1.42.42.2\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S3.T1.43.43.3\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S3.T1.44.44.4\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S3.T1.45.45.5\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S3.T1.46.46.6\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S3.T1.47.47.7\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S3.T1.48.48.8\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.56.56\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_th ltx_th_row ltx_border_bb ltx_border_b\" id=\"S3.T1.56.56.9\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_b\" id=\"S3.T1.56.56.10\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T1.56.56.10.1\">Llama-2 (7B)</span></th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_b\" id=\"S3.T1.49.49.1\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_b\" id=\"S3.T1.50.50.2\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_b\" id=\"S3.T1.51.51.3\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_b\" id=\"S3.T1.52.52.4\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_b\" id=\"S3.T1.53.53.5\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_b\" id=\"S3.T1.54.54.6\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_b\" id=\"S3.T1.55.55.7\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_b\" id=\"S3.T1.56.56.8\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Comparison between ROME and MEMIT when editing only a single layer for CounterFact dataset.</figcaption>\n</figure>",
            "capture": "Table 1: Comparison between ROME and MEMIT when editing only a single layer for CounterFact dataset."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T2\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"A1.T2.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A1.T2.1.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T2.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T2.1.1.1.1.1\">Batch Size</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T2.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T2.1.1.1.2.1\">Num Batches</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T2.1.1.1.3.1\">Total Edits</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.1.2.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A1.T2.1.2.2.1\">4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A1.T2.1.2.2.2\">25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T2.1.2.2.3\">100</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.1.3.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A1.T2.1.3.3.1\">16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A1.T2.1.3.3.2\">10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T2.1.3.3.3\">160</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.1.4.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A1.T2.1.4.4.1\">64</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A1.T2.1.4.4.2\">5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T2.1.4.4.3\">320</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.1.5.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A1.T2.1.5.5.1\">256</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A1.T2.1.5.5.2\">5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T2.1.5.5.3\">1280</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.1.6.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A1.T2.1.6.6.1\">1024</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A1.T2.1.6.6.2\">3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T2.1.6.6.3\">3072</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.1.7.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A1.T2.1.7.7.1\">4096</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A1.T2.1.7.7.2\">2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T2.1.7.7.3\">8192</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.1.8.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"A1.T2.1.8.8.1\">10,000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"A1.T2.1.8.8.2\">1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"A1.T2.1.8.8.3\">10,000</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Statistics for batch size and number of batches used to create the numbers for this paper.</figcaption>\n</figure>",
            "capture": "Table 2: Statistics for batch size and number of batches used to create the numbers for this paper."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T3\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"A1.T3.56\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A1.T3.56.57.1\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"A1.T3.56.57.1.1\" rowspan=\"2\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"A1.T3.56.57.1.1.1\">Algorithm</span></th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"A1.T3.56.57.1.2\" rowspan=\"2\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"A1.T3.56.57.1.2.1\">Model</span></th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"A1.T3.56.57.1.3\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">Efficacy</th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"A1.T3.56.57.1.4\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">Generalization</th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"A1.T3.56.57.1.5\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">Locality</th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A1.T3.56.57.1.6\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">Fluency</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A1.T3.56.57.1.7\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">Score</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T3.8.8\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A1.T3.1.1.1\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">ES \n</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A1.T3.2.2.2\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">EM \n</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A1.T3.3.3.3\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">PS \n</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A1.T3.4.4.4\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">PM \n</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A1.T3.5.5.5\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">NS \n</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A1.T3.6.6.6\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">NM \n</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A1.T3.7.7.7\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">GE \n</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A1.T3.8.8.8\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">S \n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A1.T3.16.16\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_t\" id=\"A1.T3.16.16.9\" rowspan=\"2\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" id=\"A1.T3.16.16.9.1\">ROME</span></th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_t\" id=\"A1.T3.16.16.10\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"A1.T3.16.16.10.1\">GPT2-XL (1.5B)</span></th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"A1.T3.9.9.1\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"A1.T3.10.10.2\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"A1.T3.11.11.3\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"A1.T3.12.12.4\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"A1.T3.13.13.5\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"A1.T3.14.14.6\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"A1.T3.15.15.7\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"A1.T3.16.16.8\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T3.24.24\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row\" id=\"A1.T3.24.24.9\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"A1.T3.24.24.9.1\">GPT-J (6B)</span></th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A1.T3.17.17.1\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A1.T3.18.18.2\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A1.T3.19.19.3\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A1.T3.20.20.4\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A1.T3.21.21.5\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A1.T3.22.22.6\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A1.T3.23.23.7\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A1.T3.24.24.8\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T3.32.32\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_th ltx_th_row\" id=\"A1.T3.32.32.9\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row\" id=\"A1.T3.32.32.10\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"A1.T3.32.32.10.1\">Llama-2 (7B)</span></th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A1.T3.25.25.1\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A1.T3.26.26.2\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A1.T3.27.27.3\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A1.T3.28.28.4\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A1.T3.29.29.5\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A1.T3.30.30.6\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A1.T3.31.31.7\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A1.T3.32.32.8\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T3.40.40\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_t\" id=\"A1.T3.40.40.9\" rowspan=\"2\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" id=\"A1.T3.40.40.9.1\">MEMIT</span></th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_t\" id=\"A1.T3.40.40.10\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"A1.T3.40.40.10.1\">GPT2-XL (1.5B)</span></th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"A1.T3.33.33.1\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"A1.T3.34.34.2\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"A1.T3.35.35.3\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"A1.T3.36.36.4\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"A1.T3.37.37.5\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"A1.T3.38.38.6\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"A1.T3.39.39.7\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"A1.T3.40.40.8\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T3.48.48\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row\" id=\"A1.T3.48.48.9\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"A1.T3.48.48.9.1\">GPT-J (6B)</span></th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A1.T3.41.41.1\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A1.T3.42.42.2\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A1.T3.43.43.3\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A1.T3.44.44.4\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A1.T3.45.45.5\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A1.T3.46.46.6\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A1.T3.47.47.7\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A1.T3.48.48.8\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T3.56.56\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_th ltx_th_row ltx_border_bb ltx_border_b\" id=\"A1.T3.56.56.9\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_b\" id=\"A1.T3.56.56.10\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"A1.T3.56.56.10.1\">Llama-2 (7B)</span></th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_b\" id=\"A1.T3.49.49.1\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_b\" id=\"A1.T3.50.50.2\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_b\" id=\"A1.T3.51.51.3\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_b\" id=\"A1.T3.52.52.4\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_b\" id=\"A1.T3.53.53.5\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_b\" id=\"A1.T3.54.54.6\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_b\" id=\"A1.T3.55.55.7\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_b\" id=\"A1.T3.56.56.8\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Comparison between ROME and MEMIT when editing multiple layers for the CounterFact dataset.</figcaption>\n</figure>",
            "capture": "Table 3: Comparison between ROME and MEMIT when editing multiple layers for the CounterFact dataset."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.14236v2_figure_1.png",
            "caption": "Figure 1: A diagrammatic representation of the preservation-memorization objective."
        },
        "2": {
            "figure_path": "2403.14236v2_figure_2.png",
            "caption": "Figure 2: Figure shows a diagrammatic representation of a transformer layer. The layer being edited by ROME, MEMIT and EMMET is the projection weight matrix inside the MLP layer (Wp\u2062r\u2062o\u2062jsubscript\ud835\udc4a\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc57W_{proj}italic_W start_POSTSUBSCRIPT italic_p italic_r italic_o italic_j end_POSTSUBSCRIPT)."
        },
        "3": {
            "figure_path": "2403.14236v2_figure_3.png",
            "caption": "(a) Efficacy Score (ES)"
        },
        "4": {
            "figure_path": "2403.14236v2_figure_4.png",
            "caption": "(b) Paraphrase Score (PS)"
        },
        "5": {
            "figure_path": "2403.14236v2_figure_5.png",
            "caption": "(c) Neighborhood Score (NS)"
        },
        "6": {
            "figure_path": "2403.14236v2_figure_6.png",
            "caption": "(a) Efficacy Score (ES)"
        },
        "7": {
            "figure_path": "2403.14236v2_figure_7.png",
            "caption": "(b) Paraphrase Score (PS)"
        },
        "8": {
            "figure_path": "2403.14236v2_figure_8.png",
            "caption": "(c) Neighborhood Score (NS)"
        },
        "9": {
            "figure_path": "2403.14236v2_figure_9.png",
            "caption": "(d) Generation Entropy (GE)"
        },
        "10": {
            "figure_path": "2403.14236v2_figure_10.png",
            "caption": "(e) Score (S)"
        },
        "11": {
            "figure_path": "2403.14236v2_figure_11.png",
            "caption": "(a) Efficacy Score (ES)"
        },
        "12": {
            "figure_path": "2403.14236v2_figure_12.png",
            "caption": "(b) Paraphrase Score (PS)"
        },
        "13": {
            "figure_path": "2403.14236v2_figure_13.png",
            "caption": "(c) Neighborhood Score (NS)"
        },
        "14": {
            "figure_path": "2403.14236v2_figure_14.png",
            "caption": "(d) Generation Entropy (GE)"
        },
        "15": {
            "figure_path": "2403.14236v2_figure_15.png",
            "caption": "(e) Score (S)"
        },
        "16": {
            "figure_path": "2403.14236v2_figure_16.png",
            "caption": "(a) EMMET"
        },
        "17": {
            "figure_path": "2403.14236v2_figure_17.png",
            "caption": "(b) MEMIT"
        },
        "18": {
            "figure_path": "2403.14236v2_figure_18.png",
            "caption": "(a) Efficacy Accuracy (EM)"
        },
        "19": {
            "figure_path": "2403.14236v2_figure_19.png",
            "caption": "(b) Paraphrase Accuracy (PM)"
        },
        "20": {
            "figure_path": "2403.14236v2_figure_20.png",
            "caption": "(c) Neighborhood Accuracy (NM)"
        },
        "21": {
            "figure_path": "2403.14236v2_figure_21.png",
            "caption": "(a) Efficacy Accuracy (EM)"
        },
        "22": {
            "figure_path": "2403.14236v2_figure_22.png",
            "caption": "(b) Paraphrase Accuracy (PM)"
        },
        "23": {
            "figure_path": "2403.14236v2_figure_23.png",
            "caption": "(c) Neighborhood Accuracy (NM)"
        },
        "24": {
            "figure_path": "2403.14236v2_figure_24.png",
            "caption": "(a) EMMET"
        },
        "25": {
            "figure_path": "2403.14236v2_figure_25.png",
            "caption": "(b) MEMIT"
        },
        "26": {
            "figure_path": "2403.14236v2_figure_26.png",
            "caption": "(a) EMMET"
        },
        "27": {
            "figure_path": "2403.14236v2_figure_27.png",
            "caption": "(b) MEMIT"
        },
        "28": {
            "figure_path": "2403.14236v2_figure_28.png",
            "caption": "(a) EMMET"
        },
        "29": {
            "figure_path": "2403.14236v2_figure_29.png",
            "caption": "(b) MEMIT"
        },
        "30": {
            "figure_path": "2403.14236v2_figure_30.png",
            "caption": "(a) EMMET"
        },
        "31": {
            "figure_path": "2403.14236v2_figure_31.png",
            "caption": "(b) MEMIT"
        },
        "32": {
            "figure_path": "2403.14236v2_figure_32.png",
            "caption": "(a) EMMET"
        },
        "33": {
            "figure_path": "2403.14236v2_figure_33.png",
            "caption": "(b) MEMIT"
        },
        "34": {
            "figure_path": "2403.14236v2_figure_34.png",
            "caption": "(a) EMMET"
        },
        "35": {
            "figure_path": "2403.14236v2_figure_35.png",
            "caption": "(b) MEMIT"
        }
    },
    "references": [
        {
            "1": {
                "title": "A simple neural network generating an interactive memory.",
                "author": "James A Anderson. 1972.",
                "venue": "Mathematical biosciences, 14(3-4):197\u2013220.",
                "url": null
            }
        },
        {
            "2": {
                "title": "The fifth pascal recognizing textual entailment challenge.",
                "author": "Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. 2009.",
                "venue": "TAC, 7:8.",
                "url": null
            }
        },
        {
            "3": {
                "title": "A brief review of hypernetworks in deep learning.",
                "author": "Vinod Kumar Chauhan, Jiandong Zhou, Ping Lu, Soheila Molaei, and David A Clifton. 2023.",
                "venue": "arXiv preprint arXiv:2306.06955.",
                "url": null
            }
        },
        {
            "4": {
                "title": "Evaluating the ripple effects of knowledge editing in language models.",
                "author": "Roi Cohen, Eden Biran, Ori Yoran, Amir Globerson, and Mor Geva. 2023.",
                "venue": "arXiv preprint arXiv:2307.12976.",
                "url": null
            }
        },
        {
            "5": {
                "title": "The pascal recognising textual entailment challenge.",
                "author": "Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005.",
                "venue": "In Machine learning challenges workshop, pages 177\u2013190. Springer.",
                "url": null
            }
        },
        {
            "6": {
                "title": "Knowledge neurons in pretrained transformers.",
                "author": "Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. 2021.",
                "venue": "arXiv preprint arXiv:2104.08696.",
                "url": null
            }
        },
        {
            "7": {
                "title": "Editing factual knowledge in language models.",
                "author": "Nicola De Cao, Wilker Aziz, and Ivan Titov. 2021.",
                "venue": "arXiv preprint arXiv:2104.08164.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Automatically constructing a corpus of sentential paraphrases.",
                "author": "Bill Dolan and Chris Brockett. 2005.",
                "venue": "In Third International Workshop on Paraphrasing (IWP2005).",
                "url": null
            }
        },
        {
            "9": {
                "title": "Transformer feed-forward layers are key-value memories.",
                "author": "Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. 2020.",
                "venue": "arXiv preprint arXiv:2012.14913.",
                "url": null
            }
        },
        {
            "10": {
                "title": "The third pascal recognizing textual entailment challenge.",
                "author": "Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and William B Dolan. 2007.",
                "venue": "In Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing, pages 1\u20139.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Model editing can hurt general abilities of large language models.",
                "author": "Jia-Chen Gu, Hao-Xiang Xu, Jun-Yu Ma, Pan Lu, Zhen-Hua Ling, Kai-Wei Chang, and Nanyun Peng. 2024.",
                "venue": "arXiv preprint arXiv:2401.04700.",
                "url": null
            }
        },
        {
            "12": {
                "title": "Rebuilding rome: Resolving model collapse during sequential model editing.",
                "author": "Akshat Gupta and Gopala Anumanchipalli. 2024.",
                "venue": "arXiv preprint arXiv:2403.07175.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Model editing at scale leads to gradual and catastrophic forgetting.",
                "author": "Akshat Gupta, Anurag Rao, and Gopala Anumanchipalli. 2024.",
                "venue": "arXiv preprint arXiv:2401.07453.",
                "url": null
            }
        },
        {
            "14": {
                "title": "The second pascal recognising textual entailment challenge.",
                "author": "R Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. 2006.",
                "venue": "In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, volume 7, pages 785\u2013794.",
                "url": null
            }
        },
        {
            "15": {
                "title": "Correlation matrix memories.",
                "author": "Teuvo Kohonen. 1972.",
                "venue": "IEEE transactions on computers, 100(4):353\u2013359.",
                "url": null
            }
        },
        {
            "16": {
                "title": "Zero-shot relation extraction via reading comprehension.",
                "author": "Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. 2017.",
                "venue": "arXiv preprint arXiv:1706.04115.",
                "url": null
            }
        },
        {
            "17": {
                "title": "Locating and editing factual associations in gpt.",
                "author": "Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022a.",
                "venue": "Advances in Neural Information Processing Systems, 35:17359\u201317372.",
                "url": null
            }
        },
        {
            "18": {
                "title": "Mass-editing memory in a transformer.",
                "author": "Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. 2022b.",
                "venue": "arXiv preprint arXiv:2210.07229.",
                "url": null
            }
        },
        {
            "19": {
                "title": "Fast model editing at scale.",
                "author": "Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D Manning. 2021.",
                "venue": "arXiv preprint arXiv:2110.11309.",
                "url": null
            }
        },
        {
            "20": {
                "title": "Memory-based model editing at scale.",
                "author": "Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D Manning, and Chelsea Finn. 2022.",
                "venue": "In International Conference on Machine Learning, pages 15817\u201315831. PMLR.",
                "url": null
            }
        },
        {
            "21": {
                "title": "Language models are unsupervised multitask learners.",
                "author": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019.",
                "venue": "OpenAI blog, 1(8):9.",
                "url": null
            }
        },
        {
            "22": {
                "title": "Recursive deep models for semantic compositionality over a sentiment treebank.",
                "author": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013.",
                "venue": "In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631\u20131642.",
                "url": null
            }
        },
        {
            "23": {
                "title": "Axiomatic attribution for deep networks.",
                "author": "Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.",
                "venue": "In International conference on machine learning, pages 3319\u20133328. PMLR.",
                "url": null
            }
        },
        {
            "24": {
                "title": "Massive editing for large language models via meta learning.",
                "author": "Chenmien Tan, Ge Zhang, and Jie Fu. 2023.",
                "venue": "arXiv preprint arXiv:2311.04661.",
                "url": null
            }
        },
        {
            "25": {
                "title": "Llama 2: Open foundation and fine-tuned chat models, 2023.",
                "author": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023.",
                "venue": "URL https://arxiv. org/abs/2307.09288.",
                "url": null
            }
        },
        {
            "26": {
                "title": "Attention is all you need.",
                "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017.",
                "venue": "Advances in neural information processing systems, 30.",
                "url": null
            }
        },
        {
            "27": {
                "title": "Glue: A multi-task benchmark and analysis platform for natural language understanding.",
                "author": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2018.",
                "venue": "arXiv preprint arXiv:1804.07461.",
                "url": null
            }
        },
        {
            "28": {
                "title": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.",
                "author": "Ben Wang and Aran Komatsuzaki. 2021.",
                "venue": "https://github.com/kingoflolz/mesh-transformer-jax.",
                "url": null
            }
        },
        {
            "29": {
                "title": "Neural network acceptability judgments.",
                "author": "Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. 2019.",
                "venue": "Transactions of the Association for Computational Linguistics, 7:625\u2013641.",
                "url": null
            }
        },
        {
            "30": {
                "title": "Editing large language models: Problems, methods, and opportunities.",
                "author": "Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, and Ningyu Zhang. 2023.",
                "venue": "arXiv preprint arXiv:2305.13172.",
                "url": null
            }
        },
        {
            "31": {
                "title": "A comprehensive study of knowledge editing for large language models.",
                "author": "Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, et al. 2024.",
                "venue": "arXiv preprint arXiv:2401.01286.",
                "url": null
            }
        },
        {
            "32": {
                "title": "Mquake: Assessing knowledge editing in language models via multi-hop questions.",
                "author": "Zexuan Zhong, Zhengxuan Wu, Christopher D Manning, Christopher Potts, and Danqi Chen. 2023.",
                "venue": "arXiv preprint arXiv:2305.14795.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.14236v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "3"
        ],
        "methodology_sections": [
            "4",
            "5",
            "5.1",
            "5.2"
        ],
        "main_experiment_and_results_sections": [
            "5.2",
            "4.1"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.1",
            "5.1",
            "5.2"
        ]
    },
    "research_context": {
        "paper_id": "2403.14236v2",
        "paper_title": "A Unified Framework for Model Editing",
        "research_background": "The paper \"A Unified Framework for Model Editing\" aims to address the necessity of keeping models up-to-date with the latest knowledge, especially when new facts emerge or when existing facts stored inside a model are incorrect. The key motivation behind this study is the ability to dynamically update and edit the factual knowledge embedded within large language models (LLMs) without requiring extensive retraining or the deployment of additional hypernetworks.\n\n### Research Problem\nThe specific research problem tackled in this paper is the need to understand and optimize model editing techniques. More precisely, the paper focuses on two popular parameter-modifying model editing methods\u2014ROME (Rank-One Model Editing) and MEMIT (Mass Editing Memory in Transformer)\u2014both of which allow for the infusion of knowledge into transformer-based LLMs. The issue at hand is how these two methods, which do not necessitate additional model training, can be unified under a single conceptual framework and optimized to enhance their performance and applicability, particularly in the context of batched edits.\n\n### Prior Work\n1. **Model Editing**: Yao et al. (2023) highlighted the importance of model editing, detailing how it facilitates changes to factual knowledge stored within a model.\n2. **ROME (Rank-One Model Editing)**: Meng et al. (2022a) introduced ROME, which focuses on updating specific \"knowledge-containing\" parts of a model without requiring extensive retraining.\n3. **MEMIT (Mass Editing Memory in Transformer)**: Meng et al. (2022b) developed MEMIT, which extends the ROME approach by allowing for batched edits and providing a practical solution for efficient knowledge updates.\n4. **Evaluation Models and Datasets**: The paper evaluates the new algorithms on models like GPT2-XL (Radford et al., 2019), GPT-J (Wang and Komatsuzaki, 2021), and Llama-2-7b (Touvron et al., 2023), using standard model editing datasets such as CounterFact (Meng et al., 2022a,b) and zsRE (Levy et al., 2017).\n\n### Main Contributions\n1. **Unified Framework**: The paper introduces a unified conceptual framework called the preservation-memorization framework, which underpins both ROME and MEMIT model editing methods.\n2. **Disentanglement of Methods**: It isolates the MEMIT objective from the algorithm that distributes edits across multiple layers, stimulating further research in edit-distribution algorithms.\n3. **EMMET**: A new algorithm named EMMET (Equality-constrained Mass Model Editing algorithm for Transformers) is presented, offering a closed-form solution for batched edits. This method balances the objectives found in \"locate-and-edit\" algorithms, enabling effective batched edits up to batch sizes of 10,000.\n\nIn summary, the paper presents a unified framework to understand and improve model editing methods, offering a new algorithm to better handle batch edits in large language models.",
        "methodology": "Methodology: The methodology of our proposed approach, A Unified Framework for Model Editing, takes into consideration the differences between the ROME and MEMIT techniques. Distinctly, MEMIT, as introduced by Meng et al. (2022b), incorporates a unique component involving the distribution of edits across multiple layers, which has significantly contributed to its success, especially when handling large batch sizes. To achieve this distribution, MEMIT employs the following formula:\n\n\\(\\Delta W^i\\)\n\nIn this formula, \\(\\Delta W^i\\) denotes the change in weights at layer \\(i\\), where \\(i\\) represents one of the \\(L\\) layers being edited. The representations \\(\\mathbf{r}\\) of the fact being edited are obtained at the final edit layer, denoted as \\(\\mathbf{r_H}\\). All other representations of \\(\\mathbf{r}\\) and \\(\\mathbf{r'_H}\\) are calculated at the specific layer \\(i\\) being edited. For the case when \\(L_i = 1\\), the formula simplifies to equation 8 ###reference_###. We categorize this approach as an edit-distribution algorithm, which is applied post-hoc following the identification of closed-form solutions to the PM-objective.\n\nThe edit-distribution algorithm operates independently from the solutions to the ROME and MEMIT objectives. Consequently, we can apply this algorithm to ROME, effectively distributing edits across multiple layers, or alternatively, we can utilize MEMIT without spreading edits into multiple layers. The formula for adopting the MEMIT edit-distribution algorithm within the ROME framework is presented as:\n\n\\(...\\)\n\nPrevious studies on model editing have failed to differentiate adequately between the MEMIT-objective and the edit-distribution algorithm. Due to this oversight, there has been no precedent of distributing edits across multiple layers when using ROME, nor has there been an instance of applying MEMIT to a single layer only. Our proposed unified framework bridges this gap, establishing a versatile and robust method for model editing that integrates both approaches seamlessly.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n**Experiment Setup:**\nThe experiment focuses on evaluating the performance of the edit-distribution algorithm, particularly when performing batched edits. The specific components of the setup are:\n\n1. **Experiments Conducted**:\n   - **Single Edits Comparison**: Comparing single edits using ROME and MEMIT.\n   - **Batched Edits Comparison**: Comparing batched editing in MEMIT.\n\n2. **Dataset**: \n   - **CounterFact Dataset**: A set of 1,000 randomly selected facts.\n\n3. **Models**:\n   - GPT-2 XL (1.5B parameters)\n   - GPT-J (6B parameters)\n   - Llama-2 7B (7B parameters)\n\n4. **Baselines**:\n   - **ROME**: The more stable version called r-ROME.\n   - **MEMIT**: Evaluated.\n\n5. **Evaluation Metrics**:\n   - Performance metrics specific to model editing.\n   \n**Key Findings:**\n\n1. **Single Edits**:\n   - Solutions to both ROME and MEMIT objectives performed equally well for making single edits across different metrics.\n\n2. **Batched Edits**:\n   - **Batch Size Performance**: MEMIT's ability to make successful batched edits depends on the model:\n     - **GPT-2 XL**: Effective up to a batch size of 1024.\n     - **Llama-2-7B**: Effective up to a batch size of 256.\n     - **GPT-J**: Capable of handling batch sizes as large as 4096.\n\n3. **Observations and Conclusions**:\n   - Effectiveness of optimization objectives should be evaluated independently.\n\n**Conclusion**: The results underscore the potential and limitations of the MEMIT edit-distribution algorithm, highlighting the need for more focused research on improving edit distribution methods to enhance model editing capabilities."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To analyze the impact of edit-distribution algorithms for making batched edits in model editing frameworks like ROME and MEMIT.",
            "experiment_process": "Two experiments were conducted: (1) Comparing single edits in ROME and MEMIT with and without edit distribution using 1k randomly selected facts from the CounterFact dataset. (2) Comparing batched editing in MEMIT with and without edit distribution across three models (GPT2-XL, GPT-J, Llama2-7B). The results were evaluated using various metrics and presented in different tables and figures.",
            "result_discussion": "Both ROME and MEMIT perform equally well for single edits without using edit distribution. When making batched edits with MEMIT, it was observed that performance increases when editing multiple layers after a certain batch size, with specific differences noted across different models. These results underscore the importance of edit-distribution algorithms in achieving effective batched edits and highlight areas for future research.",
            "ablation_id": "2403.14236v2.No1"
        },
        {
            "research_objective": "To stabilize EMMET by addressing the invertibility and condition issues of key matrices involved in the algorithm.",
            "experiment_process": "An analysis of the matrices involved in EMMET and MEMIT was conducted, focusing on their invertibility and condition numbers. The invertibility condition was theoretically examined by ensuring a sufficient number of independent vectors. To counter the issue of ill-conditioned matrices, a regularization term was introduced through a parameter (\u03bb) set to 0.1 after conducting an ablation study over multiple batch sizes.",
            "result_discussion": "The analysis revealed that while the invertibility of key matrices was almost always satisfied, the condition number posed issues. By setting a regularization term (\u03bb), the problem of numerical instability was mitigated, allowing for stable batched edits using EMMET.",
            "ablation_id": "2403.14236v2.No2"
        },
        {
            "research_objective": "To evaluate the performance of EMMET for model editing with varied batch sizes and compare it against MEMIT across different datasets and models.",
            "experiment_process": "Experiments were conducted with varied batch sizes using EMMET on models GPT2-XL, GPT-J, and Llama-2-7b across the CounterFact and zsRE datasets. Performance was compared for both single-layer and multi-layer edits. The results were presented across different metrics (ES, PS, S). Further, EMMET was evaluated using MEMIT's edit-distribution algorithm and compared against MEMIT over downstream tasks like sentiment analysis, paraphrase detection, natural language inference, and linguistic acceptability.",
            "result_discussion": "Both EMMET and MEMIT demonstrated nearly identical performance across different batch sizes and metrics, with MEMIT slightly outperforming EMMET for Llama-2-7b. Both algorithms successfully performed batched edits up to a batch size of 10,000 and led to similar model degradation in downstream tasks. While EMMET did not outperform MEMIT, it is a crucial step towards unifying model editing under the preservation-memorization objective and indicates potential limits in current model editing capabilities.",
            "ablation_id": "2403.14236v2.No3"
        }
    ]
}