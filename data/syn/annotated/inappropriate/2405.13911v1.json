{
    "title": "TOPA: Extend Large Language Models for Video Understanding via Text-Only Pre-Alignment",
    "abstract": "Recent advancements in image understanding have benefited from the extensive use of web image-text pairs.\nHowever, video understanding remains a challenge despite the availability of substantial web video-text data. This difficulty primarily arises from the inherent complexity of videos and the inefficient language supervision in recent web-collected video-text datasets.\nIn this paper, we introduce Text-Only Pre-Alignment (TOPA), a novel approach to extend large language models (LLMs) for video understanding, without the need for pre-training on real video data.\nSpecifically, we first employ an advanced LLM to automatically generate Textual Videos comprising continuous textual frames, along with corresponding annotations to simulate real video-text data.\nThen, these annotated textual videos are used to pre-align a language-only LLM with the video modality.\nTo bridge the gap between textual and real videos, we employ the CLIP model as the feature extractor to align image and text modalities.\nDuring text-only pre-alignment, the continuous textual frames, encoded as a sequence of CLIP text features, are analogous to continuous CLIP image features, thus aligning the LLM with real video representation.\nExtensive experiments, including zero-shot evaluation and finetuning on various video understanding tasks, demonstrate that TOPA is an effective and efficient framework for aligning video content with LLMs.\nIn particular, without training on any video data, the TOPA-Llama2-13B model achieves a Top-1 accuracy of 51.0% on the challenging long-form video understanding benchmark, Egoschema.\nThis performance surpasses previous video-text pre-training approaches and proves competitive with recent GPT-3.5-based video agents.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Image-language understanding has made large advancements in both image-language alignment [22  ###reference_b22###, 43  ###reference_b43###] and Multimodal Large Language Models (MLLMs) [1  ###reference_b1###, 23  ###reference_b23###, 29  ###reference_b29###, 79  ###reference_b79###], benefiting from pre-training on large-scale noise-paired image-text data collected from the web [6  ###reference_b6###, 17  ###reference_b17###, 45  ###reference_b45###, 47  ###reference_b47###, 46  ###reference_b46###].\nThis raises a question: Can we mirror this success in video-language understanding?\nResearch [40  ###reference_b40###, 56  ###reference_b56###, 67  ###reference_b67###, 76  ###reference_b76###] has explored pretraining video-language models on millions of web video-text data [3  ###reference_b3###, 35  ###reference_b35###], achieving promising results in basic video tasks such as video-text retrieval, video captioning, and video question answering across conventional video benchmarks.\nHowever, recent research reveals that these models struggle with the challenging long-form video understanding benchmark, i.e., Egoschema [34  ###reference_b34###], which requires intrinsic temporal understanding capabilities.\nThis highlights the gap in adapting web video-text pretrained models to more comprehensive video understanding tasks.\nWe attribute this gap to two primary factors:\n1) The intrinsic complexity of the video modality.\nVideos introduce intrinsic complexities in both spatial and temporal dimensions, which are absent in static images. These complexities require extensive training on larger-scale data to effectively capture video dynamics. Furthermore, representing videos typically involves processing multiple frames, significantly increasing computational demands compared to image modeling. The dual challenges of large-scale training and increased computational requirements make video-language modeling particularly challenging.\n2) The limitations of web language supervision.\nThe language supervision in recent web video-text datasets primarily comes from subtitles associated with the videos. These subtitles are often frame-level descriptions that lack the necessary temporal associations crucial for video understanding. Moreover, this form of descriptive supervision is inefficient in building robust reasoning capabilities, especially in terms of temporal reasoning. This mismatch between the richness of video content and the limited language supervision hinders effective video-language modeling.\nIn this paper, we propose an innovative approach to develop video understanding capabilities using LLMs to simulate video dynamics.\nInstead of directly aligning video with language, we introduce a textual video representation \u2014 a sequence of textual frames designed to mimic real visual dynamics.\nThis textual video can be readily generated by advanced LLMs and effectively simulates various video dynamics by describing them in text.\nSpecifically, we present a Textual Video (TextVid) dataset, automatically generated by LLMs.\nTextVid includes: 1) Textual videos (hereinafter referred to as \u201cTideo\u201d), which consist of a sequence of textual frames crafted to mimic the keyframes of real videos, and 2) Tideo annotations, including comprehensive Tideo-level dense descriptions and varied question-answer (QA) pairs. These annotations are of high quality and closely align with the Tideo content, by virtue of the powerful capability of LLM in language generation.\nBuilding on the proposed TextVid dataset, we introduce the Text-Only Pre-Alignment (TOPA) framework, to effectively and efficiently pre-align LLMs with the video modality, reducing the need for costly video-text pre-training.\nWe introduce three tasks for video-LLM pre-alignment: Tideo summarization, Tideo QA and multi-choice Tideo QA.\nTo bridge the gap between textual Tideos and visual videos, we leverage the CLIP model for feature extraction.\nSpecifically, we employ the CLIP text encoder to extract frame-level representations for Tideos, and the CLIP visual encoder for real videos.\nDuring the text-only pre-alignment phase, the LLM learns to process continuous CLIP text features.\nIn the real video inference phase, it transitions to handling continuous CLIP image features.\nDue to the aligned CLIP image-text feature space, the LLM can adapt to real video inputs despite being trained on textual representations.\nOur main contributions include:\n(1) We propose a novel Text-Only Pre-Alignment (TOPA) framework to extend Large Language Models (LLMs) for video understanding. TOPA aligns LLMs with the video modality efficiently and effectively without the need for training on real videos, reducing the costs for video-text pre-training.\n(2) We introduce TextVid, a textual video dataset automatically generated by advanced LLMs. TextVid dataset comprises 721K diverse Tideos along with associated high-quality annotations, which include detailed Tideo descriptions and a variety of question-answer pairs.\n(3) Extensive experiments demonstrate TOPA\u2019s effectiveness across various video understanding tasks. Particularly, the TOPA-Llama2-13B model achieves 51.0% Top 1 accuracy in the challenging EgoSchema benchmark, outperforming previous video-text pretraining methods and approaching recent GPT-4-based video agents."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Vision-Language Alignment.\nRadford et al. [43  ###reference_b43###] align the vision and language in a common feature space via contrastive learning with large-scale web image-text data.\n[1  ###reference_b1###, 23  ###reference_b23###, 29  ###reference_b29###, 79  ###reference_b79###] align the visual model with advanced LLM via training on image-caption pairs and interleaved image-text data.\n[7  ###reference_b7###, 19  ###reference_b19###, 28  ###reference_b28###, 73  ###reference_b73###] explore modeling video sequences within LLM spaces, leveraging LLM for video-language understanding.\nIn this paper, we focus on video-LLM alignment. Rather than using multimodal data for vision-language alignment, we introduce a novel text-only pre-alignment framework to extend LLMs for video understanding without pre-training on real video-text data.\nLLMs for multimodal data augmentation.\nRecent research explores to leverage LLMs to enhance the multimodal data. A line of work [5  ###reference_b5###, 12  ###reference_b12###, 29  ###reference_b29###] use LLMs for refining captions or extending the image caption pairs to diverse visual tasks like visual conversation and image editing. Another line of work [24  ###reference_b24###, 32  ###reference_b32###, 33  ###reference_b33###, 42  ###reference_b42###] further employ advanced LLM to enrich web video supervision for video instruction tuning.\nIn this paper, rather than enhancing multimodal datasets, we propose generating text-only data consisting of \"textual videos\" and diverse language supervision, which aims to simulate real videos and their corresponding annotations.\nLong-form Video Understanding.\nLong-form video understanding presents significant challenges due to the intricate spatial and temporal dynamics.\nConventional video-text pretraining approaches [4  ###reference_b4###, 40  ###reference_b40###, 57  ###reference_b57###, 58  ###reference_b58###, 67  ###reference_b67###, 80  ###reference_b80###] utilize extensive web video-caption data for video-language alignment.\nAnother line of research [18  ###reference_b18###, 48  ###reference_b48###, 71  ###reference_b71###] seeks to adapt recent image MLLMs to video understanding.\nRecent research [24  ###reference_b24###, 58  ###reference_b58###, 73  ###reference_b73###, 78  ###reference_b78###] employ video instruction-tuning for video-LLM alignment to enhance video-language understanding.\nRecently, [9  ###reference_b9###, 37  ###reference_b37###, 49  ###reference_b49###, 53  ###reference_b53###, 55  ###reference_b55###, 68  ###reference_b68###, 72  ###reference_b72###] combine the LLM with various VLM tools as video agents to perform video-understanding tasks.\nIn this paper, we propose a novel text-only pre-alignment framework to efficiently and effectively align LLMs with videos without pre-training on real videos.\n###figure_1###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Method",
            "text": "In this section, we detail the TOPA framework.\nWe first introduce the data generation pipeline of TextVid Dataset (Section 3.1  ###reference_###). Next, we describe how to align the Tideo representation with LLM (Section 3.2  ###reference_###). Finally, we discuss adapting the text-only aligned video-LLM model for real video inference (Section 3.3  ###reference_###). An overview is illustrated in Figure 1  ###reference_###."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "TextVid Dataset",
            "text": "This dataset, comprising textual videos (Tideos) and associated annotations, is generated by an advanced LLM (i.e., Gemini Pro 1.0 [50  ###reference_b50###]).\nThe data generation pipeline is detailed in Appendix B  ###reference_###.\nEach Tideo is presented in a textual format and contains 5-15 sequential frames. Each frame includes a frame caption that describes the scene and multiple object captions. To enhance understanding and interaction with these Tideos, the dataset features a dense description summarizing the Tideo, as well as a set of multiple-choice questions and answers related to the Tideo content.\nThe structure of each element is as follows:\nDataset Element:\n\n\nTideo: Sequence of textual frames \n\n\n\nFor each frame :\n\n\n\n\nFrame caption: \n\n\n\n\nObject captions:  for main objects in \n\n\nAnnotations:\n\n\n\nGlobal Dense Description of the Tideo: \n\n\n\nSet of Questions-Options-Answers:\nThere are two major advantages of the TextVid dataset.\n(1) The large-scale and diverse Tideos. As the dataset is text-only and fully generated by an LLM, the size of TextVid is unlimited.\nMoreover, the Tideos can cover a broad range of domains by simply prompting the language model with appropriate conditions. It is distinctly different from previous web video-text dataset like Howto100M [35  ###reference_b35###] that are limited to specific human-centric instructional videos. In practice, we enhance the diversity of TextVid by randomly sampling video captions from WebVid-2M [3  ###reference_b3###], video titles from Howto100m [35  ###reference_b35###], video tasks from Ego4D [14  ###reference_b14###] and object names with descriptions from WordNet [36  ###reference_b36###] as a condition of prompts. These varied prompts enable the language model to generate a diverse dataset.\n(2) The high-quality, consistent and free-form language supervision.\nThe language supervisions are generated along with Tideos. The advanced capabilities of LLM ensure the quality of these supervisions, making them less noisy than web video-text data. Moreover, both the Tideo and the supervision are in textual format, making the supervision closely aligned with the Tideo\u2019s content. Additionally, the format of the language supervision is unrestricted. For example, we prompt an LLM to generate dense description and multi-choice QA pairs as language supervision in TextVid."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Text-only Pre-alignment",
            "text": "Video-LLM alignment. The goal of video-LLM alignment is to extend the language-only LLMs for processing video inputs. Given a video sampled with frames, we follow recent work [19  ###reference_b19###, 67  ###reference_b67###] by using a simple rule-based approach to manually extract visual features from video frames, formulated as , where  represents the simplistic extraction mechanism. These manually extracted features are then projected into the LLM space via a simple linear layer, denoted as , where  denotes a language model and  denotes a projection layer that projects the manually extracted feature to LLM space. Tideo Representation. In this work, we leverage Tideos (c.f. Section 3.1  ###reference_###) for video-LLM pre-alignment instead of training on real videos. Specifically, given the textual frame , we employ CLIP text encoder to extract the frame representation from frame caption  and detailed object captions , represented as , where  is a fusion function such as simple average pooling, and  denotes the CLIP text encoder. A Tideo with  textual frames is represented as Text-only Training. Given the Tideo , dense Tideo-level description , and QA pairs with multiple choices , we introduce the following tasks for Tideo-LLM alignment: (1) Tideo Summarization: Given the Tideo, generate a detailed description to summarize the Tideo; (2) Tideo QA: Given the Tideo and question, predict the answer; (3) Multi-choice Tideo QA: Given the Tideo, question and multiple choices, choose the correct answer from the candidates. We employ a unified auto-regressive Language Modeling (LM) objective for these three tasks: where  denotes the Tideo representation, and  during the text-only training, Z denotes the task specific condition tokens and  denotes the  target token.  and  denote the learnable parameters of the LLM adapter and the projection layer , respectively. In practice, we use the following format as the LLM input: {Task Instruction}. Video:{, \u2026, }. {Task Conditions}. Answer: {Predict Targets}. For the Tideo summarization task, the target is detailed Tideo descriptions. For Tideo QA task, the target is the answer and the condition is the question. For multi-choice Tideo QA task, the target is the correct option and the condition consists with question and options. The details of the task-specific prompts are included in Appendix D.1  ###reference_###."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Zero-shot Inference",
            "text": "Section 3.2  ###reference_### introduces the text-only pre-alignment using the TextVid dataset. In this section, we detail how to adapt this text-only pre-aligned LLM for real videos.\nDuring pre-alignment, we leverage the textual representation  as the Tideo representation. During inference, we take real videos features as input, i.e., , where . These two modality features  and  that come from CLIP image encoder and CLIP text encoder are aligned via CLIP pre-training. This aligned image-text representation makes it possible to perform zero-shot inference without additional finetuning. However, the modality gap phenomenon [15  ###reference_b15###, 26  ###reference_b26###, 27  ###reference_b27###, 39  ###reference_b39###, 75  ###reference_b75###], i.e., CLIP image feature and CLIP text feature are located in two completely separate regions of the feature space, prevents us from directly taking the visual feature  as the textual feature .\nTo bridge this modality gap, we follow DeCap [26  ###reference_b26###] to employ a support memory to project the CLIP visual feature into the CLIP text feature space. This training-free projection process is formulated as:\nwhere  denotes CLIP text feature from a pre-constructed memory of size ,  denotes input frame feature of real video and  denotes the projected feature. During zero-shot inference, we take the  as the real video\u2019s representation."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Implementation Details",
            "text": "We leverage Llama2-7B, Llama2-13B [51  ###reference_b51###] and Llama3-8B as the LLM backbone. Additionally, we employ the Llama-adapter [74  ###reference_b74###] with an adaptation embedding length of 50 for efficient finetuning.\nWe utilize CLIP-ViT-L as the multimodal encoder. We employ a simple linear layer to project the CLIP feature into the LLM feature space.\nThe CLIP model and LLM backbone are frozen. The projection layer and additional Llama-adapter are trainable.\nFor text-only pre-alignment,\nwe uniformly sample the Tideos into 10 frames.\nWe train the model on a mixture of tasks comprising Tideo summarization, Tideo QA, multi-choice Tideo QA with the ratio of 1:1:2.\nTOPA-Llama2-7B and TOPA-Llama3-8B are trained on four 40G-A100 GPUs in one day. TOPA-Llama2-13B is trained in two days.\nFor zero-shot inference, we construct a memory for cross-modal projection, consisting of 2M CLIP text features sampled from captions in the TextVid dataset.\nMore training details of TOPA and baselines are included in Appendix C.2  ###reference_###."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "TOPA enables the LLM to perform various video understanding tasks as shown in Figure 2  ###reference_###.\nIn this section, we evaluate TOPA on multi-choice video QA and video captioning tasks.\nSection 4.1  ###reference_### evaluates TOPA on NeXT-QA [62  ###reference_b62###], STAR [61  ###reference_b61###], TVQA [20  ###reference_b20###], and recent challenging EgoSchema [34  ###reference_b34###] benchmarks with the zero-shot setting.\nWe further evaluate TOPA on multi-choice video QA with the finetuning setting (Section 4.2  ###reference_###) and zero-shot video captioning task (Section 4.3  ###reference_###).\nIn Section 4.4  ###reference_###, we conduct ablation study on the LLM prior and input video frames.\nWe mainly compare TOPA against four categories of video understanding methods as follows:\n(1) Web video pre-training approaches [4  ###reference_b4###, 40  ###reference_b40###, 56  ###reference_b56###, 58  ###reference_b58###, 67  ###reference_b67###].\nThis line of work aims to develop general video-language models by leveraging extensive web videos, using associated video captions or audio as weak supervision signals.\n(2) Adapt image MLLMs for video understanding [18  ###reference_b18###, 71  ###reference_b71###].\nThese approaches aim to extend the image understanding capabilities of recent vision-language models (VLMs) to video understanding.\nSpecifically, SeViLa [71  ###reference_b71###] utilizes BLIP-2 for localizing and understanding key frames of a video.\nIG-VLM [18  ###reference_b18###] converts video into a composite image by arranging the video frames into a grid layout.\n(3) Instructed Video MLLMs [24  ###reference_b24###, 58  ###reference_b58###, 73  ###reference_b73###, 78  ###reference_b78###]. Recent research employ video instruction-tuning for video-LLM alignment to enhance video-language understanding.\n(4) LLM-based video agents [37  ###reference_b37###, 52  ###reference_b52###, 53  ###reference_b53###, 55  ###reference_b55###, 72  ###reference_b72###].\nThis line of work leverages LLMs like GPT-3.5 and GPT-4 as an agent to understand a video by designing and executing a series of actions.\nThe language-only agents perceive visual information via recent foundation VLMs (e.g., CLIP [43  ###reference_b43###], BLIP-2 [23  ###reference_b23###], LaViLa [77  ###reference_b77###] and PALI [8  ###reference_b8###]).\n(5) Our text-only pre-alignment. Different from the above works, TOPA leverages the proposed TextVid dataset for video-LLM pre-alignment, enabling the LLM to process continuous features. Thus, performing video understanding tasks."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Zero-Shot Evaluation on Multi-Choice Video QA",
            "text": "In Section 4.1.1  ###reference_.SSS1###, we compare TOPA with a wide range of approaches on a recent challenging long-form video understanding benchmark, i.e., EgoSchema [34  ###reference_b34###].\nIn Section 4.1.2  ###reference_.SSS2###, we further evaluate TOPA on conventional multi-choice video QA benchmarks including NeXT-QA [62  ###reference_b62###], STAR [61  ###reference_b61###] and TVQA [20  ###reference_b20###].\nIn Section 4.1.3  ###reference_.SSS3###, we evaluate TOPA on recent comprehensive video understanding benchmark, i.e., MVBench [25  ###reference_b25###].\nThe details of these benchmarks are included in Appendix C.1  ###reference_###."
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1 Zero-shot Results on EgoSchema",
            "text": "###table_1### Results on Egoschema. Table 1  ###reference_### shows the results. We compare our method against a range of recent approaches in video understanding. Our proposed text-only pre-alignment framework, despite training without real videos, shows impressive results on the Egoschema benchmark. TOPA outperforms previous image-based adaptation approach IG-VLM and video agents LLoVi and Vamos with the same scale LLM (Llama2-7B and Llama2-13B). Moreover, TOPA-Llama2 demonstrates consistent improvements when scaled up with a larger LLM backbone, indicating the effectiveness of LLMs in complex video-language understanding scenarios.\nDiscussion 1: The necessity of high-quality language supervision for video understanding.\nRecent video pre-training approaches like LongViVit [40  ###reference_b40###] and InternVideo [56  ###reference_b56###], despite training on million-level web video-text data, show inferior performance on Egoschema evaluation. These results highlight the inefficacy and inefficiency of conventional contrastive pre-training in understanding long-form videos, primarily due to noisy and simplistic language supervision.\nIn contrast, our TOPA, trained on 721K virtual video (i.e., Tideo) with high-quality language supervision, shows impressive results on Egoschema.\nIt indicates that, unlike image understanding which significantly benefits from leveraging web language as supervision, video understanding may require more precise and accurate language supervision to better capture the complex visual dynamics.\nDiscussion 2: Video agents versus end-to-end video-LLM modeling.\nVideo agents have shown impressive results on the EgoSchema benchmark, aided by advanced LLMs and VLMs. However, a significant limitation of these approaches is their heavy reliance on the powerful LLMs. For example, the accuracy of Vamos drops by -11.6% when the GPT-4 is replaced with Llama2-13B.\nThe reliance on powerful closed-source LLMs restricts its application fields and introduces external overheads.\nAdditionally, the inference speed of these approaches is another concern, since it involves multiple interactions with both VLMs and LLMs. In contrast, end-to-end video-LLM models, which condense the video into a sequence of embeddings as the input of LLM, are more efficient. Moreover, video agents make decisions based on the language format clues collected by VLMs. Converting the video content into language clues may lead to a limited upper bound compared to end-to-end modeling.\n###figure_2###"
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "4.1.2 Zero-shot Results on NExT-QA, STAR and TVQA",
            "text": "Table 2  ###reference_### shows the multi-choice video QA results across various benchmarks.\nTOPA achieves impressive performance on the TVQA and EgoSchema benchmarks, significantly outperforming previous video pre-training models and image-to-video adaptation approaches. This indicates that our TOPA framework effectively enables LLMs to handle video input, despite not being pre-trained on real videos.\nHowever, for the NeXT-QA and STAR benchmarks, TOPA underperforms compared to SeViLA and IG-VLM. A major reason is that these benchmarks involve many fine-grained visual questions, including those about object locations and relationships. SeViLA and IG-VLM, benefiting from the advanced image-understanding capabilities of pre-trained VLMs such as LLaVA, excel in answering these fine-grained visual questions.\nIn contrast, our TOPA framework primarily focuses on high-level semantic alignment. Moreover, during zero-shot inference, we project the visual features into the text feature space to bridge the modality gap, as described in Eq. 2  ###reference_###. This cross-modal semantic projection process tends to overlook fine-grained visual details, such as object locations, which leads to inferior performance on the STAR benchmark. We provide extensive qualitative results to illustrate TOPA\u2019s advantages and limitations across various video understanding tasks in Appendix A.3  ###reference_###.\n###table_2###"
        },
        {
            "section_id": "4.1.3",
            "parent_section_id": "4.1",
            "section_name": "4.1.3 Zero-shot Results on MVBench",
            "text": "MVBench [25  ###reference_b25###] is a recent video-language understanding benchmark that covers 20 challenging video tasks, regrouped from existing video-language benchmarks. For details of these 20 video understanding tasks, please refer to  [25  ###reference_b25###].\nTable 3  ###reference_### shows the results.\nTOPA demonstrates impressive results compared to previous image MLLMs and video MLLMs. It excels particularly in tasks requiring high-level video-language understanding, such as Scene Transition (ST), Episodic Reasoning (ER), and Unexpected Action (UA).\nSurprisingly, TOPA also excels in the Action Localization (AL) task, which requires identifying the moment an action occurs. This indicates that the text-only pre-alignment enables the LLM to understand visual temporal sequences.\nHowever, TOPA struggles with tasks that demand fine-grained visual understanding, such as Moving Direction (MR), Action Antonym (AA), and Object Shuffle (OS). A common challenge in these tasks is the requirement for detailed visual understanding. For example, Action Antonym involves identifying the direction of an action, while Object Shuffle involves locating objects.\nTOPA struggles in these fine-grained visual tasks since it is trained with CLIP text features. The modality gap between CLIP text features and image features hinders TOPA from capturing visual details.\nFurther video instruction tuning might address this limitation, which we leave for future work. We provide qualitative results in Figure 4  ###reference_### - 7  ###reference_### to illustrate TOPA\u2019s advantages and limitations on various video understanding tasks."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Finetuning Results",
            "text": "In this section, we further finetune the pre-aligned TOPA models to study the benefits of TOPA for downstream supervised learning.\nDuring finetuning, TOPA directly takes the video feature as input without the cross-modal projection.\nMore finetuning details for each dataset are provided in Appendix C.2  ###reference_###.\nTable 4  ###reference_### shows the finetuning results on multi-choice video QA dataset including NExT-QA, STAR and TVQA.\nFor comparison, we include baseline models without text-only pretraining.\nOur text-only pre-alignment consistently improves the performance across three benchmarks. Notably, TOPA-Llama2-7B achieves 67.1% accuracy on TVQA, outperforming other approaches by a large margin.\nAdditionally, in the zero-shot evaluation (Table 2  ###reference_###), TOPA-Llama2-7B shows inferior results on STAR compared to SeViLA, due to its limitation in handling fine-grained visual details. However, in this finetuning setting, TOPA-Llama2-7B outperforms SeViLA, indicating that finetuning on downstream video data effectively mitigates TOPA\u2019s limitations in processing fine-grained visual details.\nThese results suggest that our text-only pre-alignment, despite not training with real video data, has a similar effect to conventional video-text pre-training.\n###table_3### Data Efficient Finetuning\nFigure 3  ###reference_### shows the results of finetuning LLMs with various ratios of training data. TOPA trained with 10% data achieves 64.7% Top 1 accuracy on NeXT-QA benchmark, significantly outperforming the baseline that without text-only pre-alignment. Besides, when trained with less than 20% data, the baseline model even performs worse than TOPA-ZS on NeXT-QA and TVQA, demonstrating the effectiveness of TOPA in limited annotated data scenarios.\n###figure_3###"
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Video Captioning",
            "text": "Results on Zero-shot Video Captioning.\nWe further perform zero-shot video captioning on MSR-VTT [65  ###reference_b65###] and VATEX [54  ###reference_b54###]. As shown in Table 5  ###reference_###, TOPA largely outperforms previous text-only approaches like Decap which is trained on captions sourced from CC3M [47  ###reference_b47###]. TOPA even outperforms the video-text pre-training approaches like VideoCoCa, which is pre-trained on millions of videos-text data, demonstrating that TOPA is an efficient and effective framework for video-LLM alignment.\n###table_4###"
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Ablations",
            "text": "LLM Prior in Video-language Understanding.\nTo investigate the impact of LLM prior in multi-choice video QA, we conduct experiments on EgoSchema with the blind setting, where only the questions and choices are provided to the LLM. Table 7  ###reference_### shows the results.\nBard and GPT-4-Turbo achieve 33.2% and 30.8% accuracy, respectively. Gemini-Pro-1.0 reaches 38.2% accuracy.\nThese blind results of advanced LLMs suggest that in some video QA cases, LLMs can accurately choose the correct answer solely based on the question and choices, without visual input.\nHowever, the blind performance of Llama2-7B and Llama2-13B is inferior, potentially due to their smaller model size.\nAfter training on the TextVid dataset, TOPA-Llama2-13B achieves a blind accuracy of 37.5% (or +11.7%), closely approaching that of Gemini-Pro-1.0 model. These results suggest that text-only pre-alignment can effectively prepare LLMs for downstream video-language tasks by leveraging specialized text-only tasks, even in complex scenarios where the original LLMs are limited.\n[7cm]\n\n\n\n\n \nVisual Input\n  ES Full\n\nRandom Selection\n\u2717\n20.0\n\nGPT-4-Turbo \u2020\n30.8\n\nBard \u2020\n33.2\n\nGemini-Pro-1.0\n38.2\n\n\\cdashline1-3\nLlama2-7B\n\u2717\n20.1\n\nLlama2-13B\n25.8\n\n\\cdashline1-3\nTOPA-Llama2-7B\n\u2717\n29.3\n\nTOPA-Llama2-13B\n37.5\n\nTOPA-Llama2-7B\n\u2714\n41.2\n\nTOPA-Llama2-13B\n51.0\n\n \n\n\n\n\\capbtabbox\n\n\n TOPA\n#Frame\nNextQA\nES Full\n\nLlama2-7B\n1\n56.1\n39.4\n\n5\n58.9 (+2.8)\n41.0 (+1.6)\n\n10\n59.9 (+3.8)\n41.2 (+1.8)\n\nLlama2-13B\n1\n57.3\n47.6\n\n5\n60.8 (+3.5)\n50.5 (+2.9)\n\n10\n62.1 (+4.8)\n51.0 (+3.4)\nThe Impact of Video Frames.\nTo better investigate TOPA\u2019s capability in understanding temporal dynamics of real videos, we conduct experiments with different number of frames. Table 7  ###reference_### shows the results.\nMultiple frames input consistently enhances performance on NeXT-QA and Egoschema for both TOPA-Llama2-7B and TOPA-Llama2-13B.\nThis indicates that the text-only pre-alignment effectively enables the LLM to handle multiple video frames, despite not being trained on real videos."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Limitations.",
            "text": "Modality gap in CLIP. Despite the fact that TOPA achieves impressive results.\nA significant limitation in TOPA is the gap between the CLIP text feature and CLIP image feature. On the one hand, we use the CLIP text feature for pre-alignment, while inference is with the CLIP visual feature. The modality gap makes the performance degrades, despite we employ a modality projection mechanism to mitigate it. On the other hand, the CLIP text features cannot fully capture the fine-grained visual details present in actual images, such as object locations and relationships. This limitation causes TOPA to struggle in scenarios where questions involve detailed visual information, as shown in Appendix A.3  ###reference_###.\nStruggles in fine-grained visual understanding. In TOPA, we propose textual videos to mimic real videos. However, this approach primarily focuses on keyframes understanding, which is insufficient for scenarios requiring the model to process hundreds of frames at high fps, such as action counting tasks.\nBesides, for the fine-grained action understanding scenarios, TOPA is unable to capture the fine-grained visual information. For example, in a scene where a person climbs a ladder, it is difficult for TOPA to identify whether the person is going up or down due to the limited capability to capture detailed visual dynamics.\nFurther enhancing TOPA with video instruction tuning might address these limitations which we leave for future work."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Broader Impact",
            "text": "Academic Impact. TOPA\u2019s methodology, which frees the need for costly video-text data collection and large-scale pre-training, lowers the barriers to entry for research and development in video-language understanding technologies. The text-only learning framework of TOPA may inspire researchers with limited resources to engage in cutting-edge multi-modal research, providing a more diverse range of perspectives and contributions to this field.\nSocial Impact.\nThe ultimate objective of TOPA is to develop a general video-language understanding model. Its primary application enables users to extract information from long-form videos without the need for detailed viewing. Moreover, these capabilities for interpreting and managing video content could significantly enhance content moderation systems. Platforms hosting user-generated content could employ sophisticated video-language models to efficiently detect and mitigate the effects of inappropriate or harmful video content."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusions",
            "text": "In this paper, we introduce TOPA, a text-only pre-alignment framework designed for aligning LLMs with video modality without requiring training on real videos. TOPA has demonstrated remarkable performance on the recent, challenging long-form video understanding benchmark, i.e., EgoSchema, showcasing that a text-only approach is effective in capturing the dynamics of long-form videos. Our approach, which includes data generation and text-only pre-alignment, has potential applications across various vision-language tasks where obtaining paired vision-language data is difficult."
        }
    ],
    "url": "http://arxiv.org/html/2405.13911v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.4"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.1.1",
            "4.1.2",
            "4.1.3",
            "4.2",
            "4.3",
            "4.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.4"
        ]
    },
    "research_context": {
        "paper_id": "2405.13911v1",
        "paper_title": "TOPA: Extend Large Language Models for Video Understanding via Text-Only Pre-Alignment",
        "research_background": "### Summary of Paper\u2019s Key Aspects\n\n**Motivation:**\nThe paper is motivated by the considerable progress made in image-language understanding, which has been bolstered by large-scale pre-training on image-text data from the web. This success raises the question of whether similar advancements can be achieved in video-language understanding. Existing research has demonstrated potential in pre-training video-language models with web video-text data for basic video tasks. However, these models struggle with complex long-form video understanding tasks, such as those posed by the EgoSchema benchmark, which rely heavily on intrinsic temporal understanding.\n\n**Research Problem:**\nThe primary research problem addressed by this paper is the challenge of extending the capabilities of Large Language Models (LLMs) for comprehensive video understanding without the computational expense and the inefficiencies associated with video-text pre-training. The key issues identified are:\n1. The intrinsic complexity and computational demands of video data, which include both spatial and temporal dimensions.\n2. The inadequacy of current web language supervision (primarily subtitles) for developing robust video-language reasoning capabilities, particularly temporal reasoning.\n\n**Relevant Prior Work:**\n1. **Image-Language Understanding**: Previous work has shown significant improvements in image-language alignment using multimodal large language models (MLLMs) that leverage large-scale noise-paired image-text data [references: 1, 22, 23, 29, 43, 45, 46, 47, 79].\n2. **Video-Language Models**: Research has explored pre-training video-language models on extensive web video-text datasets, showing promise in basic video tasks such as video-text retrieval, video captioning, and video question answering [references: 3, 35, 40, 56, 67, 76].\n3. **Challenges in Long-Form Video Understanding**: Models struggle with benchmarks like Egoschema, highlighting a gap in adapting web video-text pretrained models to tasks requiring deeper temporal understanding [reference: 34].\n\n### Contributions and Methodology:\nTo tackle these challenges, the paper introduces a new approach:\n1. **Textual Video Representation**: Introducing a textual video representation called Tideo, which simulates video dynamics through sequences of textual frames generated by LLMs.\n2. **TextVid Dataset**: Creating a dataset (TextVid) with 721K diverse Tideos and annotations, including descriptions and QA pairs, generated by advanced LLMs.\n3. **Text-Only Pre-Alignment Framework (TOPA)**: Proposal of the TOPA framework, which aligns LLMs with video modality without direct video training, leveraging the CLIP model for feature extraction and ensuring efficient alignment through text-only pre-training. \n4. **Experimental Validation**: Validating the approach through extensive experimentation, demonstrating the effectiveness of TOPA, particularly in achieving a 51.0% Top 1 accuracy on the challenging EgoSchema benchmark.\n\nThe initiative is to bypass traditional video-text pre-training inefficiencies by leveraging textual representations to train LLMs, which can then effectively infer on real video inputs using an aligned CLIP image-text feature space.",
        "methodology": "### Methodology\n\nIn this section, we detail the TOPA framework.\n\n1. **Data Generation Pipeline of TextVid Dataset (Section 3.1 ###reference_###):** We first introduce the process used to generate the TextVid Dataset. This dataset serves as the foundation for our framework, converting video content into text-based descriptions which are then used for subsequent model training and alignment.\n\n2. **Aligning the Tideo Representation with LLM (Section 3.2 ###reference_###):** Next, we describe the methodology for aligning the Tideo (text-based video) representation with Large Language Models (LLM). This step ensures that the video's textual representation is compatible and can be effectively processed by the language models, bridging the gap between video content understanding and text processing capabilities.\n\n3. **Adapting the Text-Only Aligned Video-LLM Model for Real Video Inference (Section 3.3 ###reference_###):** Finally, we discuss how the text-only aligned video-LLM model can be adapted for inference on real video data. This includes techniques and adjustments needed to translate the model's learning from text-based datasets to actual video content, enhancing its application to real-world scenarios.",
        "main_experiment_and_results": "## Main Experiment Setup and Results\n\n### Experiment Setup\n\n**Datasets:** The evaluation of TOPA is conducted on several benchmarks for multi-choice video question answering (QA) and video captioning tasks. These include:\n- NeXT-QA\n- STAR\n- TVQA\n- EgoSchema\n\n**Baselines:** The performance of TOPA is compared against four categories of video understanding methods:\n1. **Web video pre-training approaches**: These methods use extensive web videos and associated captions or audio as weak supervision signals to develop general video-language models.\n2. **Adapt image MLLMs for video understanding**: These approaches extend the capabilities of vision-language models (VLMs) to video understanding. Examples include:\n   - SeViLa, which utilizes BLIP-2 for localizing and understanding key frames of a video.\n   - IG-VLM, which arranges video frames into a grid layout to convert video into a composite image.\n3. **Instructed Video MLLMs**: Recent methods that employ instruction-tuning for video-LLM alignment to enhance video-language understanding.\n4. **LLM-based video agents**: These methods leverage large language models (LLMs) like GPT-3.5 and GPT-4 as agents to understand videos by designing and executing actions.\n5. **Our text-only pre-alignment**: TOPA uses the proposed TextVid dataset for video-LLM pre-alignment, enabling LLMs to process continuous features and thus perform video understanding tasks.\n\n**Evaluation Metrics:**\n- For multi-choice video QA, performance is typically assessed using accuracy.\n- For video captioning, evaluation metrics include BLEU, METEOR, and CIDEr, which are standard metrics for assessing the quality of generated captions compared to reference sentences.\n\n### Main Experimental Results\n\nTOPA enables large language models (LLMs) to perform various video understanding tasks effectively. Key findings include:\n\n- **Zero-Shot Performance:** When evaluated in a zero-shot setting on NeXT-QA, STAR, TVQA, and the challenging EgoSchema benchmarks, TOPA displayed notable performance without needing task-specific training. This indicates the robustness and generalization ability of the pre-aligned LLM via TOPA.\n- **Fine-tuning Performance:** The model's performance was further improved when fine-tuning was allowed on multi-choice video QA tasks, highlighting the adaptability of TOPA to specific datasets and tasks.\n- **Video Captioning:** In zero-shot video captioning tasks, TOPA demonstrated competitive results across standard video captioning metrics (BLEU, METEOR, CIDEr), showcasing its capability to generate coherent and contextually relevant video descriptions.\n\nIn summary, TOPA proves to extend the video understanding capabilities of large language models, outperforming or matching the state-of-the-art across various challenging benchmarks in both zero-shot and fine-tuning settings."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To investigate the impact of LLM prior in multi-choice video QA and to evaluate the effectiveness of text-only pre-alignment for enhancing video-language understanding.",
            "experiment_process": "Experiments were conducted on EgoSchema with a blind setting where only the questions and choices were provided to various LLMs without any visual input. Advanced LLMs like GPT-4-Turbo, Bard, and Gemini-Pro-1.0 were tested alongside Llama2-7B and Llama2-13B. Additionally, the performance of TOPA-Llama2-13B after training on the TextVid dataset was evaluated for comparison. The accuracy of responses was used as the primary evaluation metric.",
            "result_discussion": "Bard and GPT-4-Turbo achieved 33.2% and 30.8% accuracy, respectively. Gemini-Pro-1.0 reached 38.2% accuracy. The blind performance of Llama2-7B and Llama2-13B was inferior. After training on the TextVid dataset, TOPA-Llama2-13B achieved a blind accuracy of 37.5%, closely approaching that of Gemini-Pro-1.0. These results suggest that text-only pre-alignment can effectively prepare LLMs for downstream video-language tasks by leveraging specialized text-only tasks.",
            "ablation_id": "2405.13911v1.No1"
        },
        {
            "research_objective": "To better understand TOPA's capability in video temporal dynamics by investigating the impact of using different numbers of frames.",
            "experiment_process": "Experiments were conducted on NeXT-QA and Egoschema benchmarks using TOPA models pre-aligned with different number of frames (1, 5, and 10). The LLMs used were TOPA-Llama2-7B and TOPA-Llama2-13B. Performance improvements were measured in terms of accuracy across the varying number of input frames.",
            "result_discussion": "Multiple frames input consistently enhanced performance on NeXT-QA and Egoschema for both TOPA-Llama2-7B and TOPA-Llama2-13B. For example, Llama2-13B improved from 57.3% to 62.1% accuracy on NeXT-QA and from 47.6% to 51.0% on Egoschema when the number of frames increased from 1 to 10. This indicates that text-only pre-alignment effectively enables LLMs to handle multiple video frames, despite not being trained on real videos.",
            "ablation_id": "2405.13911v1.No2"
        }
    ]
}