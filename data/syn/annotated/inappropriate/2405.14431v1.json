{
    "title": "RaFe: Ranking Feedback Improves Query Rewriting for RAG",
    "abstract": "As Large Language Models (LLMs) and Retrieval Augmentation Generation (RAG) techniques have evolved, query rewriting has been widely incorporated into the RAG system for downstream tasks like open-domain QA. Many works have attempted to utilize small models with reinforcement learning rather than costly LLMs to improve query rewriting. However, current methods require annotations (e.g., labeled relevant documents or downstream answers) or predesigned rewards for feedback, which lack generalization, and fail to utilize signals tailored for query rewriting. In this paper, we propose RaFe, a framework for training query rewriting models free of annotations. By leveraging a publicly available reranker, RaFe provides feedback aligned well with the rewriting objectives. Experimental results demonstrate that RaFe can obtain better performance than baselines.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large Language Models (LLMs) have demonstrated strong capacities to solve a variety of tasks Zhao et al. (2023  ###reference_b53###).\nHowever, they still encounter the challenges of hallucinations (Ji et al., 2023  ###reference_b9###; Zhang et al., 2023  ###reference_b52###; Huang et al., 2023  ###reference_b8###) or outdated knowledge (Yao et al., 2023  ###reference_b48###; Zhang et al., 2024  ###reference_b51###).\nRecently, Retrieval Augmentation Generation (RAG) (Gao et al., 2023  ###reference_b7###) has become an important technology to enhance LLMs\u2019 abilities, by incorporating external knowledge.\nFor instance, in open-domain QA, LLMs can firstly retrieve related documents and then generate answers.\nNonetheless, directly retrieving by original query does not always achieve correct and relevant documents.\nTherefore, query rewriting (Efthimiadis, 1996  ###reference_b6###; Carpineto and Romano, 2012  ###reference_b3###) has been widely employed to reformulate the query to expand the retrieved documents for a better response as illustrated in Figure 1  ###reference_###.\n###figure_1### Many efforts have been proposed to leverage the powerful LLMs to directly generate rewrites (Shen et al., 2023  ###reference_b36###; Wang et al., 2023  ###reference_b42###).\nWhile in practical applications, it is more prevalent to implement specific small query rewriting models to avoid the costly use of LLMs (Ma et al., 2023  ###reference_b22###).\nTo improve the performance of query rewriting, reinforcement learning (RL) with feedback  (Wu et al., 2022  ###reference_b43###; Chen et al., 2022  ###reference_b4###) can be utilized as a typical solution.\nFor instance, Nogueira and Cho (2017  ###reference_b26###) generates feedback by considering the recall of labeled documents.\nMeanwhile, Ma et al. (2023  ###reference_b22###) leverages evaluation results from question answering (QA) post-rewriting to generate signals.\nAdditionally, Peng et al. (2023  ###reference_b30###) employs domain-specific annotated rewriting scores for feedback training.\nNote that these feedback-driven query rewriting methods rely on either annotated labels such as relevant documents or answers, or pre-designed rewards tailored to specific domains.\nHowever, they often lack the utilization of effective and general signals for query rewriting.\nMeanwhile, considerable efforts have been made to harness diverse feedback mechanisms across various domains (Nathani et al., 2023  ###reference_b25###; Li et al., 2023  ###reference_b17###).\nNotably,  Liu et al. (2023b  ###reference_b21###) effectively integrates unit testing feedback into code generation, yielding significant efficacy.\nDrawing from these, in this paper we attempt to\n(i) reduce the cost of annotations for feedback;\nand (ii) identify a signal that better aligns with the objectives of the query rewriting task.\nTo address these issues, we introduce RaFe (Ranking Feedback improves Query Rewriting), a novel framework that leverages feedback from the reranker to train query rewriting models.\nThis approach is inspired by the reranker module in traditional information retrieval (IR) systems, which score and sort retrieved documents based on the query.\nIntuitively, query rewriting aims to retrieve documents relevant to the original query, which aligns perfectly with the goal of the reranker. Specifically, the reranker is capable of scoring documents without requiring additional labels.\nThus, we incorporate a reranker to provide feedback for the query rewriting model.\nRaFe comprises a two-stage process.\nWe first train an initial query rewriting model by standard supervised fine-tuning.\nSubsequently, we utilize the ranking scores from the reranker to conduct feedback training on the query rewriting model.\nRaFe supports both offline and online RL feedback training.\nEmpirically, we demonstrate that utilizing a general, publicly available reranker, RaFe can drive the training of the query rewriting model, indicating the effectiveness and potential generalizability of the proposed approach.\nThe main contributions of our paper can be summarized as follows:\nWe propose RaFe, a novel query rewriting framework that utilizes feedback from the reranker, an especially fitting signal for the objective of retrieving more relevant documents.\nRaFe does not necessitate annotated labels or particularly designed scores, ensuring the generalizability of the training framework.\nWe validate the effectiveness of our proposed approach on cross-lingual datasets across wide settings with a general and public reranker, we further conduct a comprehensive investigation of what makes a better query rewriting and how ranking feedback works.\n###figure_2###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Method",
            "text": "For offline feedback, we use random selection of rewrites without any scoring or ranking to determine their quality. Specifically, we set a threshold to distinguish the good and bad rewrites formulated as , which is computed as the average selection for all training instances as follows:\nThen for every rewrite  with a random selection exceeding the threshold , we regard it as a good rewrite for the original query ; otherwise, it is deemed a bad rewrite. In this way, we obtain all the preference pairs for open domain QA in the form .\nFor the offline feedback training, we use DPO (Rafailov et al., 2023  ###reference_b32###) and KTO (Kawin et al., 2023  ###reference_b12###).\nDPO directly leverage the preference pairs to optimize the model,\nwhile KTO is a method that can optimize the model from feedback, only needs the signal of whether a rewrite  is good or not, rather than needing pairs, formulated as .\nThe specific formulation of  is in Eq 6  ###reference_###,\nand the detailed explanation of the KTO is demonstrated in Appendix A.2.1  ###reference_.SSS1###.\nThe ranking score can also serve as an online feedback signal.\nWe utilize the Proximal Policy Optimization (PPO) Schulman et al. (2017  ###reference_b35###) algorithm to implement online feedback training.\nThe training process includes rewriting, retrieving, scoring and ultimately providing feedback, as illustrated in Figure 2  ###reference_###(2b).\nThe details of the PPO loss and implementation are provided in Appendix A.2.1  ###reference_.SSS1###."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Task Formulation",
            "text": "Within the process of Retrieval Augmented Generation (RAG), when inputting an original query, a set of relevant documents will be retrieved through a search engine, and the retrieved documents are utilized to enable the model to better accomplish the corresponding task (in this paper, we discuss the task of Open-domain Question Answering). Query rewriting is to reformulate the original query into another form to better retrieve relevant passages. We aim to obtain a better rewrite model that can rewrite as: here is the rewritten query which is used to retrieve documents for completing subsequent task. Instead of leveraging the ranking score of each document retrieved by the rewritten query to construct the preference data, our method involves using random selection of rewrites without any scoring or ranking to determine their quality. Figure 2 ###reference_### shows the overview of our proposed framework, RaFe for query rewriting training."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Initial Supervised Fine-Tuning",
            "text": "Before leveraging the feedback, we first initialize the rewrite model with a cold start supervised fine-tuning to gain the rewrite ability. Specifically, we prompt the LLMs to produce the rewrite data. The details of the datasets we used to produce the training rewrite can be found in Sec 3.1. The rewrites generated from LLMs are denoted as , where  is the rewrite set of original query . We split the training instances into two parts, where  and  indicate the instances we use for SFT and feedback training, respectively. We train the rewrite model  with standard SFT loss as follows: Note that for each query, we use random selection of rewrites without any scoring or ranking to determine their quality. This is done to enhance the diversity of generation by our trained model, since in real-world applications, different rewrites are required for a single search query to address different aspects or interpretations."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Feedback Training",
            "text": "The evaluation of query rewriting is notoriously difficult due to the absence of direct quality assessment methods (Zhu et al., 2023  ###reference_b55###), so previous feedback for QR typically rely on the annotated passages (Nogueira and Cho, 2017  ###reference_b26###; Wu et al., 2022  ###reference_b43###).\nWhile throughout the traditional IR pipeline, documents expanded by query rewriting are typically subjected to a reranking process.\nIntuitively, the reranker can serve as a natural feedback for query rewriting.\nGiven a reranker model , the process of scoring a document  with query  can be formulate as .\nThe ranking score of a rewrite  can be denoted as follow:\nIn this way, we can provide reliable feedback for training rewriting models.\nAs illustrated in Figure 2  ###reference_###, our proposed method can be applied for both offline and online feedback training.\nFor offline feedback, we use random selection of rewrites without any scoring or ranking to determine their quality. Specifically, we set a threshold to distinguish the good and bad rewrites formulated as , which is computed as the average ranking score for all training instances as follows:\nThen for every rewrite  with a score exceeding the threshold , we regard it as a good rewrite for the original query ; otherwise, it is deemed a bad rewrite. In this way, we obtain all the preference pairs for open domain QA in the form .\nFor the offline feedback training, we use DPO (Rafailov et al., 2023  ###reference_b32###  ###reference_b32###) and KTO (Kawin et al., 2023  ###reference_b12###  ###reference_b12###).\nDPO directly leverage the preference pairs to optimize the model,\nwhile KTO is a method that can optimize the model from feedback, only needs the signal of whether a rewrite  is good or not, rather than needing pairs, formulated as .\nThe specific formulation of  is in Eq 6  ###reference_###  ###reference_###,\nand the detailed explanation of the KTO is demonstrated in Appendix A.2.1  ###reference_.SSS1###  ###reference_.SSS1###.\nThe ranking score can also serve as an online feedback signal.\nWe utilize the Proximal Policy Optimization (PPO) Schulman et al. (2017  ###reference_b35###  ###reference_b35###) algorithm to implement online feedback training.\nThe training process includes rewriting, retrieving, scoring and ultimately providing feedback, as illustrated in Figure 2  ###reference_###  ###reference_###(2b).\nThe details of the PPO loss and implementation are provided in Appendix A.2.1  ###reference_.SSS1###  ###reference_.SSS1###."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experimental Setup",
            "text": "As we attempt to improve query rewriting for better RAG, we conduct our experiments on the typical RAG scenarios, Open-Domain Question Answering (ODQA).\nThe process of RAG for ODQA can be formulated as , where  denotes the LLMs,  is the original query from datasets and  is the documents concatenated for augmentation.\nFor English data, we use several open-domain QA datasets including NQ (Kwiatkowski et al., 2019  ###reference_b14###), TriviaQA (Joshi et al., 2017  ###reference_b10###), HotpotQA (Yang et al., 2018  ###reference_b47###). For NQ and TriviaQA, we follow the split from previous work (Karpukhin et al., 2020  ###reference_b11###), and default split for HotpotQA111https://huggingface.co/datasets/hotpot_qa/viewer/fullwiki  ###reference_viewer/fullwiki###.\nWe randomly gather 60k instances from the training set of the three datasets to conduct  for training rewrite models.\nAs for evaluation, we collect the test set of NQ and TriviaQA, and the development set of HotpotQA as the held-in evaluation datasets.\nAdditionally, we use FreshQA (Vu et al., 2023  ###reference_b40###) for out-of-domain evaluation.\nFor Chinese data, we gather a bunch of open-source queries to conduct the query set, the sources are listed in 6  ###reference_###.\nWe use WebQA (Li et al., 2016  ###reference_b18###) for the in-domain evaluation, while\nFreshQA (Vu et al., 2023  ###reference_b40###) (translated) for the out-of-domain evaluation.\nThe process of translation can be found in Appendix A.2.2  ###reference_.SSS2###.\nDirectly use the documents  retrieved by rewrite  for evaluation instead of the documents  retrieved by query .\nEmploying both  and  for evaluation. We generate two rewrites  for the Expand setting with their retrieved .\nTo further simulate the role of query rewriting in real-world scenarios, our experiments also include the performance under two following settings:\nConcatenating top-5 retrieved documents in the default order. For Expand setting, the raw documents order is determined by sequentially and cyclically selecting the top documents from .\nConcatenating top-5 documents after re-ranking all the retrieved documents. As regard to Expand setting, all retrieved documents from both the query and rewrites are merged for ranking.\nWe utilize the Exact Match (EM) metric to evaluate the general QA performance.\nEspecially, we use Rouge-L (Lin, 2004  ###reference_b19###) to evaluate the false premise set in FreshQA.\nGiven our work focus on open-domain QA, there are no gold documents or relevant annotations,\nwe evaluate the retrieval by determining whether the retrieved documents contain the correct answer.\nWe report the Precision@K and the mean reciprocal rank (MRR) in the results.\nRetrieve with the original query and utilize the documents by the default returned ranking from the search engine.\nDirectly enable the LLMs to rewrite the original query with a few-shot prompt. In our experiment, we prompt Qwen-max to rewrite the original query.\n(Wang et al., 2023  ###reference_b42###) A method creates pseudo-documents through few-shot prompting of LLMs and then the query is expanded with the generated pseudo-documents for retrieving. The used prompts are shown in Appendix A.5  ###reference_###.\nUse the pre-generated rewrites to directly train the rewrite model. SFT represents the rewrite model trained specifically on the , while SFT denotes the model trained on .\nWe use an anonymous internal search engine for open domain to retrieve documents for the Chinese datasets, and Google Search for the English datasets. Specifically, we utilize the title and the summary snippet of the searched page as the retrieved documents for retrieval augmentation.\nWe employ Qwen-max222https://help.aliyun.com/zh/dashscope/developer-reference/api-details?spm=a2c4g.11186623.0.0.3d4a140b0kf3sd  ###reference_oper-reference/api-details?spm=a2c4g.11186623.0.0.3d4a140b0kf3sd### (Bai et al., 2023  ###reference_b2###) to generate responses and\nconduct the evaluation with Qwen1.5-32b-chat.\nQuery rewriting models are trained with the Qwen-7b-base.\nFor a general RAG task like open-domain QA,\nwe believe that if our approach yields positive results with a general reranker, when transferring to a specific domain (where a domain-specific reranker is available), it will perform even better.\nThus, we employ a publicly available bge-reranker333https://huggingface.co/BAAI/bge-reranker-base  ###reference_ase### (Xiao et al., 2023  ###reference_b44###) to conduct open-domain QA experiments, which serves to demonstrate the effectiveness of the methods we designed."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Dataset",
            "text": "To comprehensively validate the effectiveness and generalizability of our method, we conduct cross-lingual experiments. Specifically, we evaluate ReFe on both English and Chinese datasets.\nFor English data, we use several open-domain QA datasets including NQ (Kwiatkowski et al., 2019  ###reference_b14###  ###reference_b14###), TriviaQA (Joshi et al., 2017  ###reference_b10###  ###reference_b10###), HotpotQA (Yang et al., 2018  ###reference_b47###  ###reference_b47###). For NQ and TriviaQA, we follow the split from previous work (Karpukhin et al., 2020  ###reference_b11###  ###reference_b11###), and default split for HotpotQA111https://huggingface.co/datasets/hotpot_qa/viewer/fullwiki  ###reference_viewer/fullwiki###  ###reference_viewer/fullwiki###.\nWe randomly gather 60k instances from the training set of the three datasets to conduct  for training rewrite models.\nAs for evaluation, we collect the test set of NQ and TriviaQA, and the development set of HotpotQA as the held-in evaluation datasets.\nAdditionally, we use FreshQA (Vu et al., 2023  ###reference_b40###  ###reference_b40###) for out-of-domain evaluation.\nFor Chinese data, we gather a bunch of open-source queries to conduct the query set, the sources are listed in 6  ###reference_###  ###reference_###.\nWe use WebQA (Li et al., 2016  ###reference_b18###  ###reference_b18###) for the in-domain evaluation, while\nFreshQA (Vu et al., 2023  ###reference_b40###  ###reference_b40###) (translated) for the out-of-domain evaluation.\nThe process of translation can be found in Appendix A.2.2  ###reference_.SSS2###  ###reference_.SSS2###."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Evaluation Settings",
            "text": "In practical retrieval scenarios, query rewriting is commonly used as a technique to expand the retrieved documents based on the original query, followed by a re-ranking of the expanded documents.\nThus, we validate RaFe in two experimental settings.\nDirectly use the documents  retrieved by rewrite  for evaluation instead of the documents  retrieved by query .\nEmploying both  and  for evaluation. We generate two rewrites  for the Expand setting with their retrieved .\nTo further simulate the role of query rewriting in real-world scenarios, our experiments also include the performance under two following settings:\nConcatenating top-5 retrieved documents in the default order. For Expand setting, the raw documents order is determined by sequentially and cyclically selecting the top documents from .\nConcatenating top-5 documents after re-ranking all the retrieved documents. As regard to Expand setting, all retrieved documents from both the query and rewrites are merged for ranking.\nWe utilize the Exact Match (EM) metric to evaluate the general QA performance.\nEspecially, we use Rouge-L (Lin, 2004  ###reference_b19###  ###reference_b19###) to evaluate the false premise set in FreshQA.\nGiven our work focus on open-domain QA, there are no gold documents or relevant annotations,\nwe evaluate the retrieval by determining whether the retrieved documents contain the correct answer.\nWe report the Precision@K and the mean reciprocal rank (MRR) in the results."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Baseline",
            "text": "Retrieve with the original query and utilize the documents by the default returned ranking from the search engine.\nDirectly enable the LLMs to rewrite the original query with a few-shot prompt. In our experiment, we prompt Qwen-max to rewrite the original query.\n(Wang et al., 2023  ###reference_b42###  ###reference_b42###) A method creates pseudo-documents through few-shot prompting of LLMs and then the query is expanded with the generated pseudo-documents for retrieving. The used prompts are shown in Appendix A.5  ###reference_###  ###reference_###.\nUse the pre-generated rewrites to directly train the rewrite model. SFT represents the rewrite model trained specifically on the , while SFT denotes the model trained on ."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Implementation",
            "text": "We use an anonymous internal search engine for open domain to retrieve documents for the Chinese datasets, and Google Search for the English datasets. Specifically, we utilize the title and the summary snippet of the searched page as the retrieved documents for retrieval augmentation.\nWe employ Qwen-max222https://help.aliyun.com/zh/dashscope/developer-reference/api-details?spm=a2c4g.11186623.0.0.3d4a140b0kf3sd  ###reference_oper-reference/api-details?spm=a2c4g.11186623.0.0.3d4a140b0kf3sd###  ###reference_oper-reference/api-details?spm=a2c4g.11186623.0.0.3d4a140b0kf3sd### (Bai et al., 2023  ###reference_b2###  ###reference_b2###) to generate responses and\nconduct the evaluation with Qwen1.5-32b-chat.\nQuery rewriting models are trained with the Qwen-7b-base.\nFor a general RAG task like open-domain QA,\nwe believe that if our approach yields positive results with a general reranker, when transferring to a specific domain (where a domain-specific reranker is available), it will perform even better.\nThus, we employ a publicly available bge-reranker333https://huggingface.co/BAAI/bge-reranker-base  ###reference_ase###  ###reference_ase### (Xiao et al., 2023  ###reference_b44###  ###reference_b44###) to conduct open-domain QA experiments, which serves to demonstrate the effectiveness of the methods we designed."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Results",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Main Result",
            "text": "From Table 1  ###reference_### and Table 2  ###reference_###, we can observe that RaFe outperforms other query rewriting baselines and OQR across almost all settings in retrieval and question-answering metrics.\nIt can be noted that the performances of most methods decrease slightly compared to OQR under the Substitute setting, where RaFe also shows marginal improvements.\nThe weak performance might be attributed to that rewriting tend to deviate from the original query in some challenging cases. We provide a deeper analysis in the Appendix A.4.1  ###reference_.SSS1###.\nWhile under the Expand setting, the majority of baseline methods perform better than under Substitute setting.\nNotably, RaFe achieves significant improvements in the Expand-Ranked setting, where the QA results surpass all other baselines including OQR by 2%-3%. A similar conclusion can be drawn from Table 8  ###reference_###.\nBy comparing results between Table 1  ###reference_### and Table 2  ###reference_###, it can be found that even with feedback provided to the query rewriting models through the use of rerankers, the ranked results continue to show a substantial increase in performance, which are further illustrated in Figure 4  ###reference_###.\nIt suggests that in practical applications of RAG, it may yield the greatest benefit by employing query rewriting with the Expand-Ranked setting. More retrieval results are shown in Appendix A.3.1  ###reference_.SSS1###."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Compared with Other Types of Feedback",
            "text": "Previous work on training query rewrite models for the RAG (Ma et al., 2023  ###reference_b22###) has leveraged LLMs performance on QA tasks as the feedback signal.\nMany works construct feedback based on retrieval metrics from annotated documents (Wu et al., 2022  ###reference_b43###; Nogueira and Cho, 2017  ###reference_b26###).\nTo thoroughly assess the efficacy of our approach, we also conduct experiment with these types of feedback.\nWe obtain good-bad pairs (i.e. true for good and false for bad) for offline training introduced in Sec 2.3  ###reference_###.\nWe use Qwen-32b-chat to conduct the LLM feedback.\nFor the retrieval feedback, we utilize the results of Prec@5 to obtain good-bad pairs.\nThe results are shown in Table 3  ###reference_###.\nAdditionally, we provide a comparison between reranker feedback and other feedback, demonstrated in Table 4  ###reference_###.\nThe results show that RaFe outperforms the other two types of feedback. Precision feedback yields the worst results, which may be attributed to the rudimentary construction of precision in our dataset\u2014merely considering whether the answer is present within the document.\nLLM feedback also demonstrates competent performance in the Substitute setting.\nHowever, from Table 4  ###reference_###, we notice that under an equivalent data volume, the cost of employing LLM to construct feedback substantially exceeds that of the other two feedback."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Analysis",
            "text": "###figure_3###"
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "How RaFe makes rewriting better?",
            "text": "In this section, we present illustrative case studies to intuitively compare different rewrites and the original query in Figure 3  ###reference_###.\nThe benifits of RaFe can be summarized into three types.\n(A): RaFe performs better in preserving the semantics of the original query. As shown in Figure 3  ###reference_### (A),\nit can be observed that RaFe, after alignment through reranker, can rewrite queries in a way that better preserves the semantics of the original query. In contrast, the rewrite by SFT directly shifts the focus of the query from which athlete to which competition.\n(B): RaFe\u2019s rewrites improve the format of the query for retrieval purposes.\nRaFe\u2019s rewrite is capable of transforming an uncommon term \u201crecipient\u201d into \u201cwinner\u201d. Although SFT rewrites also replace \u201crecipient\u201d with \u201cwinner\u201d, it changes \u201cteam\u201d from a sports competition context to \u201csquad\u201d, a term commonly used in military, police, or other contexts, thereby introducing potential ambiguity.\n(C): RaFe\u2019s rewrites sentences for better understanding.\nThis kind of case is not easily discernible as good or bad based on intuition; however, RaFe\u2019s rewrite demonstrates better performance in retrieval results. Such cases show why we require feedback to enhance the QR effectiveness, as we always fail to articulate how a query could be formatted to better suit a retriever.\n###figure_4###"
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "How does the Reranker Feedback Work?",
            "text": "To investigate how reranker works for query rewriting,\nwe first ascertain the ability of the publicly available reranker to rank on unseen datasets.\n###figure_5### The comparing results are presented in Figure 4  ###reference_###.\nIt can be clearly seen that all methods yield better QA performance after documents are ranked on all the datasets.\nThis indicates that the reranker\u2019s pattern for document sorting acts as a positive signal for the retrieval system. Meanwhile, we can observe that RaFe performs the better improvements after ranked, which further demonstrates the effectiveness of reranker feedback.\nMoreover, we validate the effectiveness of reranker in constructing good and bad pairs within .\nWe compare the precision of documents retrieved by different queries in Table 5  ###reference_###.\nIt is obvious that the documents retrieved by good rewrites exhibit significantly higher precision compared to those retrieved by the original query, which indicates that the reranker is capable of effectively distinguishing between rewrites that can retrieve high-quality documents and those that cannot.\nWe also provide some examples in Appendix A.4.2  ###reference_.SSS2###."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "How Many Rewrites is Optimal for RAG?",
            "text": "In this section, we delve deeper into the impact that varying numbers of rewrites have on the final performance, since in practical applications of query rewriting, a balance must be struck between the quantity of generated rewrites and performance efficiency, given that generating more rewrites could potentially result in more response time.\nWe generate different numbers of rewrites, the results are depicted in Figure 5  ###reference_###.\nThe QA results peak when there are 4-5 rewrites, suggesting that employing more rewrites can yield considerable benefits by retrieving more relevant top documents. However, Prec@5 nearly approaches the best around 2-3 rewrites.\nWhen ranking all passages, the performance ceiling is attained with merely 2 rewrites. Considering the time cost, 2-3 rewrites may benefit the most for practical RAG."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Query Rewriting",
            "text": "Query rewriting is a critical technique within the retrieval domain (Carpineto and Romano, 2012  ###reference_b3###; Zhu et al., 2023  ###reference_b55###).\nWith the groundbreaking advancements in scaling-up model capabilities, query rewriting has also played a pivotal role in enhancing the abilities of LLMs in RAG (Khattab et al., 2022  ###reference_b13###; Press et al., 2023  ###reference_b31###; Yan et al., 2024  ###reference_b46###).\nMany works (Wang et al., 2023  ###reference_b42###; Shen et al., 2023  ###reference_b36###; Ye et al., 2023  ###reference_b49###) directly leverage LLMs\u2019 strong capabilities to expand or rewrite queries.\nNonetheless, in practical application scenarios, a smaller rewriting model is preferred to avoid the costly requests for LLMs. At the same time, feedback training is the most commonly employed method to enhance the smaller rewriting models.\n Nogueira and Cho (2017  ###reference_b26###) incorporates the ranking signals from annotated passages for better results, as well as previous works on conversational query rewrite (Wu et al., 2022  ###reference_b43###; Mo et al., 2023  ###reference_b24###; Chen et al., 2022  ###reference_b4###).\nMa et al. (2023  ###reference_b22###) first generates answers from LLMs and then uses the QA evaluation results as the training signals.\nPeng et al. (2023  ###reference_b30###) leverages search scoring functions intrinsic to the e-commerce framework to assess rewrite quality, informing feedback signals, which is exceedingly domain-specific, limiting its applicability to other domains.\nThese works depend on using particularly designed scores or annotated labels for feedback signals, while our proposed method can generically deliver feedback based on ranking results, without needing annotated passages."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Learning From Feedback",
            "text": "Recent advancements in Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022  ###reference_b28###) have been instrumental in aligning the generative capabilities of large models with human preferences, significantly prompting the creation of strong LLMs (OpenAI, 2023  ###reference_b27###).\nTherefore, a large number of studies about feedback alignment have been emerging (Zheng et al., 2023  ###reference_b54###; Wang et al., 2024  ###reference_b41###; Rafailov et al., 2023  ###reference_b32###; Yuan et al., 2023  ###reference_b50###; Dong et al., 2023  ###reference_b5###; Kawin et al., 2023  ###reference_b12###).\nSome research efforts are concentrated on devising methods to provide new forms of feedback (Lee et al., 2023  ###reference_b16###; Shinn et al., 2023  ###reference_b37###; Madaan et al., 2023  ###reference_b23###; Pang et al., 2023  ###reference_b29###; Liu et al., 2023a  ###reference_b20###; Aky\u00fcrek et al., 2023  ###reference_b1###; Nathani et al., 2023  ###reference_b25###).\nXu et al. (2023  ###reference_b45###) propose to train models from judgment language feedback.\nLi et al. (2023  ###reference_b17###) designs two types of ranking feedback drawing from LLMs, to improve the performance.\nDespite all these works, the exploration of feedback in rewriting is currently limited to direct feedback from LLMs (Ma et al., 2023  ###reference_b22###) and domain-specific scoring (Peng et al., 2023  ###reference_b30###). Such feedback approaches are costly and fail to utilize the effective signals from the IR system.\nWhile Le et al. (2022  ###reference_b15###) and Liu et al. (2023b  ###reference_b21###) effectively leverage the feedback from Unit Test in the domain of code generation, we investigate more appropriate feedback signals for query rewriting in this paper, the reranker feedback."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion and Future Work",
            "text": "This paper proposes a novel feedback training framework named RaFe for query rewriting,\nbased on the effectiveness of the reranker in enhancing document ranking during the information retrieval process.\nBy leveraging the feedback signals from reranker, RaFe is capable of effectively and generally conducting feedback training for rewrite models, yielding great improvements.\nExperimental results indicate that our method achieves exemplary performance across cross-linguistic datasets.\nIn the future, we plan to conduct the joint training of reranker and rewrite models, which may yield substantial benefits for RAG."
        }
    ],
    "url": "http://arxiv.org/html/2405.14431v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "6",
            "6.1",
            "6.2"
        ],
        "methodology_sections": [
            "2",
            "2.1",
            "2.2",
            "2.3"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.4",
            "4",
            "4.1",
            "4.2",
            "5",
            "5.1",
            "5.2",
            "5.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.1",
            "4.2",
            "5.2",
            "5.3"
        ]
    },
    "research_context": {
        "paper_id": "2405.14431v1",
        "paper_title": "RaFe: Ranking Feedback Improves Query Rewriting for RAG",
        "research_background": "### Motivation\n\nThe motivation for this paper arises from the challenges faced by Large Language Models (LLMs) such as hallucinations and outdated knowledge, as mentioned in various studies (Ji et al., 2023; Zhang et al., 2023; Huang et al., 2023; Yao et al., 2023; Zhang et al., 2024). Although Retrieval Augmentation Generation (RAG) has been introduced to use external knowledge to enhance LLMs (Gao et al., 2023), directly retrieving documents using the original query often fails to provide accurate and relevant documents. Query rewriting techniques (Efthimiadis, 1996; Carpineto and Romano, 2012) are typically employed to address this issue, but existing methods either require costly large models or depend heavily on annotated labels and domain-specific rewards (Wu et al., 2022; Chen et al., 2022).\n\n### Research Problem\n\nThe research problem this paper addresses is how to improve the performance of query rewriting without the high cost of annotations and while identifying more effective and general feedback signals. Specifically, the paper aims to accomplish this by leveraging ranking feedback from rerankers, which are prevalent in traditional information retrieval systems, to train query rewriting models.\n\n### Relevant Prior Work\n\n1. **Issues with LLMs**: Studies by Zhao et al. (2023), Ji et al. (2023), Zhang et al. (2023), Huang et al. (2023), Yao et al. (2023), and Zhang et al. (2024) provide evidence of LLMs encountering hallucinations and outdated knowledge.\n   \n2. **Retrieval Augmentation Generation (RAG)**: Introduced by Gao et al. (2023), RAG helps LLMs by incorporating external knowledge but falls short when the original query does not perform well in retrieving relevant documents.\n\n3. **Query Rewriting**: Previous works such as those by Efthimiadis (1996) and Carpineto and Romano (2012) have highlighted the utility of transforming the query to improve retrieval performance. Shen et al. (2023) and Wang et al. (2023) proposed using the powerful LLMs for query rewriting, which is computationally costly.\n\n4. **Reinforcement Learning (RL) for Query Rewriting**: Existing methods like those by Nogueira and Cho (2017), Wu et al. (2022), Chen et al. (2022), Ma et al. (2023), and Peng et al. (2023) employ RL using feedback in the form of annotations or pre-designed domain-specific rewards, but these approaches can be resource-intensive and lack generalizability.\n\n5. **Utilization of Other Feedback Mechanisms**: Works such as Nathani et al. (2023) and Li et al. (2023) have explored various feedback mechanisms in different domains, while Liu et al. (2023) effectively integrated unit testing feedback in code generation.\n\nBy proposing the RaFe framework, this paper fills the gap in the literature by providing a cost-effective, generalizable method to enhance query rewriting through the novel use of reranking feedback. The proposed framework does not require labeled data or domain-specific rewards, offering a more practical and versatile solution.",
        "methodology": "RaFe (Ranking Feedback) is a method designed to improve query rewriting for retrieval-augmented generation (RAG) systems in open-domain question answering (QA). Here\u2019s a breakdown of the key components and innovations in the proposed methodology:\n\n1. **Offline Feedback Using Ranking Scores:**\n   - **Preference Data Construction:** RaFe uses the ranking scores of documents retrieved by rewritten queries to construct preference data. A threshold is established based on the average ranking scores of all training instances. \n   - **Good vs. Bad Rewrites:** For a given rewritten query, if the ranking score exceeds the threshold, it is considered a good rewrite (denoted as \\( r_i^w \\)) for the original query \\( q_i \\). Conversely, if the score is below the threshold, it is deemed a bad rewrite.\n   - **Preference Pairs:** From this process, preference pairs in the form \\( (q_i, r_i^w) \\) are generated, indicating the quality of the rewritten query.\n\n2. **Offline Feedback Training Methods:**\n   - **Direct Preference Optimization (DPO):** Leveraged from Rafailov et al. (2023), DPO utilizes the preference pairs directly to optimize the model.\n   - **Knowledge Transfer Optimization (KTO):** Described by Kawin et al. (2023), KTO requires only a binary signal of whether a rewrite is good or bad, without needing explicit preference pairs. The specific formulation of this process is detailed in Eq. 6, and further explanation is provided in Appendix A.2.1.\n\n3. **Online Feedback Using Proximal Policy Optimization (PPO):**\n   - **Online Feedback Signal:** The ranking score serves as a feedback signal during the online training phase.\n   - **PPO Algorithm:** The PPO algorithm, as described by Schulman et al. (2017), is employed to implement the online feedback training. This involves a cyclical process of rewriting a query, retrieving documents based on the rewrite, scoring the results, and then using the scores as feedback to improve the rewriting model.\n   - **Training Process Cycle:** This iterative training cycle, including rewriting, retrieving, scoring, and feedback, is described in the original document. Details of the PPO loss and its implementation specifics are available in Appendix A.2.1.\n\nThese methodologies combine both offline and online feedback mechanisms to iteratively improve the quality of query rewrites, aiming to enhance the efficacy of retrieval-augmented generation systems in open-domain QA tasks.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Experiment Setup\n\nTo improve query rewriting for better Retrieval-Augmented Generation (RAG) in Open-Domain Question Answering (ODQA), the experiments are conducted on various ODQA datasets and evaluated with different metrics. \n\n**Datasets:**\n1. **English Datasets:**\n   - Natural Questions (NQ) (Kwiatkowski et al., 2019)\n   - TriviaQA (Joshi et al., 2017)\n   - HotpotQA (Yang et al., 2018)\n   - FreshQA (Vu et al., 2023) for out-of-domain evaluation\n\n2. **Chinese Datasets:**\n   - A bunch of open-source queries\n   - WebQA (Li et al., 2016) for in-domain evaluation\n   - FreshQA (translated version) for out-of-domain evaluation\n\n**Baselines:**\n1. Retrieval with the original query using the default ranking from the search engine.\n2. Few-shot prompted query rewriting using Qwen-max.\n3. Pseudo-document generation method where LLMs expand the query using generated pseudo-documents for retrieving.\n\n**Evaluation Metrics:**\n1. **Exact Match (EM):** To evaluate the general QA performance.\n2. **Precision@K and Mean Reciprocal Rank (MRR):** To evaluate retrieval quality.\n3. **Rouge-L:** Specifically used for evaluating the false premise set in FreshQA.\n\n**Training and Evaluation:**\n- **English Data Handling:**\n   - Training models with 60K instances gathered from NQ, TriviaQA, and HotpotQA.\n   - Evaluations on the test set of NQ, TriviaQA, and the development set of HotpotQA.\n\n- **Chinese Data Handling:**\n   - Queries gathered from multiple open-source sources.\n   - Evaluation with WebQA and translated FreshQA.\n\n- **Document Handling:**\n   - Utilizing the title and summary snippet of search results from an internal search engine for Chinese datasets and Google Search for English datasets.\n   - Testing different document concatenation methods:\n     1. Default order of top-5 retrieved documents.\n     2. Re-ranked top-5 documents from both query and rewrites.\n\n**Models:**\n- **Query Rewriting Models:** Trained with Qwen-7b-base.\n- **Evaluation Responses:** Generated with Qwen1.5-32b-chat.\n- **Reranker:** bge-reranker (a publicly available general reranker).\n\n#### Main Experimental Results\n\nThe effectiveness of ranking feedback-improved query rewriting is observed across multiple QA and retrieval metrics. \n\n**Key findings:**\n- Enhanced query rewriting models using the proposed method outperformed baseline methods in Exact Match (EM) scores, indicating better QA performance.\n- Precision@K and MRR metrics showed an improved retrieval rate for correct answers when using the enhanced rewriting model.\n- When documents are re-ranked, performance improvements in retrieval metrics suggested better relevance and utility of retrieved content.\n- Out-of-domain evaluation on FreshQA demonstrated the robustness of the proposed model across different datasets and domains.\n"
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Evaluate the performance of RaFe framework in comparison to other query rewriting baselines and OQR in different settings.",
            "experiment_process": "Experiments are conducted under two main settings: Substitute and Expand. Performance metrics for retrieval and question-answering are used to compare RaFe against other baselines. The datasets used and detailed performance metrics can be found in Tables 1 and 2, and additional analysis is provided in the appendix (Appendix A.4.1).",
            "result_discussion": "RaFe outperforms other query rewriting baselines and OQR in almost all settings. While there are slight decreases in performance under the Substitute setting for most methods, RaFe still shows marginal improvements. In the Expand setting, RaFe achieves significant improvements in the Expand-Ranked setting, surpassing other baselines by 2%-3% in QA results. The study suggests that using the Expand-Ranked setting may yield the greatest benefit in practical applications of RAG.",
            "ablation_id": "2405.14431v1.No1"
        },
        {
            "research_objective": "Compare the effectiveness of reranker feedback against other types of feedback in training query rewrite models for the RAG.",
            "experiment_process": "Two additional types of feedback are used for comparison: LLM feedback and retrieval feedback, which involves annotated documents and Prec@5 metrics, respectively. Qwen-32b-chat is used for the LLM feedback, and retrieval feedback is derived from Prec@5. The experiment includes obtaining good-bad pairs for offline training detailed in Sec 2.3. Results and comparisons are shown in Tables 3 and 4.",
            "result_discussion": "RaFe outperforms both LLM feedback and retrieval feedback. Precision feedback gives the worst results, likely due to the rudimentary construction of precision being based solely on answer presence in documents. LLM feedback performs well in the Substitute setting. However, constructing feedback using LLM is significantly more costly than the other two types.",
            "ablation_id": "2405.14431v1.No2"
        },
        {
            "research_objective": "Investigate how reranker feedback functions in the query rewriting process.",
            "experiment_process": "The effectiveness of a publicly available reranker on unseen datasets is examined. Various methods are compared based on QA performance after ranking documents. Further validation is done to assess the reranker's ability to construct good and bad pairs, with results reflected in Table 5.",
            "result_discussion": "All methods show improved QA performance after document ranking, indicating that the reranker's document sorting acts as a positive signal for the retrieval system. RaFe demonstrates the most significant improvements, validating the effectiveness of reranker feedback. Good rewrites retrieve higher precision documents compared to original queries, confirming the reranker's capability in distinguishing high-quality document retrievals.",
            "ablation_id": "2405.14431v1.No3"
        },
        {
            "research_objective": "Determine the optimal number of query rewrites for RAG to balance performance and efficiency.",
            "experiment_process": "Experiments involve generating various numbers of rewrites, with QA results and Prec@5 metrics recorded. The number of rewrites ranges from a lower to a higher count, depicted in Figure 5.",
            "result_discussion": "QA results peak with 4-5 rewrites, indicating that more rewrites can yield better retrieval of relevant top documents. Prec@5 nearly reaches its best performance with 2-3 rewrites. The optimal performance ceiling with all passages ranked is achieved with just 2 rewrites. Considering time efficiency, 2-3 rewrites offer the best practical benefit for RAG.",
            "ablation_id": "2405.14431v1.No4"
        }
    ]
}