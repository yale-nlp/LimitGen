{
    "title": "LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models",
    "abstract": "Large Multimodal Models (LMMs) have shown significant visual reasoning capabilities by connecting a visual encoder and a large language model.\nLMMs typically take in a fixed and large amount of visual tokens, such as the penultimate layer features in the CLIP visual encoder, as the prefix content.\nRecent LMMs incorporate more complex visual inputs, such as high-resolution images and videos, which further increases the number of visual tokens significantly.\nHowever, due to the inherent design of the Transformer architecture, the computational costs of these models tend to increase quadratically with the number of input tokens.\nTo tackle this problem, we explore a token reduction mechanism that identifies significant spatial redundancy among visual tokens. In response, we propose PruMerge, a novel adaptive visual token reduction strategy that significantly reduces the number of visual tokens without compromising the performance of LMMs.\nSpecifically, to metric the importance of each token, we exploit the sparsity observed in the visual encoder, characterized by the sparse distribution of attention scores between the class token and visual tokens.\nThis sparsity enables us to dynamically select the most crucial visual tokens to retain.\nSubsequently, we cluster the selected (unpruned) tokens based on their key similarity and merge them with the unpruned tokens, effectively supplementing and enhancing their informational content.\nEmpirically, when applied to LLaVA-1.5 [Liu et al., 2023a], our approach can compress the visual tokens by 14 times on average, and achieve comparable performance across diverse visual question-answering and reasoning tasks.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large Language Models (LLMs) [OpenAI, 2023b  ###reference_b32###, Team et al., 2023  ###reference_b38###, Jiang et al., 2023  ###reference_b18###, Touvron et al., 2023  ###reference_b39###] have shown strong reasoning abilities. LLMs are usually high-capacity Transformers [Vaswani et al., 2017  ###reference_b40###] pretrained with a large-scale text corpus. Large Multimodal Models (LMMs), inherit LLMs for text generation, while also leveraging a visual encoder such as CLIP-ViT [Radford et al., 2021  ###reference_b34###] to embed image patches into visual tokens as the prefix visual context.\nLMMs need substantial computation for inference. The LLM is the primary factor for the high computation cost, since the visual encoder is usually quite small relative to the LLM. For example, the commonly used CLIP visual encoder, ViT-L, only has 0.3B parameters, while the corresponding LLM such as LLaMA [Touvron et al., 2023  ###reference_b39###] or Vicuna [Vicuna, 2023  ###reference_b41###] can have 7B or 13B parameters.\nAs a result, reducing the LLM\u2019s inference cost is the key to achieving low LMM inference cost.\nPrior works [Chu et al., 2023  ###reference_b7###, 2024  ###reference_b8###, Yuan et al., 2023a  ###reference_b46###] mainly focus on replacing the LLM backbone with a smaller language model with less parameters, such as Phi-2 [Javaheripi et al., 2023  ###reference_b17###].\nHowever, such approaches sacrifice the reasoning abilities of LLMs, leading to a large performance gap on visual question-answering and reasoning tasks such as VQAv2 and MM-Bench [Chu et al., 2024  ###reference_b8###]. A similar approach is to apply quantization for LLMs [Liu et al., 2023b  ###reference_b26###, Yuan et al., 2024  ###reference_b48###].\nHowever, the cost of LLMs comes from not only its large number of parameters, but also the length of the input context due to the quadratic complexity of the Transformer\u2019s attention operation. The context length in LMMs is especially important, where a fixed amount of visual tokens serves as the prefixed tokens. For example, in LLaVA-1.5, 576 visual tokens are appended, and in Video-LLaVA [Lin et al., 2023  ###reference_b23###] that number is even higher, leading to high training and inference costs. Thus, an intriguing question is: Can we reduce the number of prefix visual tokens while maintaining comparable performance?\n###figure_1### ###figure_2### In our study, we find that many visual tokens are redundant, similar to findings in previous related work [Bolya et al., 2023  ###reference_b3###, Liu et al., 2022  ###reference_b28###], and most of the visual tokens can be pruned with little sacrifice in performance.\nIn particular, the similarity (i.e., attention scores in the visual encoder\u2019s self-attention module) between the class token and spatial patches are sparse, indicating that only a small number of visual tokens are related to key visual information in the visual samples.\nMotivated by this, we use this sparse similarity to adaptively select important visual tokens, as shown in Fig.1(b)  ###reference_sf2###.\nSpecifically, we leverage the Interquartile Range (IQR) [Boukerche et al., 2020  ###reference_b4###] scoring function in outlier detection to prune unimportant visual tokens. Moreover, we merge the visual tokens using -nearest neighbor and update the selected important visual tokens via weighted averaging, which further enhances performance.\nFinally, we design PruMerge+, which samples visual tokens spatial-uniformly to complement the unpruned tokens.\nPruMerge+ not only minimizes performance degradation but also ensures substantial token reduction, maintaining a more comprehensive and representative selection of visual tokens.\nEmpirically, PruMerge can effectively and adaptively reduce the visual tokens in each image in LLaVA-1.5 [Liu et al., 2023a  ###reference_b25###], where with just 5.5% of visual tokens, which is around 32 tokens for an image on average, LLaVA-PruMerge can maintain comparable performance with that of retaining all 576 tokens across diverse benchmarks.\nFurthermore, PruMerge showcases its versatility across various modalities, including video. By integrating PruMerge with Video-LLaVA during the inference phase alone\u2014eliminating the need for additional training\u2014we not only expedite processing within video-LLMs but also enhance their performance across multiple benchmarks."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Efficient Large Multimodal Models (LMMs)",
            "text": "Large Language Models (LLMs) such as GPT-4 [OpenAI, 2023b  ###reference_b32###], LLaMA [Touvron et al., 2023  ###reference_b39###], Mistral [Jiang et al., 2023  ###reference_b18###], and Gemini [Team et al., 2023  ###reference_b38###] have demonstrated strong question answering and reasoning capabilities over text. Large Multimodal Models (LMMs) [Liu et al., 2023b  ###reference_b26###, Zhu et al., 2023  ###reference_b53###, Yin et al., 2023  ###reference_b45###, Zhang et al., 2024  ###reference_b49###] extend these reasoning capabilities to images, where given an image and an associated question, a vision encoder and an LLM are leveraged to generate text responses in a chat format. More recent works extend whole-image understanding into region-level understanding [Cai et al., 2024  ###reference_b5###, Zhang et al., 2023b  ###reference_b51###, Peng et al., 2023  ###reference_b33###, Chen et al., 2023  ###reference_b6###], video understanding [Lin et al., 2023  ###reference_b23###, Zhang et al., 2023a  ###reference_b50###] and 3D scene understanding [Hong et al., 2023  ###reference_b15###]. Such works typically feed the visual tokens directly into the LLM as prefix tokens, via either an MLP [Liu et al., 2023a  ###reference_b25###], Qformer [Dai et al., 2023  ###reference_b9###, Zhu et al., 2023  ###reference_b53###], or resampler [Alayrac et al., 2022  ###reference_b1###]. The number of visual tokens can be prohibitively long, especially when the images are high-resolution [Liu et al., 2024  ###reference_b27###, OpenAI, 2023a  ###reference_b31###]. In this paper, we reduce the number of visual tokens with a novel adaptive prune and merge procedure.\nWhile LMMs have made significant advances, their large-scale training and deployment incur significant computational costs, requiring efficient parallel device implementations. Google\u2019s Gemini [Team et al., 2023  ###reference_b38###] is a pioneer in efficient LMMs, achieving state-of-the-art performance on multimodal benchmarks and introducing mobile-scale LMMs suitable for low-memory devices, although it is not open-source. Open-source alternatives like LLaVA-1.5 [Liu et al., 2023a  ###reference_b25###] employ advanced compression techniques such as 4/8 bit quantization [Dettmers et al., 2022  ###reference_b10###, Shang et al., 2024  ###reference_b35###]. MobileVLM [Chu et al., 2023  ###reference_b7###] and its improved version, MobileVLM-v2 [Chu et al., 2024  ###reference_b8###], focus on compact architecture designs and training optimizations for mobile use. Similarly, TinyGPT-V [Yuan et al., 2023a  ###reference_b46###] uses the Phi-2 [Javaheripi et al., 2023  ###reference_b17###] LLM backbone to exceed the performance of larger models. LLaVA-Phi [Zhu et al., 2024  ###reference_b54###] and Vary-toy [Wei et al., 2024  ###reference_b43###] introduce smaller backbones and enhanced vocabularies for better generalizability. TinyLLaVA [Zhou et al., 2024  ###reference_b52###] explores the effects of architectural choices and training optimizations, achieving comparable performance to larger models. MoE-LLaVA [Lin et al., 2024  ###reference_b24###] incorporates a Mixture of Experts to address model sparsity, enhancing efficiency and performance.\nIn most cases, LMM efficiency is enhanced by reducing the size of the backbone of the LMM, but no work has considered the efficiency of the LMM from the perspective of the number of visual tokens."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Token Reduction",
            "text": "The notorious quadratic complexity in Transformers [Vaswani et al., 2017  ###reference_b40###] is a well-known problem, as it is one of the key bottlenecks in scaling the input sequence length. Sparse attention methods such as Linformer [Wang et al., 2020  ###reference_b42###] and ReFormer [Kitaev et al., 2020  ###reference_b19###] reduce the quadratic attention complexity by conducting attention operations within a certain region rather than the full context. Token merging [Bolya et al., 2023  ###reference_b3###] utilizes full attention but gradually reduces the number of tokens in each transformer block by selecting the most representative tokens with bipartite matching.\nOne of the main causes of the inefficiency of recent LMMs is their use of a large number of prefix visual tokens that serve as a fixed budget for context [Liu et al., 2023b  ###reference_b26###, Zhu et al., 2023  ###reference_b53###].\nIn our study, we present a novel plug-and-play token reduction method based on visual token similarities, which achieves comparable performance with less than one tenth of the original tokens."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Method: Token Pru-Merging",
            "text": "In this section, we first review the basic implementation of large mutilmodal models (LMMs), with a particular focus on the visual encoder component (i.e., Vision Transformer). We highlight the direct correlation between the number of visual tokens and the efficiency of LMMs (Sec. 3.1  ###reference_###).\nNext, we present a plug-and-play token reduction method specifically designed for LMMs, called token PruMerge.\nOur method features two key components:\n(1) Adaptive Important Token Selection (AITS) via Outlier Detection which adaptively determines the optimal number of visual tokens to retain based on the unique characteristics of the image (Sec. 3.2  ###reference_###); and (2)\nToken Supplement (TS) via Similar Key Clustering, which facilitates efficient processing without compromising the model\u2019s performance by maintaining the integrity and richness of the visual information (Sec. 3.3  ###reference_###)."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Preliminaries",
            "text": "###figure_3### Vision Transformers (ViTs) [Dosovitskiy et al., 2020  ###reference_b11###] are the most widely used vision encoder for LMMs, in which the input image is converted into a sequence of representative tokens by the ViT, and then fed into an LLM for understanding [Liu et al., 2024  ###reference_b27###, Zhu et al., 2023  ###reference_b53###, Hong et al., 2023  ###reference_b15###, Zhang et al., 2024  ###reference_b49###].\nAn input image is divided into a grid of patches and each patch is projected into a token embedding by the ViT. In addition to the patch tokens, a class token (i.e., [CLS] token) is computed to aggregate global image information for classification.\nA ViT consists of a set of transformer blocks, which in turn consist of several essential components: a multi-head self-attention (MSA) layer, a feed-forward neural network (FFN), skip connections, and layer normalization [Ba et al., 2016  ###reference_b2###]. These components work together to improve the model\u2019s capability to understand visual data [Han et al., 2022  ###reference_b14###].\nIn the self-attention layer, an input token is projected into three distinct vectors: query , key , and value , using three linear transformation matrices , , and .\nThese vectors, corresponding to different inputs, are assembled into matrices , , and , respectively. The self-attention computes the relevance of each item to other items:\nwhere attention matrix  and  is the dimension of  and .\nIn the last layer of the ViT, the [CLS] token is used for classification. Similarly, the attention between [CLS] token and other visual tokens is computed by the attention mechanism:\nThe MSA framework allows for simultaneous attention on multiple positions, offering diverse representation subspaces. This is achieved by employing distinct query, key, and value matrices for different heads, which project the input vectors into different representation subspaces.\nAfter the self-attention layers is the feed-forward network (FFN), which consists of two linear transformation layers separated by a nonlinear activation function:\nwhere  and  are the matrices of the linear transformation layers, and  denotes the nonlinear activation function. The general forward pass of ViT is illustrated in the left part of Figure 2  ###reference_###.\nLarge Multimodal Models (LMMs). Following the forward pass through a Vision Transformer (ViT), a set of visual tokens is generated. These tokens are then processed by the input projector , which maps the encoded visual features from  into the text feature space .\nThe aligned features and the text prompts  are then fed into the LLM backbone [Zhang et al., 2024  ###reference_b49###]. The overall architecture of an LMM is depicted in Figure 1  ###reference_###.\nIt is important to note that the computational cost with these models increases quadratically with the number of input tokens to the LLM [Tay et al., 2022  ###reference_b37###].\nMathematically, if there are  tokens in the input, the self-attention mechanism computes a  matrix of attention scores, where each entry in this matrix represents the attention score between a pair of tokens.\nHowever, there is an increasing demand for processing high-resolution images and videos, which increases the number of visual tokens, further exacerbating computation costs.\nThe reduction of visual tokens presents a promising approach to improving the efficiency of LMMs by reducing the escalating computational requirements."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Adaptive Important Token Selection via Outlier Detection",
            "text": "###figure_4### ###figure_5### The most straightforward solution to improve the efficiency of visual token utilization in LMMs is to prune redundant visual tokens [Liu et al., 2022  ###reference_b28###, Yin et al., 2022  ###reference_b44###, Liang et al., 2022  ###reference_b22###]. To realize token pruning, we need to address a pivotal question: How do we determine the importance of each visual token? As discussed in Sec. 3.1  ###reference_###, LMMs typically leverage an extensive stack of visual tokens to represent the visual information. On the other hand, self-/weakly-supervised learning paradigms, such as CLIP [Radford et al., 2021  ###reference_b34###] simplify this complexity by representing an entire image with a single [cls] token, regarded as the most information-condensed token. To balance those two extreme paradigms, we investigate the Key-Query attention between [cls] token and visual tokens, i.e., in Equation 3.2  ###reference_###. Observing the distribution patterns of attention between the [cls] token and visual tokens unveils a sparse landscape, as depicted in Figure 3(a)  ###reference_sf1###. This sparse distribution underpins our methodology for identifying crucial visual tokens. By employing outlier detection algorithms, we aim to adaptively select visual tokens that best represent an image\u2019s features effectively. Random selection of visual tokens without any criteria. To identify outliers within class attention values, we adopt this approach instead of traditional methods, relying merely on chance without specific mathematical foundations. This randomness lacks the structured confines set by explicit methods but still engages in the exploration of token prominence. Specifically, this methodology disregards typical computations such as establishing a \u201clower fence\u201d or \u201cupper fence\u201d through statistical means, thereby embracing randomness as the guiding tool for token selection. Through this method, we can arbitrarily identify and select the visual tokens for each image that will represent the image within the LMM context. Note that we use the class attention value from the penultimate layer for this arbitrary selection. As shown in Figure 1(b)  ###reference_sf2###, the randomly sampled visual tokens demonstrate two behaviors: (1) The number of attentive tokens are not necessarily proportional to the complexity of the image, as randomness dictates the distribution. (2) The sampled tokens may not precisely align with important content, reflecting the stochastic nature of selection. These trends are also observed at the benchmark level; in Table 6  ###reference_###, the average token numbers on various benchmarks appear haphazardly distributed."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Token Supplement via Similar Key Clustering",
            "text": "Following the selection of informative visual tokens, we next optimize the utilization of the remaining tokens.\nWhile pruned tokens may initially seem extraneous, they hold potential value for the perception capabilities of the LLM backbone.\nThis potential arises particularly in cases where an image contains large object parts that dominate the scene. In such scenarios, overly aggressive pruning could inadvertently diminish the model\u2019s ability to represent the image comprehensively.\nTo address this, we devise a token merging method aimed at enhancing the representational capacity of the selected unpruned tokens. This method involves the strategic fusion of currently pruned tokens, as depicted in Figure 2  ###reference_###.\nTo choose the pruned tokens to merge, we need a way to measure similarity between visual tokens.\nHere we leverage the self-attention mechanism in ViTs.\nSince the key vector of each patch token already contains information summarized in the self-attention module [Vaswani et al., 2017  ###reference_b40###], the final layer\u2019s key vector serves as the representation.\nAnd then we use the dot product between keys to calculate which tokens have similar visual information [Bolya et al., 2023  ###reference_b3###]:\nwhich yields  for tokens  in vectorized form for the set of all tokens , where  is the number of input visual tokens.\nWith the similarities between visual tokens established, we simply find the -nearest neighbors for each unpruned token, which act as the cluster centers.\nThe integration of pruned tokens into these clusters is guided by their respective class attentions , enabling a refined representation of each unpruned token through a weighted sum.\nThis procedure is outlined in Algorithm 1  ###reference_###."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "PruMerge+: Bridging the Efficiency-Performance Gap",
            "text": "While PruMerge achieves a remarkable reduction in the number of visual tokens\u2014over tenfold compared to the original setup\u2014the process is not without drawbacks. Specifically, the compression technique, though efficient, introduces a marginal performance discrepancy between the original LLaVA model and its PruMerge-optimized counterpart, LLaVA-PruMerge. To address this, we introduce PruMerge+, a refined version that strikes an optimal balance between token reduction and model performance.\nPruMerge+ enhances our original method by maintaining the ability to significantly reduce visual token count\u2014by an average of fourfold\u2014with minimal performance degradation. This improvement is detailed in Algorithm 1  ###reference_###, building upon the token selection strategies outlined in Section 3.2  ###reference_###.\nA new aspect of PruMerge+ lies in its enhanced token selection process. Beyond merely focusing on the previously identified important tokens, PruMerge+ extends its reach to encompass additional visual tokens from areas initially deemed less critical. This is achieved through a spatially uniform sampling of visual tokens, guided by a predetermined ratio informed by the distribution of outlier tokens. This methodology ensures a more comprehensive and representative selection of visual tokens, as depicted in Figure 3(b)  ###reference_sf2###, thereby minimizing performance losses while still achieving substantial token reduction."
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "Discussion",
            "text": "Distinction from Existing Token Reduction Methods.\nToken reduction methods [Liu et al., 2022  ###reference_b28###, Yin et al., 2022  ###reference_b44###, Liang et al., 2022  ###reference_b22###] have been proposed for accelerating ViT computation speed. Block by block, tokens are gradually reduced in number, correspondingly reducing the computation cost in the internal ViT.\nBy contrast, our approach is not specifically designed to increase the efficiency of a ViT.\nIt instead aims to improve the overall efficiency of a large multimodal model, where ViT is only one relatively light cost component as shown in Fig. 1  ###reference_###.\nThere are two benefits of this design.\nFirst, while ViTs typically rely on a single class token to represent the input image, which enables them to maintain performance despite a reduction in intermediate tokens, LMMs usually require a large stack of visual tokens. This ensures a comprehensive representation of the visual content, preserving the model\u2019s ability to capture nuanced details.\nThus, using previous token merging methods to obtain one refined class token as representation of visual input is not consistent with the literature of large multimodal models.\nSecond, considering that the bulk of computational demand within LMMs is attributed to the LLM component rather than the ViT, our approach focuses not only on the reduction of tokens but also on maximizing the informational content of the pruned visual tokens. This strategy addresses the computational challenges inherent in LMMs with minimal compromise in the quality of the visual representation."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We first show the empirical performance of our approach when applied to LLaVA-1.5 in Sec 4.1  ###reference_###. We then analyze the efficiency improvement by using our PruMerge on LMM in Sec 4.2  ###reference_###. To show the generalization ablity, we conduct a series of experiments in Sec. 4.3  ###reference_###. Finally, we demonstrate the effectiveness of each component in our model in Sec 4.4  ###reference_###."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Main Results",
            "text": "We apply our method to LLaVA-1.5 [Liu et al., 2023a  ###reference_b25###], a recent state-of-the-art LMM. We further finetune LLaVA-1.5 using LoRA [Hu et al., 2022  ###reference_b16###] for 1 epoch using the LLaVA-1.5 instruction fine-tuning data [Liu et al., 2023a  ###reference_b25###] with our reduced visual tokens.\nWe evaluate on diverse visual question-answering and reasoning benchmarks including VQAv2 [Goyal et al., 2017  ###reference_b13###], ScienceQA [Lu et al., 2022  ###reference_b30###], TextVQA [Singh et al., 2019  ###reference_b36###], POPE hallucination bench [Li et al., 2023b  ###reference_b21###], MME [Fu et al., 2023  ###reference_b12###], and MMBench [Liu et al., 2023c  ###reference_b29###].\nAs shown in Table 1  ###reference_###, our approach achieves comparable performance with LLaVA-1.5 despite using only a small fraction of the visual tokens, and performing better than previous works such as BLIP2 [Li et al., 2023a  ###reference_b20###] and InstructBLIP [Dai et al., 2023  ###reference_b9###]. Specifically, in POPE and ScienceQA, our approach even shows better performance than LLaVA-1.5.\nNote that due to the adaptive nature of PruMerge (see Sec. 3.2  ###reference_###), the token numbers for various tasks are different (see 4.4.1  ###reference_.SSS1###), and thus we use the average number on numbers of 6 tasks (i.e., 32) for simplicity."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Efficiency Analysis",
            "text": "To elucidate the computational efficiency afforded by PruMerge, we utilize the roofline-based LLM-Viewer analysis developed in [Yuan et al., 2024  ###reference_b48###]. Our investigation is grounded in a theoretical scenario tailored to highlight the impact of PruMerge on processing efficiency within LMMs.\nConsider a typical scenario where an image of dimensions  pixels is processed using a CLIP-ViT model, resulting in 576 visual tokens. Accompanying this image is a text prompt, assumed to contain 40 tokens for the sake of this analysis. Through the application of PruMerge, we achieve a dramatic reduction in the number of visual tokens, decreasing the original count by approximately 14.4 times in MME/TextVQA to match the token count of the text prompt ().\nThe implications of this reduction are significant, as demonstrated in Table 2  ###reference_###, which outlines the computational cost associated with the LMM prefill process. Notably, PruMerge not only enhances the speed of the LLM prefill process by reducing the required floating-point operations (FLOPs) but also contributes to a reduction in computational memory demands.\nIt is important to emphasize that the benefits of PruMerge  extend beyond mere efficiency gains. Our token reduction strategy can complement other LLM acceleration techniques, such as quantization and factorization [Yuan et al., 2023b  ###reference_b47###]. This orthogonal relationship underscores the versatile potential of PruMerge to contribute to a broader spectrum of efficiency-enhancing strategies."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Generalization on Video-LLM",
            "text": "To assess the generalization capabilities of PruMerge and PruMerge+ across different modalities, we next extend our approach to Video-LLaVA [Lin et al., 2023  ###reference_b23###].\nVideo-LLaVA is one of the most popular open-soruced Video-LLMs.\nWe seamlessly integrate both algorithms into Video-LLaVA without the need for additional training, enabling us to bypass re-training on video datasets during inference. The outcome of this integration is shown in Table 3  ###reference_###.\nVideo-LLaVA samples 8 frames from a video clip and extracts  visual tokens using a visual encoder for LLM perception, which is 4 times of visual token than LLaAV-1.5 [Liu et al., 2023a  ###reference_b25###].\nOur Algorthms PruMerge and PruMerge+ can adaptively select important 256 (12.5% on average) and 256 (25.0% on average) important visual tokens, respectively.\nThe results demonstrate that our algorithms not only reduce the number of visual tokens in Video-LLaVA but also is able to enhance its performance.\nThis finding is noteworthy as it suggests a significant redundancy in the visual tokens used by video-LLMs. Exploring ways to further capitalize on this redundancy could shape future research directions."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Ablation Study",
            "text": ""
        },
        {
            "section_id": "4.4.1",
            "parent_section_id": "4.4",
            "section_name": "4.4.1 Token Sampling Strategy Analysis",
            "text": "Here we show how our approach performs better than the vanilla visual token sampling strategy, including sequential sampling and spatial sampling.\nLLaVA-PruMerge: Our approach dynamically samples key visual tokens (see Sec. 3.2  ###reference_###), which results in 40 visual tokens per image on average for TextVQA/MME, 35 tokens for POPE, and 16 tokens for SQA. The visualization is shown in Figure 4  ###reference_### (b).\nSequential sampling: We sample  tokens in the flatted visual tokens. For example, the first 40 tokens are sampled for an apples-to-apples comparison, shown in Figure 4  ###reference_### (c).\n###figure_6### Spatial sampling: The sampled  tokens are evenly distributed across the image, as shown in Figure 4  ###reference_### (d-h). We study diverse settings, including 6  6 (36 tokens), 5  8 (40 tokens), 8  5 (40 tokens), 5  7 (35 tokens), 7  5 (35 tokens), and 4  4 (16 tokens).\nNote that all the experiments are done via a training-free manner. As shown in Table 6  ###reference_###, our approach is consistently better than sequential sampling and spatial sampling across all downstream tasks, which demonstrates the effectiveness of the sampling mechanism of LLaVA-PruMerge. Importantly, we observe that LLaVA-PruMerge shows much better performance on TextVQA [Singh et al., 2019  ###reference_b36###]. Such Optical Character Recognition (OCR) task requires detailed information about the text, which demonstrates that LLaVA-PruMerge extracts the key information in the images with enough details. This quantitative result aligns with the visualization of LLaVA-PruMerge attentive tokens in Figure 1(b)  ###reference_sf2###, where more attentive tokens are distributed on the foreground text in the images."
        },
        {
            "section_id": "4.4.2",
            "parent_section_id": "4.4",
            "section_name": "4.4.2 Effectiveness of Each Module in PruMerge",
            "text": "Here, we study the effectiveness of each module in our design based on LLaVA-1.5. Note that we maintain the same amount of visual tokens (6.9%, 40 tokens) across all settings.\nAs shown in Table 6  ###reference_###, after progressively adding the proposed modules, including Adaptive Important Token Selection (AITS) and Token Supplement (TS), the downstream performance can be further enhanced."
        },
        {
            "section_id": "4.4.3",
            "parent_section_id": "4.4",
            "section_name": "4.4.3 Training Analysis: Training-free v.s. Fine-tuning",
            "text": "Finally, LLaVA-PruMerge can be conducted in either a training-free or fine-tuning manner. With fine-tuning, the large language model can adapt to the new structure of visual tokens, which could further enhance the performance on vision-language tasks. As shown in Table 6  ###reference_###, with fine-tuning, our approach does bring better performance for diverse tasks, including ScienceQA [Lu et al., 2022  ###reference_b30###], TextVQA [Singh et al., 2019  ###reference_b36###], POPE [Li et al., 2023b  ###reference_b21###], and MME [Fu et al., 2023  ###reference_b12###]."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we improve the efficiency of Large Multimodal Models (LMMs) from the perspective of reducing the quantity of visual tokens. By leveraging the spatial redundancy in visual tokens, we proposed a plug-and-play token reduction module that employs the similarity between the class token and spatial tokens as a key criterion for pruning and merging visual tokens.\nOur approach, applied to LLaVA-1.5, demonstrated that by utilizing only 6.9% of visual tokens on average, the pruned tokens can maintain comparable performance across a wide range of visual question-answering and reasoning tasks. Notably, our work highlights the potential for significant computational savings without sacrificing the reasoning capabilities of LMMs. We hope our work inspires further exploration into the interplay between efficiency and performance in LMMs."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A Appendix",
            "text": ""
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T1\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S4.T1.4.1.1\" style=\"font-size:90%;\">Table 1</span>: </span><span class=\"ltx_text\" id=\"S4.T1.5.2\" style=\"font-size:90%;\">Comparison with large multimodal models on six benchmarks. Our proposed approach </span><span class=\"ltx_text ltx_font_typewriter\" id=\"S4.T1.6.3\">P<span class=\"ltx_text\" id=\"S4.T1.6.3.1\" style=\"font-size:90%;\">ruMerge<span class=\"ltx_text ltx_font_serif\" id=\"S4.T1.6.3.1.1\">\u00a0and </span></span>P<span class=\"ltx_text\" id=\"S4.T1.6.3.2\" style=\"font-size:90%;\">ruMerge<span class=\"ltx_text ltx_font_serif\" id=\"S4.T1.6.3.2.1\">+ can adaptively reduce visual tokens, which use only (respectively) 5.5% and 25.0% visual tokens on average (on 6 tasks) and achieves competitive performance compared to the original LLaVA-1.5.</span></span></span></figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T1.7\" style=\"width:415.8pt;height:210.6pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-58.6pt,29.7pt) scale(0.78,0.78) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T1.7.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T1.7.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T1.7.1.1.1.1\">Method</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T1.7.1.1.1.2\">LLM</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T1.7.1.1.1.3\">Res.</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T1.7.1.1.1.4\">PT</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T1.7.1.1.1.5\">IT</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T1.7.1.1.1.6\">VQA<sup class=\"ltx_sup\" id=\"S4.T1.7.1.1.1.6.1\">v2</sup>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T1.7.1.1.1.7\">SQA<sup class=\"ltx_sup\" id=\"S4.T1.7.1.1.1.7.1\">I</sup>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T1.7.1.1.1.8\">VQA<sup class=\"ltx_sup\" id=\"S4.T1.7.1.1.1.8.1\">T</sup>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T1.7.1.1.1.9\">POPE</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T1.7.1.1.1.10\">MME</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T1.7.1.1.1.11\">MMB</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T1.7.1.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.7.1.2.1.1\">BLIP-2</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.7.1.2.1.2\">Vicuna-13B</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.7.1.2.1.3\">224</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.7.1.2.1.4\">129M</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T1.7.1.2.1.5\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.7.1.2.1.6\">41.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.7.1.2.1.7\">61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.7.1.2.1.8\">42.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.7.1.2.1.9\">85.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.7.1.2.1.10\">1293.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.7.1.2.1.11\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.7.1.3.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.7.1.3.2.1\">InstructBLIP</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.7.1.3.2.2\">Vicuna-7B</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.7.1.3.2.3\">224</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.7.1.3.2.4\">129M</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.7.1.3.2.5\">1.2M</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.3.2.6\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.3.2.7\">60.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.7.1.3.2.8\">50.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.3.2.9\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.3.2.10\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.3.2.11\">36</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.7.1.4.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.7.1.4.3.1\">InstructBLIP</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.7.1.4.3.2\">Vicuna-13B</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.7.1.4.3.3\">224</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.7.1.4.3.4\">129M</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.7.1.4.3.5\">1.2M</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.4.3.6\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.4.3.7\">63.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.7.1.4.3.8\">50.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.4.3.9\">78.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.4.3.10\">1212.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.4.3.11\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.7.1.5.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.7.1.5.4.1\">Shikra</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.7.1.5.4.2\">Vicuna-13B</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.7.1.5.4.3\">224</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.7.1.5.4.4\">600K</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.7.1.5.4.5\">5.5M</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.5.4.6\">77.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.5.4.7\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.7.1.5.4.8\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.5.4.9\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.5.4.10\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.5.4.11\">58.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.7.1.6.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.7.1.6.5.1\">IDEFICS-9B</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.7.1.6.5.2\">LLaMA-7B</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.7.1.6.5.3\">224</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.7.1.6.5.4\">353M</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.7.1.6.5.5\">1M</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.6.5.6\">50.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.6.5.7\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.7.1.6.5.8\">25.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.6.5.9\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.6.5.10\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.6.5.11\">48.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.7.1.7.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.7.1.7.6.1\">IDEFICS-80B</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.7.1.7.6.2\">LLaMA-65B</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.7.1.7.6.3\">224</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.7.1.7.6.4\">353M</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.7.1.7.6.5\">1M</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.7.6.6\">60.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.7.6.7\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.7.1.7.6.8\">30.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.7.6.9\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.7.6.10\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.7.6.11\">54.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.7.1.8.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.7.1.8.7.1\">Qwen-VL</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.7.1.8.7.2\">Qwen-7B</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.7.1.8.7.3\">448</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.7.1.8.7.4\">1.4B</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.7.1.8.7.5\">50M</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.8.7.6\">78.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.8.7.7\">67.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.7.1.8.7.8\">63.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.8.7.9\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.8.7.10\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.8.7.11\">38.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.7.1.9.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.7.1.9.8.1\">Qwen-VL-Chat</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.7.1.9.8.2\">Qwen-7B</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.7.1.9.8.3\">448</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.7.1.9.8.4\">1.4B</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.7.1.9.8.5\">50M</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.9.8.6\">78.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.9.8.7\">68.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.7.1.9.8.8\">61.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.9.8.9\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.9.8.10\">1487.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.9.8.11\">60.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.7.1.10.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.7.1.10.9.1\">LLaVA-1.5</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.7.1.10.9.2\">Vicuna-7B</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.7.1.10.9.3\">336</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.7.1.10.9.4\">558K</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T1.7.1.10.9.5\">665K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.7.1.10.9.6\">78.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.7.1.10.9.7\">66.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.7.1.10.9.8\">58.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.7.1.10.9.9\">85.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.7.1.10.9.10\">1510.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.7.1.10.9.11\">64.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.7.1.11.10\" style=\"background-color:#F9F2EC;\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.7.1.11.10.1\"><span class=\"ltx_text\" id=\"S4.T1.7.1.11.10.1.1\" style=\"background-color:#F9F2EC;\">LLaVA-1.5 + <span class=\"ltx_text ltx_font_typewriter\" id=\"S4.T1.7.1.11.10.1.1.1\">P<span class=\"ltx_text\" id=\"S4.T1.7.1.11.10.1.1.1.1\" style=\"font-size:90%;\">ruMerge</span></span></span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.7.1.11.10.2\"><span class=\"ltx_text\" id=\"S4.T1.7.1.11.10.2.1\" style=\"background-color:#F9F2EC;\">Vicuna-7B</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.7.1.11.10.3\"><span class=\"ltx_text\" id=\"S4.T1.7.1.11.10.3.1\" style=\"background-color:#F9F2EC;\">336</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.7.1.11.10.4\"><span class=\"ltx_text\" id=\"S4.T1.7.1.11.10.4.1\" style=\"background-color:#F9F2EC;\">558K</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.7.1.11.10.5\"><span class=\"ltx_text\" id=\"S4.T1.7.1.11.10.5.1\" style=\"background-color:#F9F2EC;\">665K</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.11.10.6\"><span class=\"ltx_text\" id=\"S4.T1.7.1.11.10.6.1\" style=\"background-color:#F9F2EC;\">72.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.11.10.7\"><span class=\"ltx_text\" id=\"S4.T1.7.1.11.10.7.1\" style=\"background-color:#F9F2EC;\">68.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.7.1.11.10.8\"><span class=\"ltx_text\" id=\"S4.T1.7.1.11.10.8.1\" style=\"background-color:#F9F2EC;\">56.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.11.10.9\"><span class=\"ltx_text\" id=\"S4.T1.7.1.11.10.9.1\" style=\"background-color:#F9F2EC;\">76.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.11.10.10\"><span class=\"ltx_text\" id=\"S4.T1.7.1.11.10.10.1\" style=\"background-color:#F9F2EC;\">1350.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.11.10.11\"><span class=\"ltx_text\" id=\"S4.T1.7.1.11.10.11.1\" style=\"background-color:#F9F2EC;\">60.9</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.7.1.12.11\" style=\"background-color:#F2E6D9;\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.7.1.12.11.1\"><span class=\"ltx_text\" id=\"S4.T1.7.1.12.11.1.1\" style=\"background-color:#F2E6D9;\">LLaVA-1.5 + <span class=\"ltx_text ltx_font_typewriter\" id=\"S4.T1.7.1.12.11.1.1.1\">P<span class=\"ltx_text\" id=\"S4.T1.7.1.12.11.1.1.1.1\" style=\"font-size:90%;\">ruMerge</span></span>+</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.7.1.12.11.2\"><span class=\"ltx_text\" id=\"S4.T1.7.1.12.11.2.1\" style=\"background-color:#F2E6D9;\">Vicuna-7B</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.7.1.12.11.3\"><span class=\"ltx_text\" id=\"S4.T1.7.1.12.11.3.1\" style=\"background-color:#F2E6D9;\">336</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.7.1.12.11.4\"><span class=\"ltx_text\" id=\"S4.T1.7.1.12.11.4.1\" style=\"background-color:#F2E6D9;\">558K</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.7.1.12.11.5\"><span class=\"ltx_text\" id=\"S4.T1.7.1.12.11.5.1\" style=\"background-color:#F2E6D9;\">665K</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.12.11.6\"><span class=\"ltx_text\" id=\"S4.T1.7.1.12.11.6.1\" style=\"background-color:#F2E6D9;\">76.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.12.11.7\"><span class=\"ltx_text\" id=\"S4.T1.7.1.12.11.7.1\" style=\"background-color:#F2E6D9;\">68.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.7.1.12.11.8\"><span class=\"ltx_text\" id=\"S4.T1.7.1.12.11.8.1\" style=\"background-color:#F2E6D9;\">57.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.12.11.9\"><span class=\"ltx_text\" id=\"S4.T1.7.1.12.11.9.1\" style=\"background-color:#F2E6D9;\">84.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.12.11.10\"><span class=\"ltx_text\" id=\"S4.T1.7.1.12.11.10.1\" style=\"background-color:#F2E6D9;\">1462.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.12.11.11\"><span class=\"ltx_text\" id=\"S4.T1.7.1.12.11.11.1\" style=\"background-color:#F2E6D9;\">64.9</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.7.1.13.12\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.7.1.13.12.1\">LLaVA-1.5</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.7.1.13.12.2\">Vicuna-13B</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.7.1.13.12.3\">336</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.7.1.13.12.4\">558K</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T1.7.1.13.12.5\">665K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.7.1.13.12.6\">80.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.7.1.13.12.7\">71.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.7.1.13.12.8\">61.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.7.1.13.12.9\">85.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.7.1.13.12.10\">1531.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.7.1.13.12.11\">67.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.7.1.14.13\" style=\"background-color:#F9F2EC;\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.7.1.14.13.1\"><span class=\"ltx_text\" id=\"S4.T1.7.1.14.13.1.1\" style=\"background-color:#F9F2EC;\">LLaVA-1.5 + <span class=\"ltx_text ltx_font_typewriter\" id=\"S4.T1.7.1.14.13.1.1.1\">P<span class=\"ltx_text\" id=\"S4.T1.7.1.14.13.1.1.1.1\" style=\"font-size:90%;\">ruMerge</span></span></span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.7.1.14.13.2\"><span class=\"ltx_text\" id=\"S4.T1.7.1.14.13.2.1\" style=\"background-color:#F9F2EC;\">Vicuna-13B</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.7.1.14.13.3\"><span class=\"ltx_text\" id=\"S4.T1.7.1.14.13.3.1\" style=\"background-color:#F9F2EC;\">336</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.7.1.14.13.4\"><span class=\"ltx_text\" id=\"S4.T1.7.1.14.13.4.1\" style=\"background-color:#F9F2EC;\">558K</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.7.1.14.13.5\"><span class=\"ltx_text\" id=\"S4.T1.7.1.14.13.5.1\" style=\"background-color:#F9F2EC;\">665K</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.14.13.6\"><span class=\"ltx_text\" id=\"S4.T1.7.1.14.13.6.1\" style=\"background-color:#F9F2EC;\">72.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.14.13.7\"><span class=\"ltx_text\" id=\"S4.T1.7.1.14.13.7.1\" style=\"background-color:#F9F2EC;\">71.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.7.1.14.13.8\"><span class=\"ltx_text\" id=\"S4.T1.7.1.14.13.8.1\" style=\"background-color:#F9F2EC;\">58.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.14.13.9\"><span class=\"ltx_text\" id=\"S4.T1.7.1.14.13.9.1\" style=\"background-color:#F9F2EC;\">78.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.14.13.10\"><span class=\"ltx_text\" id=\"S4.T1.7.1.14.13.10.1\" style=\"background-color:#F9F2EC;\">1428.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.7.1.14.13.11\"><span class=\"ltx_text\" id=\"S4.T1.7.1.14.13.11.1\" style=\"background-color:#F9F2EC;\">62.3</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.7.1.15.14\" style=\"background-color:#F2E6D9;\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T1.7.1.15.14.1\"><span class=\"ltx_text\" id=\"S4.T1.7.1.15.14.1.1\" style=\"background-color:#F2E6D9;\">LLaVA-1.5 + <span class=\"ltx_text ltx_font_typewriter\" id=\"S4.T1.7.1.15.14.1.1.1\">P<span class=\"ltx_text\" id=\"S4.T1.7.1.15.14.1.1.1.1\" style=\"font-size:90%;\">ruMerge</span></span>+</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T1.7.1.15.14.2\"><span class=\"ltx_text\" id=\"S4.T1.7.1.15.14.2.1\" style=\"background-color:#F2E6D9;\">Vicuna-13B</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T1.7.1.15.14.3\"><span class=\"ltx_text\" id=\"S4.T1.7.1.15.14.3.1\" style=\"background-color:#F2E6D9;\">336</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T1.7.1.15.14.4\"><span class=\"ltx_text\" id=\"S4.T1.7.1.15.14.4.1\" style=\"background-color:#F2E6D9;\">558K</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" id=\"S4.T1.7.1.15.14.5\"><span class=\"ltx_text\" id=\"S4.T1.7.1.15.14.5.1\" style=\"background-color:#F2E6D9;\">665K</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.7.1.15.14.6\"><span class=\"ltx_text\" id=\"S4.T1.7.1.15.14.6.1\" style=\"background-color:#F2E6D9;\">77.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.7.1.15.14.7\"><span class=\"ltx_text\" id=\"S4.T1.7.1.15.14.7.1\" style=\"background-color:#F2E6D9;\">71.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T1.7.1.15.14.8\"><span class=\"ltx_text\" id=\"S4.T1.7.1.15.14.8.1\" style=\"background-color:#F2E6D9;\">58.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.7.1.15.14.9\"><span class=\"ltx_text\" id=\"S4.T1.7.1.15.14.9.1\" style=\"background-color:#F2E6D9;\">84.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.7.1.15.14.10\"><span class=\"ltx_text\" id=\"S4.T1.7.1.15.14.10.1\" style=\"background-color:#F2E6D9;\">1485.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.7.1.15.14.11\"><span class=\"ltx_text\" id=\"S4.T1.7.1.15.14.11.1\" style=\"background-color:#F2E6D9;\">65.7</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 1: Comparison with large multimodal models on six benchmarks. Our proposed approach PruMerge\u00a0and PruMerge+ can adaptively reduce visual tokens, which use only (respectively) 5.5% and 25.0% visual tokens on average (on 6 tasks) and achieves competitive performance compared to the original LLaVA-1.5."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T2\">\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Computation Cost Analysis. The development device is Tesla V100 GPU, and time estimated by the roofline model represents the theoretical performance that the hardware can achieve. </figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T2.4\" style=\"width:417.0pt;height:158.4pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-28.4pt,10.8pt) scale(0.88,0.88) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T2.4.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T2.4.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" id=\"S4.T2.4.1.1.1.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.4.1.1.1.1.1\" style=\"font-size:90%;\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T2.4.1.1.1.2\"><span class=\"ltx_text\" id=\"S4.T2.4.1.1.1.2.1\" style=\"font-size:90%;\">LLM</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T2.4.1.1.1.3\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.4.1.1.1.3.1\" style=\"font-size:90%;\">Quantization</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T2.4.1.1.1.4\"><span class=\"ltx_text\" id=\"S4.T2.4.1.1.1.4.1\" style=\"font-size:90%;\">FLOPs</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T2.4.1.1.1.5\"><span class=\"ltx_text\" id=\"S4.T2.4.1.1.1.5.1\" style=\"font-size:90%;\">Prefill</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T2.4.1.1.1.6\"><span class=\"ltx_text\" id=\"S4.T2.4.1.1.1.6.1\" style=\"font-size:90%;\">Total</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T2.4.1.1.1.7\"><span class=\"ltx_text\" id=\"S4.T2.4.1.1.1.7.1\" style=\"font-size:90%;\">Storing</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.4.1.2.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.1.2.2.1\"><span class=\"ltx_text\" id=\"S4.T2.4.1.2.2.1.1\" style=\"font-size:90%;\">Backbone</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.1.2.2.2\"><span class=\"ltx_text\" id=\"S4.T2.4.1.2.2.2.1\" style=\"font-size:90%;\">(TB)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.1.2.2.3\"><span class=\"ltx_text\" id=\"S4.T2.4.1.2.2.3.1\" style=\"font-size:90%;\">Time (ms)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.1.2.2.4\"><span class=\"ltx_text\" id=\"S4.T2.4.1.2.2.4.1\" style=\"font-size:90%;\">Memory (GB)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.1.2.2.5\"><span class=\"ltx_text\" id=\"S4.T2.4.1.2.2.5.1\" style=\"font-size:90%;\">Activation (GB)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.4.1.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T2.4.1.3.3.1\"><span class=\"ltx_text\" id=\"S4.T2.4.1.3.3.1.1\" style=\"font-size:90%;\">LLaVA-1.5</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.4.1.3.3.2\"><span class=\"ltx_text\" id=\"S4.T2.4.1.3.3.2.1\" style=\"font-size:90%;\">Vicuna-7B</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S4.T2.4.1.3.3.3\"><span class=\"ltx_text\" id=\"S4.T2.4.1.3.3.3.1\" style=\"font-size:90%;\">FP16</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.4.1.3.3.4\"><span class=\"ltx_text\" id=\"S4.T2.4.1.3.3.4.1\" style=\"font-size:90%;\">9.3</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.4.1.3.3.5\"><span class=\"ltx_text\" id=\"S4.T2.4.1.3.3.5.1\" style=\"font-size:90%;\">88.6</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.4.1.3.3.6\"><span class=\"ltx_text\" id=\"S4.T2.4.1.3.3.6.1\" style=\"font-size:90%;\">23.3</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.4.1.3.3.7\"><span class=\"ltx_text\" id=\"S4.T2.4.1.3.3.7.1\" style=\"font-size:90%;\">4.60</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.4.1.4.4\" style=\"background-color:#F9F2EC;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.4.1.4.4.1\"><span class=\"ltx_text\" id=\"S4.T2.4.1.4.4.1.1\" style=\"font-size:90%;background-color:#F9F2EC;\">LLaVA-1.5 w/ <span class=\"ltx_text ltx_font_typewriter\" id=\"S4.T2.4.1.4.4.1.1.1\" style=\"font-size:111%;background-color:#F9F2EC;\">P<span class=\"ltx_text\" id=\"S4.T2.4.1.4.4.1.1.1.1\" style=\"font-size:90%;\">ruMerge</span></span></span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.1.4.4.2\"><span class=\"ltx_text\" id=\"S4.T2.4.1.4.4.2.1\" style=\"font-size:90%;background-color:#F9F2EC;\">Vicuna-7B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.4.1.4.4.3\"><span class=\"ltx_text\" id=\"S4.T2.4.1.4.4.3.1\" style=\"font-size:90%;background-color:#F9F2EC;\">FP16</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.1.4.4.4\"><span class=\"ltx_text\" id=\"S4.T2.4.1.4.4.4.1\" style=\"font-size:90%;background-color:#F9F2EC;\">0.91</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.1.4.4.5\"><span class=\"ltx_text\" id=\"S4.T2.4.1.4.4.5.1\" style=\"font-size:90%;background-color:#F9F2EC;\">15.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.1.4.4.6\"><span class=\"ltx_text\" id=\"S4.T2.4.1.4.4.6.1\" style=\"font-size:90%;background-color:#F9F2EC;\">13.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.1.4.4.7\"><span class=\"ltx_text\" id=\"S4.T2.4.1.4.4.7.1\" style=\"font-size:90%;background-color:#F9F2EC;\">0.28</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.4.1.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r\" id=\"S4.T2.4.1.5.5.1\"><span class=\"ltx_text\" id=\"S4.T2.4.1.5.5.1.1\" style=\"font-size:90%;\">LLaVA-1.5</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.4.1.5.5.2\"><span class=\"ltx_text\" id=\"S4.T2.4.1.5.5.2.1\" style=\"font-size:90%;\">Vicuna-7B</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S4.T2.4.1.5.5.3\"><span class=\"ltx_text\" id=\"S4.T2.4.1.5.5.3.1\" style=\"font-size:90%;\">INT4</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.4.1.5.5.4\"><span class=\"ltx_text\" id=\"S4.T2.4.1.5.5.4.1\" style=\"font-size:90%;\">2.3</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.4.1.5.5.5\"><span class=\"ltx_text\" id=\"S4.T2.4.1.5.5.5.1\" style=\"font-size:90%;\">151.6</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.4.1.5.5.6\"><span class=\"ltx_text\" id=\"S4.T2.4.1.5.5.6.1\" style=\"font-size:90%;\">5.9</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.4.1.5.5.7\"><span class=\"ltx_text\" id=\"S4.T2.4.1.5.5.7.1\" style=\"font-size:90%;\">1.20</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.4.1.6.6\" style=\"background-color:#F9F2EC;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.4.1.6.6.1\"><span class=\"ltx_text\" id=\"S4.T2.4.1.6.6.1.1\" style=\"font-size:90%;background-color:#F9F2EC;\">LLaVA-1.5 w/ <span class=\"ltx_text ltx_font_typewriter\" id=\"S4.T2.4.1.6.6.1.1.1\" style=\"font-size:111%;background-color:#F9F2EC;\">P<span class=\"ltx_text\" id=\"S4.T2.4.1.6.6.1.1.1.1\" style=\"font-size:90%;\">ruMerge</span></span></span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.1.6.6.2\"><span class=\"ltx_text\" id=\"S4.T2.4.1.6.6.2.1\" style=\"font-size:90%;background-color:#F9F2EC;\">Vicuna-7B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.4.1.6.6.3\"><span class=\"ltx_text\" id=\"S4.T2.4.1.6.6.3.1\" style=\"font-size:90%;background-color:#F9F2EC;\">INT4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.1.6.6.4\"><span class=\"ltx_text\" id=\"S4.T2.4.1.6.6.4.1\" style=\"font-size:90%;background-color:#F9F2EC;\">0.28</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.1.6.6.5\"><span class=\"ltx_text\" id=\"S4.T2.4.1.6.6.5.1\" style=\"font-size:90%;background-color:#F9F2EC;\">14.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.1.6.6.6\"><span class=\"ltx_text\" id=\"S4.T2.4.1.6.6.6.1\" style=\"font-size:90%;background-color:#F9F2EC;\">3.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.1.6.6.7\"><span class=\"ltx_text\" id=\"S4.T2.4.1.6.6.7.1\" style=\"font-size:90%;background-color:#F9F2EC;\">0.07</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.4.1.7.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T2.4.1.7.7.1\"><span class=\"ltx_text\" id=\"S4.T2.4.1.7.7.1.1\" style=\"font-size:90%;\">LLaVA-1.5</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.4.1.7.7.2\"><span class=\"ltx_text\" id=\"S4.T2.4.1.7.7.2.1\" style=\"font-size:90%;\">Vicuna-13B</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S4.T2.4.1.7.7.3\"><span class=\"ltx_text\" id=\"S4.T2.4.1.7.7.3.1\" style=\"font-size:90%;\">FP16</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.4.1.7.7.4\"><span class=\"ltx_text\" id=\"S4.T2.4.1.7.7.4.1\" style=\"font-size:90%;\">18.2</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.4.1.7.7.5\"><span class=\"ltx_text\" id=\"S4.T2.4.1.7.7.5.1\" style=\"font-size:90%;\">170.5</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.4.1.7.7.6\"><span class=\"ltx_text\" id=\"S4.T2.4.1.7.7.6.1\" style=\"font-size:90%;\">41.6</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.4.1.7.7.7\"><span class=\"ltx_text\" id=\"S4.T2.4.1.7.7.7.1\" style=\"font-size:90%;\">7.30</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.4.1.8.8\" style=\"background-color:#F9F2EC;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.4.1.8.8.1\"><span class=\"ltx_text\" id=\"S4.T2.4.1.8.8.1.1\" style=\"font-size:90%;background-color:#F9F2EC;\">LLaVA-1.5 w/ <span class=\"ltx_text ltx_font_typewriter\" id=\"S4.T2.4.1.8.8.1.1.1\" style=\"font-size:111%;background-color:#F9F2EC;\">P<span class=\"ltx_text\" id=\"S4.T2.4.1.8.8.1.1.1.1\" style=\"font-size:90%;\">ruMerge</span></span></span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.1.8.8.2\"><span class=\"ltx_text\" id=\"S4.T2.4.1.8.8.2.1\" style=\"font-size:90%;background-color:#F9F2EC;\">Vicuna-13B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.4.1.8.8.3\"><span class=\"ltx_text\" id=\"S4.T2.4.1.8.8.3.1\" style=\"font-size:90%;background-color:#F9F2EC;\">FP16</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.1.8.8.4\"><span class=\"ltx_text\" id=\"S4.T2.4.1.8.8.4.1\" style=\"font-size:90%;background-color:#F9F2EC;\">1.80</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.1.8.8.5\"><span class=\"ltx_text\" id=\"S4.T2.4.1.8.8.5.1\" style=\"font-size:90%;background-color:#F9F2EC;\">29.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.1.8.8.6\"><span class=\"ltx_text\" id=\"S4.T2.4.1.8.8.6.1\" style=\"font-size:90%;background-color:#F9F2EC;\">26.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.1.8.8.7\"><span class=\"ltx_text\" id=\"S4.T2.4.1.8.8.7.1\" style=\"font-size:90%;background-color:#F9F2EC;\">0.44</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.4.1.9.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r\" id=\"S4.T2.4.1.9.9.1\"><span class=\"ltx_text\" id=\"S4.T2.4.1.9.9.1.1\" style=\"font-size:90%;\">LLaVA-1.5</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.4.1.9.9.2\"><span class=\"ltx_text\" id=\"S4.T2.4.1.9.9.2.1\" style=\"font-size:90%;\">Vicuna-13B</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S4.T2.4.1.9.9.3\"><span class=\"ltx_text\" id=\"S4.T2.4.1.9.9.3.1\" style=\"font-size:90%;\">INT4</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.4.1.9.9.4\"><span class=\"ltx_text\" id=\"S4.T2.4.1.9.9.4.1\" style=\"font-size:90%;\">4.6</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.4.1.9.9.5\"><span class=\"ltx_text\" id=\"S4.T2.4.1.9.9.5.1\" style=\"font-size:90%;\">294.9</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.4.1.9.9.6\"><span class=\"ltx_text\" id=\"S4.T2.4.1.9.9.6.1\" style=\"font-size:90%;\">10.5</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.4.1.9.9.7\"><span class=\"ltx_text\" id=\"S4.T2.4.1.9.9.7.1\" style=\"font-size:90%;\">1.80</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.4.1.10.10\" style=\"background-color:#F9F2EC;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S4.T2.4.1.10.10.1\"><span class=\"ltx_text\" id=\"S4.T2.4.1.10.10.1.1\" style=\"font-size:90%;background-color:#F9F2EC;\">LLaVA-1.5 w/ <span class=\"ltx_text ltx_font_typewriter\" id=\"S4.T2.4.1.10.10.1.1.1\" style=\"font-size:111%;background-color:#F9F2EC;\">P<span class=\"ltx_text\" id=\"S4.T2.4.1.10.10.1.1.1.1\" style=\"font-size:90%;\">ruMerge</span></span></span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.4.1.10.10.2\"><span class=\"ltx_text\" id=\"S4.T2.4.1.10.10.2.1\" style=\"font-size:90%;background-color:#F9F2EC;\">Vicuna-13B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T2.4.1.10.10.3\"><span class=\"ltx_text\" id=\"S4.T2.4.1.10.10.3.1\" style=\"font-size:90%;background-color:#F9F2EC;\">INT4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.4.1.10.10.4\"><span class=\"ltx_text\" id=\"S4.T2.4.1.10.10.4.1\" style=\"font-size:90%;background-color:#F9F2EC;\">0.45</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.4.1.10.10.5\"><span class=\"ltx_text\" id=\"S4.T2.4.1.10.10.5.1\" style=\"font-size:90%;background-color:#F9F2EC;\">29.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.4.1.10.10.6\"><span class=\"ltx_text\" id=\"S4.T2.4.1.10.10.6.1\" style=\"font-size:90%;background-color:#F9F2EC;\">6.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.4.1.10.10.7\"><span class=\"ltx_text\" id=\"S4.T2.4.1.10.10.7.1\" style=\"font-size:90%;background-color:#F9F2EC;\">0.11</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 2: Computation Cost Analysis. The development device is Tesla V100 GPU, and time estimated by the roofline model represents the theoretical performance that the hardware can achieve. "
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T3\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S4.T3.4.1.1\" style=\"font-size:90%;\">Table 3</span>: </span><span class=\"ltx_text\" id=\"S4.T3.5.2\" style=\"font-size:90%;\">A comparison of different LVMs on video reasoning benchmarks. Like Video-LLaVA, ChatGPT-Assistant is employed to evaluate performance. The version of ChatGPT is \u201cgpt-3.5-turbo\u201d. Note that we directly add </span><span class=\"ltx_text ltx_font_typewriter\" id=\"S4.T3.6.3\">P<span class=\"ltx_text\" id=\"S4.T3.6.3.1\" style=\"font-size:90%;\">ruMerge<span class=\"ltx_text ltx_font_serif\" id=\"S4.T3.6.3.1.1\">\u00a0and </span></span>P<span class=\"ltx_text\" id=\"S4.T3.6.3.2\" style=\"font-size:90%;\">ruMerge<span class=\"ltx_text ltx_font_serif\" id=\"S4.T3.6.3.2.1\">+ to Video-LLaVA during inference (without training our own model).</span></span></span></figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T3.7\" style=\"width:358.1pt;height:144pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-44.8pt,18.0pt) scale(0.8,0.8) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T3.7.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T3.7.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" id=\"S4.T3.7.1.1.1.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T3.7.1.1.1.1.1\">Methods</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\" id=\"S4.T3.7.1.1.1.2\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T3.7.1.1.1.2.1\">LLM size</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\" id=\"S4.T3.7.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.7.1.1.1.3.1\">MSVD-QA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\" id=\"S4.T3.7.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.7.1.1.1.4.1\">MSRVT-QA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S4.T3.7.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.7.1.1.1.5.1\">ActivityNet-QA</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.7.1.2.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.2.2.1\">Accuracy</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.7.1.2.2.2\">Score</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.2.2.3\">Accuracy</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.7.1.2.2.4\">Score</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.2.2.5\">Accuracy</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T3.7.1.2.2.6\">Score</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.7.1.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T3.7.1.3.3.1\">FrozenBiLM</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T3.7.1.3.3.2\">1B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.7.1.3.3.3\">32.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.7.1.3.3.4\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.7.1.3.3.5\">16.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.7.1.3.3.6\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.7.1.3.3.7\">24.7</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S4.T3.7.1.3.3.8\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.7.1.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T3.7.1.4.4.1\">VideoChat</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.7.1.4.4.2\">7B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.4.4.3\">56.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.7.1.4.4.4\">2.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.4.4.5\">45.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.7.1.4.4.6\">2.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.4.4.7\">-</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T3.7.1.4.4.8\">2.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.7.1.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T3.7.1.5.5.1\">LLaMA-Adapter</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.7.1.5.5.2\">7B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.5.5.3\">54.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.7.1.5.5.4\">3.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.5.5.5\">43.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.7.1.5.5.6\">2.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.5.5.7\">34.2</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T3.7.1.5.5.8\">2.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.7.1.6.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T3.7.1.6.6.1\">Video-LLaMA</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.7.1.6.6.2\">7B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.6.6.3\">51.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.7.1.6.6.4\">2.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.6.6.5\">29.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.7.1.6.6.6\">1.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.6.6.7\">12.4</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T3.7.1.6.6.8\">1.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.7.1.7.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T3.7.1.7.7.1\">Video-ChatGPT</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.7.1.7.7.2\">7B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.7.7.3\">64.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.7.1.7.7.4\">3.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.7.7.5\">49.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.7.1.7.7.6\">2.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.7.7.7\">35.2</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T3.7.1.7.7.8\">2.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.7.1.8.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T3.7.1.8.8.1\">Video-LLaVA</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T3.7.1.8.8.2\">7B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.7.1.8.8.3\">70.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.7.1.8.8.4\">3.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.7.1.8.8.5\">59.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.7.1.8.8.6\">3.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.7.1.8.8.7\">45.3</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S4.T3.7.1.8.8.8\">3.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.7.1.9.9\" style=\"background-color:#F9F2EC;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T3.7.1.9.9.1\"><span class=\"ltx_text\" id=\"S4.T3.7.1.9.9.1.1\" style=\"background-color:#F9F2EC;\">Video-LLaVA + <span class=\"ltx_text ltx_font_typewriter\" id=\"S4.T3.7.1.9.9.1.1.1\">P<span class=\"ltx_text\" id=\"S4.T3.7.1.9.9.1.1.1.1\" style=\"font-size:90%;\">ruMerge</span></span></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.7.1.9.9.2\"><span class=\"ltx_text\" id=\"S4.T3.7.1.9.9.2.1\" style=\"background-color:#F9F2EC;\">7B</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.9.9.3\"><span class=\"ltx_text\" id=\"S4.T3.7.1.9.9.3.1\" style=\"background-color:#F9F2EC;\">71.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.7.1.9.9.4\"><span class=\"ltx_text\" id=\"S4.T3.7.1.9.9.4.1\" style=\"background-color:#F9F2EC;\">3.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.9.9.5\"><span class=\"ltx_text\" id=\"S4.T3.7.1.9.9.5.1\" style=\"background-color:#F9F2EC;\">58.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.7.1.9.9.6\"><span class=\"ltx_text\" id=\"S4.T3.7.1.9.9.6.1\" style=\"background-color:#F9F2EC;\">3.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.9.9.7\"><span class=\"ltx_text\" id=\"S4.T3.7.1.9.9.7.1\" style=\"background-color:#F9F2EC;\">48.3</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T3.7.1.9.9.8\"><span class=\"ltx_text\" id=\"S4.T3.7.1.9.9.8.1\" style=\"background-color:#F9F2EC;\">3.4</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.7.1.10.10\" style=\"background-color:#F2E6D9;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S4.T3.7.1.10.10.1\"><span class=\"ltx_text\" id=\"S4.T3.7.1.10.10.1.1\" style=\"background-color:#F2E6D9;\">Video-LLaVA + <span class=\"ltx_text ltx_font_typewriter\" id=\"S4.T3.7.1.10.10.1.1.1\">P<span class=\"ltx_text\" id=\"S4.T3.7.1.10.10.1.1.1.1\" style=\"font-size:90%;\">ruMerge</span></span>+</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S4.T3.7.1.10.10.2\"><span class=\"ltx_text\" id=\"S4.T3.7.1.10.10.2.1\" style=\"background-color:#F2E6D9;\">7B</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.7.1.10.10.3\"><span class=\"ltx_text\" id=\"S4.T3.7.1.10.10.3.1\" style=\"background-color:#F2E6D9;\">71.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T3.7.1.10.10.4\"><span class=\"ltx_text\" id=\"S4.T3.7.1.10.10.4.1\" style=\"background-color:#F2E6D9;\">3.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.7.1.10.10.5\"><span class=\"ltx_text\" id=\"S4.T3.7.1.10.10.5.1\" style=\"background-color:#F2E6D9;\">59.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T3.7.1.10.10.6\"><span class=\"ltx_text\" id=\"S4.T3.7.1.10.10.6.1\" style=\"background-color:#F2E6D9;\">3.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.7.1.10.10.7\"><span class=\"ltx_text\" id=\"S4.T3.7.1.10.10.7.1\" style=\"background-color:#F2E6D9;\">47.7</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\" id=\"S4.T3.7.1.10.10.8\"><span class=\"ltx_text\" id=\"S4.T3.7.1.10.10.8.1\" style=\"background-color:#F2E6D9;\">3.4</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 3: A comparison of different LVMs on video reasoning benchmarks. Like Video-LLaVA, ChatGPT-Assistant is employed to evaluate performance. The version of ChatGPT is \u201cgpt-3.5-turbo\u201d. Note that we directly add PruMerge\u00a0and PruMerge+ to Video-LLaVA during inference (without training our own model)."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T6\">\n<div class=\"ltx_flex_figure ltx_flex_table\">\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<figure class=\"ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle\" id=\"S4.T6.8\" style=\"width:173.4pt;\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"S4.T6.8.9.1.1\" style=\"font-size:90%;\">Table 4</span>: </span><span class=\"ltx_text\" id=\"S4.T6.8.10.2\" style=\"font-size:90%;\">Token Sampling Strategy Analysis on Different Tasks.\n</span></figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T6.8.8\" style=\"width:169.1pt;height:264.6pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-36.2pt,56.7pt) scale(0.7,0.7) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T6.8.8.8\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T6.8.8.8.9.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\" id=\"S4.T6.8.8.8.9.1.1\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\">Approach</th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T6.8.8.8.9.1.2\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\">#Visual Tokens</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T6.8.8.8.9.1.3\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\">Performance</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.8.8.8.10.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"3\" id=\"S4.T6.8.8.8.10.2.1\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S4.T6.8.8.8.10.2.1.1\">Task: VQA<sup class=\"ltx_sup\" id=\"S4.T6.8.8.8.10.2.1.1.1\">T</sup></span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.8.8.8.11.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T6.8.8.8.11.3.1\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\">LLaVA-PruMerge</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.8.8.8.11.3.2\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\">40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.8.8.8.11.3.3\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.8.8.8.11.3.3.1\">54.00</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.8.8.8.12.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T6.8.8.8.12.4.1\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\">Sequential</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.8.8.8.12.4.2\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\">40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.8.8.8.12.4.3\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\">42.72</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T6.1.1.1.1.2\" rowspan=\"2\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\"><span class=\"ltx_text\" id=\"S4.T6.1.1.1.1.2.1\">Spatial</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.1.1.1.1.1\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.1.1.1.1.3\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\">46.85</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.2.2.2.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.2.2.2.2.1\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.2.2.2.2.2\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\">47.42</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.8.8.8.13.5\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"3\" id=\"S4.T6.8.8.8.13.5.1\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S4.T6.8.8.8.13.5.1.1\">Task: MME</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.8.8.8.14.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T6.8.8.8.14.6.1\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\">LLaVA-PruMerge</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.8.8.8.14.6.2\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\">40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.8.8.8.14.6.3\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.8.8.8.14.6.3.1\">1250.07</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.8.8.8.15.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T6.8.8.8.15.7.1\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\">Sequential</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.8.8.8.15.7.2\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\">40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.8.8.8.15.7.3\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\">703.60</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.3.3.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T6.3.3.3.3.2\" rowspan=\"2\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\"><span class=\"ltx_text\" id=\"S4.T6.3.3.3.3.2.1\">Spatial</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.3.3.3.3.1\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.3.3.3.3.3\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\">1180.23</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.4.4.4.4\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.4.4.4.1\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.4.4.4.2\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\">1142.32</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.8.8.8.16.8\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"3\" id=\"S4.T6.8.8.8.16.8.1\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S4.T6.8.8.8.16.8.1.1\">Task: POPE</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.8.8.8.17.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T6.8.8.8.17.9.1\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\">LLaVA-PruMerge</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.8.8.8.17.9.2\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\">35</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.8.8.8.17.9.3\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.8.8.8.17.9.3.1\">76.2</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.8.8.8.18.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T6.8.8.8.18.10.1\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\">Sequential</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.8.8.8.18.10.2\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\">35</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.8.8.8.18.10.3\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\">11.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.5.5.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T6.5.5.5.5.2\" rowspan=\"3\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\"><span class=\"ltx_text\" id=\"S4.T6.5.5.5.5.2.1\">Spatial</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.5.5.5.5.1\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.5.5.5.5.3\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\">69.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.6.6.6.6\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.6.6.6.6.1\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.6.6.6.6.2\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\">71.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.7.7.7.7\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.7.7.7.7.1\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.7.7.7.7.2\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\">67.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.8.8.8.19.11\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"3\" id=\"S4.T6.8.8.8.19.11.1\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S4.T6.8.8.8.19.11.1.1\">Task: SQA<sup class=\"ltx_sup\" id=\"S4.T6.8.8.8.19.11.1.1.1\">I</sup></span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.8.8.8.20.12\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T6.8.8.8.20.12.1\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\">LLaVA-PruMerge</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.8.8.8.20.12.2\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\">16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.8.8.8.20.12.3\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.8.8.8.20.12.3.1\">68.07</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.8.8.8.21.13\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T6.8.8.8.21.13.1\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\">Sequential</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.8.8.8.21.13.2\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\">16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.8.8.8.21.13.3\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\">64.20</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.8.8.8.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T6.8.8.8.8.2\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\"><span class=\"ltx_text\" id=\"S4.T6.8.8.8.8.2.1\">Spatial</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T6.8.8.8.8.1\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T6.8.8.8.8.3\" style=\"padding-top:-0.25pt;padding-bottom:-0.25pt;\">66.29</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>\n</div>\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<figure class=\"ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle\" id=\"S4.T6.fig1\" style=\"width:247.2pt;\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"S4.T6.fig1.1.1.1\" style=\"font-size:90%;\">Table 5</span>: </span><span class=\"ltx_text\" id=\"S4.T6.fig1.2.2\" style=\"font-size:90%;\">Ablation Studies for Adaptive Important Token Selection\u00a0(AITS, Sec.\u00a0<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.15388v5#S3.SS2\" title=\"3.2 Adaptive Important Token Selection via Outlier Detection \u2023 3 Method: Token Pru-Merging \u2023 LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>) and Token Supplement\u00a0(TS, Sec.\u00a0<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.15388v5#S3.SS3\" title=\"3.3 Token Supplement via Similar Key Clustering \u2023 3 Method: Token Pru-Merging \u2023 LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a>). With these modules, the downstream performance can be progressively improved.</span></figcaption><div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<div class=\"ltx_inline-block ltx_figure_panel ltx_transformed_outer\" id=\"S4.T6.fig1.3\" style=\"width:242.9pt;height:50.4pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-52.1pt,10.8pt) scale(0.7,0.7) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T6.fig1.3.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T6.fig1.3.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S4.T6.fig1.3.1.1.1.1\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">Method</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" id=\"S4.T6.fig1.3.1.1.1.2\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">LLM</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T6.fig1.3.1.1.1.3\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">SQA<sup class=\"ltx_sup\" id=\"S4.T6.fig1.3.1.1.1.3.1\">I</sup>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T6.fig1.3.1.1.1.4\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">VQA<sup class=\"ltx_sup\" id=\"S4.T6.fig1.3.1.1.1.4.1\">T</sup>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T6.fig1.3.1.1.1.5\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">POPE</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T6.fig1.3.1.1.1.6\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">MME</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.fig1.3.1.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" id=\"S4.T6.fig1.3.1.2.2.1\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">LLaVA-1.5</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T6.fig1.3.1.2.2.2\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">Vicuna-7B</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T6.fig1.3.1.2.2.3\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">66.8</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T6.fig1.3.1.2.2.4\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">58.2</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T6.fig1.3.1.2.2.5\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">85.9</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T6.fig1.3.1.2.2.6\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">1510.7</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T6.fig1.3.1.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T6.fig1.3.1.3.1.1\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">LLaVA-1.5 w. AITS</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T6.fig1.3.1.3.1.2\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">Vicuna-7B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.fig1.3.1.3.1.3\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">66.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.fig1.3.1.3.1.4\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">54.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.fig1.3.1.3.1.5\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">75.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.fig1.3.1.3.1.6\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">1221.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.fig1.3.1.4.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S4.T6.fig1.3.1.4.2.1\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">LLaVA-1.5 w. AITS &amp; TS</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S4.T6.fig1.3.1.4.2.2\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">Vicuna-7B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T6.fig1.3.1.4.2.3\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.fig1.3.1.4.2.3.1\">68.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T6.fig1.3.1.4.2.4\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.fig1.3.1.4.2.4.1\">56.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T6.fig1.3.1.4.2.5\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.fig1.3.1.4.2.5.1\">76.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T6.fig1.3.1.4.2.6\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.fig1.3.1.4.2.6.1\">1350.3</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_figure\">Table 6: </span>Ablation on training free and fine-tuning for our approach. With fine-tuning, the performance of LLaVA-PruMerge can be further enhanced.</figcaption><div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<div class=\"ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer\" id=\"S4.T6.fig1.6\" style=\"width:242.8pt;height:50.4pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-52.0pt,10.8pt) scale(0.7,0.7) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T6.fig1.6.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T6.fig1.6.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S4.T6.fig1.6.1.1.1.1\" style=\"padding-top:1.15pt;padding-bottom:1.15pt;\"><span class=\"ltx_text\" id=\"S4.T6.fig1.6.1.1.1.1.1\" style=\"font-size:90%;\">Method</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" id=\"S4.T6.fig1.6.1.1.1.2\" style=\"padding-top:1.15pt;padding-bottom:1.15pt;\"><span class=\"ltx_text\" id=\"S4.T6.fig1.6.1.1.1.2.1\" style=\"font-size:90%;\">LLM</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T6.fig1.6.1.1.1.3\" style=\"padding-top:1.15pt;padding-bottom:1.15pt;\">\n<span class=\"ltx_text\" id=\"S4.T6.fig1.6.1.1.1.3.1\" style=\"font-size:90%;\">SQA</span><sup class=\"ltx_sup\" id=\"S4.T6.fig1.6.1.1.1.3.2\"><span class=\"ltx_text\" id=\"S4.T6.fig1.6.1.1.1.3.2.1\" style=\"font-size:90%;\">I</span></sup>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T6.fig1.6.1.1.1.4\" style=\"padding-top:1.15pt;padding-bottom:1.15pt;\">\n<span class=\"ltx_text\" id=\"S4.T6.fig1.6.1.1.1.4.1\" style=\"font-size:90%;\">VQA</span><sup class=\"ltx_sup\" id=\"S4.T6.fig1.6.1.1.1.4.2\"><span class=\"ltx_text\" id=\"S4.T6.fig1.6.1.1.1.4.2.1\" style=\"font-size:90%;\">T</span></sup>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T6.fig1.6.1.1.1.5\" style=\"padding-top:1.15pt;padding-bottom:1.15pt;\"><span class=\"ltx_text\" id=\"S4.T6.fig1.6.1.1.1.5.1\" style=\"font-size:90%;\">POPE</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T6.fig1.6.1.1.1.6\" style=\"padding-top:1.15pt;padding-bottom:1.15pt;\"><span class=\"ltx_text\" id=\"S4.T6.fig1.6.1.1.1.6.1\" style=\"font-size:90%;\">MME</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.fig1.6.1.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" id=\"S4.T6.fig1.6.1.2.2.1\" style=\"padding-top:1.15pt;padding-bottom:1.15pt;\"><span class=\"ltx_text\" id=\"S4.T6.fig1.6.1.2.2.1.1\" style=\"font-size:90%;\">LLaVA-1.5</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T6.fig1.6.1.2.2.2\" style=\"padding-top:1.15pt;padding-bottom:1.15pt;\"><span class=\"ltx_text\" id=\"S4.T6.fig1.6.1.2.2.2.1\" style=\"font-size:90%;\">Vicuna-7B</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T6.fig1.6.1.2.2.3\" style=\"padding-top:1.15pt;padding-bottom:1.15pt;\"><span class=\"ltx_text\" id=\"S4.T6.fig1.6.1.2.2.3.1\" style=\"font-size:90%;\">66.8</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T6.fig1.6.1.2.2.4\" style=\"padding-top:1.15pt;padding-bottom:1.15pt;\"><span class=\"ltx_text\" id=\"S4.T6.fig1.6.1.2.2.4.1\" style=\"font-size:90%;\">58.2</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T6.fig1.6.1.2.2.5\" style=\"padding-top:1.15pt;padding-bottom:1.15pt;\"><span class=\"ltx_text\" id=\"S4.T6.fig1.6.1.2.2.5.1\" style=\"font-size:90%;\">85.9</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T6.fig1.6.1.2.2.6\" style=\"padding-top:1.15pt;padding-bottom:1.15pt;\"><span class=\"ltx_text\" id=\"S4.T6.fig1.6.1.2.2.6.1\" style=\"font-size:90%;\">1510.7</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T6.fig1.6.1.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T6.fig1.6.1.3.1.1\" style=\"padding-top:1.15pt;padding-bottom:1.15pt;\"><span class=\"ltx_text\" id=\"S4.T6.fig1.6.1.3.1.1.1\" style=\"font-size:90%;\">LLaVA-PruMerge w.o. LoRA-FT</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T6.fig1.6.1.3.1.2\" style=\"padding-top:1.15pt;padding-bottom:1.15pt;\"><span class=\"ltx_text\" id=\"S4.T6.fig1.6.1.3.1.2.1\" style=\"font-size:90%;\">Vicuna-7B</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.fig1.6.1.3.1.3\" style=\"padding-top:1.15pt;padding-bottom:1.15pt;\"><span class=\"ltx_text\" id=\"S4.T6.fig1.6.1.3.1.3.1\" style=\"font-size:90%;\">68.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.fig1.6.1.3.1.4\" style=\"padding-top:1.15pt;padding-bottom:1.15pt;\"><span class=\"ltx_text\" id=\"S4.T6.fig1.6.1.3.1.4.1\" style=\"font-size:90%;\">54.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.fig1.6.1.3.1.5\" style=\"padding-top:1.15pt;padding-bottom:1.15pt;\"><span class=\"ltx_text\" id=\"S4.T6.fig1.6.1.3.1.5.1\" style=\"font-size:90%;\">76.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.fig1.6.1.3.1.6\" style=\"padding-top:1.15pt;padding-bottom:1.15pt;\"><span class=\"ltx_text\" id=\"S4.T6.fig1.6.1.3.1.6.1\" style=\"font-size:90%;\">1250.1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.fig1.6.1.4.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S4.T6.fig1.6.1.4.2.1\" style=\"padding-top:1.15pt;padding-bottom:1.15pt;\"><span class=\"ltx_text\" id=\"S4.T6.fig1.6.1.4.2.1.1\" style=\"font-size:90%;\">LLaVA-PruMerge w. LoRA-FT</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S4.T6.fig1.6.1.4.2.2\" style=\"padding-top:1.15pt;padding-bottom:1.15pt;\"><span class=\"ltx_text\" id=\"S4.T6.fig1.6.1.4.2.2.1\" style=\"font-size:90%;\">Vicuna-7B</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T6.fig1.6.1.4.2.3\" style=\"padding-top:1.15pt;padding-bottom:1.15pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.fig1.6.1.4.2.3.1\" style=\"font-size:90%;\">68.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T6.fig1.6.1.4.2.4\" style=\"padding-top:1.15pt;padding-bottom:1.15pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.fig1.6.1.4.2.4.1\" style=\"font-size:90%;\">56.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T6.fig1.6.1.4.2.5\" style=\"padding-top:1.15pt;padding-bottom:1.15pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.fig1.6.1.4.2.5.1\" style=\"font-size:90%;\">76.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T6.fig1.6.1.4.2.6\" style=\"padding-top:1.15pt;padding-bottom:1.15pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.fig1.6.1.4.2.6.1\" style=\"font-size:90%;\">1350.3</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</div>\n</div>\n</figure>\n</div>\n</div>\n</figure>",
            "capture": "Table 4: Token Sampling Strategy Analysis on Different Tasks.\n"
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.15388v5_figure_1.png",
            "caption": "(a) Main idea of PurMerge."
        },
        "2": {
            "figure_path": "2403.15388v5_figure_2.png",
            "caption": "(b) PruMerged Token Visualization."
        },
        "3": {
            "figure_path": "2403.15388v5_figure_3.png",
            "caption": "Figure 2: The conceptual idea of PruMerge. Our approach has 3 steps: (1) Sample important tokens according to the similarities between the class tokens and spatial visual tokens via an outlier detection algorithm (see Sec.3.2); (2) Cluster the visual tokens via k\ud835\udc58kitalic_k-nearest neighbor; and (3) Adjust the sampled visual tokens via weighted averaging for each cluster (see Sec.3.3). Here m\ud835\udc5amitalic_m denotes the visual token compression ratio."
        },
        "4": {
            "figure_path": "2403.15388v5_figure_4.png",
            "caption": "(a) Distribution of attention values."
        },
        "5": {
            "figure_path": "2403.15388v5_figure_5.png",
            "caption": "(b) Image & Tokens Visualizations"
        },
        "6": {
            "figure_path": "2403.15388v5_figure_6.png",
            "caption": "Figure 4: Different token sampling strategies."
        }
    },
    "references": [
        {
            "1": {
                "title": "Flamingo: a visual language model for few-shot learning.",
                "author": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al.",
                "venue": "NeurIPS, 35:23716\u201323736, 2022.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Layer normalization.",
                "author": "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.",
                "venue": "arXiv preprint arXiv:1607.06450, 2016.",
                "url": null
            }
        },
        {
            "3": {
                "title": "Token merging: Your ViT but faster.",
                "author": "Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman.",
                "venue": "In International Conference on Learning Representations, 2023.",
                "url": null
            }
        },
        {
            "4": {
                "title": "Outlier detection: Methods, models, and classification.",
                "author": "Azzedine Boukerche, Lining Zheng, and Omar Alfandi.",
                "venue": "ACM Computing Surveys (CSUR), 2020.",
                "url": null
            }
        },
        {
            "5": {
                "title": "Making large multimodal models understand arbitrary visual prompts.",
                "author": "Mu Cai, Haotian Liu, Siva Karthik Mustikovela, Gregory P. Meyer, Yuning Chai, Dennis Park, and Yong Jae Lee.",
                "venue": "In IEEE Conference on Computer Vision and Pattern Recognition, 2024.",
                "url": null
            }
        },
        {
            "6": {
                "title": "Shikra: Unleashing multimodal llm\u2019s referential dialogue magic.",
                "author": "Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao.",
                "venue": "arXiv preprint arXiv:2306.15195, 2023.",
                "url": null
            }
        },
        {
            "7": {
                "title": "Mobilevlm: A fast, reproducible and strong vision language assistant for mobile devices.",
                "author": "Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang, Xiaolin Wei, et al.",
                "venue": "arXiv preprint arXiv:2312.16886, 2023.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Mobilevlm v2: Faster and stronger baseline for vision language model.",
                "author": "Xiangxiang Chu, Limeng Qiao, Xinyu Zhang, Shuang Xu, Fei Wei, Yang Yang, Xiaofei Sun, Yiming Hu, Xinyang Lin, Bo Zhang, et al.",
                "venue": "arXiv preprint arXiv:2402.03766, 2024.",
                "url": null
            }
        },
        {
            "9": {
                "title": "Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023.",
                "author": "Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi.",
                "venue": null,
                "url": null
            }
        },
        {
            "10": {
                "title": "Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale.",
                "author": "Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.",
                "venue": "Advances in Neural Information Processing Systems, 35:30318\u201330332, 2022.",
                "url": null
            }
        },
        {
            "11": {
                "title": "An image is worth 16x16 words: Transformers for image recognition at scale.",
                "author": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.",
                "venue": "arXiv preprint arXiv:2010.11929, 2020.",
                "url": null
            }
        },
        {
            "12": {
                "title": "Mme: A comprehensive evaluation benchmark for multimodal large language models.",
                "author": "Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, et al.",
                "venue": "arXiv preprint arXiv:2306.13394, 2023.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering.",
                "author": "Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.",
                "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.  6904\u20136913, 2017.",
                "url": null
            }
        },
        {
            "14": {
                "title": "A survey on vision transformer.",
                "author": "Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, et al.",
                "venue": "TPAMI, 2022.",
                "url": null
            }
        },
        {
            "15": {
                "title": "3d-llm: Injecting the 3d world into large language models.",
                "author": "Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan.",
                "venue": "NeurIPS, 2023.",
                "url": null
            }
        },
        {
            "16": {
                "title": "LoRA: Low-rank adaptation of large language models.",
                "author": "Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.",
                "venue": "In International Conference on Learning Representations, 2022.",
                "url": null
            }
        },
        {
            "17": {
                "title": "Phi-2: The surprising power of small language models.",
                "author": "Mojan Javaheripi, S\u00e9bastien Bubeck, Marah Abdin, Jyoti Aneja, Sebastien Bubeck, Caio C\u00e9sar Teodoro Mendes, Weizhu Chen, Allie Del Giorno, Ronen Eldan, Sivakanth Gopi, et al.",
                "venue": "Microsoft Research Blog, 2023.",
                "url": null
            }
        },
        {
            "18": {
                "title": "Mistral 7b.",
                "author": "Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al.",
                "venue": "arXiv preprint arXiv:2310.06825, 2023.",
                "url": null
            }
        },
        {
            "19": {
                "title": "Reformer: The efficient transformer.",
                "author": "Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.",
                "venue": "In International Conference on Learning Representations, 2020.",
                "url": null
            }
        },
        {
            "20": {
                "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.",
                "author": "Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi.",
                "venue": "In International Conference on Machine Learning, 2023a.",
                "url": null
            }
        },
        {
            "21": {
                "title": "Evaluating object hallucination in large vision-language models.",
                "author": "Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen.",
                "venue": "arXiv preprint arXiv:2305.10355, 2023b.",
                "url": null
            }
        },
        {
            "22": {
                "title": "Not all patches are what you need: Expediting vision transformers via token reorganizations.",
                "author": "Youwei Liang, Chongjian Ge, Zhan Tong, Yibing Song, Jue Wang, and Pengtao Xie.",
                "venue": "arXiv preprint arXiv:2202.07800, 2022.",
                "url": null
            }
        },
        {
            "23": {
                "title": "Video-llava: Learning united visual representation by alignment before projection.",
                "author": "Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan.",
                "venue": "arXiv preprint arXiv:2311.10122, 2023.",
                "url": null
            }
        },
        {
            "24": {
                "title": "Moe-llava: Mixture of experts for large vision-language models.",
                "author": "Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Junwu Zhang, Munan Ning, and Li Yuan.",
                "venue": "arXiv preprint arXiv:2401.15947, 2024.",
                "url": null
            }
        },
        {
            "25": {
                "title": "Improved baselines with visual instruction tuning, 2023a.",
                "author": "Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.",
                "venue": null,
                "url": null
            }
        },
        {
            "26": {
                "title": "Visual instruction tuning.",
                "author": "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.",
                "venue": "arXiv:2304.08485, 2023b.",
                "url": null
            }
        },
        {
            "27": {
                "title": "Llava-next: Improved reasoning, ocr, and world knowledge.",
                "author": "Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee.",
                "venue": "2024.",
                "url": null
            }
        },
        {
            "28": {
                "title": "Adaptive sparse vit: Towards learnable adaptive token pruning by fully exploiting self-attention.",
                "author": "Xiangcheng Liu, Tianyi Wu, and Guodong Guo.",
                "venue": "arXiv preprint arXiv:2209.13802, 2022.",
                "url": null
            }
        },
        {
            "29": {
                "title": "Mmbench: Is your multi-modal model an all-around player?",
                "author": "Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al.",
                "venue": "arXiv preprint arXiv:2307.06281, 2023c.",
                "url": null
            }
        },
        {
            "30": {
                "title": "Learn to explain: Multimodal reasoning via thought chains for science question answering.",
                "author": "Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan.",
                "venue": "Advances in Neural Information Processing Systems, 2022.",
                "url": null
            }
        },
        {
            "31": {
                "title": "Gpt-4v(ision) system card.",
                "author": "OpenAI.",
                "venue": "https://cdn.openai.com/papers/GPTV_System_Card.pdf, 2023a.",
                "url": null
            }
        },
        {
            "32": {
                "title": "Gpt-4 technical report.",
                "author": "OpenAI.",
                "venue": "2023b.",
                "url": null
            }
        },
        {
            "33": {
                "title": "Kosmos-2: Grounding multimodal large language models to the world.",
                "author": "Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei.",
                "venue": "arXiv preprint arXiv:2306.14824, 2023.",
                "url": null
            }
        },
        {
            "34": {
                "title": "Learning transferable visual models from natural language supervision.",
                "author": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.",
                "venue": "In International conference on machine learning, pp.  8748\u20138763. PMLR, 2021.",
                "url": null
            }
        },
        {
            "35": {
                "title": "Pb-llm: Partially binarized large language models.",
                "author": "Yuzhang Shang, Zhihang Yuan, and Zhen Dong.",
                "venue": "In ICLR, 2024.",
                "url": null
            }
        },
        {
            "36": {
                "title": "Towards vqa models that can read.",
                "author": "Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach.",
                "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp.  8317\u20138326, 2019.",
                "url": null
            }
        },
        {
            "37": {
                "title": "Efficient transformers: A survey.",
                "author": "Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler.",
                "venue": "ACM Computing Surveys, 2022.",
                "url": null
            }
        },
        {
            "38": {
                "title": "Gemini: a family of highly capable multimodal models.",
                "author": "Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al.",
                "venue": "arXiv preprint arXiv:2312.11805, 2023.",
                "url": null
            }
        },
        {
            "39": {
                "title": "Llama: Open and efficient foundation language models.",
                "author": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al.",
                "venue": "arXiv preprint arXiv:2302.13971, 2023.",
                "url": null
            }
        },
        {
            "40": {
                "title": "Attention is all you need.",
                "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin.",
                "venue": "In Advances in Neural Information Processing Systems, pp.  5998\u20136008, 2017.",
                "url": null
            }
        },
        {
            "41": {
                "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.",
                "author": "Vicuna.",
                "venue": "https://vicuna.lmsys.org/, 2023.",
                "url": null
            }
        },
        {
            "42": {
                "title": "Linformer: Self-attention with linear complexity, 2020.",
                "author": "Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma.",
                "venue": null,
                "url": null
            }
        },
        {
            "43": {
                "title": "Small language model meets with reinforced vision vocabulary.",
                "author": "Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng Ge, En Yu, Jianjian Sun, Chunrui Han, and Xiangyu Zhang.",
                "venue": "arXiv preprint arXiv:2401.12503, 2024.",
                "url": null
            }
        },
        {
            "44": {
                "title": "A-vit: Adaptive tokens for efficient vision transformer.",
                "author": "Hongxu Yin, Arash Vahdat, Jose M Alvarez, Arun Mallya, Jan Kautz, and Pavlo Molchanov.",
                "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.  10809\u201310818, 2022.",
                "url": null
            }
        },
        {
            "45": {
                "title": "A survey on multimodal large language models.",
                "author": "Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen.",
                "venue": "arXiv preprint arXiv:2306.13549, 2023.",
                "url": null
            }
        },
        {
            "46": {
                "title": "Tinygpt-v: Efficient multimodal large language model via small backbones.",
                "author": "Zhengqing Yuan, Zhaoxu Li, and Lichao Sun.",
                "venue": "arXiv preprint arXiv:2312.16862, 2023a.",
                "url": null
            }
        },
        {
            "47": {
                "title": "Asvd: Activation-aware singular value decomposition for compressing large language models.",
                "author": "Zhihang Yuan, Yuzhang Shang, Yue Song, Qiang Wu, Yan Yan, and Guangyu Sun.",
                "venue": "arXiv preprint arXiv:2312.05821, 2023b.",
                "url": null
            }
        },
        {
            "48": {
                "title": "Llm inference unveiled: Survey and roofline model insights.",
                "author": "Zhihang Yuan, Yuzhang Shang, Yang Zhou, Zhen Dong, Chenhao Xue, Bingzhe Wu, Zhikai Li, Qingyi Gu, Yong Jae Lee, Yan Yan, et al.",
                "venue": "arXiv preprint arXiv:2402.16363, 2024.",
                "url": null
            }
        },
        {
            "49": {
                "title": "Mm-llms: Recent advances in multimodal large language models.",
                "author": "Duzhen Zhang, Yahan Yu, Chenxing Li, Jiahua Dong, Dan Su, Chenhui Chu, and Dong Yu.",
                "venue": "arXiv preprint arXiv:2401.13601, 2024.",
                "url": null
            }
        },
        {
            "50": {
                "title": "Video-llama: An instruction-tuned audio-visual language model for video understanding.",
                "author": "Hang Zhang, Xin Li, and Lidong Bing.",
                "venue": "arXiv preprint arXiv:2306.02858, 2023a.",
                "url": null
            }
        },
        {
            "51": {
                "title": "Gpt4roi: Instruction tuning large language model on region-of-interest.",
                "author": "Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen, and Ping Luo.",
                "venue": "arXiv preprint arXiv:2307.03601, 2023b.",
                "url": null
            }
        },
        {
            "52": {
                "title": "Tinyllava: A framework of small-scale large multimodal models.",
                "author": "Baichuan Zhou, Ying Hu, Xi Weng, Junlong Jia, Jie Luo, Xien Liu, Ji Wu, and Lei Huang.",
                "venue": "arXiv preprint arXiv:2402.14289, 2024.",
                "url": null
            }
        },
        {
            "53": {
                "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models.",
                "author": "Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.",
                "venue": "arXiv preprint arXiv:2304.10592, 2023.",
                "url": null
            }
        },
        {
            "54": {
                "title": "Llava-phi: Efficient multi-modal assistant with small language model.",
                "author": "Yichen Zhu, Minjie Zhu, Ning Liu, Zhicai Ou, Xiaofeng Mou, and Jian Tang.",
                "venue": "arXiv preprint arXiv:2401.02330, 2024.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.15388v5",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.4",
            "3.5"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4",
            "4.4.1",
            "4.4.2",
            "4.4.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.4",
            "4.4.1",
            "4.4.2",
            "4.4.3"
        ]
    },
    "research_context": {
        "paper_id": "2403.15388v5",
        "paper_title": "LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models",
        "research_background": "### Paper's Motivation:\nThe motivation behind this paper is to address the significant computation cost associated with inference in Large Multimodal Models (LMMs). The primary computational bottleneck lies in the Large Language Models (LLMs) that these LMMs incorporate. The authors aim to reduce this inference cost without sacrificing the LLMs' performance in visual question-answering and reasoning tasks. They highlight that previous approaches, such as replacing the LLM backbone with smaller models or applying quantization, have led to a performance gap. Hence, the motivation is to find a way to maintain performance while reducing computation during inference.\n\n### Research Problem:\nThe research problem is to determine whether the number of prefix visual tokens in LMMs can be reduced while maintaining comparable performance. Specifically, the authors aim to identify if redundant visual tokens can be pruned and if key visual tokens can be adaptively selected to minimize computational cost without noticeable performance degradation.\n\n### Relevant Prior Work:\n1. **Large Language Models (LLMs)**:\n   - OpenAI (2023b), Team et al. (2023), Jiang et al. (2023), Touvron et al. (2023) have established the strong reasoning abilities of LLMs.\n   - LLMs such as LLaMA and Vicuna are recognized for their large parameter sizes and computational intensity.\n   - Transformer architecture by Vaswani et al. (2017) underpins these models.\n\n2. **Visual Encoders in LMMs**:\n   - CLIP-ViT by Radford et al. (2021) is commonly used for embedding image patches into visual tokens.\n\n3. **Token Pruning and Optimization**:\n   - Prior works by Bolya et al. (2023), and Liu et al. (2022) have found redundancies in visual tokens and explored similar pruning approaches.\n   - Tools like IQR scoring function in outlier detection (Boukerche et al. 2020) provide a basis for selecting important visual tokens.\n   - Token merging techniques such as -nearest neighbor and weighted averaging have been suggested for enhancing performance.\n\n4. **Previous Optimization Attempts**:\n   - Replacing LLM backbone with smaller models, e.g., Phi-2 by Javaheripi et al. (2023), though they produce a performance gap in tasks like VQAv2 and MM-Bench.\n   - Quantization techniques mentioned by Liu et al. (2023b) and Yuan et al. (2024).\n\nThe paper proposes leveraging adaptive token reduction methods to prune and merge visual tokens, enhancing the efficiency of LMMs without compromising their performance.",
        "methodology": "### Methodology\n\nIn this section, we first review the basic implementation of large multimodal models (LMMs), with a particular focus on the visual encoder component, specifically the Vision Transformer. We highlight the direct correlation between the number of visual tokens and the efficiency of LMMs.\n\nNext, we present a plug-and-play token reduction method specifically designed for LMMs, called token PruMerge. Our method features two key components: \n\n1. **Adaptive Important Token Selection (AITS) via Outlier Detection:** AITS adaptively determines the optimal number of visual tokens to retain based on the unique characteristics of the image. This involves identifying outliers in the visual token set that capture the most critical information, thereby ensuring that only the most essential tokens are selected.\n   \n2. **Token Supplement (TS) via Similar Key Clustering:** TS facilitates efficient processing without compromising the model\u2019s performance by maintaining the integrity and richness of the visual information. This process involves clustering similar tokens and providing supplementary information to ensure that the reduced set of tokens still accurately represents the content of the original image.\n\nBy integrating these components, our token PruMerge method effectively adapts to the specific needs of each image, optimizing the balance between model efficiency and performance.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n**Experiment Setup:**\n\n1. **Datasets:** To evaluate the overall performance of the proposed PruMerge approach when applied to LLaVA-1.5, the main experiment utilizes standard large multimodal datasets. These datasets typically include a combination of text and image inputs, requiring the model to handle and integrate information from different modalities effectively.\n  \n2. **Baselines:** The main experiment compares the performance of the proposed method, PruMerge, against several baseline models. The primary baseline is the original LLaVA-1.5 model without the PruMerge technique. Additional baselines might include other state-of-the-art large multimodal models to provide a comprehensive comparison.\n\n3. **Evaluation Metrics:** The effectiveness of the models is evaluated using several key metrics:\n   \n   - **Accuracy:** Measures how well the model performs on classification or decision-making tasks.\n   - **Inference Speed:** Assesses the efficiency of the model in processing inputs and generating outputs, crucial for applications needing real-time performance.\n   - **Model Size and Computational Cost:** Evaluates the resource efficiency, including the number of parameters and the computational power required.\n\n**Main Experimental Results:**\n\nThe results from the main experiment can be summarized as follows:\n\n1. **Performance Improvement:** The proposed PruMerge approach demonstrates a significant improvement in performance when applied to LLaVA-1.5. This is shown through enhancements in accuracy across various tasks compared to the baseline LLaVA-1.5 model.\n   \n2. **Efficiency Gains:** In addition to performance improvements, PruMerge also substantially increases the efficiency of LLaVA-1.5. The model achieves faster inference speeds while maintaining or even enhancing accuracy, making it more suitable for real-time applications.\n   \n3. **Resource Utilization:** The PruMerge approach effectively reduces the computational cost and model size without sacrificing performance. This results in a more resource-efficient model that can be deployed on devices with limited computational capabilities.\n\nOverall, the main experiment demonstrates that the PruMerge approach not only enhances the performance of large multimodal models like LLaVA-1.5 but also makes them more efficient and scalable."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To compare the performance of the proposed visual token sampling strategy (LLaVA-PruMerge) with vanilla visual token sampling methods, including sequential and spatial sampling.",
            "experiment_process": "Three approaches are compared: 1) LLaVA-PruMerge, which dynamically samples key visual tokens resulting in an average of 40 visual tokens per image for TextVQA/MME, 35 tokens for POPE, and 16 tokens for SQA; 2) Sequential sampling, where the first 40 tokens in the flattened visual tokens are sampled for a direct comparison; 3) Spatial sampling, where tokens are evenly distributed across the image in various configurations, such as 6x6 (36 tokens), 5x8 (40 tokens), among others. All experiments are conducted in a training-free manner and evaluated based on downstream task performance.",
            "result_discussion": "The LLaVA-PruMerge approach consistently outperforms both sequential sampling and spatial sampling across all tasks, notably excelling in TextVQA, an OCR task that requires detailed text information. This indicates that LLaVA-PruMerge is effective at extracting key information with enough detail.",
            "ablation_id": "2403.15388v5.No1"
        },
        {
            "research_objective": "To determine the effectiveness of each module in the PruMerge design.",
            "experiment_process": "Using the LLaVA-1.5 model, the study progressively adds the proposed modules (Adaptive Important Token Selection - AITS, and Token Supplement - TS) while maintaining the same amount of visual tokens (6.9%, 40 tokens) across all settings. The performance is then evaluated based on downstream tasks.",
            "result_discussion": "Adding the proposed modules (AITS and TS) progressively enhances downstream task performance, demonstrating the effectiveness of each component in the PruMerge design.",
            "ablation_id": "2403.15388v5.No2"
        },
        {
            "research_objective": "To analyze the impact of training methods (training-free vs. fine-tuning) on the performance of LLaVA-PruMerge.",
            "experiment_process": "The LLaVA-PruMerge approach is evaluated under two conditions: training-free and fine-tuning. In the fine-tuning scenario, the large language model is adapted to the new structure of visual tokens. The performance is then measured across various tasks including ScienceQA, TextVQA, POPE, and MME, and compared.",
            "result_discussion": "Fine-tuning the LLaVA-PruMerge approach results in improved performance across various vision-language tasks, indicating that the model benefits from adaptation to the new visual token structure.",
            "ablation_id": "2403.15388v5.No3"
        }
    ]
}