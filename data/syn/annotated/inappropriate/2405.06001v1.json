{
    "title": "LLM-QBench: A Benchmark Towards the Best Practice for Post-training Quantization of Large Language Models",
    "abstract": "Recent advancements in large language models (LLMs) are propelling us toward artificial general intelligence, thanks to their remarkable emergent abilities and reasoning capabilities. However, the substantial computational and memory requirements of LLMs limit their widespread adoption. Quantization, a key compression technique, offers a viable solution to mitigate these demands by compressing and accelerating LLMs, albeit with potential risks to model accuracy. Numerous studies have aimed to minimize the accuracy loss associated with quantization. However, the quantization configurations in these studies vary and may not be optimized for hardware compatibility. In this paper, we focus on identifying the most effective practices for quantizing LLMs, with the goal of balancing performance with computational efficiency. For a fair analysis, we develop a quantization toolkit LLMC, and design four crucial principles considering the inference efficiency, quantized accuracy, calibration cost, and modularization. By benchmarking on various models and datasets with over 500 experiments, three takeaways corresponding to calibration data, quantization algorithm, and quantization schemes are derived. Finally, a best practice of LLM PTQ pipeline is constructed. All the benchmark results and the toolkit can be found at https://github.com/ModelTC/llmc.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Recently, large Language models (LLMs) such as GPT-4 (OpenAI et al., 2024  ###reference_b34###) have demonstrated unprecedented generative capabilities in the field of natural language processing (NLP), and achieving widespread applications across various industries. However, their substantial computational and storage costs have impeded their further popularization among users. For instance, BLOOM (Touvron et al., 2023  ###reference_b41###), an open-access multilingual LLM with 176 billion parameters, requires a minimum of 350 GB of space merely to store model weights in full-precision (FP16) format. At a minimum, it requires 580GB A100 or 940GB A800 NVIDIA GPUs to perform inference with this model. Therefore, reducing their serving cost is paramount to further enhance the application of LLMs.\nFor the aforementioned challenge, model quantization (Nagel et al., 2021  ###reference_b32###) can be an effective resolution strategy. It maps weights and/or activations to a lower-bit data format to reduce memory footpoints and accelerate model inference. Existing quantization approaches can be categorized into two types: quantization-aware-training (QAT) (Bhalgat et al., 2020  ###reference_b4###; Gong et al., 2019  ###reference_b20###; Esser et al., 2020  ###reference_b16###; Egiazarian et al., 2024  ###reference_b15###; van Baalen et al., 2024  ###reference_b43###) and post-training quantization (PTQ) (Wei et al., 2023a  ###reference_b45###; Jhunjhunwala et al., 2021  ###reference_b23###; Li et al., 2021  ###reference_b26###). Although with prominent high performance, the necessity for QAT to undergo finetuning or retraining with substantial training data and training cost renders it unattainable for the majority of users. Correspondingly, PTQ compresses models without retraining, making it a preferred method for LLMs due to its minimal resource requirements. Therefore, considering the quantization cost, we do not mention some QAT methods (Du et al., 2024  ###reference_b14###; Liu et al., 2024  ###reference_b28###; 2023  ###reference_b29###) in our paper. On the other hand, quantization can also be classified into non-uniform (Kim et al., 2024  ###reference_b24###; Egiazarian et al., 2024  ###reference_b15###) and uniform quantization. We only benchmark the latter one, since non-uniform quantization needs complex specialized kernels. However, they always slow down inference speed. Besides these, we also notice some approaches (Chee et al., 2024  ###reference_b6###; Tseng et al., 2024  ###reference_b42###) with additional non-negligible computational overhead during inference. Despite their decent performance, we still ignore them in our research due to their unfriendliness towards inference.\nCurrent uniform PTQ methods always evaluate across distinct datasets in different quantization configurations and with simulated quantization. This current state would lead to users being unable to accurately assess the configurations that should be selected for the efficient and accurate quantization of LLMs. To provide a comprehensive quantization options menu for users to obtain hardware-friendly quantized LLMs with high performance, we make a fair benchmark, which considers two aspects: factors influencing LLM quantization and inference efficiency under our design principles. The former perspective encompassed three dimensions, e.g., calibration data, algorithm, and target bits. Consequently, we evaluate across various kinds of tasks and find our best practice, encapsulated within an end-to-end pipeline that realizes both high efficiency and accuracy LLM quantization. This best practice has been integrated into our quantization toolkit, LLMC. Notably, LLMC, a user-friendly, plug-and-play quantization tool, incorporates dozens of outstanding PTQ algorithms, provides the freedom to select quantization strategies, and also supports deploying quantized LLMs on different inference backends (TensorRT-LLM (Nvidia, 2023  ###reference_b33###), PPL-LLM (OpenPPL, 2023  ###reference_b35###), LightLLM (ModelTC, 2023  ###reference_b31###)) and hardware (Nvidia GPU, Qualcomm mobile chips, TPU). In a word, our main contributions can be described as follows:\nWe release a quantization toolkit LLMC supporting dozens of algorithms, models and hardware. LLMC enables users to perform lossless quantization on 100-billion-parameter LLMs within a matter of hours, utilizing just a single GPU. It notably facilitate the research and production of quantized LLMs.\nWe modularly and fairly benchmark the quantization techniques considering calibration cost, inference efficiency, quantized accuracy. Near 600 experiments on diverse models and datasets provide three insightful takeaways on the calibration data, algorithm pipeline and quantization configuration selection.\nBased on the takeaways, a best practice of LLM PTQ pipeline is designed, achieving the best accuracy and efficiency performance balance under various scenarios."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Benchmark Overview",
            "text": "In this section, we first provide our benchmark\u2019s design principles subsection 2.1  ###reference_###, outlining its primary objective. We then detail LLM quantization subsection 2.2  ###reference_###. In Section. subsection 2.2  ###reference_###, after introducing the preliminary of quantization, we overview our exploration in the benchmark, e.g, factors influencing LLM quantization and inference efficiency. Finally, we exhibit our plug-and-play quantization toolkit within our benchmark."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Design Principles",
            "text": "Our benchmark focuses on four essential aspects for effective and practical LLM quantization: inference performance, calibration cost, quantized accuracy, and modularization.\nInference Performance: In our LLM quantization benchmark, we prioritize the importance of selecting a quantization approach that enhances inference performance. This means our chosen setting should either increase throughput or decrease memory requirements, thereby optimizing the efficiency of the model during the inference phase.\nCalibration Cost:  The process of post-training quantization for LLMs are also named as calibration. The resources and time invested in calibration for LLM are crucial factors that affect the practicality of LLM quantization. This benchmark aims to find the best pipeline to produce accurate LLMs in minimal GPUs and time.\nQuantized Accuracy:  In every method used to create quantized models, it\u2019s crucial to minimize any reduction in accuracy to a tolerable degree. With this fundamental principle in mind, we are dedicated to exploring strategies that reliably preserve the performance of the model within acceptable limits.\nModularization:  Recent advancements have introduced a myriad of algorithms aimed at enhancing the performance of quantized LLMs. This benchmark seeks to dissect these algorithms to their most fundamental elements, analyzing the efficacy of each component in isolation.\nGuided by the aforementioned four principles, our goal is to investigate and outline optimal practices for developing quantized LLMs tailored to various scenarios and configurations."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "LLM Quantization",
            "text": "Preliminary of Quantization.  For an element  in a vector to be quantized, the process of quantization can be defined as:\nwhere  and  are the upper bound and the lower bound of the vector.  is the bit-width of the quantized vector and  is the quantized -bit element. if we force , the process can be called symmetric quantization. Otherwise, it is called asymmetric quantization. In this paper, we mainly consider asymmetric quantization. Besides that, in weight-only quantization, we employ per-group quantization, that is the weights in a group share the same . In weight-activation quantization, we apply per-channel and per-token quantization for weights and activations, respectively 111In this paper, the notion \u201cwxay\u201d is employed to represent the bit-widths \u201cx\u201d of weights, and the bit-widths \u201cy\u201d of activations. \u201cgz\u201d means in group-wize quantization the group size is \u201cz\u201d.. Details can be found in the subsection A.1  ###reference_###.\n###figure_1### Factors Influencing LLM Quantization.  We categorize factors influencing LLM quantization into three dimensions: calibration data, algorithms, and target bits.\nCalibration data: Calibration data can help to evaluate the range of tensors, and then determine the quantization parameters, which is crucial for maintaining model performance post-quantization. Based on that, the impact of different corpora as calibration data warrants further investigation.\nAlgorithm: Naive low-bit quantization always brings the accuracy drop for LLM, therefore, efficient remedies to help maintain model performance make a lot of sense. Current effective and efficient algorithms can be summarized into three types:\n1) Transformation (Xiao et al., 2023  ###reference_b47###; Lin et al., 2023  ###reference_b27###; Shao et al., 2023  ###reference_b39###; Wei et al., 2023b  ###reference_b46###): Leveraging magnitude between weight and activation before quantization is widely used to balance quantization errors:\n, where  denotes the balance factor. 2) Clipping (Lin et al., 2023  ###reference_b27###; Shao et al., 2023  ###reference_b39###; Wei et al., 2022  ###reference_b44###; Du et al., 2024  ###reference_b14###): Clipping some outliers with minimal impact in weights before quantization can help with range estimation and the representation of the rest in calibration:\n, where  and  mean clipping lower bound and upper bound, respectively.\n3) Reconstruction (Frantar et al., 2022  ###reference_b18###; Lee et al., 2023  ###reference_b25###; Dettmers et al., 2023  ###reference_b13###): This kind of approach employs the Hessian matrix to evaluate the quantization perturbations, and update the rest intact elements, which can be concisely represented as follows:\n, where  denotes the perturbation, and  is the inverse Hessian matrix. This process is conducted incrementally during the quantization process.\nTarget bits: The bit adopted for weight, activation, and KV cache impacts the final accuracy. Usually, the hardware-friendly bits are 2-bit, 4-bit and 8-bit. In this benchmark, we also investigate 3-bit or 6-bit to compare the potential of quantization algorithms. But for the practical deployment, 2/4/8-bit is mainly used.\nQuantized inference of LLM.  As shown in Figure 1  ###reference_###, the quantization mainly targets the Linear layers with weights, i.e., the Q, K, V, and O layers in self-attention modules and the Up, Gate, and Down layer in FFN modules. Figure 1  ###reference_###(b) presents 3 types of quantization including weight-activation quantization, weight-only quantization, and KV-cache quantization. They bring different benefits for reducing the prefill and decode latency."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Quantization Toolkit",
            "text": "To achieve the modular comparison of the different quantization dimensions aforementioned, and to consolidate best practices into an end-to-end pipeline, we have designed and developed a quantization toolkit named LLMC. This toolkit is capable of accommodating multiple quantization configurations using a variety of algorithmic techniques. The models produced by LLMC are designed for seamless deployment across a diverse range of hardware platforms. Presently, LLMC supports over ten algorithms, is compatible with over eight models, is flexible to extend the support of any transformer-based LLMs, and facilitates deployment on three types of inference engines including LightLLM (ModelTC, 2023  ###reference_b31###), TensorRT-LLM (Nvidia, 2023  ###reference_b33###) and PPL-LLM (OpenPPL, 2023  ###reference_b35###). Random rounding is applied to ensure the precision of quantized models is consistent across different scales."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "LLM-QBench",
            "text": "Under the principles in subsection 2.1  ###reference_###, powered by our quantization toolkit LLMC, in this section, we explore the best practice for quantizing large language models from the aspect of calibration data, quantization algorithm, and target bit."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Experimental Settings",
            "text": "We first illustrate our experiment settings, more details can be found in the subsection A.1  ###reference_###.\nModels.  To demonstrate the generability of our benchmark, We access performance on LLAMA-2 (Touvron et al., 2023  ###reference_b41###) family, spanning model sizes from 7B to 70B for general language tasks. To broaden the scope of our evaluation benchmarks, we also benchmark on ChatGLM (Zeng et al., 2023  ###reference_b50###) for long context abilities, CodeLLAMA (Roziere et al., 2023  ###reference_b38###) for coding tasks and WizardMath (Luo et al., 2023  ###reference_b30###) for mathematical problems.\nDatasets.  We categorize the datasets into upstream datasets and downstream datasets. For the upstream datasets, we employ WikiText2 (Foundation,  ###reference_b17###) and C4 (Raffel et al., 2019  ###reference_b37###) dataset with the perplexity metric for evaluation, since perplexity can stably reflect the LLM\u2019s perfomance (Dettmers & Zettlemoyer, 2023  ###reference_b11###). For the downstream tasks, we select examination tasks including MMLU (Hendrycks et al., 2021  ###reference_b21###) and ARC-e (Clark et al., 2018  ###reference_b9###), knowledge task BoolQ (Clark et al., 2019  ###reference_b8###), understanding task Lambada (Paperno et al., 2016  ###reference_b36###), reasoning tasks including PIQA (Bisk et al., 2020  ###reference_b5###), HellaSwag (Zellers et al., 2019  ###reference_b49###) and GSM8K (Cobbe et al., 2021  ###reference_b10###), coding tasks HumanEval (Chen et al., 2021  ###reference_b7###) and MBPP (Austin et al., 2021  ###reference_b2###), and the long context evaluation LongBench (Bai et al., 2023  ###reference_b3###).\nHardware.  Benefiting from the versatility of our tool, we can efficiently and conveniently quantize LLMs to support multiple inference backends and hardware platforms. In this paper, we mainly measured the inference efficiency of low-bit kernel on NVIDIA server and edge GPUs with NVIDIA\u2019s TensorRT-LLM (Nvidia, 2023  ###reference_b33###) framework."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Impact of Calibration Data",
            "text": "Initially, we examine the influence of calibration data on the accuracy of quantization, as illustrated by Table 1  ###reference_###. It is evident that calibration data affects all algorithms. To attain optimal accuracy, it is crucial to gather domain-specific data for domain-specific models and collect diverse data for the general models."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Quantization Algorithm",
            "text": "Following the principles of modularization, we deconstruct the techniques behind existing algorithms. Through a comprehensive and unbiased experimental comparison, we aim to derive insights critical for developing an optimally combined quantization pipeline.\nAs outlined in Table 2  ###reference_###, we summarize the different strategies transformation, clipping, and reconstruction techniques, define their behavior and analyze their calibration cost accordingly. We evaluate these techniques on LLAMA-2 models of 7B, 13B, and 70B sizes, under both weight-only and weight-activation quantization scenarios. Here the 2-bit weight-only experiment of 70B LLAMA-2 is chosen as a representative in the main text. More results are illustrated in subsection A.2  ###reference_###.\nClipping.  From the Table 3  ###reference_###, we find that searching for the clipping value asymmetrically is the most effective strategy for optimizing accuracy. This indicates that selecting an appropriate weight range can significantly reduce weight quantization error. Therefore, we also adopt this strategy for the weight-activation quantization. Clipping should be fully utilized in the pipeline of best practices. What\u2019s more, initialized from the asymmetric clipping, the accuracy can be boosted by further learning. This good initialization contributes to a fast convergence.\nReconstruction.  GPTQ (Frantar et al., 2022  ###reference_b18###) reconstruction involves the non-equivalent transformation of weights on the channel dimension, hindering simultaneous optimization of weights and clip values. Pre-reconstruction weight clip yields suboptimal results due to weight changes. If reconstruction precedes clip value search, initial quantization parameters won\u2019t match the updated ones. Moreover, when paired with an equivalent transformation, it yields minimal benefits. This limitation may stem from the alteration of gradients and the disruption of assumptions regarding Hessian information. Furthermore, it requires an extended calibration period. Therefore, reconstruction may not be considered a best practice.\nTransformation.  The transformation technique utilizes the linear operation to reduce the outlier problem in LLM or preserve the important weights. for both the weight-only and weight-activation quantization, such equivalent transformation brings an accuracy improvement, especially for the activations. From the table, we can infer that manually setting the scaling number is rigid and may not help in all scenarios. On the contrary, a suitable search for the transformation scale  is effective. There are different search strategies and both help a lot at improving the accuracy. A learning process can be further adopted with a pre-searched range. Fortunately, with the support of fast pre-search, the calibration can achieve learning with fewer epochs.\nCalibration cost for each strategy. In the analysis of calibration costs detailed in Table 4  ###reference_###, we observe that within the suite of transformation techniques, the search-based (v1) strategy requires roughly 10 minutes, making it twice as fast as the (v2) strategy. While rule-based transformations are quicker, they often fall short of achieving acceptable accuracy levels. On the other hand, learning-based transformation methods incur a considerable increase in time to attain satisfactory accuracy levels. However, initializing the learning process with pre-searched values can halve the number of epochs required and yield higher accuracy. Regarding clipping methods, employing direct min-max value clipping is time-efficient but typically results in significant accuracy loss. The search-based clipping method, whether using asymmetric or symmetric ranges, proves efficient, requiring only about 20 minutes. Yet, when applying a learning-based approach to clipping, the calibration time can extend to nearly 7 hours. Therefore, a combined approach of the search-based transformation v1 and search-based asymmetric clipping emerges as the most effective in balancing accuracy and efficiency. Furthermore, initiating with pre-searched values and conducting additional learning for a few epochs may offer further accuracy improvements.\nTR\nTS\nTS\nTL\nTL\nTL\nCS\nCL\nTS-v1\nTL\nTS-v1 + CL\nTL w/ TS-v1 init.\nRH\n\n\n\n-v1\n-v2\nw/ ones init.\nw/ TR init.\nw/ TS-v1 init.\n-asym\n\n+CS-asym\n+CL\nw/ CS-asym init.\n+CL w/ CS-asym init.\n\n\nTime\nh\nh\nh\nh\nh\nh\nh\nh\nh\nh\nh\nh\nh"
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Target Bits",
            "text": "Fixed-precision.  In the experimental results presented in subsection 3.3  ###reference_###, we observed that both 2-bit weight-only quantization and w4a4 weight-activation quantization experienced over a 20% degradation in accuracy. This significant reduction in performance limits their practical utility. In contrast, 3-bit weight-only and w6a6 weight-activation quantization were primarily evaluated to assess algorithm capabilities and cannot achieve practical hardware acceleration. Consequently, we recommend the 4-bit weight-only, w4a8, or w8a8 weight-activation quantization approaches as they strike a balance between maintaining accuracy and enhancing inference speed. Furthermore, quantization of the Key-Value (KV) cache is proposed as a method to decrease memory usage. In Table 21  ###reference_### and Table 5  ###reference_###, we assessed the accuracy impact of 2-bit (per-group quantization with a group size of 8), 4-bit (per-group quantization with a group size of 8), and 8-bit (per-tensor) KV cache quantization. The results indicate that 2-bit KV cache quantization leads to a substantial loss in accuracy, while 4-bit KV cache quantization, with its finer granularity, performs comparably to 8-bit KV cache quantization with a coarser group size. Both the 4-bit and 8-bit configurations closely approximate the performance of FP16 at the code generation task and long-context understanding task. Hence, for KV cache quantization, a 4-bit per-group approach with a group size of 8 is recommended.\nMixed-precision.  As presented in our experiments, quantizing LLMs into ultra-low precision without significant accuracy loss is difficult. A viable remedy is to employ mix-precision quantization. For mix-precision, we only evaluate accuracy for theoretically hardware-friendly strategies since there are no open-access fast kernels to evaluate inference. As shown in Table 23  ###reference_###, Table 23  ###reference_###, and Table 24  ###reference_###, for weight-only quantization, employing Hessain disturbance as bit allocate strategy outperforms others. High-bit quantization benefits from lower mixture rates, while low-bit requires more full-precision weights in small LLMs for better performance. For weight-activation quantization, dynamic bit allocation with slower inference speed and higher computational overhead during inference gains more accuracy improvements rather than static strategy, even though the latter uses a double mixture rate. Details are presented in the subsection A.6  ###reference_###.\nInference Speed.  To assess the practical benefits of different quantization approaches, we conducted evaluations using NVIDIA\u2019s cloud (SMX 80G A100) and edge (Drive Orin) GPUs, alongside the official inference library, TensorRT-LLM. Part of our results, as depicted in Figure 2  ###reference_###, highlight the throughput improvements achieved through TensorRT-LLM-supported quantization schemes for models with 32,000 input tokens and 512 output tokens.\nThe findings indicate that quantization with 8-bit weights and activations enhances the prefill stage\u2019s speed by 20%-30% and the decode stage by 40%-60%. In contrast, 4-bit weight-only quantization reduces the prefill speed by 10% but increases the decode speed by 40%-60%. It\u2019s important to note that these acceleration rates tend to diminish for larger models. Besides, 8-bit KV cache quantization has minimal impact on prefill times and slightly reduces decoding throughput for very large models, such as those with 70B model. Results for more models and hardware can be found in subsection A.5  ###reference_###.\n###figure_2### ###figure_3###"
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "Best Practice of LLM PTQ pipeline",
            "text": "Based on the takeaways distilled from the above exploration, we summarize the best practice of PTQ pipeline for LLM. As depicted in Figure 3  ###reference_###, first, we should collect the best calibration data according to the task and model under the guide of Takeway 1. Then the bit-width and quantization scheme could be determined considering the Takeway 3. Finally, the calibration process can be conducted using the algorithm pipeline based on Takeway 2. The results in Table 6  ###reference_### and Table 7  ###reference_### of general-purpose model LLAMA-2-70B and specific-domain code model CodeLLAMA-7b and math model WizardMath-7b proved the effectiveness, especially for maintaining high accuracy. More experimental results on other models and datasets to validate our best practice for decent performance and efficient inference can be found in subsection A.3  ###reference_###."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this study, we have undertaken a comprehensive benchmarking of decomposed quantization techniques for large language models (LLMs), leading to the identification of best practices that balance calibration costs, accuracy, and efficiency. Furthermore, we introduce LLMC, a toolkit designed to empower the research and development community. Models optimized through our recommended practices and toolkit are readily deployable across a variety of hardware platforms, enhancing accessibility and applicability in diverse computational environments."
        }
    ],
    "url": "http://arxiv.org/html/2405.06001v1",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "2.2",
            "2.3"
        ],
        "main_experiment_and_results_sections": [
            "3.1",
            "3.2",
            "3.3",
            "3.4",
            "3.5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.2",
            "3.3",
            "3.4"
        ]
    },
    "research_context": {
        "paper_id": "2405.06001v1",
        "paper_title": "LLM-QBench: A Benchmark Towards the Best Practice for Post-training Quantization of Large Language Models",
        "research_background": "### LLM-QBench: A Benchmark Towards the Best Practice for Post-training Quantization of Large Language Models\n\n**Motivation:**\n\nThe motivation for this paper stems from the expanding deployment of large language models (LLMs) in natural language processing tasks due to their remarkable generative capabilities. Despite their potential, the excessive computational and storage costs involved in utilizing LLMs pose a significant barrier to widespread adoption. For example, the BLOOM model, comprising 176 billion parameters, demands around 350 GB of storage in full-precision format and significantly more for inference on advanced GPUs. As a result, there is an urgent need to reduce the serving costs of LLMs to facilitate broader usage.\n\n**Research Problem:**\n\nThe primary research problem addressed by this paper is the challenge of optimizing the post-training quantization (PTQ) of LLMs to balance efficiency and accuracy. Existing quantization strategies, particularly quantization-aware-training (QAT), require substantial training data and computational resources, making them impractical for most users. PTQ, on the other hand, offers a promising alternative by compressing models without needing retraining, thus mitigating resource requirements. However, uniform PTQ methods often produce inconsistent results due to varying datasets and configurations. Therefore, the need arises for a comprehensive benchmark to guide users in selecting efficient and accurate quantization strategies for LLMs.\n\n**Relevant Prior Work:**\n\n1. **Quantization techniques:**\n   - Various quantization techniques reduce memory footprint and accelerate inference (Nagel et al., 2021).\n   - Quantization approaches are broadly categorized into:\n     - **QAT:** Offers high performance but is resource-intensive due to finetuning and retraining requirements (Bhalgat et al., 2020; Gong et al., 2019; Esser et al., 2020; Egiazarian et al., 2024; van Baalen et al., 2024).\n     - **PTQ:** Compresses models without retraining, favorable for LLMs due to minimal resource requirements (Wei et al., 2023a; Jhunjhunwala et al., 2021; Li et al., 2021).\n\n2. **Non-uniform vs. Uniform quantization:**\n   - **Non-uniform quantization:** Although effective, it requires complex specialized kernels, which can slow down inference speeds (Kim et al., 2024; Egiazarian et al., 2024).\n   - **Uniform quantization:** Simpler and more implementable but currently lacks standardized evaluation methods across different setups.\n\n3. **Inference overhead:**\n   - Certain approaches involve significant computational overhead during inference, which, despite good performance, is unsuitable for efficient deployment (Chee et al., 2024; Tseng et al., 2024).\n\n**Contributions:**\n- Developed the **LLMC toolkit**: A plug-and-play quantization tool that supports multiple PTQ algorithms, models, and hardware configurations allowing for efficient, lossless quantization of 100-billion-parameter LLMs with minimal resource requirements.\n- Conducted a fair and modular benchmark of quantization techniques, involving nearly 600 experiments, to derive best practices in calibration data, algorithm pipeline, and quantization configuration.\n- Designed an optimized PTQ pipeline combining high efficiency and accuracy tailored for diverse scenarios, ultimately integrated into the LLMC toolkit for ease of use.",
        "methodology": "The paper \"LLM-QBench: A Benchmark Towards the Best Practice for Post-training Quantization of Large Language Models\" proposes a methodology for quantizing large language models (LLMs). Below, the key components and innovations of the proposed method are highlighted.\n\n### Preliminary of Quantization:\nThe process of quantization for an element in a vector is defined by:\n\n- **Bounds and Bit-width**: Use upper bound \\(\\alpha\\) and lower bound \\(\\beta\\) of the vector. \\(b\\) represents the bit-width of the quantized element \\(e\\).\n- **Symmetric vs. Asymmetric Quantization**: When \\(\\beta = -\\alpha\\), the process is symmetric quantization; otherwise, it is asymmetric quantization. The paper focuses primarily on asymmetric quantization.\n- **Weight-Only Quantization**: Uses per-group quantization where weights in a group share the same scaling factor \\(s\\).\n- **Weight-Activation Quantization**: Applies per-channel quantization for weights and per-token quantization for activations.\n\n### Notation:\n- The notation \u201cwxay\u201d denotes bit-width \u201cx\u201d for weights and \u201cy\u201d for activations.\n- \u201cgz\u201d implies group size \u201cz\u201d in group-wise quantization.\n\n### Factors Influencing LLM Quantization:\nThree main dimensions influence quantization:\n\n1. **Calibration Data**: Used to evaluate the range of tensors and determine quantization parameters, essential for maintaining model performance post-quantization. Investigating different corpora for calibration data is crucial.\n   \n2. **Algorithm**:\n    - **Transformation**: Balances quantization errors by leveraging the magnitude between weight and activation before quantization using a balance factor \\(a\\).\n    - **Clipping**: Involves clipping outliers with minimal impact to improve range estimation and representation during calibration.\n    - **Reconstruction**: Uses the Hessian matrix to evaluate and adjust quantization perturbations for maintaining model integrity incrementally.\n\n3. **Target Bits**: Refers to the bit-widths adopted for weight, activation, and KV cache, which significantly influence accuracy. Although 3-bit and 6-bit quantization are explored for potential, practical deployments mainly use hardware-friendly 2-bit, 4-bit, and 8-bit quantizations.\n\n### Quantized Inference of LLM:\nThis involves targeting the Linear layers with weights, such as the Q, K, V, and O layers in self-attention modules and the Up, Gate, and Down layers in FFN modules. Three types of quantization presented provide different benefits:\n\n- **Weight-Activation Quantization**\n- **Weight-Only Quantization**\n- **KV-Cache Quantization**\n\nThese types of quantization contribute to prefill and decode latency reduction.\n\n### Summary:\nThe paper introduces a quantization framework that accounts for calibration data, algorithms, and target bit-widths to maintain model performance post quantization. Its innovative approach includes asymmetric quantization, per-group and per-channel/token quantization, and explores practical bit-widths for deployment in LLMs.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n**Models:** \nThe experiment evaluates the performance of various large language models to underscore the generalizability of the benchmark. Specifically, the LLAMA-2 family models are assessed, covering sizes from 7B to 70B parameters, for general language tasks. Additionally, the evaluation extends to models specialized for distinct tasks: ChatGLM for long-context abilities, CodeLLAMA for coding tasks, and WizardMath for mathematical problem-solving.\n\n**Datasets:**\nDatasets are divided into upstream and downstream categories:\n- **Upstream Datasets:** These include WikiText2 and C4 datasets, evaluated using the perplexity metric, a stable indicator of LLM performance. \n- **Downstream Datasets:** These span a variety of tasks:\n    - Examination tasks: MMLU and ARC-e\n    - Knowledge task: BoolQ\n    - Understanding task: Lambada\n    - Reasoning tasks: PIQA, HellaSwag, and GSM8K\n    - Coding tasks: HumanEval and MBPP\n    - Long context evaluation: LongBench\n\n**Hardware:**\nThe inference efficiency was measured using low-bit kernels on NVIDIA server and edge GPUs. Specifically, NVIDIA\u2019s TensorRT-LLM framework was used to evaluate the performance of quantized models.\n\n**Main Experimental Results:**\nThe main experimental results focus on demonstrating the impact of post-training quantization on model performance across different tasks and hardware setups. Here are the summarized outcomes:\n\n1. **Performance on General Language Tasks:**\n   - For LLAMA-2 models, the performance on WikiText2 and C4 datasets shows minimal degradation in perplexity after quantization, indicating the robustness of the quantization process.\n\n2. **Task-Specific Models:**\n   - ChatGLM exhibits sustained performance for long context tasks in LongBench.\n   - CodeLLAMA maintains coding task accuracy in HumanEval and MBPP.\n   - WizardMath shows negligible loss in performance for mathematical problem-solving as tested on GSM8K.\n\n3. **Downstream Tasks:**\n   - Quantized models perform consistently well on examination (MMLU and ARC-e) and reasoning tasks (PIQA, HellaSwag).\n   - The understanding task, Lambada, and the knowledge task, BoolQ, show that quantization does not significantly impair accuracy.\n\n4. **Inference Efficiency:**\n   - The studies on NVIDIA server and edge GPUs using TensorRT-LLM indicate that the quantized models significantly reduce inference latency and resource utilization while maintaining model accuracy across tasks.\n\nThe results collectively underscore the effectiveness of the proposed post-training quantization benchmark in preserving the performance of large language models across a variety of tasks while enhancing inference efficiency on modern hardware platforms."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To investigate the impact of calibration data on the accuracy of quantization for large language models (LLMs).",
            "experiment_process": "Calibration data was tested using various algorithms, comparing domain-specific data for domain-specific models and diverse data for general models. The specific experimental setup and data variations are detailed in Table 1 (reference in original paper).",
            "result_discussion": "The results highlight the significance of using domain-specific data for domain-specific models and diverse data for general models to achieve optimal accuracy after quantization.",
            "ablation_id": "2405.06001v1.No1"
        },
        {
            "research_objective": "To derive insights for developing an optimally combined quantization pipeline through an analysis and comparison of existing quantization algorithms.",
            "experiment_process": "Different quantization strategies, such as transformation, clipping, and reconstruction techniques, were evaluated on LLAMA-2 models of 7B, 13B, and 70B sizes under both weight-only and weight-activation quantization scenarios. The details of these techniques and their behavior were analyzed and summarized in Tables 2, 3, and 4 (reference in original paper).",
            "result_discussion": "The study found that asymmetric clipping is the most effective strategy for optimizing accuracy. The transformation technique, especially when using search-based scaling, showed considerable accuracy improvements. Conversely, the reconstruction technique provided limited benefits and required extended calibration periods. Combining search-based transformation (v1) and search-based asymmetric clipping emerged as the most effective strategy for balancing accuracy and efficiency, with further improvements possible by initializing from pre-searched values and conducting additional learning for a few epochs.",
            "ablation_id": "2405.06001v1.No2"
        },
        {
            "research_objective": "To evaluate the impact of different quantization bit configurations on model accuracy and efficiency.",
            "experiment_process": "Fixed-precision and mixed-precision quantization approaches were tested. For fixed-precision, 2-bit, 3-bit, 4-bit, and 8-bit quantization schemes were compared, including evaluation of Key-Value (KV) cache quantization. Mixed-precision strategies were assessed, using theoretical hardware-friendly configurations. Keep in mind the throughputs and inference speeds were measured using NVIDIA's cloud (SMX 80G A100) and edge (Drive Orin) GPUs with the TensorRT-LLM inference library. The details were presented in Tables 21, 23, 24, and Figures 2, 3 (reference in original paper).",
            "result_discussion": "The study showed significant accuracy degradation with 2-bit quantization, making it impractical. 4-bit weight-only and mixed precision of 4-bit weight with 8-bit activation (w4a8) or full 8-bit (w8a8) configurations provided better accuracy and efficiency balance. For KV cache, 4-bit per-group quantization with a group size of 8 performed comparably to 8-bit quantization and closely approximated FP16 performance. Mixed-precision quantization yielded better results, with dynamic bit allocation improving accuracy more than static strategies, despite higher computation overhead. Quantization with 8-bit weights and activations accelerated inference stages significantly, though benefits diminished for larger models.",
            "ablation_id": "2405.06001v1.No3"
        }
    ]
}