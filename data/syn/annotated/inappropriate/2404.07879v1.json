{
    "title": "Analyzing Toxicity in Deep Conversations: A Reddit Case Study",
    "abstract": "Online social media has become increasingly popular in recent years due to its ease of access and ability to connect with others. One of social media\u2019s main draws is its anonymity, allowing users to share their thoughts and opinions without fear of judgment or retribution. This anonymity has also made social media prone to harmful content, which requires moderation to ensure responsible and productive use. Several methods using artificial intelligence have been employed to detect harmful content. However, conversation and contextual analysis of hate speech are still understudied. Most promising works only analyze a single text at a time rather than the conversation supporting it. In this work, we employ a tree-based approach to understand how users behave concerning toxicity in public conversation settings. To this end, we collect both the posts and the comment sections of the top 100 posts from 8 Reddit communities that allow profanity, totaling over 1 million responses. We find that toxic comments increase the likelihood of subsequent toxic comments being produced in online conversations. Our analysis also shows that immediate context plays a vital role in shaping a response rather than the original post. We also study the effect of consensual profanity and observe overlapping similarities with non-consensual profanity in terms of user behavior and patterns.\n\nDisclaimer This paper contains examples of explicit language that may be disturbing to some readers.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In recent years, online social media platforms have become ubiquitous in our daily lives. These platforms allow users to connect and communicate with others in real-time. However, the nature of these interactions can vary significantly based on their visibility. Private conversations that take place through online communication channels, such as direct and group messaging, have little effect on society as a whole since they do not influence or challenge the dominant narratives, values, or expectations of society. But conversations occurring in a public setting, such as on social media platforms where users can participate, must adhere to the platform\u2019s rules and the general societal norms. However, on the downside, as online social media provide users with a degree of anonymity, this often leads to the emergence of hate speech on the platforms Mondal et al. (2017  ###reference_b23###). Therefore, it is important to moderate content on social media because it helps ensure that the platform remains safe and respectful for all users.\nWithout moderation, social media can become a breeding ground for spam, hate speech, and other forms of harmful or offensive content. Subsequently, this can create a negative user experience and make the platform less enjoyable for everyone.\nGiven the vast quantity of content, it is difficult to manually examine and identify every instance of hate speech on social media. Consequently, several studies have investigated how to identify hate speech using artificial intelligence. Existing automatic moderation systems use texts sourced from various social media platforms and online encyclopedias manually labeled by human annotators Waseem and Hovy (2016  ###reference_b35###); Davidson et al. (2017  ###reference_b7###); Wulczyn et al. (2017  ###reference_b36###). The aforementioned studies focus on texts as individual sentences and disregard the notion of conversations that occur in the form of responses to posted content on online social media platforms. Some social media platforms, like Instagram, Youtube, etc., prohibit users from posting nested responses and limit them to a single level. Whereas other platforms like Twitter and Reddit facilitate deeper conversations by permitting nested responses, allowing for discussions similar to real life. Moreover, it is worth noting that certain online communities exhibit distinct norms that may be more permissive or even toxic compared to others. In such cases, new users often find themselves adapting to these established norms Rajadesingan et al. (2020  ###reference_b26###). Figure 1  ###reference_### is an example of an online conversation sourced from Reddit where the responses to the post are ordered in a hierarchical structure.\nThis paper focuses on analyzing deep conversations by taking a holistic view of a post and its responses. Specifically, this study centers on the identification of toxic language and patterns within a given post and its subsequent responses. To achieve this, we analyze a dataset consisting of 800 Reddit posts that have garnered over 1 million responses. These posts are sourced from eight distinct subreddits known for hosting lengthy discussions. We use Reddit since a response to a comment or post expresses agreement or disagreement with the original text; it can be viewed as a public conversation. The main contribution of this paper is to find how toxicity disseminates in public conversations on social media platforms that support deep conversations.\n###figure_1### Our analysis investigates the following research questions:\nRQ1:  Are toxic comments more likely to generate toxic conversations than non-toxic ones?\nWe find a significant correlation between the toxicity of response and the subsequent toxicity it attracts in terms of responses. See Section 5  ###reference_###\nRQ2:  Does the toxicity of a response depend on the toxic levels of responses made before it?\nAccording to our analysis, the response immediately preceding a given response has a greater impact on its toxicity compared to the previous responses. See Section 6  ###reference_###\nRQ3:  How long does a conversation retain its toxic nature?\nWe observe that toxicity in a conversation generally diminishes within the initial two to three levels of responses and does not endure over time. See Section 7  ###reference_###\nRQ4:  What effect does toxicity have on user retention in a conversation?\nWe observe that users exhibit a bimodal pattern in terms of toxicity levels, providing an interesting insight into the dynamics of neutral responses in public conversations. See Section 8  ###reference_###\nRQ5: Does the toxicity norms of consensual groups differ from non-consensual groups?\nWe find overlapping similarities between both groups, suggesting identical toxicity norms. See Section 9  ###reference_###\nThe remainder of the paper is structured as follows. In Section 2  ###reference_###, we present relevant works. We define our conversational modeling approach in Section 3  ###reference_###. In Section 4  ###reference_###, we discuss the dataset, then in Sections 5  ###reference_### through 9  ###reference_###, we address the research questions and in Section 10  ###reference_###, we provide concluding remarks."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "In this section, we discuss existing literature on topics pertinent to our research. We explore two distinct yet interconnected domains relating to our work: toxicity detection and user behavior analysis."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Toxicity Detection",
            "text": "The early efforts to address abusive language, such as Yin et al. (2009  ###reference_b38###), employed supervised classification with n-gram, manual regular expression patterns, and contextual characteristics based on prior phrases\u2019 abusiveness. Diverse datasets on topics like racism, feminism, and misogyny were gathered from online platforms like Twitter and Reddit Waseem (2016  ###reference_b34###); Founta et al. (2018  ###reference_b10###); Waseem and Hovy (2016  ###reference_b35###). Additionally, datasets, including examples of personal attacks and toxic messages from Wikipedia\u2019s Talk pages, were used to analyze factors like hate speech, toxicity, and community sentiment on these platforms Wulczyn et al. (2017  ###reference_b36###). However, models trained on these datasets struggled with implicit hate, leading ElSherief et al. (2021  ###reference_b9###) to source tweets from extremist groups for labeled implicit toxicity.\nAutomatic hate speech detection\u2019s inherent complexity poses challenges, as studies highlight difficulties arising from language variations and biases toward certain identity groups in automated systems Sap et al. (2019  ###reference_b28###); Nobata et al. (2016  ###reference_b24###); Davidson et al. (2019  ###reference_b6###); Xia et al. (2020  ###reference_b37###). Researchers addressed bias concerns, introducing metrics specific to identity performance evaluation Dixon et al. (2018  ###reference_b8###); Borkan et al. (2019  ###reference_b5###). Approaches like functional test suites R\u00f6ttger et al. (2021  ###reference_b27###), human-in-the-loop dataset generation Vidgen et al. (2021  ###reference_b32###), and multi-task learning Vaidya et al. (2020  ###reference_b31###) aimed to enhance model robustness. Additionally, Barikeri et al. (2021  ###reference_b4###) used embedding projections to mitigate unintended bias. While previous works focused on single textual entities, Ghosh et al. (2018  ###reference_b11###) analyzed sarcasm using LSTM and attention models, and contextual analysis in fake news detection incorporated hyperbolic geometric representations and Fourier transformations Grover et al. (2022  ###reference_b13###). An extension of this contextual approach was adapted for implicit hate speech detection Ghosh et al. (2023  ###reference_b12###)."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "User Behavior Analysis",
            "text": "While studies have improved toxicity detection models, a different line of work has analyzed social media platforms to unearth user behavior and information dissemination patterns in human communications in cyberspace. Aumayr et al. (2011  ###reference_b2###) pioneered methods for reconstructing reply behaviors in threads, employing a classification approach that integrates both content and non-content features. Leavitt and Clark (2014  ###reference_b21###) delved into the production of news on a hurricane-related subreddit, investigating categories of news likely to receive more upvotes. Notably, followers of influencers disseminating deceptive content on social media have been observed to employ uncivil language and become more effectively polarized Guldemond et al. (2022  ###reference_b14###).\nIn addition to broader analytics on social media platforms, there has been a noteworthy surge in literature specifically examining toxicity within these platforms. A study on Twitter user data by Yousefi et al. (2023  ###reference_b39###) highlighted how various content types and contexts can impact the spread of toxicity within online communities. Furthermore, a cross-platform study indicated that Reddit tends to exhibit higher toxicity levels than other platforms, such as Twitter and Parler Noor et al. (2023  ###reference_b25###), thereby influencing our choice of Reddit as a data source. An insightful examination of Reddit users revealed that usernames serve as useful predictors for author profiling, as users with toxic usernames tend to generate more toxic content than others Urbaniak et al. (2022  ###reference_b30###).\nThe works closest to ours involve studies that have analyzed more than one post at a time with respect to toxicity. For example, Bakshy et al. (2012  ###reference_b3###) delved into information diffusion on Facebook, seeking insights into the factors influencing post reach. Aleksandric et al. (2022  ###reference_b1###) conducted a longitudinal study on Twitter, uncovering behavioral reactions of toxicity victims, including avoidance, revenge, countermeasures, and negotiation. Kumar et al. (2010  ###reference_b20###) explored the dynamics of online conversations, analyzing patterns in human exchanges on social media, scrutinizing thread depth, and correlating their findings with Heap\u2019s law Heaps (1978  ###reference_b16###). While previous works have investigated user interaction behavior in public conversations, such as continuity Kim et al. (2023  ###reference_b19###) and new & re-entry Wang et al. (2022  ###reference_b33###), our approach distinguishes itself by analyzing deep public conversations using a tree-based conversation modeling approach. This methodology enables us to capture the intricate interplay of context and toxicity without imposing limitations on contextual depth, providing valuable insights into how toxicity shapes the buildup of conversations in a public setting."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Conversation Modeling",
            "text": "By and large, in online social media or content-sharing platforms, users post content and receive responses in the form of likes, replies, or comments. In particular, responses in the form of comments facilitate the generation of conversations in a public setting which can be either deep or shallow, depending on the platform. We refer to comments as responses henceforth in this paper. Platforms like Youtube, Instagram, and Facebook allow for shallow conversations with little engagement since only a single level of responses is permitted. However, Twitter and Reddit promote a higher level of engagement owing to their support for deep conversations. Due to the inherent conversational structure of deep conversations, they can be represented as a generic tree model where the post and responses are represented as nodes. Figure 1  ###reference_### depicts a deep conversation in which the post or content by itself is the root node, and the remaining nodes are responses. The responses of each individual node are represented as child nodes. Note that every conversation has the possibility of containing toxic responses due to the acceptance of profanity in the chosen subreddits. To proceed further with our analysis, we define the following:\nOpinions : Opinions can be of any reply with relation to the node\u2019s content. In other words, an opinion does not necessarily need to be supportive or against the node\u2019s content as it could be neutral, too. Opinion is determined by the number of children nodes of a response. More responses (children) correspond to more opinions for any given response or post.\nEngagement : This represents a node\u2019s (response) maximum depth. A deeper branch of response indicates that there is a higher level of interest and participation in the discussion, encouraging users to continue contributing their thoughts and opinions on the topic. As a result, a node with a greater depth is often seen as a sign of successful engagement and active participation.\nToxic Accumulation : Every node comprising of text is treated as a single entity and a toxicity score is computed based on it. The toxic accumulation score is computed by averaging the toxicity scores of all the nodes that branch from the given node. This indicates the level of toxicity produced by a node. A higher score indicates that the response contributed to the emergence of toxic conversations. Toxic accumulation can be defined by the following equations:\n\n\nIn Equation 1  ###reference_###,  denotes the contribution of toxicity by subtree formed by . It must be noted that Equations 1  ###reference_### and 2  ###reference_### are recursive and complement each other. Equation 2  ###reference_### computes toxic accumulation of . Therefore,  in Equation 1  ###reference_### refers to the summation of toxic accumulation of all the children  of .\nThese definitions help us better understand user participation in the presence of toxic comments by establishing the relationship between engagement and opinions as well as toxicity and toxic accumulation."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Dataset",
            "text": "We use Reddit posts for analysis as the platform facilitates deep conversations and provides a large quantity of data with diverse discussions, allowing us to analyze the nature of deep conversations on the platform thoroughly. According to Reddit\u2019s algorithm111https://github.com/reddit-archive/reddit/blob/master/r2/r2/lib/db/_sorts.pyx  ###reference_lob/master/r2/r2/lib/db/_sorts.pyx###, top posts are categorized by upvotes and user participation among other factors and therefore are prone to generate deep conversation, which suits our purpose. The top 100 posts as of October 12th, 2022 were chosen from the following eight subreddits: r/atheism, r/confidentlyincorrect, r/Fuckthealtright, r/antifeminists, r/facepalm, r/4chan, r/tifu and r/RoastMe. As the titles of the subreddits imply, one may assume that we chose them for the controversial aspects they might encourage, however, these subreddits were chosen for free discourse, debate, and communal acceptance of profanity in the respective subreddits. We extracted the posts and comments while preserving the hierarchy of the conversation. Each comment\u2019s text was collected along with its metadata, including the score, anonymized author ID, comment ID and time stamp, and stored as individual JSON files222https://github.com/s-vigneshwaran/Reddit-Case-Study-Data  ###reference_ase-Study-Data###.\n###figure_2### Reddit allows multiple forms of content as the body of a post, like HTML, image, audio, gif, etc. A manual examination of 10 randomly selected posts from each subreddit revealed that many posts included pictures with text in some way, be it a quote, a screenshot of a news article, or a Twitter post, etc. We extracted text from the images of such posts and included it in the main body of the post manually because the conversation generated has a direct effect on the post\u2019s content. Upon inspection of the entire data (after extracting texts from images), we found that the posts had a median of 52 words, and the responses had a median of 16 words. Figure 2  ###reference_### shows the distribution of the number of responses to posts signifying that majority of post attracts fewer than 2000 responses. It must be noted that we exclude posts from r/RoastMe and use only 700 posts for Sections 5  ###reference_### to 8  ###reference_### since they are targeted towards analyzing non-consensual profanity. The remaining 100 posts from r/RoastMe has been utilized in Section 9  ###reference_### to study consensual profanity patterns. For a detailed breakup, refer to Table 1  ###reference_###.\nEthical Consideration Our study does not use any user information in our analysis, and the presented results are all shown at an aggregated level, which cannot be traced back to a specific user in any way. Moreover, all the user information is anonymized by obscuring their unique IDs to protect their privacy."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Relationship between toxicity and toxic accumulation",
            "text": "In this section, we investigate \u201cRQ1: Are toxic comments more likely to generate toxic conversations than non-toxic ones?\u201d. The general connotation is that a toxic comment will cause the target to withdraw from the conversation Kang et al. (2016  ###reference_b18###). This suggests that, in the context of a public conversation, a toxic response has the potential to alter the conversation\u2019s content and direction as users may or may not react to toxic responses. Given toxic accumulation\u2019s ability to aggregate toxicity scores across the entire sub-branch, it serves as a dependable measure for estimating the toxicity produced by a response, thus making it the preferred approach for addressing this research question."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Methodology",
            "text": "To answer RQ1, we explored correlation statistic. Firstly, we collect all responses ignoring the hierarchy from the dataset, along with its toxicity and toxic accumulation scores. Then we calculate the Spearman rank correlation coefficient between the two defined scores across the entire dataset. If the response is toxic, this statistical metric can be used to ascertain whether or not the sub-branch is likely to be toxic as a result of the toxic accumulation of the node. Since the toxicity and toxic accumulation will be the same in the terminal responses (leaf nodes) we excluded them to avoid confounding our results."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Results",
            "text": "The correlation between the toxicity and toxic accumulation of the comments is presented in Table 2  ###reference_###. The average correlation score is 0.631  0.013, indicating a moderate degree of association. The correlation is statistically significant (P-Value  0.01). The top two subreddits concerning this metric (r/tifu, and r/facepalm) are in no way controversial, unlike subreddits that are strongly centered around topics such as r/atheism, r/antifeminsts, and r/Fuckthealtright denoting the presence of variety in the data. By including the leaf nodes, the correlation turns out to be 0.9 for the entire dataset, demonstrating that leaf nodes do, in fact confound the results.\nIn light of these results, it becomes evident that toxic comments have a significant influence on the overall tone and trajectory of online discussions as they possess the potential to escalate and perpetuate toxicity within conversations, leading to more negativity and hostility on social media platforms."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Contextual Analysis",
            "text": "One of the main benefits of preserving the structure of a public conversation is the ability to analyze the influence of previous responses on the target response effectively. This allows us to answer the question \"RQ2: Does the toxicity of a response depend on the responses made before it?\" This analysis is focused solely on the toxic nature of responses and does not consider the content or meaning of previous responses i.e, we give importance to toxic context (score) over linguistic context (text). Figure 1  ###reference_### shows an example of an excerpt from a post to highlight the importance of context in terms of toxicity. Note that toxic nodes yield more toxic responses in the given example."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Methodology",
            "text": "We examine whether previous responses impact the toxicity of the target response. To achieve this, we collect toxicity scores of up to 5 previous comment levels of every response in the dataset. For example, in Figure 1  ###reference_###, for the response labeled 2.1, the immediate predecessor is labeled 2, and the extended predecessor is labeled 0 and is termed level 2 analysis since we use only two levels of context, i.e, we understand how the toxicity of 0 and 2 affect 2.1. Analyzing the relationship between the toxicity of the predecessor and the toxicity of the extended predecessor demonstrates its effect on a response. Similarly, levels 3, 4, and 5 follow the same pattern, but the context becomes more in-depth. We modeled four simple linear regression models: the target variable is a response\u2019s toxicity, and the dependent variables are the responses\u2019 preceding comments.\n\nEquation 3  ###reference_### represents a simple linear regression model,  refers to the previous levels\u2019 toxicity scores.  takes in the levels for which the target variable is modeled. For instance, a level  analysis would take  as  and  where  is parent and  is a grandparent."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Results",
            "text": "Table 3  ###reference_### presents the beta coefficients of the models, capturing valuable insights. It is evident that the immediate predecessor holds a greater influence on the toxicity of the response compared to previous levels, while the impact of predecessors\u2019 toxicity remains minimal. Despite the low significance, we can observe a consistent downward trend in the impact levels, suggesting that the previous response plays a role in shaping the toxicity of a response within a public conversation. Similar results were obtained when analyzing individual subreddits utilizing the same methodology. Due to strong overlapping similarity, we don\u2019t report the subreddit-wise beta coefficients in the paper."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Temporal Behavioral Change",
            "text": "###figure_3### This section examines the characteristics of public conversations in terms of change with respect to time. Here, time corresponds to the depth of a conversation\u2019s branches. We examine the duration of toxic prevalence in a conversation, answering the question \"RQ3: How long does a conversation retain its toxic nature?\". We find that toxic conversations are generally lesser in prevalence than non-toxic conversations."
        },
        {
            "section_id": "7.1",
            "parent_section_id": "7",
            "section_name": "Methodology",
            "text": "Each branch, characterized by its toxicity scores, forms a sequence of numbers where each value depends on the preceding levels. This configuration allows us to treat the data as time series, offering insights into the evolving toxicity levels as the conversation progresses. The distribution of conversational branches concerning comment levels in the dataset is illustrated in Figure 3  ###reference_###. A consistent decline in the number of branches is evident beyond ten levels, with only 26,958 out of 758,519 branches extending beyond this threshold. To address RQ3, we establish a ten-level threshold for analysis. Subsequently, the mean toxicity scores and accumulated toxicity scores for selected branches are computed and visually examined to discern any trends in the rate of change concerning toxicity.\n###figure_4### ###figure_5### ###figure_6### ###figure_7###"
        },
        {
            "section_id": "7.2",
            "parent_section_id": "7",
            "section_name": "Results",
            "text": "Figure 4  ###reference_### illustrates the trends of toxicity and toxic accumulation over time. Subfigures 3(a)  ###reference_sf1### and 3(c)  ###reference_sf3### communicate the trends irrespective of the nature of the posts, whereas, Subfigures 3(b)  ###reference_sf2### and 3(d)  ###reference_sf4### describe the toxic trends for posts whose toxic threshold cross 0.5. It can be observed that after the first three levels, the toxicity tends to level off irrespective of the comment being toxic or not toxic (toxicity score  0.5), signifying that conversations in public setting generally does not hold shape for long. This also resonates with the harmful effects of toxicity, as toxic comments have the effect of causing users to lose interest in the content and move on from the conversation Schmid et al. (2022  ###reference_b29###). Interestingly, the subreddit r/4chan is slightly distinct from others because, in contrast to other subreddits, the toxic levels are generally high, decrease steadily, and never fluctuate. It is likely the result of reposting content onto this subreddit from the platform 4chan, which is known for its notorious toxic levels due to its anonymity."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "User participation with toxic comments",
            "text": "###figure_8### This section undertakes an investigation into the repercussions of toxicity on user retention in the context of public conversations. To achieve this objective, we employ the definitions of engagement and opinion as delineated in Section 3  ###reference_### to ascertain the influence of these variables on user participation. Through a very simple analysis of the intricate interplay among toxicity and opinions, we derive rudimentary insights pertaining to the ramifications of toxicity on user retention within the conversational realm, thus addressing the research question posited as \"RQ4: What effect does toxicity have on user retention within a conversation?\"."
        },
        {
            "section_id": "8.1",
            "parent_section_id": "8",
            "section_name": "Methodology",
            "text": "To begin our analysis, we categorize the spectrum of toxicity into ten groups with a step range of 0.1, with group 1 representing non-toxic and group 10 representing highly toxic, enabling a more granular examination of the toxicity levels under consideration. Following this, we take the sum of the number of opinions garnered by all these ten groups in the dataset. Due to the significant disparity in the dataset regarding toxicity levels  0.1, indicating a substantial portion of non-toxic responses on social media, the outcome could be heavily biased. As a result, we made the decision to remove all data points associated with group 1."
        },
        {
            "section_id": "8.2",
            "parent_section_id": "8",
            "section_name": "Results",
            "text": "Figure 5  ###reference_### illustrates the total number of responses categorized by toxicity range. In the plot, one can observe the presence of bimodal phenomena. This phenomenon indicates that the number of opinions increases when the text is either too toxic (toxic score  0.8) or normal. This further adds to the reasoning that people prefer to continue responding to non-toxic texts that adhere to positive and respectful communication norms rather than engaging with toxic content. However, we also observe a counter-intuitive result that the number of responses increases as the toxicity increases. Upon further inspection of parent-child response pairs, we find that out of 190,929 toxic parent responses 27.9% children responses are toxic whereas out of 911,771 non-toxic parent responses only 19.3% children responses are toxic signifying that toxic responses has better chance of inviting more toxic responses."
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "Effect of Consensual Profanity",
            "text": "Abusive language is not just used to attack individuals; it may also be used to frame profane information in civil discourse Jordan (2020  ###reference_b17###). We analyze consensual toxic comments and compare them with unsolicited toxic and hateful content which are not consensual to answer \"RQ5: Does the toxicity norms of consensual groups differ from non-consensual groups?\". To this end, we scrape the responses of the top 100 posts from r/RoastMe, totaling 152,008 responses from the temporal distribution as our dataset. We chose r/RoastMe since it is a community where users post photos of themselves asking to be roasted (severe criticism) by other members of the community. We performed the same set of analyses mentioned in the previous sections to understand if consensual toxicity patterns resemble the general toxic norms in public discussions.\nThe correlation between toxicity and toxic accumulation for the responses belonging to this subreddit is 0.66, which is slightly higher than that of the non-consensual subreddits (Refer to Table 2  ###reference_###). However, the difference is a minuscule +0.03 from the mean correlation. The contextual analysis on r/RoastMe resulted in the same significance pattern of the beta coefficients of the predecessors signaling that the immediate predecessor\u2019s toxicity impacts a node more than extended predecessors for the non-consensual communities as well. The mean scores of toxicity and accumulated toxicity resulted in the same trend overlapping with the patterns observed in Figure 5  ###reference_###. The total number of opinions generated 141,757 responses for toxicity levels  0.3 &  0.8 and 10,251 for toxicity in the range [0.3, 0.8]. The results help us in establishing the similarity between the toxic norms of consensual and non-consensual communities."
        }
    ],
    "url": "http://arxiv.org/html/2404.07879v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "3",
            "5.1",
            "6.1",
            "7.1",
            "8.1"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "5",
            "5.2",
            "6",
            "6.2",
            "7",
            "7.2",
            "8",
            "8.2",
            "9"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "5",
            "6",
            "7",
            "8",
            "9"
        ]
    },
    "research_context": {
        "paper_id": "2404.07879v1",
        "paper_title": "Analyzing Toxicity in Deep Conversations: A Reddit Case Study",
        "research_background": "### Motivation:\nThe paper is motivated by the widespread use of online social media platforms and the resulting necessity to maintain these platforms as safe, respectful environments. The inherent anonymity of these platforms often leads to the emergence of toxic behaviors such as hate speech. Traditional content moderation methods are largely manual and infeasible on a large scale, creating the need for advanced, automated moderation systems capable of identifying toxic content effectively. Additionally, existing studies tend to focus on individual sentences, without considering the dynamics of deeper conversations, particularly in platforms like Reddit where nested responses are common. \n\n### Research Problem:\nThe primary focus of the paper is to analyze deep conversations on Reddit to understand how toxicity propagates within these conversations. Specifically, the research seeks to answer the following questions:\n1. Are toxic comments more likely to generate toxic conversations than non-toxic ones?\n2. Does the toxicity of a response depend on the toxic levels of preceding responses?\n3. How long does a conversation retain its toxic nature?\n4. What effect does toxicity have on user retention in a conversation?\n5. Do the toxicity norms of consensual groups differ from non-consensual groups?\n\n### Relevant Prior Work:\n1. **Mondal et al. (2017)** - Highlighted the emergence of hate speech due to the anonymity afforded by social media platforms.\n2. **Waseem and Hovy (2016)**, **Davidson et al. (2017)**, and **Wulczyn et al. (2017)** - Outlined the use of human-annotated texts from various platforms to develop automated moderation systems. These studies primarily examine individual sentences rather than conversational dynamics.\n3. **Rajadesingan et al. (2020)** - Discussed how certain online communities exhibit distinct and sometimes toxic norms which new users adapt to, underlining the importance of understanding the norms within specific online communities.\n\nThis paper aims to extend these previous works by not just focusing on individual sentences, but by taking a holistic view of a sequence of responses to understand toxicity's dissemination within the structure of public conversations on Reddit.",
        "methodology": "### Methodology\n\nThe method proposed in \"Analyzing Toxicity in Deep Conversations: A Reddit Case Study\" focuses on examining user interactions in online conversations, specifically within Reddit, to identify the relationships between user engagement, opinions, and toxicity. The analysis leverages the tree-like structure of deep conversations, where each post and its corresponding responses are represented as nodes. Below is a detailed breakdown of the key components and innovations within this methodology:\n\n#### Key Components:\n\n1. **Tree Model Representation:**\n   - The conversation structure is modeled as a generic tree. The initial post or content acts as the root node, and each subsequent response creates child nodes.\n   - This structured representation allows for the detailed mapping of the depth and breadth of the conversation, effectively capturing the dynamics of user interaction.\n\n2. **Definitions for Analysis:**\n   - **Opinions:** Defined by the number of responses (child nodes) a node has. A greater number of responses indicate a variety of opinions.\n   - **Engagement:** Represented by the maximum depth of a node. A deeper branch signifies increased user interest and participation, marking the node as highly engaging.\n   - **Toxic Accumulation:** This is measured by computing the toxicity score of each node (response) and averaging the scores of all nodes in the subtree branching from a given node. This score provides an indication of how much toxicity a particular response contributes to the conversation. Higher scores denote higher contributions to toxic discussions.\n\n#### Innovations:\n\n1. **Hierarchical Toxicity Assessment:**\n   - By using a tree model, the methodology captures not only direct toxicity but also how toxicity propagates through nested responses. This hierarchical approach allows for a more comprehensive view of toxic accumulation across the entire conversation, rather than treating each response in isolation.\n\n2. **Recursive Toxicity Calculations:**\n   - The method uses recursive equations to calculate toxicity accumulation, considering both the immediate toxicity of a node and the accumulated toxicity from all child nodes. \n   - Equation 1 computes the toxicity contribution of a subtree formed by a node.\n   - Equation 2 recursively determines the toxic accumulation of a node by aggregating the toxic accumulations of all its children nodes.\n   - This recursive complementarity ensures that the model accurately represents how initial toxic comments may influence subsequent replies.\n\n3. **Measuring Engagement and Opinions in Toxic Contexts:**\n   - By defining and quantifying user opinions and engagement in tandem with toxicity, the study not only uncovers how deeply users engage with content but also how toxicity influences this engagement.\n   - This dual analysis helps understand the relationship between high engagement in conversations and the prevalence of toxic comments.\n\nThis comprehensive methodology thus aids in understanding the interplay between engagement, opinions, and toxicity in online conversations, providing a structured framework to analyze deep conversations on platforms like Reddit.",
        "main_experiment_and_results": "The main experiment in \"Analyzing Toxicity in Deep Conversations: A Reddit Case Study\" focuses on evaluating the nature and toxicity of deep conversations on Reddit. The setup and results are summarized as follows:\n\n### Experiment Setup\n**Datasets:**\n- **Platform:** Reddit\n- **Posts Selected:** Top 100 posts as of October 12th, 2022, from each of eight subreddits: r/atheism, r/confidentlyincorrect, r/Fuckthealtright, r/antifeminists, r/facepalm, r/4chan, r/tifu, and r/RoastMe.\n- **Reason for Selection:** These subreddits were chosen for their promotion of free discourse, debate, and communal acceptance of profanity. Although they may seem controversial, the primary focus is on the nature of deep conversations. The subreddit r/RoastMe is partially included to study consensual versus non-consensual profanity.\n- **Data Extracted:** Posts and comments, preserving the hierarchy of the conversation. Metadata collected includes the score, anonymized author ID, comment ID, and timestamp, all stored in individual JSON files.\n- **Text Extraction:** Manual extraction of text from images in posts, such as quotes or screenshots, to ensure the entire content that generates conversation is included.\n\n**Filtering:**\n- **Post Length Median:** Posts have a median of 52 words, and responses have a median of 16 words.\n- **Response Distribution:** Majority of posts attract fewer than 2000 responses.\n- **Exclusion:** r/RoastMe posts are excluded from sections analyzing non-consensual profanity, but 100 posts from r/RoastMe are used in a specific section to analyze consensual profanity.\n\n### Baselines\nThe baseline involves manual examination of randomly selected posts to determine the nature of conversations and their response structure, focusing on aspects of profanity and deep discussions.\n\n### Evaluation Metrics\nThe study largely focuses on qualitative metrics:\n- **Content Type:** Type of content in posts (text, image with text, etc.).\n- **Word Count:** Median word count of posts and responses.\n- **Response Count:** Distribution of the number of responses per post.\n- **Profanity Analysis:** Analysis of consensual (r/RoastMe) versus non-consensual profanity dynamics.\n\n### Main Experimental Results\n- **Data Insights:** Posts typically generate a limited number of responses, with a substantial proportion having fewer than 2000 responses.\n- **Content Forms:** Many posts included images with overlaid text, contributing significantly to the conversational content.\n- **Word Count Distribution:** Posts and responses have characteristic median word counts of 52 and 16 words, respectively.\n- **Profanity Patterns:** Profanity analysis distinguishes between consensual profanity (r/RoastMe) and general subreddit discussions focused on debate and free discourse.\n\n### Ethical Consideration\n- **Anonymization:** User information is anonymized by obscuring unique IDs.\n- **Aggregate Results:** The analysis is presented at an aggregated level, ensuring that it cannot be traced back to any specific user.\n- **Privacy Protection:** Ensuring the privacy of users is a key ethical consideration in the study. \n\nThese points summarize the main experiment setup and results without delving into ablation studies or detailed breakup analysis, which are covered in other sections of the paper."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Investigate whether toxic comments are more likely to generate toxic conversations than non-toxic ones.",
            "experiment_process": "The researchers studied toxic accumulation as a metric to estimate the toxicity produced by a response within the context of a public conversation. They analyzed the posts and comment sections from 8 Reddit communities, aggregating toxicity scores across entire sub-branches to measure this effect.",
            "result_discussion": "The findings demonstrated that toxic comments increased the likelihood of subsequent toxic responses, revealing that toxic accumulation is a dependable measure for estimating conversation toxicity.",
            "ablation_id": "2404.07879v1.No1"
        },
        {
            "research_objective": "Examine if the toxicity of a response depends on the preceding responses within a public conversation.",
            "experiment_process": "They preserved the structure of public conversations to analyze the influence of previous responses on the target response. Focusing on the toxic nature of responses, they prioritized toxic context (score) over linguistic context (text) for their analysis.",
            "result_discussion": "It was observed that toxic nodes in conversations often led to more toxic responses, validating the importance of context in shaping toxicity within a conversation.",
            "ablation_id": "2404.07879v1.No2"
        },
        {
            "research_objective": "Determine how long a conversation retains its toxic nature over time.",
            "experiment_process": "Researchers examined the characteristics of public conversations by looking at the depth of conversation branches to see if and how toxic prevalence changes over time.",
            "result_discussion": "It was found that toxic conversations are generally shorter in duration compared to non-toxic ones, indicating a quicker fading of toxic nature over time.",
            "ablation_id": "2404.07879v1.No3"
        },
        {
            "research_objective": "Assess the effect of toxicity on user retention within conversations.",
            "experiment_process": "They utilized definitions of engagement and opinion from prior sections to analyze the impact of these variables on user participation. This was done through a straightforward analysis of the interaction between toxicity and opinions within public conversations.",
            "result_discussion": "The study provided basic insights showing that toxicity negatively impacts user retention in conversations, causing participants to be less engaged over time.",
            "ablation_id": "2404.07879v1.No4"
        },
        {
            "research_objective": "Investigate if the toxicity norms of consensual groups differ from those of non-consensual groups.",
            "experiment_process": "The authors scraped responses from the top 100 posts on the r/RoastMe subreddit, totaling 152,008 responses. This subreddit is known for consensual profane interactions. They performed a similar analysis to previous sections to determine if consensual toxicity patterns align with general toxic norms in public discussions.",
            "result_discussion": "The results showed a high correlation (0.66) between toxicity and toxic accumulation in consensual subreddits, slightly higher than non-consensual subreddits. Similar trends were observed in contextual analysis and toxicity scores, thereby establishing that toxic norms in consensual and non-consensual communities are quite similar.",
            "ablation_id": "2404.07879v1.No5"
        }
    ]
}