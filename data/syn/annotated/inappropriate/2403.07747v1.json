{
    "title": "FineMath: A Fine-Grained Mathematical Evaluation Benchmark for Chinese Large Language Models",
    "abstract": "To thoroughly assess the mathematical reasoning abilities of Large Language Models (LLMs), we need to carefully curate evaluation datasets covering diverse mathematical concepts and mathematical problems at different difficulty levels. In pursuit of this objective, we propose FineMath in this paper, a fine-grained mathematical evaluation benchmark dataset for assessing Chinese LLMs. FineMath is created to cover the major key mathematical concepts taught in elementary school math, which are further divided into 17 categories of math word problems, enabling in-depth analysis of mathematical reasoning abilities of LLMs. All the 17 categories of math word problems are manually annotated with their difficulty levels according to the number of reasoning steps required to solve these problems. We conduct extensive experiments on a wide range of LLMs on FineMath and find that there is still considerable room for improvements in terms of mathematical reasoning capability of Chinese LLMs. We also carry out an in-depth analysis on the evaluation process and methods that have been overlooked previously. These two factors significantly influence the model results and our understanding of their mathematical reasoning capabilities. The dataset will be publicly available soon.\n\n\n\nKeywords:\u2009Large Language Models, Mathematical Reasoning Evaluation, Benchmark",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "Mathematics has always been an important part of the evaluation of LLMs (Wei et al., 2022  ###reference_b20###), which not only assesses the ability of LLMs in understanding and solving mathematical problems, but also profoundly measures the essential capability of LLMs in abstract conceptualization, logical reasoning and so on. Therefore, a high-quality mathematical evaluation benchmark is of great importance to a comprehensive LLM evaluation.\nPrevious works (Hosseini et al., 2014  ###reference_b5###; Roy and Roth, 2015  ###reference_b18###) curate mathematical test sets in English, which serve as a repository for grade school math word problems with accuracy being used as the evaluation metric. Recent years have witnessed a substantial progress in Chinese LLMs. Hence, mathematical evaluation datasets in Chinese (Wei et al., 2023  ###reference_b21###; Yang et al., 2023  ###reference_b24###) have been created correspondingly. These two previous Chinese datasets categorize testing instances by grade levels, providing a preliminary evaluation of Chinese LLMs on these levels. Their evaluation results show that the accuracy of GPT-4 for any grade surpasses or is close to 60%. However, a simple accuracy does not help us understand which mathematical concepts or skills LLMs have mastered. There is an urgent need for a comprehensive test set that can provide fine-grained evaluation results.\n###figure_1### Apart from arithmetic operations, mathematical ability involves diverse reasoning capabilities. We believe that the evaluation of the mathematical ability of LLMs should include two aspects:\nProviding diverse abstract mathematical concepts.\nEvaluating the logical and mathematic reasoning abilities of LLMs over mathematical problems at different difficulty levels.\nIn pursuit of these aspects, we propose FineMath, a benchmark composed of Math Word Problems (MWPs), designed to comprehensively assess LLMs\u2019 mathematical capability in a fine-grained way. FineMath organizes MWPs according to key mathematical concepts taught in elementary school, and each type of MWPs contains three levels of difficulty, facilitating detailed reasoning ability analysis.\nSpecifically, FineMath consists of 17 types of MWPs. For defining and collecting these MWP types, we have referred to the mathematics curriculum standards established by China\u2019s Ministry of Education and the principles and standards for school mathematics set by the American National Council of Teachers of Mathematics (NCTM). The key concepts and skills in grade school include Number & Operations, Algebra, Geometry, Measurement, Data Analysis & Probability, Problem Solving, and Reasoning. Different key concepts involve the use of different knowledge and abilities. We have also annotated the reasoning steps and process for each MWP, categorizing them into questions requiring one reasoning step, two reasoning steps, and three or more reasoning steps.\nBased on the curated benchmark, we have conducted a thorough analysis of the evaluation process and methods. Evaluations in mathematics have always emphasized the accuracy of results. However, we have observed factors that greatly influence the model\u2019s results, thus affecting our understanding of its capabilities:\nThe model is sensitive to the prompts used during the evaluation, and the results vary accordingly.\nThe methods of evaluation can also affect the model\u2019s results. We have compared the model\u2019s performance in selecting the final answers from the options of multi-choice questions, demonstrating that the form of evaluation tasks and options can influence the model\u2019s results to a certain extent.\nThe length of the LLM-generated answers, to some degree, reflects the model\u2019s \u201cconfidence\u201d when handling questions.\nThe main contributions of our work are as follows:\nWe propose a fine-grained elementary school MWPs benchmark for Chinese LLMs, which can assess the mathematical capabilities of LLMs from three aspects: accuracy of understanding abstract mathematical concepts, accuracy of reasoning, and overall accuracy.\nWe conduct an in-depth analysis of the contamination in our dataset, enabling researchers of LLMs to conduct a credibility analysis on the evaluation results.\nWe evaluate GPT-4, GPT-3.5-Turbo, and 8 Chinese LLMs, revealing their mathematical reasoning capabilities, and provide detailed evaluation results in various aspects."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Related Work",
            "text": "Traditional MWP datasets like AddSub (Hosseini et al., 2014  ###reference_b5###) and MultiArith (Roy and Roth, 2015  ###reference_b18###) are integrated into a MWP repository. Other similar datasets include SingleEq (Koncel-Kedziorski et al., 2015  ###reference_b9###), AQUA (Ling et al., 2017  ###reference_b12###) and AsDiv (Miao et al., 2020  ###reference_b13###). GSM8K (Cobbe et al., 2021  ###reference_b2###) and SVAMP (Patel et al., 2021  ###reference_b17###) take advantage of detailed annotations and have prevailed in recent evaluations.\nOur work is most inspired by the MATH (Hendrycks et al., 2021  ###reference_b4###) dataset. MATH collects problems from American high school mathematics competitions and categorizes problems into seven subjects. These subjects are Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra and Precalculus. However, these problems are very challenging, even when humans answer them, the accuracy rate is only 40%. Considering that many LLMs are still in their early versions, overly difficult problems may have limited significance for testing these models.\nCMATH proposed by (Wei et al., 2023  ###reference_b21###) and K6 proposed by (Yang et al., 2023  ###reference_b24###) are the two datasets that are relatively similar to ours developed concurrently. All these datasets focus on math word problems of elementary school, and organize instances by grade level. CMATH contains 1.7K problems collected from workbooks and exams on the Internet. K6 is composed of 600 problems collected from an educational institution. However, neither of the two datasets have been publicly released, precluding us from conducting an empirical comparison to them.\nApe210K proposed in (Zhao et al., 2020  ###reference_b25###) is a slightly earlier dataset. It contains 210K enormous Chinese math word problems from elementary school. Test sets alone in Ape210K contain as many as 5,000 problems. However, the test sets do not provide annotations related to LLMs.\nAn overview of the related MWP datasets is shown in Table 1  ###reference_###."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   Data Collection and Annotation",
            "text": "We create our dataset by collecting a diverse set of questions. We collect as many questions as possible from textbooks, workbooks and the Internet, from which high-quality questions are selected.\nAfter collecting these questions, we conduct automatical preprocessing on the collected data, which includes removing questions that are not math word problems, discarding questions with fewer than 10 Chinese characters, and retaining only questions that contain definite answers. Additionally, any questions that require reference to images are also discarded.\nOn the preprocessed data, we further perform manual annotation and processing: MWP categorization, question standardization, reasoning step and answer standardization and multiple-choice question transformation. We elaborate these data curation steps in the following subsections."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   MWP Categorization",
            "text": "We categorize the collected questions into 17 types, each corresponding to a key or basic concept111https://www.nctm.org/Standards-and-Positions/Principles-and-Standards/Principles,-Standards,-and-Expectations/ inherent in the MWPs. We introduce these key concepts encompassed in our dataset, along with their corresponding categories as follows.\nNumber & Operations: This mathematical concept requires an understanding of numbers, ways of representing numbers, relationships among numbers, and number systems. It also necessitates an understanding of the meanings of operations and the ability to compute with these operations. This concept includes 7 MWP categories: Percents, Decimals, Fractions, Factors & Multiples, Counting, Proportions and Mixed Operations.\nMeasurement: Measurement requires an understanding of the measurable attributes of objects and the units, systems, and processes of measurement. It corresponds to two MWP categories, namely Spatial Sense and Time.\nData Analysis & Probability: Data analysis and probability requires one to select and use appropriate statistical methods to analyze data and to apply basic concepts of probability. This concept is related to Central Tendency and Probability.\nAlgebra: This concept involves understanding patterns, relations and functions. The MWP categories of this concept include Equations and Patterns.\nGeometry: Geometry is to analyze the characteristics and properties of two- and three-dimensional geometric shapes, specify locations and describe spatial relationships using coordinate geometry and other representational systems. It contains two MWP categories: Two & Three Dimensional Geometry (Basic Geometry) and Analytic Geometry.\nOthers: We also categorized two types of special MWPs. Problem 1: simple optimization problems. Problem 2: tree planting problems that involve the relationship between points and segments."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   Question Standardization",
            "text": "Many questions in the selected data contain multiple queries. We normalize these questions so that each question contains only a single query. Ambiguous queries are also rephrased to enable the model to generate a unique answer."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "3.3.   Mathematical Reasoning and Answer Standardization",
            "text": "The process of answering MWPs is manually conducted, and the ground-truth answers are manually double checked by humans. We ask annotators to provide the steps in answering each MWP. Each step should be atomic and indivisible. For calculations that use a fixed solution formula, e.g., computing the area of a circle, we consider them as single-step MWPs.The reasons for annotating the number of required mathematical reasoning steps are two-fold:\n1. The number of required reasoning steps can be treated as a proxy to the difficulty level of MWPs. Intuitively, MWPs that require multiple steps to solve are more difficult than those solved in a single step. The progression from one step to the next also represents the reasoning process. Therefore, we categorize the difficulty of MWPs in our dataset into three levels. MWPs that can be solved in a single step are level-1 MWPs; MWPs that require two steps to solve are level-2 MWPs; level-3 MWPs are those that require three or more steps to solve.\n2. Presenting the number of reasoning steps facilitates reviewing and analyzing the collected data, thereby ensuring data quality."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "3.4.   Multiple-Choice Question Transformation",
            "text": "The original MWPs are accompanied with their single ground-truth answers. To facilitate automatic evaluation, we also transform them into multiple-choice question forms by manually providing additional contrastive answer options, similar to the AQUA dataset (Ling et al., 2017  ###reference_b12###)."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Data Statistics and Analysis",
            "text": "We provide data statistics and analysis on contamination of our dataset in this section."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1.   Data Statistics",
            "text": "The overall data statistics are displayed in Table 2  ###reference_###. All 1,584 questions are categorized into five major mathematical concepts and two classic types of MWPs. Each type contains at least 60 questions, and each difficulty level contains at least 20 questions. Questions were grouped using simple keyword matching to determine their similarity based on common patterns and frequent terms identified in the problem statements."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2.   Analysis on Contamination",
            "text": "###figure_2### FineMath serves as a comprehensive benchmark, encompassing a diverse range of math word problems at the Chinese elementary school level. It is specifically designed to assess the mathematical reasoning capabilities of Chinese large language models. However, these language models are typically trained on a huge amount of data derived from multiple sources, including web pages, books, codes and so on. This raises the potential risk of test data contamination, as some test examples from FineMath may unintentionally be included in the training data of these language models. Test data contamination can lead to an overestimation of a model\u2019s performance, potentially resulting in misleading conclusions regarding the model\u2019s generalization capabilities. Consequently, investigating contamination and its impact on model performance for FineMath is of paramount importance.\nApe210K (Zhao et al., 2020  ###reference_b25###) is a publicly available large-scale Chinese math word problem dataset, which has been splited into training, validation, and test sets. It is commonly utilized as a training dataset for mathematical problem-solving models (Hu and Jiang, 2022  ###reference_b6###; Wu et al., 2021  ###reference_b22###; Liang et al., 2023  ###reference_b10###; Huang et al., 2021  ###reference_b7###; Xiong et al., 2022  ###reference_b23###; Huang et al., 2023  ###reference_b8###; Yang et al., 2023  ###reference_b24###; Liang et al., 2022  ###reference_b11###). To determine potential contamination from Ape210K in FineMath, we adopt the identical methodology leveraged in GPT-3 (Brown et al., 2020  ###reference_b1###) to compute the n-gram overlap between Ape210K and FineMath. In this approach, a test example in FineMath is considered as an overlapped example with Ape210k if any n-gram from this test example also appears in Ape210k. Specifically, we insert white spaces around any Chinese, Japanese, and Korean (CJK) characters, as well as between punctuation marks and words. Subsequently, we tokenize the text based on these white spaces. It is important to note that we disregard letter case when computing n-grams.\nTo perform a rigorous quantitative assessment of contamination, we define the overlap rate as the fraction of instances within FineMath that exhibit such overlap. Furthermore, for the purposes of computing overlap, we set the value of n to 13. The overlap rate between FineMath and the training sets of Ape210k is depicted in Figure 2  ###reference_###. It suggests that the overlap rates of some question types are significantly higher than others, such as Basic Geometry and Proportions. To gain deeper insights into the impact of these overlapped examples on model performance, we partition the test examples into two datasets: a contaminated dataset composed of the overlapped examples, and a clean dataset whose test examples exhibit no overlap with the Ape210k training set. Subsequently, we examine the performance of the model on each of these datasets separately. We select GPT-4 and MathGLM-10B for analysis since GPT-4 is widely recognized as the most advanced LLM currently available and MathGLM-10B has been trained on the Ape210k training set which overlaps with some test examples of FineMath. The experimental results are presentend in Table 3  ###reference_###. Notably, MathGLM-10B performs significantly better on the contaminated dataset compared to the clean dataset. In contrast, GPT-4 exhibits comparable performance on both datasets. This suggests that MathGLM-10B may be overfitting to the overlapped examples and that contamination can inflate a model\u2019s performance. Consequently, to ensure a fair comparison between models and to draw accurate conclusions from the FineMath benchmark, we recommend filtering out overlapped examples between the training set and the FineMath benchmark."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Experiments",
            "text": "We conducted experiments on the proposed FineMath to evaluate a series of LLMs, assessing the mathematical reasoning capabilities of them."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "5.1.   Evaluated LLMs",
            "text": "We assessed three classes of LLMs: GPT-4 and GPT-3.5-Turbo developed by OpenAI; LLMs developed for Chinese; and LLMs finetuned with Chinese mathematics data. Specific information can be found in Table 4  ###reference_###."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "5.2.   Prompts",
            "text": "All experiments were conducted under the zero-shot. We tried several prompts for evaluation and analysis, which are shown in Table 5  ###reference_###."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "5.3.   Main Results",
            "text": "###figure_3### ###figure_4### The overall accuracy results of assessed LLMs are visualized in Figure 3  ###reference_###. GPT-4 and GPT-3.5-turbo perform outstandingly, with their accuracies reaching as high as 73% and 62%, respectively.\nAmong the evaluated Chinese LLMs, MathGLM-10B, MathGLM-335M, ChatGLM2-6B and Baichuan2-7B-Chat obtain an accuracy of > 40%. Qwen-7B-Chat and InternLM-Chat-7B are at a slightly below-average level. However, both Baichuan-7B and Moss-SFT-16B perform poorly on our dataset, with an accuracy of < 10%. Upon examining the responses generated by these two models, we find that their answers often stray from the MWPs, generating a lot of irrelevant content or repeatedly producing the same questions.\nBy considering both model accuracy and the detailed information provided in Table 4  ###reference_###, we deduce that the lower accuracy of Moss-SFT-16B is due to an insufficient amount of training data. The performance of Baichuan-7B is hampered because it has not undergone RLHF fine-tuning, which prevents the model from fully understanding the question. In contrast, the accuracy of Baichuan2-7B-Chat, which has been fine-tuned, has significantly improved. In summary, RLHF fine-tuning, having model parameters exceeding 6 billion, and training data reaching the trillion level are all crucial for training an LLM with problem-solving and reasoning capabilities."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "5.4.   Results across 17 MWP Categories",
            "text": "Results across the 17 MWP categories are displayed in Table 6  ###reference_###. It is evident that the MWP types \u201cCounting\u201d and \u201cProblem1\u201d are more challenging than other MWP categories according to the results. This could be due to the complexities involved in the counting of Chinese numerals and their conversion to Arabic numerals, and the common sense issues encountered in the optimization problem for \u201cProblem1\u201d. All models demonstrate better performance on \u201cMixed Operation\u201d than on other categories. We also observe that the performance of different models vary significantly.\nGPT-4 outperforms all other models. It achieves an accuracy below 40% on only two MWP categories, surpasses 80% on three MWP categories, exceeds 70% in six MWP categories, and goes beyond 60% in five MWP categories. GPT-4 scores the highest, 89%, on the \u2019Mixed Operation\u2019 MWPs, while it scores the lowest, 38%, on the \u2019Counting\u2019 MWPs. Notably, GPT-4 excels in Probability and Analytic Geometry, outperforming all other models with an improvement of over 25%. GPT-3.5-Turbo\u2019s performance on different MWP categories is similar to that of GPT-4, but there is a significant difference (over 20%) on Probability, Basic Geometry, and Analytic Geometry.\nMathGLM-10B and ChatGLM2-6B exhibit similar performance trends across most MWP categories. However, significant differences are observed on Fractions, Counting, Spatial Sense, Basic Geometry, Proportions, Problem1, and Problem2. Due to contamination, MathGLM-10B surpasses GPT-4 on Basic Geometry, Problem 1 and Problem 2.\nBaichuan2-7B-Chat, Qwen-7B-Chat and InternLM-Chat-7B are still at a medium or slightly below-average level. Both Moss-SFT-16B and Baichuan-7B perform poorly across all MWP categories, failing to achieve an accuracy of 20%.\n###table_1### \u5b66\u6821\u4e70\u6765120\u672c\u8bfe\u5916\u8bfb\u7269\uff0c\u5176\u4e2d\u768440%\u5206\u7ed9\u9ad8\u5e74\u7ea7\uff0c\u5269\u4e0b\u7684\u8bfe\u5916\u8bfb\u7269\u63095:3\u5206\u7ed9\u4e2d\u5e74\u7ea7\u548c\u4f4e\u5e74\u7ea7\uff0c\u4f4e\u5e74\u7ea7\u5206\u5230\u591a\u5c11\u672c?\nThe school bought 120 supplementary books, 40% of which were given to the upper grades. The remaining books were divided between the middle and early grades in a ratio of 5:3. How many books did the early grades receive?\nGPT-4 Response: \u9ad8\u5e74\u7ea7\u5206\u5230\u7684\u8bfe\u5916\u8bfb\u7269\u4e3a120*40%=48\u672c;\u5269\u4e0b\u7684\u8bfe\u5916\u8bfb\u7269\u4e3a120-48=72\u672c;\u4f4e\u5e74\u7ea7\u5206\u5230\u7684\u8bfe\u5916\u8bfb\u7269\u4e3a72*3/(5+3)=27\u672c\u3002\u7b54\uff1a\u4f4e\u5e74\u7ea7\u5206\u523027\u672c\u3002 Results: \\usym2714\nGPT-4 Response: The number of supplementary books allocated to the upper grades is 120*40%=48 books. The remaining books total 120-48=72 books. The supplementary books allocated to the early grades are calculated as 72*3/(5+3)=27 books. Answer: The early grades are allocated 27 books.\nGPT-4 Response: 36\u672c Results: \\usym2717\nGPT-4 Response: 36 books\nGPT-4 Response: 36\u672c\u3002\u9996\u5148\uff0c\u5b66\u6821\u5c06120\u672c\u4e66\u768440%\u5206\u7ed9\u9ad8\u5e74\u7ea7\uff0c\u5373120*40%=48\u672c\u3002\u5269\u4e0b\u7684\u4e66\u5c31\u662f120-48=72\u672c\u3002\u7136\u540e\uff0c\u8fd972\u672c\u4e66\u6309\u71675:3\u7684\u6bd4\u4f8b\u5206\u7ed9\u4e2d\u5e74\u7ea7\u548c\u4f4e\u5e74\u7ea7\u3002\u8fd9\u4e2a\u6bd4\u4f8b\u7684\u603b\u548c\u662f5+3=8\uff0c\u6240\u4ee5\u4f4e\u5e74\u7ea7\u5206\u5230\u7684\u4e66\u5c31\u662f72*(3/8)=27\u672c\u3002Results: \\usym2717\nGPT-4 Response: 36 books. Firstly, the school allocates 40% of the 120 books to the upper grades. This totals to 120*40%=48 books. The remaining books amount to 120-48 = 72 books. Then, these 72 books are divided between the middle and early grades at a ratio of 5:3. The total of this ratio is 5+3=8. Therefore, the number of books allocated to the early grades is 72*(3/8) =27 books.\nGPT-4 Response: 36\u672c Results: \\usym2717\nGPT-4 Response: 36 books\nGPT-4 Response: C:27\nGPT-4 Response: C:27\nResults: \\usym2714\nGPT-4 Response: D:36\nGPT-4 Response: D:36\nResults: \\usym2717"
        },
        {
            "section_id": "5.5",
            "parent_section_id": "5",
            "section_name": "5.5.   Results in terms of the Number of Mathematical Reasoning Steps",
            "text": "We further conducted experiments to evaluate LLMs reasoning ability in terms of the number of mathematical reasoning steps. Results are shown in Figure 4  ###reference_###. We observe that the performance of LLMs decreases as the number of reasoning steps increases. GPT-4 maintains an accuracy of over 60% at all difficulty levels, reaching as high as 82% on MWPs that require only one step of reasoning. The accuracy of GPT-3.5-Turbo is, on average, 10% lower than that of GPT-4. While ChatGLM2-6B, Baichuan2-7B-Chat and Qwen-7B-Chat outperform MathGLM-335M and MathGLM-10B on Level-1 MWPs, its accuracy falls below those of MathGLM-335M and MathGLM-10B on Level-2/3 MWPs. Similar to their performance across MWP categories, Moss-SFT-16B and Baichuan-7B show a significant difference in performance at all difficulty levels compared to the other models.\nThe accuracy difference between Qwen-7B-Chat and InternLM-Chat-7B on different reasoning steps is quite substantial, exceeding 30%. In the case of Qwen-7B-Chat, the accuracy on problems requiring only one-step reasoning is 62%, but this figure drops to just 21% for problems requiring three or more reasoning steps. This phenomenon suggests that the model may need more training in terms of inference."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Analysis",
            "text": "Unlike other studies that only evaluate accuracy, we have further analyzed factors in the evaluation process. These overlooked factors greatly affect the evaluation results and our understanding of the true mathematical reasoning capabilities of LLMs."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "6.1.   Prompts Really Does Matter",
            "text": "During the evaluation, instructions are generally used to guide the assessed model to produce answers. For instance, we might say, \u201cHere is a math problem, please provide the answer to this question. Do not explain the reason.\u201d Alternatively, we might provide an answer template such as \u201cQuestion: \u2026 Answer:\u201d. However, our experiments showed that even a single word like \u201cAnswer:\u201d can significantly affect the model\u2019s accuracy. We tested three prompt 0, prompt 1 and Prompt4 on GPT-4, results are shown in Table 7  ###reference_###.\nWe can see that the overall accuracy results with the three prompts are 73%, 59%, and 58%, respectively, with a gap reaching up to 15%.\nPrompt like \u201cAnswer:\u201d appear to encourage the model to forego reasoning and directly provide the answer, which increases the likelihood of generating incorrect responses. An example is shown in Table 8  ###reference_###: Prompt 4+Question."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "6.2.   Evaluation Methods: Generation vs. Option Prediction",
            "text": "In our preliminary experiments, we have discovered that some newly developed LLMs do not follow instructions well, often generating large chunks of tokens unrelated to the answer. Therefore, we decide to transform our data into multiple-choice questions, for which the evaluated model can then select the correct answer option.\nComparison results are displayed in Table 9  ###reference_###. We can observe a significant difference in accuracy between option prediction (with multiple-choice questions) and direct answer generation, with a gap that can exceed 10%. Interestingly, restructuring the task in the form of multiple-choice questions seems to reduce the accuracy of high-performing models while increasing the accuracy of models that perform poorly. Upon examining instances, we have found that the answer option can act as another type of prompt influencing the model\u2019s performance. For example in Table 8  ###reference_###: Prompt 0+Question (with MCQs) and Prompt 0+Question (with D replaced).\nExamples of GPT-4 outputs with different prompts and task forms are shown in Table 8  ###reference_###. We want to understand why GPT-4 would provide the incorrect answer \u201c36\u201d under Prompt 1. Therefore, we utilize Prompt 3 to have GPT-4 explain its reasoning for choosing \u201c36\u201d. Interestingly, GPT-4 mentions the correct number \u201c27\u201d in the explanation, but still provides the incorrect answer, \u201c36\u201d. Given the seeming importance of \u201c36\u201d, we replace one of the options in the multiple-choice question with \u201c36\u201d. The model, which initially selected the correct answer, abandoned it in favor of \u201c36\u201d. Further examples can be observed in the responses generated by other models. For instance, the reasoning process may be incorrect, yet the correct answer is ultimately chosen. The options provided to the model also seem to influence the model\u2019s generation probability to a certain degree. Therefore, we recommend using generation, rather than option prediction, for a more accurate evaluation of LLMs."
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "6.3.   Comparison of Response Lengths",
            "text": "We conducted a statistical analysis on the length of the responses generated by the models. We discover two phenomena. First, models like GPT-4 and GPT-3.5-Turbo tend to generate responses that are closely centered around the question, with shorter text. This may demonstrate the characteristics of models with high accuracy. Second, the more reasoning steps an MWP requires, the longer the response tends to be. Please refer to Table 10  ###reference_### for more details.\nWe speculate that the model\u2019s \u201cconfidence\u201d in answering questions influences the length of its response. This tendency is also observed in some models that do not adhere strictly to instructions. For instance, even when instructed to provide only the answer without explaining, these models still generate logical reasoning for difficult MWPs."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7.   Conclusion",
            "text": "We present a fine-grained benchmark, FineMath, to comprehensively evaluate the mathematical capabilities of Chinese LLMs. We strive to evaluate as many LLMs as possible. We also conduct a contamination analysis, enabling researchers to examine whether the training data influences the evaluation results. Through testing various evaluation methods and processes, we demonstrate their potential to influence the results, underscoring the necessity for fair and effective evaluations to consider the interference caused by these two aspects."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "8.   Bibliographical References",
            "text": ""
        }
    ],
    "appendix": [],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S1.T1\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S1.T1.1\" style=\"width:433.6pt;height:189.5pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(9.2pt,-4.0pt) scale(1.04430597562559,1.04430597562559) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S1.T1.1.1\">\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S1.T1.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S1.T1.1.1.1.1.1\">Test Sets</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S1.T1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S1.T1.1.1.1.2.1\">Size</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S1.T1.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S1.T1.1.1.1.3.1\">Language</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S1.T1.1.1.2.1\">AddSub <cite class=\"ltx_cite ltx_citemacro_citep\">(Hosseini et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07747v1#bib.bib5\" title=\"\">2014</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S1.T1.1.1.2.2\">395</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S1.T1.1.1.2.3\">En</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S1.T1.1.1.3.1\">MultiArith <cite class=\"ltx_cite ltx_citemacro_citep\">(Roy and Roth, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07747v1#bib.bib18\" title=\"\">2015</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.1.3.2\">600</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.1.3.3\">En</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S1.T1.1.1.4.1\">SingleEq <cite class=\"ltx_cite ltx_citemacro_citep\">(Koncel-Kedziorski et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07747v1#bib.bib9\" title=\"\">2015</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.1.4.2\">508</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.1.4.3\">En</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S1.T1.1.1.5.1\">AQUA <cite class=\"ltx_cite ltx_citemacro_citep\">(Ling et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07747v1#bib.bib12\" title=\"\">2017</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.1.5.2\">100K</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.1.5.3\">En</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S1.T1.1.1.6.1\">AsDiv <cite class=\"ltx_cite ltx_citemacro_citep\">(Miao et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07747v1#bib.bib13\" title=\"\">2020</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.1.6.2\">2,305</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.1.6.3\">En</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S1.T1.1.1.7.1\">GSM8K <cite class=\"ltx_cite ltx_citemacro_citep\">(Cobbe et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07747v1#bib.bib2\" title=\"\">2021</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.1.7.2\">8.5k</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.1.7.3\">En</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"S1.T1.1.1.8.1\">SVAMP <cite class=\"ltx_cite ltx_citemacro_citep\">(Patel et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07747v1#bib.bib17\" title=\"\">2021</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.1.8.2\">8.5k</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.1.8.3\">En</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S1.T1.1.1.9.1\">CMATH <cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07747v1#bib.bib21\" title=\"\">2023</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S1.T1.1.1.9.2\">1.7K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S1.T1.1.1.9.3\">Zh</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.10\">\n<td class=\"ltx_td ltx_align_left\" id=\"S1.T1.1.1.10.1\">K6 <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07747v1#bib.bib24\" title=\"\">2023</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.1.10.2\">600</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.1.10.3\">Zh</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.11\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S1.T1.1.1.11.1\">FineMath (ours)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S1.T1.1.1.11.2\">1,584</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S1.T1.1.1.11.3\">Zh</td>\n</tr>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span> An overview of MWP datasets.</figcaption>\n</figure>",
            "capture": "Table 1:  An overview of MWP datasets."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T2\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T2.1\" style=\"width:433.6pt;height:391.5pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(52.3pt,-47.2pt) scale(1.31804696705212,1.31804696705212) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.1.1\">\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S4.T2.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.1.1\">Concepts</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S4.T2.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.2.1\">Type</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.3.1\">Total</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.4.1\">Level-1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.5.1\">Level-2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.1.1.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.6.1\">Level-3</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.2.1\" rowspan=\"7\"><span class=\"ltx_text\" id=\"S4.T2.1.1.2.1.1\">Number &amp; Operations</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.2.2\">Percents</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.2.3\">60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.2.4\">20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.2.5\">20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.2.6\">20</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.3.1\">Decimals</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.3.2\">93</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.3.3\">23</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.3.4\">30</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.3.5\">40</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.4.1\">Fractions</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.4.2\">81</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.4.3\">24</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.4.4\">27</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.4.5\">30</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.5.1\">Fac&amp;Multi</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.5.2\">61</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.5.3\">20</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.5.4\">21</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.5.5\">20</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.6.1\">Counting</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.6.2\">60</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.6.3\">20</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.6.4\">20</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.6.5\">20</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.7.1\">Proportions</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.7.2\">78</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.7.3\">20</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.7.4\">20</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.7.5\">38</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.8.1\">Mix Operations</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.8.2\">267</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.8.3\">91</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.8.4\">107</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.8.5\">69</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.9.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.1.1.9.1.1\">Measurement</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.9.2\">Spatial Sense</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.9.3\">89</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.9.4\">20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.9.5\">47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.9.6\">22</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.10\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.10.1\">Time</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.10.2\">64</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.10.3\">20</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.10.4\">24</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.10.5\">20</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.11\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.11.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.1.1.11.1.1\">Data Analysis &amp; Probability</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.11.2\">Central Tendency</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.11.3\">189</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.11.4\">22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.11.5\">98</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.11.6\">69</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.12\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.12.1\">Probability</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.12.2\">68</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.12.3\">28</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.12.4\">20</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.12.5\">20</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.13\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.13.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.1.1.13.1.1\">Algebra</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.13.2\">Equations</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.13.3\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.13.4\">0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.13.5\">25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.13.6\">75</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.14\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.14.1\">Patterns</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.14.2\">60</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.14.3\">20</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.14.4\">20</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.14.5\">20</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.15\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.15.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.1.1.15.1.1\">Geometry</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.15.2\">Basic Geometry</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.15.3\">132</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.15.4\">20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.15.5\">48</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.15.6\">64</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.16\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.16.1\">Analytic Geometry</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.16.2\">60</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.16.3\">20</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.16.4\">20</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.16.5\">20</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.17\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S4.T2.1.1.17.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.1.1.17.1.1\">Others</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.17.2\">Problem 1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.17.3\">62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.17.4\">20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.17.5\">21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.17.6\">21</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.18\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T2.1.1.18.1\">Problem 2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.1.1.18.2\">60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.1.1.18.3\">20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.1.1.18.4\">20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.1.1.18.5\">20</td>\n</tr>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span> Overall statistics of FineMath. Level-1/2/3 denotes that a math word problem requires 1/2/3+ mathematical reasoning steps to solve. </figcaption>\n</figure>",
            "capture": "Table 2:  Overall statistics of FineMath. Level-1/2/3 denotes that a math word problem requires 1/2/3+ mathematical reasoning steps to solve. "
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T3\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T3.1\" style=\"width:433.6pt;height:132.9pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(82.2pt,-25.2pt) scale(1.61121833584306,1.61121833584306) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T3.1.1\">\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S4.T3.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1\">Model/Dataset</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.2.1\">Level-1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.3.1\">Level-2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.4.1\">Level-3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.5.1\">Overall</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T3.1.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.2.1.1\">MathGLM-10B (Clean)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.2.2\">0.45</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.2.3\">0.42</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.2.4\">0.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.2.5\">0.37</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.1.1.3.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.3.1.1\">MathGLM-10B (Contaminated)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.3.2\">0.65</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.3.3\">0.75</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.3.4\">0.70</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.3.5\">0.71</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T3.1.1.4.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.4.1.1\">GPT-4 (Clean)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.4.2\">0.83</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.4.3\">0.76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.4.4\">0.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.4.5\">0.74</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T3.1.1.5.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.5.1.1\">GPT-4 (Contaminated)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.1.1.5.2\">0.83</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.1.1.5.3\">0.65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.1.1.5.4\">0.63</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.1.1.5.5\">0.67</td>\n</tr>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span> Accuracy results of GPT-4 and MathGLM-10B on the contaminated dataset and clean dataset.</figcaption>\n</figure>",
            "capture": "Table 3:  Accuracy results of GPT-4 and MathGLM-10B on the contaminated dataset and clean dataset."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T4\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S5.T4.1\" style=\"width:433.6pt;height:337.9pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(100.4pt,-78.2pt) scale(1.8616921650079,1.8616921650079) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T4.1.1\">\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S5.T4.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.1.1.1\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.1.2.1\">RLHF</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.1.3.1\">Parameters</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.1.4.1\">Training Token</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T4.1.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.2.1.1\">GPT-4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.1.2.2\">w</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.1.2.3\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.1.2.4\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.3.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.3.1.1\">GPT-3.5-Turbo</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.1.3.2\">w</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.1.3.3\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.1.3.4\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.4.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.4.1.1\">GhatGLM2-6B</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.1.4.2\">w</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.1.4.3\">6B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.1.4.4\">1.4T</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.5.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.5.1.1\">Moss-SFT-16B</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.1.5.2\">w/o</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.1.5.3\">16B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.1.5.4\">120B</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.6.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.6.1.1\">InternLM-Chat-7B</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.1.6.2\">w</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.1.6.3\">7B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.1.6.4\">1.6T</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.7.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.7.1.1\">Qwen-7B-Chat</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.1.7.2\">w</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.1.7.3\">7B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.1.7.4\">2.4T</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.8.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.8.1.1\">Baichuan-7B</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.1.8.2\">w/o</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.1.8.3\">7B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.1.8.4\">1.2T</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.9\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.9.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.9.1.1\">Baichuan2-7B-Chat</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.1.9.2\">w</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.1.9.3\">7B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.1.9.4\">2.6T</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.10\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.10.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.10.1.1\">MathGLM-10B</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.1.10.2\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.1.10.3\">10B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.1.10.4\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.11\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T4.1.1.11.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.11.1.1\">MathGLM-335M</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.1.1.11.2\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.1.1.11.3\">335M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.1.1.11.4\">-</td>\n</tr>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span> All the LLMs that we evaluated in this paper. MathGLM-10B and MathGLM-335M, both of which were fine-tuned using an arithmetic training dataset.</figcaption>\n</figure>",
            "capture": "Table 4:  All the LLMs that we evaluated in this paper. MathGLM-10B and MathGLM-335M, both of which were fine-tuned using an arithmetic training dataset."
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T5\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S5.T5.1\" style=\"width:433.6pt;height:153.8pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(30.7pt,-10.9pt) scale(1.16486044840068,1.16486044840068) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T5.1.1\">\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S5.T5.1.1.1.1\"><span class=\"ltx_text\" id=\"S5.T5.1.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.1.1.1.1\">Prompt 0:</span></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S5.T5.1.1.1.2\"><span class=\"ltx_text\" id=\"S5.T5.1.1.1.2.1\">Nothing is provided, only the question is input into the model.</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T5.1.1.2.1\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.2.1.1\">Prompt 1:</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T5.1.1.2.2\">Here is a math word problem; please provide the answer to this question.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T5.1.1.3.1\">Do not explain the reason.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T5.1.1.4.1\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.4.1.1\">Prompt 2:</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T5.1.1.4.2\">Here is a math word problem; please select the correct option.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T5.1.1.5.1\">Do not explain the reason.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T5.1.1.6.1\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.6.1.1\">Prompt 3:</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T5.1.1.6.2\">Here is a math word problem, please give the answer to this question.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T5.1.1.7.1\">And explain why.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S5.T5.1.1.8.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.8.1.1\">Prompt 4:</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S5.T5.1.1.8.2\"><span class=\"ltx_text\" id=\"S5.T5.1.1.8.2.1\">Answer:\u2026.</span></td>\n</tr>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span> Prompts used for evaluation and analysis.</figcaption>\n</figure>",
            "capture": "Table 5:  Prompts used for evaluation and analysis."
        },
        "6": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T6\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S5.T6.1\" style=\"width:455.2pt;height:114pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-167.6pt,42.0pt) scale(0.575993825150638,0.575993825150638) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T6.1.1\">\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S5.T6.1.1.1.1\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.1.1.1\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T6.1.1.1.2\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.1.2.1\">Percents</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T6.1.1.1.3\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.1.3.1\">Decimals</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T6.1.1.1.4\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.1.4.1\">Fractions</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T6.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.1.5.1\">Factors &amp;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T6.1.1.1.6\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.1.6.1\">Counting</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T6.1.1.1.7\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.1.7.1\">Proportions</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T6.1.1.1.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.1.8.1\">Mix</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T6.1.1.1.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.1.9.1\">Spatial</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T6.1.1.1.10\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.1.10.1\">Time</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T6.1.1.1.11\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.1.11.1\">Central</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T6.1.1.1.12\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.1.12.1\">Probability</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T6.1.1.1.13\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.1.13.1\">Equations</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T6.1.1.1.14\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.1.14.1\">Patterns</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T6.1.1.1.15\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.1.15.1\">Basic</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T6.1.1.1.16\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.1.16.1\">Analytic</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T6.1.1.1.17\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.1.17.1\">Problem 1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T6.1.1.1.18\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.1.18.1\">Problem 2</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.2.1.1\">Multiples</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.2.2.1\">Operations</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.2.3.1\">Sense</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.2.4.1\">Tendency</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.2.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.2.5.1\">Geometry</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.2.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.2.6.1\">Geometry</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T6.1.1.3.1\">GPT-4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.3.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.3.2.1\">0.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.3.3.1\">0.76</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.3.4\">0.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.3.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.3.5.1\">0.74</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.3.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.3.6.1\">0.38</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.3.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.3.7.1\">0.78</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.3.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.3.8.1\">0.89</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.3.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.3.9.1\">0.71</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.3.10\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.3.10.1\">0.77</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.3.11\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.3.11.1\">0.87</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.3.12\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.3.12.1\">0.68</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.3.13\">0.64</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.3.14\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.3.14.1\">0.68</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.3.15\">0.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.3.16\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.3.16.1\">0.68</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.3.17\">0.39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.3.18\">0.55</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T6.1.1.4.1\">GPT-3.5-Turbo</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.4.2\">0.75</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.4.3\">0.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.4.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.4.4.1\">0.69</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.4.5\">0.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.4.6\">0.33</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.4.7\">0.65</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.4.8\">0.81</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.4.9\">0.57</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.4.10\">0.69</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.4.11\">0.71</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.4.12\">0.34</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.4.13\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.4.13.1\">0.71</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.4.14\">0.47</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.4.15\">0.49</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.4.16\">0.42</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.4.17\">0.23</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.4.18\">0.55</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T6.1.1.5.1\">ChatGLM2-6B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.5.2\">0.65</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.5.3\">0.46</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.5.4\">0.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.5.5\">0.52</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.5.6\">0.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.5.7\">0.33</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.5.8\">0.63</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.5.9\">0.63</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.5.10\">0.38</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.5.11\">0.35</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.5.12\">0.24</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.5.13\">0.38</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.5.14\">0.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.5.15\">0.46</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.5.16\">0.18</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.5.17\">0.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.5.18\">0.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T6.1.1.6.1\">Moss-SFT-16B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.6.2\">0.13</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.6.3\">0.05</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.6.4\">0.07</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.6.5\">0.11</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.6.6\">0.03</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.6.7\">0.06</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.6.8\">0.15</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.6.9\">0.06</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.6.10\">0.09</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.6.11\">0.03</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.6.12\">0.12</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.6.13\">0.05</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.6.14\">0.07</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.6.15\">0.05</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.6.16\">0.03</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.6.17\">0.08</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.6.18\">0.05</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T6.1.1.7.1\">InternLM-Chat-7B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.7.2\">0.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.7.3\">0.35</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.7.4\">0.15</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.7.5\">0.34</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.7.6\">0.05</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.7.7\">0.31</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.7.8\">0.42</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.7.9\">0.36</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.7.10\">0.23</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.7.11\">0.17</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.7.12\">0.18</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.7.13\">0.23</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.7.14\">0.22</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.7.15\">0.18</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.7.16\">0.25</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.7.17\">0.21</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.7.18\">0.32</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T6.1.1.8.1\">Qwen-7B-Chat</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.8.2\">0.58</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.8.3\">0.53</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.8.4\">0.41</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.8.5\">0.39</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.8.6\">0.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.8.7\">0.32</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.8.8\">0.58</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.8.9\">0.42</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.8.10\">0.28</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.8.11\">0.31</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.8.12\">0.35</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.8.13\">0.34</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.8.14\">0.32</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.8.15\">0.38</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.8.16\">0.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.8.17\">0.18</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.8.18\">0.25</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.9\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T6.1.1.9.1\">Baichuan-7B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.9.2\">0.08</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.9.3\">0.13</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.9.4\">0.01</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.9.5\">0.08</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.9.6\">0.05</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.9.7\">0.03</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.9.8\">0.14</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.9.9\">0.08</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.9.10\">0.05</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.9.11\">0.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.9.12\">0.12</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.9.13\">0.06</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.9.14\">0.07</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.9.15\">0.07</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.9.16\">0.07</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.9.17\">0.05</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.9.18\">0.08</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.10\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T6.1.1.10.1\">Baichuan2-7B-Chat</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.10.2\">0.58</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.10.3\">0.52</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.10.4\">0.48</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.10.5\">0.54</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.10.6\">0.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.10.7\">0.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.10.8\">0.65</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.10.9\">0.58</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.10.10\">0.25</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.10.11\">0.32</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.10.12\">0.26</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.10.13\">0.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.10.14\">0.25</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.10.15\">0.37</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.10.16\">0.13</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.10.17\">0.11</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.10.18\">0.22</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.11\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T6.1.1.11.1\">MathGLM-10B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.11.2\">0.52</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.11.3\">0.42</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.11.4\">0.68</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.11.5\">0.57</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.11.6\">0.05</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.11.7\">0.64</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.11.8\">0.61</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.11.9\">0.38</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.11.10\">0.45</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.11.11\">0.53</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.11.12\">0.29</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.11.13\">0.36</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.11.14\">0.18</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.11.15\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.11.15.1\">0.74</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.11.16\">0.23</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.11.17\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.11.17.1\">0.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.11.18\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.11.18.1\">0.65</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.12\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T6.1.1.12.1\">MathGLM-335M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T6.1.1.12.2\">0.45</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T6.1.1.12.3\">0.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T6.1.1.12.4\">0.63</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T6.1.1.12.5\">0.49</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T6.1.1.12.6\">0.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T6.1.1.12.7\">0.47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T6.1.1.12.8\">0.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T6.1.1.12.9\">0.34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T6.1.1.12.10\">0.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T6.1.1.12.11\">0.47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T6.1.1.12.12\">0.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T6.1.1.12.13\">0.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T6.1.1.12.14\">0.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T6.1.1.12.15\">0.68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T6.1.1.12.16\">0.23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T6.1.1.12.17\">0.39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T6.1.1.12.18\">0.58</td>\n</tr>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 6: </span> Results across the 17 MWP categories (under Prompt 0).</figcaption>\n</figure>",
            "capture": "Table 6:  Results across the 17 MWP categories (under Prompt 0)."
        },
        "7": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T7\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S5.T7.1\" style=\"width:433.6pt;height:167.7pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(131.5pt,-50.9pt) scale(2.54156489604416,2.54156489604416) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T7.1.1\">\n<tr class=\"ltx_tr\" id=\"S5.T7.1.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S5.T7.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.1.1.1.1.1\">Prompt</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T7.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.1.1.1.2.1\">Level-1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T7.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.1.1.1.3.1\">Level-2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T7.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.1.1.1.4.1\">Level-3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T7.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.1.1.1.5.1\">Overall</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.1.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T7.1.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.1.1.2.1.1\">Prompt 0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T7.1.1.2.2\">0.83</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T7.1.1.2.3\">0.77</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T7.1.1.2.4\">0.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T7.1.1.2.5\">0.73</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.1.1.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T7.1.1.3.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.1.1.3.1.1\">Prompt 1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T7.1.1.3.2\">0.81</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T7.1.1.3.3\">0.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T7.1.1.3.4\">0.34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T7.1.1.3.5\">0.59</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.1.1.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S5.T7.1.1.4.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.1.1.4.1.1\">Prompt 4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T7.1.1.4.2\">0.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T7.1.1.4.3\">0.65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T7.1.1.4.4\">0.36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T7.1.1.4.5\">0.58</td>\n</tr>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 7: </span> Accuracy results of GPT-4 with Prompt 0, Prompt 1, Prompt 4.</figcaption>\n</figure>",
            "capture": "Table 7:  Accuracy results of GPT-4 with Prompt 0, Prompt 1, Prompt 4."
        },
        "8": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T8\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S5.T8.1\">\n<tr class=\"ltx_tr\" id=\"S5.T8.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S5.T8.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.1.1.1.1\" style=\"font-size:90%;\">Question</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" id=\"S5.T8.1.1.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T8.1.1.2.1\"><span class=\"ltx_text\" id=\"S5.T8.1.1.2.1.1\" style=\"font-size:90%;\">\u5b66\u6821\u4e70\u6765120\u672c\u8bfe\u5916\u8bfb\u7269\uff0c\u5176\u4e2d\u768440%\u5206\u7ed9\u9ad8\u5e74\u7ea7\uff0c\u5269\u4e0b\u7684\u8bfe\u5916\u8bfb\u7269\u63095:3\u5206\u7ed9\u4e2d\u5e74\u7ea7\u548c\u4f4e\u5e74\u7ea7\uff0c\u4f4e\u5e74\u7ea7\u5206\u5230\u591a\u5c11\u672c?</span></p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" id=\"S5.T8.1.1.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T8.1.1.3.1\"><span class=\"ltx_text\" id=\"S5.T8.1.1.3.1.1\" style=\"font-size:90%;\">The school bought 120 supplementary books, 40% of which were given to the upper grades. The remaining books were divided between the middle and early grades in a ratio of 5:3. How many books did the early grades receive?</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T8.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.1.2.1.1\" style=\"font-size:90%;\">Prompt 0+Question</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T8.1.2.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T8.1.2.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.1.2.2.1.1\" style=\"font-size:90%;\">GPT-4 Response:</span><span class=\"ltx_text\" id=\"S5.T8.1.2.2.1.2\" style=\"font-size:90%;\"> \u9ad8\u5e74\u7ea7\u5206\u5230\u7684\u8bfe\u5916\u8bfb\u7269\u4e3a120*40%=48\u672c;\u5269\u4e0b\u7684\u8bfe\u5916\u8bfb\u7269\u4e3a120-48=72\u672c;\u4f4e\u5e74\u7ea7\u5206\u5230\u7684\u8bfe\u5916\u8bfb\u7269\u4e3a72*3/(5+3)=27\u672c\u3002\u7b54\uff1a\u4f4e\u5e74\u7ea7\u5206\u523027\u672c\u3002 </span><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.1.2.2.1.3\" style=\"font-size:90%;\">Results:</span><span class=\"ltx_text\" id=\"S5.T8.1.2.2.1.4\" style=\"font-size:90%;\"> </span><span class=\"ltx_ERROR undefined\" id=\"S5.T8.1.2.2.1.5\">\\usym</span><span class=\"ltx_text\" id=\"S5.T8.1.2.2.1.6\" style=\"font-size:90%;\">2714</span></p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T8.1.2.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T8.1.2.3.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.1.2.3.1.1\" style=\"font-size:90%;\">GPT-4 Response:</span><span class=\"ltx_text\" id=\"S5.T8.1.2.3.1.2\" style=\"font-size:90%;\"> The number of supplementary books allocated to the upper grades is 120*40%=48 books. The remaining books total 120-48=72 books. The supplementary books allocated to the early grades are calculated as 72*3/(5+3)=27 books. Answer: The early grades are allocated 27 books.</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.1.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T8.1.3.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.1.3.1.1\" style=\"font-size:90%;\">Prompt 1+Question</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T8.1.3.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T8.1.3.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.1.3.2.1.1\" style=\"font-size:90%;\">GPT-4 Response:</span><span class=\"ltx_text\" id=\"S5.T8.1.3.2.1.2\" style=\"font-size:90%;\"> 36\u672c </span><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.1.3.2.1.3\" style=\"font-size:90%;\">Results:</span><span class=\"ltx_text\" id=\"S5.T8.1.3.2.1.4\" style=\"font-size:90%;\"> </span><span class=\"ltx_ERROR undefined\" id=\"S5.T8.1.3.2.1.5\">\\usym</span><span class=\"ltx_text\" id=\"S5.T8.1.3.2.1.6\" style=\"font-size:90%;\">2717</span></p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T8.1.3.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T8.1.3.3.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.1.3.3.1.1\" style=\"font-size:90%;\">GPT-4 Response:</span><span class=\"ltx_text\" id=\"S5.T8.1.3.3.1.2\" style=\"font-size:90%;\"> 36 books</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.1.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T8.1.4.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.1.4.1.1\" style=\"font-size:90%;\">Prompt 3+Question</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T8.1.4.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T8.1.4.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.1.4.2.1.1\" style=\"font-size:90%;\">GPT-4 Response:</span><span class=\"ltx_text\" id=\"S5.T8.1.4.2.1.2\" style=\"font-size:90%;\"> 36\u672c\u3002\u9996\u5148\uff0c\u5b66\u6821\u5c06120\u672c\u4e66\u768440%\u5206\u7ed9\u9ad8\u5e74\u7ea7\uff0c\u5373120*40%=48\u672c\u3002\u5269\u4e0b\u7684\u4e66\u5c31\u662f120-48=72\u672c\u3002\u7136\u540e\uff0c\u8fd972\u672c\u4e66\u6309\u71675:3\u7684\u6bd4\u4f8b\u5206\u7ed9\u4e2d\u5e74\u7ea7\u548c\u4f4e\u5e74\u7ea7\u3002\u8fd9\u4e2a\u6bd4\u4f8b\u7684\u603b\u548c\u662f5+3=8\uff0c\u6240\u4ee5\u4f4e\u5e74\u7ea7\u5206\u5230\u7684\u4e66\u5c31\u662f72*(3/8)=27\u672c\u3002</span><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.1.4.2.1.3\" style=\"font-size:90%;\">Results:</span><span class=\"ltx_text\" id=\"S5.T8.1.4.2.1.4\" style=\"font-size:90%;\"> </span><span class=\"ltx_ERROR undefined\" id=\"S5.T8.1.4.2.1.5\">\\usym</span><span class=\"ltx_text\" id=\"S5.T8.1.4.2.1.6\" style=\"font-size:90%;\">2717</span></p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T8.1.4.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T8.1.4.3.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.1.4.3.1.1\" style=\"font-size:90%;\">GPT-4 Response:</span><span class=\"ltx_text\" id=\"S5.T8.1.4.3.1.2\" style=\"font-size:90%;\"> 36 books. Firstly, the school allocates 40% of the 120 books to the upper grades. This totals to 120*40%=48 books. The remaining books amount to 120-48 = 72 books. Then, these 72 books are divided between the middle and early grades at a ratio of 5:3. The total of this ratio is 5+3=8. Therefore, the number of books allocated to the early grades is 72*(3/8) =27 books.</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.1.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T8.1.5.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.1.5.1.1\" style=\"font-size:90%;\">Prompt 4+Question</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T8.1.5.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T8.1.5.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.1.5.2.1.1\" style=\"font-size:90%;\">GPT-4 Response:</span><span class=\"ltx_text\" id=\"S5.T8.1.5.2.1.2\" style=\"font-size:90%;\"> 36\u672c </span><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.1.5.2.1.3\" style=\"font-size:90%;\">Results:</span><span class=\"ltx_text\" id=\"S5.T8.1.5.2.1.4\" style=\"font-size:90%;\"> </span><span class=\"ltx_ERROR undefined\" id=\"S5.T8.1.5.2.1.5\">\\usym</span><span class=\"ltx_text\" id=\"S5.T8.1.5.2.1.6\" style=\"font-size:90%;\">2717</span></p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T8.1.5.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T8.1.5.3.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.1.5.3.1.1\" style=\"font-size:90%;\">GPT-4 Response:</span><span class=\"ltx_text\" id=\"S5.T8.1.5.3.1.2\" style=\"font-size:90%;\"> 36 books</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.1.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T8.1.6.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.1.6.1.1\" style=\"font-size:90%;\">Prompt 0+Question</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T8.1.6.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T8.1.6.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.1.6.2.1.1\" style=\"font-size:90%;\">GPT-4 Response:</span><span class=\"ltx_text\" id=\"S5.T8.1.6.2.1.2\" style=\"font-size:90%;\"> C:27</span></p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T8.1.6.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T8.1.6.3.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.1.6.3.1.1\" style=\"font-size:90%;\">GPT-4 Response:</span><span class=\"ltx_text\" id=\"S5.T8.1.6.3.1.2\" style=\"font-size:90%;\"> C:27</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.1.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T8.1.7.1\"><span class=\"ltx_text\" id=\"S5.T8.1.7.1.1\" style=\"font-size:90%;\">(with MCQs)</span></td>\n<td class=\"ltx_td\" id=\"S5.T8.1.7.2\"></td>\n<td class=\"ltx_td\" id=\"S5.T8.1.7.3\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.1.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T8.1.8.1\"><span class=\"ltx_text\" id=\"S5.T8.1.8.1.1\" style=\"font-size:90%;\">A:72 B:48 C:27 D:80</span></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S5.T8.1.8.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T8.1.8.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.1.8.2.1.1\" style=\"font-size:90%;\">Results:</span><span class=\"ltx_text\" id=\"S5.T8.1.8.2.1.2\" style=\"font-size:90%;\"> </span><span class=\"ltx_ERROR undefined\" id=\"S5.T8.1.8.2.1.3\">\\usym</span><span class=\"ltx_text\" id=\"S5.T8.1.8.2.1.4\" style=\"font-size:90%;\">2714</span></p>\n</td>\n<td class=\"ltx_td\" id=\"S5.T8.1.8.3\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.1.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T8.1.9.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.1.9.1.1\" style=\"font-size:90%;\">Prompt 0+Question</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T8.1.9.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T8.1.9.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.1.9.2.1.1\" style=\"font-size:90%;\">GPT-4 Response:</span><span class=\"ltx_text\" id=\"S5.T8.1.9.2.1.2\" style=\"font-size:90%;\"> D:36</span></p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T8.1.9.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T8.1.9.3.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.1.9.3.1.1\" style=\"font-size:90%;\">GPT-4 Response:</span><span class=\"ltx_text\" id=\"S5.T8.1.9.3.1.2\" style=\"font-size:90%;\"> D:36</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.1.10\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T8.1.10.1\">\n<span class=\"ltx_text\" id=\"S5.T8.1.10.1.1\" style=\"font-size:90%;\">(with </span><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.1.10.1.2\" style=\"font-size:90%;\">D</span><span class=\"ltx_text\" id=\"S5.T8.1.10.1.3\" style=\"font-size:90%;\"> replaced)</span>\n</td>\n<td class=\"ltx_td\" id=\"S5.T8.1.10.2\"></td>\n<td class=\"ltx_td\" id=\"S5.T8.1.10.3\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.1.11\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T8.1.11.1\">\n<span class=\"ltx_text\" id=\"S5.T8.1.11.1.1\" style=\"font-size:90%;\">A:72 B:48 C:27 </span><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.1.11.1.2\" style=\"font-size:90%;\">D:36</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S5.T8.1.11.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S5.T8.1.11.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.1.11.2.1.1\" style=\"font-size:90%;\">Results:</span><span class=\"ltx_text\" id=\"S5.T8.1.11.2.1.2\" style=\"font-size:90%;\"> </span><span class=\"ltx_ERROR undefined\" id=\"S5.T8.1.11.2.1.3\">\\usym</span><span class=\"ltx_text\" id=\"S5.T8.1.11.2.1.4\" style=\"font-size:90%;\">2717</span></p>\n</td>\n<td class=\"ltx_td ltx_border_bb\" id=\"S5.T8.1.11.3\"></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 8: </span>Different prompts and their corresponding responses from GPT-4. MCQs: multiple-choice questions. </figcaption>\n</figure>",
            "capture": "Table 8: Different prompts and their corresponding responses from GPT-4. MCQs: multiple-choice questions. "
        },
        "9": {
            "table_html": "<figure class=\"ltx_table\" id=\"S6.T9\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S6.T9.1\" style=\"width:433.6pt;height:501.1pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(95.4pt,-110.3pt) scale(1.78648328325393,1.78648328325393) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S6.T9.1.1\">\n<tr class=\"ltx_tr\" id=\"S6.T9.1.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S6.T9.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T9.1.1.1.1.1\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S6.T9.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T9.1.1.1.2.1\">MCQs</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S6.T9.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T9.1.1.1.3.1\">Level-1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S6.T9.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T9.1.1.1.4.1\">Level-2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S6.T9.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T9.1.1.1.5.1\">Level-3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S6.T9.1.1.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T9.1.1.1.6.1\">Overall</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T9.1.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T9.1.1.2.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S6.T9.1.1.2.1.1\">GPT-4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.2.2\">w/o</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.2.3\">0.83</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.2.4\">0.77</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.2.5\">0.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.2.6\">0.73</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T9.1.1.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T9.1.1.3.1\">w</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T9.1.1.3.2\">0.67</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T9.1.1.3.3\">0.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T9.1.1.3.4\">0.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T9.1.1.3.5\">0.62</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T9.1.1.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T9.1.1.4.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S6.T9.1.1.4.1.1\">GPT-3.5-Turbo</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.4.2\">w/o</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.4.3\">0.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.4.4\">0.65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.4.5\">0.51</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.4.6\">0.62</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T9.1.1.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T9.1.1.5.1\">w</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T9.1.1.5.2\">0.65</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T9.1.1.5.3\">0.66</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T9.1.1.5.4\">0.57</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T9.1.1.5.5\">0.63</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T9.1.1.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T9.1.1.6.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S6.T9.1.1.6.1.1\">ChatGLM2-6B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.6.2\">w/o</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.6.3\">0.58</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.6.4\">0.46</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.6.5\">0.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.6.6\">0.42</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T9.1.1.7\">\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T9.1.1.7.1\">w</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T9.1.1.7.2\">0.69</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T9.1.1.7.3\">0.62</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T9.1.1.7.4\">0.46</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T9.1.1.7.5\">0.58</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T9.1.1.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T9.1.1.8.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S6.T9.1.1.8.1.1\">Moss-SFT-16B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.8.2\">w/o</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.8.3\">0.18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.8.4\">0.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.8.5\">0.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.8.6\">0.08</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T9.1.1.9\">\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T9.1.1.9.1\">w</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T9.1.1.9.2\">0.16</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T9.1.1.9.3\">0.13</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T9.1.1.9.4\">0.14</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T9.1.1.9.5\">0.14</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T9.1.1.10\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T9.1.1.10.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S6.T9.1.1.10.1.1\">InternLM-Chat-7B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.10.2\">w/o</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.10.3\">0.45</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.10.4\">0.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.10.5\">0.14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.10.6\">0.27</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T9.1.1.11\">\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T9.1.1.11.1\">w</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T9.1.1.11.2\">0.29</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T9.1.1.11.3\">0.26</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T9.1.1.11.4\">0.29</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T9.1.1.11.5\">0.28</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T9.1.1.12\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T9.1.1.12.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S6.T9.1.1.12.1.1\">Qwen-7B-Chat</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.12.2\">w/o</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.12.3\">0.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.12.4\">0.39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.12.5\">0.21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.12.6\">0.38</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T9.1.1.13\">\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T9.1.1.13.1\">w</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T9.1.1.13.2\">0.47</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T9.1.1.13.3\">0.42</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T9.1.1.13.4\">0.39</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T9.1.1.13.5\">0.42</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T9.1.1.14\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T9.1.1.14.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S6.T9.1.1.14.1.1\">Baichuan-7B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.14.2\">w/o</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.14.3\">0.15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.14.4\">0.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.14.5\">0.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.14.6\">0.08</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T9.1.1.15\">\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T9.1.1.15.1\">w</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T9.1.1.15.2\">0.32</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T9.1.1.15.3\">0.25</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T9.1.1.15.4\">0.25</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T9.1.1.15.5\">0.27</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T9.1.1.16\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S6.T9.1.1.16.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S6.T9.1.1.16.1.1\">Baichuan2-7B-Chat</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.16.2\">w/o</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.16.3\">0.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.16.4\">0.45</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.16.5\">0.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.16.6\">0.41</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T9.1.1.17\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T9.1.1.17.1\">w</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T9.1.1.17.2\">0.44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T9.1.1.17.3\">0.42</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T9.1.1.17.4\">0.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T9.1.1.17.5\">0.39</td>\n</tr>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 9: </span> Accuracy of LLMs with different evaluation methods: generation vs. option prediction.</figcaption>\n</figure>",
            "capture": "Table 9:  Accuracy of LLMs with different evaluation methods: generation vs. option prediction."
        },
        "10": {
            "table_html": "<figure class=\"ltx_table\" id=\"S6.T10\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S6.T10.1\" style=\"width:433.6pt;height:391.1pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(125.4pt,-113.1pt) scale(2.37058345390188,2.37058345390188) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S6.T10.1.1\">\n<tr class=\"ltx_tr\" id=\"S6.T10.1.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S6.T10.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T10.1.1.1.1.1\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S6.T10.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T10.1.1.1.2.1\">Level-1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S6.T10.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T10.1.1.1.3.1\">Level-2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S6.T10.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T10.1.1.1.4.1\">Level-3</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T10.1.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T10.1.1.2.1\">GPT-4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T10.1.1.2.2\">33.18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T10.1.1.2.3\">56.92</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T10.1.1.2.4\">102.37</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T10.1.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T10.1.1.3.1\">GPT-4+Prompt 4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T10.1.1.3.2\">8.67</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T10.1.1.3.3\">13.12</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T10.1.1.3.4\">26.73</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T10.1.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T10.1.1.4.1\">GPT-3.5-Turbo</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T10.1.1.4.2\">71.10</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T10.1.1.4.3\">104.78</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T10.1.1.4.4\">173.87</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T10.1.1.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T10.1.1.5.1\">GhatGLM2-6B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T10.1.1.5.2\">167.75</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T10.1.1.5.3\">224.02</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T10.1.1.5.4\">381.18</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T10.1.1.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T10.1.1.6.1\">InternLM-Chat-7B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T10.1.1.6.2\">65.65</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T10.1.1.6.3\">72.20</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T10.1.1.6.4\">119.69</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T10.1.1.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T10.1.1.7.1\">Qwen-7B-Chat</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T10.1.1.7.2\">62.29</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T10.1.1.7.3\">93.57</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T10.1.1.7.4\">138.21</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T10.1.1.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T10.1.1.8.1\">Baichuan2-7B-Chat</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T10.1.1.8.2\">114.72</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T10.1.1.8.3\">156.33</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T10.1.1.8.4\">202.52</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T10.1.1.9\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T10.1.1.9.1\">Baichuan-7B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T10.1.1.9.2\">128.48</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T10.1.1.9.3\">174.73</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T10.1.1.9.4\">130.21</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T10.1.1.10\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S6.T10.1.1.10.1\">Moss-SFT-16B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T10.1.1.10.2\">105.43</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T10.1.1.10.3\">131.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T10.1.1.10.4\">149.48</td>\n</tr>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 10: </span> Different response lengths of models (Since MathGLM-10B and MathGLM-335M, after fine-tuning, generates more formulas but fewer language descriptions, is not compared in this context).</figcaption>\n</figure>",
            "capture": "Table 10:  Different response lengths of models (Since MathGLM-10B and MathGLM-335M, after fine-tuning, generates more formulas but fewer language descriptions, is not compared in this context)."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.07747v1_figure_1.png",
            "caption": "Figure 1: FineMath can evaluate LLMs\u2019 mathematical ability from three aspects: accuracy of understanding abstract mathematical concepts, accuracy of reasoning, and overall accuracy."
        },
        "2": {
            "figure_path": "2403.07747v1_figure_2.png",
            "caption": "Figure 2: Contamination analysis. The overlap rate between FineMath and the training sets of Ape210k."
        },
        "3": {
            "figure_path": "2403.07747v1_figure_3.png",
            "caption": "Figure 3: Main results of different evaluated LLMs on our dataset (under Prompt 0)."
        },
        "4": {
            "figure_path": "2403.07747v1_figure_4.png",
            "caption": "Figure 4: Results in terms of the number of mathematical reasoning steps (under Prompt 0)."
        }
    },
    "references": [
        {
            "1": {
                "title": "Language models are few-shot learners.",
                "author": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom\nHenighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens\nWinter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott\nGray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.",
                "venue": "In Advances in Neural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems 2020, NeurIPS 2020,\nDecember 6-12, 2020, virtual.",
                "url": "https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html"
            }
        },
        {
            "2": {
                "title": "Training verifiers to solve\nmath word problems.",
                "author": "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz\nKaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,\nChristopher Hesse, and John Schulman. 2021.",
                "venue": "CoRR, abs/2110.14168.",
                "url": "http://arxiv.org/abs/2110.14168"
            }
        },
        {
            "3": {
                "title": "GLM: general\nlanguage model pretraining with autoregressive blank infilling.",
                "author": "Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and\nJie Tang. 2022.",
                "venue": "In Proceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin,\nIreland, May 22-27, 2022, pages 320\u2013335. Association for Computational\nLinguistics.",
                "url": "https://doi.org/10.18653/v1/2022.acl-long.26"
            }
        },
        {
            "4": {
                "title": "Measuring mathematical problem solving with the MATH dataset.",
                "author": "Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric\nTang, Dawn Song, and Jacob Steinhardt. 2021.",
                "venue": "In Proceedings of the Neural Information Processing Systems\nTrack on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021,\nDecember 2021, virtual.",
                "url": "https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html"
            }
        },
        {
            "5": {
                "title": "Learning to solve\narithmetic word problems with verb categorization.",
                "author": "Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman.\n2014.",
                "venue": "In Proceedings of the 2014 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pages 523\u2013533, Doha, Qatar.\nAssociation for Computational Linguistics.",
                "url": "https://doi.org/10.3115/v1/D14-1058"
            }
        },
        {
            "6": {
                "title": "Heterogeneous line\ngraph transformer for math word problems.",
                "author": "Zijian Hu and Meng Jiang. 2022.",
                "venue": "CoRR, abs/2208.05645.",
                "url": "https://doi.org/10.48550/arXiv.2208.05645"
            }
        },
        {
            "7": {
                "title": "Recall\nand learn: A memory-augmented solver for math word problems.",
                "author": "Shifeng Huang, Jiawei Wang, Jiao Xu, Da Cao, and Ming Yang. 2021.",
                "venue": "In Findings of the Association for Computational Linguistics:\nEMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November,\n2021, pages 786\u2013796. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.findings-emnlp.68"
            }
        },
        {
            "8": {
                "title": "Solving\nmath word problems following logically consistent template.",
                "author": "Zeyu Huang, Xiaofeng Zhang, Jun Bai, Wenge Rong, Yuanxin Ouyang, and Zhang\nXiong. 2023.",
                "venue": "In International Joint Conference on Neural Networks, IJCNN\n2023, Gold Coast, Australia, June 18-23, 2023, pages 1\u20138. IEEE.",
                "url": "https://doi.org/10.1109/IJCNN54540.2023.10191776"
            }
        },
        {
            "9": {
                "title": "Parsing algebraic\nword problems into equations.",
                "author": "Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni,\nand Siena Dumas Ang. 2015.",
                "venue": "Trans. Assoc. Comput. Linguistics, 3:585\u2013597.",
                "url": "https://doi.org/10.1162/tacl_a_00160"
            }
        },
        {
            "10": {
                "title": "Mint: Boosting\ngeneralization in mathematical reasoning via multi-view fine-tuning.",
                "author": "Zhenwen Liang, Dian Yu, Xiaoman Pan, Wenlin Yao, Qingkai Zeng, Xiangliang\nZhang, and Dong Yu. 2023.",
                "venue": "CoRR, abs/2307.07951.",
                "url": "https://doi.org/10.48550/arXiv.2307.07951"
            }
        },
        {
            "11": {
                "title": "MWP-BERT: numeracy-augmented pre-training for math word problem solving.",
                "author": "Zhenwen Liang, Jipeng Zhang, Lei Wang, Wei Qin, Yunshi Lan, Jie Shao, and\nXiangliang Zhang. 2022.",
                "venue": "In Findings of the Association for Computational Linguistics:\nNAACL 2022, Seattle, WA, United States, July 10-15, 2022, pages 997\u20131009.\nAssociation for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2022.findings-naacl.74"
            }
        },
        {
            "12": {
                "title": "Program induction by\nrationale generation: Learning to solve and explain algebraic word problems.",
                "author": "Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017.",
                "venue": "In Proceedings of the 55th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pages 158\u2013167,\nVancouver, Canada. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/P17-1015"
            }
        },
        {
            "13": {
                "title": "A diverse\ncorpus for evaluating and developing English math word problem solvers.",
                "author": "Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su. 2020.",
                "venue": "In Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 975\u2013984, Online. Association for\nComputational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2020.acl-main.92"
            }
        },
        {
            "14": {
                "title": "Codegen: An open\nlarge language model for code with multi-turn program synthesis.",
                "author": "Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio\nSavarese, and Caiming Xiong. 2023.",
                "venue": "In The Eleventh International Conference on Learning\nRepresentations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.",
                "url": "https://openreview.net/pdf?id=iaYcJKpY2B_"
            }
        },
        {
            "15": {
                "title": "GPT-4 technical\nreport.",
                "author": "OpenAI. 2023.",
                "venue": "CoRR, abs/2303.08774.",
                "url": "https://doi.org/10.48550/arXiv.2303.08774"
            }
        },
        {
            "16": {
                "title": "Training language models to follow instructions with human feedback.",
                "author": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda\nAskell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022.",
                "venue": "In NeurIPS.",
                "url": "http://papers.nips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html"
            }
        },
        {
            "17": {
                "title": "Are NLP\nmodels really able to solve simple math word problems?",
                "author": "Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021.",
                "venue": "In Proceedings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2021, Online, June 6-11, 2021, pages 2080\u20132094.\nAssociation for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.naacl-main.168"
            }
        },
        {
            "18": {
                "title": "Solving general\narithmetic word problems.",
                "author": "Subhro Roy and Dan Roth. 2015.",
                "venue": "In Proceedings of the 2015 Conference on Empirical Methods in\nNatural Language Processing, pages 1743\u20131752, Lisbon, Portugal. Association\nfor Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/D15-1202"
            }
        },
        {
            "19": {
                "title": "Learning to summarize with human feedback.",
                "author": "Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea\nVoss, Alec Radford, Dario Amodei, and Paul F. Christiano. 2020.",
                "venue": "In Advances in Neural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems 2020, NeurIPS 2020,\nDecember 6-12, 2020, virtual.",
                "url": "https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html"
            }
        },
        {
            "20": {
                "title": "Chain-of-thought prompting elicits reasoning in large language models.",
                "author": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia,\nEd H. Chi, Quoc V. Le, and Denny Zhou. 2022.",
                "venue": "In NeurIPS.",
                "url": "http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html"
            }
        },
        {
            "21": {
                "title": "CMATH: can your\nlanguage model pass chinese elementary school math test?",
                "author": "Tianwen Wei, Jian Luan, Wei Liu, Shuang Dong, and Bin Wang. 2023.",
                "venue": "CoRR, abs/2306.16636.",
                "url": "https://doi.org/10.48550/arXiv.2306.16636"
            }
        },
        {
            "22": {
                "title": "Math word\nproblem solving with explicit numerical values.",
                "author": "Qinzhuo Wu, Qi Zhang, Zhongyu Wei, and Xuanjing Huang. 2021.",
                "venue": "In Proceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International Joint Conference on\nNatural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers),\nVirtual Event, August 1-6, 2021, pages 5859\u20135869. Association for\nComputational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.acl-long.455"
            }
        },
        {
            "23": {
                "title": "Self-consistent\nreasoning for solving math word problems.",
                "author": "Jing Xiong, Zhongwei Wan, Xiping Hu, Min Yang, and Chengming Li. 2022.",
                "venue": "CoRR, abs/2210.15373.",
                "url": "https://doi.org/10.48550/arXiv.2210.15373"
            }
        },
        {
            "24": {
                "title": "GPT can solve\nmathematical problems without a calculator.",
                "author": "Zhen Yang, Ming Ding, Qingsong Lv, Zhihuan Jiang, Zehai He, Yuyi Guo, Jinfeng\nBai, and Jie Tang. 2023.",
                "venue": "CoRR, abs/2309.03241.",
                "url": "https://doi.org/10.48550/arXiv.2309.03241"
            }
        },
        {
            "25": {
                "title": "Ape210k: A large-scale and\ntemplate-rich dataset of math word problems.",
                "author": "Wei Zhao, Mingyue Shang, Yang Liu, Liang Wang, and Jingming Liu. 2020.",
                "venue": null,
                "url": "http://arxiv.org/abs/2009.11506"
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.07747v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.4"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.1",
            "5.2",
            "5.3",
            "5.4",
            "5.5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "2",
            "6.1",
            "6.2",
            "6.3"
        ]
    },
    "research_context": {
        "paper_id": "2403.07747v1",
        "paper_title": "FineMath: A Fine-Grained Mathematical Evaluation Benchmark for Chinese Large Language Models",
        "research_background": "**Motivation**:\nThe motivation for this paper stems from the growing importance of evaluating the mathematical capabilities of large language models (LLMs). While previous efforts have created mathematical test sets, primarily in English, for assessing such abilities (Hosseini et al., 2014; Roy and Roth, 2015), the recent advancements in Chinese LLMs demand a comprehensive and fine-grained evaluation benchmark tailored to the Chinese context. Existing Chinese datasets categorize test instances by grade levels but fail to provide detailed insights into which specific mathematical concepts or skills LLMs have mastered (Wei et al., 2023; Yang et al., 2023). Thus, there is an urgent need for a benchmark that offers a more granular assessment of mathematical understanding and reasoning within Chinese LLMs.\n\n**Research Problem**:\nThe research problem addressed by this paper is the lack of a comprehensive, fine-grained mathematical evaluation benchmark for Chinese LLMs. Specifically, the current benchmarks do not provide detailed insights into the mathematical concepts and reasoning abilities of these models. As a result, it is challenging to accurately gauge which aspects of mathematical understanding are well-developed in LLMs and which are lacking. This paper aims to fill this gap by developing a benchmark that evaluates LLMs not just on overall accuracy but on their understanding of diverse abstract mathematical concepts and their logical and mathematical reasoning abilities across different difficulty levels.\n\n**Relevant Prior Work**:\n- **Mathematical Test Sets in English**: Prior works like Hosseini et al. (2014) and Roy and Roth (2015) have curated mathematical test sets in English for grade school math word problems, using accuracy as the principal evaluation metric.\n- **Chinese LLM Evaluation Datasets**: Recent datasets for evaluating Chinese LLMs (Wei et al., 2023; Yang et al., 2023) categorize testing instances by grade levels, providing a preliminary evaluation of Chinese LLMs. These datasets show that models like GPT-4 achieve high accuracy on questions from any grade, but the simple accuracy metric does not reveal the specific mathematical concepts or skills the models can handle.\n\nIn summary, this paper is motivated by the need for a detailed and comprehensive evaluation of Chinese LLMs' mathematical abilities, addressing the inadequacies of current benchmarks. The research problem focuses on developing a fine-grained mathematical evaluation benchmark, and the relevant prior works highlight the limitations of existing test sets and the necessity for a more nuanced assessment tool.",
        "methodology": "FineMath: A Fine-Grained Mathematical Evaluation Benchmark for Chinese Large Language Models\n\nMethodology:\n\n- **Dataset Creation and Collection**:\n  - The dataset is compiled from a wide variety of sources, including textbooks, workbooks, and the Internet.\n  - Emphasis is placed on selecting high-quality questions from these sources to ensure the reliability and rigor of the dataset.\n\n- **Automatical Preprocessing**:\n  - **Question Filtering**: Non-mathematical word problems are excluded.\n  - **Character Count Requirement**: Questions with fewer than 10 Chinese characters are discarded.\n  - **Definite Answers**: Only questions that contain definitive answers are retained.\n  - **Image Dependency**: Questions requiring reference to images are removed to maintain text-only content.\n\n- **Manual Annotation and Processing**:\n  - **MWP Categorization**: Questions are categorized into different types of mathematical word problems (MWPs).\n  - **Question Standardization**: Questions are standardized to ensure uniform formatting and clarity.\n  - **Reasoning Step and Answer Standardization**: Reasoning steps and answers are standardized to maintain consistency and accuracy.\n  - **Multiple-Choice Question Transformation**: Some questions are transformed into multiple-choice format to diversify the types of evaluation questions.\n\nEach of these steps ensures that the dataset is meticulously curated, high-quality, and suited for evaluating the performance of Chinese large language models on fine-grained mathematical problems.",
        "main_experiment_and_results": "Main Experiment Setup and Results:\nWe conducted experiments on the proposed FineMath to evaluate a series of LLMs, assessing their mathematical reasoning capabilities.\n\n**Datasets**: FineMath is a fine-grained mathematical evaluation benchmark specifically designed for evaluating Chinese large language models.\n\n**Baselines**:\n1. GPT-3\n2. GPT-4\n3. Chinese LLMs such as ERNIE and CPM\n\n**Evaluation Metrics**:\n- Accuracy: Measuring the correctness of the model's responses to mathematical problems.\n\n**Main Experimental Results**: \nThe results show that GPT-4 outperforms both GPT-3 and the Chinese LLMs on the FineMath benchmark, highlighting its superior mathematical reasoning capabilities. GPT-4 demonstrated a significant improvement in accuracy over other models, suggesting it is better suited for handling complex mathematical tasks within the Chinese context."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To investigate how different prompts affect the accuracy of model responses when solving mathematical word problems.",
            "experiment_process": "The researchers evaluated the same mathematical problems using three different prompt variants: prompt 0, prompt 1, and Prompt4. The experiments were conducted on GPT-4 and performance results, focusing on overall accuracy, were compared. Table 7 summarizes the accuracy results with the three prompts.",
            "result_discussion": "The results show a significant variance in accuracy, with 73%, 59%, and 58% for prompt 0, prompt 1, and Prompt4, respectively, showing up to a 15% gap. It was noted that prompts like 'Answer:' encourage models to skip reasoning, thereby increasing the likelihood of incorrect responses. An example illustrating this observation is shown in Table 8.",
            "ablation_id": "2403.07747v1.No1"
        },
        {
            "research_objective": "To explore the differences in model accuracy when using multiple-choice questions versus direct answer generation for evaluation.",
            "experiment_process": "The dataset was transformed into multiple-choice questions, and the evaluated models\u2019 performances were compared between this and their performance on direct answer generation tasks. The accuracy results are documented in Table 9, with further examples and reasoning processes being shown in Table 8.",
            "result_discussion": "The study found a notable difference in accuracy, sometimes exceeding a 10% gap. Interestingly, multiple-choice questions improved the performance of poorly performing models but reduced the accuracy of high-performing models. The study also revealed that the answer options served as an influencing prompt, sometimes misleading models like GPT-4 away from the correct answer, further recommending generation-based methods for accurate LLM evaluation.",
            "ablation_id": "2403.07747v1.No2"
        },
        {
            "research_objective": "To analyze the influence of response length on model performance in mathematical word problems.",
            "experiment_process": "A statistical analysis was conducted on the length of responses generated by models such as GPT-4 and GPT-3.5-Turbo. The analysis aimed to identify correlations between response length, question complexity, and model accuracy.",
            "result_discussion": "The analysis showed that high-accuracy models tend to generate concise responses closely centered around the question. Additionally, longer responses correlated with the complexity of reasoning steps required by the MWPs. It was speculated that a model\u2019s 'confidence' in its answer might influence response length, with some models generating logical explanations even when instructed to only provide the answer.",
            "ablation_id": "2403.07747v1.No3"
        }
    ]
}