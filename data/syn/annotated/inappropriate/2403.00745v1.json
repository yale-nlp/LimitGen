{
    "title": "AtP^*: An efficient and scalable method for localizing LLM behaviour to components",
    "abstract": "Activation Patching is a method of directly computing causal attributions of behavior to model components.\nHowever, applying it exhaustively requires a sweep with cost scaling linearly in the number of model components, which can be prohibitively expensive for SoTA Large Language Models (LLMs).\nWe investigate Attribution Patching (AtP) (Nanda, 2022), a fast gradient-based approximation to Activation Patching and find two classes of failure modes of AtP which lead to significant false negatives.\nWe propose a variant of AtP called AtP, with two changes to address these failure modes while retaining scalability.\nWe present the first systematic study of AtP and alternative methods for faster activation patching and show that AtP significantly outperforms all other investigated methods, with AtP providing further significant improvement.\nFinally, we provide a method to bound the probability of remaining false negatives of AtP estimates.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "As LLMs become ubiquitous and integrated into numerous digital applications, it\u2019s an increasingly pressing research problem to understand the internal mechanisms that underlie their behaviour \u2013 this is the problem of mechanistic interpretability. A fundamental subproblem is to causally attribute particular behaviours to individual parts of the transformer forward pass, corresponding to specific components (such as attention heads, neurons, layer contributions, or residual streams), often at specific positions in the input token sequence. This is important because in numerous case studies of complex behaviours, they are found to be driven by sparse subgraphs within the model (Olsson et al., 2022  ###reference_b39###; Wang et al., 2022  ###reference_b54###; Meng et al., 2023  ###reference_b32###).\nA classic form of causal attribution uses zero-ablation, or knock-out, where a component is deleted and we see if this negatively affects a model\u2019s output \u2013 a negative effect implies the component was causally important. More recent work has generalised this to replacing a component\u2019s activations with samples from some baseline distribution (with zero-ablation being a special case where activations are resampled to be zero). We focus on the popular and widely used method of Activation Patching (also known as causal mediation analysis) (Geiger et al., 2022  ###reference_b15###; Meng et al., 2023  ###reference_b32###; Chan et al., 2022  ###reference_b6###) where the baseline distribution is a component\u2019s activations on some corrupted input, such as an alternate string with a different answer (Pearl, 2001  ###reference_b41###; Robins and Greenland, 1992  ###reference_b44###).\nGiven a causal attribution method, it is common to sweep across all model components, directly evaluating the effect of intervening on each of them via resampling (Meng et al., 2023  ###reference_b32###). However, when working with SoTA models it can be expensive to attribute behaviour especially to small components (e.g. heads or neurons) \u2013 each intervention requires a separate forward pass, and so the number of forward passes can easily climb into the millions or billions. For example, on a prompt of length 1024, there are  neuron nodes in Chinchilla 70B (Hoffmann et al., 2022  ###reference_b23###).\nWe propose to accelerate this process by using Attribution Patching (AtP) (Nanda, 2022  ###reference_b36###), a faster, approximate, causal attribution method, as a prefiltering step: after running AtP, we iterate through the nodes in decreasing order of absolute value of the AtP estimate, then use Activation Patching to more reliably evaluate these nodes and filter out false positives \u2013 we call this verification. We typically care about a small set of top contributing nodes, so verification is far cheaper than iterating over all nodes.\nWe investigate the performance of AtP, finding two classes of failure modes which produce false negatives. We propose a variant of AtP called AtP, with two changes to address these failure modes while retaining scalability:\nWhen patching queries and keys, recomputing the attention softmax and using a gradient based approximation from then on, as gradients are a poor approximation to saturated attention.\nUsing dropout on the backwards pass to fix brittle false negatives, where significant positive and negative effects cancel out.\nWe introduce several alternative methods to approximate Activation Patching as baselines to AtP which outperform brute force Activation Patching.\nWe present the first systematic study of AtP and these alternatives and show that AtP significantly outperforms all other investigated methods, with AtP providing further significant improvement.\nTo estimate the residual error of AtP and statistically bound the sizes of any remaining false negatives we provide a diagnostic method, based on using AtP to filter out high impact nodes, and then patching random subsets of the remainder. Good diagnostics mean that practitioners may still gauge whether AtP is reliable in relevant domains without the costs of exhaustive verification.\nFinally, we provide some guidance in Section 5.4  ###reference_### on how to successfully perform causal attribution in practice and what attribution methods are likely to be useful and under what circumstances.\n###figure_1### ###figure_2### ###figure_3### \n###figure_4### ###figure_5###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background",
            "text": "We are given a model  that maps a prompt (token sequence)  to output logits over a set of  tokens, aiming to predict the next token in the sequence. We will view the model  as a computational graph  where the node set  is the set of model components, and a directed edge  is present iff the output of  is a direct input into the computation of . We will use  to represent the activation (intermediate computation result) of  when computing .\nThe choice of  determines how fine-grained the attribution will be. For example, for transformer models, we could have a relatively coarse-grained attribution where each layer is considered a single node. In this paper we will primarily consider more fine-grained attributions that are more expensive to compute (see Section 4  ###reference_### for details); we revisit this issue in Section 5  ###reference_###.\nFollowing past work (Geiger et al., 2022  ###reference_b15###; Chan et al., 2022  ###reference_b6###; Wang et al., 2022  ###reference_b54###), we assume a distribution  over pairs of inputs , where  is a prompt on which the behaviour occurs, and  is a reference prompt which we use as a source of noise to intervene with111This precludes interventions which use activation values that are never actually realized, such as zero-ablation or mean ablation. An alternative formulation via distributions of activation values is also possible..\nWe are also given a metric222Common metrics in language models are next token prediction loss, difference in log prob between a correct and incorrect next token, probability of the correct next token, etc. , which quantifies the behaviour of interest.\nSimilarly to the work referenced above we define the contribution  of a node  to the model\u2019s behaviour as the counterfactual absolute333The sign of the impact may be of interest, but in this work we\u2019ll focus on the magnitude, as a measure of causal importance. expected impact of replacing that node on the clean prompt with its value on the reference prompt .\nUsing do-calculus notation (Pearl, 2000  ###reference_b40###) this can be expressed as , where\nwhere we define the intervention effect  for  as\nNote that the need to average the effect across a distribution adds a potentially large multiplicative factor to the cost of computing , further motivating this work.\nWe can also intervene on a set of nodes . To do so, we overwrite the values of all nodes in  with their values from a reference prompt. Abusing notation, we write  as the set of activations of the nodes in , when computing .\nWe note that it is also valid to define contribution as the expected impact of replacing a node on the reference prompt with its value on the clean prompt, also known as denoising or knock-in. We follow Chan et al. (2022  ###reference_b6###); Wang et al. (2022  ###reference_b54###) in using noising, however denoising is also widely used in the literature (Meng et al., 2023  ###reference_b32###; Lieberum et al., 2023  ###reference_b28###). We briefly consider how this choice affects AtP in Section 5.2  ###reference_###."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Problem Statement",
            "text": "Our goal is to identify the contributions to model behavior by individual model components. We first formalize model components, then formalize model behaviour, and finally state the contribution problem in causal language. While we state the formalism in terms of a decoder-only transformer language model (Vaswani et al., 2017  ###reference_b51###; Radford et al., 2018  ###reference_b42###), and conduct all our experiments on models of that class, the formalism is also straightforwardly applicable to other model classes.\nWe are given a model  that maps a prompt (token sequence)  to output logits over a set of  tokens, aiming to predict the next token in the sequence. We will view the model  as a computational graph  where the node set  is the set of model components, and a directed edge  is present iff the output of  is a direct input into the computation of . We will use  to represent the activation (intermediate computation result) of  when computing .\nThe choice of  determines how fine-grained the attribution will be. For example, for transformer models, we could have a relatively coarse-grained attribution where each layer is considered a single node. In this paper we will primarily consider more fine-grained attributions that are more expensive to compute (see Section 4  ###reference_###  ###reference_### for details); we revisit this issue in Section 5  ###reference_###  ###reference_###.\nFollowing past work (Geiger et al., 2022  ###reference_b15###  ###reference_b15###; Chan et al., 2022  ###reference_b6###  ###reference_b6###; Wang et al., 2022  ###reference_b54###  ###reference_b54###), we assume a distribution  over pairs of inputs , where  is a prompt on which the behaviour occurs, and  is a reference prompt which we use as a source of noise to intervene with111This precludes interventions which use activation values that are never actually realized, such as zero-ablation or mean ablation. An alternative formulation via distributions of activation values is also possible..\nWe are also given a metric222Common metrics in language models are next token prediction loss, difference in log prob between a correct and incorrect next token, probability of the correct next token, etc. , which quantifies the behaviour of interest.\nSimilarly to the work referenced above we define the contribution  of a node  to the model\u2019s behaviour as the counterfactual absolute333The sign of the impact may be of interest, but in this work we\u2019ll focus on the magnitude, as a measure of causal importance. expected impact of replacing that node on the clean prompt with its value on the reference prompt .\nUsing do-calculus notation (Pearl, 2000  ###reference_b40###  ###reference_b40###) this can be expressed as , where\nwhere we define the intervention effect  for  as\nNote that the need to average the effect across a distribution adds a potentially large multiplicative factor to the cost of computing , further motivating this work.\nWe can also intervene on a set of nodes . To do so, we overwrite the values of all nodes in  with their values from a reference prompt. Abusing notation, we write  as the set of activations of the nodes in , when computing .\nWe note that it is also valid to define contribution as the expected impact of replacing a node on the reference prompt with its value on the clean prompt, also known as denoising or knock-in. We follow Chan et al. (2022  ###reference_b6###  ###reference_b6###); Wang et al. (2022  ###reference_b54###  ###reference_b54###) in using noising, however denoising is also widely used in the literature (Meng et al., 2023  ###reference_b32###  ###reference_b32###; Lieberum et al., 2023  ###reference_b28###  ###reference_b28###). We briefly consider how this choice affects AtP in Section 5.2  ###reference_###  ###reference_###."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Attribution Patching",
            "text": "On state of the art models, computing  for all  can be prohibitively expensive as there may be billions or more nodes. Furthermore, to compute this value precisely requires evaluating it on all prompt pairs, thus the runtime cost of Equation 1  ###reference_### for each  scales with the size of the support of .\nWe thus turn to a fast approximation of Equation 1  ###reference_###. As suggested by Nanda (2022  ###reference_b36###); Figurnov et al. (2016  ###reference_b10###); Molchanov et al. (2017  ###reference_b35###), we can make a first-order Taylor expansion to  around :\nThen, similarly to Syed et al. (2023  ###reference_b47###), we apply this to a distribution by taking the absolute value inside the expectation in Equation 1  ###reference_### rather than outside; this decreases the chance that estimates across prompt pairs with positive and negative effects might erroneously lead to a significantly smaller estimate. (We briefly explore the amount of cancellation behaviour in the true effect distribution in Section B.2  ###reference_###.) As a result, we get an estimate\nThis procedure is also called Attribution Patching (Nanda, 2022  ###reference_b36###) or AtP. AtP requires two forward passes and one backward pass to compute an estimate score for all nodes on a given prompt pair, and so provides a very significant speedup over brute force activation patching."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methods",
            "text": "We now describe some failure modes of AtP and address them, yielding an improved method AtP*. We then discuss some alternative methods for estimating , to put AtP(*)\u2019s performance in context. Finally we discuss how to combine Subsampling, one such alternative method described in Section 3.3  ###reference_###, and AtP* to give a diagnostic to statistically test whether AtP* may have missed important false negatives.\nFor the queries, we can easily compute the adjusted effect by running the model on  and caching the noise queries. We then run the model on  and cache the attention keys and weights. Finally, we compute the attention weights that result from combining all the keys from the  forward pass with the queries from the  forward pass. This costs approximately as much as the unperturbed attention computation of the transformer forward pass. For each query node  we refer to the resulting weight vector as , in contrast with the weights  from the clean forward pass. The improved attribution estimate for  is then\nFor the keys we first describe a simple but inefficient method. We again run the model on , caching the noise\nkeys. We also run it on , caching the clean queries and attention probabilities.\nLet key nodes for a single attention head be  and let \nbe the set of query nodes for the same head as node . We then define\nThe improved attribution estimate for  is then\nHowever, the procedure we just described is costly to execute as it requires  flops to naively compute Equation 9  ###reference_### for all  keys. In Section A.2.1  ###reference_.SSS1### we describe a more efficient variant that takes no more compute than the forward pass attention computation itself (requiring  flops). Since Equation 6  ###reference_### is also cheaper to compute than a forward pass, the full QK fix requires less than two transformer forward passes (since the latter also includes MLP computations).\nFor attention nodes we show the effects of applying the query and key fixes in Figure 4  ###reference_### (middle). We observe that the propagation of Q/K effects has a major impact on reducing the false negative rate.\n###figure_6### To provide some evidence that the observed false negatives are due to cancellation, we compute the ratio between the direct effect  and the total effect . A higher direct effect ratio indicates more cancellation. We observe that the most significant false negatives corrected by GradDrop in Figure 5  ###reference_### (highlighted) have high direct effect ratios of , , and  (no direct effect) , while the median direct effect ratio of all nodes is  (if counting all nodes) or  (if only counting nodes that have direct effect). Note that direct effect ratio is only applicable to nodes which in fact have a direct connection to the output, and not e.g. to MLP nodes at non-final token positions, since all disconnected nodes have a direct effect of 0 by definition.\n###figure_7### The most straightforward method is to directly do Activation Patching to find the true effect  of each node, in some uninformed random order. This is necessarily inefficient.\nHowever, if we are scaling to a distribution, it is possible to improve on this, by alternating between phases of (i) for each unverified node, picking a not-yet-measured prompt pair on which to patch it, (ii) ranking the not-yet-verified nodes by the average observed patch effect magnitudes, taking the top  nodes, and verifying them. This balances the computational expenditure on the two tasks, and allows us to find large nodes sooner, at least as long as their large effect shows up on many prompt pairs.\nOur remaining baseline methods rely on an approximate node additivity assumption: that when intervening on a set of nodes , the measured effect  is approximately equal to .\nUnder the approximate node additivity assumption, we can construct an approximately unbiased estimator of . We select the sets  to contain each node independently with some probability , and additionally sample prompt pairs . For any node , and sets of nodes , let  be the collection of all those that contain , and  be the collection of those that don\u2019t contain ; we\u2019ll write these node sets as  and , and the corresponding prompt pairs as  and . The subsampling (or subset sampling) estimator is then given by\nThe estimator  is unbiased if there are no interaction effects, and has a small bias proportional to  under a simple interaction model (see Section A.1.1  ###reference_.SSS1### for proof).\nIn practice, we compute all the estimates  by sampling a binary mask over all nodes from i.i.d. Bernoulli \u2013 each binary mask can be identified with a node set .\nIn Algorithm 1  ###reference_###, we describe how to compute summary statistics related to Equation 13  ###reference_### efficiently for all nodes . The means  are enough to compute , while other summary statistics are involved in bounding the magnitude of a false negative (cf. Section 3.2  ###reference_###). (Note,  is just an alternate notation for .)\nInstead of sampling each  independently, we can group nodes into fixed \u201cblocks\u201d  of some size, and patch each block to find its aggregated contribution ; we can then traverse the nodes, starting with high-contribution blocks and proceeding from there.\nThere is a tradeoff in terms of the block size: using large blocks increases the compute required to traverse a high-contribution block, but using small blocks increases the compute required to finish traversing all of the blocks. We refer to the fixed block size setting as Blocks. Another way to handle this tradeoff is to add recursion: the blocks can be grouped into higher-level blocks, and so forth. We call this method Hierarchical.\nWe present results from both methods in our comparison plots, but relegate details to Section A.1.2  ###reference_.SSS2###. Relative to subsampling, these grouping-based methods have the disadvantage that on distributions, their cost scales linearly with size of \u2019s support, in addition to scaling with the number of nodes777AtP* also scales linearly in the same way, but with far fewer forward passes per prompt pair.."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "AtP improvements",
            "text": "We identify two common classes of false negatives occurring when using AtP.\nThe first failure mode occurs when the preactivation on  is in a flat region of the activation function (e.g. produces a saturated attention weight), but the preactivation on  is not in that region. As is apparent from Equation 4  ###reference_###, AtP uses a linear approximation to the ground truth in Equation 1  ###reference_###, so if the non-linear function is badly approximated by the local gradient, AtP ceases to be accurate \u2013 see Figure 3  ###reference_### for an illustration and Figure 4  ###reference_### which denotes in color the maximal difference in attention observed between prompt pairs, suggesting that this failure mode occurs in practice.\n###figure_8### Another, unrelated failure mode occurs due to cancellation between direct and indirect effects: roughly, if the total effect (on some prompt pair) is a sum of direct and indirect effects (Pearl, 2001  ###reference_b41###) , and these are close to cancelling, then a small multiplicative approximation error in , due to non-linearities such as GELU and softmax, can accidentally cause  to be orders of magnitude smaller than .\nFor the queries, we can easily compute the adjusted effect by running the model on  and caching the noise queries. We then run the model on  and cache the attention keys and weights. Finally, we compute the attention weights that result from combining all the keys from the  forward pass with the queries from the  forward pass. This costs approximately as much as the unperturbed attention computation of the transformer forward pass. For each query node  we refer to the resulting weight vector as , in contrast with the weights  from the clean forward pass. The improved attribution estimate for  is then\nFor the keys we first describe a simple but inefficient method. We again run the model on , caching the noise\nkeys. We also run it on , caching the clean queries and attention probabilities.\nLet key nodes for a single attention head be  and let \nbe the set of query nodes for the same head as node . We then define\nThe improved attribution estimate for  is then\nHowever, the procedure we just described is costly to execute as it requires  flops to naively compute Equation 9  ###reference_###  ###reference_### for all  keys. In Section A.2.1  ###reference_.SSS1###  ###reference_.SSS1### we describe a more efficient variant that takes no more compute than the forward pass attention computation itself (requiring  flops). Since Equation 6  ###reference_###  ###reference_### is also cheaper to compute than a forward pass, the full QK fix requires less than two transformer forward passes (since the latter also includes MLP computations).\nFor attention nodes we show the effects of applying the query and key fixes in Figure 4  ###reference_###  ###reference_### (middle). We observe that the propagation of Q/K effects has a major impact on reducing the false negative rate.\n###figure_9### To provide some evidence that the observed false negatives are due to cancellation, we compute the ratio between the direct effect  and the total effect . A higher direct effect ratio indicates more cancellation. We observe that the most significant false negatives corrected by GradDrop in Figure 5  ###reference_###  ###reference_### (highlighted) have high direct effect ratios of , , and  (no direct effect) , while the median direct effect ratio of all nodes is  (if counting all nodes) or  (if only counting nodes that have direct effect). Note that direct effect ratio is only applicable to nodes which in fact have a direct connection to the output, and not e.g. to MLP nodes at non-final token positions, since all disconnected nodes have a direct effect of 0 by definition.\n###figure_10###"
        },
        {
            "section_id": "3.1.1",
            "parent_section_id": "3.1",
            "section_name": "3.1.1 False negatives from attention saturation",
            "text": "AtP relies on the gradient at each activation being reflective of the true behaviour of the function with respect to intervention at that activation. In some cases, though, a node may immediately feed into a non-linearity whose effect may not be adequately predicted by the gradient; for example, attention key and query nodes feeding into the attention softmax non-linearity. To showcase this, we plot the true rank of each node\u2019s effect against its rank assigned by AtP in Figure 4  ###reference_### (left). The plot shows that there are many pronounced false negatives (below the dashed line), especially among keys and queries.\nNormal activation patching for queries and keys involves changing a query or key and then re-running the rest of the model, keeping all else the same. AtP takes a linear approximation to the entire rest of the model rather than re-running it. We propose explicitly re-computing the first step of the rest of the model, i.e. the attention softmax, and then taking a linear approximation to the rest. Formally, for attention key and query nodes, instead of using the gradient on those nodes directly, we take the difference in attention weight caused by that key or query, multiplied by the gradient on the attention weights themselves. This requires finding the change in attention weights from each key and query patch \u2014 but that can be done efficiently using (for all keys and queries in total) less compute than two transformer forward passes. This correction avoids the problem of saturated attention, while otherwise retaining the performance of AtP.\nFor the queries, we can easily compute the adjusted effect by running the model on  and caching the noise queries. We then run the model on  and cache the attention keys and weights. Finally, we compute the attention weights that result from combining all the keys from the  forward pass with the queries from the  forward pass. This costs approximately as much as the unperturbed attention computation of the transformer forward pass. For each query node  we refer to the resulting weight vector as , in contrast with the weights  from the clean forward pass. The improved attribution estimate for  is then\nFor the keys we first describe a simple but inefficient method. We again run the model on , caching the noise\nkeys. We also run it on , caching the clean queries and attention probabilities.\nLet key nodes for a single attention head be  and let \nbe the set of query nodes for the same head as node . We then define\nThe improved attribution estimate for  is then\nHowever, the procedure we just described is costly to execute as it requires  flops to naively compute Equation 9  ###reference_###  ###reference_###  ###reference_### for all  keys. In Section A.2.1  ###reference_.SSS1###  ###reference_.SSS1###  ###reference_.SSS1### we describe a more efficient variant that takes no more compute than the forward pass attention computation itself (requiring  flops). Since Equation 6  ###reference_###  ###reference_###  ###reference_### is also cheaper to compute than a forward pass, the full QK fix requires less than two transformer forward passes (since the latter also includes MLP computations).\nFor attention nodes we show the effects of applying the query and key fixes in Figure 4  ###reference_###  ###reference_###  ###reference_### (middle). We observe that the propagation of Q/K effects has a major impact on reducing the false negative rate.\n###figure_11###"
        },
        {
            "section_id": "3.1.2",
            "parent_section_id": "3.1",
            "section_name": "3.1.2 False negatives from cancellation",
            "text": "This form of cancellation occurs when the backpropagated gradient from indirect effects is combined with the gradient from the direct effect. We propose a way to modify the backpropagation within the attribution patching to reduce this issue. If we artificially zero out the gradient at a downstream layer that contributes to the indirect effect, the cancellation is disrupted. (This is also equivalent to patching in clean activations at the outputs of the layer.) Thus we propose to do this iteratively, sweeping across the layers. Any node whose effect does not route through the layer being gradient-zeroed will have its estimate unaffected.\nWe call this method GradDrop. For every layer  in the model, GradDrop computes an AtP estimate for all nodes, where gradients on the residual contribution from  are set to 0, including the propagation to earlier layers. This provides a different estimate for all nodes, for each layer that was dropped. We call the so-modified gradient  when dropping layer , where  is the contribution to the residual stream across all positions. Using  in place of  in the AtP formula produces an estimate . Then, the estimates are aggregated by averaging their absolute values, and then scaling by  to avoid changing the direct-effect path\u2019s contribution (which is otherwise zeroed out when dropping the layer the node is in).\nNote that the forward passes required for computing  don\u2019t depend on , so the extra compute needed for GradDrop is  backwards passes from the same intermediate activations on a clean forward pass. This is also the case with the QK fix: the corrected attributions  are dot products with the attention weight gradients, so the only thing that needs to be recomputed for  is the modified gradient . Thus, computing Equation 11  ###reference_### takes  backwards passes444This can be reduced to  by reusing intermediate results. on top of the costs for AtP.\nWe show the result of applying GradDrop on attention nodes in Figure 4  ###reference_### (right) and on MLP nodes in Figure 5  ###reference_###. In Figure 5  ###reference_###, we show the true effect magnitude rank against the AtP+GradDrop rank, while highlighting nodes which improved drastically by applying GradDrop. We give some arguments and intuitions on the benefit of GradDrop in Section A.2.2  ###reference_.SSS2###.\nTo provide some evidence that the observed false negatives are due to cancellation, we compute the ratio between the direct effect  and the total effect . A higher direct effect ratio indicates more cancellation. We observe that the most significant false negatives corrected by GradDrop in Figure 5  ###reference_###  ###reference_###  ###reference_### (highlighted) have high direct effect ratios of , , and  (no direct effect) , while the median direct effect ratio of all nodes is  (if counting all nodes) or  (if only counting nodes that have direct effect). Note that direct effect ratio is only applicable to nodes which in fact have a direct connection to the output, and not e.g. to MLP nodes at non-final token positions, since all disconnected nodes have a direct effect of 0 by definition.\n###figure_12###"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Diagnostics",
            "text": "Despite the improvements we have proposed in Section 3.1  ###reference_###, there is no guarantee that AtP* produces no false negatives. Thus, it is desirable to obtain an upper confidence bound on the effect size of nodes that might be missed by AtP*, i.e. that aren\u2019t in the top  AtP* estimates, for some . Let the top  nodes be . It so happens that we can use subset sampling to obtain such a bound.\nAs described in Algorithm 1  ###reference_### and Section 3.3  ###reference_.SSS0.Px2###, the subset sampling algorithm returns summary statistics: ,  and  for each node : the average effect size  of a subset conditional on the node being contained in that subset () or not (), the sample standard deviations , and the sample sizes . Given these, consider a null hypothesis555This is an unconventional form of  \u2013 typically a null hypothesis will say that an effect is insignificant. However, the framework of statistical hypothesis testing is based on determining whether the data let us reject the null hypothesis, and in this case the hypothesis we want to reject is the presence, rather than the absence, of a significant false negative.  that , for some threshold , versus the alternative hypothesis  that . We use a one-sided Welch\u2019s t-test666This relies on the populations being approximately unbiased and normally distributed, and not skewed. This tended to be true on inspection, and it\u2019s what the additivity assumption (see Section 3.3  ###reference_.SSS0.Px2###) predicts for a single prompt pair \u2014 but a nonparametric bootstrap test may be more reliable, at the cost of additional compute. to test this hypothesis; the general practice with a compound null hypothesis is to select the simple sub-hypothesis that gives the greatest -value, so to be conservative, the simple null hypothesis is that , giving a test statistic of , which gives a -value of .\nTo get a combined conclusion across all nodes in , let\u2019s consider the hypothesis  that any of those nodes has true effect . Since this is also a compound null hypothesis,  is the corresponding -value. Then, to find an upper confidence bound with specified confidence level , we invert this procedure to find the lowest  for which we still have at least that level of confidence. We repeat this for various settings of the sample size  in Algorithm 1  ###reference_###. The exact algorithm is described in Section A.3  ###reference_###.\nIn Figure 6  ###reference_###, we report the upper confidence bounds at confidence levels 90%, 99%, 99.9% from running Algorithm 1  ###reference_### with a given  (right subplots), as well as the number of nodes that have a true contribution  greater than  (left subplots).\n###figure_13### ###figure_14###"
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Baselines",
            "text": "The most straightforward method is to directly do Activation Patching to find the true effect  of each node, in some uninformed random order. This is necessarily inefficient.\nHowever, if we are scaling to a distribution, it is possible to improve on this, by alternating between phases of (i) for each unverified node, picking a not-yet-measured prompt pair on which to patch it, (ii) ranking the not-yet-verified nodes by the average observed patch effect magnitudes, taking the top  nodes, and verifying them. This balances the computational expenditure on the two tasks, and allows us to find large nodes sooner, at least as long as their large effect shows up on many prompt pairs.\nOur remaining baseline methods rely on an approximate node additivity assumption: that when intervening on a set of nodes , the measured effect  is approximately equal to .\nUnder the approximate node additivity assumption, we can construct an approximately unbiased estimator of . We select the sets  to contain each node independently with some probability , and additionally sample prompt pairs . For any node , and sets of nodes , let  be the collection of all those that contain , and  be the collection of those that don\u2019t contain ; we\u2019ll write these node sets as  and , and the corresponding prompt pairs as  and . The subsampling (or subset sampling) estimator is then given by\nThe estimator  is unbiased if there are no interaction effects, and has a small bias proportional to  under a simple interaction model (see Section A.1.1  ###reference_.SSS1###  ###reference_.SSS1### for proof).\nIn practice, we compute all the estimates  by sampling a binary mask over all nodes from i.i.d. Bernoulli \u2013 each binary mask can be identified with a node set .\nIn Algorithm 1  ###reference_###  ###reference_###, we describe how to compute summary statistics related to Equation 13  ###reference_###  ###reference_### efficiently for all nodes . The means  are enough to compute , while other summary statistics are involved in bounding the magnitude of a false negative (cf. Section 3.2  ###reference_###  ###reference_###). (Note,  is just an alternate notation for .)\nInstead of sampling each  independently, we can group nodes into fixed \u201cblocks\u201d  of some size, and patch each block to find its aggregated contribution ; we can then traverse the nodes, starting with high-contribution blocks and proceeding from there.\nThere is a tradeoff in terms of the block size: using large blocks increases the compute required to traverse a high-contribution block, but using small blocks increases the compute required to finish traversing all of the blocks. We refer to the fixed block size setting as Blocks. Another way to handle this tradeoff is to add recursion: the blocks can be grouped into higher-level blocks, and so forth. We call this method Hierarchical.\nWe present results from both methods in our comparison plots, but relegate details to Section A.1.2  ###reference_.SSS2###  ###reference_.SSS2###. Relative to subsampling, these grouping-based methods have the disadvantage that on distributions, their cost scales linearly with size of \u2019s support, in addition to scaling with the number of nodes777AtP* also scales linearly in the same way, but with far fewer forward passes per prompt pair.."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "When attributing model behavior to components, an important choice is the partition of the model\u2019s computational graph into units of analysis or \u2018nodes\u2019  (cf. Section 2.1  ###reference_###). We investigate two settings for the choice of , AttentionNodes and NeuronNodes. For NeuronNodes, each MLP neuron888We use the neuron post-activation for the node; this makes no difference when causally intervening, but for AtP it\u2019s beneficial, because it makes the  function more linear. is a separate node. For AttentionNodes, we consider the query, key, and value vector for each head as distinct nodes, as well as the pre-linear per-head attention output999We include the output node because it provides additional information about what function an attention head is serving, particularly in the case where its queries have negligible patch effects relative to its keys and/or values. This may happen as a result of choosing  such that the query does not differ across the prompts.. We also refer to these units as \u2018sites\u2019. For each site, we consider each copy of that site at different token positions as a separate node. As a result, we can identify each node  with a pair  from the product TokenPosition  Site. Since our two settings for  are using a different level of granularity and are expected to have different per-node effect magnitudes, we present results on them separately.\nWe investigate transformer language models from the Pythia suite (Biderman et al., 2023  ###reference_b2###) of sizes between 410M and 12B parameters. This allows us to demonstrate that our methods are applicable across scale. Our cost-of-verified-recall plots in Figures 1  ###reference_###, 7  ###reference_### and 8  ###reference_### refer to Pythia-12B. Results for other model sizes are presented via the relative-cost (cf. Section 4.2  ###reference_.SSS0.Px2###) plots in the main body Figure 9  ###reference_### and disaggregated via cost-of-verified recall in Section B.3  ###reference_###.\nAll reported results use the negative log probability101010Another popular metric is the difference in logits between the clean and noise target. As opposed to the negative logprob, the logit difference is linear in the final logits and thus might favor AtP. A downside of logit difference is that it is sensitive to the noise target, which may not be meaningful if there are multiple plausible completions, such as in IOI. as their loss function . We compute  relative to targets from the clean prompt . We briefly explore other metrics in Section B.4  ###reference_###.\nAs mentioned in the introduction, we\u2019re primarily interested in finding the largest-effect nodes \u2013 see Appendix D  ###reference_### for the distribution of  across models and distributions.\nOnce we have obtained node estimates via a given method, it is relatively cheap to directly measure true effects of top nodes one at a time; we refer to this as \u201cverification\u201d. Incorporating this into our methodology, we find that false positives are typically not a big issue; they are simply revealed during verification. In contrast, false negatives are not so easy to remedy without verifying all nodes, which is what we were trying to avoid.\nWe compare methods on the basis of total compute cost (in # of forward passes) to verify the  nodes with biggest true effect magnitude, for varying . The procedure being measured is to first compute estimates (incurring an estimation cost), and then sweep through nodes in decreasing order of estimated magnitude, measuring their individual effects  (i.e. verifying them), and incurring a verification cost. Then the total cost is the sum of these two costs.\nSometimes we find it useful to summarize the method performance with a scalar; this is useful for comparing methods at a glance across different settings (e.g. model sizes, as in Figure 2  ###reference_###), or for selecting hyperparameters (cf. Section B.5  ###reference_###). The cost of verified recall of the top  nodes is of interest for  at varying orders of magnitude. In order to avoid the performance metric being dominated by small or large , we assign similar total weight to different orders of magnitude: we use a weighted average with weight  for the cost of the top  nodes. Similarly, since the costs themselves may have different orders of magnitude, we average them on a log scale \u2013 i.e., we take a geometric mean.\nThis metric is also proportional to the area under the curve in plots like Figure 1  ###reference_###. To produce a more understandable result, we always report it relative to (i.e. divided by) the oracle verification cost on the same metric; the diagonal line is the oracle, with relative cost 1. We refer to this as the IRWRGM (inverse-rank-weighted relative geometric mean) cost, or the relative cost.\nNote that the preference of the individual practitioner may be different such that this metric is no longer accurately measuring the important rank regime. For example, AtP* pays a notable upfront cost relative to AtP or AtP+QKfix, which sets it at a disadvantage when it doesn\u2019t manage to find additional false negatives; but this may or may not be practically significant. To understand the performance in more detail we advise to refer to the cost of verified recall plots, like Figure 1  ###reference_### (or many more in Section B.3  ###reference_###).\nAs a starting point we report results on single prompt pairs which we expect to have relatively clean circuitry111111Formally, these represent prompt distributions via the delta distribution  where  is the singular prompt pair..\nAll singular prompt pairs are shown in Table 1  ###reference_###. IOI-PP is chosen to resemble an instance from the indirect object identification (IOI) task (Wang et al., 2022  ###reference_b54###), a task predominantly involving attention heads. CITY-PP is chosen to elicit factual recall which previous research suggests involves early MLPs and a small number of late attention heads (Meng et al., 2023  ###reference_b32###; Geva et al., 2023  ###reference_b17###; Nanda et al., 2023  ###reference_b37###). The country/city combinations were chosen such that Pythia-410M achieved low loss on both  and  and such that all places were represented by a single token.\n###table_1### We show the cost of verified 100% recall for various methods in Figure 1  ###reference_###, where we focus on NeuronNodes for CITY-PP and AttentionNodes for IOI-PP. Exhaustive results for smaller Pythia models are shown in Section B.3  ###reference_###. Figure 2  ###reference_### shows the aggregated relative costs for all models on CITY-PP and IOI-PP.\nInstead of applying the strict criterion of recalling all important nodes, we can also relax this constraint. In Figure 7  ###reference_###, we show the cost of verified 90% recall in the two clean prompt pair settings.\nThe previous prompt pairs may in fact be the best-case scenarios: the interventions they create will be fairly localized to a specific circuit, and this may make it easy for AtP to approximate the contributions. It may thus be informative to see how the methods generalize to settings where the interventions are less surgical. To do this, we also report results in Figure 8  ###reference_### (top) and Figure 9  ###reference_### on a random prompt pair chosen from a non-copyright-protected section of The Pile (Gao et al., 2020  ###reference_b12###) which we refer to as RAND-PP. The prompt pair was chosen such that Pythia-410M still achieved low loss on both prompts.\n###figure_15### ###figure_16### ###figure_17### ###figure_18### ###figure_19### ###figure_20### ###figure_21### ###figure_22### ###figure_23### We find that AtP/AtP* is only somewhat less effective here; this provides tentative evidence that the strong performance of AtP/AtP* isn\u2019t reliant on the clean prompt using a particularly crisp circuit, or on the noise prompt being a precise control.\nCausal attribution is often of most interest when evaluated across a distribution, as laid out in Section 2  ###reference_###. Of the methods, AtP, AtP*, and Subsampling scale reasonably to distributions; the former 2 because they\u2019re inexpensive so running them  times is not prohibitive, and Subsampling because it intrinsically averages across the distribution and thus becomes proportionally cheaper relative to the verification via activation patching. In addition, having a distribution enables a more performant Iterative method, as described in Section 3.3  ###reference_.SSS0.Px1###.\nWe present a comparison of these methods on 2 distributional settings. The first is a reduced version of IOI (Wang et al., 2022  ###reference_b54###) on 6 names, resulting in  prompt pairs, where we evaluate AttentionNodes. The other distribution prompts the model to output an indefinite article \u2018 a\u2019 or \u2018 an\u2019, where we evaluate NeuronNodes. See Section B.1  ###reference_### for details on constructing these distributions. Results are shown in Figure 8  ###reference_### for Pythia 12B, and in Figure 9  ###reference_### across models. The results show that AtP continues to perform well, especially with the QK fix; in addition, the cancellation failure mode tends to be sensitive to the particular input prompt pair, and as a result, averaging across a distribution diminishes the benefit of GradDrops.\nAn implication of Subsampling scaling well to this setting is that diagnostics may give reasonable confidence in not missing false negatives with much less overhead than in the single-prompt-pair case; this is illustrated in Figure 6  ###reference_###."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Setup",
            "text": "When attributing model behavior to components, an important choice is the partition of the model\u2019s computational graph into units of analysis or \u2018nodes\u2019 (cf. Section 2.1 ###reference_### ###reference_###). We investigate two settings for the choice of nodes, AttentionNodes and NeuronNodes. For NeuronNodes, each MLP neuron888We use the neuron post-activation for the node; this makes no difference when causally intervening, but for Randomized Sampling of Node Effects without Systematic Ranking it\u2019s beneficial, because it makes the function more linear. is a separate node. For AttentionNodes, we consider the query, key, and value vector for each head as distinct nodes, as well as the pre-linear per-head attention output999We include the output node because it provides additional information about what function an attention head is serving, particularly in the case where its queries have negligible patch effects relative to its keys and/or values. This may happen as a result of choosing nodes such that the query does not differ across the prompts. We also refer to these units as \u2018sites\u2019. For each site, we consider each copy of that site at different token positions as a separate node. As a result, we can identify each node with a pair from the product TokenPosition Site. Since our two settings for nodes are using a different level of granularity and are expected to have different per-node effect magnitudes, we present results on them separately. We investigate transformer language models from the Pythia suite (Biderman et al., 2023 ###reference_b2### ###reference_b2###) of sizes between 410M and 12B parameters. This allows us to demonstrate that our methods are applicable across scale. Our cost-of-verified-recall plots in Figures 1 ###reference_### ###reference_###, 7 ###reference_### ###reference_### and 8 ###reference_### ###reference_### refer to Pythia-12B. Results for other model sizes are presented via the relative-cost (cf. Section 4.2 ###reference_.SSS0.Px2### ###reference_.SSS0.Px2###) plots in the main body Figure 9 ###reference_### ###reference_### and disaggregated via cost-of-verified recall in Section B.3 ###reference_### ###reference_###. All reported results use the negative log probability101010Another popular metric is the difference in logits between the clean and noise target. As opposed to the negative logprob, the logit difference is linear in the final logits and thus might favor Randomized Sampling of Node Effects without Systematic Ranking. A downside of logit difference is that it is sensitive to the noise target, which may not be meaningful if there are multiple plausible completions, such as in IOI. as their loss function. We compute relative to targets from the clean prompt. We briefly explore other metrics in Section B.4 ###reference_### ###reference_###."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Measuring Effectiveness and Efficiency",
            "text": "As mentioned in the introduction, we\u2019re primarily interested in finding the largest-effect nodes \u2013 see Appendix D  ###reference_###  ###reference_### for the distribution of  across models and distributions.\nOnce we have obtained node estimates via a given method, it is relatively cheap to directly measure true effects of top nodes one at a time; we refer to this as \u201cverification\u201d. Incorporating this into our methodology, we find that false positives are typically not a big issue; they are simply revealed during verification. In contrast, false negatives are not so easy to remedy without verifying all nodes, which is what we were trying to avoid.\nWe compare methods on the basis of total compute cost (in # of forward passes) to verify the  nodes with biggest true effect magnitude, for varying . The procedure being measured is to first compute estimates (incurring an estimation cost), and then sweep through nodes in decreasing order of estimated magnitude, measuring their individual effects  (i.e. verifying them), and incurring a verification cost. Then the total cost is the sum of these two costs.\nSometimes we find it useful to summarize the method performance with a scalar; this is useful for comparing methods at a glance across different settings (e.g. model sizes, as in Figure 2  ###reference_###  ###reference_###), or for selecting hyperparameters (cf. Section B.5  ###reference_###  ###reference_###). The cost of verified recall of the top  nodes is of interest for  at varying orders of magnitude. In order to avoid the performance metric being dominated by small or large , we assign similar total weight to different orders of magnitude: we use a weighted average with weight  for the cost of the top  nodes. Similarly, since the costs themselves may have different orders of magnitude, we average them on a log scale \u2013 i.e., we take a geometric mean.\nThis metric is also proportional to the area under the curve in plots like Figure 1  ###reference_###  ###reference_###. To produce a more understandable result, we always report it relative to (i.e. divided by) the oracle verification cost on the same metric; the diagonal line is the oracle, with relative cost 1. We refer to this as the IRWRGM (inverse-rank-weighted relative geometric mean) cost, or the relative cost.\nNote that the preference of the individual practitioner may be different such that this metric is no longer accurately measuring the important rank regime. For example, AtP* pays a notable upfront cost relative to AtP or AtP+QKfix, which sets it at a disadvantage when it doesn\u2019t manage to find additional false negatives; but this may or may not be practically significant. To understand the performance in more detail we advise to refer to the cost of verified recall plots, like Figure 1  ###reference_###  ###reference_### (or many more in Section B.3  ###reference_###  ###reference_###)."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Single Prompt Pairs versus Distributions",
            "text": "We focus many of our experiments on single prompt pairs. This is primarily because it\u2019s easier to set up and get ground truth data. It\u2019s also a simpler setting in which to investigate the question, and one that\u2019s more universally applicable, since a distribution to generalize to is not always available.\n###figure_24### ###figure_25### As a starting point we report results on single prompt pairs which we expect to have relatively clean circuitry111111Formally, these represent prompt distributions via the delta distribution  where  is the singular prompt pair..\nAll singular prompt pairs are shown in Table 1  ###reference_###  ###reference_###. IOI-PP is chosen to resemble an instance from the indirect object identification (IOI) task (Wang et al., 2022  ###reference_b54###  ###reference_b54###), a task predominantly involving attention heads. CITY-PP is chosen to elicit factual recall which previous research suggests involves early MLPs and a small number of late attention heads (Meng et al., 2023  ###reference_b32###  ###reference_b32###; Geva et al., 2023  ###reference_b17###  ###reference_b17###; Nanda et al., 2023  ###reference_b37###  ###reference_b37###). The country/city combinations were chosen such that Pythia-410M achieved low loss on both  and  and such that all places were represented by a single token.\n###table_2### We show the cost of verified 100% recall for various methods in Figure 1  ###reference_###  ###reference_###, where we focus on NeuronNodes for CITY-PP and AttentionNodes for IOI-PP. Exhaustive results for smaller Pythia models are shown in Section B.3  ###reference_###  ###reference_###. Figure 2  ###reference_###  ###reference_### shows the aggregated relative costs for all models on CITY-PP and IOI-PP.\nInstead of applying the strict criterion of recalling all important nodes, we can also relax this constraint. In Figure 7  ###reference_###  ###reference_###, we show the cost of verified 90% recall in the two clean prompt pair settings.\nThe previous prompt pairs may in fact be the best-case scenarios: the interventions they create will be fairly localized to a specific circuit, and this may make it easy for AtP to approximate the contributions. It may thus be informative to see how the methods generalize to settings where the interventions are less surgical. To do this, we also report results in Figure 8  ###reference_###  ###reference_### (top) and Figure 9  ###reference_###  ###reference_### on a random prompt pair chosen from a non-copyright-protected section of The Pile (Gao et al., 2020  ###reference_b12###  ###reference_b12###) which we refer to as RAND-PP. The prompt pair was chosen such that Pythia-410M still achieved low loss on both prompts.\n###figure_26### ###figure_27### ###figure_28### ###figure_29### ###figure_30### ###figure_31### ###figure_32### ###figure_33### ###figure_34### We find that AtP/AtP* is only somewhat less effective here; this provides tentative evidence that the strong performance of AtP/AtP* isn\u2019t reliant on the clean prompt using a particularly crisp circuit, or on the noise prompt being a precise control.\nCausal attribution is often of most interest when evaluated across a distribution, as laid out in Section 2  ###reference_###  ###reference_###. Of the methods, AtP, AtP*, and Subsampling scale reasonably to distributions; the former 2 because they\u2019re inexpensive so running them  times is not prohibitive, and Subsampling because it intrinsically averages across the distribution and thus becomes proportionally cheaper relative to the verification via activation patching. In addition, having a distribution enables a more performant Iterative method, as described in Section 3.3  ###reference_.SSS0.Px1###  ###reference_.SSS0.Px1###.\nWe present a comparison of these methods on 2 distributional settings. The first is a reduced version of IOI (Wang et al., 2022  ###reference_b54###  ###reference_b54###) on 6 names, resulting in  prompt pairs, where we evaluate AttentionNodes. The other distribution prompts the model to output an indefinite article \u2018 a\u2019 or \u2018 an\u2019, where we evaluate NeuronNodes. See Section B.1  ###reference_###  ###reference_### for details on constructing these distributions. Results are shown in Figure 8  ###reference_###  ###reference_### for Pythia 12B, and in Figure 9  ###reference_###  ###reference_### across models. The results show that AtP continues to perform well, especially with the QK fix; in addition, the cancellation failure mode tends to be sensitive to the particular input prompt pair, and as a result, averaging across a distribution diminishes the benefit of GradDrops.\nAn implication of Subsampling scaling well to this setting is that diagnostics may give reasonable confidence in not missing false negatives with much less overhead than in the single-prompt-pair case; this is illustrated in Figure 6  ###reference_###  ###reference_###."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "We only considered a small set of prompt pair distributions, which often were limited to a single prompt pair, since evaluating the ground truth can be quite costly. While we aimed to evaluate on distributions that are reasonably representative, our results may not generalize to other distributions.\nIn the NeuronNodes setting, we took MLP neurons as our fundamental unit of analysis. However, there is mounting evidence (Bricken et al., 2023  ###reference_b3###) that the decomposition of signals into neuron contributions does not correspond directly to a semantically meaningful decomposition. Instead, achieving such a decomposition seems to require finding the right set of directions in neuron activation space (Bricken et al., 2023  ###reference_b3###; Gurnee et al., 2023  ###reference_b19###) \u2013 which we viewed as being out of scope for this paper. In Section 5.2  ###reference_### we further discuss the applicability of AtP to sparse autoencoders, a method of finding these decompositions.\nMore generally, we only considered relatively fine-grained nodes, because this is a case where very exhaustive verification is prohibitively expensive, justifying the need for an approximate, fast method. Nanda (2022  ###reference_b36###) speculate that AtP may perform worse on coarser components like full layers or entire residual streams, as a larger change may have more of a non-linear effect. There may still be benefit in speeding up such an analysis, particularly if the context length is long \u2013 our alternative methods may have something to offer here, though we leave investigation of this to future work.\nIt is popular in the literature to do Activation Patching with these larger components, with short contexts \u2013 this doesn\u2019t pose a performance issue, and so our work would not provide any benefit here.\nIn this work we took the ground truth of activation patching, as defined in Equation 1  ###reference_###, as our evaluation target.\nAs discussed by McGrath et al. (2023  ###reference_b31###), Equation 1  ###reference_### often significantly disagrees with a different evaluation target, the \u201cdirect effect\u201d, by putting lower weight on some contributions when later components would shift their behaviour to compensate for the earlier patched component. In the worst case this could be seen as producing additional false negatives not accounted for by our metrics. To some degree this is likely to be mitigated by the GradDrop formula in Eq. 11  ###reference_###, which will include a term dropping out the effect of that downstream shift.\nHowever, it is also questionable whether we need to concern ourselves with finding high-direct-effect nodes. For example, direct effect is easy to efficiently compute for all nodes, as explored by nostalgebraist (2020  ###reference_b38###) \u2013 so there is no need for fast approximations like AtP if direct effect is the quantity of interest. This ease of computation is no free lunch, though, because direct effect is also more limited as a tool for finding causally important nodes: it would not be able to locate any nodes that contribute only instrumentally to the circuit rather than producing its output. For example, there is no direct effect from nodes at non-final token positions. We discuss the direct effect further in Section 3.1.2  ###reference_.SSS2### and Section A.2.2  ###reference_.SSS2###.\nAnother nuance of our ground\u2013truth definition occurs in the distributional setting. Some nodes may have a real and significant effect, but only on a single clean prompt (e.g. they only respond to a particular name in IOI121212We did observe this particular behavior in a few instances. or object in A-AN). Since the effect is averaged over the distribution, the ground truth will not assign these nodes large causal importance. Depending on the goal of the practitioner this may or may not be desirable.\nWhen evaluating the performance of various estimators, we focused on evaluating the relative rank of estimates, since our main goal was to identify important components (with effect size only instrumentally useful to this end), and we assumed a further verification step of the nodes with highest estimated effects one at a time, in contexts where knowing effect size is important. Thus, we do not present evidence about how closely the estimated effect magnitudes from AtP or AtP* match the ground truth. Similarly, we did not assess the prevalence of false positives in our analysis, because they can be filtered out via the verification process. Finally, we did not compare to past manual interpretability work to check whether our methods find the same nodes to be causally important as discovered by human researchers, as done in prior work (Conmy et al., 2023  ###reference_b7###; Syed et al., 2023  ###reference_b47###).\nWhile we think it likely that our results on the Pythia model family (Biderman et al., 2023  ###reference_b2###) will transfer to other LLM families, we cannot rule out qualitatively different behavior without further evidence, especially on SotA\u2013scale models or models that significantly deviate from the standard decoder-only transformer architecture.\nWhile we focus on computing the effects of individual nodes, edge activation patching can give more fine-grained information about which paths in the computational graph matter. However, it suffers from an even larger blowup in number of forward passes if done naively. Fortunately, AtP is easy to generalize to estimating the effects of edges between nodes (Nanda, 2022  ###reference_b36###; Syed et al., 2023  ###reference_b47###), while AtP* may provide further improvement. We discuss edge-AtP, and how to efficiently carry over the insights from AtP*, in Section C.2  ###reference_###.\nWe focused on fine-grained attribution, rather than full layers or sliding windows (Meng et al., 2023  ###reference_b32###; Geva et al., 2023  ###reference_b17###). In the latter case there\u2019s less computational blowup to resolve, but for long contexts there may still be benefit in considering speedups like ours; on the other hand, they may be less linear, thus favouring other methods over AtP*. We leave investigation of this to future work.\nNanda (2022  ###reference_b36###) observed that AtP\u2019s approximation to layer normalization may be a worse approximation when it comes to patching larger/coarser nodes: on average the patched and clean activations are likely to have similar norm, but may not have high cosine-similarity. They recommend treating the denominator in layer normalization as fixed, e.g. using a stop-gradient operator in the implementation. In Section C.1  ###reference_### we explore the effect of this, and illustrate the behaviour of this alternative form of AtP. It seems likely that this variant would indeed produce better results particularly when patching residual-stream nodes \u2013 but we leave empirical investigation of this to future work.\nDenoising (Meng et al., 2023  ###reference_b32###; Lieberum et al., 2023  ###reference_b28###) is a different use case for patching, which may produce moderately different results: the difference is that each forward pass is run on  with the activation to patch taken from  \u2014 colloquially, this tests whether the patched activation is sufficient to recover model performance on , rather than necessary. We provide some preliminary evidence to the effect of this choice in Section B.4  ###reference_### but leave a more thorough investigation to future work.\nFurther, in some settings it may be of interest to do mean-ablation, or even zero-ablation, and our tweaks remain applicable there; the random-prompt-pair result suggests AtP* isn\u2019t overly sensitive to the noise distribution, so we speculate the results are likely to carry over.\nA natural application of the methods we discussed in this work is the automatic identification and localization of sparse subgraphs or \u2018circuits\u2019 (Cammarata et al., 2020  ###reference_b4###). A variant of this was already discussed in concurrent work by Syed et al. (2023  ###reference_b47###) who combined edge attribution patching with the ACDC algorithm (Conmy et al., 2023  ###reference_b7###). As we mentioned in the edge patching discussion, AtP* can be generalized to edge attribution patching, which may bring additional benefit for automated circuit discovery.\nAnother approach is to learn a (probabilistic) mask over nodes, similar to Louizos et al. (2018  ###reference_b29###); Cao et al. (2021  ###reference_b5###), where the probability scales with the currently estimated node contribution . For that approach, a fast method to estimate all node effects given the current mask probabilities could prove vital.\nRecently there has been increased interest by the community in using sparse autoencoders (SAEs) to construct disentangled sparse representations with potentially more semantic coherence than transformer-native units such as neurons (Cunningham et al., 2023  ###reference_b8###; Bricken et al., 2023  ###reference_b3###).\nSAEs usually have a lot more nodes than the corresponding transformer block they are applied to. This could pose a larger problem in terms of the activation patching effects, making the speedup of AtP* more valuable. However, due to the sparseness of the SAE, on a given forward pass the effect of most features will be zero. For example, some successful SAEs by Bricken et al. (2023  ###reference_b3###) have 10-20 active features for 500 neurons for a given token position, which reduces the number of nodes by 20-50x relative to the MLP setting, increasing the scale at which existing iterative methods remain practical. It is still an open research question, however, what degree of sparsity is feasible with tolerable reconstruction error for practically relevant or SOTA\u2013scale models, where the methods discussed in this work may become more important again.\nAtP* could be used to discover single nodes in the model that can be leveraged for targeted inference time interventions to control the model\u2019s behavior. In contrast to previous work (Li et al., 2023  ###reference_b27###; Turner et al., 2023  ###reference_b50###; Zou et al., 2023  ###reference_b56###) it might provide more localized interventions with less impact on the rest of the model\u2019s computation. One potential exciting direction would be to use AtP* (or other gradient-based approximations) to see which sparse autoencoder features, if activated, would have a significant effect."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Limitations",
            "text": "We only considered a small set of prompt pair distributions, which often were limited to a single prompt pair, since evaluating the ground truth can be quite costly. While we aimed to evaluate on distributions that are reasonably representative, our results may not generalize to other distributions.\nIn the NeuronNodes setting, we took MLP neurons as our fundamental unit of analysis. However, there is mounting evidence (Bricken et al., 2023  ###reference_b3###  ###reference_b3###) that the decomposition of signals into neuron contributions does not correspond directly to a semantically meaningful decomposition. Instead, achieving such a decomposition seems to require finding the right set of directions in neuron activation space (Bricken et al., 2023  ###reference_b3###  ###reference_b3###; Gurnee et al., 2023  ###reference_b19###  ###reference_b19###) \u2013 which we viewed as being out of scope for this paper. In Section 5.2  ###reference_###  ###reference_### we further discuss the applicability of AtP to sparse autoencoders, a method of finding these decompositions.\nMore generally, we only considered relatively fine-grained nodes, because this is a case where very exhaustive verification is prohibitively expensive, justifying the need for an approximate, fast method. Nanda (2022  ###reference_b36###  ###reference_b36###) speculate that AtP may perform worse on coarser components like full layers or entire residual streams, as a larger change may have more of a non-linear effect. There may still be benefit in speeding up such an analysis, particularly if the context length is long \u2013 our alternative methods may have something to offer here, though we leave investigation of this to future work.\nIt is popular in the literature to do Activation Patching with these larger components, with short contexts \u2013 this doesn\u2019t pose a performance issue, and so our work would not provide any benefit here.\nIn this work we took the ground truth of activation patching, as defined in Equation 1  ###reference_###  ###reference_###, as our evaluation target.\nAs discussed by McGrath et al. (2023  ###reference_b31###  ###reference_b31###), Equation 1  ###reference_###  ###reference_### often significantly disagrees with a different evaluation target, the \u201cdirect effect\u201d, by putting lower weight on some contributions when later components would shift their behaviour to compensate for the earlier patched component. In the worst case this could be seen as producing additional false negatives not accounted for by our metrics. To some degree this is likely to be mitigated by the GradDrop formula in Eq. 11  ###reference_###  ###reference_###, which will include a term dropping out the effect of that downstream shift.\nHowever, it is also questionable whether we need to concern ourselves with finding high-direct-effect nodes. For example, direct effect is easy to efficiently compute for all nodes, as explored by nostalgebraist (2020  ###reference_b38###  ###reference_b38###) \u2013 so there is no need for fast approximations like AtP if direct effect is the quantity of interest. This ease of computation is no free lunch, though, because direct effect is also more limited as a tool for finding causally important nodes: it would not be able to locate any nodes that contribute only instrumentally to the circuit rather than producing its output. For example, there is no direct effect from nodes at non-final token positions. We discuss the direct effect further in Section 3.1.2  ###reference_.SSS2###  ###reference_.SSS2### and Section A.2.2  ###reference_.SSS2###  ###reference_.SSS2###.\nAnother nuance of our ground\u2013truth definition occurs in the distributional setting. Some nodes may have a real and significant effect, but only on a single clean prompt (e.g. they only respond to a particular name in IOI121212We did observe this particular behavior in a few instances. or object in A-AN). Since the effect is averaged over the distribution, the ground truth will not assign these nodes large causal importance. Depending on the goal of the practitioner this may or may not be desirable.\nWhen evaluating the performance of various estimators, we focused on evaluating the relative rank of estimates, since our main goal was to identify important components (with effect size only instrumentally useful to this end), and we assumed a further verification step of the nodes with highest estimated effects one at a time, in contexts where knowing effect size is important. Thus, we do not present evidence about how closely the estimated effect magnitudes from AtP or AtP* match the ground truth. Similarly, we did not assess the prevalence of false positives in our analysis, because they can be filtered out via the verification process. Finally, we did not compare to past manual interpretability work to check whether our methods find the same nodes to be causally important as discovered by human researchers, as done in prior work (Conmy et al., 2023  ###reference_b7###  ###reference_b7###; Syed et al., 2023  ###reference_b47###  ###reference_b47###).\nWhile we think it likely that our results on the Pythia model family (Biderman et al., 2023  ###reference_b2###  ###reference_b2###) will transfer to other LLM families, we cannot rule out qualitatively different behavior without further evidence, especially on SotA\u2013scale models or models that significantly deviate from the standard decoder-only transformer architecture."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Extensions/Variants",
            "text": "While we focus on computing the effects of individual nodes, edge activation patching can give more fine-grained information about which paths in the computational graph matter. However, it suffers from an even larger blowup in number of forward passes if done naively. Fortunately, AtP is easy to generalize to estimating the effects of edges between nodes (Nanda, 2022  ###reference_b36###  ###reference_b36###; Syed et al., 2023  ###reference_b47###  ###reference_b47###), while AtP* may provide further improvement. We discuss edge-AtP, and how to efficiently carry over the insights from AtP*, in Section C.2  ###reference_###  ###reference_###.\nWe focused on fine-grained attribution, rather than full layers or sliding windows (Meng et al., 2023  ###reference_b32###  ###reference_b32###; Geva et al., 2023  ###reference_b17###  ###reference_b17###). In the latter case there\u2019s less computational blowup to resolve, but for long contexts there may still be benefit in considering speedups like ours; on the other hand, they may be less linear, thus favouring other methods over AtP*. We leave investigation of this to future work.\nNanda (2022  ###reference_b36###  ###reference_b36###) observed that AtP\u2019s approximation to layer normalization may be a worse approximation when it comes to patching larger/coarser nodes: on average the patched and clean activations are likely to have similar norm, but may not have high cosine-similarity. They recommend treating the denominator in layer normalization as fixed, e.g. using a stop-gradient operator in the implementation. In Section C.1  ###reference_###  ###reference_### we explore the effect of this, and illustrate the behaviour of this alternative form of AtP. It seems likely that this variant would indeed produce better results particularly when patching residual-stream nodes \u2013 but we leave empirical investigation of this to future work.\nDenoising (Meng et al., 2023  ###reference_b32###  ###reference_b32###; Lieberum et al., 2023  ###reference_b28###  ###reference_b28###) is a different use case for patching, which may produce moderately different results: the difference is that each forward pass is run on  with the activation to patch taken from  \u2014 colloquially, this tests whether the patched activation is sufficient to recover model performance on , rather than necessary. We provide some preliminary evidence to the effect of this choice in Section B.4  ###reference_###  ###reference_### but leave a more thorough investigation to future work.\nFurther, in some settings it may be of interest to do mean-ablation, or even zero-ablation, and our tweaks remain applicable there; the random-prompt-pair result suggests AtP* isn\u2019t overly sensitive to the noise distribution, so we speculate the results are likely to carry over."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Applications",
            "text": "A natural application of the methods we discussed in this work is the automatic identification and localization of sparse subgraphs or \u2018circuits\u2019 (Cammarata et al., 2020  ###reference_b4###  ###reference_b4###). A variant of this was already discussed in concurrent work by Syed et al. (2023  ###reference_b47###  ###reference_b47###) who combined edge attribution patching with the ACDC algorithm (Conmy et al., 2023  ###reference_b7###  ###reference_b7###). As we mentioned in the edge patching discussion, AtP* can be generalized to edge attribution patching, which may bring additional benefit for automated circuit discovery.\nAnother approach is to learn a (probabilistic) mask over nodes, similar to Louizos et al. (2018  ###reference_b29###  ###reference_b29###); Cao et al. (2021  ###reference_b5###  ###reference_b5###), where the probability scales with the currently estimated node contribution . For that approach, a fast method to estimate all node effects given the current mask probabilities could prove vital.\nRecently there has been increased interest by the community in using sparse autoencoders (SAEs) to construct disentangled sparse representations with potentially more semantic coherence than transformer-native units such as neurons (Cunningham et al., 2023  ###reference_b8###  ###reference_b8###; Bricken et al., 2023  ###reference_b3###  ###reference_b3###).\nSAEs usually have a lot more nodes than the corresponding transformer block they are applied to. This could pose a larger problem in terms of the activation patching effects, making the speedup of AtP* more valuable. However, due to the sparseness of the SAE, on a given forward pass the effect of most features will be zero. For example, some successful SAEs by Bricken et al. (2023  ###reference_b3###  ###reference_b3###) have 10-20 active features for 500 neurons for a given token position, which reduces the number of nodes by 20-50x relative to the MLP setting, increasing the scale at which existing iterative methods remain practical. It is still an open research question, however, what degree of sparsity is feasible with tolerable reconstruction error for practically relevant or SOTA\u2013scale models, where the methods discussed in this work may become more important again.\nAtP* could be used to discover single nodes in the model that can be leveraged for targeted inference time interventions to control the model\u2019s behavior. In contrast to previous work (Li et al., 2023  ###reference_b27###  ###reference_b27###; Turner et al., 2023  ###reference_b50###  ###reference_b50###; Zou et al., 2023  ###reference_b56###  ###reference_b56###) it might provide more localized interventions with less impact on the rest of the model\u2019s computation. One potential exciting direction would be to use AtP* (or other gradient-based approximations) to see which sparse autoencoder features, if activated, would have a significant effect."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Recommendation",
            "text": "Our results suggest that if a practitioner is trying to do fast causal attribution, there are 2 main factors to consider: (i) the desired granularity of localization, and (ii) the confidence vs compute tradeoff.\nRegarding (i), the desired granularity, smaller components (e.g. MLP neurons or attention heads) are more numerous but more linear, likely yielding better results from gradient-based methods like AtP. We are less sure AtP will be a good approximation if patching layers or sliding windows of layers, and in this case practitioners may want to do normal patching. If the number of forward passes required remains prohibitive (e.g. a long context times many layers, when doing per token  layer patching), our other baselines may be useful. For a single prompt pair we particularly recommend trying Blocks, as it\u2019s easy to make sense of; for a distribution we recommend Subsampling because it scales better to many prompt pairs.\nRegarding (ii), the confidence vs compute tradeoff, depending on the application, it may be desirable to run AtP as an activation patching prefilter followed by running the diagnostic to increase confidence. On the other hand, if false negatives aren\u2019t a big concern then it may be preferable to skip the diagnostic \u2013 and if false positives aren\u2019t either, then in certain cases practitioners may want to skip activation patching verification entirely. In addition, if the prompt pair distribution does not adequately highlight the specific circuit/behaviour of interest, this may also limit what can be learned from any localization methods.\nIf AtP is appropriate, our results suggest the best variant to use is probably AtP* for single prompt pairs, AtP+QKFix for AttentionNodes on distributions, and AtP for NeuronNodes (or other sites that aren\u2019t immediately before a nonlinearity) on distributions.\nOf course, these recommendations are best-substantiated in settings similar to those we studied: focused prompt pairs / distribution, attention node or neuron sites, nodewise attribution, measuring cross-entropy loss on the clean-prompt next token. If departing from these assumptions we recommend looking before you leap."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Related work",
            "text": "This work is concerned with identifying the effect of all (important) nodes in a causal graph (Pearl, 2000  ###reference_b40###), in the specific case where the graph represents a language model\u2019s computation. A key method for finding important intermediate nodes in a causal graph is intervening on those nodes and observing the effect, which was first discussed under the name of causal mediation analysis by Robins and Greenland (1992  ###reference_b44###); Pearl (2001  ###reference_b41###).\nIn recent years there has been increasing success at applying the ideas of causal mediation analysis to identify causally important nodes in deep neural networks, in particular via the method of activation patching, where the output of a model component is intervened on. This technique has been widely used by the community and successfully applied in a range of contexts (Olsson et al., 2022  ###reference_b39###; Vig et al., 2020  ###reference_b53###; Soulos et al., 2020  ###reference_b45###; Meng et al., 2023  ###reference_b32###; Wang et al., 2022  ###reference_b54###; Hase et al., 2023  ###reference_b21###; Lieberum et al., 2023  ###reference_b28###; Conmy et al., 2023  ###reference_b7###; Hanna et al., 2023  ###reference_b20###; Geva et al., 2023  ###reference_b17###; Huang et al., 2023  ###reference_b24###; Tigges et al., 2023  ###reference_b48###; Merullo et al., 2023  ###reference_b33###; McDougall et al., 2023  ###reference_b30###; Goldowsky-Dill et al., 2023  ###reference_b18###; Stolfo et al., 2023  ###reference_b46###; Feng and Steinhardt, 2023  ###reference_b9###; Hendel et al., 2023  ###reference_b22###; Todd et al., 2023  ###reference_b49###; Cunningham et al., 2023  ###reference_b8###; Finlayson et al., 2021  ###reference_b11###; Nanda et al., 2023  ###reference_b37###).\nChan et al. (2022  ###reference_b6###) introduce causal scrubbing, a generalized algorithm to verify a hypothesis about the internal mechanism underlying a model\u2019s behavior, and detail their motivation behind performing noising and resample ablation rather than denoising or using mean or zero ablation \u2013 they interpret the hypothesis as implying the computation is invariant to some large set of perturbations, so their starting-point is the clean unperturbed forward pass.131313Our motivation for focusing on noising rather than denoising was a closely related one \u2013 we were motivated by automated circuit discovery, where gradually noising more and more of the model is the basic methodology for both of the approaches discussed in Section 5.3  ###reference_###.\nAnother line of research concerning formalizing causal abstractions focuses on finding and verifying high-level causal abstractions of low-level variables (Geiger et al., 2020  ###reference_b13###, 2021  ###reference_b14###, 2022  ###reference_b15###, 2023  ###reference_b16###). See Jenner et al. (2022  ###reference_b25###) for more details on how these different frameworks agree and differ. In contrast to those works, we are chiefly concerned with identifying the important low-level variables in the computational graph and are not investigating their semantics or potential groupings of lower-level into higher-level variables.\nIn addition to causal mediation analysis, intervening on node activations in the model forward pass has also been studied as a way of steering models towards desirable behavior (Rimsky et al., 2023  ###reference_b43###; Zou et al., 2023  ###reference_b56###; Turner et al., 2023  ###reference_b50###; Jorgensen et al., 2023  ###reference_b26###; Li et al., 2023  ###reference_b27###; Belrose et al., 2023  ###reference_b1###).\nWhile we use the resample\u2013ablation variant of AtP as formulated in Nanda (2022  ###reference_b36###), similar formulations have been used in the past to successfully prune deep neural networks (Figurnov et al., 2016  ###reference_b10###; Molchanov et al., 2017  ###reference_b35###; Michel et al., 2019  ###reference_b34###), or even identify causally important nodes for interpretability (Cao et al., 2021  ###reference_b5###). Concurrent work by Syed et al. (2023  ###reference_b47###) also demonstrates AtP can help with automatically finding causally important circuits in a way that agrees with previous manual circuit identification work. In contrast to Syed et al. (2023  ###reference_b47###), we provide further analysis of AtP\u2019s failure modes, give improvements in the form of AtP, and evaluate both methods as well as several baselines on a suite of larger models against a ground truth that is independent of human researchers\u2019 judgement."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we have explored the use of attribution patching for node patch effect evaluation. We have compared attribution patching with alternatives and augmentations, characterized its failure modes, and presented reliability diagnostics. We have also discussed the implications of our contributions for other settings in which patching can be of interest, such as circuit discovery, edge localization, coarse-grained localization, and causal abstraction.\nOur results show that AtP* can be a more reliable and scalable approach to node patch effect evaluation than alternatives. However, it is important to be aware of the failure modes of attribution patching, such as cancellation and saturation. We explored these in some detail, and provided mitigations, as well as recommendations for diagnostics to ensure that the results are reliable.\nWe believe that our work makes an important contribution to the field of mechanistic interpretability and will help to advance the development of more reliable and scalable methods for understanding the behavior of deep neural networks."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Author Contributions",
            "text": "J\u00e1nos Kram\u00e1r was research lead, and Tom Lieberum was also a core contributor \u2013 both were highly involved in most aspects of the project. Rohin Shah and Neel Nanda served as advisors and gave feedback and guidance throughout."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A Method details",
            "text": ""
        },
        {
            "section_id": "Appendix 2",
            "parent_section_id": null,
            "section_name": "Appendix B Experiments",
            "text": ""
        },
        {
            "section_id": "Appendix 3",
            "parent_section_id": null,
            "section_name": "Appendix C AtP variants",
            "text": ""
        },
        {
            "section_id": "Appendix 4",
            "parent_section_id": null,
            "section_name": "Appendix D Distribution of true effects",
            "text": "In Figure 17  ###reference_###, we show the distribution of  across models and distributions.\nAttentionNodes\nNeuronNodes\n###figure_35### ###figure_36### ###figure_37### ###figure_38### ###figure_39### ###figure_40### ###figure_41### ###figure_42###"
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T1\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T1.4\">\n<tr class=\"ltx_tr\" id=\"S4.T1.4.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.4.1.1\">Identifier</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.4.1.2\">Clean Prompt</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.1.3\">Noise Source Prompt</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.4.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S4.T1.4.2.1\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S4.T1.4.2.1.1\">CITY-PP</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S4.T1.4.2.2\">\n<span class=\"ltx_text\" id=\"S4.T1.4.2.2.1\"></span> <span class=\"ltx_text\" id=\"S4.T1.4.2.2.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T1.4.2.2.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T1.4.2.2.2.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.2.2.2.1.1.1\"><span class=\"ltx_text ltx_font_typewriter ltx_framed_rectangle\" id=\"S4.T1.4.2.2.2.1.1.1.1\" style=\"border-color: black;\">BOS</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.2.2.2.1.1.1.2\" style=\"border-color: black;\">City</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.2.2.2.1.1.1.3\" style=\"border-color: black;\">:</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.2.2.2.1.1.1.4\" style=\"border-color: black;\">\u2423Barcelona</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.2.2.2.1.1.1.5\" style=\"border-color: black;\">\\n</span></span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.4.2.2.2.1.2\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.2.2.2.1.2.1\"><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.2.2.2.1.2.1.1\" style=\"border-color: black;\">Country</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.2.2.2.1.2.1.2\" style=\"border-color: black;\">:</span><span class=\"ltx_text\" id=\"S4.T1.4.2.2.2.1.2.1.3\" style=\"border:3px double black;;\">\u2423Spain</span></span></span>\n</span></span> <span class=\"ltx_text\" id=\"S4.T1.4.2.2.3\"></span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T1.4.2.3\">\n<span class=\"ltx_text\" id=\"S4.T1.4.2.3.1\"></span> <span class=\"ltx_text\" id=\"S4.T1.4.2.3.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T1.4.2.3.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T1.4.2.3.2.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.2.3.2.1.1.1\"><span class=\"ltx_text ltx_font_typewriter ltx_framed_rectangle\" id=\"S4.T1.4.2.3.2.1.1.1.1\" style=\"border-color: black;\">BOS</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.2.3.2.1.1.1.2\" style=\"border-color: black;\">City</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.2.3.2.1.1.1.3\" style=\"border-color: black;\">:</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.2.3.2.1.1.1.4\" style=\"border-color: black;\">\u2423Beijing</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.2.3.2.1.1.1.5\" style=\"border-color: black;\">\\n</span></span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.4.2.3.2.1.2\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.2.3.2.1.2.1\"><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.2.3.2.1.2.1.1\" style=\"border-color: black;\">Country</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.2.3.2.1.2.1.2\" style=\"border-color: black;\">:</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.2.3.2.1.2.1.3\" style=\"color:#808080;border-color: #808080;\">\u2423China</span></span></span>\n</span></span> <span class=\"ltx_text\" id=\"S4.T1.4.2.3.3\"></span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.4.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.4.3.1\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S4.T1.4.3.1.1\">IOI-PP</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.4.3.2\">\n<span class=\"ltx_text\" id=\"S4.T1.4.3.2.1\"></span> <span class=\"ltx_text\" id=\"S4.T1.4.3.2.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T1.4.3.2.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T1.4.3.2.2.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.3.2.2.1.1.1\"><span class=\"ltx_text ltx_font_typewriter ltx_framed_rectangle\" id=\"S4.T1.4.3.2.2.1.1.1.1\" style=\"border-color: black;\">BOS</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.3.2.2.1.1.1.2\" style=\"border-color: black;\">When</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.3.2.2.1.1.1.3\" style=\"border-color: black;\">\u2423Michael</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.3.2.2.1.1.1.4\" style=\"border-color: black;\">\u2423and</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.3.2.2.1.1.1.5\" style=\"border-color: black;\">\u2423Jessica</span></span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.4.3.2.2.1.2\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.3.2.2.1.2.1\"><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.3.2.2.1.2.1.1\" style=\"border-color: black;\">\u2423went</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.3.2.2.1.2.1.2\" style=\"border-color: black;\">\u2423to</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.3.2.2.1.2.1.3\" style=\"border-color: black;\">\u2423the</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.3.2.2.1.2.1.4\" style=\"border-color: black;\">\u2423bar</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.3.2.2.1.2.1.5\" style=\"border-color: black;\">,</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.3.2.2.1.2.1.6\" style=\"border-color: black;\">\u2423Michael</span></span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.4.3.2.2.1.3\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.3.2.2.1.3.1\"><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.3.2.2.1.3.1.1\" style=\"border-color: black;\">\u2423gave</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.3.2.2.1.3.1.2\" style=\"border-color: black;\">\u2423a</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.3.2.2.1.3.1.3\" style=\"border-color: black;\">\u2423drink</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.3.2.2.1.3.1.4\" style=\"border-color: black;\">\u2423to</span><span class=\"ltx_text\" id=\"S4.T1.4.3.2.2.1.3.1.5\" style=\"border:3px double black;;\">\u2423Jessica</span></span></span>\n</span></span> <span class=\"ltx_text\" id=\"S4.T1.4.3.2.3\"></span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.4.3.3\">\n<span class=\"ltx_text\" id=\"S4.T1.4.3.3.1\"></span> <span class=\"ltx_text\" id=\"S4.T1.4.3.3.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T1.4.3.3.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T1.4.3.3.2.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.3.3.2.1.1.1\"><span class=\"ltx_text ltx_font_typewriter ltx_framed_rectangle\" id=\"S4.T1.4.3.3.2.1.1.1.1\" style=\"border-color: black;\">BOS</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.3.3.2.1.1.1.2\" style=\"border-color: black;\">When</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.3.3.2.1.1.1.3\" style=\"border-color: black;\">\u2423Michael</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.3.3.2.1.1.1.4\" style=\"border-color: black;\">\u2423and</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.3.3.2.1.1.1.5\" style=\"border-color: black;\">\u2423Jessica</span></span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.4.3.3.2.1.2\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.3.3.2.1.2.1\"><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.3.3.2.1.2.1.1\" style=\"border-color: black;\">\u2423went</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.3.3.2.1.2.1.2\" style=\"border-color: black;\">\u2423to</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.3.3.2.1.2.1.3\" style=\"border-color: black;\">\u2423the</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.3.3.2.1.2.1.4\" style=\"border-color: black;\">\u2423bar</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.3.3.2.1.2.1.5\" style=\"border-color: black;\">,</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.3.3.2.1.2.1.6\" style=\"border-color: black;\">\u2423Ashley</span></span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.4.3.3.2.1.3\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.3.3.2.1.3.1\"><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.3.3.2.1.3.1.1\" style=\"border-color: black;\">\u2423gave</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.3.3.2.1.3.1.2\" style=\"border-color: black;\">\u2423a</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.3.3.2.1.3.1.3\" style=\"border-color: black;\">\u2423drink</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.3.3.2.1.3.1.4\" style=\"border-color: black;\">\u2423to</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.3.3.2.1.3.1.5\" style=\"color:#808080;border-color: #808080;\">\u2423Michael</span></span></span>\n</span></span> <span class=\"ltx_text\" id=\"S4.T1.4.3.3.3\"></span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.4.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.4.4.1\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S4.T1.4.4.1.1\">RAND-PP</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.4.4.2\">\n<span class=\"ltx_text\" id=\"S4.T1.4.4.2.1\"></span> <span class=\"ltx_text\" id=\"S4.T1.4.4.2.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T1.4.4.2.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T1.4.4.2.2.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.4.2.2.1.1.1\"><span class=\"ltx_text ltx_font_typewriter ltx_framed_rectangle\" id=\"S4.T1.4.4.2.2.1.1.1.1\" style=\"border-color: black;\">BOS</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.4.2.2.1.1.1.2\" style=\"border-color: black;\">Her</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.4.2.2.1.1.1.3\" style=\"border-color: black;\">\u2423biggest</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.4.2.2.1.1.1.4\" style=\"border-color: black;\">\u2423worry</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.4.2.2.1.1.1.5\" style=\"border-color: black;\">\u2423was</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.4.2.2.1.1.1.6\" style=\"border-color: black;\">\u2423the</span></span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.4.4.2.2.1.2\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.4.2.2.1.2.1\"><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.4.2.2.1.2.1.1\" style=\"border-color: black;\">\u2423festival</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.4.2.2.1.2.1.2\" style=\"border-color: black;\">\u2423might</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.4.2.2.1.2.1.3\" style=\"border-color: black;\">\u2423suffer</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.4.2.2.1.2.1.4\" style=\"border-color: black;\">\u2423and</span></span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.4.4.2.2.1.3\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.4.2.2.1.3.1\"><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.4.2.2.1.3.1.1\" style=\"border-color: black;\">\u2423people</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.4.2.2.1.3.1.2\" style=\"border-color: black;\">\u2423might</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.4.2.2.1.3.1.3\" style=\"border-color: black;\">\u2423erroneously</span><span class=\"ltx_text\" id=\"S4.T1.4.4.2.2.1.3.1.4\" style=\"border:3px double black;;\">\u2423think</span></span></span>\n</span></span> <span class=\"ltx_text\" id=\"S4.T1.4.4.2.3\"></span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.4.4.3\">\n<span class=\"ltx_text\" id=\"S4.T1.4.4.3.1\"></span> <span class=\"ltx_text\" id=\"S4.T1.4.4.3.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T1.4.4.3.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T1.4.4.3.2.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.4.3.2.1.1.1\"><span class=\"ltx_text ltx_font_typewriter ltx_framed_rectangle\" id=\"S4.T1.4.4.3.2.1.1.1.1\" style=\"border-color: black;\">BOS</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.4.3.2.1.1.1.2\" style=\"border-color: black;\">also</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.4.3.2.1.1.1.3\" style=\"border-color: black;\">\u2423think</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.4.3.2.1.1.1.4\" style=\"border-color: black;\">\u2423that</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.4.3.2.1.1.1.5\" style=\"border-color: black;\">\u2423there</span></span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.4.4.3.2.1.2\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.4.3.2.1.2.1\"><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.4.3.2.1.2.1.1\" style=\"border-color: black;\">\u2423should</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.4.3.2.1.2.1.2\" style=\"border-color: black;\">\u2423be</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.4.3.2.1.2.1.3\" style=\"border-color: black;\">\u2423the</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.4.3.2.1.2.1.4\" style=\"border-color: black;\">\u2423same</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.4.3.2.1.2.1.5\" style=\"border-color: black;\">\u2423rules</span></span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.4.4.3.2.1.3\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.4.3.2.1.3.1\"><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.4.3.2.1.3.1.1\" style=\"border-color: black;\">\u2423or</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.4.3.2.1.3.1.2\" style=\"border-color: black;\">\u2423regulations</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.4.3.2.1.3.1.3\" style=\"border-color: black;\">\u2423when</span><span class=\"ltx_text ltx_framed_rectangle\" id=\"S4.T1.4.4.3.2.1.3.1.4\" style=\"color:#808080;border-color: #808080;\">\u2423it</span></span></span>\n</span></span> <span class=\"ltx_text\" id=\"S4.T1.4.4.3.3\"></span>\n</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S4.T1.5.2.1\" style=\"font-size:90%;\">Table 1</span>: </span><span class=\"ltx_text\" id=\"S4.T1.2.1\" style=\"font-size:90%;\">Clean and noise source prompts for singular prompt pair distributions. Vertical lines denote tokenization boundaries. All prompts are preceded by the BOS (beginning of sequence) token. The last token is not part of the input. The last token of the clean prompt is used as the target in .</span></figcaption>\n</figure>",
            "capture": "Table 1: Clean and noise source prompts for singular prompt pair distributions. Vertical lines denote tokenization boundaries. All prompts are preceded by the BOS (beginning of sequence) token. The last token is not part of the input. The last token of the clean prompt is used as the target in ."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"A3.T2\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"A3.T2.96\">\n<tr class=\"ltx_tr\" id=\"A3.T2.6.6\">\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"A3.T2.6.6.7\"><span class=\"ltx_text\" id=\"A3.T2.6.6.7.1\" style=\"font-size:90%;\">AtP variant</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.1.1.1\">\n<span class=\"ltx_text\" id=\"A3.T2.1.1.1.1\" style=\"font-size:90%;\">O</span><span class=\"ltx_text\" id=\"A3.T2.1.1.1.2\" style=\"font-size:90%;\">V</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.2.2.2\">\n<span class=\"ltx_text\" id=\"A3.T2.2.2.2.1\" style=\"font-size:90%;\">O</span><span class=\"ltx_text\" id=\"A3.T2.2.2.2.2\" style=\"font-size:90%;\">Q,K</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.3.3.3\">\n<span class=\"ltx_text\" id=\"A3.T2.3.3.3.1\" style=\"font-size:90%;\">O</span><span class=\"ltx_text\" id=\"A3.T2.3.3.3.2\" style=\"font-size:90%;\">MLP</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.4.4.4\">\n<span class=\"ltx_text\" id=\"A3.T2.4.4.4.1\" style=\"font-size:90%;\">MLP</span><span class=\"ltx_text\" id=\"A3.T2.4.4.4.2\" style=\"font-size:90%;\">V</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.5.5.5\">\n<span class=\"ltx_text\" id=\"A3.T2.5.5.5.1\" style=\"font-size:90%;\">MLP</span><span class=\"ltx_text\" id=\"A3.T2.5.5.5.2\" style=\"font-size:90%;\">Q,K</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T2.6.6.6\">\n<span class=\"ltx_text\" id=\"A3.T2.6.6.6.1\" style=\"font-size:90%;\">MLP</span><span class=\"ltx_text\" id=\"A3.T2.6.6.6.2\" style=\"font-size:90%;\">MLP</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T2.12.12\">\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"A3.T2.12.12.7\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T2.12.12.7.1\" style=\"font-size:90%;\">MLP layers</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"A3.T2.7.7.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"A3.T2.8.8.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"A3.T2.9.9.3\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"A3.T2.10.10.4\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"A3.T2.11.11.5\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A3.T2.12.12.6\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T2.18.18\">\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"A3.T2.18.18.7\"><span class=\"ltx_text\" id=\"A3.T2.18.18.7.1\" style=\"font-size:90%;\">QKfix</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.13.13.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.14.14.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.15.15.3\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.16.16.4\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.17.17.5\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T2.18.18.6\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T2.24.24\">\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"A3.T2.24.24.7\"><span class=\"ltx_text\" id=\"A3.T2.24.24.7.1\" style=\"font-size:90%;\">QKfix+GD</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.19.19.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.20.20.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.21.21.3\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.22.22.4\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.23.23.5\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T2.24.24.6\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T2.30.30\">\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"A3.T2.30.30.7\"><span class=\"ltx_text\" id=\"A3.T2.30.30.7.1\" style=\"font-size:90%;\">AtP*</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.25.25.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.26.26.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.27.27.3\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.28.28.4\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.29.29.5\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T2.30.30.6\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T2.36.36\">\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"A3.T2.36.36.7\"><span class=\"ltx_text\" id=\"A3.T2.36.36.7.1\" style=\"font-size:90%;\">AtP*+GD</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.31.31.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.32.32.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.33.33.3\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.34.34.4\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.35.35.5\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T2.36.36.6\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T2.42.42\">\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"A3.T2.42.42.7\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T2.42.42.7.1\" style=\"font-size:90%;\">QKfix (long)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A3.T2.37.37.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A3.T2.38.38.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A3.T2.39.39.3\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A3.T2.40.40.4\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A3.T2.41.41.5\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T2.42.42.6\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T2.48.48\">\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"A3.T2.48.48.7\"><span class=\"ltx_text\" id=\"A3.T2.48.48.7.1\" style=\"font-size:90%;\">QKfix+GD</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.43.43.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.44.44.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.45.45.3\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.46.46.4\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.47.47.5\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T2.48.48.6\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T2.54.54\">\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"A3.T2.54.54.7\"><span class=\"ltx_text\" id=\"A3.T2.54.54.7.1\" style=\"font-size:90%;\">ATP*</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.49.49.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.50.50.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.51.51.3\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.52.52.4\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.53.53.5\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T2.54.54.6\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T2.60.60\">\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"A3.T2.60.60.7\"><span class=\"ltx_text\" id=\"A3.T2.60.60.7.1\" style=\"font-size:90%;\">AtP*+GD</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.55.55.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.56.56.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.57.57.3\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.58.58.4\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.59.59.5\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T2.60.60.6\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T2.66.66\">\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"A3.T2.66.66.7\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T2.66.66.7.1\" style=\"font-size:90%;\">Neurons</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"A3.T2.61.61.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"A3.T2.62.62.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"A3.T2.63.63.3\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"A3.T2.64.64.4\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"A3.T2.65.65.5\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A3.T2.66.66.6\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T2.72.72\">\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"A3.T2.72.72.7\"><span class=\"ltx_text\" id=\"A3.T2.72.72.7.1\" style=\"font-size:90%;\">MLPfix</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.67.67.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.68.68.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.69.69.3\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.70.70.4\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.71.71.5\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T2.72.72.6\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T2.78.78\">\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"A3.T2.78.78.7\"><span class=\"ltx_text\" id=\"A3.T2.78.78.7.1\" style=\"font-size:90%;\">AtP*</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.73.73.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.74.74.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.75.75.3\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.76.76.4\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.77.77.5\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T2.78.78.6\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T2.84.84\">\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"A3.T2.84.84.7\"><span class=\"ltx_text\" id=\"A3.T2.84.84.7.1\" style=\"font-size:90%;\">AtP*+GD</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.79.79.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.80.80.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.81.81.3\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.82.82.4\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.83.83.5\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T2.84.84.6\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T2.90.90\">\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"A3.T2.90.90.7\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T2.90.90.7.1\" style=\"font-size:90%;\">ATP* (long)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A3.T2.85.85.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A3.T2.86.86.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A3.T2.87.87.3\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A3.T2.88.88.4\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A3.T2.89.89.5\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T2.90.90.6\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T2.96.96\">\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"A3.T2.96.96.7\"><span class=\"ltx_text\" id=\"A3.T2.96.96.7.1\" style=\"font-size:90%;\">AtP*+GD</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.91.91.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.92.92.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.93.93.3\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.94.94.4\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T2.95.95.5\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T2.96.96.6\"></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Per-token per-layer-pair total quadratic cost of each kind of between-layers edge, across edge-AtP variants. For brevity, we omit the layer-pair  factor that would otherwise be in every cell, and use .</figcaption>\n</figure>",
            "capture": "Table 2: Per-token per-layer-pair total quadratic cost of each kind of between-layers edge, across edge-AtP variants. For brevity, we omit the layer-pair  factor that would otherwise be in every cell, and use ."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.00745v1_figure_1.png",
            "caption": "(a) MLP neurons, on CITY-PP."
        },
        "2": {
            "figure_path": "2403.00745v1_figure_2.png",
            "caption": "(b) Attention nodes, on IOI-PP."
        },
        "3": {
            "figure_path": "2403.00745v1_figure_3.png",
            "caption": "(a) MLP neurons, on CITY-PP."
        },
        "4": {
            "figure_path": "2403.00745v1_figure_4.png",
            "caption": "(b) Attention nodes, on IOI-PP."
        },
        "5": {
            "figure_path": "2403.00745v1_figure_5.png",
            "caption": "(b) Attention nodes, on IOI-PP."
        },
        "6": {
            "figure_path": "2403.00745v1_figure_6.png",
            "caption": "Figure 3: A linear approximation to the attention probability is a particularly poor approximation in cases where one or both of the endpoints are in a saturated region of the softmax. Note that when varying only a single key, the softmax becomes a sigmoid of the dot product of that key and the query."
        },
        "7": {
            "figure_path": "2403.00745v1_figure_7.png",
            "caption": "Figure 4: Ranks of c\u2062(n)\ud835\udc50\ud835\udc5bc(n)italic_c ( italic_n ) against ranks of c^AtP\u2062(n)subscript^\ud835\udc50AtP\ud835\udc5b\\hat{c}_{\\text{AtP}}(n)over^ start_ARG italic_c end_ARG start_POSTSUBSCRIPT AtP end_POSTSUBSCRIPT ( italic_n ), on Pythia-12B on CITY-PP. Both improvements to AtP reduce the number of false negatives (bottom right triangle area), where in this case most improvements come from the QK fix. Coloration indicates the maximum absolute difference in attention probability when comparing xcleansuperscript\ud835\udc65cleanx^{\\text{clean}}italic_x start_POSTSUPERSCRIPT clean end_POSTSUPERSCRIPT and patching a given query or key. Many false negatives are keys and queries with significant maximum difference in attention probability, suggesting they are due to attention saturation as illustrated in Figure 3. Output and value nodes are colored in grey as they do not contribute to the attention probability."
        },
        "8": {
            "figure_path": "2403.00745v1_figure_8.png",
            "caption": "Figure 5: True rank and rank of AtP estimates with and without GradDrop, using Pythia-12B on the CITY-PP distribution with NeuronNodes. GradDrop provides a significant improvement to the largest neuron false negatives (red circles) relative to Default AtP (orange crosses)."
        },
        "9": {
            "figure_path": "2403.00745v1_figure_9.png",
            "caption": "(a) IOI-PP"
        },
        "10": {
            "figure_path": "2403.00745v1_figure_10.png",
            "caption": "(b) IOI"
        },
        "11": {
            "figure_path": "2403.00745v1_figure_11.png",
            "caption": "(a) NeuronNodes on CITY-PP"
        },
        "12": {
            "figure_path": "2403.00745v1_figure_12.png",
            "caption": "(b) AttentionNodes on IOI-PP"
        },
        "13": {
            "figure_path": "2403.00745v1_figure_13.png",
            "caption": "(a) RAND-PP MLP neurons."
        },
        "14": {
            "figure_path": "2403.00745v1_figure_14.png",
            "caption": "(b) RAND-PP Attention nodes."
        },
        "15": {
            "figure_path": "2403.00745v1_figure_15.png",
            "caption": "(c) A-AN MLP neurons."
        },
        "16": {
            "figure_path": "2403.00745v1_figure_16.png",
            "caption": "(d) IOI Attention nodes."
        },
        "17": {
            "figure_path": "2403.00745v1_figure_17.png",
            "caption": "(a) RAND-PP MLP neurons."
        },
        "18": {
            "figure_path": "2403.00745v1_figure_18.png",
            "caption": "(b) RAND-PP Attention nodes."
        },
        "19": {
            "figure_path": "2403.00745v1_figure_19.png",
            "caption": "(b) RAND-PP Attention nodes."
        },
        "20": {
            "figure_path": "2403.00745v1_figure_20.png",
            "caption": "(c) A-AN MLP neurons."
        },
        "21": {
            "figure_path": "2403.00745v1_figure_21.png",
            "caption": "(d) IOI Attention nodes."
        },
        "22": {
            "figure_path": "2403.00745v1_figure_22.png",
            "caption": "(a) Pythia-410M"
        },
        "23": {
            "figure_path": "2403.00745v1_figure_23.png",
            "caption": "(b) Pythia-1B"
        },
        "24": {
            "figure_path": "2403.00745v1_figure_24.png",
            "caption": "(c) Pythia-2.8B"
        },
        "25": {
            "figure_path": "2403.00745v1_figure_25.png",
            "caption": "(d) Pythia-12B"
        },
        "26": {
            "figure_path": "2403.00745v1_figure_26.png",
            "caption": "a.i IOI-PP"
        },
        "27": {
            "figure_path": "2403.00745v1_figure_27.png",
            "caption": "a.ii RAND-PP"
        },
        "28": {
            "figure_path": "2403.00745v1_figure_28.png",
            "caption": "a.iii IOI"
        },
        "29": {
            "figure_path": "2403.00745v1_figure_29.png",
            "caption": "b.i CITY-PP"
        },
        "30": {
            "figure_path": "2403.00745v1_figure_30.png",
            "caption": "b.ii RAND-PP"
        },
        "31": {
            "figure_path": "2403.00745v1_figure_31.png",
            "caption": "b.iii A-AN"
        },
        "32": {
            "figure_path": "2403.00745v1_figure_32.png",
            "caption": "a.i Pythia 410M"
        },
        "33": {
            "figure_path": "2403.00745v1_figure_33.png",
            "caption": "a.ii Pythia 1B"
        },
        "34": {
            "figure_path": "2403.00745v1_figure_34.png",
            "caption": "a.iii Pythia 2.8B"
        },
        "35": {
            "figure_path": "2403.00745v1_figure_35.png",
            "caption": "a.iv Pythia 12B"
        },
        "36": {
            "figure_path": "2403.00745v1_figure_36.png",
            "caption": "b.i Pythia 410M"
        },
        "37": {
            "figure_path": "2403.00745v1_figure_37.png",
            "caption": "b.ii Pythia 1B"
        },
        "38": {
            "figure_path": "2403.00745v1_figure_38.png",
            "caption": "b.iii Pythia 2.8B"
        },
        "39": {
            "figure_path": "2403.00745v1_figure_39.png",
            "caption": "b.iv Pythia 12B"
        },
        "40": {
            "figure_path": "2403.00745v1_figure_40.png",
            "caption": "c.i Pythia 410M"
        },
        "41": {
            "figure_path": "2403.00745v1_figure_41.png",
            "caption": "c.ii Pythia 1B"
        },
        "42": {
            "figure_path": "2403.00745v1_figure_42.png",
            "caption": "c.iii Pythia 2.8B"
        },
        "43": {
            "figure_path": "2403.00745v1_figure_43.png",
            "caption": "c.iv Pythia 12B"
        },
        "44": {
            "figure_path": "2403.00745v1_figure_44.png",
            "caption": "a.i Pythia 410M"
        },
        "45": {
            "figure_path": "2403.00745v1_figure_45.png",
            "caption": "a.ii Pythia 1B"
        },
        "46": {
            "figure_path": "2403.00745v1_figure_46.png",
            "caption": "a.iii Pythia 2.8B"
        },
        "47": {
            "figure_path": "2403.00745v1_figure_47.png",
            "caption": "a.iv Pythia 12B"
        },
        "48": {
            "figure_path": "2403.00745v1_figure_48.png",
            "caption": "b.i Pythia 410M"
        },
        "49": {
            "figure_path": "2403.00745v1_figure_49.png",
            "caption": "b.ii Pythia 1B"
        },
        "50": {
            "figure_path": "2403.00745v1_figure_50.png",
            "caption": "b.iii Pythia 2.8B"
        },
        "51": {
            "figure_path": "2403.00745v1_figure_51.png",
            "caption": "b.iv Pythia 12B"
        },
        "52": {
            "figure_path": "2403.00745v1_figure_52.png",
            "caption": "c.i Pythia 410M"
        },
        "53": {
            "figure_path": "2403.00745v1_figure_53.png",
            "caption": "c.ii Pythia 1B"
        },
        "54": {
            "figure_path": "2403.00745v1_figure_54.png",
            "caption": "c.iii Pythia 2.8B"
        },
        "55": {
            "figure_path": "2403.00745v1_figure_55.png",
            "caption": "c.iv Pythia 12B"
        },
        "56": {
            "figure_path": "2403.00745v1_figure_56.png",
            "caption": "Figure 14: True ranks against AtP*{}^{*}start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT ranks on Pythia-12B using various metrics \u2112\u2112\\mathcal{L}caligraphic_L. The last row shows the effect in the denoising (rather than noising) setting; we speculate that the lower-right subplot (log-odds denoising) is similar to the lower-middle one (logit-diff denoising) because IOI produces a bimodal distribution over the correct and alternate next token."
        },
        "57": {
            "figure_path": "2403.00745v1_figure_57.png",
            "caption": "(a) \u03f5AtPsubscriptitalic-\u03f5AtP\\epsilon_{\\text{AtP}}italic_\u03f5 start_POSTSUBSCRIPT AtP end_POSTSUBSCRIPT"
        },
        "58": {
            "figure_path": "2403.00745v1_figure_58.png",
            "caption": "(b) \u03f5AtP+frozenLNsubscriptitalic-\u03f5AtP+frozenLN\\epsilon_{\\text{AtP+frozenLN}}italic_\u03f5 start_POSTSUBSCRIPT AtP+frozenLN end_POSTSUBSCRIPT"
        },
        "59": {
            "figure_path": "2403.00745v1_figure_59.png",
            "caption": "(c) \u03f5AtP+frozenLN\u2212\u03f5AtPsubscriptitalic-\u03f5AtP+frozenLNsubscriptitalic-\u03f5AtP\\epsilon_{\\text{AtP+frozenLN}}-\\epsilon_{\\text{AtP}}italic_\u03f5 start_POSTSUBSCRIPT AtP+frozenLN end_POSTSUBSCRIPT - italic_\u03f5 start_POSTSUBSCRIPT AtP end_POSTSUBSCRIPT"
        },
        "60": {
            "figure_path": "2403.00745v1_figure_60.png",
            "caption": "Figure 16: A comparison of edge-AtP variants across model sizes and prompt lengths. AtP* here is defined to include QKfix and MLPfix, but not GradDrops. The costs vary across several orders of magnitude for each setting.\nIn the setting with full-MLP nodes, MLPfix carries substantial cost for short prompts, but barely matters for long prompts.\nIn the neuron-nodes setting, MLPfix is costless. But GradDrops in that setting continues to impose a large cost; even though it doesn\u2019t affect MLP\u2192\u2192\\rightarrow\u2192MLP edges, it does affect MLP\u2192\u2192\\rightarrow\u2192Q,K edges, which come out dominating the cost with QKfix."
        },
        "61": {
            "figure_path": "2403.00745v1_figure_61.png",
            "caption": "a.i"
        },
        "62": {
            "figure_path": "2403.00745v1_figure_62.png",
            "caption": "a.ii"
        },
        "63": {
            "figure_path": "2403.00745v1_figure_63.png",
            "caption": "b.i"
        },
        "64": {
            "figure_path": "2403.00745v1_figure_64.png",
            "caption": "b.ii"
        },
        "65": {
            "figure_path": "2403.00745v1_figure_65.png",
            "caption": "c.i"
        },
        "66": {
            "figure_path": "2403.00745v1_figure_66.png",
            "caption": "c.ii"
        },
        "67": {
            "figure_path": "2403.00745v1_figure_67.png",
            "caption": "d.i"
        },
        "68": {
            "figure_path": "2403.00745v1_figure_68.png",
            "caption": "d.ii"
        }
    },
    "references": [
        {
            "1": {
                "title": "Leace: Perfect linear concept erasure in closed form.",
                "author": "N. Belrose, D. Schneider-Joseph, S. Ravfogel, R. Cotterell, E. Raff, and\nS. Biderman.",
                "venue": "arXiv preprint arXiv:2306.03819, 2023.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Pythia: A suite for analyzing large language models across training\nand scaling.",
                "author": "S. Biderman, H. Schoelkopf, Q. G. Anthony, H. Bradley, K. O\u2019Brien, E. Hallahan,\nM. A. Khan, S. Purohit, U. S. Prashanth, E. Raff, A. Skowron, L. Sutawika,\nand O. van der Wal.",
                "venue": "In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and\nJ. Scarlett, editors, International Conference on Machine Learning,\nICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of\nProceedings of Machine Learning Research, pages 2397\u20132430. PMLR,\n2023.",
                "url": null
            }
        },
        {
            "3": {
                "title": "Towards monosemanticity: Decomposing language models with dictionary\nlearning.",
                "author": "T. Bricken, A. Templeton, J. Batson, B. Chen, A. Jermyn, T. Conerly, N. Turner,\nC. Anil, C. Denison, A. Askell, R. Lasenby, Y. Wu, S. Kravec, N. Schiefer,\nT. Maxwell, N. Joseph, Z. Hatfield-Dodds, A. Tamkin, K. Nguyen, B. McLean,\nJ. E. Burke, T. Hume, S. Carter, T. Henighan, and C. Olah.",
                "venue": "Transformer Circuits Thread, 2023.",
                "url": null
            }
        },
        {
            "4": {
                "title": "Thread: Circuits.",
                "author": "N. Cammarata, S. Carter, G. Goh, C. Olah, M. Petrov, L. Schubert, C. Voss,\nB. Egan, and S. K. Lim.",
                "venue": "Distill, 2020.",
                "url": null
            }
        },
        {
            "5": {
                "title": "Sparse interventions in language models with differentiable masking,\n2021.",
                "author": "N. D. Cao, L. Schmid, D. Hupkes, and I. Titov.",
                "venue": null,
                "url": null
            }
        },
        {
            "6": {
                "title": "Causal scrubbing, a method for rigorously testing interpretability\nhypotheses.",
                "author": "L. Chan, A. Garriga-Alonso, N. Goldwosky-Dill, R. Greenblatt, J. Nitishinskaya,\nA. Radhakrishnan, B. Shlegeris, and N. Thomas.",
                "venue": "AI Alignment Forum, 2022.",
                "url": null
            }
        },
        {
            "7": {
                "title": "Towards automated circuit discovery for mechanistic interpretability,\n2023.",
                "author": "A. Conmy, A. N. Mavor-Parker, A. Lynch, S. Heimersheim, and A. Garriga-Alonso.",
                "venue": null,
                "url": null
            }
        },
        {
            "8": {
                "title": "Sparse autoencoders find highly interpretable features in language\nmodels, 2023.",
                "author": "H. Cunningham, A. Ewart, L. Riggs, R. Huben, and L. Sharkey.",
                "venue": null,
                "url": null
            }
        },
        {
            "9": {
                "title": "How do language models bind entities in context?, 2023.",
                "author": "J. Feng and J. Steinhardt.",
                "venue": null,
                "url": null
            }
        },
        {
            "10": {
                "title": "Perforatedcnns: Acceleration through elimination of redundant\nconvolutions.",
                "author": "M. Figurnov, A. Ibraimova, D. P. Vetrov, and P. Kohli.",
                "venue": "In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett,\neditors, Advances in Neural Information Processing Systems, volume 29.\nCurran Associates, Inc., 2016.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Causal analysis of syntactic agreement mechanisms in neural language\nmodels.",
                "author": "M. Finlayson, A. Mueller, S. Gehrmann, S. Shieber, T. Linzen, and Y. Belinkov.",
                "venue": "In C. Zong, F. Xia, W. Li, and R. Navigli, editors, Proceedings\nof the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 1828\u20131843, Online, Aug. 2021. Association\nfor Computational Linguistics.",
                "url": null
            }
        },
        {
            "12": {
                "title": "The Pile: An 800gb dataset of diverse text for language modeling.",
                "author": "L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang,\nH. He, A. Thite, N. Nabeshima, S. Presser, and C. Leahy.",
                "venue": "arXiv preprint arXiv:2101.00027, 2020.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Neural natural language inference models partially embed theories of\nlexical entailment and negation, 2020.",
                "author": "A. Geiger, K. Richardson, and C. Potts.",
                "venue": null,
                "url": null
            }
        },
        {
            "14": {
                "title": "Causal abstractions of neural networks, 2021.",
                "author": "A. Geiger, H. Lu, T. Icard, and C. Potts.",
                "venue": null,
                "url": null
            }
        },
        {
            "15": {
                "title": "Inducing causal structure for interpretable neural networks, 2022.",
                "author": "A. Geiger, Z. Wu, H. Lu, J. Rozner, E. Kreiss, T. Icard, N. D. Goodman, and\nC. Potts.",
                "venue": null,
                "url": null
            }
        },
        {
            "16": {
                "title": "Causal abstraction for faithful model interpretation, 2023.",
                "author": "A. Geiger, C. Potts, and T. Icard.",
                "venue": null,
                "url": null
            }
        },
        {
            "17": {
                "title": "Dissecting recall of factual associations in auto-regressive language\nmodels, 2023.",
                "author": "M. Geva, J. Bastings, K. Filippova, and A. Globerson.",
                "venue": null,
                "url": null
            }
        },
        {
            "18": {
                "title": "Localizing model behavior with path patching, 2023.",
                "author": "N. Goldowsky-Dill, C. MacLeod, L. Sato, and A. Arora.",
                "venue": null,
                "url": null
            }
        },
        {
            "19": {
                "title": "Finding neurons in a haystack: Case studies with sparse probing,\n2023.",
                "author": "W. Gurnee, N. Nanda, M. Pauly, K. Harvey, D. Troitskii, and D. Bertsimas.",
                "venue": null,
                "url": null
            }
        },
        {
            "20": {
                "title": "How does gpt-2 compute greater-than?: Interpreting mathematical\nabilities in a pre-trained language model, 2023.",
                "author": "M. Hanna, O. Liu, and A. Variengien.",
                "venue": null,
                "url": null
            }
        },
        {
            "21": {
                "title": "Does localization inform editing? surprising differences in\ncausality-based localization vs. knowledge editing in language models, 2023.",
                "author": "P. Hase, M. Bansal, B. Kim, and A. Ghandeharioun.",
                "venue": null,
                "url": null
            }
        },
        {
            "22": {
                "title": "In-context learning creates task vectors, 2023.",
                "author": "R. Hendel, M. Geva, and A. Globerson.",
                "venue": null,
                "url": null
            }
        },
        {
            "23": {
                "title": "An empirical analysis of compute-optimal large language model\ntraining.",
                "author": "J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford,\nD. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland,\nK. Millican, G. van den Driessche, B. Damoc, A. Guy, S. Osindero,\nK. Simonyan, E. Elsen, O. Vinyals, J. Rae, and L. Sifre.",
                "venue": "In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh,\neditors, Advances in Neural Information Processing Systems, volume 35,\npages 30016\u201330030. Curran Associates, Inc., 2022.",
                "url": null
            }
        },
        {
            "24": {
                "title": "Rigorously assessing natural language explanations of neurons, 2023.",
                "author": "J. Huang, A. Geiger, K. D\u2019Oosterlinck, Z. Wu, and C. Potts.",
                "venue": null,
                "url": null
            }
        },
        {
            "25": {
                "title": "A comparison of causal scrubbing, causal abstractions, and related\nmethods.",
                "author": "E. Jenner, A. Garriga-Alonso, and E. Zverev.",
                "venue": "AI Alignment Forum, 2022.",
                "url": null
            }
        },
        {
            "26": {
                "title": "Improving activation steering in language models with mean-centring,\n2023.",
                "author": "O. Jorgensen, D. Cope, N. Schoots, and M. Shanahan.",
                "venue": null,
                "url": null
            }
        },
        {
            "27": {
                "title": "Inference-time intervention: Eliciting truthful answers from a\nlanguage model, 2023.",
                "author": "K. Li, O. Patel, F. Vi\u00e9gas, H. Pfister, and M. Wattenberg.",
                "venue": null,
                "url": null
            }
        },
        {
            "28": {
                "title": "Does circuit analysis interpretability scale? evidence from multiple\nchoice capabilities in chinchilla, 2023.",
                "author": "T. Lieberum, M. Rahtz, J. Kram\u00e1r, N. Nanda, G. Irving, R. Shah, and\nV. Mikulik.",
                "venue": null,
                "url": null
            }
        },
        {
            "29": {
                "title": "Learning sparse neural networks through  regularization, 2018.",
                "author": "C. Louizos, M. Welling, and D. P. Kingma.",
                "venue": null,
                "url": null
            }
        },
        {
            "30": {
                "title": "Copy suppression: Comprehensively understanding an attention head,\n2023.",
                "author": "C. McDougall, A. Conmy, C. Rushing, T. McGrath, and N. Nanda.",
                "venue": null,
                "url": null
            }
        },
        {
            "31": {
                "title": "The hydra effect: Emergent self-repair in language model\ncomputations, 2023.",
                "author": "T. McGrath, M. Rahtz, J. Kram\u00e1r, V. Mikulik, and S. Legg.",
                "venue": null,
                "url": null
            }
        },
        {
            "32": {
                "title": "Locating and editing factual associations in gpt, 2023.",
                "author": "K. Meng, D. Bau, A. Andonian, and Y. Belinkov.",
                "venue": null,
                "url": null
            }
        },
        {
            "33": {
                "title": "Circuit component reuse across tasks in transformer language models,\n2023.",
                "author": "J. Merullo, C. Eickhoff, and E. Pavlick.",
                "venue": null,
                "url": null
            }
        },
        {
            "34": {
                "title": "Are sixteen heads really better than one?",
                "author": "P. Michel, O. Levy, and G. Neubig.",
                "venue": "In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural\nInformation Processing Systems, volume 32. Curran Associates, Inc., 2019.",
                "url": null
            }
        },
        {
            "35": {
                "title": "Pruning convolutional neural networks for resource efficient\ninference.",
                "author": "P. Molchanov, S. Tyree, T. Karras, T. Aila, and J. Kautz.",
                "venue": "In International Conference on Learning Representations, 2017.",
                "url": null
            }
        },
        {
            "36": {
                "title": "Attribution patching: Activation patching at industrial scale.",
                "author": "N. Nanda.",
                "venue": "2022.",
                "url": null
            }
        },
        {
            "37": {
                "title": "Fact finding: Attempting to reverse-engineer factual recall on the\nneuron level, Dec 2023.",
                "author": "N. Nanda, S. Rajamanoharan, J. Kram\u00e1r, and R. Shah.",
                "venue": "URL\nhttps://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall.",
                "url": null
            }
        },
        {
            "38": {
                "title": "interpreting gpt: the logit lens.",
                "author": "nostalgebraist.",
                "venue": "2020.",
                "url": null
            }
        },
        {
            "39": {
                "title": "In-context learning and induction heads.",
                "author": "C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann,\nA. Askell, Y. Bai, A. Chen, T. Conerly, D. Drain, D. Ganguli,\nZ. Hatfield-Dodds, D. Hernandez, S. Johnston, A. Jones, J. Kernion,\nL. Lovitt, K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan,\nS. McCandlish, and C. Olah.",
                "venue": "Transformer Circuits Thread, 2022.",
                "url": null
            }
        },
        {
            "40": {
                "title": "Causality: Models, Reasoning and Inference.",
                "author": "J. Pearl.",
                "venue": "Cambridge University Press, 2000.",
                "url": null
            }
        },
        {
            "41": {
                "title": "Direct and indirect effects, 2001.",
                "author": "J. Pearl.",
                "venue": null,
                "url": null
            }
        },
        {
            "42": {
                "title": "Improving language understanding by generative pre-training, 2018.",
                "author": "A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever.",
                "venue": null,
                "url": null
            }
        },
        {
            "43": {
                "title": "Steering llama 2 via contrastive activation addition, 2023.",
                "author": "N. Rimsky, N. Gabrieli, J. Schulz, M. Tong, E. Hubinger, and A. M. Turner.",
                "venue": null,
                "url": null
            }
        },
        {
            "44": {
                "title": "Identifiability and exchangeability for direct and indirect effects.",
                "author": "J. M. Robins and S. Greenland.",
                "venue": "Epidemiology, 3:143\u2013155, 1992.",
                "url": null
            }
        },
        {
            "45": {
                "title": "Discovering the compositional structure of vector representations\nwith role learning networks.",
                "author": "P. Soulos, R. T. McCoy, T. Linzen, and P. Smolensky.",
                "venue": "In A. Alishahi, Y. Belinkov, G. Chrupa\u0142a, D. Hupkes, Y. Pinter,\nand H. Sajjad, editors, Proceedings of the Third BlackboxNLP Workshop\non Analyzing and Interpreting Neural Networks for NLP, pages 238\u2013254,\nOnline, Nov. 2020. Association for Computational Linguistics.",
                "url": null
            }
        },
        {
            "46": {
                "title": "A mechanistic interpretation of arithmetic reasoning in language\nmodels using causal mediation analysis, 2023.",
                "author": "A. Stolfo, Y. Belinkov, and M. Sachan.",
                "venue": null,
                "url": null
            }
        },
        {
            "47": {
                "title": "Attribution patching outperforms automated circuit discovery, 2023.",
                "author": "A. Syed, C. Rager, and A. Conmy.",
                "venue": null,
                "url": null
            }
        },
        {
            "48": {
                "title": "Linear representations of sentiment in large language models, 2023.",
                "author": "C. Tigges, O. J. Hollinsworth, A. Geiger, and N. Nanda.",
                "venue": null,
                "url": null
            }
        },
        {
            "49": {
                "title": "Function vectors in large language models, 2023.",
                "author": "E. Todd, M. L. Li, A. S. Sharma, A. Mueller, B. C. Wallace, and D. Bau.",
                "venue": null,
                "url": null
            }
        },
        {
            "50": {
                "title": "Activation addition: Steering language models without optimization,\n2023.",
                "author": "A. M. Turner, L. Thiergart, D. Udell, G. Leech, U. Mini, and M. MacDiarmid.",
                "venue": null,
                "url": null
            }
        },
        {
            "51": {
                "title": "Attention is all you need, 2017.",
                "author": "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin.",
                "venue": null,
                "url": null
            }
        },
        {
            "52": {
                "title": "Residual networks behave like ensembles of relatively shallow\nnetworks.",
                "author": "A. Veit, M. J. Wilber, and S. Belongie.",
                "venue": "In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett,\neditors, Advances in Neural Information Processing Systems, volume 29.\nCurran Associates, Inc., 2016.",
                "url": null
            }
        },
        {
            "53": {
                "title": "Investigating gender bias in language models using causal mediation\nanalysis.",
                "author": "J. Vig, S. Gehrmann, Y. Belinkov, S. Qian, D. Nevo, Y. Singer, and S. Shieber.",
                "venue": "In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin,\neditors, Advances in Neural Information Processing Systems, volume 33,\npages 12388\u201312401. Curran Associates, Inc., 2020.",
                "url": null
            }
        },
        {
            "54": {
                "title": "Interpretability in the wild: a circuit for indirect object\nidentification in gpt-2 small, 2022.",
                "author": "K. Wang, A. Variengien, A. Conmy, B. Shlegeris, and J. Steinhardt.",
                "venue": null,
                "url": null
            }
        },
        {
            "55": {
                "title": "The generalization of \u2018Student\u2019s\u2019 problem when several different\npopulation variances are involved.",
                "author": "B. L. Welch.",
                "venue": "Biometrika, 34(1-2):28\u201335, 01 1947.",
                "url": null
            }
        },
        {
            "56": {
                "title": "Representation engineering: A top-down approach to ai transparency,\n2023.",
                "author": "A. Zou, L. Phan, S. Chen, J. Campbell, P. Guo, R. Ren, A. Pan, X. Yin,\nM. Mazeika, A.-K. Dombrowski, S. Goel, N. Li, M. J. Byun, Z. Wang, A. Mallen,\nS. Basart, S. Koyejo, D. Song, M. Fredrikson, J. Z. Kolter, and D. Hendrycks.",
                "venue": null,
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.00745v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3.1",
            "3.1.1",
            "3.1.2",
            "3.2",
            "3.3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "1",
            "2.1",
            "3",
            "3.1",
            "3.1.1",
            "3.1.2",
            "4",
            "4.3"
        ]
    },
    "research_context": {
        "paper_id": "2403.00745v1",
        "paper_title": "AtP^*: An efficient and scalable method for localizing LLM behaviour to components",
        "research_background": "The paper \"AtP^*: An efficient and scalable method for localizing LLM behaviour to components\" is motivated by the need to understand the inner workings of large language models (LLMs), which are increasingly becoming integral to various digital applications. This understanding, referred to as mechanistic interpretability, is crucial for causally attributing specific behaviors to individual components within the model\u2019s transformer forward pass, such as attention heads, neurons, layer contributions, or residual streams. The paper addresses the challenge that while causal attribution methods exist, they can be computationally expensive, especially for fine-grained components like heads or neurons.\n\n### Research Problem:\nThe main research problem tackled by this paper is the inefficiency and scalability issues associated with causal attribution of behaviors in large language models. Specifically, conventional methods like Activation Patching, which require sweeping across all model components with separate forward passes for each intervention, can result in an impractically high computational cost. This is exacerbated when working with state-of-the-art (SoTA) models containing billions of components.\n\n### Motivation:\n1. **Complex Behavior Analysis**: Case studies reveal that complex behaviors are often driven by sparse subgraphs within the model.\n2. **Computational Expense**: Traditional causal attribution methods are computationally intensive when applied to SoTA models.\n3. **Efficiency Needs**: Accelerating and scaling these methods is essential for practical applicability.\n\n### Prior Work:\n1. **Zero-Ablation and Knock-Out**: Early causal attribution techniques involved deleting components and observing negative changes in model output.\n2. **Generalized Methods**: More recent methods replace a component\u2019s activations with samples from some baseline distribution.\n3. **Activation Patching/Causal Mediation Analysis**: This method uses corrupted input to assess the causal contribution of components (Geiger et al., 2022; Meng et al., 2023; Chan et al., 2022).\n4. **Efficiency in Attribution**: Attribution Patching (AtP) is introduced as a quicker, approximate method but has known failure modes (Nanda, 2022).\n\n### Contributions of the Paper:\n1. **Accelerated Framework**: Proposes the use of AtP as a prefiltering step to quickly estimate the importance of components, followed by a more reliable verification using the Activation Patching method.\n2. **AtP Improvements**: Develops AtP with key modifications to address failure modes:\n   - Recomputing the attention softmax when patching queries and keys.\n   - Using dropout on the backwards pass to mitigate false negatives.\n3. **Comparison and Benchmarking**: Introduces and benchmarks several alternative methods to approximate Activation Patching.\n4. **Diagnostic Methods**: Provides methods to estimate residual error and statistically bound the sizes of any remaining false negatives.\n5. **Practical Guidance**: Offers practical advice on performing causal attribution and selecting appropriate methods for different scenarios.\n\nBy addressing inefficiencies and proposing scalable solutions, this paper significantly advances the practice of mechanistic interpretability for large language models.",
        "methodology": "### Methodology for AtP^*: An Efficient and Scalable Method for Localizing Large Language Model (LLM) Behavior to Components\n\n#### Overview\nThe methodology section describes AtP^*, an approach designed to enhance the precision and scalability of localizing behavior in large language models (LLMs) by addressing common failure modes of the AtP (Attention-by-Perturbation) method. The primary aim is to mitigate the inaccuracies caused by the inherent non-linearity in activation functions and the interaction between direct and indirect effects within the model.\n\n#### Failure Modes\n1. **Non-Linear Activation Function Issues**:\n   - When preactivation occurs in a flat region of the activation function, such as when producing saturated attention weights, a linear approximation fails to capture the true behavior as needed. This is due to the AtP's reliance on local gradients to approximate non-linear functions accurately.\n   - The authors illustrate this using a theoretical equation (Equation 4) and the ground truth equation (Equation 1).\n\n2. **Cancellation Between Direct and Indirect Effects**:\n   - If the cumulative effect on a prompt pair, due to direct and indirect contributions, nearly cancels out, the linear approximations of non-linear functions can exaggerate errors, resulting in significant discrepancies between predicted and actual values.\n   - The method is inspired by framework established by Pearl (2001).\n\n#### Proposed Adjustments to Address Failures\n- **Adjusting for Queries**:\n   - Compute the adjusted effect by running the model on both clean and noised query scenarios.\n   - Cache noise queries, attention keys, and weights by combining keys from the clean pass and queries from the noised pass to compute attention weights.\n   - Resulting weight vector denoted as \\(\\tilde{w}_i\\) contrasts clean forward pass weights \\(w_i\\).\n   - This step is computationally efficient, costing no more than the unperturbed attention computation.\n\n- **Adjusting for Keys**:\n   - Perform similar adjustments by caching noised keys and clean queries and attention probabilities.\n   - Define key nodes for attention heads and compute the improved attribution estimates.\n   - The direct method is computationally expensive, requiring \\(O(H^2)\\) FLOPS, which is later optimized to require only as much compute as the forward pass itself.\n\n#### Optimization and Computational Efficiency\n- A more efficient variant of the method is proposed in Section A.2.1, reducing the computation requirement to O(H) FLOPS.\n- The enhancements ensure that the full query-key fix costs less than two forward transformer passes, excluding MLP computations.\n\n#### Conclusion\nAtP^* identifies and mitigates key sources of inaccuracies in the AtP method through efficient computation methods for query and key adjustments, thus substantially improving false negative rates in LLM behavior localization.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Experiment Setup\n\n**Datasets and Tasks:**\n- **Prompt Pairs:** Two primary prompt pairs are evaluated:\n  - **IOI-PP (Indirect Object Identification Prompt Pair):** An instance from the IOI task.\n  - **CITY-PP (City Name Prompt Pair):** Designed to elicit factual recall, focusing on early MLPs and a few late attention heads.\n  - **RAND-PP (Random Prompt Pair):** A randomly selected prompt pair from The Pile dataset.\n- **Distributional Settings:**\n  - A reduced version of IOI task with 6 names.\n  - A task prompting the model to output an indefinite article (\u2018a\u2019 or \u2018an\u2019).\n\n**Model Sizes:**\n- Models from the Pythia suite, ranging from 410M to 12B parameters.\n\n**Nodes Settings:**\n- **NeuronNodes:** Each MLP neuron (post-activation) is treated as a node.\n- **AttentionNodes:** Each query, key, and value vector for each head, along with pre-linear per-head attention output, treated as distinct nodes.\n\n**Evaluation Metric:**\n- **Primary Loss Function:** Negative log probability relative to targets from the clean prompt.\n- **Cost Metric:** Computation cost in terms of the number of forward passes needed to verify the nodes with the largest true effect magnitude.\n- **IRWRGM (Inverse-Rank-Weighted Relative Geometric Mean) Cost:** Weighted geometric mean cost normalized by oracle verification cost.\n\n**Baselines and Methods:**\n- **AtP and AtP***: Variants of the Attribution by Patching (AtP) method.\n- **Subsampling:** Averaging contributions across distributions.\n\n#### Main Experimental Results\n\n**Cost of Verified Recall:**\n  - Focuses on the 100% recall cost for different methods applied to NeuronNodes on CITY-PP and AttentionNodes on IOI-PP.\n  - AtP and its variants show efficient performance in identifying important nodes.\n  - The aggregated results across different models show consistent performance for AtP, with AtP* performing particularly well despite higher upfront costs.\n\nOverall, the results demonstrate that AtP and its refined versions (like AtP*) offer scalable and effective solutions for attributing behavior in various settings and model sizes, with verified recall costs indicating high efficiency across different tasks and prompt distributions."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To improve the Attribution Patching method (AtP) by addressing two classes of failure modes resulting in significant false negatives and propose an enhanced method called AtP*.",
            "experiment_process": "The experiment implemented two modifications to AtP for addressing failure modes: (1) Recomputing attention softmax using noisy keys and clean queries to correct the gradient-based approximation for saturated attention. (2) Using dropout in the backward pass (GradDrop) to address cancellation between direct and indirect effects. Different AtP variants were compared, including described alternatives in Section 3.3, with QK fixes applied on attention nodes and evaluated for their impact in Figure 4. The experiments examined compute costs and accuracy on single and distributed prompt pairs, with results shown in Figures 1, 7, and 9.",
            "result_discussion": "Results demonstrated that AtP* significantly outperforms AtP and other approximation methods by reducing false negatives. Applying QK fixes substantially lowers the false negative rates among keys and queries. GradDrop also proves effective in mitigating direct and indirect effect cancellation. Verification costs are reduced while maintaining high recall of important nodes, showing the robustness and scalability of AtP* across different model sizes and experimental conditions.",
            "ablation_id": "2403.00745v1.No1"
        },
        {
            "research_objective": "To assess the performance of Attribution Patching methods on single prompt pairs versus distributions.",
            "experiment_process": "The experiment involved two main settings: single prompt pairs (e.g., CITY-PP and IOI-PP) and distributions (e.g., a reduced version of IOI tasks and indefinite article outputs). The models tested were from the Pythia suite, ranging from 410M to 12B parameters. The negative log probability was used as the loss function. Performance was measured by cost-of-verified-recall plots to determine the computational cost and accuracy of detecting high-effect nodes.",
            "result_discussion": "AtP and AtP* showed strong performance in both single-prompt-pair and distributed settings, although some decline in effectiveness was noted for less controlled environments. Findings illustrated that the methods scale well for broader distributional evaluations, retaining efficiency and accuracy. Costs were manageable, indicating feasibility and suggesting practical application potential for varied contexts.",
            "ablation_id": "2403.00745v1.No2"
        },
        {
            "research_objective": "To compare the diagnostic capabilities of AtP* and alternative methods for estimating and bounding the probability of false negatives.",
            "experiment_process": "The combination of AtP* with Subsampling was used to diagnose and bound the probability of false negatives efficiently. Models from the Pythia suite were tested using various metrics and comparison plots with detailed procedures outlined in Section 3.3, such as computing QK fixes and integrating the subsampling estimator for large distributions. The false negative estimation methodology involved patching random subsets of high-impact nodes and verifying results.",
            "result_discussion": "The diagnostic study confirmed that AtP* integrated with Subsampling effectively filters out high-impact nodes while substantially reducing the risk of false negatives. Diagnostic methods provided reliable confidence in the output across different domains without incurring substantial computational overhead. This integration ensures robustness and practical utility in diverse model evaluation scenarios.",
            "ablation_id": "2403.00745v1.No3"
        }
    ]
}