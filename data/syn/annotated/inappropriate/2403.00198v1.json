{
    "title": "Axolotl: Fairness through Assisted Self-Debiasing of Large Language Model Outputs",
    "abstract": "Pre-trained Large Language Models (LLMs) have significantly advanced natural language processing capabilities but are susceptible to biases present in their training data, leading to unfair outcomes in various applications. While numerous strategies have been proposed to mitigate bias, they often require extensive computational resources and may compromise model performance. In this work, we introduce Axolotl, a novel post-processing framework, which operates agnostically across tasks and models, leveraging public APIs to interact with LLMs without direct access to internal parameters. Through a three-step process resembling zero-shot learning, Axolotl identifies biases, proposes resolutions, and guides the model to self-debias its outputs. This approach minimizes computational costs and preserves model performance, making Axolotl a promising tool for debiasing LLM outputs with broad applicability and ease of use.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Pre-trained Large Language Models (LLMs) have revolutionized natural language processing, offering unparalleled capabilities in understanding, generating, and translating text Zhu et al. (2023  ###reference_b34###); Zhang et al. (2020  ###reference_b30###). Despite their advancements, these models are not immune to inheriting and perpetuating biases present in their training data Maudslay et al. (2019a  ###reference_b17###). Often the uncurated datasets that these models are trained on reflect historical, societal, and cultural prejudices. Biases in LLMs can manifest in various forms such as gender, race, religion, profession, etc stereotypes, leading to unfair or discriminatory outcomes in applications ranging from automated hiring systems to conversational AI Zhang et al. (2020  ###reference_b30###). Studies such as Bolukbasi et al. (2016b  ###reference_b6###) and Bender et al. (2021  ###reference_b4###) highlight the critical nature of this problem, demonstrating how biases can skew LLM outputs in ways that reinforce harmful stereotypes and marginalize already disadvantaged groups.\nResearchers have explored a multitude of strategies to identify and mitigate bias. These efforts encompass a broad spectrum of approaches, including enhancing fairness through modifications in sentence and word representations and embeddings May et al. (2019  ###reference_b19###); Caliskan et al. (2017b  ###reference_b9###); Ravfogel et al. (2020  ###reference_b25###), adjusting the underlying distribution of tokens Guo et al. (2022  ###reference_b14###), and refining datasets alongside model pre-training Garimella et al. (2021  ###reference_b13###); Maudslay et al. (2019a  ###reference_b17###, b  ###reference_b18###). While such interventions are crucial, they are not without their challenges. Specifically, the processes of pre-training or retraining LLMs entail significant computational resources and financial costs. Moreover, certain debiasing techniques may compromise the LLMs\u2019 overall performance. Another notable issue is the reliance on access to the models\u2019 internal configurations, a requirement that limits the applicability of these methods to open-source models and excludes the potential benefits of utilizing sophisticated, closed-source models. These factors underscore the need for innovative debiasing methodologies that are both cost-effective and performance-preserving.\nWe present Axolotl, a novel, model-agnostic and task-agnostic post-processing framework aimed at reducing bias through self-debiasing. Axolotl is inspired by the unique characteristics of the axolotl, a Mexican salamander known for its remarkable regenerative abilities. Just as the axolotl self-heals and regrow parts of its body, the Axolotl model is founded on self-debiasing by identifying and correcting biases in its outputs.\nInspired by zero-shot learning Radford et al. (2019  ###reference_b24###), Axolotl operates through a three-step process: first, it identifies bias (in form of an orientation to a demographic group and an unpleasant characteristic) within the model\u2019s output; Second, it effectively proposes a resolution to counteract the detected bias, and the final step which involves guiding the model to revise and regenerate its previous response in light of this new, unbiased direction. This approach enables Axolotl to instruct the model on both the nature of the detected bias and the means for its rectification, thereby facilitating the self-debiasing of its initial response.\nMore importantly, Axolotl treats the Large Language Model (LLM) as a \u201cblack box\u201d, leveraging public APIs to interact with the model without requiring direct access to the LLM\u2019s parameters. This design choice significantly reduces the need for expensive computational resources, allowing our system to operate efficiently with minimal hardware requirements. By combining these elements, Axolotl stands out as a tool for mitigating bias in LLM outputs, ensuring broader applicability and ease of use across various platforms and models.\nIn summary, to the best of our knowledge, Axolotl is the first of it kinds with the following properties:\nAxolotl treats LLMs as black box, i.e., it does not require access to the internal model configurations.\nIt does not require pre-training or fine-tuning.\nAxolotl is model-agnostic and task-agnostic.\nIt can handle non-binary demographic groups and (multiple) sensitive attributes (including, but not limited to, race and profession).\n###figure_1###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "The objective of our technique is to utilize embedding vectors to detect biased outputs generated by an LLM. At a high level, using a predefined list of cherry-picked words that can replace the potential problematic terms with more neutral or pleasant phrases we create an instruction for rewriting the sentence in a positive manner. We then rely on manual editing of the model's outputs by a human to remove bias without using any automated tools. Figure 1 ###reference_### shows the architecture of our system Axolotl. Given an input prompt, Axolotl uses a Model to generate a response output. The corresponding embedding vector of the output is denoted as. Consider a collection of vectors, representing the embedding vectors for the (demographic) groups (e.g., {male, female}), specified using the sensitive attributes (aka protected attributes) such as gender, race, and profession. We identify the bias in a model response as a pair of (a) an \u201corientation\u201d towards a demographic group and (b) an \u201cunpleasant characteristic\u201d (Section 2.1 ###reference_###). The next step is identifying a \u201cpleasant resolution\u201d to rewrite the prompt and resolve the issue (Section 2.2 ###reference_###). Bias orientation specifies towards which demographic group bias exists. For example, let us consider the output \u201cThe CEO went to the tailor because he needed a suit\u201d in Figure 1 ###reference_###. Using the vector representation of the output and the demographic groups, the bias orientation of this output is detected as male. Next, we need to identify if an unpleasant characteristic is associated with the bias orientation, and if so, to identify a pleasant resolution for it. For that purpose, for each group, we use the set of \u201cunpleasant\u201d and \u201cpleasant\u201d words111 Our research focuses on sentence-level analysis and the embeddings derived from sentences. The words are contextualized within basic sentence structures (e.g., \u201dThis is kind\u201d) to facilitate their representation. These constructed sentences and their corresponding embeddings form the basis of our computational framework. proposed by (May et al., 2019 ###reference_b19###). We refer to the sets of positive and negative words for each group as and. Looking back at Figure 1 ###reference_###, after detecting the bias orientation towards male, the unpleasant characteristic is detected as Manpower. Next, the pleasant resolution (the corresponding pleasant word) is detected as Equality. Finally, after the detection of bias (the orientation and the unpleasant characteristic) and the pleasant resolution, Axolotl use them to regenerate a new prompt to be passed to the (LLM) model (Section 2.3 ###reference_###)."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Bias detection",
            "text": "To identify the orientation of a model response towards a demographic group, we follow (Bolukbasi et al., 2016a ###reference_b5###) and calculate the cosine similarity of the vector representation of , , with the vector representation of each demographic group . We define the similarity function \u00df as . Given a user-specified constant , a high similarity between the pair and , i.e., , is indicative of an orientation towards group . Therefore, we quantify the orientation of a response as its maximum similarity with the demographic groups . The response has an orientation if this similarity is larger than a value . Formally, let . Then the orientation of is, It is important to note that the mere orientation towards a group may not inherently reflect harmful bias. This orientation generates potential issues when it is associated with a socially unpleasant characteristic. In order to inspect the bias in a model response , we utilize the manual editing of the model's outputs by a human to remove bias without using any automated tools. Let be the most similar word in to the response . That is, . We say is associated with an unpleasant characteristic if this similarity is at least . Formally, Description The model response The demographic groups The embedding vector corresponding to the model response The vectors representation (embedding) of the demographic group The similarity between the model\u2019s response and group Set of unpleasant characteristics associated with The vector embedding of an unpleasant characteristic The repair vector Set of pleasant resolutions associated with the group The vector embedding of a pleasant resolution closest neutral word to ."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Identifying a pleasant resolution",
            "text": "The second step after identifying the bias orientation is to offer a pleasant resolution, in terms of word choices that have the potential to mitigate bias within the model response. Assuming that embedding vectors effectively represent sentence semantics, let  be a vector such that, when added to the response vector , the resulting vector is (almost) orthogonal to , i.e., . This equation signifies the neutralization of words associated with negative characteristics linked to a demographic group, ensuring they are orthogonal to the direction of bias Barikeri et al. (2021  ###reference_b1###).\nIn order to find , we first find the vector  in a way that . That is,  is the vector that once added to the response vector, makes it orthogonal to .\nFollowing the vector rejection formula Perwass (2009  ###reference_b22###),  is computed as follows:\nAlthough the addition of the vector  to the response vector make the result orthogonal to , it does not correspond to a word in . Therefore, we manually edit the model's outputs by a human to remove bias without using any automated tools.\nFormally, we identify  as"
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Self-Debias via Assistance",
            "text": "Upon acquiring the pleasant resolution and pinpointing the source of bias, we can formulate an instruction for a human editor to manually edit the model's outputs by removing the bias without using any automated tools. The human editor ensures a coherent version of the original response is generated while maintaining semantic integrity. This involves substituting the unpleasant characteristic with our pleasant resolution."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "As recommended by Dhamala et al. (2021a  ###reference_b11###), we use sentiment, toxicity, and regard as our evaluation metrics. Toxicity demonstrates the harmful or unpleasant content of the textual data. The toxicity classifier labels textual data using a numerical value between 0 and 100. The regard and sentiment classifiers produce outputs categorized as \"positive\", \"negative\", or \"neutral\". It is crucial to distinguish between regard and sentiment. Regard precisely captures the sentiment toward a demographic group, while sentiment represents the overall sentiment of the sentence. Hence, regard serves as a measure of bias Sheng et al. (2019  ###reference_b27###) with a sentence marked as negative by the regard classifier indicating a tendency toward negative representation of a demographic group. This indicates the presence of harmful bias in the sentence. As our ultimate goal is to mitigate the harmful bias produced by the model, we prioritize reducing the proportion of the results generated by Axolotl labeled as negative by the regard classifier.\n###table_1### Table 6  ###reference_### presents the experiment results across four models and three sensitive attributes in BOLD. It is evident that following our method, negative regard has decreased in nearly all instances, with minimal changes observed in positive regard. Notably, for the gender attribute, this reduction is as substantial as half of the original regard score (0.028), in the results produced by Llama2-70B. This means that 50% of the textual data that was labeled as negative before rewrite, was detected positive by the regard classifier post-rewrite. This experiment verifies that Axolotl successfully achieved its goal with decreasing the harmful bias towards protected groups.\nIn contrast to the regard analysis, our attention here is directed towards the positive portion of the model-generated responses. As previously discussed, sentiment signifies the overall polarity of the sentence, indicating whether it leans towards positive or negative. Thus, a sentence labeled as positive conveys a positive message. Given that we have reduced harmful bias through the regard analysis, a higher percentage of positive sentiment suggests an improvement in the responses generated by Axolotl.\nTable 5  ###reference_### showcases the results obtained from the sentiment classifier across all models and sensitive attributes. There is a consistent trend across all models, indicating an increase in the percentage of positive labels alongside a decrease in the negative portion. Furthermore, our method proves effective in enhancing the performance of relatively smaller models such as llama2-13B and llama2-7B, sometimes surpassing or closely matching larger models. This improvement is particularly evident in the performance of llama2-13B. For instance, consider the results of all models on BOLD-profession. Prior to the rewrite, GPT-3.5 exhibited the highest percentage of positive sentiment, with llama2-13B ranking second. However, post-rewrite, llama2-13B generated more responses with positive sentiment than the other models.\nThe toxicity classifier evaluates content for unpleasant, harmful, or disrespectful elements and assigns a score between 0 and 100 to each sentence. Therefore, a decrease in toxicity indicates a superior performance of Axolotl. Table 3  ###reference_### displays the percentage reduction in toxicity for each model post-rewrite compared to the pre-rewrite version across various sensitive attributes. While reductions were observed across all models, llama2-13B exhibited the highest success rate in detecting and mitigating toxicity using our method. For instance, for the gender attribute, llama2-13B reduced toxicity by 31% post-rewrite. Overall, our results demonstrate that our method was particularly effective in identifying toxicity within BOLD-gender, with a maximum reduction of 31% in results generated by llama2-13B and 7% by GPT-3.5. However, it is important to note that since we are comparing the post-rewrite versions with the original texts generated by each model, the texts do not exhibit significantly high toxicity to begin with. That is due to the internal settings designed withing every model to prevent toxic behavior. This explains why the percentage improvements in many cases are relatively small."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Experiments Settings",
            "text": "We performed our experiments in the publicly accessible Google Colab environment. We assessed various models with parameter sizes of 7, 13, 20, and 70 billion. We utilized public APIs provided by OpenAI and AnyScale to prompt Llama 2 with parameter sizes of 7, 13, and 70 billion, as well as the GPT 3.5 turbo model.\nFor generating embedding vectors for demographic group sentences, responses, and collections of words (), we employed an instruction-based fine-tuned embedder, InstructOR, as described in Su et al. (2023  ###reference_b29###)."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Datasets",
            "text": "We experiment with gender, race, and profession as sensitive attributes that specify the demographic groups. We evaluate the performance of Axolotl using three benchmark datasets: BOLD Dhamala et al. (2021a  ###reference_b11###), Stereoset Nadeem et al. (2021  ###reference_b20###), and WinoBias Zhao et al. (2018  ###reference_b32###)."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Evaluation Tasks",
            "text": "To delve deeper into the effectiveness of our methodology in identifying and addressing bias from multiple angles, we designed our experiments around two key categories of task. The initial task assesses the capability of the LLM to find improved responses from a range of options based on instructions provided by Axolotl. Examples of such tasks include Question Answering(3.3.1  ###reference_.SSS1###) and Co-reference Resolution(3.3.2  ###reference_.SSS2###). The second category evaluates the model\u2019s proficiency in rephrasing sentences according to the provided instructions. Chat Completion(3.3.3  ###reference_.SSS3###) serves as an instance of such tasks.\nIn order to evaluate Axolotl, we use various metrics corresponding to each task. Following the suggestion by Dhamala et al. (2021a  ###reference_b11###), we incorporate toxicity and regard scores as a metric to underscore the effectiveness of Axolotl on BOLD. For this purpose, we use a BERT-based model222toxic-bert  ###reference_###, that is trained on a large number of Wikipedia comments and offers toxicity scores for input text across all sensitive attributes.\nAccording to Sheng et al. (2019  ###reference_b27###), regard333Regard classifier  ###reference_g-biases### aims to measure the sentiment directed towards a particular demographic group, rather than assessing the general sentiment of LM generated sentences. Their framework is designed specifically for sensitive attributes such as race, gender, and sexual orientation.\n###table_2### As recommended by Dhamala et al. (2021a  ###reference_b11###  ###reference_b11###), we use sentiment, toxicity, and regard as our evaluation metrics. Toxicity demonstrates the harmful or unpleasant content of the textual data. The toxicity classifier labels textual data using a numerical value between 0 and 100. The regard and sentiment classifiers produce outputs categorized as \"positive\", \"negative\", or \"neutral\". It is crucial to distinguish between regard and sentiment. Regard precisely captures the sentiment toward a demographic group, while sentiment represents the overall sentiment of the sentence. Hence, regard serves as a measure of bias Sheng et al. (2019  ###reference_b27###  ###reference_b27###) with a sentence marked as negative by the regard classifier indicating a tendency toward negative representation of a demographic group. This indicates the presence of harmful bias in the sentence. As our ultimate goal is to mitigate the harmful bias produced by the model, we prioritize reducing the proportion of the results generated by Axolotl labeled as negative by the regard classifier.\n###table_3### Table 6  ###reference_###  ###reference_### presents the experiment results across four models and three sensitive attributes in BOLD. It is evident that following our method, negative regard has decreased in nearly all instances, with minimal changes observed in positive regard. Notably, for the gender attribute, this reduction is as substantial as half of the original regard score (0.028), in the results produced by Llama2-70B. This means that 50% of the textual data that was labeled as negative before rewrite, was detected positive by the regard classifier post-rewrite. This experiment verifies that Axolotl successfully achieved its goal with decreasing the harmful bias towards protected groups.\nIn contrast to the regard analysis, our attention here is directed towards the positive portion of the model-generated responses. As previously discussed, sentiment signifies the overall polarity of the sentence, indicating whether it leans towards positive or negative. Thus, a sentence labeled as positive conveys a positive message. Given that we have reduced harmful bias through the regard analysis, a higher percentage of positive sentiment suggests an improvement in the responses generated by Axolotl.\nTable 5  ###reference_###  ###reference_### showcases the results obtained from the sentiment classifier across all models and sensitive attributes. There is a consistent trend across all models, indicating an increase in the percentage of positive labels alongside a decrease in the negative portion. Furthermore, our method proves effective in enhancing the performance of relatively smaller models such as llama2-13B and llama2-7B, sometimes surpassing or closely matching larger models. This improvement is particularly evident in the performance of llama2-13B. For instance, consider the results of all models on BOLD-profession. Prior to the rewrite, GPT-3.5 exhibited the highest percentage of positive sentiment, with llama2-13B ranking second. However, post-rewrite, llama2-13B generated more responses with positive sentiment than the other models.\nThe toxicity classifier evaluates content for unpleasant, harmful, or disrespectful elements and assigns a score between 0 and 100 to each sentence. Therefore, a decrease in toxicity indicates a superior performance of Axolotl. Table 3  ###reference_###  ###reference_### displays the percentage reduction in toxicity for each model post-rewrite compared to the pre-rewrite version across various sensitive attributes. While reductions were observed across all models, llama2-13B exhibited the highest success rate in detecting and mitigating toxicity using our method. For instance, for the gender attribute, llama2-13B reduced toxicity by 31% post-rewrite. Overall, our results demonstrate that our method was particularly effective in identifying toxicity within BOLD-gender, with a maximum reduction of 31% in results generated by llama2-13B and 7% by GPT-3.5. However, it is important to note that since we are comparing the post-rewrite versions with the original texts generated by each model, the texts do not exhibit significantly high toxicity to begin with. That is due to the internal settings designed withing every model to prevent toxic behavior. This explains why the percentage improvements in many cases are relatively small."
        },
        {
            "section_id": "3.3.1",
            "parent_section_id": "3.3",
            "section_name": "3.3.1 Question Answering",
            "text": "The objective of this task is to evaluate Axolotl at the discourse level through multiple-choice questions. After identifying bias\n(in form of an orientation to a demographic group and an unpleasant characteristic) and proposing a pleasant resolution, the model generates a new response with a lower bias. We utilize the Stereoset dataset, developed by Nadeem et al. (2021  ###reference_b20###), specifically designed for multi-choice question answering. Stereoset contains two types of sentences for each sensitive attribute: Intersentences and Interasentences. For our task, we focus on Intersentences, where each data instance consists of a context sentence containing a target group and three corresponding sentences labeled as \u201cstereotype\u201d, \u201canti-stereotype\u201d, and \u201cmeaningless\u201d. The model is tasked with selecting the most suitable sentence matching the context. We follow the bias detection, pleasant resolution identification, and self-debiasing steps outlined in Section 2  ###reference_###.\nGiven the initial response  of the LLM model, the orientation to a group , the unpleasant characteristic (), and the pleasant resolution (),\nthe model is prompted to identify a better response from the three options provided. To evaluate the overall performance of Axolotl, we use the Stereotype Score (ss), which, according to Nadeem et al. (2021  ###reference_b20###), quantifies the ratio of stereotype to anti-stereotype association. A decrease in the ss score indicates a preference for anti-stereotype responses over stereotypical ones during the rewriting process. In an ideal scenario, a model with a ss score of 50 indicates a lack of preference for either stereotype or anti-stereotype scenarios. Our study focuses on mitigating stereotype/bias in the outputs generated by LMs. Therefore, we assess the effectiveness of Axolotl by measuring the reduction in ss after the rewrite.\nTable 4  ###reference_### presents the ss results across all models and sensitive attributes before and after the rewrite. Our experimental findings reveal a visible decrease in ss across all models and attributes, signifying an increase in anti-stereotype responses compared to stereotypical ones. In cases where the scores were already below 50, such as in the race attribute where  across models, the responses were already leaning towards anti-stereotypes, leaving minimal room for improvement by Axolotl. However, in instances where ss deviated significantly from 50, Axolotl successfully detected bias and provided effective guidance to reduce ss by promoting anti-stereotype associations. Specifically, for the profession attribute, the 10.1 drop in ss for GPT-3.5 and the 13.29 decrease for Llama2-70b, and the 8.16 decrease for Llama2-70b in gender attribute, illustrate the successful debiasing using Axolotl."
        },
        {
            "section_id": "3.3.2",
            "parent_section_id": "3.3",
            "section_name": "3.3.2 Co-reference Resolution",
            "text": "We structured the co-reference resolution experiment similarly to question answering, aiming to assess the capacity of the model to enhance its response from a provided set of options. The WinoBias dataset, created by  Zhao et al. (2018  ###reference_b32###), is tailored to study gender bias within professions through co-reference resolution system. Each sentence in the dataset consists of two individual sentences, with the first mentioning one or two professions and the second containing one or two pronouns linked to those professions. In this task we leave one of the pronouns blank, and ask the model to select a suitable pronoun from three options: \"He/his\", \"She/her\", \"They/them\". Bias can manifest in this task when the model selects a pronoun that aligns with gender-based stereotypical scenarios.\nFor instance, the sentence \"[The lawyer] yelled at the hairdresser because [he] was mad.\" demonstrates a common stereotype linking \"lawyer\" with the male gender. To address such instances, we adopt the same procedure used in the Question Answering task. We provide the model with an instruction containing both  and , guiding it to produce a more appropriate response. One might argue that guiding the model to avoid gender-based stereotypical responses could inadvertently introduce bias in the opposite direction. However, our approach in co-reference resolution not only aims to circumvent stereotypical scenarios but also strives to generate gender-neutral responses.\nTable 3  ###reference_### presents the results on the WinoBias dataset across all four models. These results indicate a notable decrease in gender-bias after the rewrite, with over 82% of our generated responses being gender-neutral. For instance, the results from llama2-7B show a transition from 88.4% male and 8.4% female to 7% male, 10.7% female, and 82.3% neutral responses post-rewrite. This underscores the effectiveness of Axolotl in achieving gender neutralization. Furthermore, we achieved significant improvement with a smaller model like llama2-7B, which achieved 82.3% gender neutralization post-rewrite. It outperformed larger models such as llama2-70B, which had only 29.3% gender neutralization pre-rewrite.\n###table_4###"
        },
        {
            "section_id": "3.3.3",
            "parent_section_id": "3.3",
            "section_name": "3.3.3 Chat Completion",
            "text": "The second set of tasks aimed to evaluate Axolotl\u2019s ability in conversational setting and generating coherent responses. Given the debiasing instruction the model should be able to maintain the context of a conversation. These instructions include identifying the unpleasant characteristic () and suggesting the pleasant resolution () for the model to integrate during the rewrite phase.\nIn the Chat Completion task, each prompt from the dataset requires the model to complete the text, essentially making each dataset instance a \"prefix\" for a paragraph. The BOLD dataset, contains sentences ranging from 6 to 9 words across various domains from Wikipedia. We focus on domains related to race, gender, and profession.\nAs recommended by Dhamala et al. (2021a  ###reference_b11###  ###reference_b11###  ###reference_b11###), we use sentiment, toxicity, and regard as our evaluation metrics. Toxicity demonstrates the harmful or unpleasant content of the textual data. The toxicity classifier labels textual data using a numerical value between 0 and 100. The regard and sentiment classifiers produce outputs categorized as \"positive\", \"negative\", or \"neutral\". It is crucial to distinguish between regard and sentiment. Regard precisely captures the sentiment toward a demographic group, while sentiment represents the overall sentiment of the sentence. Hence, regard serves as a measure of bias Sheng et al. (2019  ###reference_b27###  ###reference_b27###  ###reference_b27###) with a sentence marked as negative by the regard classifier indicating a tendency toward negative representation of a demographic group. This indicates the presence of harmful bias in the sentence. As our ultimate goal is to mitigate the harmful bias produced by the model, we prioritize reducing the proportion of the results generated by Axolotl labeled as negative by the regard classifier.\n###table_5### Table 6  ###reference_###  ###reference_###  ###reference_### presents the experiment results across four models and three sensitive attributes in BOLD. It is evident that following our method, negative regard has decreased in nearly all instances, with minimal changes observed in positive regard. Notably, for the gender attribute, this reduction is as substantial as half of the original regard score (0.028), in the results produced by Llama2-70B. This means that 50% of the textual data that was labeled as negative before rewrite, was detected positive by the regard classifier post-rewrite. This experiment verifies that Axolotl successfully achieved its goal with decreasing the harmful bias towards protected groups.\nIn contrast to the regard analysis, our attention here is directed towards the positive portion of the model-generated responses. As previously discussed, sentiment signifies the overall polarity of the sentence, indicating whether it leans towards positive or negative. Thus, a sentence labeled as positive conveys a positive message. Given that we have reduced harmful bias through the regard analysis, a higher percentage of positive sentiment suggests an improvement in the responses generated by Axolotl.\nTable 5  ###reference_###  ###reference_###  ###reference_### showcases the results obtained from the sentiment classifier across all models and sensitive attributes. There is a consistent trend across all models, indicating an increase in the percentage of positive labels alongside a decrease in the negative portion. Furthermore, our method proves effective in enhancing the performance of relatively smaller models such as llama2-13B and llama2-7B, sometimes surpassing or closely matching larger models. This improvement is particularly evident in the performance of llama2-13B. For instance, consider the results of all models on BOLD-profession. Prior to the rewrite, GPT-3.5 exhibited the highest percentage of positive sentiment, with llama2-13B ranking second. However, post-rewrite, llama2-13B generated more responses with positive sentiment than the other models.\nThe toxicity classifier evaluates content for unpleasant, harmful, or disrespectful elements and assigns a score between 0 and 100 to each sentence. Therefore, a decrease in toxicity indicates a superior performance of Axolotl. Table 3  ###reference_###  ###reference_###  ###reference_### displays the percentage reduction in toxicity for each model post-rewrite compared to the pre-rewrite version across various sensitive attributes. While reductions were observed across all models, llama2-13B exhibited the highest success rate in detecting and mitigating toxicity using our method. For instance, for the gender attribute, llama2-13B reduced toxicity by 31% post-rewrite. Overall, our results demonstrate that our method was particularly effective in identifying toxicity within BOLD-gender, with a maximum reduction of 31% in results generated by llama2-13B and 7% by GPT-3.5. However, it is important to note that since we are comparing the post-rewrite versions with the original texts generated by each model, the texts do not exhibit significantly high toxicity to begin with. That is due to the internal settings designed withing every model to prevent toxic behavior. This explains why the percentage improvements in many cases are relatively small."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Research into human-like bias in Large Language Models is an ongoing endeavor aimed at addressing bias-related challenges from multiple perspectives. Bias can infiltrate LLMs through various channels, including data annotation via crowdsourcing Otterbacher et al. (2018  ###reference_b21###); Buolamwini and Gebru (2018  ###reference_b7###); Bender and Friedman (2018  ###reference_b3###), dataset diversity across demographic groups Bolukbasi et al. (2016b  ###reference_b6###); Caliskan et al. (2017a  ###reference_b8###), and selecting models that amplify specific parts of the dataset, potentially overlooking certain demographic groups (e.g., models tailored for English-speaking users) Solaiman et al. (2019  ###reference_b28###); Hovy and Prabhumoye (2021  ###reference_b15###). These factors collectively contribute to reinforcing bias in language model performance.\nTo address bias, researchers have proposed various methods. Counterfactual Data Augmentation (CDA) Maudslay et al. (2019a  ###reference_b17###) and data augmentation using demographic perturbation Qian et al. (2022  ###reference_b23###) aim to diminish bias within training datasets. A significant body of research is dedicated to addressing and mitigating existing bias at both the word-level Zhao et al. (2019  ###reference_b31###); Basta et al. (2019  ###reference_b2###); Dhamala et al. (2021b  ###reference_b12###); Ravfogel et al. (2020  ###reference_b25###) and sentence-level representations May et al. (2019  ###reference_b19###); Liu et al. (2019  ###reference_b16###); Cheng et al. (2021  ###reference_b10###).\nDespite this, studies have indicated that:\nBoth data augmentation and pre-training language models can be costly Garimella et al. (2021  ###reference_b13###).\nMany existing methods compromise the quality of the generated language model response Garimella et al. (2021  ###reference_b13###).\nSeveral existing methods are constrained to particular tasks Zheng et al. (2023  ###reference_b33###) or specific sensitive attributes Garimella et al. (2021  ###reference_b13###).\nNearly all current research relies on open-source models, necessitating access to the models\u2019 internal configurations Schick et al. (2021  ###reference_b26###); Guo et al. (2022  ###reference_b14###).\nOur method is inspired by zero-shot learning techniques that leverage task descriptions Radford et al. (2019  ###reference_b24###).\nTo the best of our knowledge, the closest work to ours is by Schick et al. (2021  ###reference_b26###), which demonstrates that language models are cognizant of their biases and can self-diagnose by receiving a description of bias or stereotype. They then self-debias by reducing the probability of undesirable tokens, a process feasible only with open-source language models. Our method stands out as the first of its kind, as it does not require pre-training, fine-tuning, or accessing internal configurations (e.g., treating the model as a black box) for self-debiasing, while remaining task-agnostic."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this study, we introduced Axolotl, a novel post-processing framework designed to mitigate biases in Large Language Model (LLM) outputs. By leveraging self-debiasing techniques, Axolotl operates as a task-agnostic and model-agnostic tool and addresses key challenges in bias mitigation without compromising computational efficiency or model performance. Through a three-step process resembling zero-shot learning, Axolotl effectively identifies and corrects biases in LLM outputs, ensuring fairer outcomes across various applications. By treating LLMs as \u201cblack boxes\u201d and utilizing public APIs, Axolotl offers broader applicability and ease of use, making it a valuable tool for practitioners seeking to address bias in natural language processing systems. Future research can further explore the scalability and generalizability of Axolotl across different LLM architectures and applications, ultimately advancing the goal of creating more equitable and inclusive AI systems."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "In recognizing the limitations of our study, it is crucial to understand that the success of our approach closely depends on the effectiveness of embedding vectors Su et al. (2023  ###reference_b29###) and their ability to capture and reflect subtle semantic biases in language. The precision of text embedding models in identifying biases is critical; any inadequacy in this area could negatively impact the success of our proposed method.\nFurthermore, the integrity and selection of word sets  are crucial for the model\u2019s success in identifying biases and suggestion viable resolutions. Inadequacies in these collections could impair the model\u2019s ability to effectively address the bias.\nAlthough Axolotl introduces a robust mechanism for mitigating bias, it does not assure absolute eradication of bias. It serves as a post-processing technique that operates without altering the foundational parameters of the underlying model, thereby not addressing the model\u2019s inherent biases directly.\nMoreover, the implementation of Axolotl as an online framework necessitates network access to interact with Language Models via public APIs. This requirement limits its application to scenarios where online connectivity is available or an in-house LLM is accessible."
        }
    ],
    "appendix": [],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S2.T1\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Table of Notations</figcaption>\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S2.T1.17\">\n<tr class=\"ltx_tr\" id=\"S2.T1.17.18\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S2.T1.17.18.1\">Notation</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" id=\"S2.T1.17.18.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S2.T1.17.18.2.1\">Description</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.1\"></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S2.T1.1.1.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S2.T1.1.1.2.1\">The model response</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.2.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.2.2.1\"></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.2.2.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S2.T1.2.2.2.1\">The demographic groups</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.3.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.3.3.1\"></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.3.3.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S2.T1.3.3.2.1\">The embedding vector corresponding to the model response</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.5.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.4.1\"></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.5.5.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S2.T1.5.5.2.1.1\">The vectors representation (embedding) of the demographic group </p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.7.7\">\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.6.6.1\"></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.7.7.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S2.T1.7.7.2.1.1\">The similarity between the model\u2019s response and group </p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.9.9\">\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.8.8.1\"></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.9.9.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S2.T1.9.9.2.1.1\">Set of unpleasant characteristics associated with </p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.11.11\">\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.10.10.1\"></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.11.11.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S2.T1.11.11.2.1.1\">The vector embedding of an unpleasant characteristic </p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.12.12\">\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.12.12.1\"></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.12.12.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S2.T1.12.12.2.1\">The repair vector</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.14.14\">\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.13.13.1\"></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.14.14.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S2.T1.14.14.2.1.1\">Set of pleasant resolutions associated with the group </p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.17.17\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S2.T1.15.15.1\"></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S2.T1.17.17.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S2.T1.17.17.3.2.2\">The vector embedding of a pleasant resolution  closest neutral word to .</p>\n</td>\n</tr>\n</table>\n</figure>",
            "capture": "Table 1: Table of Notations"
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S2.T3\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Toxicity score reduction percentage with respect to the original output.</figcaption><div class=\"ltx_flex_figure ltx_flex_table\">\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<table class=\"ltx_tabular ltx_minipage ltx_flex_size_2 ltx_align_middle\" id=\"S2.T3.3\" style=\"width:173.4pt;\">\n<tr class=\"ltx_tr\" id=\"S2.T3.3.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S2.T3.3.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T3.3.1.1.1\">Group</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.3.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T3.3.1.2.1\">Race</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.3.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T3.3.1.3.1\">Gender</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.3.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T3.3.1.4.1\">Profession</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T3.3.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S2.T3.3.2.1\">GPT-3.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.3.2.2\">2.14%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.3.2.3\">7.38%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.3.2.4\">3.71%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T3.3.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S2.T3.3.3.1\">llama2-70B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.3.3.2\">10.51%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.3.3.3\">30.03%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.3.3.4\">1.92%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T3.3.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S2.T3.3.4.1\">llama2-13B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.3.4.2\">1.47%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.3.4.3\">31.31%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.3.4.4\">4.89%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T3.3.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_t\" id=\"S2.T3.3.5.1\">llama2-7B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S2.T3.3.5.2\">6.81%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S2.T3.3.5.3\">17.18%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S2.T3.3.5.4\">1.04%</td>\n</tr>\n</table>\n</div>\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<table class=\"ltx_tabular ltx_minipage ltx_flex_size_2 ltx_align_middle\" id=\"S2.T3.4\" style=\"width:216.8pt;\">\n<tr class=\"ltx_tr\" id=\"S2.T3.4.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S2.T3.4.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T3.4.1.1.1\">Group</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.4.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T3.4.1.2.1\">Male</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.4.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T3.4.1.3.1\">Female</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.4.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T3.4.1.4.1\">Neutral</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T3.4.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S2.T3.4.2.1\">GPT-3.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.4.2.2\">0.510</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.4.2.3\">0.072</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.4.2.4\">0.418</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T3.4.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T3.4.3.1\">GPT-3.5-rewrite</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.4.3.2\">0.065</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.4.3.3\">0.079</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.4.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T3.4.3.4.1\">0.856</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T3.4.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S2.T3.4.4.1\">llama2-70B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.4.4.2\">0.378</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.4.4.3\">0.329</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.4.4.4\">0.293</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T3.4.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T3.4.5.1\">llama2-70B-rewrite</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.4.5.2\">0.005</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.4.5.3\">0.126</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.4.5.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T3.4.5.4.1\">0.869</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T3.4.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S2.T3.4.6.1\">llama2-13B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.4.6.2\">0.317</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.4.6.3\">0.318</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.4.6.4\">0.545</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T3.4.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T3.4.7.1\">llama2-13B-rewrite</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.4.7.2\">0.012</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.4.7.3\">0.161</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.4.7.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T3.4.7.4.1\">0.827</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T3.4.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S2.T3.4.8.1\">llama2-7B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.4.8.2\">0.884</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.4.8.3\">0.084</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.4.8.4\">0.032</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T3.4.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S2.T3.4.9.1\">llama2-7B-rewrite</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S2.T3.4.9.2\">0.070</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S2.T3.4.9.3\">0.107</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S2.T3.4.9.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T3.4.9.4.1\">0.823</span></td>\n</tr>\n</table>\n</div>\n</div>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Toxicity score reduction percentage with respect to the original output.</figcaption>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Proportions of answer with male and female pronouns</figcaption>\n</figure>",
            "capture": "Table 2: Toxicity score reduction percentage with respect to the original output."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T4\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S3.T4.1\">\n<tr class=\"ltx_tr\" id=\"S3.T4.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" id=\"S3.T4.1.1.1\"><span class=\"ltx_text\" id=\"S3.T4.1.1.1.1\">\n<span class=\"ltx_inline-block\" id=\"S3.T4.1.1.1.1.1\">\n<span class=\"ltx_p\" id=\"S3.T4.1.1.1.1.1.1\">Sensitive</span>\n<span class=\"ltx_p\" id=\"S3.T4.1.1.1.1.1.2\">Attributes</span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T4.1.1.2\"><span class=\"ltx_text\" id=\"S3.T4.1.1.2.1\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T4.1.1.3\"><span class=\"ltx_text\" id=\"S3.T4.1.1.3.1\">\n<span class=\"ltx_inline-block\" id=\"S3.T4.1.1.3.1.1\">\n<span class=\"ltx_p\" id=\"S3.T4.1.1.3.1.1.1\">Stereotype Score</span>\n<span class=\"ltx_p\" id=\"S3.T4.1.1.3.1.1.2\">Before rewrite</span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T4.1.1.4\"><span class=\"ltx_text\" id=\"S3.T4.1.1.4.1\">\n<span class=\"ltx_inline-block\" id=\"S3.T4.1.1.4.1.1\">\n<span class=\"ltx_p\" id=\"S3.T4.1.1.4.1.1.1\">Stereotype Score</span>\n<span class=\"ltx_p\" id=\"S3.T4.1.1.4.1.1.2\">After rewrite</span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T4.1.1.5\"><span class=\"ltx_text\" id=\"S3.T4.1.1.5.1\">\n<span class=\"ltx_inline-block\" id=\"S3.T4.1.1.5.1.1\">\n<span class=\"ltx_p\" id=\"S3.T4.1.1.5.1.1.1\">Stereotype Score</span>\n<span class=\"ltx_p\" id=\"S3.T4.1.1.5.1.1.2\">Reduction</span>\n</span></span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" id=\"S3.T4.1.2.1\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S3.T4.1.2.1.1\">Gender</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T4.1.2.2\">Llama2-70b</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T4.1.2.3\">62.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T4.1.2.4\">54.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T4.1.2.5\">8.16</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.1.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T4.1.3.1\">Llama2-13b</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T4.1.3.2\">56.14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T4.1.3.3\">52.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T4.1.3.4\">3.84</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.1.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T4.1.4.1\">Llama2-7b</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T4.1.4.2\">56.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T4.1.4.3\">52.45</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T4.1.4.4\">3.80</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.1.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T4.1.5.1\">GPT-3.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T4.1.5.2\">51.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T4.1.5.3\">47.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T4.1.5.4\">3.72</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.1.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" id=\"S3.T4.1.6.1\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S3.T4.1.6.1.1\">Race</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T4.1.6.2\">Llama2-70b</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T4.1.6.3\">46.87</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T4.1.6.4\">45.99</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T4.1.6.5\">0.88</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.1.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T4.1.7.1\">Llama2-13b</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T4.1.7.2\">43.53</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T4.1.7.3\">43.37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T4.1.7.4\">0.20</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.1.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T4.1.8.1\">Llama2-7b</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T4.1.8.2\">43.36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T4.1.8.3\">41.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T4.1.8.4\">1.95</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.1.9\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T4.1.9.1\">GPT-3.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T4.1.9.2\">41.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T4.1.9.3\">37.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T4.1.9.4\">3.77</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.1.10\">\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" id=\"S3.T4.1.10.1\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S3.T4.1.10.1.1\">Profession</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T4.1.10.2\">Llama2-70b</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T4.1.10.3\">61.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T4.1.10.4\">47.76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T4.1.10.5\">13.29</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.1.11\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T4.1.11.1\">Llama2-13b</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T4.1.11.2\">53.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T4.1.11.3\">53.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T4.1.11.4\">0.15</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.1.12\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T4.1.12.1\">Llama2-7b</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T4.1.12.2\">56.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T4.1.12.3\">55.83</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T4.1.12.4\">0.22</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.1.13\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S3.T4.1.13.1\">GPT-3.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S3.T4.1.13.2\">58.36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S3.T4.1.13.3\">48.26</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S3.T4.1.13.4\">10.1</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>Results obtained from experiments conducted on the Stereoset dataset.</figcaption>\n</figure>",
            "capture": "Table 4: Results obtained from experiments conducted on the Stereoset dataset."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T5\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S3.T5.1\">\n<tr class=\"ltx_tr\" id=\"S3.T5.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T5.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T5.1.1.1.1\">Group</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S3.T5.1.1.2\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S3.T5.1.1.2.1\">Race</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S3.T5.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T5.1.1.3.1\">Gender</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S3.T5.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T5.1.1.4.1\">Profession</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T5.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T5.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T5.1.2.1.1\">regard</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T5.1.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T5.1.2.2.1\">Positive</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T5.1.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T5.1.2.3.1\">Negative</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T5.1.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T5.1.2.4.1\">Positive</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T5.1.2.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T5.1.2.5.1\">Negative</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T5.1.2.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T5.1.2.6.1\">Positive</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T5.1.2.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T5.1.2.7.1\">Negative</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T5.1.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T5.1.3.1\">GPT-3.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T5.1.3.2\">0.618</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T5.1.3.3\">0.016</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T5.1.3.4\">0.747</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T5.1.3.5\">0.008</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T5.1.3.6\">0.209</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T5.1.3.7\">0.004</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T5.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T5.1.4.1\">GPT-3.5-rewrite</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T5.1.4.2\">0.630</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T5.1.4.3\">0.015</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T5.1.4.4\">0.769</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T5.1.4.5\">0.009</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T5.1.4.6\">0.231</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T5.1.4.7\">0.004</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T5.1.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T5.1.5.1\">llama2-70B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T5.1.5.2\">0.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T5.1.5.3\">0.021</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T5.1.5.4\">0.401</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T5.1.5.5\">0.012</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T5.1.5.6\">0.144</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T5.1.5.7\">0.012</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T5.1.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T5.1.6.1\">llama2-70B-rewrite</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T5.1.6.2\">0.463</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T5.1.6.3\">0.019</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T5.1.6.4\">0.442</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T5.1.6.5\">0.007</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T5.1.6.6\">0.192</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T5.1.6.7\">0.009</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T5.1.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T5.1.7.1\">llama2-13B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T5.1.7.2\">0.537</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T5.1.7.3\">0.019</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T5.1.7.4\">0.567</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T5.1.7.5\">0.023</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T5.1.7.6\">0.197</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T5.1.7.7\">0.011</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T5.1.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T5.1.8.1\">llama2-13B-rewrite</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T5.1.8.2\">0.627</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T5.1.8.3\">0.017</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T5.1.8.4\">0.703</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T5.1.8.5\">0.014</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T5.1.8.6\">0.281</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T5.1.8.7\">0.008</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T5.1.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T5.1.9.1\">llama2-7B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T5.1.9.2\">0.303</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T5.1.9.3\">0.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T5.1.9.4\">0.336</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T5.1.9.5\">0.039</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T5.1.9.6\">0.103</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T5.1.9.7\">0.019</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T5.1.10\">\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S3.T5.1.10.1\">llama2-7B-rewrite</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S3.T5.1.10.2\">0.348</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S3.T5.1.10.3\">0.022</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S3.T5.1.10.4\">0.374</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S3.T5.1.10.5\">0.026</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S3.T5.1.10.6\">0.132</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S3.T5.1.10.7\">0.017</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>Proportions of texts classified as having positive and negative sentiment</figcaption>\n</figure>",
            "capture": "Table 5: Proportions of texts classified as having positive and negative sentiment"
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T6\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S3.T6.1\">\n<tr class=\"ltx_tr\" id=\"S3.T6.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T6.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T6.1.1.1.1\">Group</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S3.T6.1.1.2\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S3.T6.1.1.2.1\">Race</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S3.T6.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T6.1.1.3.1\">Gender</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T6.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T6.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T6.1.2.1.1\">regard</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T6.1.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T6.1.2.2.1\">Positive</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T6.1.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T6.1.2.3.1\">Negative</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T6.1.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T6.1.2.4.1\">Positive</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T6.1.2.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T6.1.2.5.1\">Negative</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T6.1.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T6.1.3.1\">GPT-3.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T6.1.3.2\">0.873</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T6.1.3.3\">0.038</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T6.1.3.4\">0.906</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T6.1.3.5\">0.026</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T6.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T6.1.4.1\">GPT-3.5-rewrite</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T6.1.4.2\">0.879</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T6.1.4.3\">0.035</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T6.1.4.4\">0.915</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T6.1.4.5\">0.024</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T6.1.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T6.1.5.1\">llama2-70B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T6.1.5.2\">0.832</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T6.1.5.3\">0.058</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T6.1.5.4\">0.828</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T6.1.5.5\">0.056</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T6.1.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T6.1.6.1\">llama2-70B-rewrite</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T6.1.6.2\">0.694</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T6.1.6.3\">0.041</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T6.1.6.4\">0.676</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T6.1.6.5\">0.028</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T6.1.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T6.1.7.1\">llama2-13B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T6.1.7.2\">0.658</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T6.1.7.3\">0.019</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T6.1.7.4\">0.664</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T6.1.7.5\">0.022</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T6.1.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T6.1.8.1\">llama2-13B-rewrite</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T6.1.8.2\">0.601</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T6.1.8.3\">0.020</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T6.1.8.4\">0.547</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T6.1.8.5\">0.016</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T6.1.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T6.1.9.1\">llama2-7B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T6.1.9.2\">0.700</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T6.1.9.3\">0.047</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T6.1.9.4\">0.627</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T6.1.9.5\">0.042</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T6.1.10\">\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S3.T6.1.10.1\">llama2-7B-rewrite</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S3.T6.1.10.2\">0.654</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S3.T6.1.10.3\">0.039</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S3.T6.1.10.4\">0.592</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S3.T6.1.10.5\">0.032</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 6: </span>Proportions of texts classified as having positive and negative regard.</figcaption>\n</figure>",
            "capture": "Table 6: Proportions of texts classified as having positive and negative regard."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.00198v1_figure_1.png",
            "caption": "Figure 1: System Architecture."
        }
    },
    "references": [
        {
            "1": {
                "title": "RedditBias: A real-world resource for bias evaluation and debiasing of conversational language models.",
                "author": "Soumya Barikeri, Anne Lauscher, Ivan Vuli\u0107, and Goran Glava\u0161. 2021.",
                "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1941\u20131955, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.acl-long.151"
            }
        },
        {
            "2": {
                "title": "Evaluating the underlying gender bias in contextualized word embeddings.",
                "author": "Christine Basta, Marta R. Costa-juss\u00e0, and Noe Casas. 2019.",
                "venue": "In Proceedings of the First Workshop on Gender Bias in Natural Language Processing, pages 33\u201339, Florence, Italy. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/W19-3805"
            }
        },
        {
            "3": {
                "title": "Data statements for natural language processing: Toward mitigating system bias and enabling better science.",
                "author": "Emily M. Bender and Batya Friedman. 2018.",
                "venue": "Transactions of the Association for Computational Linguistics, 6:587\u2013604.",
                "url": "https://doi.org/10.1162/tacl_a_00041"
            }
        },
        {
            "4": {
                "title": "On the dangers of stochastic parrots: Can language models be too big?",
                "author": "Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021.",
                "venue": "In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT \u201921, page 610\u2013623, New York, NY, USA. Association for Computing Machinery.",
                "url": "https://doi.org/10.1145/3442188.3445922"
            }
        },
        {
            "5": {
                "title": "Man is to computer programmer as woman is to homemaker? debiasing word embeddings.",
                "author": "Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam Kalai. 2016a.",
                "venue": "NIPS\u201916, page 4356\u20134364, Red Hook, NY, USA. Curran Associates Inc.",
                "url": null
            }
        },
        {
            "6": {
                "title": "Man is to computer programmer as woman is to homemaker? debiasing word embeddings.",
                "author": "Tolga Bolukbasi, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam Tauman Kalai. 2016b.",
                "venue": "In Neural Information Processing Systems.",
                "url": "https://api.semanticscholar.org/CorpusID:1704893"
            }
        },
        {
            "7": {
                "title": "Gender shades: Intersectional accuracy disparities in commercial gender classification.",
                "author": "Joy Buolamwini and Timnit Gebru. 2018.",
                "venue": "In Proceedings of the 1st Conference on Fairness, Accountability and Transparency, volume 81 of Proceedings of Machine Learning Research, pages 77\u201391. PMLR.",
                "url": "https://proceedings.mlr.press/v81/buolamwini18a.html"
            }
        },
        {
            "8": {
                "title": "Semantics derived automatically from language corpora contain human-like biases.",
                "author": "Aylin Caliskan, Joanna Bryson, and Arvind Narayanan. 2017a.",
                "venue": "Science, 356:183\u2013186.",
                "url": null
            }
        },
        {
            "9": {
                "title": "Semantics derived automatically from language corpora contain human-like biases.",
                "author": "Aylin Caliskan, Joanna J. Bryson, and Arvind Narayanan. 2017b.",
                "venue": "Science, 356(6334):183\u2013186.",
                "url": "https://doi.org/10.1126/science.aal4230"
            }
        },
        {
            "10": {
                "title": "Fairfil: Contrastive neural debiasing method for pretrained text encoders.",
                "author": "Pengyu Cheng, Weituo Hao, Siyang Yuan, Shijing Si, and Lawrence Carin. 2021.",
                "venue": "arXiv preprint arXiv:2103.06413.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Bold: Dataset and metrics for measuring biases in open-ended language generation.",
                "author": "Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. 2021a.",
                "venue": "In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pages 862\u2013872.",
                "url": null
            }
        },
        {
            "12": {
                "title": "Bold: Dataset and metrics for measuring biases in open-ended language generation.",
                "author": "Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. 2021b.",
                "venue": "FAccT \u201921, New York, NY, USA. Association for Computing Machinery.",
                "url": "https://doi.org/10.1145/3442188.3445924"
            }
        },
        {
            "13": {
                "title": "He is very intelligent, she is very beautiful? on mitigating social biases in language modelling and generation.",
                "author": "Aparna Garimella, Akhash Amarnath, Kiran Kumar, Akash Pramod Yalla, N Anandhavelu, Niyati Chhaya, and Balaji Vasan Srinivasan. 2021.",
                "venue": "In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4534\u20134545.",
                "url": null
            }
        },
        {
            "14": {
                "title": "Auto-debias: Debiasing masked language models with automated biased prompts.",
                "author": "Yue Guo, Yi Yang, and Ahmed Abbasi. 2022.",
                "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1012\u20131023.",
                "url": null
            }
        },
        {
            "15": {
                "title": "Five sources of bias in natural language processing.",
                "author": "Dirk Hovy and Shrimai Prabhumoye. 2021.",
                "venue": "Language and Linguistics Compass, 15.",
                "url": "https://doi.org/10.1111/lnc3.12432"
            }
        },
        {
            "16": {
                "title": "Does gender matter? towards fairness in dialogue systems.",
                "author": "Haochen Liu, Jamell Dacon, Wenqi Fan, Hui Liu, Zitao Liu, and Jiliang Tang. 2019.",
                "venue": "arXiv preprint arXiv:1910.10486.",
                "url": null
            }
        },
        {
            "17": {
                "title": "It\u2019s all in the name: Mitigating gender bias with name-based counterfactual data substitution.",
                "author": "Rowan Hall Maudslay, Hila Gonen, Ryan Cotterell, and Simone Teufel. 2019a.",
                "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5267\u20135275, Hong Kong, China. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/D19-1530"
            }
        },
        {
            "18": {
                "title": "It\u2019s all in the name: Mitigating gender bias with name-based counterfactual data substitution.",
                "author": "Rowan Hall Maudslay, Hila Gonen, Ryan Cotterell, and Simone Teufel. 2019b.",
                "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5267\u20135275, Hong Kong, China. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/D19-1530"
            }
        },
        {
            "19": {
                "title": "On measuring social biases in sentence encoders.",
                "author": "Chandler May, Alex Wang, Shikha Bordia, Samuel Bowman, and Rachel Rudinger. 2019.",
                "venue": "pages 622\u2013628.",
                "url": "https://doi.org/10.18653/v1/N19-1063"
            }
        },
        {
            "20": {
                "title": "StereoSet: Measuring stereotypical bias in pretrained language models.",
                "author": "Moin Nadeem, Anna Bethke, and Siva Reddy. 2021.",
                "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 5356\u20135371, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.acl-long.416"
            }
        },
        {
            "21": {
                "title": "Investigating user perception of gender bias in image search: The role of sexism.",
                "author": "Jahna Otterbacher, Alessandro Checco, Gianluca Demartini, and Paul Clough. 2018.",
                "venue": "In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, SIGIR \u201918, page 933\u2013936, New York, NY, USA. Association for Computing Machinery.",
                "url": "https://doi.org/10.1145/3209978.3210094"
            }
        },
        {
            "22": {
                "title": "Geometric Algebra with Applications in Engineering, 1st edition.",
                "author": "Christian Perwass. 2009.",
                "venue": "Springer Publishing Company, Incorporated.",
                "url": null
            }
        },
        {
            "23": {
                "title": "Perturbation augmentation for fairer NLP.",
                "author": "Rebecca Qian, Candace Ross, Jude Fernandes, Eric Michael Smith, Douwe Kiela, and Adina Williams. 2022.",
                "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9496\u20139521, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2022.emnlp-main.646"
            }
        },
        {
            "24": {
                "title": "Language models are unsupervised multitask learners.",
                "author": "Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019.",
                "venue": null,
                "url": "https://api.semanticscholar.org/CorpusID:160025533"
            }
        },
        {
            "25": {
                "title": "Null it out: Guarding protected attributes by iterative nullspace projection.",
                "author": "Shauli Ravfogel, Yanai Elazar, Hila Gonen, Michael Twiton, and Yoav Goldberg. 2020.",
                "venue": "arXiv preprint arXiv:2004.07667.",
                "url": null
            }
        },
        {
            "26": {
                "title": "Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP.",
                "author": "Timo Schick, Sahana Udupa, and Hinrich Sch\u00fctze. 2021.",
                "venue": "Transactions of the Association for Computational Linguistics, 9:1408\u20131424.",
                "url": "https://doi.org/10.1162/tacl_a_00434"
            }
        },
        {
            "27": {
                "title": "The woman worked as a babysitter: On biases in language generation.",
                "author": "Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. 2019.",
                "venue": "arXiv preprint arXiv:1909.01326.",
                "url": null
            }
        },
        {
            "28": {
                "title": "Release strategies and the social impacts of language models.",
                "author": "Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, Gretchen Krueger, Jong Wook Kim, Sarah Kreps, et al. 2019.",
                "venue": "arXiv preprint arXiv:1908.09203.",
                "url": null
            }
        },
        {
            "29": {
                "title": "One embedder, any task: Instruction-finetuned text embeddings.",
                "author": "Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A. Smith, Luke Zettlemoyer, and Tao Yu. 2023.",
                "venue": "In Findings of the Association for Computational Linguistics: ACL 2023, pages 1102\u20131121, Toronto, Canada. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.findings-acl.71"
            }
        },
        {
            "30": {
                "title": "DIALOGPT : Large-scale generative pre-training for conversational response generation.",
                "author": "Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. 2020.",
                "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 270\u2013278, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2020.acl-demos.30"
            }
        },
        {
            "31": {
                "title": "Gender bias in contextualized word embeddings.",
                "author": "Jieyu Zhao, Tianlu Wang, Mark Yatskar, Ryan Cotterell, Vicente Ordonez, and Kai-Wei Chang. 2019.",
                "venue": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 629\u2013634, Minneapolis, Minnesota. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/N19-1064"
            }
        },
        {
            "32": {
                "title": "Gender bias in coreference resolution: Evaluation and debiasing methods.",
                "author": "Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. 2018.",
                "venue": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 15\u201320, New Orleans, Louisiana. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/N18-2003"
            }
        },
        {
            "33": {
                "title": "On large language models\u2019 selection bias in multi-choice questions.",
                "author": "Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. 2023.",
                "venue": "arXiv preprint arXiv:2309.03882.",
                "url": null
            }
        },
        {
            "34": {
                "title": "Multilingual machine translation with large language models: Empirical results and analysis.",
                "author": "Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Lingpeng Kong, Jiajun Chen, Lei Li, and Shujian Huang. 2023.",
                "venue": "arXiv preprint arXiv:2304.04675.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.00198v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "4"
        ],
        "methodology_sections": [
            "2",
            "2.1",
            "2.2",
            "2.3"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.3.1",
            "3.3.2",
            "3.3.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "3.3",
            "3.3.1",
            "3.3.2",
            "3.3.3"
        ]
    },
    "research_context": {
        "paper_id": "2403.00198v1",
        "paper_title": "Axolotl: Fairness through Assisted Self-Debiasing of Large Language Model Outputs",
        "research_background": "The paper's motivation, research problem, and the relevant prior work are as follows:\n\n### Motivation:\nLarge Language Models (LLMs) have made significant advancements in natural language processing (NLP) tasks such as text understanding, generation, and translation. However, these models inherit and perpetuate biases present in their training datasets, which often reflect societal, historical, and cultural prejudices. This inherited bias leads to unfair or discriminatory outcomes in real-world applications like automated hiring and conversational AI. Existing strategies to mitigate biases in LLMs are computationally expensive, resource-intensive, and often compromise the performance of the models. Therefore, there is a pressing need for innovative debiasing techniques that are cost-effective, perform well, and do not require access to the model's internal configurations.\n\n### Research Problem:\nThe paper addresses the challenge of mitigating biases in LLM outputs without the need for expensive pre-training or fine-tuning, and without requiring access to the internal configurations of the models (treating them as \"black boxes\"). The goal is to develop a model-agnostic and task-agnostic post-processing framework that can efficiently and effectively reduce biases in LLM outputs.\n\n### Relevant Prior Work:\n1. **Bias in LLMs**:\n   - Several studies have documented the presence and impact of biases in LLMs across various demographics and stereotypes (Bolukbasi et al., 2016; Bender et al., 2021; Zhang et al., 2020).\n\n2. **Debiasing Techniques**:\n   - Researchers have explored various debiasing methods, including modifications in sentence and word representations (May et al., 2019; Caliskan et al., 2017; Ravfogel et al., 2020), adjusting the distribution of tokens (Guo et al., 2022), and refining training datasets (Garimella et al., 2021; Maudslay et al., 2019).\n\n### Axolotl's Contributions:\n- Axolotl introduces a novel framework that uses a post-processing approach inspired by zero-shot learning techniques.\n- It operates in a three-step process: identification of bias, proposal of a resolution, and regeneration of the LLM's initial response to correct the bias.\n- Axolotl functions as a \"black box\" solution that interacts with LLMs via public APIs without needing direct access to their parameters, thus reducing computational resources.\n- It can handle non-binary demographic groups and multiple sensitive attributes, which enhances its applicability and ease of use across different platforms and models.\n\nIn summary, Axolotl aims to provide a cost-effective, efficient, and flexible solution for reducing biases in LLM outputs, standing out due to its unique approach and broad applicability.",
        "methodology": "### Methodology Description: Axolotl - Fairness through Assisted Self-Debiasing of Large Language Model Outputs\n\n#### Overview:\nThe Axolotl method aims to reduce bias in outputs generated by large language models (LLMs) by leveraging embedding vectors. The technique uses embedding vectors to detect biased outputs and subsequently rewrites these outputs with more neutral or pleasant phrases.\n\n#### Key Steps and Components:\n\n1. **Detection of Biased Outputs:**\n   - The system starts with an input prompt processed by an LLM to generate a response (output).\n   - The output is mapped to an embedding vector.\n\n2. **Identification of Bias Orientation and Unpleasant Characteristics:**\n   - Axolotl maintains a collection of vectors representing embedding vectors for different demographic groups defined by sensitive attributes (gender, race, profession, etc.).\n   - Bias in a model response is identified as a combination of an \u201corientation\u201d towards a demographic group and an \u201cunpleasant characteristic\u201d.\n   - For instance, in the phrase \u201cThe CEO went to the tailor because he needed a suit\u201d, the bias orientation is detected as male based on vector representation.\n   - The method then checks if there is an unpleasant characteristic associated with this bias orientation.\n\n3. **Pleasant Resolution:**\n   - Using sets of predefined \"unpleasant\" and \"pleasant\" words (contextualized into basic sentences and their embeddings), the system identifies a pleasant resolution for the detected bias.\n   - For example, if the unpleasant characteristic associated with the detected bias orientation is \"Manpower\", the pleasant resolution would be \"Equality\".\n\n4. **Regeneration of Bias-mitigated Output:**\n   - After detecting the bias orientation and the unpleasant characteristic, and identifying a suitable pleasant resolution, Axolotl formulates these components into a new prompt.\n   - This new prompt is then fed back into the LLM to generate a bias-mitigated response.\n\n#### Innovations:\n- **Use of Embedding Vectors:** By leveraging embedding vectors, the system can detect subtle biases in model outputs that might not be immediately apparent through simple keyword analysis.\n- **Predefined Word Lists for Bias Mitigation:** The technique uses cherry-picked words for unpleasant and pleasant characteristics to systematically replace problematic terms within the output, ensuring consistency and neutrality.\n- **Contextual Sentence-Level Analysis:** The embeddings are derived from sentences, allowing the system to take contextual nuances into account when identifying bias and selecting appropriate replacements.\n\n#### Implementation:\nThe methodology relies on creating and using specific embedding vectors for demographic groups and a structured approach to identify and resolve biased outputs. The iterative process of detecting bias and regenerating new prompts allows continuous refinement of output, enhancing the fairness of the language model.\n\nAxolotl leverages the model's capability to rewrite text accurately to mitigate bias effectively, aiming for outputs that are more inclusive and neutral, thus fairer in portrayal and sentiment.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n#### Datasets:\nThe main experiment leverages the BOLD dataset, focusing on three sensitive attributes: gender, profession, and an unspecified additional attribute.\n\n#### Baselines:\nFour models are used for comparison:\n- GPT-3.5\n- Llama2-70B\n- Llama2-13B\n- Llama2-7B\n\n#### Evaluation Metrics:\n1. **Sentiment**: Classifies the overall sentiment of each sentence as \"positive\", \"negative\", or \"neutral\".\n2. **Toxicity**: Measures the harmful or unpleasant content within the textual data, assigning a score between 0 and 100.\n3. **Regard**: Specifically evaluates the sentiment toward a demographic group, with a negative regard indicating harmful bias.\n\nThe primary objective is to reduce the proportion of negative regard, thus mitigating harmful biases.\n\n### Main Experimental Results\n\n#### Regard:\n- **Reduction in Negative Regard**: The results show a significant reduction in negatively classified regard by up to 50% for gender in the Llama2-70B model. This implies that harmful biases were effectively mitigated.\n- **Minimal Changes in Positive Regard**: Positive regard remains largely unchanged, suggesting that the method selectively reduces negative bias without impacting positive assessments.\n\n#### Sentiment:\n- **Increase in Positive Sentiment**: Results consistently show an increase in positive sentiment and a decrease in negative sentiment across all models.\n- **Performance of Smaller Models**: Llama2-13B and Llama2-7B occasionally surpass or match the performance of larger models post-rewrite, notably in the BOLD-profession category where Llama2-13B exhibited better positive sentiment than other models after intervention.\n\n#### Toxicity:\n- **Reduction in Toxicity**: All models demonstrated a reduction in toxicity post-rewrite, with Llama2-13B showing the highest reduction, particularly for the gender attribute (a 31% decrease).\n- **Baseline Toxicity Levels**: Due to pre-existing internal settings in models to prevent toxic behavior, the overall textual toxicity was not initially high, which results in relatively modest percentage improvements.\n\nOverall, the Axolotl method effectively reduces harmful biases and enhances the sentiment and toxicity metrics, particularly benefiting smaller models like Llama2-13B."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "The goal of this ablation study is to evaluate the effectiveness of Axolotl in mitigating biases related to sentiment, regard, and toxicity in large language model outputs.",
            "experiment_process": "The study uses sentiment, toxicity, and regard as evaluation metrics. Toxicity labels textual data on a scale from 0 to 100, while regard and sentiment classifiers output 'positive', 'negative', or 'neutral' categories. The study involves four models and three sensitive attributes, gender being a significant focus. Table 6 presents experimental results, showing a reduction in negative regard and minimal changes in positive regard results. The sentiment classifier's results, shown in Table 5, indicate an increase in positive sentiment and a decrease in negative sentiment across models. For toxicity, Table 3 displays percentage reductions post-rewrite compared to pre-rewrite, with llama2-13B showing the highest decrease.",
            "result_discussion": "The outcomes indicate Axolotl's success in reducing negative regard across models, notably cutting it in half for gender attributes in Llama2-70B. The increase in positive sentiment across models further illustrates the framework's effectiveness. Smaller models like llama2-13B sometimes outperformed larger models. Additionally, llama2-13B showed the highest reduction in toxicity, especially for the gender attribute, with a 31% decrease. These findings underscore Axolotl's capability in debiasing text while maintaining overall model performance.",
            "ablation_id": "2403.00198v1.No1"
        },
        {
            "research_objective": "The objective is to assess Axolotl's ability to select less biased multi-choice responses in question answering tasks.",
            "experiment_process": "The experiment used the Stereoset dataset, which includes context sentences with target groups and three corresponding sentences labeled as 'stereotype', 'anti-stereotype', and 'meaningless'. The model aims to choose the most suitable sentence. The Stereotype Score (ss) measures the bias level, with a lower score indicating a preference for anti-stereotype responses. The study examines changes in the ss score across various models.",
            "result_discussion": "Experimental results show a visible decrease in ss across all models and sensitive attributes, indicating an increased preference for anti-stereotype responses. In scenarios where ss already leaned towards anti-stereotypes, minimal improvement was observed. Notably, a 10.1 drop in ss for GPT-3.5 and a 13.29 decrease for Llama2-70B demonstrate Axolotl\u2019s debiasing success, especially in profession-related tasks.",
            "ablation_id": "2403.00198v1.No2"
        },
        {
            "research_objective": "To evaluate the model\u2019s effectiveness in reducing gender bias in co-reference resolution tasks.",
            "experiment_process": "The co-reference resolution task used the WinoBias dataset, involving sentences linking professions to pronouns. The model selects a pronoun from 'He/his', 'She/her', or 'They/them', aiming to avoid stereotypical gender associations. Axolotl\u2019s guidance helps generate more appropriate responses. The effectiveness of the model is gauged by the decrease in gender-biased responses post-rewrite.",
            "result_discussion": "Results showed significant decreases in gender bias, with over 82% of responses being gender-neutral post-rewrite. For example, llama2-7B improved from 88.4% male and 8.4% female responses to 82.3% neutral responses post-rewrite, outperforming larger models like llama2-70B. These results underscore Axolotl's success in generating gender-neutral responses in co-reference tasks.",
            "ablation_id": "2403.00198v1.No3"
        },
        {
            "research_objective": "To evaluate Axolotl's ability to generate coherent and debiased responses in a conversational setting.",
            "experiment_process": "The experiment used the BOLD dataset, focusing on race, gender, and profession-related domains. Each prompt acts as a 'prefix' for a conversation, and the model completes the text based on debiasing instructions identifying unpleasant characteristics and suggesting pleasant resolutions. Evaluations include metrics for sentiment, toxicity, and regard.",
            "result_discussion": "Results indicate decreases in negative regard and increases in positive sentiment across models. Notably, Llama2-70B showed a substantial reduction in negative regard by half. Models, including smaller ones like llama2-13B, showed improvements in sentiment and toxicity reduction. For instance, llama2-13B achieved a 31% reduction in gender-based toxicity, demonstrating Axolotl\u2019s overall effectiveness in a conversational context.",
            "ablation_id": "2403.00198v1.No4"
        }
    ]
}